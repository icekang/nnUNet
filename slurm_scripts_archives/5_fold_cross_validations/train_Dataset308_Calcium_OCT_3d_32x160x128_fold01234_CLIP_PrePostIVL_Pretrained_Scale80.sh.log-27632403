/home/gridsan/nchutisilp/.conda/envs/nnunet/bin/python
Starting training with CONFIG=3d_32x160x128_b10, DATASET_ID=308, TRAINER=nnUNetTrainerScaleAnalysis80

############################
INFO: You are using the old nnU-Net default plans. We have updated our recommendations. Please consider using those instead! Read more here: https://github.com/MIC-DKFZ/nnUNet/blob/master/documentation/resenc_presets.md
############################

Using device: cuda:0

#######################################################################
Please cite the following paper when using nnU-Net:
Isensee, F., Jaeger, P. F., Kohl, S. A., Petersen, J., & Maier-Hein, K. H. (2021). nnU-Net: a self-configuring method for deep learning-based biomedical image segmentation. Nature methods, 18(2), 203-211.
#######################################################################

2024-12-19 12:04:22.631535: Using torch.compile...

############################
INFO: You are using the old nnU-Net default plans. We have updated our recommendations. Please consider using those instead! Read more here: https://github.com/MIC-DKFZ/nnUNet/blob/master/documentation/resenc_presets.md
############################

Using device: cuda:0

#######################################################################
Please cite the following paper when using nnU-Net:
Isensee, F., Jaeger, P. F., Kohl, S. A., Petersen, J., & Maier-Hein, K. H. (2021). nnU-Net: a self-configuring method for deep learning-based biomedical image segmentation. Nature methods, 18(2), 203-211.
#######################################################################

2024-12-19 12:04:22.634948: Using torch.compile...
/home/gridsan/nchutisilp/.local/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn("The verbose parameter is deprecated. Please use get_last_lr() "
/home/gridsan/nchutisilp/.local/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn("The verbose parameter is deprecated. Please use get_last_lr() "
################### Loading pretrained weights from file  /home/gridsan/nchutisilp/projects/OpenAI-CLIP/clip_pretrained_nnUNet.pt ###################
Below is the list of overlapping blocks in pretrained model and nnUNet architecture:
encoder.stages.0.0.convs.0.conv.weight shape torch.Size([32, 1, 3, 3, 3])
encoder.stages.0.0.convs.0.conv.bias shape torch.Size([32])
encoder.stages.0.0.convs.0.norm.weight shape torch.Size([32])
encoder.stages.0.0.convs.0.norm.bias shape torch.Size([32])
encoder.stages.0.0.convs.0.all_modules.0.weight shape torch.Size([32, 1, 3, 3, 3])
encoder.stages.0.0.convs.0.all_modules.0.bias shape torch.Size([32])
encoder.stages.0.0.convs.0.all_modules.1.weight shape torch.Size([32])
encoder.stages.0.0.convs.0.all_modules.1.bias shape torch.Size([32])
encoder.stages.0.0.convs.1.conv.weight shape torch.Size([32, 32, 3, 3, 3])
encoder.stages.0.0.convs.1.conv.bias shape torch.Size([32])
encoder.stages.0.0.convs.1.norm.weight shape torch.Size([32])
encoder.stages.0.0.convs.1.norm.bias shape torch.Size([32])
encoder.stages.0.0.convs.1.all_modules.0.weight shape torch.Size([32, 32, 3, 3, 3])
encoder.stages.0.0.convs.1.all_modules.0.bias shape torch.Size([32])
encoder.stages.0.0.convs.1.all_modules.1.weight shape torch.Size([32])
encoder.stages.0.0.convs.1.all_modules.1.bias shape torch.Size([32])
encoder.stages.1.0.convs.0.conv.weight shape torch.Size([64, 32, 3, 3, 3])
encoder.stages.1.0.convs.0.conv.bias shape torch.Size([64])
encoder.stages.1.0.convs.0.norm.weight shape torch.Size([64])
encoder.stages.1.0.convs.0.norm.bias shape torch.Size([64])
encoder.stages.1.0.convs.0.all_modules.0.weight shape torch.Size([64, 32, 3, 3, 3])
encoder.stages.1.0.convs.0.all_modules.0.bias shape torch.Size([64])
encoder.stages.1.0.convs.0.all_modules.1.weight shape torch.Size([64])
encoder.stages.1.0.convs.0.all_modules.1.bias shape torch.Size([64])
encoder.stages.1.0.convs.1.conv.weight shape torch.Size([64, 64, 3, 3, 3])
encoder.stages.1.0.convs.1.conv.bias shape torch.Size([64])
encoder.stages.1.0.convs.1.norm.weight shape torch.Size([64])
encoder.stages.1.0.convs.1.norm.bias shape torch.Size([64])
encoder.stages.1.0.convs.1.all_modules.0.weight shape torch.Size([64, 64, 3, 3, 3])
encoder.stages.1.0.convs.1.all_modules.0.bias shape torch.Size([64])
encoder.stages.1.0.convs.1.all_modules.1.weight shape torch.Size([64])
encoder.stages.1.0.convs.1.all_modules.1.bias shape torch.Size([64])
encoder.stages.2.0.convs.0.conv.weight shape torch.Size([128, 64, 3, 3, 3])
encoder.stages.2.0.convs.0.conv.bias shape torch.Size([128])
encoder.stages.2.0.convs.0.norm.weight shape torch.Size([128])
encoder.stages.2.0.convs.0.norm.bias shape torch.Size([128])
encoder.stages.2.0.convs.0.all_modules.0.weight shape torch.Size([128, 64, 3, 3, 3])
encoder.stages.2.0.convs.0.all_modules.0.bias shape torch.Size([128])
encoder.stages.2.0.convs.0.all_modules.1.weight shape torch.Size([128])
encoder.stages.2.0.convs.0.all_modules.1.bias shape torch.Size([128])
encoder.stages.2.0.convs.1.conv.weight shape torch.Size([128, 128, 3, 3, 3])
encoder.stages.2.0.convs.1.conv.bias shape torch.Size([128])
encoder.stages.2.0.convs.1.norm.weight shape torch.Size([128])
encoder.stages.2.0.convs.1.norm.bias shape torch.Size([128])
encoder.stages.2.0.convs.1.all_modules.0.weight shape torch.Size([128, 128, 3, 3, 3])
encoder.stages.2.0.convs.1.all_modules.0.bias shape torch.Size([128])
encoder.stages.2.0.convs.1.all_modules.1.weight shape torch.Size([128])
encoder.stages.2.0.convs.1.all_modules.1.bias shape torch.Size([128])
encoder.stages.3.0.convs.0.conv.weight shape torch.Size([256, 128, 3, 3, 3])
encoder.stages.3.0.convs.0.conv.bias shape torch.Size([256])
encoder.stages.3.0.convs.0.norm.weight shape torch.Size([256])
encoder.stages.3.0.convs.0.norm.bias shape torch.Size([256])
encoder.stages.3.0.convs.0.all_modules.0.weight shape torch.Size([256, 128, 3, 3, 3])
encoder.stages.3.0.convs.0.all_modules.0.bias shape torch.Size([256])
encoder.stages.3.0.convs.0.all_modules.1.weight shape torch.Size([256])
encoder.stages.3.0.convs.0.all_modules.1.bias shape torch.Size([256])
encoder.stages.3.0.convs.1.conv.weight shape torch.Size([256, 256, 3, 3, 3])
encoder.stages.3.0.convs.1.conv.bias shape torch.Size([256])
encoder.stages.3.0.convs.1.norm.weight shape torch.Size([256])
encoder.stages.3.0.convs.1.norm.bias shape torch.Size([256])
encoder.stages.3.0.convs.1.all_modules.0.weight shape torch.Size([256, 256, 3, 3, 3])
encoder.stages.3.0.convs.1.all_modules.0.bias shape torch.Size([256])
encoder.stages.3.0.convs.1.all_modules.1.weight shape torch.Size([256])
encoder.stages.3.0.convs.1.all_modules.1.bias shape torch.Size([256])
encoder.stages.4.0.convs.0.conv.weight shape torch.Size([320, 256, 3, 3, 3])
encoder.stages.4.0.convs.0.conv.bias shape torch.Size([320])
encoder.stages.4.0.convs.0.norm.weight shape torch.Size([320])
encoder.stages.4.0.convs.0.norm.bias shape torch.Size([320])
encoder.stages.4.0.convs.0.all_modules.0.weight shape torch.Size([320, 256, 3, 3, 3])
encoder.stages.4.0.convs.0.all_modules.0.bias shape torch.Size([320])
encoder.stages.4.0.convs.0.all_modules.1.weight shape torch.Size([320])
encoder.stages.4.0.convs.0.all_modules.1.bias shape torch.Size([320])
encoder.stages.4.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.4.0.convs.1.conv.bias shape torch.Size([320])
encoder.stages.4.0.convs.1.norm.weight shape torch.Size([320])
encoder.stages.4.0.convs.1.norm.bias shape torch.Size([320])
encoder.stages.4.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.4.0.convs.1.all_modules.0.bias shape torch.Size([320])
encoder.stages.4.0.convs.1.all_modules.1.weight shape torch.Size([320])
encoder.stages.4.0.convs.1.all_modules.1.bias shape torch.Size([320])
encoder.stages.5.0.convs.0.conv.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.5.0.convs.0.conv.bias shape torch.Size([320])
encoder.stages.5.0.convs.0.norm.weight shape torch.Size([320])
encoder.stages.5.0.convs.0.norm.bias shape torch.Size([320])
encoder.stages.5.0.convs.0.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.5.0.convs.0.all_modules.0.bias shape torch.Size([320])
encoder.stages.5.0.convs.0.all_modules.1.weight shape torch.Size([320])
encoder.stages.5.0.convs.0.all_modules.1.bias shape torch.Size([320])
encoder.stages.5.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.5.0.convs.1.conv.bias shape torch.Size([320])
encoder.stages.5.0.convs.1.norm.weight shape torch.Size([320])
encoder.stages.5.0.convs.1.norm.bias shape torch.Size([320])
encoder.stages.5.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.5.0.convs.1.all_modules.0.bias shape torch.Size([320])
encoder.stages.5.0.convs.1.all_modules.1.weight shape torch.Size([320])
encoder.stages.5.0.convs.1.all_modules.1.bias shape torch.Size([320])
decoder.encoder.stages.0.0.convs.0.conv.weight shape torch.Size([32, 1, 3, 3, 3])
decoder.encoder.stages.0.0.convs.0.conv.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.norm.weight shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.norm.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.all_modules.0.weight shape torch.Size([32, 1, 3, 3, 3])
decoder.encoder.stages.0.0.convs.0.all_modules.0.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.all_modules.1.weight shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.all_modules.1.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.conv.weight shape torch.Size([32, 32, 3, 3, 3])
decoder.encoder.stages.0.0.convs.1.conv.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.norm.weight shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.norm.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.all_modules.0.weight shape torch.Size([32, 32, 3, 3, 3])
decoder.encoder.stages.0.0.convs.1.all_modules.0.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.all_modules.1.weight shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.all_modules.1.bias shape torch.Size([32])
decoder.encoder.stages.1.0.convs.0.conv.weight shape torch.Size([64, 32, 3, 3, 3])
decoder.encoder.stages.1.0.convs.0.conv.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.norm.weight shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.norm.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.all_modules.0.weight shape torch.Size([64, 32, 3, 3, 3])
decoder.encoder.stages.1.0.convs.0.all_modules.0.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.all_modules.1.weight shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.all_modules.1.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.conv.weight shape torch.Size([64, 64, 3, 3, 3])
decoder.encoder.stages.1.0.convs.1.conv.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.norm.weight shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.norm.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.all_modules.0.weight shape torch.Size([64, 64, 3, 3, 3])
decoder.encoder.stages.1.0.convs.1.all_modules.0.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.all_modules.1.weight shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.all_modules.1.bias shape torch.Size([64])
decoder.encoder.stages.2.0.convs.0.conv.weight shape torch.Size([128, 64, 3, 3, 3])
decoder.encoder.stages.2.0.convs.0.conv.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.norm.weight shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.norm.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.all_modules.0.weight shape torch.Size([128, 64, 3, 3, 3])
decoder.encoder.stages.2.0.convs.0.all_modules.0.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.all_modules.1.weight shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.all_modules.1.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.conv.weight shape torch.Size([128, 128, 3, 3, 3])
decoder.encoder.stages.2.0.convs.1.conv.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.norm.weight shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.norm.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.all_modules.0.weight shape torch.Size([128, 128, 3, 3, 3])
decoder.encoder.stages.2.0.convs.1.all_modules.0.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.all_modules.1.weight shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.all_modules.1.bias shape torch.Size([128])
decoder.encoder.stages.3.0.convs.0.conv.weight shape torch.Size([256, 128, 3, 3, 3])
decoder.encoder.stages.3.0.convs.0.conv.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.norm.weight shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.norm.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.all_modules.0.weight shape torch.Size([256, 128, 3, 3, 3])
decoder.encoder.stages.3.0.convs.0.all_modules.0.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.all_modules.1.weight shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.all_modules.1.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.conv.weight shape torch.Size([256, 256, 3, 3, 3])
decoder.encoder.stages.3.0.convs.1.conv.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.norm.weight shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.norm.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.all_modules.0.weight shape torch.Size([256, 256, 3, 3, 3])
decoder.encoder.stages.3.0.convs.1.all_modules.0.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.all_modules.1.weight shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.all_modules.1.bias shape torch.Size([256])
decoder.encoder.stages.4.0.convs.0.conv.weight shape torch.Size([320, 256, 3, 3, 3])
decoder.encoder.stages.4.0.convs.0.conv.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.norm.weight shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.norm.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.all_modules.0.weight shape torch.Size([320, 256, 3, 3, 3])
decoder.encoder.stages.4.0.convs.0.all_modules.0.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.all_modules.1.weight shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.all_modules.1.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.4.0.convs.1.conv.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.norm.weight shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.norm.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.4.0.convs.1.all_modules.0.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.all_modules.1.weight shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.all_modules.1.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.conv.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.5.0.convs.0.conv.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.norm.weight shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.norm.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.5.0.convs.0.all_modules.0.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.all_modules.1.weight shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.all_modules.1.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.5.0.convs.1.conv.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.norm.weight shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.norm.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.5.0.convs.1.all_modules.0.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.all_modules.1.weight shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.all_modules.1.bias shape torch.Size([320])
decoder.stages.0.convs.0.conv.weight shape torch.Size([320, 640, 3, 3, 3])
decoder.stages.0.convs.0.conv.bias shape torch.Size([320])
decoder.stages.0.convs.0.norm.weight shape torch.Size([320])
decoder.stages.0.convs.0.norm.bias shape torch.Size([320])
decoder.stages.0.convs.0.all_modules.0.weight shape torch.Size([320, 640, 3, 3, 3])
decoder.stages.0.convs.0.all_modules.0.bias shape torch.Size([320])
decoder.stages.0.convs.0.all_modules.1.weight shape torch.Size([320])
decoder.stages.0.convs.0.all_modules.1.bias shape torch.Size([320])
decoder.stages.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.stages.0.convs.1.conv.bias shape torch.Size([320])
decoder.stages.0.convs.1.norm.weight shape torch.Size([320])
decoder.stages.0.convs.1.norm.bias shape torch.Size([320])
decoder.stages.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.stages.0.convs.1.all_modules.0.bias shape torch.Size([320])
decoder.stages.0.convs.1.all_modules.1.weight shape torch.Size([320])
decoder.stages.0.convs.1.all_modules.1.bias shape torch.Size([320])
decoder.stages.1.convs.0.conv.weight shape torch.Size([256, 512, 3, 3, 3])
decoder.stages.1.convs.0.conv.bias shape torch.Size([256])
decoder.stages.1.convs.0.norm.weight shape torch.Size([256])
decoder.stages.1.convs.0.norm.bias shape torch.Size([256])
decoder.stages.1.convs.0.all_modules.0.weight shape torch.Size([256, 512, 3, 3, 3])
decoder.stages.1.convs.0.all_modules.0.bias shape torch.Size([256])
decoder.stages.1.convs.0.all_modules.1.weight shape torch.Size([256])
decoder.stages.1.convs.0.all_modules.1.bias shape torch.Size([256])
decoder.stages.1.convs.1.conv.weight shape torch.Size([256, 256, 3, 3, 3])
decoder.stages.1.convs.1.conv.bias shape torch.Size([256])
decoder.stages.1.convs.1.norm.weight shape torch.Size([256])
decoder.stages.1.convs.1.norm.bias shape torch.Size([256])
decoder.stages.1.convs.1.all_modules.0.weight shape torch.Size([256, 256, 3, 3, 3])
decoder.stages.1.convs.1.all_modules.0.bias shape torch.Size([256])
decoder.stages.1.convs.1.all_modules.1.weight shape torch.Size([256])
decoder.stages.1.convs.1.all_modules.1.bias shape torch.Size([256])
decoder.stages.2.convs.0.conv.weight shape torch.Size([128, 256, 3, 3, 3])
decoder.stages.2.convs.0.conv.bias shape torch.Size([128])
decoder.stages.2.convs.0.norm.weight shape torch.Size([128])
decoder.stages.2.convs.0.norm.bias shape torch.Size([128])
decoder.stages.2.convs.0.all_modules.0.weight shape torch.Size([128, 256, 3, 3, 3])
decoder.stages.2.convs.0.all_modules.0.bias shape torch.Size([128])
decoder.stages.2.convs.0.all_modules.1.weight shape torch.Size([128])
decoder.stages.2.convs.0.all_modules.1.bias shape torch.Size([128])
decoder.stages.2.convs.1.conv.weight shape torch.Size([128, 128, 3, 3, 3])
decoder.stages.2.convs.1.conv.bias shape torch.Size([128])
decoder.stages.2.convs.1.norm.weight shape torch.Size([128])
decoder.stages.2.convs.1.norm.bias shape torch.Size([128])
decoder.stages.2.convs.1.all_modules.0.weight shape torch.Size([128, 128, 3, 3, 3])
decoder.stages.2.convs.1.all_modules.0.bias shape torch.Size([128])
decoder.stages.2.convs.1.all_modules.1.weight shape torch.Size([128])
decoder.stages.2.convs.1.all_modules.1.bias shape torch.Size([128])
decoder.stages.3.convs.0.conv.weight shape torch.Size([64, 128, 3, 3, 3])
decoder.stages.3.convs.0.conv.bias shape torch.Size([64])
decoder.stages.3.convs.0.norm.weight shape torch.Size([64])
decoder.stages.3.convs.0.norm.bias shape torch.Size([64])
decoder.stages.3.convs.0.all_modules.0.weight shape torch.Size([64, 128, 3, 3, 3])
decoder.stages.3.convs.0.all_modules.0.bias shape torch.Size([64])
decoder.stages.3.convs.0.all_modules.1.weight shape torch.Size([64])
decoder.stages.3.convs.0.all_modules.1.bias shape torch.Size([64])
decoder.stages.3.convs.1.conv.weight shape torch.Size([64, 64, 3, 3, 3])
decoder.stages.3.convs.1.conv.bias shape torch.Size([64])
decoder.stages.3.convs.1.norm.weight shape torch.Size([64])
decoder.stages.3.convs.1.norm.bias shape torch.Size([64])
decoder.stages.3.convs.1.all_modules.0.weight shape torch.Size([64, 64, 3, 3, 3])
decoder.stages.3.convs.1.all_modules.0.bias shape torch.Size([64])
decoder.stages.3.convs.1.all_modules.1.weight shape torch.Size([64])
decoder.stages.3.convs.1.all_modules.1.bias shape torch.Size([64])
decoder.stages.4.convs.0.conv.weight shape torch.Size([32, 64, 3, 3, 3])
decoder.stages.4.convs.0.conv.bias shape torch.Size([32])
decoder.stages.4.convs.0.norm.weight shape torch.Size([32])
decoder.stages.4.convs.0.norm.bias shape torch.Size([32])
decoder.stages.4.convs.0.all_modules.0.weight shape torch.Size([32, 64, 3, 3, 3])
decoder.stages.4.convs.0.all_modules.0.bias shape torch.Size([32])
decoder.stages.4.convs.0.all_modules.1.weight shape torch.Size([32])
decoder.stages.4.convs.0.all_modules.1.bias shape torch.Size([32])
decoder.stages.4.convs.1.conv.weight shape torch.Size([32, 32, 3, 3, 3])
decoder.stages.4.convs.1.conv.bias shape torch.Size([32])
decoder.stages.4.convs.1.norm.weight shape torch.Size([32])
decoder.stages.4.convs.1.norm.bias shape torch.Size([32])
decoder.stages.4.convs.1.all_modules.0.weight shape torch.Size([32, 32, 3, 3, 3])
decoder.stages.4.convs.1.all_modules.0.bias shape torch.Size([32])
decoder.stages.4.convs.1.all_modules.1.weight shape torch.Size([32])
decoder.stages.4.convs.1.all_modules.1.bias shape torch.Size([32])
decoder.transpconvs.0.weight shape torch.Size([320, 320, 1, 2, 2])
decoder.transpconvs.0.bias shape torch.Size([320])
decoder.transpconvs.1.weight shape torch.Size([320, 256, 2, 2, 2])
decoder.transpconvs.1.bias shape torch.Size([256])
decoder.transpconvs.2.weight shape torch.Size([256, 128, 2, 2, 2])
decoder.transpconvs.2.bias shape torch.Size([128])
decoder.transpconvs.3.weight shape torch.Size([128, 64, 2, 2, 2])
decoder.transpconvs.3.bias shape torch.Size([64])
decoder.transpconvs.4.weight shape torch.Size([64, 32, 2, 2, 2])
decoder.transpconvs.4.bias shape torch.Size([32])
################### Done ###################
2024-12-19 12:04:41.454778: do_dummy_2d_data_aug: True
2024-12-19 12:04:41.745214: Using splits from existing split file: /home/gridsan/nchutisilp/datasets/nnUNet_Datasets/nnUNet_preprocessed/Dataset308_Sohee_Calcium_OCT_CrossValidation/splits_final_80.json
2024-12-19 12:04:41.764594: The split file contains 5 splits.
2024-12-19 12:04:41.766302: Desired fold for training: 0
2024-12-19 12:04:41.767604: This split has 6 training and 4 validation cases.
################### Loading pretrained weights from file  /home/gridsan/nchutisilp/projects/OpenAI-CLIP/clip_pretrained_nnUNet.pt ###################
Below is the list of overlapping blocks in pretrained model and nnUNet architecture:
encoder.stages.0.0.convs.0.conv.weight shape torch.Size([32, 1, 3, 3, 3])
encoder.stages.0.0.convs.0.conv.bias shape torch.Size([32])
encoder.stages.0.0.convs.0.norm.weight shape torch.Size([32])
encoder.stages.0.0.convs.0.norm.bias shape torch.Size([32])
encoder.stages.0.0.convs.0.all_modules.0.weight shape torch.Size([32, 1, 3, 3, 3])
encoder.stages.0.0.convs.0.all_modules.0.bias shape torch.Size([32])
encoder.stages.0.0.convs.0.all_modules.1.weight shape torch.Size([32])
encoder.stages.0.0.convs.0.all_modules.1.bias shape torch.Size([32])
encoder.stages.0.0.convs.1.conv.weight shape torch.Size([32, 32, 3, 3, 3])
encoder.stages.0.0.convs.1.conv.bias shape torch.Size([32])
encoder.stages.0.0.convs.1.norm.weight shape torch.Size([32])
encoder.stages.0.0.convs.1.norm.bias shape torch.Size([32])
encoder.stages.0.0.convs.1.all_modules.0.weight shape torch.Size([32, 32, 3, 3, 3])
encoder.stages.0.0.convs.1.all_modules.0.bias shape torch.Size([32])
encoder.stages.0.0.convs.1.all_modules.1.weight shape torch.Size([32])
encoder.stages.0.0.convs.1.all_modules.1.bias shape torch.Size([32])
encoder.stages.1.0.convs.0.conv.weight shape torch.Size([64, 32, 3, 3, 3])
encoder.stages.1.0.convs.0.conv.bias shape torch.Size([64])
encoder.stages.1.0.convs.0.norm.weight shape torch.Size([64])
encoder.stages.1.0.convs.0.norm.bias shape torch.Size([64])
encoder.stages.1.0.convs.0.all_modules.0.weight shape torch.Size([64, 32, 3, 3, 3])
encoder.stages.1.0.convs.0.all_modules.0.bias shape torch.Size([64])
encoder.stages.1.0.convs.0.all_modules.1.weight shape torch.Size([64])
encoder.stages.1.0.convs.0.all_modules.1.bias shape torch.Size([64])
encoder.stages.1.0.convs.1.conv.weight shape torch.Size([64, 64, 3, 3, 3])
encoder.stages.1.0.convs.1.conv.bias shape torch.Size([64])
encoder.stages.1.0.convs.1.norm.weight shape torch.Size([64])
encoder.stages.1.0.convs.1.norm.bias shape torch.Size([64])
encoder.stages.1.0.convs.1.all_modules.0.weight shape torch.Size([64, 64, 3, 3, 3])
encoder.stages.1.0.convs.1.all_modules.0.bias shape torch.Size([64])
encoder.stages.1.0.convs.1.all_modules.1.weight shape torch.Size([64])
encoder.stages.1.0.convs.1.all_modules.1.bias shape torch.Size([64])
encoder.stages.2.0.convs.0.conv.weight shape torch.Size([128, 64, 3, 3, 3])
encoder.stages.2.0.convs.0.conv.bias shape torch.Size([128])
encoder.stages.2.0.convs.0.norm.weight shape torch.Size([128])
encoder.stages.2.0.convs.0.norm.bias shape torch.Size([128])
encoder.stages.2.0.convs.0.all_modules.0.weight shape torch.Size([128, 64, 3, 3, 3])
encoder.stages.2.0.convs.0.all_modules.0.bias shape torch.Size([128])
encoder.stages.2.0.convs.0.all_modules.1.weight shape torch.Size([128])
encoder.stages.2.0.convs.0.all_modules.1.bias shape torch.Size([128])
encoder.stages.2.0.convs.1.conv.weight shape torch.Size([128, 128, 3, 3, 3])
encoder.stages.2.0.convs.1.conv.bias shape torch.Size([128])
encoder.stages.2.0.convs.1.norm.weight shape torch.Size([128])
encoder.stages.2.0.convs.1.norm.bias shape torch.Size([128])
encoder.stages.2.0.convs.1.all_modules.0.weight shape torch.Size([128, 128, 3, 3, 3])
encoder.stages.2.0.convs.1.all_modules.0.bias shape torch.Size([128])
encoder.stages.2.0.convs.1.all_modules.1.weight shape torch.Size([128])
encoder.stages.2.0.convs.1.all_modules.1.bias shape torch.Size([128])
encoder.stages.3.0.convs.0.conv.weight shape torch.Size([256, 128, 3, 3, 3])
encoder.stages.3.0.convs.0.conv.bias shape torch.Size([256])
encoder.stages.3.0.convs.0.norm.weight shape torch.Size([256])
encoder.stages.3.0.convs.0.norm.bias shape torch.Size([256])
encoder.stages.3.0.convs.0.all_modules.0.weight shape torch.Size([256, 128, 3, 3, 3])
encoder.stages.3.0.convs.0.all_modules.0.bias shape torch.Size([256])
encoder.stages.3.0.convs.0.all_modules.1.weight shape torch.Size([256])
encoder.stages.3.0.convs.0.all_modules.1.bias shape torch.Size([256])
encoder.stages.3.0.convs.1.conv.weight shape torch.Size([256, 256, 3, 3, 3])
encoder.stages.3.0.convs.1.conv.bias shape torch.Size([256])
encoder.stages.3.0.convs.1.norm.weight shape torch.Size([256])
encoder.stages.3.0.convs.1.norm.bias shape torch.Size([256])
encoder.stages.3.0.convs.1.all_modules.0.weight shape torch.Size([256, 256, 3, 3, 3])
encoder.stages.3.0.convs.1.all_modules.0.bias shape torch.Size([256])
encoder.stages.3.0.convs.1.all_modules.1.weight shape torch.Size([256])
encoder.stages.3.0.convs.1.all_modules.1.bias shape torch.Size([256])
encoder.stages.4.0.convs.0.conv.weight shape torch.Size([320, 256, 3, 3, 3])
encoder.stages.4.0.convs.0.conv.bias shape torch.Size([320])
encoder.stages.4.0.convs.0.norm.weight shape torch.Size([320])
encoder.stages.4.0.convs.0.norm.bias shape torch.Size([320])
encoder.stages.4.0.convs.0.all_modules.0.weight shape torch.Size([320, 256, 3, 3, 3])
encoder.stages.4.0.convs.0.all_modules.0.bias shape torch.Size([320])
encoder.stages.4.0.convs.0.all_modules.1.weight shape torch.Size([320])
encoder.stages.4.0.convs.0.all_modules.1.bias shape torch.Size([320])
encoder.stages.4.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.4.0.convs.1.conv.bias shape torch.Size([320])
encoder.stages.4.0.convs.1.norm.weight shape torch.Size([320])
encoder.stages.4.0.convs.1.norm.bias shape torch.Size([320])
encoder.stages.4.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.4.0.convs.1.all_modules.0.bias shape torch.Size([320])
encoder.stages.4.0.convs.1.all_modules.1.weight shape torch.Size([320])
encoder.stages.4.0.convs.1.all_modules.1.bias shape torch.Size([320])
encoder.stages.5.0.convs.0.conv.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.5.0.convs.0.conv.bias shape torch.Size([320])
encoder.stages.5.0.convs.0.norm.weight shape torch.Size([320])
encoder.stages.5.0.convs.0.norm.bias shape torch.Size([320])
encoder.stages.5.0.convs.0.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.5.0.convs.0.all_modules.0.bias shape torch.Size([320])
encoder.stages.5.0.convs.0.all_modules.1.weight shape torch.Size([320])
encoder.stages.5.0.convs.0.all_modules.1.bias shape torch.Size([320])
encoder.stages.5.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.5.0.convs.1.conv.bias shape torch.Size([320])
encoder.stages.5.0.convs.1.norm.weight shape torch.Size([320])
encoder.stages.5.0.convs.1.norm.bias shape torch.Size([320])
encoder.stages.5.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.5.0.convs.1.all_modules.0.bias shape torch.Size([320])
encoder.stages.5.0.convs.1.all_modules.1.weight shape torch.Size([320])
encoder.stages.5.0.convs.1.all_modules.1.bias shape torch.Size([320])
decoder.encoder.stages.0.0.convs.0.conv.weight shape torch.Size([32, 1, 3, 3, 3])
decoder.encoder.stages.0.0.convs.0.conv.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.norm.weight shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.norm.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.all_modules.0.weight shape torch.Size([32, 1, 3, 3, 3])
decoder.encoder.stages.0.0.convs.0.all_modules.0.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.all_modules.1.weight shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.all_modules.1.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.conv.weight shape torch.Size([32, 32, 3, 3, 3])
decoder.encoder.stages.0.0.convs.1.conv.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.norm.weight shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.norm.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.all_modules.0.weight shape torch.Size([32, 32, 3, 3, 3])
decoder.encoder.stages.0.0.convs.1.all_modules.0.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.all_modules.1.weight shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.all_modules.1.bias shape torch.Size([32])
decoder.encoder.stages.1.0.convs.0.conv.weight shape torch.Size([64, 32, 3, 3, 3])
decoder.encoder.stages.1.0.convs.0.conv.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.norm.weight shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.norm.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.all_modules.0.weight shape torch.Size([64, 32, 3, 3, 3])
decoder.encoder.stages.1.0.convs.0.all_modules.0.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.all_modules.1.weight shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.all_modules.1.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.conv.weight shape torch.Size([64, 64, 3, 3, 3])
decoder.encoder.stages.1.0.convs.1.conv.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.norm.weight shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.norm.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.all_modules.0.weight shape torch.Size([64, 64, 3, 3, 3])
decoder.encoder.stages.1.0.convs.1.all_modules.0.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.all_modules.1.weight shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.all_modules.1.bias shape torch.Size([64])
decoder.encoder.stages.2.0.convs.0.conv.weight shape torch.Size([128, 64, 3, 3, 3])
decoder.encoder.stages.2.0.convs.0.conv.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.norm.weight shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.norm.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.all_modules.0.weight shape torch.Size([128, 64, 3, 3, 3])
decoder.encoder.stages.2.0.convs.0.all_modules.0.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.all_modules.1.weight shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.all_modules.1.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.conv.weight shape torch.Size([128, 128, 3, 3, 3])
decoder.encoder.stages.2.0.convs.1.conv.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.norm.weight shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.norm.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.all_modules.0.weight shape torch.Size([128, 128, 3, 3, 3])
decoder.encoder.stages.2.0.convs.1.all_modules.0.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.all_modules.1.weight shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.all_modules.1.bias shape torch.Size([128])
decoder.encoder.stages.3.0.convs.0.conv.weight shape torch.Size([256, 128, 3, 3, 3])
decoder.encoder.stages.3.0.convs.0.conv.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.norm.weight shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.norm.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.all_modules.0.weight shape torch.Size([256, 128, 3, 3, 3])
decoder.encoder.stages.3.0.convs.0.all_modules.0.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.all_modules.1.weight shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.all_modules.1.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.conv.weight shape torch.Size([256, 256, 3, 3, 3])
decoder.encoder.stages.3.0.convs.1.conv.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.norm.weight shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.norm.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.all_modules.0.weight shape torch.Size([256, 256, 3, 3, 3])
decoder.encoder.stages.3.0.convs.1.all_modules.0.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.all_modules.1.weight shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.all_modules.1.bias shape torch.Size([256])
decoder.encoder.stages.4.0.convs.0.conv.weight shape torch.Size([320, 256, 3, 3, 3])
decoder.encoder.stages.4.0.convs.0.conv.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.norm.weight shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.norm.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.all_modules.0.weight shape torch.Size([320, 256, 3, 3, 3])
decoder.encoder.stages.4.0.convs.0.all_modules.0.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.all_modules.1.weight shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.all_modules.1.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.4.0.convs.1.conv.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.norm.weight shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.norm.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.4.0.convs.1.all_modules.0.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.all_modules.1.weight shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.all_modules.1.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.conv.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.5.0.convs.0.conv.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.norm.weight shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.norm.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.5.0.convs.0.all_modules.0.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.all_modules.1.weight shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.all_modules.1.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.5.0.convs.1.conv.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.norm.weight shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.norm.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.5.0.convs.1.all_modules.0.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.all_modules.1.weight shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.all_modules.1.bias shape torch.Size([320])
decoder.stages.0.convs.0.conv.weight shape torch.Size([320, 640, 3, 3, 3])
decoder.stages.0.convs.0.conv.bias shape torch.Size([320])
decoder.stages.0.convs.0.norm.weight shape torch.Size([320])
decoder.stages.0.convs.0.norm.bias shape torch.Size([320])
decoder.stages.0.convs.0.all_modules.0.weight shape torch.Size([320, 640, 3, 3, 3])
decoder.stages.0.convs.0.all_modules.0.bias shape torch.Size([320])
decoder.stages.0.convs.0.all_modules.1.weight shape torch.Size([320])
decoder.stages.0.convs.0.all_modules.1.bias shape torch.Size([320])
decoder.stages.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.stages.0.convs.1.conv.bias shape torch.Size([320])
decoder.stages.0.convs.1.norm.weight shape torch.Size([320])
decoder.stages.0.convs.1.norm.bias shape torch.Size([320])
decoder.stages.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.stages.0.convs.1.all_modules.0.bias shape torch.Size([320])
decoder.stages.0.convs.1.all_modules.1.weight shape torch.Size([320])
decoder.stages.0.convs.1.all_modules.1.bias shape torch.Size([320])
decoder.stages.1.convs.0.conv.weight shape torch.Size([256, 512, 3, 3, 3])
decoder.stages.1.convs.0.conv.bias shape torch.Size([256])
decoder.stages.1.convs.0.norm.weight shape torch.Size([256])
decoder.stages.1.convs.0.norm.bias shape torch.Size([256])
decoder.stages.1.convs.0.all_modules.0.weight shape torch.Size([256, 512, 3, 3, 3])
decoder.stages.1.convs.0.all_modules.0.bias shape torch.Size([256])
decoder.stages.1.convs.0.all_modules.1.weight shape torch.Size([256])
decoder.stages.1.convs.0.all_modules.1.bias shape torch.Size([256])
decoder.stages.1.convs.1.conv.weight shape torch.Size([256, 256, 3, 3, 3])
decoder.stages.1.convs.1.conv.bias shape torch.Size([256])
decoder.stages.1.convs.1.norm.weight shape torch.Size([256])
decoder.stages.1.convs.1.norm.bias shape torch.Size([256])
decoder.stages.1.convs.1.all_modules.0.weight shape torch.Size([256, 256, 3, 3, 3])
decoder.stages.1.convs.1.all_modules.0.bias shape torch.Size([256])
decoder.stages.1.convs.1.all_modules.1.weight shape torch.Size([256])
decoder.stages.1.convs.1.all_modules.1.bias shape torch.Size([256])
decoder.stages.2.convs.0.conv.weight shape torch.Size([128, 256, 3, 3, 3])
decoder.stages.2.convs.0.conv.bias shape torch.Size([128])
decoder.stages.2.convs.0.norm.weight shape torch.Size([128])
decoder.stages.2.convs.0.norm.bias shape torch.Size([128])
decoder.stages.2.convs.0.all_modules.0.weight shape torch.Size([128, 256, 3, 3, 3])
decoder.stages.2.convs.0.all_modules.0.bias shape torch.Size([128])
decoder.stages.2.convs.0.all_modules.1.weight shape torch.Size([128])
decoder.stages.2.convs.0.all_modules.1.bias shape torch.Size([128])
decoder.stages.2.convs.1.conv.weight shape torch.Size([128, 128, 3, 3, 3])
decoder.stages.2.convs.1.conv.bias shape torch.Size([128])
decoder.stages.2.convs.1.norm.weight shape torch.Size([128])
decoder.stages.2.convs.1.norm.bias shape torch.Size([128])
decoder.stages.2.convs.1.all_modules.0.weight shape torch.Size([128, 128, 3, 3, 3])
decoder.stages.2.convs.1.all_modules.0.bias shape torch.Size([128])
decoder.stages.2.convs.1.all_modules.1.weight shape torch.Size([128])
decoder.stages.2.convs.1.all_modules.1.bias shape torch.Size([128])
decoder.stages.3.convs.0.conv.weight shape torch.Size([64, 128, 3, 3, 3])
decoder.stages.3.convs.0.conv.bias shape torch.Size([64])
decoder.stages.3.convs.0.norm.weight shape torch.Size([64])
decoder.stages.3.convs.0.norm.bias shape torch.Size([64])
decoder.stages.3.convs.0.all_modules.0.weight shape torch.Size([64, 128, 3, 3, 3])
decoder.stages.3.convs.0.all_modules.0.bias shape torch.Size([64])
decoder.stages.3.convs.0.all_modules.1.weight shape torch.Size([64])
decoder.stages.3.convs.0.all_modules.1.bias shape torch.Size([64])
decoder.stages.3.convs.1.conv.weight shape torch.Size([64, 64, 3, 3, 3])
decoder.stages.3.convs.1.conv.bias shape torch.Size([64])
decoder.stages.3.convs.1.norm.weight shape torch.Size([64])
decoder.stages.3.convs.1.norm.bias shape torch.Size([64])
decoder.stages.3.convs.1.all_modules.0.weight shape torch.Size([64, 64, 3, 3, 3])
decoder.stages.3.convs.1.all_modules.0.bias shape torch.Size([64])
decoder.stages.3.convs.1.all_modules.1.weight shape torch.Size([64])
decoder.stages.3.convs.1.all_modules.1.bias shape torch.Size([64])
decoder.stages.4.convs.0.conv.weight shape torch.Size([32, 64, 3, 3, 3])
decoder.stages.4.convs.0.conv.bias shape torch.Size([32])
decoder.stages.4.convs.0.norm.weight shape torch.Size([32])
decoder.stages.4.convs.0.norm.bias shape torch.Size([32])
decoder.stages.4.convs.0.all_modules.0.weight shape torch.Size([32, 64, 3, 3, 3])
decoder.stages.4.convs.0.all_modules.0.bias shape torch.Size([32])
decoder.stages.4.convs.0.all_modules.1.weight shape torch.Size([32])
decoder.stages.4.convs.0.all_modules.1.bias shape torch.Size([32])
decoder.stages.4.convs.1.conv.weight shape torch.Size([32, 32, 3, 3, 3])
decoder.stages.4.convs.1.conv.bias shape torch.Size([32])
decoder.stages.4.convs.1.norm.weight shape torch.Size([32])
decoder.stages.4.convs.1.norm.bias shape torch.Size([32])
decoder.stages.4.convs.1.all_modules.0.weight shape torch.Size([32, 32, 3, 3, 3])
decoder.stages.4.convs.1.all_modules.0.bias shape torch.Size([32])
decoder.stages.4.convs.1.all_modules.1.weight shape torch.Size([32])
decoder.stages.4.convs.1.all_modules.1.bias shape torch.Size([32])
decoder.transpconvs.0.weight shape torch.Size([320, 320, 1, 2, 2])
decoder.transpconvs.0.bias shape torch.Size([320])
decoder.transpconvs.1.weight shape torch.Size([320, 256, 2, 2, 2])
decoder.transpconvs.1.bias shape torch.Size([256])
decoder.transpconvs.2.weight shape torch.Size([256, 128, 2, 2, 2])
decoder.transpconvs.2.bias shape torch.Size([128])
decoder.transpconvs.3.weight shape torch.Size([128, 64, 2, 2, 2])
decoder.transpconvs.3.bias shape torch.Size([64])
decoder.transpconvs.4.weight shape torch.Size([64, 32, 2, 2, 2])
decoder.transpconvs.4.bias shape torch.Size([32])
################### Done ###################
2024-12-19 12:04:41.454849: do_dummy_2d_data_aug: True
2024-12-19 12:04:41.745250: Using splits from existing split file: /home/gridsan/nchutisilp/datasets/nnUNet_Datasets/nnUNet_preprocessed/Dataset308_Sohee_Calcium_OCT_CrossValidation/splits_final_80.json
2024-12-19 12:04:41.763912: The split file contains 5 splits.
2024-12-19 12:04:41.766123: Desired fold for training: 1
2024-12-19 12:04:41.767407: This split has 6 training and 4 validation cases.
using pin_memory on device 0
using pin_memory on device 0
using pin_memory on device 0

This is the configuration used by this training:
Configuration name: 3d_32x160x128_b10
 {'data_identifier': 'nnUNetPlans_3d_fullres', 'preprocessor_name': 'DefaultPreprocessor', 'batch_size': 10, 'patch_size': [32, 160, 128], 'median_image_size_in_voxels': [375.0, 498.0, 498.0], 'spacing': [1.0, 1.0, 1.0], 'normalization_schemes': ['ZScoreNormalization'], 'use_mask_for_norm': [False], 'resampling_fn_data': 'resample_data_or_seg_to_shape', 'resampling_fn_seg': 'resample_data_or_seg_to_shape', 'resampling_fn_data_kwargs': {'is_seg': False, 'order': 3, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_seg_kwargs': {'is_seg': True, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_probabilities': 'resample_data_or_seg_to_shape', 'resampling_fn_probabilities_kwargs': {'is_seg': False, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'architecture': {'network_class_name': 'dynamic_network_architectures.architectures.unet.PlainConvUNet', 'arch_kwargs': {'n_stages': 6, 'features_per_stage': [32, 64, 128, 256, 320, 320], 'conv_op': 'torch.nn.modules.conv.Conv3d', 'kernel_sizes': [[3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3]], 'strides': [[1, 1, 1], [2, 2, 2], [2, 2, 2], [2, 2, 2], [2, 2, 2], [1, 2, 2]], 'n_conv_per_stage': [2, 2, 2, 2, 2, 2], 'n_conv_per_stage_decoder': [2, 2, 2, 2, 2], 'conv_bias': True, 'norm_op': 'torch.nn.modules.instancenorm.InstanceNorm3d', 'norm_op_kwargs': {'eps': 1e-05, 'affine': True}, 'dropout_op': None, 'dropout_op_kwargs': None, 'nonlin': 'torch.nn.LeakyReLU', 'nonlin_kwargs': {'inplace': True}, 'deep_supervision': True}, '_kw_requires_import': ['conv_op', 'norm_op', 'dropout_op', 'nonlin']}, 'batch_dice': True, 'inherits_from': '3d_fullres'} 

These are the global plan.json settings:
 {'dataset_name': 'Dataset308_Sohee_Calcium_OCT_CrossValidation', 'plans_name': 'nnUNetPlans', 'original_median_spacing_after_transp': [1.0, 1.0, 1.0], 'original_median_shape_after_transp': [375, 498, 498], 'image_reader_writer': 'SimpleITKIO', 'transpose_forward': [0, 1, 2], 'transpose_backward': [0, 1, 2], 'experiment_planner_used': 'ExperimentPlanner', 'label_manager': 'LabelManager', 'foreground_intensity_properties_per_channel': {'0': {'max': 0.4977976381778717, 'mean': 0.13507525622844696, 'median': 0.09599608182907104, 'min': 0.0, 'percentile_00_5': 0.014754901640117168, 'percentile_99_5': 0.4977976381778717, 'std': 0.12025152146816254}}} 

2024-12-19 12:04:58.782479: unpacking dataset...
using pin_memory on device 0

This is the configuration used by this training:
Configuration name: 3d_32x160x128_b10
 {'data_identifier': 'nnUNetPlans_3d_fullres', 'preprocessor_name': 'DefaultPreprocessor', 'batch_size': 10, 'patch_size': [32, 160, 128], 'median_image_size_in_voxels': [375.0, 498.0, 498.0], 'spacing': [1.0, 1.0, 1.0], 'normalization_schemes': ['ZScoreNormalization'], 'use_mask_for_norm': [False], 'resampling_fn_data': 'resample_data_or_seg_to_shape', 'resampling_fn_seg': 'resample_data_or_seg_to_shape', 'resampling_fn_data_kwargs': {'is_seg': False, 'order': 3, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_seg_kwargs': {'is_seg': True, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_probabilities': 'resample_data_or_seg_to_shape', 'resampling_fn_probabilities_kwargs': {'is_seg': False, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'architecture': {'network_class_name': 'dynamic_network_architectures.architectures.unet.PlainConvUNet', 'arch_kwargs': {'n_stages': 6, 'features_per_stage': [32, 64, 128, 256, 320, 320], 'conv_op': 'torch.nn.modules.conv.Conv3d', 'kernel_sizes': [[3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3]], 'strides': [[1, 1, 1], [2, 2, 2], [2, 2, 2], [2, 2, 2], [2, 2, 2], [1, 2, 2]], 'n_conv_per_stage': [2, 2, 2, 2, 2, 2], 'n_conv_per_stage_decoder': [2, 2, 2, 2, 2], 'conv_bias': True, 'norm_op': 'torch.nn.modules.instancenorm.InstanceNorm3d', 'norm_op_kwargs': {'eps': 1e-05, 'affine': True}, 'dropout_op': None, 'dropout_op_kwargs': None, 'nonlin': 'torch.nn.LeakyReLU', 'nonlin_kwargs': {'inplace': True}, 'deep_supervision': True}, '_kw_requires_import': ['conv_op', 'norm_op', 'dropout_op', 'nonlin']}, 'batch_dice': True, 'inherits_from': '3d_fullres'} 

These are the global plan.json settings:
 {'dataset_name': 'Dataset308_Sohee_Calcium_OCT_CrossValidation', 'plans_name': 'nnUNetPlans', 'original_median_spacing_after_transp': [1.0, 1.0, 1.0], 'original_median_shape_after_transp': [375, 498, 498], 'image_reader_writer': 'SimpleITKIO', 'transpose_forward': [0, 1, 2], 'transpose_backward': [0, 1, 2], 'experiment_planner_used': 'ExperimentPlanner', 'label_manager': 'LabelManager', 'foreground_intensity_properties_per_channel': {'0': {'max': 0.4977976381778717, 'mean': 0.13507525622844696, 'median': 0.09599608182907104, 'min': 0.0, 'percentile_00_5': 0.014754901640117168, 'percentile_99_5': 0.4977976381778717, 'std': 0.12025152146816254}}} 

2024-12-19 12:05:00.218943: unpacking dataset...
2024-12-19 12:05:05.322125: unpacking done...
2024-12-19 12:05:06.042120: Unable to plot network architecture: nnUNet_compile is enabled!
2024-12-19 12:05:06.757102: 
2024-12-19 12:05:06.758162: Epoch 0
2024-12-19 12:05:06.759183: Current learning rate: 0.01
2024-12-19 12:08:42.136936: Validation loss improved from 1000.00000 to -0.24468! Patience: 0/50
2024-12-19 12:08:42.137719: train_loss -0.0914
2024-12-19 12:08:42.138866: val_loss -0.2447
2024-12-19 12:08:42.139899: Pseudo dice [0.5644]
2024-12-19 12:08:42.140812: Epoch time: 215.38 s
2024-12-19 12:08:42.141719: Yayy! New best EMA pseudo Dice: 0.5644
2024-12-19 12:08:44.065049: 
2024-12-19 12:08:44.066460: Epoch 1
2024-12-19 12:08:44.067600: Current learning rate: 0.00994
2024-12-19 12:10:39.614361: Validation loss improved from -0.24468 to -0.29997! Patience: 0/50
2024-12-19 12:10:39.615278: train_loss -0.2619
2024-12-19 12:10:39.616204: val_loss -0.3
2024-12-19 12:10:39.617020: Pseudo dice [0.6078]
2024-12-19 12:10:39.617748: Epoch time: 115.55 s
2024-12-19 12:10:39.618387: Yayy! New best EMA pseudo Dice: 0.5688
2024-12-19 12:10:41.405816: 
2024-12-19 12:10:41.406864: Epoch 2
2024-12-19 12:10:41.407616: Current learning rate: 0.00988
2024-12-19 12:13:15.353126: Validation loss improved from -0.29997 to -0.33815! Patience: 0/50
2024-12-19 12:13:15.354149: train_loss -0.3336
2024-12-19 12:13:15.355937: val_loss -0.3381
2024-12-19 12:13:15.357913: Pseudo dice [0.6189]
2024-12-19 12:13:15.359480: Epoch time: 153.95 s
2024-12-19 12:13:15.360864: Yayy! New best EMA pseudo Dice: 0.5738
2024-12-19 12:13:17.270879: 
2024-12-19 12:13:17.272029: Epoch 3
2024-12-19 12:13:17.272855: Current learning rate: 0.00982
2024-12-19 12:16:04.779527: Validation loss improved from -0.33815 to -0.35112! Patience: 0/50
2024-12-19 12:16:04.780323: train_loss -0.3691
2024-12-19 12:16:04.781298: val_loss -0.3511
2024-12-19 12:16:04.782207: Pseudo dice [0.6219]
2024-12-19 12:16:04.783259: Epoch time: 167.51 s
2024-12-19 12:16:04.784225: Yayy! New best EMA pseudo Dice: 0.5786
2024-12-19 12:16:06.655666: 
2024-12-19 12:16:06.657905: Epoch 4
2024-12-19 12:16:06.659260: Current learning rate: 0.00976
2024-12-19 12:19:09.006533: Validation loss improved from -0.35112 to -0.36544! Patience: 0/50
2024-12-19 12:19:09.007475: train_loss -0.3966
2024-12-19 12:19:09.008345: val_loss -0.3654
2024-12-19 12:19:09.008971: Pseudo dice [0.6517]
2024-12-19 12:19:09.009661: Epoch time: 182.35 s
2024-12-19 12:19:09.629024: Yayy! New best EMA pseudo Dice: 0.5859
2024-12-19 12:19:11.529872: 
2024-12-19 12:19:11.531084: Epoch 5
2024-12-19 12:19:11.532032: Current learning rate: 0.0097
2024-12-19 12:22:20.676787: Validation loss did not improve from -0.36544. Patience: 1/50
2024-12-19 12:22:20.678103: train_loss -0.4351
2024-12-19 12:22:20.679082: val_loss -0.3639
2024-12-19 12:22:20.680377: Pseudo dice [0.6337]
2024-12-19 12:22:20.681539: Epoch time: 189.15 s
2024-12-19 12:22:20.682522: Yayy! New best EMA pseudo Dice: 0.5907
2024-12-19 12:22:22.458452: 
2024-12-19 12:22:22.459503: Epoch 6
2024-12-19 12:22:22.460225: Current learning rate: 0.00964
2024-12-19 12:25:43.886338: Validation loss improved from -0.36544 to -0.38449! Patience: 1/50
2024-12-19 12:25:43.887164: train_loss -0.4616
2024-12-19 12:25:43.887983: val_loss -0.3845
2024-12-19 12:25:43.888847: Pseudo dice [0.6482]
2024-12-19 12:25:43.889638: Epoch time: 201.43 s
2024-12-19 12:25:43.890401: Yayy! New best EMA pseudo Dice: 0.5964
2024-12-19 12:25:45.710610: 
2024-12-19 12:25:45.712906: Epoch 7
2024-12-19 12:25:45.714338: Current learning rate: 0.00958
2024-12-19 12:29:13.384803: Validation loss did not improve from -0.38449. Patience: 1/50
2024-12-19 12:29:13.385959: train_loss -0.4472
2024-12-19 12:29:13.387400: val_loss -0.3787
2024-12-19 12:29:13.388636: Pseudo dice [0.6506]
2024-12-19 12:29:13.389684: Epoch time: 207.68 s
2024-12-19 12:29:13.391115: Yayy! New best EMA pseudo Dice: 0.6018
2024-12-19 12:29:15.187926: 
2024-12-19 12:29:15.191214: Epoch 8
2024-12-19 12:29:15.193613: Current learning rate: 0.00952
2024-12-19 12:32:46.637538: Validation loss improved from -0.38449 to -0.39851! Patience: 1/50
2024-12-19 12:32:46.638737: train_loss -0.4882
2024-12-19 12:32:46.640181: val_loss -0.3985
2024-12-19 12:32:46.641438: Pseudo dice [0.6742]
2024-12-19 12:32:46.642847: Epoch time: 211.45 s
2024-12-19 12:32:46.644490: Yayy! New best EMA pseudo Dice: 0.6091
2024-12-19 12:32:48.928215: 
2024-12-19 12:32:48.930622: Epoch 9
2024-12-19 12:32:48.932687: Current learning rate: 0.00946
2024-12-19 12:36:33.675946: Validation loss improved from -0.39851 to -0.43059! Patience: 0/50
2024-12-19 12:36:33.677237: train_loss -0.4965
2024-12-19 12:36:33.678824: val_loss -0.4306
2024-12-19 12:36:33.680189: Pseudo dice [0.6806]
2024-12-19 12:36:33.681765: Epoch time: 224.75 s
2024-12-19 12:36:34.108757: Yayy! New best EMA pseudo Dice: 0.6162
2024-12-19 12:36:36.130502: 
2024-12-19 12:36:36.131924: Epoch 10
2024-12-19 12:36:36.132770: Current learning rate: 0.0094
2024-12-19 12:40:15.199945: Validation loss did not improve from -0.43059. Patience: 1/50
2024-12-19 12:40:15.200841: train_loss -0.5118
2024-12-19 12:40:15.201504: val_loss -0.4256
2024-12-19 12:40:15.202083: Pseudo dice [0.6823]
2024-12-19 12:40:15.202823: Epoch time: 219.07 s
2024-12-19 12:40:15.203420: Yayy! New best EMA pseudo Dice: 0.6228
2024-12-19 12:40:16.948974: 
2024-12-19 12:40:16.950794: Epoch 11
2024-12-19 12:40:16.952210: Current learning rate: 0.00934
2024-12-19 12:44:05.268250: Validation loss improved from -0.43059 to -0.43331! Patience: 1/50
2024-12-19 12:44:05.269106: train_loss -0.5259
2024-12-19 12:44:05.270010: val_loss -0.4333
2024-12-19 12:44:05.270844: Pseudo dice [0.6827]
2024-12-19 12:44:05.271554: Epoch time: 228.32 s
2024-12-19 12:44:05.272256: Yayy! New best EMA pseudo Dice: 0.6288
2024-12-19 12:44:07.032279: 
2024-12-19 12:44:07.033493: Epoch 12
2024-12-19 12:44:07.034229: Current learning rate: 0.00928
2024-12-19 12:47:40.341609: Validation loss improved from -0.43331 to -0.43986! Patience: 0/50
2024-12-19 12:47:40.342499: train_loss -0.5413
2024-12-19 12:47:40.343328: val_loss -0.4399
2024-12-19 12:47:40.344148: Pseudo dice [0.6964]
2024-12-19 12:47:40.344837: Epoch time: 213.31 s
2024-12-19 12:47:40.345639: Yayy! New best EMA pseudo Dice: 0.6356
2024-12-19 12:47:42.495567: 
2024-12-19 12:47:42.497586: Epoch 13
2024-12-19 12:47:42.498878: Current learning rate: 0.00922
2024-12-19 12:51:28.558251: Validation loss did not improve from -0.43986. Patience: 1/50
2024-12-19 12:51:28.558974: train_loss -0.5479
2024-12-19 12:51:28.559790: val_loss -0.4322
2024-12-19 12:51:28.560454: Pseudo dice [0.6872]
2024-12-19 12:51:28.561314: Epoch time: 226.07 s
2024-12-19 12:51:28.562030: Yayy! New best EMA pseudo Dice: 0.6407
2024-12-19 12:51:30.420142: 
2024-12-19 12:51:30.421889: Epoch 14
2024-12-19 12:51:30.423180: Current learning rate: 0.00916
2024-12-19 12:55:16.131903: Validation loss improved from -0.43986 to -0.46532! Patience: 1/50
2024-12-19 12:55:16.132888: train_loss -0.5587
2024-12-19 12:55:16.134009: val_loss -0.4653
2024-12-19 12:55:16.135023: Pseudo dice [0.7119]
2024-12-19 12:55:16.135936: Epoch time: 225.71 s
2024-12-19 12:55:16.744242: Yayy! New best EMA pseudo Dice: 0.6479
2024-12-19 12:55:18.608503: 
2024-12-19 12:55:18.610629: Epoch 15
2024-12-19 12:55:18.613011: Current learning rate: 0.0091
2024-12-19 12:58:53.141297: Validation loss did not improve from -0.46532. Patience: 1/50
2024-12-19 12:58:53.142943: train_loss -0.5614
2024-12-19 12:58:53.144640: val_loss -0.4354
2024-12-19 12:58:53.146331: Pseudo dice [0.6824]
2024-12-19 12:58:53.147859: Epoch time: 214.54 s
2024-12-19 12:58:53.148968: Yayy! New best EMA pseudo Dice: 0.6513
2024-12-19 12:58:55.001889: 
2024-12-19 12:58:55.003431: Epoch 16
2024-12-19 12:58:55.004495: Current learning rate: 0.00903
2024-12-19 13:02:52.084249: Validation loss did not improve from -0.46532. Patience: 2/50
2024-12-19 13:02:52.085153: train_loss -0.5807
2024-12-19 13:02:52.086276: val_loss -0.4425
2024-12-19 13:02:52.087277: Pseudo dice [0.6982]
2024-12-19 13:02:52.088323: Epoch time: 237.08 s
2024-12-19 13:02:52.089283: Yayy! New best EMA pseudo Dice: 0.656
2024-12-19 13:02:54.187801: 
2024-12-19 13:02:54.189322: Epoch 17
2024-12-19 13:02:54.190146: Current learning rate: 0.00897
2024-12-19 13:06:37.600172: Validation loss did not improve from -0.46532. Patience: 3/50
2024-12-19 13:06:37.601222: train_loss -0.5806
2024-12-19 13:06:37.602112: val_loss -0.4327
2024-12-19 13:06:37.602845: Pseudo dice [0.6888]
2024-12-19 13:06:37.603660: Epoch time: 223.41 s
2024-12-19 13:06:37.604352: Yayy! New best EMA pseudo Dice: 0.6593
2024-12-19 13:06:40.448842: 
2024-12-19 13:06:40.450287: Epoch 18
2024-12-19 13:06:40.451280: Current learning rate: 0.00891
2024-12-19 13:10:25.198761: Validation loss did not improve from -0.46532. Patience: 4/50
2024-12-19 13:10:25.200863: train_loss -0.5845
2024-12-19 13:10:25.202233: val_loss -0.4496
2024-12-19 13:10:25.203485: Pseudo dice [0.6854]
2024-12-19 13:10:25.204620: Epoch time: 224.75 s
2024-12-19 13:10:25.205628: Yayy! New best EMA pseudo Dice: 0.6619
2024-12-19 13:10:27.796277: 
2024-12-19 13:10:27.798308: Epoch 19
2024-12-19 13:10:27.800375: Current learning rate: 0.00885
2024-12-19 13:14:23.012066: Validation loss did not improve from -0.46532. Patience: 5/50
2024-12-19 13:14:23.014322: train_loss -0.5932
2024-12-19 13:14:23.016359: val_loss -0.3895
2024-12-19 13:14:23.017295: Pseudo dice [0.6702]
2024-12-19 13:14:23.018452: Epoch time: 235.22 s
2024-12-19 13:14:24.025441: Yayy! New best EMA pseudo Dice: 0.6627
2024-12-19 13:14:26.333898: 
2024-12-19 13:14:26.334964: Epoch 20
2024-12-19 13:14:26.335704: Current learning rate: 0.00879
2024-12-19 13:18:16.827559: Validation loss improved from -0.46532 to -0.48855! Patience: 5/50
2024-12-19 13:18:16.829163: train_loss -0.6023
2024-12-19 13:18:16.830683: val_loss -0.4886
2024-12-19 13:18:16.832519: Pseudo dice [0.7186]
2024-12-19 13:18:16.834120: Epoch time: 230.5 s
2024-12-19 13:18:16.835679: Yayy! New best EMA pseudo Dice: 0.6683
2024-12-19 13:18:18.816876: 
2024-12-19 13:18:18.818502: Epoch 21
2024-12-19 13:18:18.819673: Current learning rate: 0.00873
2024-12-19 13:22:16.367232: Validation loss did not improve from -0.48855. Patience: 1/50
2024-12-19 13:22:16.368146: train_loss -0.6127
2024-12-19 13:22:16.368900: val_loss -0.4551
2024-12-19 13:22:16.369564: Pseudo dice [0.706]
2024-12-19 13:22:16.370327: Epoch time: 237.55 s
2024-12-19 13:22:16.371014: Yayy! New best EMA pseudo Dice: 0.6721
2024-12-19 13:22:18.208007: 
2024-12-19 13:22:18.208967: Epoch 22
2024-12-19 13:22:18.209700: Current learning rate: 0.00867
2024-12-19 13:23:59.380974: Validation loss did not improve from -0.48855. Patience: 2/50
2024-12-19 13:23:59.382092: train_loss -0.6084
2024-12-19 13:23:59.383070: val_loss -0.4817
2024-12-19 13:23:59.384032: Pseudo dice [0.7127]
2024-12-19 13:23:59.384882: Epoch time: 101.18 s
2024-12-19 13:23:59.385711: Yayy! New best EMA pseudo Dice: 0.6761
2024-12-19 13:24:01.110150: 
2024-12-19 13:24:01.111520: Epoch 23
2024-12-19 13:24:01.112319: Current learning rate: 0.00861
2024-12-19 13:25:35.704984: Validation loss did not improve from -0.48855. Patience: 3/50
2024-12-19 13:25:35.706074: train_loss -0.6155
2024-12-19 13:25:35.706849: val_loss -0.4674
2024-12-19 13:25:35.707685: Pseudo dice [0.712]
2024-12-19 13:25:35.708382: Epoch time: 94.6 s
2024-12-19 13:25:35.709017: Yayy! New best EMA pseudo Dice: 0.6797
2024-12-19 13:25:37.450913: 
2024-12-19 13:25:37.452077: Epoch 24
2024-12-19 13:25:37.452872: Current learning rate: 0.00855
2024-12-19 13:27:10.225986: Validation loss did not improve from -0.48855. Patience: 4/50
2024-12-19 13:27:10.226912: train_loss -0.6138
2024-12-19 13:27:10.227615: val_loss -0.3999
2024-12-19 13:27:10.228220: Pseudo dice [0.6652]
2024-12-19 13:27:10.228888: Epoch time: 92.78 s
2024-12-19 13:27:12.635052: 
2024-12-19 13:27:12.636448: Epoch 25
2024-12-19 13:27:12.637300: Current learning rate: 0.00849
2024-12-19 13:28:46.424961: Validation loss did not improve from -0.48855. Patience: 5/50
2024-12-19 13:28:46.426029: train_loss -0.6194
2024-12-19 13:28:46.426949: val_loss -0.4756
2024-12-19 13:28:46.427661: Pseudo dice [0.7041]
2024-12-19 13:28:46.428577: Epoch time: 93.79 s
2024-12-19 13:28:46.429486: Yayy! New best EMA pseudo Dice: 0.6809
2024-12-19 13:28:48.200168: 
2024-12-19 13:28:48.201782: Epoch 26
2024-12-19 13:28:48.202649: Current learning rate: 0.00843
2024-12-19 13:30:39.100994: Validation loss did not improve from -0.48855. Patience: 6/50
2024-12-19 13:30:39.102100: train_loss -0.6239
2024-12-19 13:30:39.102867: val_loss -0.4167
2024-12-19 13:30:39.103516: Pseudo dice [0.6811]
2024-12-19 13:30:39.104187: Epoch time: 110.9 s
2024-12-19 13:30:39.104831: Yayy! New best EMA pseudo Dice: 0.6809
2024-12-19 13:30:40.917135: 
2024-12-19 13:30:40.918401: Epoch 27
2024-12-19 13:30:40.919193: Current learning rate: 0.00836
2024-12-19 13:34:07.719839: Validation loss did not improve from -0.48855. Patience: 7/50
2024-12-19 13:34:07.721067: train_loss -0.6298
2024-12-19 13:34:07.722385: val_loss -0.453
2024-12-19 13:34:07.723378: Pseudo dice [0.7007]
2024-12-19 13:34:07.724633: Epoch time: 206.81 s
2024-12-19 13:34:07.725744: Yayy! New best EMA pseudo Dice: 0.6829
2024-12-19 13:34:09.555747: 
2024-12-19 13:34:09.556780: Epoch 28
2024-12-19 13:34:09.557450: Current learning rate: 0.0083
2024-12-19 13:36:39.955611: Validation loss did not improve from -0.48855. Patience: 8/50
2024-12-19 13:36:39.956652: train_loss -0.6345
2024-12-19 13:36:39.957872: val_loss -0.4827
2024-12-19 13:36:39.958797: Pseudo dice [0.7128]
2024-12-19 13:36:39.960033: Epoch time: 150.4 s
2024-12-19 13:36:39.961136: Yayy! New best EMA pseudo Dice: 0.6858
2024-12-19 13:36:43.520891: 
2024-12-19 13:36:43.522356: Epoch 29
2024-12-19 13:36:43.523491: Current learning rate: 0.00824
2024-12-19 13:40:24.285844: Validation loss did not improve from -0.48855. Patience: 9/50
2024-12-19 13:40:24.286981: train_loss -0.6376
2024-12-19 13:40:24.287941: val_loss -0.4533
2024-12-19 13:40:24.288977: Pseudo dice [0.707]
2024-12-19 13:40:24.290076: Epoch time: 220.77 s
2024-12-19 13:40:24.731644: Yayy! New best EMA pseudo Dice: 0.688
2024-12-19 13:40:26.696736: 
2024-12-19 13:40:26.697944: Epoch 30
2024-12-19 13:40:26.698810: Current learning rate: 0.00818
2024-12-19 13:44:33.354464: Validation loss did not improve from -0.48855. Patience: 10/50
2024-12-19 13:44:33.355385: train_loss -0.6432
2024-12-19 13:44:33.356276: val_loss -0.468
2024-12-19 13:44:33.356975: Pseudo dice [0.7169]
2024-12-19 13:44:33.357661: Epoch time: 246.66 s
2024-12-19 13:44:33.358415: Yayy! New best EMA pseudo Dice: 0.6909
2024-12-19 13:44:35.254619: 
2024-12-19 13:44:35.255537: Epoch 31
2024-12-19 13:44:35.256243: Current learning rate: 0.00812
2024-12-19 13:48:16.708779: Validation loss did not improve from -0.48855. Patience: 11/50
2024-12-19 13:48:16.709681: train_loss -0.6543
2024-12-19 13:48:16.710687: val_loss -0.4782
2024-12-19 13:48:16.711696: Pseudo dice [0.6986]
2024-12-19 13:48:16.712649: Epoch time: 221.46 s
2024-12-19 13:48:16.713623: Yayy! New best EMA pseudo Dice: 0.6916
2024-12-19 13:48:18.695193: 
2024-12-19 13:48:18.696737: Epoch 32
2024-12-19 13:48:18.697699: Current learning rate: 0.00806
2024-12-19 13:51:55.656102: Validation loss did not improve from -0.48855. Patience: 12/50
2024-12-19 13:51:55.657202: train_loss -0.6513
2024-12-19 13:51:55.658592: val_loss -0.4516
2024-12-19 13:51:55.660299: Pseudo dice [0.7047]
2024-12-19 13:51:55.661482: Epoch time: 216.96 s
2024-12-19 13:51:55.662809: Yayy! New best EMA pseudo Dice: 0.6929
2024-12-19 13:51:57.561414: 
2024-12-19 13:51:57.563248: Epoch 33
2024-12-19 13:51:57.564938: Current learning rate: 0.008
2024-12-19 13:55:39.230992: Validation loss did not improve from -0.48855. Patience: 13/50
2024-12-19 13:55:39.231786: train_loss -0.6511
2024-12-19 13:55:39.232504: val_loss -0.4577
2024-12-19 13:55:39.233238: Pseudo dice [0.7003]
2024-12-19 13:55:39.233892: Epoch time: 221.67 s
2024-12-19 13:55:39.234504: Yayy! New best EMA pseudo Dice: 0.6937
2024-12-19 13:55:41.379687: 
2024-12-19 13:55:41.380719: Epoch 34
2024-12-19 13:55:41.381748: Current learning rate: 0.00793
2024-12-19 13:59:16.160317: Validation loss did not improve from -0.48855. Patience: 14/50
2024-12-19 13:59:16.161643: train_loss -0.6643
2024-12-19 13:59:16.162903: val_loss -0.4822
2024-12-19 13:59:16.164008: Pseudo dice [0.7128]
2024-12-19 13:59:16.164937: Epoch time: 214.78 s
2024-12-19 13:59:16.630351: Yayy! New best EMA pseudo Dice: 0.6956
2024-12-19 13:59:18.558914: 
2024-12-19 13:59:18.560494: Epoch 35
2024-12-19 13:59:18.561456: Current learning rate: 0.00787
2024-12-19 14:02:45.342424: Validation loss did not improve from -0.48855. Patience: 15/50
2024-12-19 14:02:45.343477: train_loss -0.6698
2024-12-19 14:02:45.344189: val_loss -0.4218
2024-12-19 14:02:45.344851: Pseudo dice [0.6834]
2024-12-19 14:02:45.345521: Epoch time: 206.79 s
2024-12-19 14:02:46.854028: 
2024-12-19 14:02:46.855398: Epoch 36
2024-12-19 14:02:46.856207: Current learning rate: 0.00781
2024-12-19 14:06:17.688249: Validation loss did not improve from -0.48855. Patience: 16/50
2024-12-19 14:06:17.689153: train_loss -0.6629
2024-12-19 14:06:17.690081: val_loss -0.4743
2024-12-19 14:06:17.690891: Pseudo dice [0.7177]
2024-12-19 14:06:17.691945: Epoch time: 210.84 s
2024-12-19 14:06:17.692896: Yayy! New best EMA pseudo Dice: 0.6967
2024-12-19 14:06:19.895819: 
2024-12-19 14:06:19.896882: Epoch 37
2024-12-19 14:06:19.897651: Current learning rate: 0.00775
2024-12-19 14:10:00.416315: Validation loss improved from -0.48855 to -0.49510! Patience: 16/50
2024-12-19 14:10:00.417378: train_loss -0.6615
2024-12-19 14:10:00.418536: val_loss -0.4951
2024-12-19 14:10:00.419694: Pseudo dice [0.7211]
2024-12-19 14:10:00.420847: Epoch time: 220.52 s
2024-12-19 14:10:00.421818: Yayy! New best EMA pseudo Dice: 0.6991
2024-12-19 14:10:02.394933: 
2024-12-19 14:10:02.396367: Epoch 38
2024-12-19 14:10:02.397598: Current learning rate: 0.00769
2024-12-19 14:13:43.700434: Validation loss did not improve from -0.49510. Patience: 1/50
2024-12-19 14:13:43.701463: train_loss -0.6675
2024-12-19 14:13:43.702213: val_loss -0.4798
2024-12-19 14:13:43.703024: Pseudo dice [0.7234]
2024-12-19 14:13:43.703891: Epoch time: 221.31 s
2024-12-19 14:13:43.704501: Yayy! New best EMA pseudo Dice: 0.7016
2024-12-19 14:13:46.484096: 
2024-12-19 14:13:46.485642: Epoch 39
2024-12-19 14:13:46.486638: Current learning rate: 0.00763
2024-12-19 14:17:36.314986: Validation loss did not improve from -0.49510. Patience: 2/50
2024-12-19 14:17:36.318265: train_loss -0.6723
2024-12-19 14:17:36.320219: val_loss -0.4822
2024-12-19 14:17:36.321027: Pseudo dice [0.7236]
2024-12-19 14:17:36.322144: Epoch time: 229.83 s
2024-12-19 14:17:36.733788: Yayy! New best EMA pseudo Dice: 0.7038
2024-12-19 14:17:38.687382: 
2024-12-19 14:17:38.688754: Epoch 40
2024-12-19 14:17:38.689747: Current learning rate: 0.00756
2024-12-19 14:21:22.466207: Validation loss did not improve from -0.49510. Patience: 3/50
2024-12-19 14:21:22.467441: train_loss -0.6787
2024-12-19 14:21:22.468393: val_loss -0.494
2024-12-19 14:21:22.469121: Pseudo dice [0.7156]
2024-12-19 14:21:22.469836: Epoch time: 223.78 s
2024-12-19 14:21:22.473739: Yayy! New best EMA pseudo Dice: 0.7049
2024-12-19 14:21:24.462827: 
2024-12-19 14:21:24.464124: Epoch 41
2024-12-19 14:21:24.465050: Current learning rate: 0.0075
2024-12-19 14:25:32.778359: Validation loss did not improve from -0.49510. Patience: 4/50
2024-12-19 14:25:32.779764: train_loss -0.6814
2024-12-19 14:25:32.781115: val_loss -0.4863
2024-12-19 14:25:32.782235: Pseudo dice [0.7116]
2024-12-19 14:25:32.783449: Epoch time: 248.32 s
2024-12-19 14:25:32.784863: Yayy! New best EMA pseudo Dice: 0.7056
2024-12-19 14:25:34.585826: 
2024-12-19 14:25:34.586948: Epoch 42
2024-12-19 14:25:34.588107: Current learning rate: 0.00744
2024-12-19 14:29:17.543679: Validation loss did not improve from -0.49510. Patience: 5/50
2024-12-19 14:29:17.544897: train_loss -0.6804
2024-12-19 14:29:17.545905: val_loss -0.4624
2024-12-19 14:29:17.547063: Pseudo dice [0.7036]
2024-12-19 14:29:17.547970: Epoch time: 222.96 s
2024-12-19 14:29:19.034811: 
2024-12-19 14:29:19.036009: Epoch 43
2024-12-19 14:29:19.036899: Current learning rate: 0.00738
2024-12-19 14:33:06.682844: Validation loss did not improve from -0.49510. Patience: 6/50
2024-12-19 14:33:06.683906: train_loss -0.6863
2024-12-19 14:33:06.684962: val_loss -0.4038
2024-12-19 14:33:06.685900: Pseudo dice [0.6641]
2024-12-19 14:33:06.686946: Epoch time: 227.65 s
2024-12-19 14:33:08.109119: 
2024-12-19 14:33:08.110351: Epoch 44
2024-12-19 14:33:08.111180: Current learning rate: 0.00732
2024-12-19 14:36:34.980694: Validation loss did not improve from -0.49510. Patience: 7/50
2024-12-19 14:36:34.981952: train_loss -0.6891
2024-12-19 14:36:34.982874: val_loss -0.4643
2024-12-19 14:36:34.983752: Pseudo dice [0.6996]
2024-12-19 14:36:34.984485: Epoch time: 206.87 s
2024-12-19 14:36:36.813420: 
2024-12-19 14:36:36.815269: Epoch 45
2024-12-19 14:36:36.817014: Current learning rate: 0.00725
2024-12-19 14:39:59.713546: Validation loss did not improve from -0.49510. Patience: 8/50
2024-12-19 14:39:59.716048: train_loss -0.6864
2024-12-19 14:39:59.718338: val_loss -0.4537
2024-12-19 14:39:59.719859: Pseudo dice [0.6905]
2024-12-19 14:39:59.721174: Epoch time: 202.9 s
2024-12-19 14:40:01.131200: 
2024-12-19 14:40:01.132380: Epoch 46
2024-12-19 14:40:01.133111: Current learning rate: 0.00719
2024-12-19 14:44:02.526787: Validation loss did not improve from -0.49510. Patience: 9/50
2024-12-19 14:44:02.527862: train_loss -0.6949
2024-12-19 14:44:02.528730: val_loss -0.4256
2024-12-19 14:44:02.529616: Pseudo dice [0.6791]
2024-12-19 14:44:02.530566: Epoch time: 241.4 s
2024-12-19 14:44:03.881094: 
2024-12-19 14:44:03.882454: Epoch 47
2024-12-19 14:44:03.883400: Current learning rate: 0.00713
2024-12-19 14:47:57.314841: Validation loss did not improve from -0.49510. Patience: 10/50
2024-12-19 14:47:57.315813: train_loss -0.6943
2024-12-19 14:47:57.316691: val_loss -0.4516
2024-12-19 14:47:57.317420: Pseudo dice [0.7011]
2024-12-19 14:47:57.318194: Epoch time: 233.44 s
2024-12-19 14:47:58.742858: 
2024-12-19 14:47:58.744625: Epoch 48
2024-12-19 14:47:58.745894: Current learning rate: 0.00707
2024-12-19 14:51:32.040499: Validation loss did not improve from -0.49510. Patience: 11/50
2024-12-19 14:51:32.041620: train_loss -0.6957
2024-12-19 14:51:32.042550: val_loss -0.4629
2024-12-19 14:51:32.043500: Pseudo dice [0.714]
2024-12-19 14:51:32.044508: Epoch time: 213.3 s
2024-12-19 14:51:33.448099: 
2024-12-19 14:51:33.449822: Epoch 49
2024-12-19 14:51:33.451263: Current learning rate: 0.007
2024-12-19 14:55:24.819879: Validation loss did not improve from -0.49510. Patience: 12/50
2024-12-19 14:55:24.820785: train_loss -0.6868
2024-12-19 14:55:24.821576: val_loss -0.466
2024-12-19 14:55:24.822344: Pseudo dice [0.705]
2024-12-19 14:55:24.823044: Epoch time: 231.37 s
2024-12-19 14:55:27.163616: 
2024-12-19 14:55:27.165116: Epoch 50
2024-12-19 14:55:27.166179: Current learning rate: 0.00694
2024-12-19 14:59:21.488799: Validation loss did not improve from -0.49510. Patience: 13/50
2024-12-19 14:59:21.489675: train_loss -0.6972
2024-12-19 14:59:21.490650: val_loss -0.4381
2024-12-19 14:59:21.491632: Pseudo dice [0.6889]
2024-12-19 14:59:21.492564: Epoch time: 234.33 s
2024-12-19 14:59:22.938094: 
2024-12-19 14:59:22.939710: Epoch 51
2024-12-19 14:59:22.940785: Current learning rate: 0.00688
2024-12-19 15:02:58.855540: Validation loss did not improve from -0.49510. Patience: 14/50
2024-12-19 15:02:58.856372: train_loss -0.7086
2024-12-19 15:02:58.857139: val_loss -0.4508
2024-12-19 15:02:58.857807: Pseudo dice [0.7059]
2024-12-19 15:02:58.858490: Epoch time: 215.92 s
2024-12-19 15:03:00.392998: 
2024-12-19 15:03:00.394905: Epoch 52
2024-12-19 15:03:00.395843: Current learning rate: 0.00682
2024-12-19 15:06:52.859465: Validation loss improved from -0.49510 to -0.50106! Patience: 14/50
2024-12-19 15:06:52.860941: train_loss -0.7049
2024-12-19 15:06:52.862449: val_loss -0.5011
2024-12-19 15:06:52.863798: Pseudo dice [0.739]
2024-12-19 15:06:52.865126: Epoch time: 232.47 s
2024-12-19 15:06:54.352081: 
2024-12-19 15:06:54.353180: Epoch 53
2024-12-19 15:06:54.353989: Current learning rate: 0.00675
2024-12-19 15:10:31.613435: Validation loss did not improve from -0.50106. Patience: 1/50
2024-12-19 15:10:31.615476: train_loss -0.7016
2024-12-19 15:10:31.617898: val_loss -0.4966
2024-12-19 15:10:31.620463: Pseudo dice [0.7241]
2024-12-19 15:10:31.622920: Epoch time: 217.26 s
2024-12-19 15:10:31.625272: Yayy! New best EMA pseudo Dice: 0.7058
2024-12-19 15:10:33.525383: 
2024-12-19 15:10:33.528126: Epoch 54
2024-12-19 15:10:33.529533: Current learning rate: 0.00669
2024-12-19 15:14:27.240206: Validation loss did not improve from -0.50106. Patience: 2/50
2024-12-19 15:14:27.241035: train_loss -0.7023
2024-12-19 15:14:27.242623: val_loss -0.4722
2024-12-19 15:14:27.244260: Pseudo dice [0.7069]
2024-12-19 15:14:27.245289: Epoch time: 233.72 s
2024-12-19 15:14:27.890397: Yayy! New best EMA pseudo Dice: 0.7059
2024-12-19 15:14:29.803004: 
2024-12-19 15:14:29.804471: Epoch 55
2024-12-19 15:14:29.805596: Current learning rate: 0.00663
2024-12-19 15:17:55.978359: Validation loss improved from -0.50106 to -0.50393! Patience: 2/50
2024-12-19 15:17:55.979432: train_loss -0.7065
2024-12-19 15:17:55.980433: val_loss -0.5039
2024-12-19 15:17:55.981340: Pseudo dice [0.7296]
2024-12-19 15:17:55.982246: Epoch time: 206.18 s
2024-12-19 15:17:55.983225: Yayy! New best EMA pseudo Dice: 0.7083
2024-12-19 15:17:57.993679: 
2024-12-19 15:17:57.995146: Epoch 56
2024-12-19 15:17:57.996407: Current learning rate: 0.00657
2024-12-19 15:21:49.130208: Validation loss did not improve from -0.50393. Patience: 1/50
2024-12-19 15:21:49.133190: train_loss -0.7072
2024-12-19 15:21:49.134892: val_loss -0.4642
2024-12-19 15:21:49.136013: Pseudo dice [0.7103]
2024-12-19 15:21:49.137221: Epoch time: 231.14 s
2024-12-19 15:21:49.138484: Yayy! New best EMA pseudo Dice: 0.7085
2024-12-19 15:21:51.073075: 
2024-12-19 15:21:51.073946: Epoch 57
2024-12-19 15:21:51.074689: Current learning rate: 0.0065
2024-12-19 15:26:13.830772: Validation loss did not improve from -0.50393. Patience: 2/50
2024-12-19 15:26:13.831754: train_loss -0.7116
2024-12-19 15:26:13.832904: val_loss -0.4529
2024-12-19 15:26:13.834399: Pseudo dice [0.704]
2024-12-19 15:26:13.835767: Epoch time: 262.76 s
2024-12-19 15:26:15.353544: 
2024-12-19 15:26:15.355213: Epoch 58
2024-12-19 15:26:15.356382: Current learning rate: 0.00644
2024-12-19 15:30:19.895833: Validation loss did not improve from -0.50393. Patience: 3/50
2024-12-19 15:30:19.896812: train_loss -0.717
2024-12-19 15:30:19.897688: val_loss -0.4452
2024-12-19 15:30:19.898428: Pseudo dice [0.685]
2024-12-19 15:30:19.899042: Epoch time: 244.54 s
2024-12-19 15:30:21.334859: 
2024-12-19 15:30:21.336267: Epoch 59
2024-12-19 15:30:21.337240: Current learning rate: 0.00638
2024-12-19 15:34:34.881729: Validation loss did not improve from -0.50393. Patience: 4/50
2024-12-19 15:34:34.884240: train_loss -0.7145
2024-12-19 15:34:34.885466: val_loss -0.4672
2024-12-19 15:34:34.886422: Pseudo dice [0.7053]
2024-12-19 15:34:34.887427: Epoch time: 253.55 s
2024-12-19 15:34:36.768483: 
2024-12-19 15:34:36.770788: Epoch 60
2024-12-19 15:34:36.771738: Current learning rate: 0.00631
2024-12-19 15:38:43.880241: Validation loss did not improve from -0.50393. Patience: 5/50
2024-12-19 15:38:43.881160: train_loss -0.7109
2024-12-19 15:38:43.882225: val_loss -0.3928
2024-12-19 15:38:43.883170: Pseudo dice [0.674]
2024-12-19 15:38:43.883964: Epoch time: 247.11 s
2024-12-19 15:38:47.450310: 
2024-12-19 15:38:47.451702: Epoch 61
2024-12-19 15:38:47.452743: Current learning rate: 0.00625
2024-12-19 15:42:46.799016: Validation loss did not improve from -0.50393. Patience: 6/50
2024-12-19 15:42:46.799735: train_loss -0.7151
2024-12-19 15:42:46.800346: val_loss -0.4666
2024-12-19 15:42:46.800997: Pseudo dice [0.7075]
2024-12-19 15:42:46.801608: Epoch time: 239.35 s
2024-12-19 15:42:48.209682: 
2024-12-19 15:42:48.210634: Epoch 62
2024-12-19 15:42:48.211481: Current learning rate: 0.00619
2024-12-19 15:46:52.617827: Validation loss did not improve from -0.50393. Patience: 7/50
2024-12-19 15:46:52.618703: train_loss -0.7247
2024-12-19 15:46:52.619768: val_loss -0.4377
2024-12-19 15:46:52.620718: Pseudo dice [0.6911]
2024-12-19 15:46:52.621386: Epoch time: 244.41 s
2024-12-19 15:46:54.237175: 
2024-12-19 15:46:54.238892: Epoch 63
2024-12-19 15:46:54.239979: Current learning rate: 0.00612
2024-12-19 15:50:56.489046: Validation loss did not improve from -0.50393. Patience: 8/50
2024-12-19 15:50:56.490113: train_loss -0.7221
2024-12-19 15:50:56.490928: val_loss -0.4872
2024-12-19 15:50:56.491679: Pseudo dice [0.7257]
2024-12-19 15:50:56.492417: Epoch time: 242.25 s
2024-12-19 15:50:57.919012: 
2024-12-19 15:50:57.920299: Epoch 64
2024-12-19 15:50:57.921523: Current learning rate: 0.00606
2024-12-19 15:55:13.077192: Validation loss did not improve from -0.50393. Patience: 9/50
2024-12-19 15:55:13.078264: train_loss -0.7288
2024-12-19 15:55:13.079024: val_loss -0.4225
2024-12-19 15:55:13.079649: Pseudo dice [0.6847]
2024-12-19 15:55:13.080383: Epoch time: 255.16 s
2024-12-19 15:55:15.003899: 
2024-12-19 15:55:15.005050: Epoch 65
2024-12-19 15:55:15.005928: Current learning rate: 0.006
2024-12-19 15:59:19.294161: Validation loss did not improve from -0.50393. Patience: 10/50
2024-12-19 15:59:19.295133: train_loss -0.7297
2024-12-19 15:59:19.295825: val_loss -0.467
2024-12-19 15:59:19.296418: Pseudo dice [0.7035]
2024-12-19 15:59:19.297136: Epoch time: 244.29 s
2024-12-19 15:59:20.733653: 
2024-12-19 15:59:20.734756: Epoch 66
2024-12-19 15:59:20.735520: Current learning rate: 0.00593
2024-12-19 16:03:47.796687: Validation loss did not improve from -0.50393. Patience: 11/50
2024-12-19 16:03:47.797847: train_loss -0.7313
2024-12-19 16:03:47.798648: val_loss -0.4782
2024-12-19 16:03:47.799364: Pseudo dice [0.7177]
2024-12-19 16:03:47.800076: Epoch time: 267.07 s
2024-12-19 16:03:49.274985: 
2024-12-19 16:03:49.277095: Epoch 67
2024-12-19 16:03:49.278633: Current learning rate: 0.00587
2024-12-19 16:08:04.163514: Validation loss did not improve from -0.50393. Patience: 12/50
2024-12-19 16:08:04.164746: train_loss -0.7247
2024-12-19 16:08:04.165684: val_loss -0.4868
2024-12-19 16:08:04.166585: Pseudo dice [0.7269]
2024-12-19 16:08:04.167556: Epoch time: 254.89 s
2024-12-19 16:08:05.725108: 
2024-12-19 16:08:05.727429: Epoch 68
2024-12-19 16:08:05.728701: Current learning rate: 0.00581
2024-12-19 16:12:11.864980: Validation loss did not improve from -0.50393. Patience: 13/50
2024-12-19 16:12:11.865995: train_loss -0.7327
2024-12-19 16:12:11.867269: val_loss -0.4528
2024-12-19 16:12:11.868335: Pseudo dice [0.7039]
2024-12-19 16:12:11.869313: Epoch time: 246.14 s
2024-12-19 16:12:13.401519: 
2024-12-19 16:12:13.402739: Epoch 69
2024-12-19 16:12:13.403585: Current learning rate: 0.00574
2024-12-19 16:16:06.511482: Validation loss did not improve from -0.50393. Patience: 14/50
2024-12-19 16:16:06.512488: train_loss -0.7357
2024-12-19 16:16:06.513551: val_loss -0.4287
2024-12-19 16:16:06.514571: Pseudo dice [0.7002]
2024-12-19 16:16:06.515573: Epoch time: 233.11 s
2024-12-19 16:16:08.526102: 
2024-12-19 16:16:08.528069: Epoch 70
2024-12-19 16:16:08.529735: Current learning rate: 0.00568
2024-12-19 16:20:04.445325: Validation loss did not improve from -0.50393. Patience: 15/50
2024-12-19 16:20:04.446443: train_loss -0.7404
2024-12-19 16:20:04.447587: val_loss -0.4572
2024-12-19 16:20:04.448487: Pseudo dice [0.7097]
2024-12-19 16:20:04.449326: Epoch time: 235.92 s
2024-12-19 16:20:05.926195: 
2024-12-19 16:20:05.928043: Epoch 71
2024-12-19 16:20:05.929469: Current learning rate: 0.00562
2024-12-19 16:24:06.149754: Validation loss did not improve from -0.50393. Patience: 16/50
2024-12-19 16:24:06.150716: train_loss -0.7369
2024-12-19 16:24:06.151513: val_loss -0.4911
2024-12-19 16:24:06.152218: Pseudo dice [0.7285]
2024-12-19 16:24:06.152947: Epoch time: 240.23 s
2024-12-19 16:24:08.432801: 
2024-12-19 16:24:08.434084: Epoch 72
2024-12-19 16:24:08.434877: Current learning rate: 0.00555
2024-12-19 16:28:06.653020: Validation loss did not improve from -0.50393. Patience: 17/50
2024-12-19 16:28:06.655989: train_loss -0.7401
2024-12-19 16:28:06.657822: val_loss -0.4424
2024-12-19 16:28:06.658747: Pseudo dice [0.6888]
2024-12-19 16:28:06.659952: Epoch time: 238.22 s
2024-12-19 16:28:08.241015: 
2024-12-19 16:28:08.242306: Epoch 73
2024-12-19 16:28:08.243131: Current learning rate: 0.00549
2024-12-19 16:32:02.017401: Validation loss did not improve from -0.50393. Patience: 18/50
2024-12-19 16:32:02.018527: train_loss -0.7394
2024-12-19 16:32:02.020070: val_loss -0.4724
2024-12-19 16:32:02.021431: Pseudo dice [0.7264]
2024-12-19 16:32:02.022603: Epoch time: 233.78 s
2024-12-19 16:32:03.536269: 
2024-12-19 16:32:03.537356: Epoch 74
2024-12-19 16:32:03.538081: Current learning rate: 0.00542
2024-12-19 16:35:52.093810: Validation loss did not improve from -0.50393. Patience: 19/50
2024-12-19 16:35:52.094955: train_loss -0.739
2024-12-19 16:35:52.096518: val_loss -0.4403
2024-12-19 16:35:52.097642: Pseudo dice [0.6815]
2024-12-19 16:35:52.098990: Epoch time: 228.56 s
2024-12-19 16:35:54.109057: 
2024-12-19 16:35:54.110204: Epoch 75
2024-12-19 16:35:54.111074: Current learning rate: 0.00536
2024-12-19 16:39:43.682819: Validation loss did not improve from -0.50393. Patience: 20/50
2024-12-19 16:39:43.683639: train_loss -0.7374
2024-12-19 16:39:43.684805: val_loss -0.4818
2024-12-19 16:39:43.685796: Pseudo dice [0.7178]
2024-12-19 16:39:43.686737: Epoch time: 229.58 s
2024-12-19 16:39:45.222937: 
2024-12-19 16:39:45.224079: Epoch 76
2024-12-19 16:39:45.225124: Current learning rate: 0.00529
2024-12-19 16:44:01.504382: Validation loss did not improve from -0.50393. Patience: 21/50
2024-12-19 16:44:01.506293: train_loss -0.7429
2024-12-19 16:44:01.507268: val_loss -0.4793
2024-12-19 16:44:01.507933: Pseudo dice [0.7091]
2024-12-19 16:44:01.508631: Epoch time: 256.28 s
2024-12-19 16:44:03.026236: 
2024-12-19 16:44:03.027820: Epoch 77
2024-12-19 16:44:03.028680: Current learning rate: 0.00523
2024-12-19 16:47:51.103148: Validation loss did not improve from -0.50393. Patience: 22/50
2024-12-19 16:47:51.104146: train_loss -0.7455
2024-12-19 16:47:51.105051: val_loss -0.4546
2024-12-19 16:47:51.106200: Pseudo dice [0.704]
2024-12-19 16:47:51.107084: Epoch time: 228.08 s
2024-12-19 16:47:52.684883: 
2024-12-19 16:47:52.686601: Epoch 78
2024-12-19 16:47:52.687575: Current learning rate: 0.00517
2024-12-19 16:51:56.523935: Validation loss did not improve from -0.50393. Patience: 23/50
2024-12-19 16:51:56.524957: train_loss -0.7471
2024-12-19 16:51:56.525859: val_loss -0.4783
2024-12-19 16:51:56.526659: Pseudo dice [0.715]
2024-12-19 16:51:56.527491: Epoch time: 243.84 s
2024-12-19 16:51:58.074744: 
2024-12-19 16:51:58.076324: Epoch 79
2024-12-19 16:51:58.077429: Current learning rate: 0.0051
2024-12-19 16:55:37.362803: Validation loss did not improve from -0.50393. Patience: 24/50
2024-12-19 16:55:37.363607: train_loss -0.7421
2024-12-19 16:55:37.364623: val_loss -0.485
2024-12-19 16:55:37.365766: Pseudo dice [0.724]
2024-12-19 16:55:37.367119: Epoch time: 219.29 s
2024-12-19 16:55:37.817243: Yayy! New best EMA pseudo Dice: 0.7092
2024-12-19 16:55:39.862137: 
2024-12-19 16:55:39.863963: Epoch 80
2024-12-19 16:55:39.865145: Current learning rate: 0.00504
2024-12-19 16:59:39.025220: Validation loss did not improve from -0.50393. Patience: 25/50
2024-12-19 16:59:39.026140: train_loss -0.7444
2024-12-19 16:59:39.026938: val_loss -0.4336
2024-12-19 16:59:39.027820: Pseudo dice [0.6866]
2024-12-19 16:59:39.028730: Epoch time: 239.17 s
2024-12-19 16:59:40.579105: 
2024-12-19 16:59:40.580711: Epoch 81
2024-12-19 16:59:40.582076: Current learning rate: 0.00497
2024-12-19 17:03:35.633425: Validation loss did not improve from -0.50393. Patience: 26/50
2024-12-19 17:03:35.634428: train_loss -0.7486
2024-12-19 17:03:35.635686: val_loss -0.4368
2024-12-19 17:03:35.636684: Pseudo dice [0.7079]
2024-12-19 17:03:35.637560: Epoch time: 235.06 s
2024-12-19 17:03:37.798068: 
2024-12-19 17:03:37.799576: Epoch 82
2024-12-19 17:03:37.800951: Current learning rate: 0.00491
2024-12-19 17:07:22.447424: Validation loss improved from -0.50393 to -0.51596! Patience: 26/50
2024-12-19 17:07:22.448283: train_loss -0.7388
2024-12-19 17:07:22.449144: val_loss -0.516
2024-12-19 17:07:22.449944: Pseudo dice [0.7305]
2024-12-19 17:07:22.450812: Epoch time: 224.65 s
2024-12-19 17:07:22.451740: Yayy! New best EMA pseudo Dice: 0.7094
2024-12-19 17:07:24.333440: 
2024-12-19 17:07:24.334530: Epoch 83
2024-12-19 17:07:24.335631: Current learning rate: 0.00484
2024-12-19 17:11:26.142919: Validation loss did not improve from -0.51596. Patience: 1/50
2024-12-19 17:11:26.143819: train_loss -0.7486
2024-12-19 17:11:26.144825: val_loss -0.4774
2024-12-19 17:11:26.145738: Pseudo dice [0.7191]
2024-12-19 17:11:26.146749: Epoch time: 241.81 s
2024-12-19 17:11:26.147677: Yayy! New best EMA pseudo Dice: 0.7103
2024-12-19 17:11:27.996327: 
2024-12-19 17:11:27.997819: Epoch 84
2024-12-19 17:11:27.998784: Current learning rate: 0.00478
2024-12-19 17:15:18.430259: Validation loss did not improve from -0.51596. Patience: 2/50
2024-12-19 17:15:18.430980: train_loss -0.744
2024-12-19 17:15:18.431843: val_loss -0.4648
2024-12-19 17:15:18.432608: Pseudo dice [0.7019]
2024-12-19 17:15:18.433365: Epoch time: 230.44 s
2024-12-19 17:15:20.276552: 
2024-12-19 17:15:20.278011: Epoch 85
2024-12-19 17:15:20.279120: Current learning rate: 0.00471
2024-12-19 17:19:21.580646: Validation loss did not improve from -0.51596. Patience: 3/50
2024-12-19 17:19:21.582305: train_loss -0.751
2024-12-19 17:19:21.583324: val_loss -0.4877
2024-12-19 17:19:21.584076: Pseudo dice [0.7271]
2024-12-19 17:19:21.584841: Epoch time: 241.31 s
2024-12-19 17:19:21.585709: Yayy! New best EMA pseudo Dice: 0.7113
2024-12-19 17:19:23.476731: 
2024-12-19 17:19:23.478006: Epoch 86
2024-12-19 17:19:23.478796: Current learning rate: 0.00465
2024-12-19 17:23:30.175575: Validation loss did not improve from -0.51596. Patience: 4/50
2024-12-19 17:23:30.176300: train_loss -0.7544
2024-12-19 17:23:30.177661: val_loss -0.4415
2024-12-19 17:23:30.178687: Pseudo dice [0.7021]
2024-12-19 17:23:30.179611: Epoch time: 246.7 s
2024-12-19 17:23:31.668413: 
2024-12-19 17:23:31.669656: Epoch 87
2024-12-19 17:23:31.670802: Current learning rate: 0.00458
2024-12-19 17:27:44.011153: Validation loss did not improve from -0.51596. Patience: 5/50
2024-12-19 17:27:44.012202: train_loss -0.7558
2024-12-19 17:27:44.013252: val_loss -0.4676
2024-12-19 17:27:44.014138: Pseudo dice [0.7143]
2024-12-19 17:27:44.014948: Epoch time: 252.35 s
2024-12-19 17:27:45.574184: 
2024-12-19 17:27:45.575462: Epoch 88
2024-12-19 17:27:45.576426: Current learning rate: 0.00452
2024-12-19 17:31:57.942486: Validation loss did not improve from -0.51596. Patience: 6/50
2024-12-19 17:31:57.945296: train_loss -0.7533
2024-12-19 17:31:57.946882: val_loss -0.4456
2024-12-19 17:31:57.947722: Pseudo dice [0.7024]
2024-12-19 17:31:57.948929: Epoch time: 252.37 s
2024-12-19 17:31:59.408422: 
2024-12-19 17:31:59.410249: Epoch 89
2024-12-19 17:31:59.411809: Current learning rate: 0.00445
2024-12-19 17:35:52.042859: Validation loss did not improve from -0.51596. Patience: 7/50
2024-12-19 17:35:52.043922: train_loss -0.7556
2024-12-19 17:35:52.044921: val_loss -0.4686
2024-12-19 17:35:52.045792: Pseudo dice [0.7082]
2024-12-19 17:35:52.046832: Epoch time: 232.64 s
2024-12-19 17:35:53.939588: 
2024-12-19 17:35:53.941000: Epoch 90
2024-12-19 17:35:53.942122: Current learning rate: 0.00438
2024-12-19 17:39:38.773739: Validation loss did not improve from -0.51596. Patience: 8/50
2024-12-19 17:39:38.775050: train_loss -0.7639
2024-12-19 17:39:38.776267: val_loss -0.4745
2024-12-19 17:39:38.777337: Pseudo dice [0.7193]
2024-12-19 17:39:38.778371: Epoch time: 224.84 s
2024-12-19 17:39:40.237733: 
2024-12-19 17:39:40.239218: Epoch 91
2024-12-19 17:39:40.240069: Current learning rate: 0.00432
2024-12-19 17:43:47.500393: Validation loss did not improve from -0.51596. Patience: 9/50
2024-12-19 17:43:47.501501: train_loss -0.7599
2024-12-19 17:43:47.502501: val_loss -0.455
2024-12-19 17:43:47.504143: Pseudo dice [0.7033]
2024-12-19 17:43:47.505265: Epoch time: 247.27 s
2024-12-19 17:43:49.098179: 
2024-12-19 17:43:49.099637: Epoch 92
2024-12-19 17:43:49.100655: Current learning rate: 0.00425
2024-12-19 17:47:40.191243: Validation loss did not improve from -0.51596. Patience: 10/50
2024-12-19 17:47:40.192456: train_loss -0.7611
2024-12-19 17:47:40.193389: val_loss -0.4453
2024-12-19 17:47:40.194467: Pseudo dice [0.703]
2024-12-19 17:47:40.195406: Epoch time: 231.1 s
2024-12-19 17:47:41.657108: 
2024-12-19 17:47:41.658671: Epoch 93
2024-12-19 17:47:41.659977: Current learning rate: 0.00419
2024-12-19 17:52:08.685973: Validation loss did not improve from -0.51596. Patience: 11/50
2024-12-19 17:52:08.686751: train_loss -0.7679
2024-12-19 17:52:08.687719: val_loss -0.4559
2024-12-19 17:52:08.688590: Pseudo dice [0.7142]
2024-12-19 17:52:08.689528: Epoch time: 267.03 s
2024-12-19 17:52:10.603288: 
2024-12-19 17:52:10.604592: Epoch 94
2024-12-19 17:52:10.605410: Current learning rate: 0.00412
2024-12-19 17:56:42.126140: Validation loss did not improve from -0.51596. Patience: 12/50
2024-12-19 17:56:42.126991: train_loss -0.7598
2024-12-19 17:56:42.127775: val_loss -0.4103
2024-12-19 17:56:42.128434: Pseudo dice [0.6802]
2024-12-19 17:56:42.129133: Epoch time: 271.53 s
2024-12-19 17:56:44.014392: 
2024-12-19 17:56:44.015862: Epoch 95
2024-12-19 17:56:44.016788: Current learning rate: 0.00405
2024-12-19 18:01:12.220412: Validation loss did not improve from -0.51596. Patience: 13/50
2024-12-19 18:01:12.221709: train_loss -0.7652
2024-12-19 18:01:12.223211: val_loss -0.4221
2024-12-19 18:01:12.225376: Pseudo dice [0.6882]
2024-12-19 18:01:12.227228: Epoch time: 268.21 s
2024-12-19 18:01:13.723205: 
2024-12-19 18:01:13.725036: Epoch 96
2024-12-19 18:01:13.726619: Current learning rate: 0.00399
2024-12-19 18:05:30.497780: Validation loss did not improve from -0.51596. Patience: 14/50
2024-12-19 18:05:30.498775: train_loss -0.7659
2024-12-19 18:05:30.499573: val_loss -0.4375
2024-12-19 18:05:30.500326: Pseudo dice [0.7066]
2024-12-19 18:05:30.501094: Epoch time: 256.78 s
2024-12-19 18:05:31.932088: 
2024-12-19 18:05:31.933537: Epoch 97
2024-12-19 18:05:31.934411: Current learning rate: 0.00392
2024-12-19 18:09:36.686916: Validation loss did not improve from -0.51596. Patience: 15/50
2024-12-19 18:09:36.688214: train_loss -0.7678
2024-12-19 18:09:36.689297: val_loss -0.4814
2024-12-19 18:09:36.690136: Pseudo dice [0.728]
2024-12-19 18:09:36.690887: Epoch time: 244.76 s
2024-12-19 18:09:38.185317: 
2024-12-19 18:09:38.187059: Epoch 98
2024-12-19 18:09:38.188708: Current learning rate: 0.00385
2024-12-19 18:13:37.528952: Validation loss did not improve from -0.51596. Patience: 16/50
2024-12-19 18:13:37.530039: train_loss -0.769
2024-12-19 18:13:37.531047: val_loss -0.469
2024-12-19 18:13:37.531940: Pseudo dice [0.7124]
2024-12-19 18:13:37.532846: Epoch time: 239.35 s
2024-12-19 18:13:39.075406: 
2024-12-19 18:13:39.076686: Epoch 99
2024-12-19 18:13:39.077612: Current learning rate: 0.00379
2024-12-19 18:17:32.232136: Validation loss did not improve from -0.51596. Patience: 17/50
2024-12-19 18:17:32.233275: train_loss -0.7701
2024-12-19 18:17:32.234137: val_loss -0.4737
2024-12-19 18:17:32.234839: Pseudo dice [0.7127]
2024-12-19 18:17:32.235635: Epoch time: 233.16 s
2024-12-19 18:17:34.164772: 
2024-12-19 18:17:34.166211: Epoch 100
2024-12-19 18:17:34.167049: Current learning rate: 0.00372
2024-12-19 18:21:31.207921: Validation loss did not improve from -0.51596. Patience: 18/50
2024-12-19 18:21:31.208986: train_loss -0.7684
2024-12-19 18:21:31.209994: val_loss -0.442
2024-12-19 18:21:31.210784: Pseudo dice [0.6992]
2024-12-19 18:21:31.211720: Epoch time: 237.05 s
2024-12-19 18:21:32.676769: 
2024-12-19 18:21:32.678703: Epoch 101
2024-12-19 18:21:32.680009: Current learning rate: 0.00365
2024-12-19 18:25:31.658099: Validation loss did not improve from -0.51596. Patience: 19/50
2024-12-19 18:25:31.659105: train_loss -0.7694
2024-12-19 18:25:31.660544: val_loss -0.4236
2024-12-19 18:25:31.662045: Pseudo dice [0.6863]
2024-12-19 18:25:31.663760: Epoch time: 238.98 s
2024-12-19 18:25:33.230484: 
2024-12-19 18:25:33.231717: Epoch 102
2024-12-19 18:25:33.232700: Current learning rate: 0.00359
2024-12-19 18:29:41.490276: Validation loss did not improve from -0.51596. Patience: 20/50
2024-12-19 18:29:41.491188: train_loss -0.774
2024-12-19 18:29:41.492339: val_loss -0.412
2024-12-19 18:29:41.493332: Pseudo dice [0.685]
2024-12-19 18:29:41.494294: Epoch time: 248.26 s
2024-12-19 18:29:42.954439: 
2024-12-19 18:29:42.956215: Epoch 103
2024-12-19 18:29:42.957524: Current learning rate: 0.00352
2024-12-19 18:34:04.430906: Validation loss did not improve from -0.51596. Patience: 21/50
2024-12-19 18:34:04.433175: train_loss -0.7719
2024-12-19 18:34:04.434236: val_loss -0.4681
2024-12-19 18:34:04.435253: Pseudo dice [0.7185]
2024-12-19 18:34:04.436470: Epoch time: 261.48 s
2024-12-19 18:34:05.974430: 
2024-12-19 18:34:05.975553: Epoch 104
2024-12-19 18:34:05.976349: Current learning rate: 0.00345
2024-12-19 18:38:28.058887: Validation loss did not improve from -0.51596. Patience: 22/50
2024-12-19 18:38:28.060054: train_loss -0.772
2024-12-19 18:38:28.060963: val_loss -0.4181
2024-12-19 18:38:28.061806: Pseudo dice [0.6889]
2024-12-19 18:38:28.062584: Epoch time: 262.09 s
2024-12-19 18:38:30.498543: 
2024-12-19 18:38:30.499830: Epoch 105
2024-12-19 18:38:30.500607: Current learning rate: 0.00338
2024-12-19 18:42:11.153492: Validation loss did not improve from -0.51596. Patience: 23/50
2024-12-19 18:42:11.154380: train_loss -0.773
2024-12-19 18:42:11.155267: val_loss -0.4563
2024-12-19 18:42:11.156250: Pseudo dice [0.7095]
2024-12-19 18:42:11.157051: Epoch time: 220.66 s
2024-12-19 18:42:12.625228: 
2024-12-19 18:42:12.627156: Epoch 106
2024-12-19 18:42:12.628599: Current learning rate: 0.00332
2024-12-19 18:45:40.695596: Validation loss did not improve from -0.51596. Patience: 24/50
2024-12-19 18:45:40.696914: train_loss -0.7754
2024-12-19 18:45:40.698587: val_loss -0.4774
2024-12-19 18:45:40.699980: Pseudo dice [0.7126]
2024-12-19 18:45:40.701586: Epoch time: 208.07 s
2024-12-19 18:45:42.148609: 
2024-12-19 18:45:42.150100: Epoch 107
2024-12-19 18:45:42.151361: Current learning rate: 0.00325
2024-12-19 18:49:49.326845: Validation loss did not improve from -0.51596. Patience: 25/50
2024-12-19 18:49:49.328467: train_loss -0.7741
2024-12-19 18:49:49.331280: val_loss -0.4517
2024-12-19 18:49:49.333833: Pseudo dice [0.7043]
2024-12-19 18:49:49.335222: Epoch time: 247.18 s
2024-12-19 18:49:50.780891: 
2024-12-19 18:49:50.782146: Epoch 108
2024-12-19 18:49:50.783051: Current learning rate: 0.00318
2024-12-19 18:53:47.526968: Validation loss did not improve from -0.51596. Patience: 26/50
2024-12-19 18:53:47.528039: train_loss -0.7774
2024-12-19 18:53:47.529057: val_loss -0.4591
2024-12-19 18:53:47.530044: Pseudo dice [0.7193]
2024-12-19 18:53:47.531996: Epoch time: 236.75 s
2024-12-19 18:53:48.999745: 
2024-12-19 18:53:49.001137: Epoch 109
2024-12-19 18:53:49.002829: Current learning rate: 0.00311
2024-12-19 18:57:50.904333: Validation loss did not improve from -0.51596. Patience: 27/50
2024-12-19 18:57:50.905030: train_loss -0.7758
2024-12-19 18:57:50.905919: val_loss -0.4433
2024-12-19 18:57:50.906754: Pseudo dice [0.7118]
2024-12-19 18:57:50.907578: Epoch time: 241.91 s
2024-12-19 18:57:52.805940: 
2024-12-19 18:57:52.807188: Epoch 110
2024-12-19 18:57:52.808126: Current learning rate: 0.00304
2024-12-19 19:01:56.668938: Validation loss did not improve from -0.51596. Patience: 28/50
2024-12-19 19:01:56.669861: train_loss -0.7788
2024-12-19 19:01:56.671036: val_loss -0.4346
2024-12-19 19:01:56.672143: Pseudo dice [0.7023]
2024-12-19 19:01:56.673253: Epoch time: 243.87 s
2024-12-19 19:01:58.110495: 
2024-12-19 19:01:58.111624: Epoch 111
2024-12-19 19:01:58.112385: Current learning rate: 0.00297
2024-12-19 19:06:07.843247: Validation loss did not improve from -0.51596. Patience: 29/50
2024-12-19 19:06:07.844113: train_loss -0.7812
2024-12-19 19:06:07.844895: val_loss -0.466
2024-12-19 19:06:07.845651: Pseudo dice [0.7205]
2024-12-19 19:06:07.846243: Epoch time: 249.73 s
2024-12-19 19:06:09.376201: 
2024-12-19 19:06:09.377474: Epoch 112
2024-12-19 19:06:09.378430: Current learning rate: 0.00291
2024-12-19 19:10:29.496077: Validation loss did not improve from -0.51596. Patience: 30/50
2024-12-19 19:10:29.497203: train_loss -0.7811
2024-12-19 19:10:29.498820: val_loss -0.4636
2024-12-19 19:10:29.500115: Pseudo dice [0.7246]
2024-12-19 19:10:29.501523: Epoch time: 260.12 s
2024-12-19 19:10:30.974794: 
2024-12-19 19:10:30.976628: Epoch 113
2024-12-19 19:10:30.977975: Current learning rate: 0.00284
2024-12-19 19:14:45.833508: Validation loss did not improve from -0.51596. Patience: 31/50
2024-12-19 19:14:45.834944: train_loss -0.7789
2024-12-19 19:14:45.835840: val_loss -0.4772
2024-12-19 19:14:45.836477: Pseudo dice [0.7186]
2024-12-19 19:14:45.837079: Epoch time: 254.86 s
2024-12-19 19:14:47.330694: 
2024-12-19 19:14:47.332267: Epoch 114
2024-12-19 19:14:47.332956: Current learning rate: 0.00277
2024-12-19 19:19:04.791039: Validation loss did not improve from -0.51596. Patience: 32/50
2024-12-19 19:19:04.792003: train_loss -0.782
2024-12-19 19:19:04.793162: val_loss -0.4681
2024-12-19 19:19:04.794133: Pseudo dice [0.7169]
2024-12-19 19:19:04.794820: Epoch time: 257.46 s
2024-12-19 19:19:06.687774: 
2024-12-19 19:19:06.688888: Epoch 115
2024-12-19 19:19:06.689808: Current learning rate: 0.0027
2024-12-19 19:23:30.565130: Validation loss did not improve from -0.51596. Patience: 33/50
2024-12-19 19:23:30.566086: train_loss -0.7809
2024-12-19 19:23:30.566852: val_loss -0.4387
2024-12-19 19:23:30.567698: Pseudo dice [0.7035]
2024-12-19 19:23:30.568475: Epoch time: 263.88 s
2024-12-19 19:23:32.678922: 
2024-12-19 19:23:32.680209: Epoch 116
2024-12-19 19:23:32.681259: Current learning rate: 0.00263
2024-12-19 19:28:05.064818: Validation loss did not improve from -0.51596. Patience: 34/50
2024-12-19 19:28:05.065768: train_loss -0.7832
2024-12-19 19:28:05.066560: val_loss -0.4818
2024-12-19 19:28:05.067363: Pseudo dice [0.7203]
2024-12-19 19:28:05.068096: Epoch time: 272.39 s
2024-12-19 19:28:06.613572: 
2024-12-19 19:28:06.614948: Epoch 117
2024-12-19 19:28:06.615673: Current learning rate: 0.00256
2024-12-19 19:32:15.442137: Validation loss did not improve from -0.51596. Patience: 35/50
2024-12-19 19:32:15.442898: train_loss -0.7814
2024-12-19 19:32:15.443675: val_loss -0.454
2024-12-19 19:32:15.444360: Pseudo dice [0.7121]
2024-12-19 19:32:15.445129: Epoch time: 248.83 s
2024-12-19 19:32:15.445859: Yayy! New best EMA pseudo Dice: 0.7113
2024-12-19 19:32:17.421268: 
2024-12-19 19:32:17.422664: Epoch 118
2024-12-19 19:32:17.423553: Current learning rate: 0.00249
2024-12-19 19:36:27.899534: Validation loss did not improve from -0.51596. Patience: 36/50
2024-12-19 19:36:27.900459: train_loss -0.7823
2024-12-19 19:36:27.901300: val_loss -0.4544
2024-12-19 19:36:27.902151: Pseudo dice [0.7078]
2024-12-19 19:36:27.902991: Epoch time: 250.48 s
2024-12-19 19:36:29.430771: 
2024-12-19 19:36:29.431866: Epoch 119
2024-12-19 19:36:29.433063: Current learning rate: 0.00242
2024-12-19 19:40:20.546779: Validation loss did not improve from -0.51596. Patience: 37/50
2024-12-19 19:40:20.566769: train_loss -0.7831
2024-12-19 19:40:20.568940: val_loss -0.4753
2024-12-19 19:40:20.569893: Pseudo dice [0.7212]
2024-12-19 19:40:20.570982: Epoch time: 231.12 s
2024-12-19 19:40:20.942367: Yayy! New best EMA pseudo Dice: 0.712
2024-12-19 19:40:22.808189: 
2024-12-19 19:40:22.809734: Epoch 120
2024-12-19 19:40:22.810889: Current learning rate: 0.00235
2024-12-19 19:46:20.350838: Validation loss did not improve from -0.51596. Patience: 38/50
2024-12-19 19:46:20.353311: train_loss -0.7823
2024-12-19 19:46:20.354337: val_loss -0.4575
2024-12-19 19:46:20.355018: Pseudo dice [0.7109]
2024-12-19 19:46:20.355786: Epoch time: 357.55 s
2024-12-19 19:46:21.945530: 
2024-12-19 19:46:21.947309: Epoch 121
2024-12-19 19:46:21.948399: Current learning rate: 0.00228
2024-12-19 19:52:03.196965: Validation loss did not improve from -0.51596. Patience: 39/50
2024-12-19 19:52:03.198558: train_loss -0.7871
2024-12-19 19:52:03.199700: val_loss -0.4616
2024-12-19 19:52:03.200646: Pseudo dice [0.7193]
2024-12-19 19:52:03.201644: Epoch time: 341.25 s
2024-12-19 19:52:03.202682: Yayy! New best EMA pseudo Dice: 0.7126
2024-12-19 19:52:05.243308: 
2024-12-19 19:52:05.244457: Epoch 122
2024-12-19 19:52:05.245342: Current learning rate: 0.00221
2024-12-19 19:58:00.386085: Validation loss did not improve from -0.51596. Patience: 40/50
2024-12-19 19:58:00.387166: train_loss -0.7842
2024-12-19 19:58:00.388536: val_loss -0.4374
2024-12-19 19:58:00.389540: Pseudo dice [0.6982]
2024-12-19 19:58:00.390603: Epoch time: 355.15 s
2024-12-19 19:58:01.909427: 
2024-12-19 19:58:01.910873: Epoch 123
2024-12-19 19:58:01.912054: Current learning rate: 0.00214
2024-12-19 20:04:10.443670: Validation loss did not improve from -0.51596. Patience: 41/50
2024-12-19 20:04:10.444709: train_loss -0.7885
2024-12-19 20:04:10.445648: val_loss -0.4337
2024-12-19 20:04:10.446470: Pseudo dice [0.7011]
2024-12-19 20:04:10.447311: Epoch time: 368.54 s
2024-12-19 20:04:11.989076: 
2024-12-19 20:04:11.990262: Epoch 124
2024-12-19 20:04:11.990983: Current learning rate: 0.00207
2024-12-19 20:10:33.468310: Validation loss did not improve from -0.51596. Patience: 42/50
2024-12-19 20:10:33.469293: train_loss -0.7858
2024-12-19 20:10:33.470221: val_loss -0.4689
2024-12-19 20:10:33.471041: Pseudo dice [0.7219]
2024-12-19 20:10:33.471839: Epoch time: 381.48 s
2024-12-19 20:10:35.515376: 
2024-12-19 20:10:35.517382: Epoch 125
2024-12-19 20:10:35.519284: Current learning rate: 0.00199
2024-12-19 20:16:41.569134: Validation loss did not improve from -0.51596. Patience: 43/50
2024-12-19 20:16:41.570071: train_loss -0.7911
2024-12-19 20:16:41.570801: val_loss -0.4188
2024-12-19 20:16:41.571457: Pseudo dice [0.6923]
2024-12-19 20:16:41.572097: Epoch time: 366.06 s
2024-12-19 20:16:43.457244: 
2024-12-19 20:16:43.459040: Epoch 126
2024-12-19 20:16:43.460950: Current learning rate: 0.00192
2024-12-19 20:23:01.720722: Validation loss did not improve from -0.51596. Patience: 44/50
2024-12-19 20:23:01.722016: train_loss -0.786
2024-12-19 20:23:01.723431: val_loss -0.4326
2024-12-19 20:23:01.724631: Pseudo dice [0.6912]
2024-12-19 20:23:01.725978: Epoch time: 378.27 s
2024-12-19 20:23:03.209502: 
2024-12-19 20:23:03.210781: Epoch 127
2024-12-19 20:23:03.211486: Current learning rate: 0.00185
2024-12-19 20:29:42.441309: Validation loss did not improve from -0.51596. Patience: 45/50
2024-12-19 20:29:42.442411: train_loss -0.789
2024-12-19 20:29:42.443486: val_loss -0.44
2024-12-19 20:29:42.444997: Pseudo dice [0.7071]
2024-12-19 20:29:42.447712: Epoch time: 399.23 s
2024-12-19 20:29:44.007374: 
2024-12-19 20:29:44.008318: Epoch 128
2024-12-19 20:29:44.009160: Current learning rate: 0.00178
2024-12-19 20:36:36.083287: Validation loss did not improve from -0.51596. Patience: 46/50
2024-12-19 20:36:36.084505: train_loss -0.7913
2024-12-19 20:36:36.086318: val_loss -0.485
2024-12-19 20:36:36.087839: Pseudo dice [0.73]
2024-12-19 20:36:36.088916: Epoch time: 412.08 s
2024-12-19 20:36:37.740624: 
2024-12-19 20:36:37.742975: Epoch 129
2024-12-19 20:36:37.744550: Current learning rate: 0.0017
2024-12-19 20:42:57.828418: Validation loss did not improve from -0.51596. Patience: 47/50
2024-12-19 20:42:57.829468: train_loss -0.7919
2024-12-19 20:42:57.831854: val_loss -0.4873
2024-12-19 20:42:57.832679: Pseudo dice [0.7235]
2024-12-19 20:42:57.833604: Epoch time: 380.09 s
2024-12-19 20:43:00.276469: 
2024-12-19 20:43:00.278125: Epoch 130
2024-12-19 20:43:00.279830: Current learning rate: 0.00163
2024-12-19 20:49:14.157543: Validation loss did not improve from -0.51596. Patience: 48/50
2024-12-19 20:49:14.158866: train_loss -0.7903
2024-12-19 20:49:14.160493: val_loss -0.4543
2024-12-19 20:49:14.161952: Pseudo dice [0.719]
2024-12-19 20:49:14.163076: Epoch time: 373.88 s
2024-12-19 20:49:15.741496: 
2024-12-19 20:49:15.742846: Epoch 131
2024-12-19 20:49:15.743627: Current learning rate: 0.00156
2024-12-19 20:55:29.993713: Validation loss did not improve from -0.51596. Patience: 49/50
2024-12-19 20:55:29.995229: train_loss -0.791
2024-12-19 20:55:29.996569: val_loss -0.4641
2024-12-19 20:55:29.998207: Pseudo dice [0.7186]
2024-12-19 20:55:30.000189: Epoch time: 374.26 s
2024-12-19 20:55:30.001692: Yayy! New best EMA pseudo Dice: 0.7126
2024-12-19 20:55:31.952735: 
2024-12-19 20:55:31.954269: Epoch 132
2024-12-19 20:55:31.955174: Current learning rate: 0.00148
2024-12-19 21:01:32.906461: Validation loss did not improve from -0.51596. Patience: 50/50
2024-12-19 21:01:32.907536: train_loss -0.7937
2024-12-19 21:01:32.910022: val_loss -0.4229
2024-12-19 21:01:32.911809: Pseudo dice [0.7016]
2024-12-19 21:01:32.913732: Epoch time: 360.96 s
2024-12-19 21:01:34.416698: 
2024-12-19 21:01:34.419513: Epoch 133
2024-12-19 21:01:34.421219: Current learning rate: 0.00141
2024-12-19 21:07:34.144649: Validation loss did not improve from -0.51596. Patience: 51/50
2024-12-19 21:07:34.145751: train_loss -0.7905
2024-12-19 21:07:34.146817: val_loss -0.4551
2024-12-19 21:07:34.147676: Pseudo dice [0.7173]
2024-12-19 21:07:34.148478: Epoch time: 359.73 s
2024-12-19 21:07:35.648132: 
2024-12-19 21:07:35.649108: Epoch 134
2024-12-19 21:07:35.649776: Current learning rate: 0.00133
2024-12-19 21:13:53.336179: Validation loss did not improve from -0.51596. Patience: 52/50
2024-12-19 21:13:53.338092: train_loss -0.7978
2024-12-19 21:13:53.340249: val_loss -0.4558
2024-12-19 21:13:53.341590: Pseudo dice [0.7087]
2024-12-19 21:13:53.342929: Epoch time: 377.69 s
2024-12-19 21:13:55.368314: 
2024-12-19 21:13:55.369373: Epoch 135
2024-12-19 21:13:55.370114: Current learning rate: 0.00126
2024-12-19 21:20:23.228711: Validation loss did not improve from -0.51596. Patience: 53/50
2024-12-19 21:20:23.229917: train_loss -0.7914
2024-12-19 21:20:23.232059: val_loss -0.4562
2024-12-19 21:20:23.233578: Pseudo dice [0.7039]
2024-12-19 21:20:23.234919: Epoch time: 387.86 s
2024-12-19 21:20:24.741865: 
2024-12-19 21:20:24.743382: Epoch 136
2024-12-19 21:20:24.744394: Current learning rate: 0.00118
2024-12-19 21:27:02.028880: Validation loss did not improve from -0.51596. Patience: 54/50
2024-12-19 21:27:02.029843: train_loss -0.7949
2024-12-19 21:27:02.030637: val_loss -0.4311
2024-12-19 21:27:02.031460: Pseudo dice [0.7068]
2024-12-19 21:27:02.032241: Epoch time: 397.29 s
2024-12-19 21:27:04.055650: 
2024-12-19 21:27:04.057111: Epoch 137
2024-12-19 21:27:04.058243: Current learning rate: 0.00111
2024-12-19 21:33:56.216871: Validation loss did not improve from -0.51596. Patience: 55/50
2024-12-19 21:33:56.217599: train_loss -0.7951
2024-12-19 21:33:56.218367: val_loss -0.4541
2024-12-19 21:33:56.219114: Pseudo dice [0.7157]
2024-12-19 21:33:56.219796: Epoch time: 412.16 s
2024-12-19 21:33:57.828079: 
2024-12-19 21:33:57.829952: Epoch 138
2024-12-19 21:33:57.831832: Current learning rate: 0.00103
2024-12-19 21:41:42.411685: Validation loss did not improve from -0.51596. Patience: 56/50
2024-12-19 21:41:42.412476: train_loss -0.7954
2024-12-19 21:41:42.413253: val_loss -0.4348
2024-12-19 21:41:42.413873: Pseudo dice [0.7057]
2024-12-19 21:41:42.414558: Epoch time: 464.59 s
2024-12-19 21:41:43.973429: 
2024-12-19 21:41:43.974591: Epoch 139
2024-12-19 21:41:43.976339: Current learning rate: 0.00095
2024-12-19 21:49:25.307298: Validation loss did not improve from -0.51596. Patience: 57/50
2024-12-19 21:49:25.334222: train_loss -0.7942
2024-12-19 21:49:25.335552: val_loss -0.4611
2024-12-19 21:49:25.336832: Pseudo dice [0.7082]
2024-12-19 21:49:25.338989: Epoch time: 461.36 s
2024-12-19 21:49:27.560352: 
2024-12-19 21:49:27.562571: Epoch 140
2024-12-19 21:49:27.564060: Current learning rate: 0.00087
2024-12-19 21:57:10.662017: Validation loss did not improve from -0.51596. Patience: 58/50
2024-12-19 21:57:10.663043: train_loss -0.7942
2024-12-19 21:57:10.663877: val_loss -0.4269
2024-12-19 21:57:10.664611: Pseudo dice [0.6908]
2024-12-19 21:57:10.665390: Epoch time: 463.1 s
2024-12-19 21:57:12.199695: 
2024-12-19 21:57:12.200832: Epoch 141
2024-12-19 21:57:12.201705: Current learning rate: 0.00079
2024-12-19 22:04:57.724827: Validation loss did not improve from -0.51596. Patience: 59/50
2024-12-19 22:04:57.725652: train_loss -0.7957
2024-12-19 22:04:57.726631: val_loss -0.4784
2024-12-19 22:04:57.727417: Pseudo dice [0.7299]
2024-12-19 22:04:57.728127: Epoch time: 465.53 s
2024-12-19 22:04:59.266077: 
2024-12-19 22:04:59.268457: Epoch 142
2024-12-19 22:04:59.271084: Current learning rate: 0.00071
2024-12-19 22:12:42.036881: Validation loss did not improve from -0.51596. Patience: 60/50
2024-12-19 22:12:42.037561: train_loss -0.7967
2024-12-19 22:12:42.038298: val_loss -0.4197
2024-12-19 22:12:42.038942: Pseudo dice [0.6922]
2024-12-19 22:12:42.039673: Epoch time: 462.77 s
2024-12-19 22:12:43.566328: 
2024-12-19 22:12:43.567348: Epoch 143
2024-12-19 22:12:43.568136: Current learning rate: 0.00063
2024-12-19 22:20:20.014134: Validation loss did not improve from -0.51596. Patience: 61/50
2024-12-19 22:20:20.015528: train_loss -0.7966
2024-12-19 22:20:20.017085: val_loss -0.4754
2024-12-19 22:20:20.019356: Pseudo dice [0.7225]
2024-12-19 22:20:20.021008: Epoch time: 456.45 s
2024-12-19 22:20:21.637509: 
2024-12-19 22:20:21.638894: Epoch 144
2024-12-19 22:20:21.640086: Current learning rate: 0.00055
2024-12-19 22:28:20.332819: Validation loss did not improve from -0.51596. Patience: 62/50
2024-12-19 22:28:20.334389: train_loss -0.7994
2024-12-19 22:28:20.335733: val_loss -0.4417
2024-12-19 22:28:20.337095: Pseudo dice [0.709]
2024-12-19 22:28:20.338361: Epoch time: 478.7 s
2024-12-19 22:28:22.409659: 
2024-12-19 22:28:22.411174: Epoch 145
2024-12-19 22:28:22.412258: Current learning rate: 0.00047
2024-12-19 22:36:25.075937: Validation loss did not improve from -0.51596. Patience: 63/50
2024-12-19 22:36:25.076906: train_loss -0.7966
2024-12-19 22:36:25.077717: val_loss -0.4716
2024-12-19 22:36:25.078436: Pseudo dice [0.7227]
2024-12-19 22:36:25.079221: Epoch time: 482.67 s
2024-12-19 22:36:26.658395: 
2024-12-19 22:36:26.659752: Epoch 146
2024-12-19 22:36:26.660727: Current learning rate: 0.00038
2024-12-19 22:44:10.017146: Validation loss did not improve from -0.51596. Patience: 64/50
2024-12-19 22:44:10.017879: train_loss -0.7971
2024-12-19 22:44:10.018706: val_loss -0.4687
2024-12-19 22:44:10.019562: Pseudo dice [0.7176]
2024-12-19 22:44:10.020225: Epoch time: 463.36 s
2024-12-19 22:44:11.923520: 
2024-12-19 22:44:11.925506: Epoch 147
2024-12-19 22:44:11.927381: Current learning rate: 0.0003
2024-12-19 22:51:51.743550: Validation loss did not improve from -0.51596. Patience: 65/50
2024-12-19 22:51:51.745104: train_loss -0.7998
2024-12-19 22:51:51.746879: val_loss -0.4609
2024-12-19 22:51:51.748109: Pseudo dice [0.7171]
2024-12-19 22:51:51.750113: Epoch time: 459.82 s
2024-12-19 22:51:53.310848: 
2024-12-19 22:51:53.312019: Epoch 148
2024-12-19 22:51:53.312775: Current learning rate: 0.00021
2024-12-19 22:59:44.177586: Validation loss did not improve from -0.51596. Patience: 66/50
2024-12-19 22:59:44.178585: train_loss -0.7993
2024-12-19 22:59:44.179428: val_loss -0.479
2024-12-19 22:59:44.180131: Pseudo dice [0.7203]
2024-12-19 22:59:44.180950: Epoch time: 470.87 s
2024-12-19 22:59:44.181602: Yayy! New best EMA pseudo Dice: 0.7132
2024-12-19 22:59:46.150156: 
2024-12-19 22:59:46.152614: Epoch 149
2024-12-19 22:59:46.154329: Current learning rate: 0.00011
2024-12-19 23:07:57.675660: Validation loss did not improve from -0.51596. Patience: 67/50
2024-12-19 23:07:57.676577: train_loss -0.797
2024-12-19 23:07:57.678448: val_loss -0.484
2024-12-19 23:07:57.680476: Pseudo dice [0.7219]
2024-12-19 23:07:57.682545: Epoch time: 491.53 s
2024-12-19 23:07:57.683823: Yayy! New best EMA pseudo Dice: 0.714
2024-12-19 23:08:00.092941: Training done.
2024-12-19 23:08:00.486735: Using splits from existing split file: /home/gridsan/nchutisilp/datasets/nnUNet_Datasets/nnUNet_preprocessed/Dataset308_Sohee_Calcium_OCT_CrossValidation/splits_final_80.json
2024-12-19 23:08:00.508896: The split file contains 5 splits.
2024-12-19 23:08:00.509824: Desired fold for training: 0
2024-12-19 23:08:00.510618: This split has 6 training and 4 validation cases.
2024-12-19 23:08:00.511727: predicting 101-045
2024-12-19 23:08:00.579211: 101-045, shape torch.Size([1, 375, 498, 498]), rank 0
2024-12-19 23:10:17.336530: predicting 701-013
2024-12-19 23:10:17.354764: 701-013, shape torch.Size([1, 375, 498, 498]), rank 0
2024-12-19 23:12:35.141807: predicting 704-003
2024-12-19 23:12:35.155941: 704-003, shape torch.Size([1, 375, 498, 498]), rank 0
2024-12-19 23:14:30.861460: predicting 706-005
2024-12-19 23:14:30.874774: 706-005, shape torch.Size([1, 375, 498, 498]), rank 0
2024-12-19 23:16:52.183807: Validation complete
2024-12-19 23:16:52.184373: Mean Validation Dice:  0.7177430148278142
2024-12-19 12:05:05.321045: unpacking done...
2024-12-19 12:05:06.041951: Unable to plot network architecture: nnUNet_compile is enabled!
2024-12-19 12:05:06.723501: 
2024-12-19 12:05:06.724714: Epoch 0
2024-12-19 12:05:06.725698: Current learning rate: 0.01
2024-12-19 12:08:38.504728: Validation loss improved from 1000.00000 to -0.15208! Patience: 0/50
2024-12-19 12:08:38.506946: train_loss -0.0722
2024-12-19 12:08:38.508364: val_loss -0.1521
2024-12-19 12:08:38.509908: Pseudo dice [0.5128]
2024-12-19 12:08:38.510937: Epoch time: 211.78 s
2024-12-19 12:08:38.512139: Yayy! New best EMA pseudo Dice: 0.5128
2024-12-19 12:08:40.869370: 
2024-12-19 12:08:40.871185: Epoch 1
2024-12-19 12:08:40.873565: Current learning rate: 0.00994
2024-12-19 12:10:47.594326: Validation loss improved from -0.15208 to -0.23529! Patience: 0/50
2024-12-19 12:10:47.595378: train_loss -0.2203
2024-12-19 12:10:47.596593: val_loss -0.2353
2024-12-19 12:10:47.597602: Pseudo dice [0.5643]
2024-12-19 12:10:47.598637: Epoch time: 126.73 s
2024-12-19 12:10:47.599611: Yayy! New best EMA pseudo Dice: 0.5179
2024-12-19 12:10:49.605778: 
2024-12-19 12:10:49.607233: Epoch 2
2024-12-19 12:10:49.608104: Current learning rate: 0.00988
2024-12-19 12:13:35.714782: Validation loss did not improve from -0.23529. Patience: 1/50
2024-12-19 12:13:35.715682: train_loss -0.2762
2024-12-19 12:13:35.716579: val_loss -0.2287
2024-12-19 12:13:35.717497: Pseudo dice [0.5847]
2024-12-19 12:13:35.718562: Epoch time: 166.11 s
2024-12-19 12:13:35.719701: Yayy! New best EMA pseudo Dice: 0.5246
2024-12-19 12:13:37.855098: 
2024-12-19 12:13:37.856504: Epoch 3
2024-12-19 12:13:37.857417: Current learning rate: 0.00982
2024-12-19 12:16:36.384472: Validation loss did not improve from -0.23529. Patience: 2/50
2024-12-19 12:16:36.385497: train_loss -0.3086
2024-12-19 12:16:36.386673: val_loss -0.2315
2024-12-19 12:16:36.387791: Pseudo dice [0.6028]
2024-12-19 12:16:36.388923: Epoch time: 178.53 s
2024-12-19 12:16:36.390007: Yayy! New best EMA pseudo Dice: 0.5324
2024-12-19 12:16:38.385791: 
2024-12-19 12:16:38.387165: Epoch 4
2024-12-19 12:16:38.388094: Current learning rate: 0.00976
2024-12-19 12:20:01.101781: Validation loss improved from -0.23529 to -0.26057! Patience: 2/50
2024-12-19 12:20:01.102780: train_loss -0.3383
2024-12-19 12:20:01.103733: val_loss -0.2606
2024-12-19 12:20:01.104665: Pseudo dice [0.5857]
2024-12-19 12:20:01.105566: Epoch time: 202.72 s
2024-12-19 12:20:01.502338: Yayy! New best EMA pseudo Dice: 0.5378
2024-12-19 12:20:03.578808: 
2024-12-19 12:20:03.580122: Epoch 5
2024-12-19 12:20:03.581342: Current learning rate: 0.0097
2024-12-19 12:23:15.550601: Validation loss improved from -0.26057 to -0.28272! Patience: 0/50
2024-12-19 12:23:15.551404: train_loss -0.3365
2024-12-19 12:23:15.552228: val_loss -0.2827
2024-12-19 12:23:15.553242: Pseudo dice [0.6031]
2024-12-19 12:23:15.554429: Epoch time: 191.97 s
2024-12-19 12:23:15.555395: Yayy! New best EMA pseudo Dice: 0.5443
2024-12-19 12:23:17.388517: 
2024-12-19 12:23:17.389725: Epoch 6
2024-12-19 12:23:17.390764: Current learning rate: 0.00964
2024-12-19 12:26:52.798392: Validation loss improved from -0.28272 to -0.30893! Patience: 0/50
2024-12-19 12:26:52.799448: train_loss -0.3864
2024-12-19 12:26:52.800379: val_loss -0.3089
2024-12-19 12:26:52.801297: Pseudo dice [0.6395]
2024-12-19 12:26:52.802170: Epoch time: 215.41 s
2024-12-19 12:26:52.802866: Yayy! New best EMA pseudo Dice: 0.5538
2024-12-19 12:26:54.709734: 
2024-12-19 12:26:54.711374: Epoch 7
2024-12-19 12:26:54.712785: Current learning rate: 0.00958
2024-12-19 12:30:36.045394: Validation loss improved from -0.30893 to -0.32524! Patience: 0/50
2024-12-19 12:30:36.047991: train_loss -0.4086
2024-12-19 12:30:36.049854: val_loss -0.3252
2024-12-19 12:30:36.051754: Pseudo dice [0.6429]
2024-12-19 12:30:36.053540: Epoch time: 221.34 s
2024-12-19 12:30:36.055208: Yayy! New best EMA pseudo Dice: 0.5627
2024-12-19 12:30:37.955779: 
2024-12-19 12:30:37.956860: Epoch 8
2024-12-19 12:30:37.957794: Current learning rate: 0.00952
2024-12-19 12:34:08.629272: Validation loss improved from -0.32524 to -0.33082! Patience: 0/50
2024-12-19 12:34:08.630076: train_loss -0.4148
2024-12-19 12:34:08.631179: val_loss -0.3308
2024-12-19 12:34:08.632532: Pseudo dice [0.6368]
2024-12-19 12:34:08.633834: Epoch time: 210.68 s
2024-12-19 12:34:08.634933: Yayy! New best EMA pseudo Dice: 0.5701
2024-12-19 12:34:10.614146: 
2024-12-19 12:34:10.615439: Epoch 9
2024-12-19 12:34:10.616269: Current learning rate: 0.00946
2024-12-19 12:37:53.390413: Validation loss did not improve from -0.33082. Patience: 1/50
2024-12-19 12:37:53.391441: train_loss -0.4254
2024-12-19 12:37:53.392240: val_loss -0.289
2024-12-19 12:37:53.393067: Pseudo dice [0.6021]
2024-12-19 12:37:53.393835: Epoch time: 222.78 s
2024-12-19 12:37:53.875927: Yayy! New best EMA pseudo Dice: 0.5733
2024-12-19 12:37:55.737140: 
2024-12-19 12:37:55.738475: Epoch 10
2024-12-19 12:37:55.739728: Current learning rate: 0.0094
2024-12-19 12:41:33.163222: Validation loss improved from -0.33082 to -0.38190! Patience: 1/50
2024-12-19 12:41:33.164176: train_loss -0.4472
2024-12-19 12:41:33.165154: val_loss -0.3819
2024-12-19 12:41:33.165964: Pseudo dice [0.6709]
2024-12-19 12:41:33.166739: Epoch time: 217.43 s
2024-12-19 12:41:33.167433: Yayy! New best EMA pseudo Dice: 0.5831
2024-12-19 12:41:35.147652: 
2024-12-19 12:41:35.149497: Epoch 11
2024-12-19 12:41:35.151106: Current learning rate: 0.00934
2024-12-19 12:45:29.441132: Validation loss improved from -0.38190 to -0.42410! Patience: 0/50
2024-12-19 12:45:29.442021: train_loss -0.4679
2024-12-19 12:45:29.442858: val_loss -0.4241
2024-12-19 12:45:29.443535: Pseudo dice [0.6859]
2024-12-19 12:45:29.444257: Epoch time: 234.3 s
2024-12-19 12:45:29.444979: Yayy! New best EMA pseudo Dice: 0.5934
2024-12-19 12:45:31.334092: 
2024-12-19 12:45:31.335655: Epoch 12
2024-12-19 12:45:31.336688: Current learning rate: 0.00928
2024-12-19 12:49:19.165121: Validation loss did not improve from -0.42410. Patience: 1/50
2024-12-19 12:49:19.166115: train_loss -0.4797
2024-12-19 12:49:19.166965: val_loss -0.3978
2024-12-19 12:49:19.167626: Pseudo dice [0.6665]
2024-12-19 12:49:19.168452: Epoch time: 227.83 s
2024-12-19 12:49:19.169140: Yayy! New best EMA pseudo Dice: 0.6007
2024-12-19 12:49:21.114486: 
2024-12-19 12:49:21.115993: Epoch 13
2024-12-19 12:49:21.117632: Current learning rate: 0.00922
2024-12-19 12:53:17.195326: Validation loss improved from -0.42410 to -0.43558! Patience: 1/50
2024-12-19 12:53:17.196640: train_loss -0.4924
2024-12-19 12:53:17.197781: val_loss -0.4356
2024-12-19 12:53:17.198817: Pseudo dice [0.7]
2024-12-19 12:53:17.199697: Epoch time: 236.08 s
2024-12-19 12:53:17.200863: Yayy! New best EMA pseudo Dice: 0.6106
2024-12-19 12:53:19.137075: 
2024-12-19 12:53:19.138306: Epoch 14
2024-12-19 12:53:19.139217: Current learning rate: 0.00916
2024-12-19 12:57:24.244687: Validation loss did not improve from -0.43558. Patience: 1/50
2024-12-19 12:57:24.245559: train_loss -0.5019
2024-12-19 12:57:24.246614: val_loss -0.3992
2024-12-19 12:57:24.247500: Pseudo dice [0.6792]
2024-12-19 12:57:24.248258: Epoch time: 245.11 s
2024-12-19 12:57:24.702956: Yayy! New best EMA pseudo Dice: 0.6175
2024-12-19 12:57:26.562801: 
2024-12-19 12:57:26.564304: Epoch 15
2024-12-19 12:57:26.565246: Current learning rate: 0.0091
2024-12-19 13:01:33.086405: Validation loss improved from -0.43558 to -0.43887! Patience: 1/50
2024-12-19 13:01:33.087435: train_loss -0.513
2024-12-19 13:01:33.088444: val_loss -0.4389
2024-12-19 13:01:33.089142: Pseudo dice [0.697]
2024-12-19 13:01:33.089956: Epoch time: 246.53 s
2024-12-19 13:01:33.090676: Yayy! New best EMA pseudo Dice: 0.6254
2024-12-19 13:01:34.974487: 
2024-12-19 13:01:34.975412: Epoch 16
2024-12-19 13:01:34.976224: Current learning rate: 0.00903
2024-12-19 13:05:36.618659: Validation loss did not improve from -0.43887. Patience: 1/50
2024-12-19 13:05:36.619417: train_loss -0.5147
2024-12-19 13:05:36.620265: val_loss -0.4103
2024-12-19 13:05:36.621003: Pseudo dice [0.6728]
2024-12-19 13:05:36.621807: Epoch time: 241.65 s
2024-12-19 13:05:36.622683: Yayy! New best EMA pseudo Dice: 0.6302
2024-12-19 13:05:38.548555: 
2024-12-19 13:05:38.550438: Epoch 17
2024-12-19 13:05:38.551684: Current learning rate: 0.00897
2024-12-19 13:09:45.618512: Validation loss did not improve from -0.43887. Patience: 2/50
2024-12-19 13:09:45.619812: train_loss -0.5122
2024-12-19 13:09:45.620882: val_loss -0.4325
2024-12-19 13:09:45.621958: Pseudo dice [0.6965]
2024-12-19 13:09:45.623032: Epoch time: 247.07 s
2024-12-19 13:09:45.624497: Yayy! New best EMA pseudo Dice: 0.6368
2024-12-19 13:09:47.658620: 
2024-12-19 13:09:47.659765: Epoch 18
2024-12-19 13:09:47.660575: Current learning rate: 0.00891
2024-12-19 13:13:22.411001: Validation loss did not improve from -0.43887. Patience: 3/50
2024-12-19 13:13:22.417123: train_loss -0.5243
2024-12-19 13:13:22.418612: val_loss -0.4151
2024-12-19 13:13:22.419571: Pseudo dice [0.682]
2024-12-19 13:13:22.420827: Epoch time: 214.76 s
2024-12-19 13:13:22.421834: Yayy! New best EMA pseudo Dice: 0.6413
2024-12-19 13:13:24.841371: 
2024-12-19 13:13:24.842334: Epoch 19
2024-12-19 13:13:24.843182: Current learning rate: 0.00885
2024-12-19 13:17:27.031085: Validation loss did not improve from -0.43887. Patience: 4/50
2024-12-19 13:17:27.031779: train_loss -0.5162
2024-12-19 13:17:27.032571: val_loss -0.4055
2024-12-19 13:17:27.033212: Pseudo dice [0.6739]
2024-12-19 13:17:27.033903: Epoch time: 242.19 s
2024-12-19 13:17:27.599809: Yayy! New best EMA pseudo Dice: 0.6446
2024-12-19 13:17:29.780966: 
2024-12-19 13:17:29.782195: Epoch 20
2024-12-19 13:17:29.782914: Current learning rate: 0.00879
2024-12-19 13:21:29.487216: Validation loss improved from -0.43887 to -0.44241! Patience: 4/50
2024-12-19 13:21:29.488127: train_loss -0.5334
2024-12-19 13:21:29.488875: val_loss -0.4424
2024-12-19 13:21:29.489537: Pseudo dice [0.709]
2024-12-19 13:21:29.490365: Epoch time: 239.71 s
2024-12-19 13:21:29.491164: Yayy! New best EMA pseudo Dice: 0.651
2024-12-19 13:21:31.411937: 
2024-12-19 13:21:31.412985: Epoch 21
2024-12-19 13:21:31.413733: Current learning rate: 0.00873
2024-12-19 13:24:03.692111: Validation loss did not improve from -0.44241. Patience: 1/50
2024-12-19 13:24:03.692944: train_loss -0.5359
2024-12-19 13:24:03.693914: val_loss -0.4377
2024-12-19 13:24:03.694726: Pseudo dice [0.6873]
2024-12-19 13:24:03.695456: Epoch time: 152.28 s
2024-12-19 13:24:03.696204: Yayy! New best EMA pseudo Dice: 0.6546
2024-12-19 13:24:05.568074: 
2024-12-19 13:24:05.569163: Epoch 22
2024-12-19 13:24:05.569991: Current learning rate: 0.00867
2024-12-19 13:25:39.484467: Validation loss did not improve from -0.44241. Patience: 2/50
2024-12-19 13:25:39.485727: train_loss -0.5431
2024-12-19 13:25:39.486740: val_loss -0.4315
2024-12-19 13:25:39.487465: Pseudo dice [0.6829]
2024-12-19 13:25:39.488232: Epoch time: 93.92 s
2024-12-19 13:25:39.488903: Yayy! New best EMA pseudo Dice: 0.6575
2024-12-19 13:25:41.234496: 
2024-12-19 13:25:41.236084: Epoch 23
2024-12-19 13:25:41.237148: Current learning rate: 0.00861
2024-12-19 13:27:15.865653: Validation loss improved from -0.44241 to -0.44423! Patience: 2/50
2024-12-19 13:27:15.866743: train_loss -0.55
2024-12-19 13:27:15.867723: val_loss -0.4442
2024-12-19 13:27:15.868563: Pseudo dice [0.6931]
2024-12-19 13:27:15.869574: Epoch time: 94.63 s
2024-12-19 13:27:15.870511: Yayy! New best EMA pseudo Dice: 0.661
2024-12-19 13:27:17.635381: 
2024-12-19 13:27:17.636519: Epoch 24
2024-12-19 13:27:17.637357: Current learning rate: 0.00855
2024-12-19 13:28:52.020736: Validation loss improved from -0.44423 to -0.45043! Patience: 0/50
2024-12-19 13:28:52.021982: train_loss -0.5639
2024-12-19 13:28:52.022846: val_loss -0.4504
2024-12-19 13:28:52.023709: Pseudo dice [0.705]
2024-12-19 13:28:52.024436: Epoch time: 94.39 s
2024-12-19 13:28:52.441474: Yayy! New best EMA pseudo Dice: 0.6654
2024-12-19 13:28:54.302865: 
2024-12-19 13:28:54.304215: Epoch 25
2024-12-19 13:28:54.304956: Current learning rate: 0.00849
2024-12-19 13:30:45.755791: Validation loss did not improve from -0.45043. Patience: 1/50
2024-12-19 13:30:45.758513: train_loss -0.561
2024-12-19 13:30:45.759828: val_loss -0.4307
2024-12-19 13:30:45.760480: Pseudo dice [0.7051]
2024-12-19 13:30:45.761283: Epoch time: 111.46 s
2024-12-19 13:30:45.761915: Yayy! New best EMA pseudo Dice: 0.6694
2024-12-19 13:30:47.552739: 
2024-12-19 13:30:47.554414: Epoch 26
2024-12-19 13:30:47.555339: Current learning rate: 0.00843
2024-12-19 13:34:17.361798: Validation loss improved from -0.45043 to -0.45162! Patience: 1/50
2024-12-19 13:34:17.362938: train_loss -0.5662
2024-12-19 13:34:17.363786: val_loss -0.4516
2024-12-19 13:34:17.364642: Pseudo dice [0.7111]
2024-12-19 13:34:17.365446: Epoch time: 209.81 s
2024-12-19 13:34:17.366128: Yayy! New best EMA pseudo Dice: 0.6736
2024-12-19 13:34:19.310449: 
2024-12-19 13:34:19.311680: Epoch 27
2024-12-19 13:34:19.312550: Current learning rate: 0.00836
2024-12-19 13:38:04.359548: Validation loss improved from -0.45162 to -0.45188! Patience: 0/50
2024-12-19 13:38:04.360572: train_loss -0.5728
2024-12-19 13:38:04.361500: val_loss -0.4519
2024-12-19 13:38:04.362354: Pseudo dice [0.7073]
2024-12-19 13:38:04.363083: Epoch time: 225.05 s
2024-12-19 13:38:04.363786: Yayy! New best EMA pseudo Dice: 0.6769
2024-12-19 13:38:06.305828: 
2024-12-19 13:38:06.307127: Epoch 28
2024-12-19 13:38:06.307933: Current learning rate: 0.0083
2024-12-19 13:42:07.507325: Validation loss improved from -0.45188 to -0.46529! Patience: 0/50
2024-12-19 13:42:07.508471: train_loss -0.574
2024-12-19 13:42:07.509890: val_loss -0.4653
2024-12-19 13:42:07.511491: Pseudo dice [0.7047]
2024-12-19 13:42:07.512818: Epoch time: 241.2 s
2024-12-19 13:42:07.513741: Yayy! New best EMA pseudo Dice: 0.6797
2024-12-19 13:42:09.366965: 
2024-12-19 13:42:09.368694: Epoch 29
2024-12-19 13:42:09.370038: Current learning rate: 0.00824
2024-12-19 13:45:46.697271: Validation loss did not improve from -0.46529. Patience: 1/50
2024-12-19 13:45:46.698101: train_loss -0.5859
2024-12-19 13:45:46.698921: val_loss -0.4433
2024-12-19 13:45:46.699777: Pseudo dice [0.687]
2024-12-19 13:45:46.701079: Epoch time: 217.33 s
2024-12-19 13:45:47.492298: Yayy! New best EMA pseudo Dice: 0.6804
2024-12-19 13:45:49.415162: 
2024-12-19 13:45:49.416542: Epoch 30
2024-12-19 13:45:49.417507: Current learning rate: 0.00818
2024-12-19 13:49:47.231314: Validation loss improved from -0.46529 to -0.47450! Patience: 1/50
2024-12-19 13:49:47.232256: train_loss -0.5835
2024-12-19 13:49:47.233174: val_loss -0.4745
2024-12-19 13:49:47.234124: Pseudo dice [0.7174]
2024-12-19 13:49:47.235257: Epoch time: 237.82 s
2024-12-19 13:49:47.236175: Yayy! New best EMA pseudo Dice: 0.6841
2024-12-19 13:49:49.087102: 
2024-12-19 13:49:49.088184: Epoch 31
2024-12-19 13:49:49.088984: Current learning rate: 0.00812
2024-12-19 13:53:24.954376: Validation loss improved from -0.47450 to -0.48131! Patience: 0/50
2024-12-19 13:53:24.955546: train_loss -0.5944
2024-12-19 13:53:24.957343: val_loss -0.4813
2024-12-19 13:53:24.959642: Pseudo dice [0.723]
2024-12-19 13:53:24.961841: Epoch time: 215.87 s
2024-12-19 13:53:24.963610: Yayy! New best EMA pseudo Dice: 0.688
2024-12-19 13:53:26.901826: 
2024-12-19 13:53:26.903211: Epoch 32
2024-12-19 13:53:26.904136: Current learning rate: 0.00806
2024-12-19 13:57:15.383404: Validation loss improved from -0.48131 to -0.49690! Patience: 0/50
2024-12-19 13:57:15.384555: train_loss -0.6026
2024-12-19 13:57:15.386040: val_loss -0.4969
2024-12-19 13:57:15.387149: Pseudo dice [0.7188]
2024-12-19 13:57:15.388154: Epoch time: 228.48 s
2024-12-19 13:57:15.389062: Yayy! New best EMA pseudo Dice: 0.6911
2024-12-19 13:57:17.272175: 
2024-12-19 13:57:17.273479: Epoch 33
2024-12-19 13:57:17.274280: Current learning rate: 0.008
2024-12-19 14:01:22.005996: Validation loss did not improve from -0.49690. Patience: 1/50
2024-12-19 14:01:22.006878: train_loss -0.6025
2024-12-19 14:01:22.007880: val_loss -0.4445
2024-12-19 14:01:22.008960: Pseudo dice [0.7103]
2024-12-19 14:01:22.010744: Epoch time: 244.74 s
2024-12-19 14:01:22.012267: Yayy! New best EMA pseudo Dice: 0.693
2024-12-19 14:01:23.867733: 
2024-12-19 14:01:23.869031: Epoch 34
2024-12-19 14:01:23.869891: Current learning rate: 0.00793
2024-12-19 14:05:24.612655: Validation loss did not improve from -0.49690. Patience: 2/50
2024-12-19 14:05:24.613558: train_loss -0.6089
2024-12-19 14:05:24.614290: val_loss -0.411
2024-12-19 14:05:24.614930: Pseudo dice [0.68]
2024-12-19 14:05:24.615606: Epoch time: 240.75 s
2024-12-19 14:05:26.654332: 
2024-12-19 14:05:26.656119: Epoch 35
2024-12-19 14:05:26.657350: Current learning rate: 0.00787
2024-12-19 14:09:40.263298: Validation loss did not improve from -0.49690. Patience: 3/50
2024-12-19 14:09:40.264391: train_loss -0.6083
2024-12-19 14:09:40.265161: val_loss -0.4759
2024-12-19 14:09:40.265915: Pseudo dice [0.7138]
2024-12-19 14:09:40.266706: Epoch time: 253.61 s
2024-12-19 14:09:40.267498: Yayy! New best EMA pseudo Dice: 0.6939
2024-12-19 14:09:42.199248: 
2024-12-19 14:09:42.200509: Epoch 36
2024-12-19 14:09:42.201432: Current learning rate: 0.00781
2024-12-19 14:13:57.845157: Validation loss did not improve from -0.49690. Patience: 4/50
2024-12-19 14:13:57.846170: train_loss -0.6174
2024-12-19 14:13:57.847136: val_loss -0.4452
2024-12-19 14:13:57.848067: Pseudo dice [0.709]
2024-12-19 14:13:57.848976: Epoch time: 255.65 s
2024-12-19 14:13:57.849802: Yayy! New best EMA pseudo Dice: 0.6954
2024-12-19 14:13:59.752397: 
2024-12-19 14:13:59.753929: Epoch 37
2024-12-19 14:13:59.755146: Current learning rate: 0.00775
2024-12-19 14:18:17.589360: Validation loss did not improve from -0.49690. Patience: 5/50
2024-12-19 14:18:17.590416: train_loss -0.6107
2024-12-19 14:18:17.591224: val_loss -0.4887
2024-12-19 14:18:17.592000: Pseudo dice [0.7362]
2024-12-19 14:18:17.592899: Epoch time: 257.84 s
2024-12-19 14:18:17.593642: Yayy! New best EMA pseudo Dice: 0.6995
2024-12-19 14:18:19.524100: 
2024-12-19 14:18:19.526077: Epoch 38
2024-12-19 14:18:19.527564: Current learning rate: 0.00769
2024-12-19 14:22:27.626119: Validation loss did not improve from -0.49690. Patience: 6/50
2024-12-19 14:22:27.627050: train_loss -0.618
2024-12-19 14:22:27.628140: val_loss -0.454
2024-12-19 14:22:27.629234: Pseudo dice [0.7085]
2024-12-19 14:22:27.630320: Epoch time: 248.1 s
2024-12-19 14:22:27.631531: Yayy! New best EMA pseudo Dice: 0.7004
2024-12-19 14:22:29.512585: 
2024-12-19 14:22:29.514131: Epoch 39
2024-12-19 14:22:29.515545: Current learning rate: 0.00763
2024-12-19 14:26:13.653102: Validation loss did not improve from -0.49690. Patience: 7/50
2024-12-19 14:26:13.654108: train_loss -0.6158
2024-12-19 14:26:13.654852: val_loss -0.4583
2024-12-19 14:26:13.655569: Pseudo dice [0.722]
2024-12-19 14:26:13.656272: Epoch time: 224.14 s
2024-12-19 14:26:14.061077: Yayy! New best EMA pseudo Dice: 0.7026
2024-12-19 14:26:16.811192: 
2024-12-19 14:26:16.813125: Epoch 40
2024-12-19 14:26:16.814365: Current learning rate: 0.00756
2024-12-19 14:29:58.960047: Validation loss did not improve from -0.49690. Patience: 8/50
2024-12-19 14:29:58.960951: train_loss -0.6188
2024-12-19 14:29:58.961895: val_loss -0.4461
2024-12-19 14:29:58.962688: Pseudo dice [0.6974]
2024-12-19 14:29:58.963420: Epoch time: 222.15 s
2024-12-19 14:30:00.551937: 
2024-12-19 14:30:00.553097: Epoch 41
2024-12-19 14:30:00.553947: Current learning rate: 0.0075
2024-12-19 14:33:39.398636: Validation loss did not improve from -0.49690. Patience: 9/50
2024-12-19 14:33:39.399567: train_loss -0.6245
2024-12-19 14:33:39.400374: val_loss -0.4774
2024-12-19 14:33:39.401105: Pseudo dice [0.7228]
2024-12-19 14:33:39.401812: Epoch time: 218.85 s
2024-12-19 14:33:39.402669: Yayy! New best EMA pseudo Dice: 0.7041
2024-12-19 14:33:41.214214: 
2024-12-19 14:33:41.215553: Epoch 42
2024-12-19 14:33:41.216497: Current learning rate: 0.00744
2024-12-19 14:37:45.272537: Validation loss did not improve from -0.49690. Patience: 10/50
2024-12-19 14:37:45.273391: train_loss -0.6299
2024-12-19 14:37:45.274330: val_loss -0.4793
2024-12-19 14:37:45.275094: Pseudo dice [0.7207]
2024-12-19 14:37:45.275850: Epoch time: 244.06 s
2024-12-19 14:37:45.276692: Yayy! New best EMA pseudo Dice: 0.7058
2024-12-19 14:37:47.098362: 
2024-12-19 14:37:47.099283: Epoch 43
2024-12-19 14:37:47.100066: Current learning rate: 0.00738
2024-12-19 14:41:27.887416: Validation loss did not improve from -0.49690. Patience: 11/50
2024-12-19 14:41:27.888438: train_loss -0.6351
2024-12-19 14:41:27.889943: val_loss -0.4877
2024-12-19 14:41:27.891452: Pseudo dice [0.7222]
2024-12-19 14:41:27.892930: Epoch time: 220.79 s
2024-12-19 14:41:27.894155: Yayy! New best EMA pseudo Dice: 0.7074
2024-12-19 14:41:29.728188: 
2024-12-19 14:41:29.730402: Epoch 44
2024-12-19 14:41:29.731522: Current learning rate: 0.00732
2024-12-19 14:45:20.647022: Validation loss did not improve from -0.49690. Patience: 12/50
2024-12-19 14:45:20.647815: train_loss -0.6414
2024-12-19 14:45:20.648567: val_loss -0.4664
2024-12-19 14:45:20.649244: Pseudo dice [0.7187]
2024-12-19 14:45:20.649966: Epoch time: 230.92 s
2024-12-19 14:45:21.110063: Yayy! New best EMA pseudo Dice: 0.7086
2024-12-19 14:45:23.000450: 
2024-12-19 14:45:23.002409: Epoch 45
2024-12-19 14:45:23.004069: Current learning rate: 0.00725
2024-12-19 14:49:14.390848: Validation loss did not improve from -0.49690. Patience: 13/50
2024-12-19 14:49:14.391783: train_loss -0.6389
2024-12-19 14:49:14.392880: val_loss -0.492
2024-12-19 14:49:14.393923: Pseudo dice [0.7325]
2024-12-19 14:49:14.394972: Epoch time: 231.39 s
2024-12-19 14:49:14.395921: Yayy! New best EMA pseudo Dice: 0.7109
2024-12-19 14:49:16.242698: 
2024-12-19 14:49:16.244080: Epoch 46
2024-12-19 14:49:16.244891: Current learning rate: 0.00719
2024-12-19 14:52:57.360848: Validation loss did not improve from -0.49690. Patience: 14/50
2024-12-19 14:52:57.362080: train_loss -0.6313
2024-12-19 14:52:57.363217: val_loss -0.4909
2024-12-19 14:52:57.364235: Pseudo dice [0.7334]
2024-12-19 14:52:57.365251: Epoch time: 221.12 s
2024-12-19 14:52:57.366105: Yayy! New best EMA pseudo Dice: 0.7132
2024-12-19 14:52:59.258316: 
2024-12-19 14:52:59.259507: Epoch 47
2024-12-19 14:52:59.260584: Current learning rate: 0.00713
2024-12-19 14:56:58.139362: Validation loss did not improve from -0.49690. Patience: 15/50
2024-12-19 14:56:58.140219: train_loss -0.6428
2024-12-19 14:56:58.141026: val_loss -0.4727
2024-12-19 14:56:58.141715: Pseudo dice [0.7263]
2024-12-19 14:56:58.142478: Epoch time: 238.88 s
2024-12-19 14:56:58.143273: Yayy! New best EMA pseudo Dice: 0.7145
2024-12-19 14:57:00.102777: 
2024-12-19 14:57:00.104745: Epoch 48
2024-12-19 14:57:00.106302: Current learning rate: 0.00707
2024-12-19 15:01:15.841298: Validation loss did not improve from -0.49690. Patience: 16/50
2024-12-19 15:01:15.842149: train_loss -0.6509
2024-12-19 15:01:15.843388: val_loss -0.4754
2024-12-19 15:01:15.844675: Pseudo dice [0.7066]
2024-12-19 15:01:15.845762: Epoch time: 255.74 s
2024-12-19 15:01:17.297317: 
2024-12-19 15:01:17.298832: Epoch 49
2024-12-19 15:01:17.299980: Current learning rate: 0.007
2024-12-19 15:05:34.754538: Validation loss did not improve from -0.49690. Patience: 17/50
2024-12-19 15:05:34.755442: train_loss -0.6513
2024-12-19 15:05:34.756413: val_loss -0.4772
2024-12-19 15:05:34.757277: Pseudo dice [0.7244]
2024-12-19 15:05:34.758230: Epoch time: 257.46 s
2024-12-19 15:05:35.202079: Yayy! New best EMA pseudo Dice: 0.7148
2024-12-19 15:05:37.091327: 
2024-12-19 15:05:37.092872: Epoch 50
2024-12-19 15:05:37.093976: Current learning rate: 0.00694
2024-12-19 15:09:56.777084: Validation loss did not improve from -0.49690. Patience: 18/50
2024-12-19 15:09:56.778286: train_loss -0.6455
2024-12-19 15:09:56.779880: val_loss -0.4469
2024-12-19 15:09:56.781467: Pseudo dice [0.7033]
2024-12-19 15:09:56.782792: Epoch time: 259.69 s
2024-12-19 15:09:58.664697: 
2024-12-19 15:09:58.666519: Epoch 51
2024-12-19 15:09:58.667828: Current learning rate: 0.00688
2024-12-19 15:14:18.642639: Validation loss improved from -0.49690 to -0.50412! Patience: 18/50
2024-12-19 15:14:18.643533: train_loss -0.6536
2024-12-19 15:14:18.644454: val_loss -0.5041
2024-12-19 15:14:18.645440: Pseudo dice [0.7366]
2024-12-19 15:14:18.646722: Epoch time: 259.98 s
2024-12-19 15:14:18.648134: Yayy! New best EMA pseudo Dice: 0.7159
2024-12-19 15:14:20.510216: 
2024-12-19 15:14:20.511618: Epoch 52
2024-12-19 15:14:20.512643: Current learning rate: 0.00682
2024-12-19 15:18:32.602269: Validation loss did not improve from -0.50412. Patience: 1/50
2024-12-19 15:18:32.614558: train_loss -0.6628
2024-12-19 15:18:32.616361: val_loss -0.4857
2024-12-19 15:18:32.617645: Pseudo dice [0.7187]
2024-12-19 15:18:32.618683: Epoch time: 252.11 s
2024-12-19 15:18:32.619502: Yayy! New best EMA pseudo Dice: 0.7162
2024-12-19 15:18:34.521185: 
2024-12-19 15:18:34.523176: Epoch 53
2024-12-19 15:18:34.524583: Current learning rate: 0.00675
2024-12-19 15:22:58.667001: Validation loss did not improve from -0.50412. Patience: 2/50
2024-12-19 15:22:58.668093: train_loss -0.6641
2024-12-19 15:22:58.669070: val_loss -0.4692
2024-12-19 15:22:58.670016: Pseudo dice [0.7262]
2024-12-19 15:22:58.670864: Epoch time: 264.15 s
2024-12-19 15:22:58.671856: Yayy! New best EMA pseudo Dice: 0.7172
2024-12-19 15:23:00.613490: 
2024-12-19 15:23:00.614977: Epoch 54
2024-12-19 15:23:00.616275: Current learning rate: 0.00669
2024-12-19 15:26:55.343312: Validation loss did not improve from -0.50412. Patience: 3/50
2024-12-19 15:26:55.344141: train_loss -0.661
2024-12-19 15:26:55.344853: val_loss -0.4939
2024-12-19 15:26:55.345553: Pseudo dice [0.7296]
2024-12-19 15:26:55.346307: Epoch time: 234.73 s
2024-12-19 15:26:55.732088: Yayy! New best EMA pseudo Dice: 0.7185
2024-12-19 15:26:57.796253: 
2024-12-19 15:26:57.797248: Epoch 55
2024-12-19 15:26:57.798000: Current learning rate: 0.00663
2024-12-19 15:31:10.387275: Validation loss did not improve from -0.50412. Patience: 4/50
2024-12-19 15:31:10.388319: train_loss -0.6697
2024-12-19 15:31:10.389623: val_loss -0.4844
2024-12-19 15:31:10.390755: Pseudo dice [0.7249]
2024-12-19 15:31:10.391723: Epoch time: 252.59 s
2024-12-19 15:31:10.392570: Yayy! New best EMA pseudo Dice: 0.7191
2024-12-19 15:31:12.231575: 
2024-12-19 15:31:12.233102: Epoch 56
2024-12-19 15:31:12.234317: Current learning rate: 0.00657
2024-12-19 15:35:12.070880: Validation loss did not improve from -0.50412. Patience: 5/50
2024-12-19 15:35:12.072199: train_loss -0.6691
2024-12-19 15:35:12.073673: val_loss -0.5017
2024-12-19 15:35:12.075065: Pseudo dice [0.7348]
2024-12-19 15:35:12.076530: Epoch time: 239.84 s
2024-12-19 15:35:12.077968: Yayy! New best EMA pseudo Dice: 0.7207
2024-12-19 15:35:14.014028: 
2024-12-19 15:35:14.015316: Epoch 57
2024-12-19 15:35:14.016212: Current learning rate: 0.0065
2024-12-19 15:39:06.709658: Validation loss improved from -0.50412 to -0.51334! Patience: 5/50
2024-12-19 15:39:06.710734: train_loss -0.6738
2024-12-19 15:39:06.711652: val_loss -0.5133
2024-12-19 15:39:06.712384: Pseudo dice [0.746]
2024-12-19 15:39:06.713203: Epoch time: 232.7 s
2024-12-19 15:39:06.713903: Yayy! New best EMA pseudo Dice: 0.7232
2024-12-19 15:39:08.637244: 
2024-12-19 15:39:08.638453: Epoch 58
2024-12-19 15:39:08.639278: Current learning rate: 0.00644
2024-12-19 15:43:06.509732: Validation loss did not improve from -0.51334. Patience: 1/50
2024-12-19 15:43:06.510818: train_loss -0.6754
2024-12-19 15:43:06.512437: val_loss -0.5028
2024-12-19 15:43:06.513478: Pseudo dice [0.7341]
2024-12-19 15:43:06.514522: Epoch time: 237.87 s
2024-12-19 15:43:06.515680: Yayy! New best EMA pseudo Dice: 0.7243
2024-12-19 15:43:08.534515: 
2024-12-19 15:43:08.535806: Epoch 59
2024-12-19 15:43:08.536700: Current learning rate: 0.00638
2024-12-19 15:47:14.693564: Validation loss did not improve from -0.51334. Patience: 2/50
2024-12-19 15:47:14.694822: train_loss -0.6808
2024-12-19 15:47:14.695745: val_loss -0.4459
2024-12-19 15:47:14.696544: Pseudo dice [0.7055]
2024-12-19 15:47:14.697273: Epoch time: 246.16 s
2024-12-19 15:47:16.671701: 
2024-12-19 15:47:16.672865: Epoch 60
2024-12-19 15:47:16.673562: Current learning rate: 0.00631
2024-12-19 15:51:35.602613: Validation loss did not improve from -0.51334. Patience: 3/50
2024-12-19 15:51:35.603692: train_loss -0.679
2024-12-19 15:51:35.605154: val_loss -0.4689
2024-12-19 15:51:35.606236: Pseudo dice [0.718]
2024-12-19 15:51:35.607483: Epoch time: 258.93 s
2024-12-19 15:51:37.160679: 
2024-12-19 15:51:37.161846: Epoch 61
2024-12-19 15:51:37.162926: Current learning rate: 0.00625
2024-12-19 15:55:32.000140: Validation loss did not improve from -0.51334. Patience: 4/50
2024-12-19 15:55:32.001175: train_loss -0.681
2024-12-19 15:55:32.001945: val_loss -0.5037
2024-12-19 15:55:32.002645: Pseudo dice [0.7339]
2024-12-19 15:55:32.003357: Epoch time: 234.84 s
2024-12-19 15:55:33.993634: 
2024-12-19 15:55:33.994802: Epoch 62
2024-12-19 15:55:33.995516: Current learning rate: 0.00619
2024-12-19 15:59:30.417701: Validation loss did not improve from -0.51334. Patience: 5/50
2024-12-19 15:59:30.418825: train_loss -0.6823
2024-12-19 15:59:30.420904: val_loss -0.502
2024-12-19 15:59:30.422692: Pseudo dice [0.7346]
2024-12-19 15:59:30.424975: Epoch time: 236.43 s
2024-12-19 15:59:30.426537: Yayy! New best EMA pseudo Dice: 0.7243
2024-12-19 15:59:32.784463: 
2024-12-19 15:59:32.786241: Epoch 63
2024-12-19 15:59:32.787837: Current learning rate: 0.00612
2024-12-19 16:03:29.045588: Validation loss did not improve from -0.51334. Patience: 6/50
2024-12-19 16:03:29.046560: train_loss -0.6845
2024-12-19 16:03:29.047391: val_loss -0.4699
2024-12-19 16:03:29.048075: Pseudo dice [0.7122]
2024-12-19 16:03:29.048751: Epoch time: 236.26 s
2024-12-19 16:03:30.622101: 
2024-12-19 16:03:30.623501: Epoch 64
2024-12-19 16:03:30.624355: Current learning rate: 0.00606
2024-12-19 16:07:34.871320: Validation loss did not improve from -0.51334. Patience: 7/50
2024-12-19 16:07:34.872445: train_loss -0.6871
2024-12-19 16:07:34.873277: val_loss -0.4809
2024-12-19 16:07:34.874065: Pseudo dice [0.7322]
2024-12-19 16:07:34.874830: Epoch time: 244.25 s
2024-12-19 16:07:36.851091: 
2024-12-19 16:07:36.852517: Epoch 65
2024-12-19 16:07:36.853480: Current learning rate: 0.006
2024-12-19 16:11:27.795085: Validation loss did not improve from -0.51334. Patience: 8/50
2024-12-19 16:11:27.796058: train_loss -0.688
2024-12-19 16:11:27.796889: val_loss -0.4522
2024-12-19 16:11:27.797549: Pseudo dice [0.706]
2024-12-19 16:11:27.798160: Epoch time: 230.95 s
2024-12-19 16:11:29.266723: 
2024-12-19 16:11:29.268186: Epoch 66
2024-12-19 16:11:29.269637: Current learning rate: 0.00593
2024-12-19 16:15:24.766017: Validation loss did not improve from -0.51334. Patience: 9/50
2024-12-19 16:15:24.766987: train_loss -0.688
2024-12-19 16:15:24.767946: val_loss -0.486
2024-12-19 16:15:24.768749: Pseudo dice [0.7247]
2024-12-19 16:15:24.769591: Epoch time: 235.5 s
2024-12-19 16:15:26.258277: 
2024-12-19 16:15:26.259886: Epoch 67
2024-12-19 16:15:26.261537: Current learning rate: 0.00587
2024-12-19 16:19:30.715832: Validation loss did not improve from -0.51334. Patience: 10/50
2024-12-19 16:19:30.716913: train_loss -0.6951
2024-12-19 16:19:30.717771: val_loss -0.4877
2024-12-19 16:19:30.718561: Pseudo dice [0.7305]
2024-12-19 16:19:30.719425: Epoch time: 244.46 s
2024-12-19 16:19:32.207333: 
2024-12-19 16:19:32.208632: Epoch 68
2024-12-19 16:19:32.209453: Current learning rate: 0.00581
2024-12-19 16:23:48.862651: Validation loss did not improve from -0.51334. Patience: 11/50
2024-12-19 16:23:48.894231: train_loss -0.6968
2024-12-19 16:23:48.895502: val_loss -0.4548
2024-12-19 16:23:48.896921: Pseudo dice [0.7161]
2024-12-19 16:23:48.898391: Epoch time: 256.69 s
2024-12-19 16:23:50.396437: 
2024-12-19 16:23:50.397930: Epoch 69
2024-12-19 16:23:50.399072: Current learning rate: 0.00574
2024-12-19 16:28:18.395693: Validation loss did not improve from -0.51334. Patience: 12/50
2024-12-19 16:28:18.397054: train_loss -0.701
2024-12-19 16:28:18.398111: val_loss -0.4728
2024-12-19 16:28:18.399217: Pseudo dice [0.7193]
2024-12-19 16:28:18.400481: Epoch time: 268.0 s
2024-12-19 16:28:20.479516: 
2024-12-19 16:28:20.481284: Epoch 70
2024-12-19 16:28:20.482580: Current learning rate: 0.00568
2024-12-19 16:32:47.468937: Validation loss did not improve from -0.51334. Patience: 13/50
2024-12-19 16:32:47.469805: train_loss -0.7009
2024-12-19 16:32:47.470910: val_loss -0.4887
2024-12-19 16:32:47.471844: Pseudo dice [0.7307]
2024-12-19 16:32:47.472630: Epoch time: 266.99 s
2024-12-19 16:32:48.950643: 
2024-12-19 16:32:48.951985: Epoch 71
2024-12-19 16:32:48.952723: Current learning rate: 0.00562
2024-12-19 16:37:17.679065: Validation loss did not improve from -0.51334. Patience: 14/50
2024-12-19 16:37:17.680040: train_loss -0.6996
2024-12-19 16:37:17.680906: val_loss -0.4704
2024-12-19 16:37:17.681717: Pseudo dice [0.7263]
2024-12-19 16:37:17.682528: Epoch time: 268.73 s
2024-12-19 16:37:19.220736: 
2024-12-19 16:37:19.222021: Epoch 72
2024-12-19 16:37:19.222814: Current learning rate: 0.00555
2024-12-19 16:41:43.295378: Validation loss did not improve from -0.51334. Patience: 15/50
2024-12-19 16:41:43.296400: train_loss -0.6962
2024-12-19 16:41:43.297395: val_loss -0.4817
2024-12-19 16:41:43.298205: Pseudo dice [0.7336]
2024-12-19 16:41:43.298954: Epoch time: 264.08 s
2024-12-19 16:41:43.299639: Yayy! New best EMA pseudo Dice: 0.7244
2024-12-19 16:41:45.794485: 
2024-12-19 16:41:45.796514: Epoch 73
2024-12-19 16:41:45.798341: Current learning rate: 0.00549
2024-12-19 16:45:51.504361: Validation loss did not improve from -0.51334. Patience: 16/50
2024-12-19 16:45:51.505295: train_loss -0.703
2024-12-19 16:45:51.506058: val_loss -0.4965
2024-12-19 16:45:51.506751: Pseudo dice [0.7331]
2024-12-19 16:45:51.507456: Epoch time: 245.71 s
2024-12-19 16:45:51.508116: Yayy! New best EMA pseudo Dice: 0.7253
2024-12-19 16:45:53.538961: 
2024-12-19 16:45:53.540443: Epoch 74
2024-12-19 16:45:53.542061: Current learning rate: 0.00542
2024-12-19 16:49:58.274725: Validation loss did not improve from -0.51334. Patience: 17/50
2024-12-19 16:49:58.275743: train_loss -0.7011
2024-12-19 16:49:58.276676: val_loss -0.4743
2024-12-19 16:49:58.277600: Pseudo dice [0.717]
2024-12-19 16:49:58.278875: Epoch time: 244.74 s
2024-12-19 16:50:00.259176: 
2024-12-19 16:50:00.260674: Epoch 75
2024-12-19 16:50:00.261721: Current learning rate: 0.00536
2024-12-19 16:54:00.437705: Validation loss did not improve from -0.51334. Patience: 18/50
2024-12-19 16:54:00.438690: train_loss -0.7127
2024-12-19 16:54:00.439498: val_loss -0.4566
2024-12-19 16:54:00.440265: Pseudo dice [0.7202]
2024-12-19 16:54:00.440957: Epoch time: 240.18 s
2024-12-19 16:54:01.974787: 
2024-12-19 16:54:01.977598: Epoch 76
2024-12-19 16:54:01.979495: Current learning rate: 0.00529
2024-12-19 16:58:13.678267: Validation loss did not improve from -0.51334. Patience: 19/50
2024-12-19 16:58:13.679307: train_loss -0.7099
2024-12-19 16:58:13.680858: val_loss -0.4602
2024-12-19 16:58:13.682149: Pseudo dice [0.7148]
2024-12-19 16:58:13.683462: Epoch time: 251.71 s
2024-12-19 16:58:15.154725: 
2024-12-19 16:58:15.156640: Epoch 77
2024-12-19 16:58:15.158707: Current learning rate: 0.00523
2024-12-19 17:02:29.378544: Validation loss did not improve from -0.51334. Patience: 20/50
2024-12-19 17:02:29.379884: train_loss -0.7114
2024-12-19 17:02:29.381391: val_loss -0.4838
2024-12-19 17:02:29.382474: Pseudo dice [0.7273]
2024-12-19 17:02:29.383918: Epoch time: 254.23 s
2024-12-19 17:02:30.921128: 
2024-12-19 17:02:30.922411: Epoch 78
2024-12-19 17:02:30.923424: Current learning rate: 0.00517
2024-12-19 17:06:58.799889: Validation loss did not improve from -0.51334. Patience: 21/50
2024-12-19 17:06:58.800638: train_loss -0.7103
2024-12-19 17:06:58.801371: val_loss -0.5014
2024-12-19 17:06:58.802040: Pseudo dice [0.7335]
2024-12-19 17:06:58.802861: Epoch time: 267.88 s
2024-12-19 17:07:00.316036: 
2024-12-19 17:07:00.317219: Epoch 79
2024-12-19 17:07:00.318251: Current learning rate: 0.0051
2024-12-19 17:11:30.220477: Validation loss did not improve from -0.51334. Patience: 22/50
2024-12-19 17:11:30.221392: train_loss -0.7107
2024-12-19 17:11:30.222419: val_loss -0.4772
2024-12-19 17:11:30.223355: Pseudo dice [0.723]
2024-12-19 17:11:30.224297: Epoch time: 269.91 s
2024-12-19 17:11:32.311953: 
2024-12-19 17:11:32.313161: Epoch 80
2024-12-19 17:11:32.314134: Current learning rate: 0.00504
2024-12-19 17:15:49.693192: Validation loss improved from -0.51334 to -0.51593! Patience: 22/50
2024-12-19 17:15:49.694179: train_loss -0.7196
2024-12-19 17:15:49.694946: val_loss -0.5159
2024-12-19 17:15:49.695741: Pseudo dice [0.7479]
2024-12-19 17:15:49.696779: Epoch time: 257.38 s
2024-12-19 17:15:49.697570: Yayy! New best EMA pseudo Dice: 0.7267
2024-12-19 17:15:51.618366: 
2024-12-19 17:15:51.619294: Epoch 81
2024-12-19 17:15:51.620062: Current learning rate: 0.00497
2024-12-19 17:20:25.077165: Validation loss did not improve from -0.51593. Patience: 1/50
2024-12-19 17:20:25.078037: train_loss -0.717
2024-12-19 17:20:25.079063: val_loss -0.4729
2024-12-19 17:20:25.079872: Pseudo dice [0.7342]
2024-12-19 17:20:25.080651: Epoch time: 273.46 s
2024-12-19 17:20:25.081375: Yayy! New best EMA pseudo Dice: 0.7275
2024-12-19 17:20:27.133763: 
2024-12-19 17:20:27.135180: Epoch 82
2024-12-19 17:20:27.136481: Current learning rate: 0.00491
2024-12-19 17:24:41.745116: Validation loss did not improve from -0.51593. Patience: 2/50
2024-12-19 17:24:41.746065: train_loss -0.7219
2024-12-19 17:24:41.747390: val_loss -0.482
2024-12-19 17:24:41.748696: Pseudo dice [0.7313]
2024-12-19 17:24:41.749811: Epoch time: 254.61 s
2024-12-19 17:24:41.750609: Yayy! New best EMA pseudo Dice: 0.7279
2024-12-19 17:24:43.546553: 
2024-12-19 17:24:43.548226: Epoch 83
2024-12-19 17:24:43.549608: Current learning rate: 0.00484
2024-12-19 17:28:45.842642: Validation loss did not improve from -0.51593. Patience: 3/50
2024-12-19 17:28:45.845406: train_loss -0.7168
2024-12-19 17:28:45.847177: val_loss -0.4519
2024-12-19 17:28:45.848511: Pseudo dice [0.706]
2024-12-19 17:28:45.849806: Epoch time: 242.3 s
2024-12-19 17:28:47.785581: 
2024-12-19 17:28:47.786942: Epoch 84
2024-12-19 17:28:47.788266: Current learning rate: 0.00478
2024-12-19 17:32:44.599810: Validation loss did not improve from -0.51593. Patience: 4/50
2024-12-19 17:32:44.600896: train_loss -0.7249
2024-12-19 17:32:44.601676: val_loss -0.5159
2024-12-19 17:32:44.602342: Pseudo dice [0.7462]
2024-12-19 17:32:44.603017: Epoch time: 236.82 s
2024-12-19 17:32:46.394785: 
2024-12-19 17:32:46.396331: Epoch 85
2024-12-19 17:32:46.397464: Current learning rate: 0.00471
2024-12-19 17:36:38.294381: Validation loss did not improve from -0.51593. Patience: 5/50
2024-12-19 17:36:38.295567: train_loss -0.7219
2024-12-19 17:36:38.296438: val_loss -0.4779
2024-12-19 17:36:38.297227: Pseudo dice [0.7292]
2024-12-19 17:36:38.298010: Epoch time: 231.9 s
2024-12-19 17:36:38.298676: Yayy! New best EMA pseudo Dice: 0.7279
2024-12-19 17:36:40.201683: 
2024-12-19 17:36:40.202949: Epoch 86
2024-12-19 17:36:40.203853: Current learning rate: 0.00465
2024-12-19 17:40:37.940905: Validation loss did not improve from -0.51593. Patience: 6/50
2024-12-19 17:40:37.941867: train_loss -0.7308
2024-12-19 17:40:37.942816: val_loss -0.4759
2024-12-19 17:40:37.943629: Pseudo dice [0.7287]
2024-12-19 17:40:37.944296: Epoch time: 237.74 s
2024-12-19 17:40:37.944963: Yayy! New best EMA pseudo Dice: 0.728
2024-12-19 17:40:39.841803: 
2024-12-19 17:40:39.843212: Epoch 87
2024-12-19 17:40:39.844068: Current learning rate: 0.00458
2024-12-19 17:44:36.176792: Validation loss did not improve from -0.51593. Patience: 7/50
2024-12-19 17:44:36.177557: train_loss -0.7291
2024-12-19 17:44:36.178317: val_loss -0.4818
2024-12-19 17:44:36.179028: Pseudo dice [0.7364]
2024-12-19 17:44:36.179923: Epoch time: 236.34 s
2024-12-19 17:44:36.180714: Yayy! New best EMA pseudo Dice: 0.7288
2024-12-19 17:44:37.995794: 
2024-12-19 17:44:37.997822: Epoch 88
2024-12-19 17:44:37.999473: Current learning rate: 0.00452
2024-12-19 17:48:32.644622: Validation loss did not improve from -0.51593. Patience: 8/50
2024-12-19 17:48:32.645767: train_loss -0.7217
2024-12-19 17:48:32.647100: val_loss -0.4629
2024-12-19 17:48:32.648138: Pseudo dice [0.7217]
2024-12-19 17:48:32.649095: Epoch time: 234.65 s
2024-12-19 17:48:34.102265: 
2024-12-19 17:48:34.105960: Epoch 89
2024-12-19 17:48:34.107597: Current learning rate: 0.00445
2024-12-19 17:52:36.762541: Validation loss did not improve from -0.51593. Patience: 9/50
2024-12-19 17:52:36.763683: train_loss -0.7291
2024-12-19 17:52:36.764555: val_loss -0.4742
2024-12-19 17:52:36.765370: Pseudo dice [0.7243]
2024-12-19 17:52:36.766387: Epoch time: 242.67 s
2024-12-19 17:52:38.619297: 
2024-12-19 17:52:38.621938: Epoch 90
2024-12-19 17:52:38.622876: Current learning rate: 0.00438
2024-12-19 17:56:46.528269: Validation loss did not improve from -0.51593. Patience: 10/50
2024-12-19 17:56:46.529292: train_loss -0.732
2024-12-19 17:56:46.530175: val_loss -0.4643
2024-12-19 17:56:46.530793: Pseudo dice [0.7245]
2024-12-19 17:56:46.531450: Epoch time: 247.91 s
2024-12-19 17:56:47.936674: 
2024-12-19 17:56:47.937923: Epoch 91
2024-12-19 17:56:47.938790: Current learning rate: 0.00432
2024-12-19 18:00:37.117335: Validation loss did not improve from -0.51593. Patience: 11/50
2024-12-19 18:00:37.118230: train_loss -0.7299
2024-12-19 18:00:37.118891: val_loss -0.5084
2024-12-19 18:00:37.119514: Pseudo dice [0.7346]
2024-12-19 18:00:37.120131: Epoch time: 229.18 s
2024-12-19 18:00:38.576336: 
2024-12-19 18:00:38.577856: Epoch 92
2024-12-19 18:00:38.579000: Current learning rate: 0.00425
2024-12-19 18:04:54.847740: Validation loss did not improve from -0.51593. Patience: 12/50
2024-12-19 18:04:54.848686: train_loss -0.7379
2024-12-19 18:04:54.849576: val_loss -0.4749
2024-12-19 18:04:54.850497: Pseudo dice [0.7306]
2024-12-19 18:04:54.851431: Epoch time: 256.27 s
2024-12-19 18:04:56.296604: 
2024-12-19 18:04:56.297877: Epoch 93
2024-12-19 18:04:56.298622: Current learning rate: 0.00419
2024-12-19 18:09:16.763691: Validation loss did not improve from -0.51593. Patience: 13/50
2024-12-19 18:09:16.764776: train_loss -0.7347
2024-12-19 18:09:16.765697: val_loss -0.4705
2024-12-19 18:09:16.766490: Pseudo dice [0.7223]
2024-12-19 18:09:16.767208: Epoch time: 260.47 s
2024-12-19 18:09:18.192799: 
2024-12-19 18:09:18.194046: Epoch 94
2024-12-19 18:09:18.194761: Current learning rate: 0.00412
2024-12-19 18:13:41.566807: Validation loss did not improve from -0.51593. Patience: 14/50
2024-12-19 18:13:41.567698: train_loss -0.7321
2024-12-19 18:13:41.568461: val_loss -0.4902
2024-12-19 18:13:41.569167: Pseudo dice [0.7342]
2024-12-19 18:13:41.569779: Epoch time: 263.38 s
2024-12-19 18:13:43.498465: 
2024-12-19 18:13:43.499771: Epoch 95
2024-12-19 18:13:43.500768: Current learning rate: 0.00405
2024-12-19 18:18:04.209035: Validation loss did not improve from -0.51593. Patience: 15/50
2024-12-19 18:18:04.210019: train_loss -0.7358
2024-12-19 18:18:04.211156: val_loss -0.4844
2024-12-19 18:18:04.212529: Pseudo dice [0.7273]
2024-12-19 18:18:04.213706: Epoch time: 260.71 s
2024-12-19 18:18:06.264690: 
2024-12-19 18:18:06.266225: Epoch 96
2024-12-19 18:18:06.267766: Current learning rate: 0.00399
2024-12-19 18:22:30.674660: Validation loss did not improve from -0.51593. Patience: 16/50
2024-12-19 18:22:30.675491: train_loss -0.7378
2024-12-19 18:22:30.676255: val_loss -0.4599
2024-12-19 18:22:30.677011: Pseudo dice [0.7185]
2024-12-19 18:22:30.677680: Epoch time: 264.41 s
2024-12-19 18:22:32.207161: 
2024-12-19 18:22:32.209280: Epoch 97
2024-12-19 18:22:32.210606: Current learning rate: 0.00392
2024-12-19 18:27:00.393657: Validation loss did not improve from -0.51593. Patience: 17/50
2024-12-19 18:27:00.394707: train_loss -0.7411
2024-12-19 18:27:00.395635: val_loss -0.486
2024-12-19 18:27:00.396471: Pseudo dice [0.737]
2024-12-19 18:27:00.397435: Epoch time: 268.19 s
2024-12-19 18:27:01.826886: 
2024-12-19 18:27:01.828113: Epoch 98
2024-12-19 18:27:01.828851: Current learning rate: 0.00385
2024-12-19 18:31:47.985800: Validation loss did not improve from -0.51593. Patience: 18/50
2024-12-19 18:31:47.986680: train_loss -0.7417
2024-12-19 18:31:47.987687: val_loss -0.469
2024-12-19 18:31:47.988428: Pseudo dice [0.7184]
2024-12-19 18:31:47.989226: Epoch time: 286.16 s
2024-12-19 18:31:49.488756: 
2024-12-19 18:31:49.490143: Epoch 99
2024-12-19 18:31:49.491603: Current learning rate: 0.00379
2024-12-19 18:35:53.145566: Validation loss did not improve from -0.51593. Patience: 19/50
2024-12-19 18:35:53.148093: train_loss -0.7417
2024-12-19 18:35:53.149827: val_loss -0.4883
2024-12-19 18:35:53.150646: Pseudo dice [0.7443]
2024-12-19 18:35:53.151639: Epoch time: 243.66 s
2024-12-19 18:35:53.538806: Yayy! New best EMA pseudo Dice: 0.729
2024-12-19 18:35:55.440874: 
2024-12-19 18:35:55.442517: Epoch 100
2024-12-19 18:35:55.443636: Current learning rate: 0.00372
2024-12-19 18:40:12.509956: Validation loss did not improve from -0.51593. Patience: 20/50
2024-12-19 18:40:12.511179: train_loss -0.7428
2024-12-19 18:40:12.513062: val_loss -0.4804
2024-12-19 18:40:12.514712: Pseudo dice [0.7377]
2024-12-19 18:40:12.516033: Epoch time: 257.07 s
2024-12-19 18:40:12.517434: Yayy! New best EMA pseudo Dice: 0.7299
2024-12-19 18:40:14.392360: 
2024-12-19 18:40:14.393673: Epoch 101
2024-12-19 18:40:14.394443: Current learning rate: 0.00365
2024-12-19 18:44:25.795625: Validation loss did not improve from -0.51593. Patience: 21/50
2024-12-19 18:44:25.796565: train_loss -0.7481
2024-12-19 18:44:25.797532: val_loss -0.4848
2024-12-19 18:44:25.798498: Pseudo dice [0.7302]
2024-12-19 18:44:25.799459: Epoch time: 251.41 s
2024-12-19 18:44:25.800301: Yayy! New best EMA pseudo Dice: 0.7299
2024-12-19 18:44:27.679392: 
2024-12-19 18:44:27.680904: Epoch 102
2024-12-19 18:44:27.682115: Current learning rate: 0.00359
2024-12-19 18:48:46.802444: Validation loss did not improve from -0.51593. Patience: 22/50
2024-12-19 18:48:46.803405: train_loss -0.7481
2024-12-19 18:48:46.804148: val_loss -0.4998
2024-12-19 18:48:46.804914: Pseudo dice [0.7465]
2024-12-19 18:48:46.805794: Epoch time: 259.13 s
2024-12-19 18:48:46.806503: Yayy! New best EMA pseudo Dice: 0.7315
2024-12-19 18:48:48.633750: 
2024-12-19 18:48:48.634797: Epoch 103
2024-12-19 18:48:48.635720: Current learning rate: 0.00352
2024-12-19 18:53:20.195525: Validation loss did not improve from -0.51593. Patience: 23/50
2024-12-19 18:53:20.196692: train_loss -0.7463
2024-12-19 18:53:20.197566: val_loss -0.495
2024-12-19 18:53:20.198231: Pseudo dice [0.734]
2024-12-19 18:53:20.198926: Epoch time: 271.56 s
2024-12-19 18:53:20.199585: Yayy! New best EMA pseudo Dice: 0.7318
2024-12-19 18:53:22.013455: 
2024-12-19 18:53:22.015413: Epoch 104
2024-12-19 18:53:22.016839: Current learning rate: 0.00345
2024-12-19 18:57:53.344563: Validation loss did not improve from -0.51593. Patience: 24/50
2024-12-19 18:57:53.345406: train_loss -0.7457
2024-12-19 18:57:53.346154: val_loss -0.4879
2024-12-19 18:57:53.346866: Pseudo dice [0.7332]
2024-12-19 18:57:53.347508: Epoch time: 271.33 s
2024-12-19 18:57:53.857674: Yayy! New best EMA pseudo Dice: 0.7319
2024-12-19 18:57:55.736810: 
2024-12-19 18:57:55.738080: Epoch 105
2024-12-19 18:57:55.738814: Current learning rate: 0.00338
2024-12-19 19:02:20.429167: Validation loss did not improve from -0.51593. Patience: 25/50
2024-12-19 19:02:20.430034: train_loss -0.7471
2024-12-19 19:02:20.430999: val_loss -0.5079
2024-12-19 19:02:20.431820: Pseudo dice [0.7422]
2024-12-19 19:02:20.432679: Epoch time: 264.69 s
2024-12-19 19:02:20.433394: Yayy! New best EMA pseudo Dice: 0.733
2024-12-19 19:02:22.301820: 
2024-12-19 19:02:22.303732: Epoch 106
2024-12-19 19:02:22.305701: Current learning rate: 0.00332
2024-12-19 19:06:52.147360: Validation loss did not improve from -0.51593. Patience: 26/50
2024-12-19 19:06:52.148320: train_loss -0.7523
2024-12-19 19:06:52.149061: val_loss -0.4653
2024-12-19 19:06:52.149764: Pseudo dice [0.7269]
2024-12-19 19:06:52.150541: Epoch time: 269.85 s
2024-12-19 19:06:54.022669: 
2024-12-19 19:06:54.024425: Epoch 107
2024-12-19 19:06:54.026068: Current learning rate: 0.00325
2024-12-19 19:11:09.181075: Validation loss did not improve from -0.51593. Patience: 27/50
2024-12-19 19:11:09.182100: train_loss -0.7504
2024-12-19 19:11:09.183052: val_loss -0.4864
2024-12-19 19:11:09.183872: Pseudo dice [0.7324]
2024-12-19 19:11:09.184892: Epoch time: 255.16 s
2024-12-19 19:11:10.661910: 
2024-12-19 19:11:10.663314: Epoch 108
2024-12-19 19:11:10.664299: Current learning rate: 0.00318
2024-12-19 19:15:11.028474: Validation loss did not improve from -0.51593. Patience: 28/50
2024-12-19 19:15:11.029519: train_loss -0.7538
2024-12-19 19:15:11.030428: val_loss -0.4912
2024-12-19 19:15:11.031299: Pseudo dice [0.7282]
2024-12-19 19:15:11.032348: Epoch time: 240.37 s
2024-12-19 19:15:12.473492: 
2024-12-19 19:15:12.474672: Epoch 109
2024-12-19 19:15:12.475441: Current learning rate: 0.00311
2024-12-19 19:19:21.997067: Validation loss did not improve from -0.51593. Patience: 29/50
2024-12-19 19:19:21.998050: train_loss -0.7539
2024-12-19 19:19:21.999284: val_loss -0.4873
2024-12-19 19:19:22.000420: Pseudo dice [0.7371]
2024-12-19 19:19:22.001698: Epoch time: 249.53 s
2024-12-19 19:19:23.934374: 
2024-12-19 19:19:23.935616: Epoch 110
2024-12-19 19:19:23.936668: Current learning rate: 0.00304
2024-12-19 19:23:24.824490: Validation loss did not improve from -0.51593. Patience: 30/50
2024-12-19 19:23:24.825524: train_loss -0.7505
2024-12-19 19:23:24.826366: val_loss -0.4949
2024-12-19 19:23:24.827137: Pseudo dice [0.7506]
2024-12-19 19:23:24.827940: Epoch time: 240.89 s
2024-12-19 19:23:24.828806: Yayy! New best EMA pseudo Dice: 0.7343
2024-12-19 19:23:26.708676: 
2024-12-19 19:23:26.709955: Epoch 111
2024-12-19 19:23:26.711046: Current learning rate: 0.00297
2024-12-19 19:27:27.810934: Validation loss did not improve from -0.51593. Patience: 31/50
2024-12-19 19:27:27.811980: train_loss -0.755
2024-12-19 19:27:27.812832: val_loss -0.4772
2024-12-19 19:27:27.813537: Pseudo dice [0.7367]
2024-12-19 19:27:27.814229: Epoch time: 241.1 s
2024-12-19 19:27:27.814903: Yayy! New best EMA pseudo Dice: 0.7345
2024-12-19 19:27:29.676291: 
2024-12-19 19:27:29.677448: Epoch 112
2024-12-19 19:27:29.678222: Current learning rate: 0.00291
2024-12-19 19:31:47.753982: Validation loss did not improve from -0.51593. Patience: 32/50
2024-12-19 19:31:47.754882: train_loss -0.7602
2024-12-19 19:31:47.756172: val_loss -0.4995
2024-12-19 19:31:47.757269: Pseudo dice [0.7459]
2024-12-19 19:31:47.758325: Epoch time: 258.08 s
2024-12-19 19:31:47.759268: Yayy! New best EMA pseudo Dice: 0.7356
2024-12-19 19:31:49.560431: 
2024-12-19 19:31:49.561515: Epoch 113
2024-12-19 19:31:49.562322: Current learning rate: 0.00284
2024-12-19 19:36:18.635158: Validation loss did not improve from -0.51593. Patience: 33/50
2024-12-19 19:36:18.636112: train_loss -0.7569
2024-12-19 19:36:18.637082: val_loss -0.5009
2024-12-19 19:36:18.637719: Pseudo dice [0.7455]
2024-12-19 19:36:18.638470: Epoch time: 269.08 s
2024-12-19 19:36:18.639158: Yayy! New best EMA pseudo Dice: 0.7366
2024-12-19 19:36:20.530592: 
2024-12-19 19:36:20.531802: Epoch 114
2024-12-19 19:36:20.532571: Current learning rate: 0.00277
2024-12-19 19:40:42.253311: Validation loss did not improve from -0.51593. Patience: 34/50
2024-12-19 19:40:42.254165: train_loss -0.7556
2024-12-19 19:40:42.255221: val_loss -0.4817
2024-12-19 19:40:42.256312: Pseudo dice [0.7358]
2024-12-19 19:40:42.257356: Epoch time: 261.72 s
2024-12-19 19:40:44.117747: 
2024-12-19 19:40:44.118843: Epoch 115
2024-12-19 19:40:44.119523: Current learning rate: 0.0027
2024-12-19 19:46:31.680434: Validation loss did not improve from -0.51593. Patience: 35/50
2024-12-19 19:46:31.681721: train_loss -0.7578
2024-12-19 19:46:31.682723: val_loss -0.4783
2024-12-19 19:46:31.683750: Pseudo dice [0.7319]
2024-12-19 19:46:31.684814: Epoch time: 347.57 s
2024-12-19 19:46:33.211711: 
2024-12-19 19:46:33.213134: Epoch 116
2024-12-19 19:46:33.214013: Current learning rate: 0.00263
2024-12-19 19:52:53.894760: Validation loss did not improve from -0.51593. Patience: 36/50
2024-12-19 19:52:53.896158: train_loss -0.7594
2024-12-19 19:52:53.897602: val_loss -0.4777
2024-12-19 19:52:53.898895: Pseudo dice [0.7286]
2024-12-19 19:52:53.900122: Epoch time: 380.69 s
2024-12-19 19:52:55.345374: 
2024-12-19 19:52:55.346753: Epoch 117
2024-12-19 19:52:55.347603: Current learning rate: 0.00256
2024-12-19 19:59:18.410774: Validation loss did not improve from -0.51593. Patience: 37/50
2024-12-19 19:59:18.411892: train_loss -0.7608
2024-12-19 19:59:18.413203: val_loss -0.4716
2024-12-19 19:59:18.414495: Pseudo dice [0.7349]
2024-12-19 19:59:18.415508: Epoch time: 383.07 s
2024-12-19 19:59:20.698730: 
2024-12-19 19:59:20.700316: Epoch 118
2024-12-19 19:59:20.701579: Current learning rate: 0.00249
2024-12-19 20:05:50.877162: Validation loss did not improve from -0.51593. Patience: 38/50
2024-12-19 20:05:50.877868: train_loss -0.7611
2024-12-19 20:05:50.878577: val_loss -0.4883
2024-12-19 20:05:50.879325: Pseudo dice [0.7286]
2024-12-19 20:05:50.880225: Epoch time: 390.18 s
2024-12-19 20:05:52.375416: 
2024-12-19 20:05:52.376786: Epoch 119
2024-12-19 20:05:52.377661: Current learning rate: 0.00242
2024-12-19 20:12:18.779221: Validation loss did not improve from -0.51593. Patience: 39/50
2024-12-19 20:12:18.780097: train_loss -0.7622
2024-12-19 20:12:18.780934: val_loss -0.491
2024-12-19 20:12:18.781839: Pseudo dice [0.7416]
2024-12-19 20:12:18.782853: Epoch time: 386.41 s
2024-12-19 20:12:20.775172: 
2024-12-19 20:12:20.776283: Epoch 120
2024-12-19 20:12:20.777184: Current learning rate: 0.00235
2024-12-19 20:18:24.461316: Validation loss did not improve from -0.51593. Patience: 40/50
2024-12-19 20:18:24.462204: train_loss -0.763
2024-12-19 20:18:24.463454: val_loss -0.471
2024-12-19 20:18:24.464889: Pseudo dice [0.7266]
2024-12-19 20:18:24.466160: Epoch time: 363.69 s
2024-12-19 20:18:26.073709: 
2024-12-19 20:18:26.075040: Epoch 121
2024-12-19 20:18:26.075959: Current learning rate: 0.00228
2024-12-19 20:24:42.022664: Validation loss did not improve from -0.51593. Patience: 41/50
2024-12-19 20:24:42.023432: train_loss -0.7645
2024-12-19 20:24:42.024204: val_loss -0.4815
2024-12-19 20:24:42.024995: Pseudo dice [0.7323]
2024-12-19 20:24:42.025889: Epoch time: 375.95 s
2024-12-19 20:24:43.536876: 
2024-12-19 20:24:43.550883: Epoch 122
2024-12-19 20:24:43.552021: Current learning rate: 0.00221
2024-12-19 20:31:22.160978: Validation loss did not improve from -0.51593. Patience: 42/50
2024-12-19 20:31:22.162070: train_loss -0.7689
2024-12-19 20:31:22.162945: val_loss -0.4698
2024-12-19 20:31:22.163689: Pseudo dice [0.7256]
2024-12-19 20:31:22.164379: Epoch time: 398.63 s
2024-12-19 20:31:23.665041: 
2024-12-19 20:31:23.667570: Epoch 123
2024-12-19 20:31:23.669793: Current learning rate: 0.00214
2024-12-19 20:37:56.832427: Validation loss did not improve from -0.51593. Patience: 43/50
2024-12-19 20:37:56.833435: train_loss -0.766
2024-12-19 20:37:56.834642: val_loss -0.4803
2024-12-19 20:37:56.836245: Pseudo dice [0.7338]
2024-12-19 20:37:56.837844: Epoch time: 393.17 s
2024-12-19 20:37:58.375956: 
2024-12-19 20:37:58.377462: Epoch 124
2024-12-19 20:37:58.378861: Current learning rate: 0.00207
2024-12-19 20:44:32.147044: Validation loss did not improve from -0.51593. Patience: 44/50
2024-12-19 20:44:32.188818: train_loss -0.7631
2024-12-19 20:44:32.191392: val_loss -0.4916
2024-12-19 20:44:32.192696: Pseudo dice [0.7362]
2024-12-19 20:44:32.194392: Epoch time: 393.8 s
2024-12-19 20:44:34.185761: 
2024-12-19 20:44:34.186878: Epoch 125
2024-12-19 20:44:34.187725: Current learning rate: 0.00199
2024-12-19 20:51:19.084133: Validation loss did not improve from -0.51593. Patience: 45/50
2024-12-19 20:51:19.085061: train_loss -0.7652
2024-12-19 20:51:19.086079: val_loss -0.4667
2024-12-19 20:51:19.086980: Pseudo dice [0.7288]
2024-12-19 20:51:19.087749: Epoch time: 404.9 s
2024-12-19 20:51:20.613112: 
2024-12-19 20:51:20.614270: Epoch 126
2024-12-19 20:51:20.615258: Current learning rate: 0.00192
2024-12-19 20:57:47.983008: Validation loss did not improve from -0.51593. Patience: 46/50
2024-12-19 20:57:47.984035: train_loss -0.7672
2024-12-19 20:57:47.984776: val_loss -0.498
2024-12-19 20:57:47.985472: Pseudo dice [0.7412]
2024-12-19 20:57:47.986107: Epoch time: 387.37 s
2024-12-19 20:57:49.518607: 
2024-12-19 20:57:49.520061: Epoch 127
2024-12-19 20:57:49.520994: Current learning rate: 0.00185
2024-12-19 21:04:10.072594: Validation loss did not improve from -0.51593. Patience: 47/50
2024-12-19 21:04:10.073604: train_loss -0.7718
2024-12-19 21:04:10.074559: val_loss -0.4528
2024-12-19 21:04:10.075234: Pseudo dice [0.7251]
2024-12-19 21:04:10.076048: Epoch time: 380.56 s
2024-12-19 21:04:12.191060: 
2024-12-19 21:04:12.192286: Epoch 128
2024-12-19 21:04:12.193017: Current learning rate: 0.00178
2024-12-19 21:10:27.175334: Validation loss did not improve from -0.51593. Patience: 48/50
2024-12-19 21:10:27.176247: train_loss -0.7693
2024-12-19 21:10:27.177735: val_loss -0.4805
2024-12-19 21:10:27.179294: Pseudo dice [0.7301]
2024-12-19 21:10:27.180812: Epoch time: 374.99 s
2024-12-19 21:10:28.672896: 
2024-12-19 21:10:28.674393: Epoch 129
2024-12-19 21:10:28.675440: Current learning rate: 0.0017
2024-12-19 21:16:31.730771: Validation loss did not improve from -0.51593. Patience: 49/50
2024-12-19 21:16:31.731646: train_loss -0.7706
2024-12-19 21:16:31.733271: val_loss -0.4966
2024-12-19 21:16:31.734845: Pseudo dice [0.7346]
2024-12-19 21:16:31.736228: Epoch time: 363.06 s
2024-12-19 21:16:33.661273: 
2024-12-19 21:16:33.662935: Epoch 130
2024-12-19 21:16:33.664237: Current learning rate: 0.00163
2024-12-19 21:22:57.097514: Validation loss did not improve from -0.51593. Patience: 50/50
2024-12-19 21:22:57.098245: train_loss -0.7724
2024-12-19 21:22:57.098982: val_loss -0.4795
2024-12-19 21:22:57.099660: Pseudo dice [0.7348]
2024-12-19 21:22:57.100321: Epoch time: 383.44 s
2024-12-19 21:22:58.615985: 
2024-12-19 21:22:58.617207: Epoch 131
2024-12-19 21:22:58.618162: Current learning rate: 0.00156
2024-12-19 21:29:43.295679: Validation loss did not improve from -0.51593. Patience: 51/50
2024-12-19 21:29:43.296871: train_loss -0.7717
2024-12-19 21:29:43.298213: val_loss -0.4982
2024-12-19 21:29:43.299296: Pseudo dice [0.7395]
2024-12-19 21:29:43.300255: Epoch time: 404.68 s
2024-12-19 21:29:44.781189: 
2024-12-19 21:29:44.783096: Epoch 132
2024-12-19 21:29:44.784369: Current learning rate: 0.00148
2024-12-19 21:37:02.927496: Validation loss did not improve from -0.51593. Patience: 52/50
2024-12-19 21:37:02.928821: train_loss -0.7744
2024-12-19 21:37:02.930691: val_loss -0.4921
2024-12-19 21:37:02.931751: Pseudo dice [0.7394]
2024-12-19 21:37:02.932789: Epoch time: 438.15 s
2024-12-19 21:37:04.359286: 
2024-12-19 21:37:04.360586: Epoch 133
2024-12-19 21:37:04.361563: Current learning rate: 0.00141
2024-12-19 21:45:00.610335: Validation loss did not improve from -0.51593. Patience: 53/50
2024-12-19 21:45:00.611316: train_loss -0.7725
2024-12-19 21:45:00.612134: val_loss -0.4998
2024-12-19 21:45:00.612867: Pseudo dice [0.7487]
2024-12-19 21:45:00.613573: Epoch time: 476.25 s
2024-12-19 21:45:02.071130: 
2024-12-19 21:45:02.072542: Epoch 134
2024-12-19 21:45:02.073417: Current learning rate: 0.00133
2024-12-19 21:52:48.606467: Validation loss improved from -0.51593 to -0.52194! Patience: 53/50
2024-12-19 21:52:48.621767: train_loss -0.776
2024-12-19 21:52:48.623875: val_loss -0.5219
2024-12-19 21:52:48.625227: Pseudo dice [0.7503]
2024-12-19 21:52:48.626739: Epoch time: 466.54 s
2024-12-19 21:52:49.031878: Yayy! New best EMA pseudo Dice: 0.7372
2024-12-19 21:52:51.195856: 
2024-12-19 21:52:51.197608: Epoch 135
2024-12-19 21:52:51.198965: Current learning rate: 0.00126
2024-12-19 22:00:35.096882: Validation loss did not improve from -0.52194. Patience: 1/50
2024-12-19 22:00:35.097922: train_loss -0.775
2024-12-19 22:00:35.099342: val_loss -0.4809
2024-12-19 22:00:35.100306: Pseudo dice [0.7349]
2024-12-19 22:00:35.101072: Epoch time: 463.9 s
2024-12-19 22:00:36.705714: 
2024-12-19 22:00:36.707262: Epoch 136
2024-12-19 22:00:36.708414: Current learning rate: 0.00118
2024-12-19 22:08:29.051054: Validation loss did not improve from -0.52194. Patience: 2/50
2024-12-19 22:08:29.052329: train_loss -0.7761
2024-12-19 22:08:29.053986: val_loss -0.5078
2024-12-19 22:08:29.055487: Pseudo dice [0.7415]
2024-12-19 22:08:29.057950: Epoch time: 472.35 s
2024-12-19 22:08:29.059253: Yayy! New best EMA pseudo Dice: 0.7375
2024-12-19 22:08:31.011416: 
2024-12-19 22:08:31.012826: Epoch 137
2024-12-19 22:08:31.013681: Current learning rate: 0.00111
2024-12-19 22:16:02.629197: Validation loss did not improve from -0.52194. Patience: 3/50
2024-12-19 22:16:02.630199: train_loss -0.7773
2024-12-19 22:16:02.631066: val_loss -0.5096
2024-12-19 22:16:02.631757: Pseudo dice [0.7393]
2024-12-19 22:16:02.632481: Epoch time: 451.62 s
2024-12-19 22:16:02.633105: Yayy! New best EMA pseudo Dice: 0.7376
2024-12-19 22:16:04.607468: 
2024-12-19 22:16:04.609301: Epoch 138
2024-12-19 22:16:04.610655: Current learning rate: 0.00103
2024-12-19 22:23:25.624993: Validation loss did not improve from -0.52194. Patience: 4/50
2024-12-19 22:23:25.626094: train_loss -0.7751
2024-12-19 22:23:25.627039: val_loss -0.4826
2024-12-19 22:23:25.628119: Pseudo dice [0.7373]
2024-12-19 22:23:25.629101: Epoch time: 441.02 s
2024-12-19 22:23:27.122550: 
2024-12-19 22:23:27.123747: Epoch 139
2024-12-19 22:23:27.124546: Current learning rate: 0.00095
2024-12-19 22:30:58.520753: Validation loss did not improve from -0.52194. Patience: 5/50
2024-12-19 22:30:58.522131: train_loss -0.7757
2024-12-19 22:30:58.524041: val_loss -0.5063
2024-12-19 22:30:58.525515: Pseudo dice [0.7434]
2024-12-19 22:30:58.527164: Epoch time: 451.4 s
2024-12-19 22:30:58.986965: Yayy! New best EMA pseudo Dice: 0.7382
2024-12-19 22:31:00.967351: 
2024-12-19 22:31:00.968659: Epoch 140
2024-12-19 22:31:00.969477: Current learning rate: 0.00087
2024-12-19 22:38:57.205554: Validation loss did not improve from -0.52194. Patience: 6/50
2024-12-19 22:38:57.206620: train_loss -0.7748
2024-12-19 22:38:57.207639: val_loss -0.4849
2024-12-19 22:38:57.208544: Pseudo dice [0.7393]
2024-12-19 22:38:57.209558: Epoch time: 476.24 s
2024-12-19 22:38:57.210489: Yayy! New best EMA pseudo Dice: 0.7383
2024-12-19 22:38:59.170840: 
2024-12-19 22:38:59.172363: Epoch 141
2024-12-19 22:38:59.173456: Current learning rate: 0.00079
2024-12-19 22:47:06.543892: Validation loss did not improve from -0.52194. Patience: 7/50
2024-12-19 22:47:06.544574: train_loss -0.7776
2024-12-19 22:47:06.545538: val_loss -0.4835
2024-12-19 22:47:06.546455: Pseudo dice [0.7355]
2024-12-19 22:47:06.547470: Epoch time: 487.38 s
2024-12-19 22:47:08.144373: 
2024-12-19 22:47:08.145430: Epoch 142
2024-12-19 22:47:08.146363: Current learning rate: 0.00071
2024-12-19 22:55:09.058828: Validation loss did not improve from -0.52194. Patience: 8/50
2024-12-19 22:55:09.062215: train_loss -0.7753
2024-12-19 22:55:09.064822: val_loss -0.507
2024-12-19 22:55:09.065715: Pseudo dice [0.7508]
2024-12-19 22:55:09.066898: Epoch time: 480.92 s
2024-12-19 22:55:09.067873: Yayy! New best EMA pseudo Dice: 0.7393
2024-12-19 22:55:11.088874: 
2024-12-19 22:55:11.090414: Epoch 143
2024-12-19 22:55:11.091648: Current learning rate: 0.00063
2024-12-19 23:03:11.799034: Validation loss did not improve from -0.52194. Patience: 9/50
2024-12-19 23:03:11.800128: train_loss -0.7803
2024-12-19 23:03:11.800786: val_loss -0.4984
2024-12-19 23:03:11.801978: Pseudo dice [0.7422]
2024-12-19 23:03:11.802844: Epoch time: 480.71 s
2024-12-19 23:03:11.803568: Yayy! New best EMA pseudo Dice: 0.7396
2024-12-19 23:03:13.808597: 
2024-12-19 23:03:13.810328: Epoch 144
2024-12-19 23:03:13.811489: Current learning rate: 0.00055
2024-12-19 23:11:07.111412: Validation loss did not improve from -0.52194. Patience: 10/50
2024-12-19 23:11:07.112492: train_loss -0.7787
2024-12-19 23:11:07.113175: val_loss -0.4698
2024-12-19 23:11:07.113786: Pseudo dice [0.7367]
2024-12-19 23:11:07.114389: Epoch time: 473.31 s
2024-12-19 23:11:09.016612: 
2024-12-19 23:11:09.017793: Epoch 145
2024-12-19 23:11:09.018799: Current learning rate: 0.00047
2024-12-19 23:18:38.423383: Validation loss did not improve from -0.52194. Patience: 11/50
2024-12-19 23:18:38.424489: train_loss -0.778
2024-12-19 23:18:38.425812: val_loss -0.4743
2024-12-19 23:18:38.427114: Pseudo dice [0.7264]
2024-12-19 23:18:38.427981: Epoch time: 449.41 s
2024-12-19 23:18:39.851930: 
2024-12-19 23:18:39.852757: Epoch 146
2024-12-19 23:18:39.853737: Current learning rate: 0.00038
2024-12-19 23:26:18.594740: Validation loss did not improve from -0.52194. Patience: 12/50
2024-12-19 23:26:18.596614: train_loss -0.7803
2024-12-19 23:26:18.597707: val_loss -0.4913
2024-12-19 23:26:18.598563: Pseudo dice [0.7428]
2024-12-19 23:26:18.599842: Epoch time: 458.75 s
2024-12-19 23:26:20.078696: 
2024-12-19 23:26:20.079886: Epoch 147
2024-12-19 23:26:20.080749: Current learning rate: 0.0003
2024-12-19 23:33:33.745752: Validation loss did not improve from -0.52194. Patience: 13/50
2024-12-19 23:33:33.747018: train_loss -0.7822
2024-12-19 23:33:33.748322: val_loss -0.4907
2024-12-19 23:33:33.749167: Pseudo dice [0.7426]
2024-12-19 23:33:33.750054: Epoch time: 433.67 s
2024-12-19 23:33:35.223470: 
2024-12-19 23:33:35.225353: Epoch 148
2024-12-19 23:33:35.226703: Current learning rate: 0.00021
2024-12-19 23:40:54.363763: Validation loss did not improve from -0.52194. Patience: 14/50
2024-12-19 23:40:54.364604: train_loss -0.7786
2024-12-19 23:40:54.365404: val_loss -0.5056
2024-12-19 23:40:54.366102: Pseudo dice [0.7523]
2024-12-19 23:40:54.366844: Epoch time: 439.14 s
2024-12-19 23:40:54.367550: Yayy! New best EMA pseudo Dice: 0.7402
2024-12-19 23:40:56.378359: 
2024-12-19 23:40:56.379735: Epoch 149
2024-12-19 23:40:56.380909: Current learning rate: 0.00011
2024-12-19 23:48:29.451513: Validation loss did not improve from -0.52194. Patience: 15/50
2024-12-19 23:48:29.452613: train_loss -0.7799
2024-12-19 23:48:29.453367: val_loss -0.4824
2024-12-19 23:48:29.454326: Pseudo dice [0.7355]
2024-12-19 23:48:29.455196: Epoch time: 453.08 s
2024-12-19 23:48:31.708287: Training done.
2024-12-19 23:48:31.832302: Using splits from existing split file: /home/gridsan/nchutisilp/datasets/nnUNet_Datasets/nnUNet_preprocessed/Dataset308_Sohee_Calcium_OCT_CrossValidation/splits_final_80.json
2024-12-19 23:48:31.833831: The split file contains 5 splits.
2024-12-19 23:48:31.834441: Desired fold for training: 1
2024-12-19 23:48:31.834994: This split has 6 training and 4 validation cases.
2024-12-19 23:48:31.835798: predicting 106-002
2024-12-19 23:48:31.855804: 106-002, shape torch.Size([1, 539, 498, 498]), rank 0
2024-12-19 23:51:41.071848: predicting 401-004
2024-12-19 23:51:41.091043: 401-004, shape torch.Size([1, 375, 498, 498]), rank 0
2024-12-19 23:54:05.934484: predicting 704-003
2024-12-19 23:54:05.951363: 704-003, shape torch.Size([1, 375, 498, 498]), rank 0
2024-12-19 23:56:04.369804: predicting 706-005
2024-12-19 23:56:04.382914: 706-005, shape torch.Size([1, 375, 498, 498]), rank 0
2024-12-19 23:58:26.420013: Validation complete
2024-12-19 23:58:26.421218: Mean Validation Dice:  0.7215700686377957

############################
INFO: You are using the old nnU-Net default plans. We have updated our recommendations. Please consider using those instead! Read more here: https://github.com/MIC-DKFZ/nnUNet/blob/master/documentation/resenc_presets.md
############################

Using device: cuda:0

#######################################################################
Please cite the following paper when using nnU-Net:
Isensee, F., Jaeger, P. F., Kohl, S. A., Petersen, J., & Maier-Hein, K. H. (2021). nnU-Net: a self-configuring method for deep learning-based biomedical image segmentation. Nature methods, 18(2), 203-211.
#######################################################################

2024-12-19 23:58:34.180562: Using torch.compile...

############################
INFO: You are using the old nnU-Net default plans. We have updated our recommendations. Please consider using those instead! Read more here: https://github.com/MIC-DKFZ/nnUNet/blob/master/documentation/resenc_presets.md
############################

Using device: cuda:0

#######################################################################
Please cite the following paper when using nnU-Net:
Isensee, F., Jaeger, P. F., Kohl, S. A., Petersen, J., & Maier-Hein, K. H. (2021). nnU-Net: a self-configuring method for deep learning-based biomedical image segmentation. Nature methods, 18(2), 203-211.
#######################################################################

2024-12-19 23:58:34.182907: Using torch.compile...
/home/gridsan/nchutisilp/.local/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn("The verbose parameter is deprecated. Please use get_last_lr() "
/home/gridsan/nchutisilp/.local/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn("The verbose parameter is deprecated. Please use get_last_lr() "
################### Loading pretrained weights from file  /home/gridsan/nchutisilp/projects/OpenAI-CLIP/clip_pretrained_nnUNet.pt ###################
Below is the list of overlapping blocks in pretrained model and nnUNet architecture:
encoder.stages.0.0.convs.0.conv.weight shape torch.Size([32, 1, 3, 3, 3])
encoder.stages.0.0.convs.0.conv.bias shape torch.Size([32])
encoder.stages.0.0.convs.0.norm.weight shape torch.Size([32])
encoder.stages.0.0.convs.0.norm.bias shape torch.Size([32])
encoder.stages.0.0.convs.0.all_modules.0.weight shape torch.Size([32, 1, 3, 3, 3])
encoder.stages.0.0.convs.0.all_modules.0.bias shape torch.Size([32])
encoder.stages.0.0.convs.0.all_modules.1.weight shape torch.Size([32])
encoder.stages.0.0.convs.0.all_modules.1.bias shape torch.Size([32])
encoder.stages.0.0.convs.1.conv.weight shape torch.Size([32, 32, 3, 3, 3])
encoder.stages.0.0.convs.1.conv.bias shape torch.Size([32])
encoder.stages.0.0.convs.1.norm.weight shape torch.Size([32])
encoder.stages.0.0.convs.1.norm.bias shape torch.Size([32])
encoder.stages.0.0.convs.1.all_modules.0.weight shape torch.Size([32, 32, 3, 3, 3])
encoder.stages.0.0.convs.1.all_modules.0.bias shape torch.Size([32])
encoder.stages.0.0.convs.1.all_modules.1.weight shape torch.Size([32])
encoder.stages.0.0.convs.1.all_modules.1.bias shape torch.Size([32])
encoder.stages.1.0.convs.0.conv.weight shape torch.Size([64, 32, 3, 3, 3])
encoder.stages.1.0.convs.0.conv.bias shape torch.Size([64])
encoder.stages.1.0.convs.0.norm.weight shape torch.Size([64])
encoder.stages.1.0.convs.0.norm.bias shape torch.Size([64])
encoder.stages.1.0.convs.0.all_modules.0.weight shape torch.Size([64, 32, 3, 3, 3])
encoder.stages.1.0.convs.0.all_modules.0.bias shape torch.Size([64])
encoder.stages.1.0.convs.0.all_modules.1.weight shape torch.Size([64])
encoder.stages.1.0.convs.0.all_modules.1.bias shape torch.Size([64])
encoder.stages.1.0.convs.1.conv.weight shape torch.Size([64, 64, 3, 3, 3])
encoder.stages.1.0.convs.1.conv.bias shape torch.Size([64])
encoder.stages.1.0.convs.1.norm.weight shape torch.Size([64])
encoder.stages.1.0.convs.1.norm.bias shape torch.Size([64])
encoder.stages.1.0.convs.1.all_modules.0.weight shape torch.Size([64, 64, 3, 3, 3])
encoder.stages.1.0.convs.1.all_modules.0.bias shape torch.Size([64])
encoder.stages.1.0.convs.1.all_modules.1.weight shape torch.Size([64])
encoder.stages.1.0.convs.1.all_modules.1.bias shape torch.Size([64])
encoder.stages.2.0.convs.0.conv.weight shape torch.Size([128, 64, 3, 3, 3])
encoder.stages.2.0.convs.0.conv.bias shape torch.Size([128])
encoder.stages.2.0.convs.0.norm.weight shape torch.Size([128])
encoder.stages.2.0.convs.0.norm.bias shape torch.Size([128])
encoder.stages.2.0.convs.0.all_modules.0.weight shape torch.Size([128, 64, 3, 3, 3])
encoder.stages.2.0.convs.0.all_modules.0.bias shape torch.Size([128])
encoder.stages.2.0.convs.0.all_modules.1.weight shape torch.Size([128])
encoder.stages.2.0.convs.0.all_modules.1.bias shape torch.Size([128])
encoder.stages.2.0.convs.1.conv.weight shape torch.Size([128, 128, 3, 3, 3])
encoder.stages.2.0.convs.1.conv.bias shape torch.Size([128])
encoder.stages.2.0.convs.1.norm.weight shape torch.Size([128])
encoder.stages.2.0.convs.1.norm.bias shape torch.Size([128])
encoder.stages.2.0.convs.1.all_modules.0.weight shape torch.Size([128, 128, 3, 3, 3])
encoder.stages.2.0.convs.1.all_modules.0.bias shape torch.Size([128])
encoder.stages.2.0.convs.1.all_modules.1.weight shape torch.Size([128])
encoder.stages.2.0.convs.1.all_modules.1.bias shape torch.Size([128])
encoder.stages.3.0.convs.0.conv.weight shape torch.Size([256, 128, 3, 3, 3])
encoder.stages.3.0.convs.0.conv.bias shape torch.Size([256])
encoder.stages.3.0.convs.0.norm.weight shape torch.Size([256])
encoder.stages.3.0.convs.0.norm.bias shape torch.Size([256])
encoder.stages.3.0.convs.0.all_modules.0.weight shape torch.Size([256, 128, 3, 3, 3])
encoder.stages.3.0.convs.0.all_modules.0.bias shape torch.Size([256])
encoder.stages.3.0.convs.0.all_modules.1.weight shape torch.Size([256])
encoder.stages.3.0.convs.0.all_modules.1.bias shape torch.Size([256])
encoder.stages.3.0.convs.1.conv.weight shape torch.Size([256, 256, 3, 3, 3])
encoder.stages.3.0.convs.1.conv.bias shape torch.Size([256])
encoder.stages.3.0.convs.1.norm.weight shape torch.Size([256])
encoder.stages.3.0.convs.1.norm.bias shape torch.Size([256])
encoder.stages.3.0.convs.1.all_modules.0.weight shape torch.Size([256, 256, 3, 3, 3])
encoder.stages.3.0.convs.1.all_modules.0.bias shape torch.Size([256])
encoder.stages.3.0.convs.1.all_modules.1.weight shape torch.Size([256])
encoder.stages.3.0.convs.1.all_modules.1.bias shape torch.Size([256])
encoder.stages.4.0.convs.0.conv.weight shape torch.Size([320, 256, 3, 3, 3])
encoder.stages.4.0.convs.0.conv.bias shape torch.Size([320])
encoder.stages.4.0.convs.0.norm.weight shape torch.Size([320])
encoder.stages.4.0.convs.0.norm.bias shape torch.Size([320])
encoder.stages.4.0.convs.0.all_modules.0.weight shape torch.Size([320, 256, 3, 3, 3])
encoder.stages.4.0.convs.0.all_modules.0.bias shape torch.Size([320])
encoder.stages.4.0.convs.0.all_modules.1.weight shape torch.Size([320])
encoder.stages.4.0.convs.0.all_modules.1.bias shape torch.Size([320])
encoder.stages.4.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.4.0.convs.1.conv.bias shape torch.Size([320])
encoder.stages.4.0.convs.1.norm.weight shape torch.Size([320])
encoder.stages.4.0.convs.1.norm.bias shape torch.Size([320])
encoder.stages.4.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.4.0.convs.1.all_modules.0.bias shape torch.Size([320])
encoder.stages.4.0.convs.1.all_modules.1.weight shape torch.Size([320])
encoder.stages.4.0.convs.1.all_modules.1.bias shape torch.Size([320])
encoder.stages.5.0.convs.0.conv.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.5.0.convs.0.conv.bias shape torch.Size([320])
encoder.stages.5.0.convs.0.norm.weight shape torch.Size([320])
encoder.stages.5.0.convs.0.norm.bias shape torch.Size([320])
encoder.stages.5.0.convs.0.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.5.0.convs.0.all_modules.0.bias shape torch.Size([320])
encoder.stages.5.0.convs.0.all_modules.1.weight shape torch.Size([320])
encoder.stages.5.0.convs.0.all_modules.1.bias shape torch.Size([320])
encoder.stages.5.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.5.0.convs.1.conv.bias shape torch.Size([320])
encoder.stages.5.0.convs.1.norm.weight shape torch.Size([320])
encoder.stages.5.0.convs.1.norm.bias shape torch.Size([320])
encoder.stages.5.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.5.0.convs.1.all_modules.0.bias shape torch.Size([320])
encoder.stages.5.0.convs.1.all_modules.1.weight shape torch.Size([320])
encoder.stages.5.0.convs.1.all_modules.1.bias shape torch.Size([320])
decoder.encoder.stages.0.0.convs.0.conv.weight shape torch.Size([32, 1, 3, 3, 3])
decoder.encoder.stages.0.0.convs.0.conv.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.norm.weight shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.norm.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.all_modules.0.weight shape torch.Size([32, 1, 3, 3, 3])
decoder.encoder.stages.0.0.convs.0.all_modules.0.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.all_modules.1.weight shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.all_modules.1.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.conv.weight shape torch.Size([32, 32, 3, 3, 3])
decoder.encoder.stages.0.0.convs.1.conv.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.norm.weight shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.norm.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.all_modules.0.weight shape torch.Size([32, 32, 3, 3, 3])
decoder.encoder.stages.0.0.convs.1.all_modules.0.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.all_modules.1.weight shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.all_modules.1.bias shape torch.Size([32])
decoder.encoder.stages.1.0.convs.0.conv.weight shape torch.Size([64, 32, 3, 3, 3])
decoder.encoder.stages.1.0.convs.0.conv.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.norm.weight shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.norm.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.all_modules.0.weight shape torch.Size([64, 32, 3, 3, 3])
decoder.encoder.stages.1.0.convs.0.all_modules.0.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.all_modules.1.weight shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.all_modules.1.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.conv.weight shape torch.Size([64, 64, 3, 3, 3])
decoder.encoder.stages.1.0.convs.1.conv.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.norm.weight shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.norm.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.all_modules.0.weight shape torch.Size([64, 64, 3, 3, 3])
decoder.encoder.stages.1.0.convs.1.all_modules.0.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.all_modules.1.weight shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.all_modules.1.bias shape torch.Size([64])
decoder.encoder.stages.2.0.convs.0.conv.weight shape torch.Size([128, 64, 3, 3, 3])
decoder.encoder.stages.2.0.convs.0.conv.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.norm.weight shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.norm.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.all_modules.0.weight shape torch.Size([128, 64, 3, 3, 3])
decoder.encoder.stages.2.0.convs.0.all_modules.0.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.all_modules.1.weight shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.all_modules.1.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.conv.weight shape torch.Size([128, 128, 3, 3, 3])
decoder.encoder.stages.2.0.convs.1.conv.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.norm.weight shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.norm.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.all_modules.0.weight shape torch.Size([128, 128, 3, 3, 3])
decoder.encoder.stages.2.0.convs.1.all_modules.0.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.all_modules.1.weight shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.all_modules.1.bias shape torch.Size([128])
decoder.encoder.stages.3.0.convs.0.conv.weight shape torch.Size([256, 128, 3, 3, 3])
decoder.encoder.stages.3.0.convs.0.conv.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.norm.weight shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.norm.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.all_modules.0.weight shape torch.Size([256, 128, 3, 3, 3])
decoder.encoder.stages.3.0.convs.0.all_modules.0.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.all_modules.1.weight shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.all_modules.1.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.conv.weight shape torch.Size([256, 256, 3, 3, 3])
decoder.encoder.stages.3.0.convs.1.conv.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.norm.weight shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.norm.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.all_modules.0.weight shape torch.Size([256, 256, 3, 3, 3])
decoder.encoder.stages.3.0.convs.1.all_modules.0.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.all_modules.1.weight shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.all_modules.1.bias shape torch.Size([256])
decoder.encoder.stages.4.0.convs.0.conv.weight shape torch.Size([320, 256, 3, 3, 3])
decoder.encoder.stages.4.0.convs.0.conv.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.norm.weight shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.norm.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.all_modules.0.weight shape torch.Size([320, 256, 3, 3, 3])
decoder.encoder.stages.4.0.convs.0.all_modules.0.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.all_modules.1.weight shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.all_modules.1.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.4.0.convs.1.conv.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.norm.weight shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.norm.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.4.0.convs.1.all_modules.0.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.all_modules.1.weight shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.all_modules.1.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.conv.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.5.0.convs.0.conv.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.norm.weight shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.norm.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.5.0.convs.0.all_modules.0.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.all_modules.1.weight shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.all_modules.1.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.5.0.convs.1.conv.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.norm.weight shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.norm.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.5.0.convs.1.all_modules.0.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.all_modules.1.weight shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.all_modules.1.bias shape torch.Size([320])
decoder.stages.0.convs.0.conv.weight shape torch.Size([320, 640, 3, 3, 3])
decoder.stages.0.convs.0.conv.bias shape torch.Size([320])
decoder.stages.0.convs.0.norm.weight shape torch.Size([320])
decoder.stages.0.convs.0.norm.bias shape torch.Size([320])
decoder.stages.0.convs.0.all_modules.0.weight shape torch.Size([320, 640, 3, 3, 3])
decoder.stages.0.convs.0.all_modules.0.bias shape torch.Size([320])
decoder.stages.0.convs.0.all_modules.1.weight shape torch.Size([320])
decoder.stages.0.convs.0.all_modules.1.bias shape torch.Size([320])
decoder.stages.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.stages.0.convs.1.conv.bias shape torch.Size([320])
decoder.stages.0.convs.1.norm.weight shape torch.Size([320])
decoder.stages.0.convs.1.norm.bias shape torch.Size([320])
decoder.stages.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.stages.0.convs.1.all_modules.0.bias shape torch.Size([320])
decoder.stages.0.convs.1.all_modules.1.weight shape torch.Size([320])
decoder.stages.0.convs.1.all_modules.1.bias shape torch.Size([320])
decoder.stages.1.convs.0.conv.weight shape torch.Size([256, 512, 3, 3, 3])
decoder.stages.1.convs.0.conv.bias shape torch.Size([256])
decoder.stages.1.convs.0.norm.weight shape torch.Size([256])
decoder.stages.1.convs.0.norm.bias shape torch.Size([256])
decoder.stages.1.convs.0.all_modules.0.weight shape torch.Size([256, 512, 3, 3, 3])
decoder.stages.1.convs.0.all_modules.0.bias shape torch.Size([256])
decoder.stages.1.convs.0.all_modules.1.weight shape torch.Size([256])
decoder.stages.1.convs.0.all_modules.1.bias shape torch.Size([256])
decoder.stages.1.convs.1.conv.weight shape torch.Size([256, 256, 3, 3, 3])
decoder.stages.1.convs.1.conv.bias shape torch.Size([256])
decoder.stages.1.convs.1.norm.weight shape torch.Size([256])
decoder.stages.1.convs.1.norm.bias shape torch.Size([256])
decoder.stages.1.convs.1.all_modules.0.weight shape torch.Size([256, 256, 3, 3, 3])
decoder.stages.1.convs.1.all_modules.0.bias shape torch.Size([256])
decoder.stages.1.convs.1.all_modules.1.weight shape torch.Size([256])
decoder.stages.1.convs.1.all_modules.1.bias shape torch.Size([256])
decoder.stages.2.convs.0.conv.weight shape torch.Size([128, 256, 3, 3, 3])
decoder.stages.2.convs.0.conv.bias shape torch.Size([128])
decoder.stages.2.convs.0.norm.weight shape torch.Size([128])
decoder.stages.2.convs.0.norm.bias shape torch.Size([128])
decoder.stages.2.convs.0.all_modules.0.weight shape torch.Size([128, 256, 3, 3, 3])
decoder.stages.2.convs.0.all_modules.0.bias shape torch.Size([128])
decoder.stages.2.convs.0.all_modules.1.weight shape torch.Size([128])
decoder.stages.2.convs.0.all_modules.1.bias shape torch.Size([128])
decoder.stages.2.convs.1.conv.weight shape torch.Size([128, 128, 3, 3, 3])
decoder.stages.2.convs.1.conv.bias shape torch.Size([128])
decoder.stages.2.convs.1.norm.weight shape torch.Size([128])
decoder.stages.2.convs.1.norm.bias shape torch.Size([128])
decoder.stages.2.convs.1.all_modules.0.weight shape torch.Size([128, 128, 3, 3, 3])
decoder.stages.2.convs.1.all_modules.0.bias shape torch.Size([128])
decoder.stages.2.convs.1.all_modules.1.weight shape torch.Size([128])
decoder.stages.2.convs.1.all_modules.1.bias shape torch.Size([128])
decoder.stages.3.convs.0.conv.weight shape torch.Size([64, 128, 3, 3, 3])
decoder.stages.3.convs.0.conv.bias shape torch.Size([64])
decoder.stages.3.convs.0.norm.weight shape torch.Size([64])
decoder.stages.3.convs.0.norm.bias shape torch.Size([64])
decoder.stages.3.convs.0.all_modules.0.weight shape torch.Size([64, 128, 3, 3, 3])
decoder.stages.3.convs.0.all_modules.0.bias shape torch.Size([64])
decoder.stages.3.convs.0.all_modules.1.weight shape torch.Size([64])
decoder.stages.3.convs.0.all_modules.1.bias shape torch.Size([64])
decoder.stages.3.convs.1.conv.weight shape torch.Size([64, 64, 3, 3, 3])
decoder.stages.3.convs.1.conv.bias shape torch.Size([64])
decoder.stages.3.convs.1.norm.weight shape torch.Size([64])
decoder.stages.3.convs.1.norm.bias shape torch.Size([64])
decoder.stages.3.convs.1.all_modules.0.weight shape torch.Size([64, 64, 3, 3, 3])
decoder.stages.3.convs.1.all_modules.0.bias shape torch.Size([64])
decoder.stages.3.convs.1.all_modules.1.weight shape torch.Size([64])
decoder.stages.3.convs.1.all_modules.1.bias shape torch.Size([64])
decoder.stages.4.convs.0.conv.weight shape torch.Size([32, 64, 3, 3, 3])
decoder.stages.4.convs.0.conv.bias shape torch.Size([32])
decoder.stages.4.convs.0.norm.weight shape torch.Size([32])
decoder.stages.4.convs.0.norm.bias shape torch.Size([32])
decoder.stages.4.convs.0.all_modules.0.weight shape torch.Size([32, 64, 3, 3, 3])
decoder.stages.4.convs.0.all_modules.0.bias shape torch.Size([32])
decoder.stages.4.convs.0.all_modules.1.weight shape torch.Size([32])
decoder.stages.4.convs.0.all_modules.1.bias shape torch.Size([32])
decoder.stages.4.convs.1.conv.weight shape torch.Size([32, 32, 3, 3, 3])
decoder.stages.4.convs.1.conv.bias shape torch.Size([32])
decoder.stages.4.convs.1.norm.weight shape torch.Size([32])
decoder.stages.4.convs.1.norm.bias shape torch.Size([32])
decoder.stages.4.convs.1.all_modules.0.weight shape torch.Size([32, 32, 3, 3, 3])
decoder.stages.4.convs.1.all_modules.0.bias shape torch.Size([32])
decoder.stages.4.convs.1.all_modules.1.weight shape torch.Size([32])
decoder.stages.4.convs.1.all_modules.1.bias shape torch.Size([32])
decoder.transpconvs.0.weight shape torch.Size([320, 320, 1, 2, 2])
decoder.transpconvs.0.bias shape torch.Size([320])
decoder.transpconvs.1.weight shape torch.Size([320, 256, 2, 2, 2])
decoder.transpconvs.1.bias shape torch.Size([256])
decoder.transpconvs.2.weight shape torch.Size([256, 128, 2, 2, 2])
decoder.transpconvs.2.bias shape torch.Size([128])
decoder.transpconvs.3.weight shape torch.Size([128, 64, 2, 2, 2])
decoder.transpconvs.3.bias shape torch.Size([64])
decoder.transpconvs.4.weight shape torch.Size([64, 32, 2, 2, 2])
decoder.transpconvs.4.bias shape torch.Size([32])
################### Done ###################
2024-12-19 23:58:56.296106: do_dummy_2d_data_aug: True
2024-12-19 23:58:56.297478: Using splits from existing split file: /home/gridsan/nchutisilp/datasets/nnUNet_Datasets/nnUNet_preprocessed/Dataset308_Sohee_Calcium_OCT_CrossValidation/splits_final_80.json
2024-12-19 23:58:56.299006: The split file contains 5 splits.
2024-12-19 23:58:56.299698: Desired fold for training: 3
2024-12-19 23:58:56.300483: This split has 6 training and 3 validation cases.
################### Loading pretrained weights from file  /home/gridsan/nchutisilp/projects/OpenAI-CLIP/clip_pretrained_nnUNet.pt ###################
Below is the list of overlapping blocks in pretrained model and nnUNet architecture:
encoder.stages.0.0.convs.0.conv.weight shape torch.Size([32, 1, 3, 3, 3])
encoder.stages.0.0.convs.0.conv.bias shape torch.Size([32])
encoder.stages.0.0.convs.0.norm.weight shape torch.Size([32])
encoder.stages.0.0.convs.0.norm.bias shape torch.Size([32])
encoder.stages.0.0.convs.0.all_modules.0.weight shape torch.Size([32, 1, 3, 3, 3])
encoder.stages.0.0.convs.0.all_modules.0.bias shape torch.Size([32])
encoder.stages.0.0.convs.0.all_modules.1.weight shape torch.Size([32])
encoder.stages.0.0.convs.0.all_modules.1.bias shape torch.Size([32])
encoder.stages.0.0.convs.1.conv.weight shape torch.Size([32, 32, 3, 3, 3])
encoder.stages.0.0.convs.1.conv.bias shape torch.Size([32])
encoder.stages.0.0.convs.1.norm.weight shape torch.Size([32])
encoder.stages.0.0.convs.1.norm.bias shape torch.Size([32])
encoder.stages.0.0.convs.1.all_modules.0.weight shape torch.Size([32, 32, 3, 3, 3])
encoder.stages.0.0.convs.1.all_modules.0.bias shape torch.Size([32])
encoder.stages.0.0.convs.1.all_modules.1.weight shape torch.Size([32])
encoder.stages.0.0.convs.1.all_modules.1.bias shape torch.Size([32])
encoder.stages.1.0.convs.0.conv.weight shape torch.Size([64, 32, 3, 3, 3])
encoder.stages.1.0.convs.0.conv.bias shape torch.Size([64])
encoder.stages.1.0.convs.0.norm.weight shape torch.Size([64])
encoder.stages.1.0.convs.0.norm.bias shape torch.Size([64])
encoder.stages.1.0.convs.0.all_modules.0.weight shape torch.Size([64, 32, 3, 3, 3])
encoder.stages.1.0.convs.0.all_modules.0.bias shape torch.Size([64])
encoder.stages.1.0.convs.0.all_modules.1.weight shape torch.Size([64])
encoder.stages.1.0.convs.0.all_modules.1.bias shape torch.Size([64])
encoder.stages.1.0.convs.1.conv.weight shape torch.Size([64, 64, 3, 3, 3])
encoder.stages.1.0.convs.1.conv.bias shape torch.Size([64])
encoder.stages.1.0.convs.1.norm.weight shape torch.Size([64])
encoder.stages.1.0.convs.1.norm.bias shape torch.Size([64])
encoder.stages.1.0.convs.1.all_modules.0.weight shape torch.Size([64, 64, 3, 3, 3])
encoder.stages.1.0.convs.1.all_modules.0.bias shape torch.Size([64])
encoder.stages.1.0.convs.1.all_modules.1.weight shape torch.Size([64])
encoder.stages.1.0.convs.1.all_modules.1.bias shape torch.Size([64])
encoder.stages.2.0.convs.0.conv.weight shape torch.Size([128, 64, 3, 3, 3])
encoder.stages.2.0.convs.0.conv.bias shape torch.Size([128])
encoder.stages.2.0.convs.0.norm.weight shape torch.Size([128])
encoder.stages.2.0.convs.0.norm.bias shape torch.Size([128])
encoder.stages.2.0.convs.0.all_modules.0.weight shape torch.Size([128, 64, 3, 3, 3])
encoder.stages.2.0.convs.0.all_modules.0.bias shape torch.Size([128])
encoder.stages.2.0.convs.0.all_modules.1.weight shape torch.Size([128])
encoder.stages.2.0.convs.0.all_modules.1.bias shape torch.Size([128])
encoder.stages.2.0.convs.1.conv.weight shape torch.Size([128, 128, 3, 3, 3])
encoder.stages.2.0.convs.1.conv.bias shape torch.Size([128])
encoder.stages.2.0.convs.1.norm.weight shape torch.Size([128])
encoder.stages.2.0.convs.1.norm.bias shape torch.Size([128])
encoder.stages.2.0.convs.1.all_modules.0.weight shape torch.Size([128, 128, 3, 3, 3])
encoder.stages.2.0.convs.1.all_modules.0.bias shape torch.Size([128])
encoder.stages.2.0.convs.1.all_modules.1.weight shape torch.Size([128])
encoder.stages.2.0.convs.1.all_modules.1.bias shape torch.Size([128])
encoder.stages.3.0.convs.0.conv.weight shape torch.Size([256, 128, 3, 3, 3])
encoder.stages.3.0.convs.0.conv.bias shape torch.Size([256])
encoder.stages.3.0.convs.0.norm.weight shape torch.Size([256])
encoder.stages.3.0.convs.0.norm.bias shape torch.Size([256])
encoder.stages.3.0.convs.0.all_modules.0.weight shape torch.Size([256, 128, 3, 3, 3])
encoder.stages.3.0.convs.0.all_modules.0.bias shape torch.Size([256])
encoder.stages.3.0.convs.0.all_modules.1.weight shape torch.Size([256])
encoder.stages.3.0.convs.0.all_modules.1.bias shape torch.Size([256])
encoder.stages.3.0.convs.1.conv.weight shape torch.Size([256, 256, 3, 3, 3])
encoder.stages.3.0.convs.1.conv.bias shape torch.Size([256])
encoder.stages.3.0.convs.1.norm.weight shape torch.Size([256])
encoder.stages.3.0.convs.1.norm.bias shape torch.Size([256])
encoder.stages.3.0.convs.1.all_modules.0.weight shape torch.Size([256, 256, 3, 3, 3])
encoder.stages.3.0.convs.1.all_modules.0.bias shape torch.Size([256])
encoder.stages.3.0.convs.1.all_modules.1.weight shape torch.Size([256])
encoder.stages.3.0.convs.1.all_modules.1.bias shape torch.Size([256])
encoder.stages.4.0.convs.0.conv.weight shape torch.Size([320, 256, 3, 3, 3])
encoder.stages.4.0.convs.0.conv.bias shape torch.Size([320])
encoder.stages.4.0.convs.0.norm.weight shape torch.Size([320])
encoder.stages.4.0.convs.0.norm.bias shape torch.Size([320])
encoder.stages.4.0.convs.0.all_modules.0.weight shape torch.Size([320, 256, 3, 3, 3])
encoder.stages.4.0.convs.0.all_modules.0.bias shape torch.Size([320])
encoder.stages.4.0.convs.0.all_modules.1.weight shape torch.Size([320])
encoder.stages.4.0.convs.0.all_modules.1.bias shape torch.Size([320])
encoder.stages.4.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.4.0.convs.1.conv.bias shape torch.Size([320])
encoder.stages.4.0.convs.1.norm.weight shape torch.Size([320])
encoder.stages.4.0.convs.1.norm.bias shape torch.Size([320])
encoder.stages.4.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.4.0.convs.1.all_modules.0.bias shape torch.Size([320])
encoder.stages.4.0.convs.1.all_modules.1.weight shape torch.Size([320])
encoder.stages.4.0.convs.1.all_modules.1.bias shape torch.Size([320])
encoder.stages.5.0.convs.0.conv.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.5.0.convs.0.conv.bias shape torch.Size([320])
encoder.stages.5.0.convs.0.norm.weight shape torch.Size([320])
encoder.stages.5.0.convs.0.norm.bias shape torch.Size([320])
encoder.stages.5.0.convs.0.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.5.0.convs.0.all_modules.0.bias shape torch.Size([320])
encoder.stages.5.0.convs.0.all_modules.1.weight shape torch.Size([320])
encoder.stages.5.0.convs.0.all_modules.1.bias shape torch.Size([320])
encoder.stages.5.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.5.0.convs.1.conv.bias shape torch.Size([320])
encoder.stages.5.0.convs.1.norm.weight shape torch.Size([320])
encoder.stages.5.0.convs.1.norm.bias shape torch.Size([320])
encoder.stages.5.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.5.0.convs.1.all_modules.0.bias shape torch.Size([320])
encoder.stages.5.0.convs.1.all_modules.1.weight shape torch.Size([320])
encoder.stages.5.0.convs.1.all_modules.1.bias shape torch.Size([320])
decoder.encoder.stages.0.0.convs.0.conv.weight shape torch.Size([32, 1, 3, 3, 3])
decoder.encoder.stages.0.0.convs.0.conv.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.norm.weight shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.norm.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.all_modules.0.weight shape torch.Size([32, 1, 3, 3, 3])
decoder.encoder.stages.0.0.convs.0.all_modules.0.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.all_modules.1.weight shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.all_modules.1.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.conv.weight shape torch.Size([32, 32, 3, 3, 3])
decoder.encoder.stages.0.0.convs.1.conv.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.norm.weight shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.norm.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.all_modules.0.weight shape torch.Size([32, 32, 3, 3, 3])
decoder.encoder.stages.0.0.convs.1.all_modules.0.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.all_modules.1.weight shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.all_modules.1.bias shape torch.Size([32])
decoder.encoder.stages.1.0.convs.0.conv.weight shape torch.Size([64, 32, 3, 3, 3])
decoder.encoder.stages.1.0.convs.0.conv.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.norm.weight shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.norm.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.all_modules.0.weight shape torch.Size([64, 32, 3, 3, 3])
decoder.encoder.stages.1.0.convs.0.all_modules.0.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.all_modules.1.weight shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.all_modules.1.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.conv.weight shape torch.Size([64, 64, 3, 3, 3])
decoder.encoder.stages.1.0.convs.1.conv.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.norm.weight shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.norm.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.all_modules.0.weight shape torch.Size([64, 64, 3, 3, 3])
decoder.encoder.stages.1.0.convs.1.all_modules.0.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.all_modules.1.weight shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.all_modules.1.bias shape torch.Size([64])
decoder.encoder.stages.2.0.convs.0.conv.weight shape torch.Size([128, 64, 3, 3, 3])
decoder.encoder.stages.2.0.convs.0.conv.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.norm.weight shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.norm.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.all_modules.0.weight shape torch.Size([128, 64, 3, 3, 3])
decoder.encoder.stages.2.0.convs.0.all_modules.0.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.all_modules.1.weight shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.all_modules.1.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.conv.weight shape torch.Size([128, 128, 3, 3, 3])
decoder.encoder.stages.2.0.convs.1.conv.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.norm.weight shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.norm.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.all_modules.0.weight shape torch.Size([128, 128, 3, 3, 3])
decoder.encoder.stages.2.0.convs.1.all_modules.0.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.all_modules.1.weight shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.all_modules.1.bias shape torch.Size([128])
decoder.encoder.stages.3.0.convs.0.conv.weight shape torch.Size([256, 128, 3, 3, 3])
decoder.encoder.stages.3.0.convs.0.conv.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.norm.weight shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.norm.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.all_modules.0.weight shape torch.Size([256, 128, 3, 3, 3])
decoder.encoder.stages.3.0.convs.0.all_modules.0.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.all_modules.1.weight shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.all_modules.1.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.conv.weight shape torch.Size([256, 256, 3, 3, 3])
decoder.encoder.stages.3.0.convs.1.conv.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.norm.weight shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.norm.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.all_modules.0.weight shape torch.Size([256, 256, 3, 3, 3])
decoder.encoder.stages.3.0.convs.1.all_modules.0.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.all_modules.1.weight shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.all_modules.1.bias shape torch.Size([256])
decoder.encoder.stages.4.0.convs.0.conv.weight shape torch.Size([320, 256, 3, 3, 3])
decoder.encoder.stages.4.0.convs.0.conv.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.norm.weight shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.norm.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.all_modules.0.weight shape torch.Size([320, 256, 3, 3, 3])
decoder.encoder.stages.4.0.convs.0.all_modules.0.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.all_modules.1.weight shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.all_modules.1.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.4.0.convs.1.conv.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.norm.weight shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.norm.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.4.0.convs.1.all_modules.0.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.all_modules.1.weight shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.all_modules.1.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.conv.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.5.0.convs.0.conv.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.norm.weight shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.norm.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.5.0.convs.0.all_modules.0.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.all_modules.1.weight shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.all_modules.1.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.5.0.convs.1.conv.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.norm.weight shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.norm.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.5.0.convs.1.all_modules.0.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.all_modules.1.weight shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.all_modules.1.bias shape torch.Size([320])
decoder.stages.0.convs.0.conv.weight shape torch.Size([320, 640, 3, 3, 3])
decoder.stages.0.convs.0.conv.bias shape torch.Size([320])
decoder.stages.0.convs.0.norm.weight shape torch.Size([320])
decoder.stages.0.convs.0.norm.bias shape torch.Size([320])
decoder.stages.0.convs.0.all_modules.0.weight shape torch.Size([320, 640, 3, 3, 3])
decoder.stages.0.convs.0.all_modules.0.bias shape torch.Size([320])
decoder.stages.0.convs.0.all_modules.1.weight shape torch.Size([320])
decoder.stages.0.convs.0.all_modules.1.bias shape torch.Size([320])
decoder.stages.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.stages.0.convs.1.conv.bias shape torch.Size([320])
decoder.stages.0.convs.1.norm.weight shape torch.Size([320])
decoder.stages.0.convs.1.norm.bias shape torch.Size([320])
decoder.stages.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.stages.0.convs.1.all_modules.0.bias shape torch.Size([320])
decoder.stages.0.convs.1.all_modules.1.weight shape torch.Size([320])
decoder.stages.0.convs.1.all_modules.1.bias shape torch.Size([320])
decoder.stages.1.convs.0.conv.weight shape torch.Size([256, 512, 3, 3, 3])
decoder.stages.1.convs.0.conv.bias shape torch.Size([256])
decoder.stages.1.convs.0.norm.weight shape torch.Size([256])
decoder.stages.1.convs.0.norm.bias shape torch.Size([256])
decoder.stages.1.convs.0.all_modules.0.weight shape torch.Size([256, 512, 3, 3, 3])
decoder.stages.1.convs.0.all_modules.0.bias shape torch.Size([256])
decoder.stages.1.convs.0.all_modules.1.weight shape torch.Size([256])
decoder.stages.1.convs.0.all_modules.1.bias shape torch.Size([256])
decoder.stages.1.convs.1.conv.weight shape torch.Size([256, 256, 3, 3, 3])
decoder.stages.1.convs.1.conv.bias shape torch.Size([256])
decoder.stages.1.convs.1.norm.weight shape torch.Size([256])
decoder.stages.1.convs.1.norm.bias shape torch.Size([256])
decoder.stages.1.convs.1.all_modules.0.weight shape torch.Size([256, 256, 3, 3, 3])
decoder.stages.1.convs.1.all_modules.0.bias shape torch.Size([256])
decoder.stages.1.convs.1.all_modules.1.weight shape torch.Size([256])
decoder.stages.1.convs.1.all_modules.1.bias shape torch.Size([256])
decoder.stages.2.convs.0.conv.weight shape torch.Size([128, 256, 3, 3, 3])
decoder.stages.2.convs.0.conv.bias shape torch.Size([128])
decoder.stages.2.convs.0.norm.weight shape torch.Size([128])
decoder.stages.2.convs.0.norm.bias shape torch.Size([128])
decoder.stages.2.convs.0.all_modules.0.weight shape torch.Size([128, 256, 3, 3, 3])
decoder.stages.2.convs.0.all_modules.0.bias shape torch.Size([128])
decoder.stages.2.convs.0.all_modules.1.weight shape torch.Size([128])
decoder.stages.2.convs.0.all_modules.1.bias shape torch.Size([128])
decoder.stages.2.convs.1.conv.weight shape torch.Size([128, 128, 3, 3, 3])
decoder.stages.2.convs.1.conv.bias shape torch.Size([128])
decoder.stages.2.convs.1.norm.weight shape torch.Size([128])
decoder.stages.2.convs.1.norm.bias shape torch.Size([128])
decoder.stages.2.convs.1.all_modules.0.weight shape torch.Size([128, 128, 3, 3, 3])
decoder.stages.2.convs.1.all_modules.0.bias shape torch.Size([128])
decoder.stages.2.convs.1.all_modules.1.weight shape torch.Size([128])
decoder.stages.2.convs.1.all_modules.1.bias shape torch.Size([128])
decoder.stages.3.convs.0.conv.weight shape torch.Size([64, 128, 3, 3, 3])
decoder.stages.3.convs.0.conv.bias shape torch.Size([64])
decoder.stages.3.convs.0.norm.weight shape torch.Size([64])
decoder.stages.3.convs.0.norm.bias shape torch.Size([64])
decoder.stages.3.convs.0.all_modules.0.weight shape torch.Size([64, 128, 3, 3, 3])
decoder.stages.3.convs.0.all_modules.0.bias shape torch.Size([64])
decoder.stages.3.convs.0.all_modules.1.weight shape torch.Size([64])
decoder.stages.3.convs.0.all_modules.1.bias shape torch.Size([64])
decoder.stages.3.convs.1.conv.weight shape torch.Size([64, 64, 3, 3, 3])
decoder.stages.3.convs.1.conv.bias shape torch.Size([64])
decoder.stages.3.convs.1.norm.weight shape torch.Size([64])
decoder.stages.3.convs.1.norm.bias shape torch.Size([64])
decoder.stages.3.convs.1.all_modules.0.weight shape torch.Size([64, 64, 3, 3, 3])
decoder.stages.3.convs.1.all_modules.0.bias shape torch.Size([64])
decoder.stages.3.convs.1.all_modules.1.weight shape torch.Size([64])
decoder.stages.3.convs.1.all_modules.1.bias shape torch.Size([64])
decoder.stages.4.convs.0.conv.weight shape torch.Size([32, 64, 3, 3, 3])
decoder.stages.4.convs.0.conv.bias shape torch.Size([32])
decoder.stages.4.convs.0.norm.weight shape torch.Size([32])
decoder.stages.4.convs.0.norm.bias shape torch.Size([32])
decoder.stages.4.convs.0.all_modules.0.weight shape torch.Size([32, 64, 3, 3, 3])
decoder.stages.4.convs.0.all_modules.0.bias shape torch.Size([32])
decoder.stages.4.convs.0.all_modules.1.weight shape torch.Size([32])
decoder.stages.4.convs.0.all_modules.1.bias shape torch.Size([32])
decoder.stages.4.convs.1.conv.weight shape torch.Size([32, 32, 3, 3, 3])
decoder.stages.4.convs.1.conv.bias shape torch.Size([32])
decoder.stages.4.convs.1.norm.weight shape torch.Size([32])
decoder.stages.4.convs.1.norm.bias shape torch.Size([32])
decoder.stages.4.convs.1.all_modules.0.weight shape torch.Size([32, 32, 3, 3, 3])
decoder.stages.4.convs.1.all_modules.0.bias shape torch.Size([32])
decoder.stages.4.convs.1.all_modules.1.weight shape torch.Size([32])
decoder.stages.4.convs.1.all_modules.1.bias shape torch.Size([32])
decoder.transpconvs.0.weight shape torch.Size([320, 320, 1, 2, 2])
decoder.transpconvs.0.bias shape torch.Size([320])
decoder.transpconvs.1.weight shape torch.Size([320, 256, 2, 2, 2])
decoder.transpconvs.1.bias shape torch.Size([256])
decoder.transpconvs.2.weight shape torch.Size([256, 128, 2, 2, 2])
decoder.transpconvs.2.bias shape torch.Size([128])
decoder.transpconvs.3.weight shape torch.Size([128, 64, 2, 2, 2])
decoder.transpconvs.3.bias shape torch.Size([64])
decoder.transpconvs.4.weight shape torch.Size([64, 32, 2, 2, 2])
decoder.transpconvs.4.bias shape torch.Size([32])
################### Done ###################
2024-12-19 23:58:56.296182: do_dummy_2d_data_aug: True
2024-12-19 23:58:56.297665: Using splits from existing split file: /home/gridsan/nchutisilp/datasets/nnUNet_Datasets/nnUNet_preprocessed/Dataset308_Sohee_Calcium_OCT_CrossValidation/splits_final_80.json
2024-12-19 23:58:56.299317: The split file contains 5 splits.
2024-12-19 23:58:56.300061: Desired fold for training: 2
2024-12-19 23:58:56.300999: This split has 6 training and 3 validation cases.
using pin_memory on device 0
using pin_memory on device 0
using pin_memory on device 0

This is the configuration used by this training:
Configuration name: 3d_32x160x128_b10
 {'data_identifier': 'nnUNetPlans_3d_fullres', 'preprocessor_name': 'DefaultPreprocessor', 'batch_size': 10, 'patch_size': [32, 160, 128], 'median_image_size_in_voxels': [375.0, 498.0, 498.0], 'spacing': [1.0, 1.0, 1.0], 'normalization_schemes': ['ZScoreNormalization'], 'use_mask_for_norm': [False], 'resampling_fn_data': 'resample_data_or_seg_to_shape', 'resampling_fn_seg': 'resample_data_or_seg_to_shape', 'resampling_fn_data_kwargs': {'is_seg': False, 'order': 3, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_seg_kwargs': {'is_seg': True, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_probabilities': 'resample_data_or_seg_to_shape', 'resampling_fn_probabilities_kwargs': {'is_seg': False, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'architecture': {'network_class_name': 'dynamic_network_architectures.architectures.unet.PlainConvUNet', 'arch_kwargs': {'n_stages': 6, 'features_per_stage': [32, 64, 128, 256, 320, 320], 'conv_op': 'torch.nn.modules.conv.Conv3d', 'kernel_sizes': [[3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3]], 'strides': [[1, 1, 1], [2, 2, 2], [2, 2, 2], [2, 2, 2], [2, 2, 2], [1, 2, 2]], 'n_conv_per_stage': [2, 2, 2, 2, 2, 2], 'n_conv_per_stage_decoder': [2, 2, 2, 2, 2], 'conv_bias': True, 'norm_op': 'torch.nn.modules.instancenorm.InstanceNorm3d', 'norm_op_kwargs': {'eps': 1e-05, 'affine': True}, 'dropout_op': None, 'dropout_op_kwargs': None, 'nonlin': 'torch.nn.LeakyReLU', 'nonlin_kwargs': {'inplace': True}, 'deep_supervision': True}, '_kw_requires_import': ['conv_op', 'norm_op', 'dropout_op', 'nonlin']}, 'batch_dice': True, 'inherits_from': '3d_fullres'} 

These are the global plan.json settings:
 {'dataset_name': 'Dataset308_Sohee_Calcium_OCT_CrossValidation', 'plans_name': 'nnUNetPlans', 'original_median_spacing_after_transp': [1.0, 1.0, 1.0], 'original_median_shape_after_transp': [375, 498, 498], 'image_reader_writer': 'SimpleITKIO', 'transpose_forward': [0, 1, 2], 'transpose_backward': [0, 1, 2], 'experiment_planner_used': 'ExperimentPlanner', 'label_manager': 'LabelManager', 'foreground_intensity_properties_per_channel': {'0': {'max': 0.4977976381778717, 'mean': 0.13507525622844696, 'median': 0.09599608182907104, 'min': 0.0, 'percentile_00_5': 0.014754901640117168, 'percentile_99_5': 0.4977976381778717, 'std': 0.12025152146816254}}} 

2024-12-19 23:59:25.121330: unpacking dataset...
using pin_memory on device 0

This is the configuration used by this training:
Configuration name: 3d_32x160x128_b10
 {'data_identifier': 'nnUNetPlans_3d_fullres', 'preprocessor_name': 'DefaultPreprocessor', 'batch_size': 10, 'patch_size': [32, 160, 128], 'median_image_size_in_voxels': [375.0, 498.0, 498.0], 'spacing': [1.0, 1.0, 1.0], 'normalization_schemes': ['ZScoreNormalization'], 'use_mask_for_norm': [False], 'resampling_fn_data': 'resample_data_or_seg_to_shape', 'resampling_fn_seg': 'resample_data_or_seg_to_shape', 'resampling_fn_data_kwargs': {'is_seg': False, 'order': 3, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_seg_kwargs': {'is_seg': True, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_probabilities': 'resample_data_or_seg_to_shape', 'resampling_fn_probabilities_kwargs': {'is_seg': False, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'architecture': {'network_class_name': 'dynamic_network_architectures.architectures.unet.PlainConvUNet', 'arch_kwargs': {'n_stages': 6, 'features_per_stage': [32, 64, 128, 256, 320, 320], 'conv_op': 'torch.nn.modules.conv.Conv3d', 'kernel_sizes': [[3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3]], 'strides': [[1, 1, 1], [2, 2, 2], [2, 2, 2], [2, 2, 2], [2, 2, 2], [1, 2, 2]], 'n_conv_per_stage': [2, 2, 2, 2, 2, 2], 'n_conv_per_stage_decoder': [2, 2, 2, 2, 2], 'conv_bias': True, 'norm_op': 'torch.nn.modules.instancenorm.InstanceNorm3d', 'norm_op_kwargs': {'eps': 1e-05, 'affine': True}, 'dropout_op': None, 'dropout_op_kwargs': None, 'nonlin': 'torch.nn.LeakyReLU', 'nonlin_kwargs': {'inplace': True}, 'deep_supervision': True}, '_kw_requires_import': ['conv_op', 'norm_op', 'dropout_op', 'nonlin']}, 'batch_dice': True, 'inherits_from': '3d_fullres'} 

These are the global plan.json settings:
 {'dataset_name': 'Dataset308_Sohee_Calcium_OCT_CrossValidation', 'plans_name': 'nnUNetPlans', 'original_median_spacing_after_transp': [1.0, 1.0, 1.0], 'original_median_shape_after_transp': [375, 498, 498], 'image_reader_writer': 'SimpleITKIO', 'transpose_forward': [0, 1, 2], 'transpose_backward': [0, 1, 2], 'experiment_planner_used': 'ExperimentPlanner', 'label_manager': 'LabelManager', 'foreground_intensity_properties_per_channel': {'0': {'max': 0.4977976381778717, 'mean': 0.13507525622844696, 'median': 0.09599608182907104, 'min': 0.0, 'percentile_00_5': 0.014754901640117168, 'percentile_99_5': 0.4977976381778717, 'std': 0.12025152146816254}}} 

2024-12-19 23:59:27.918073: unpacking dataset...
2024-12-19 23:59:32.397494: unpacking done...
2024-12-19 23:59:32.406683: Unable to plot network architecture: nnUNet_compile is enabled!
2024-12-19 23:59:32.470490: 
2024-12-19 23:59:32.472569: Epoch 0
2024-12-19 23:59:32.474259: Current learning rate: 0.01
2024-12-20 00:06:56.447813: Validation loss improved from 1000.00000 to -0.14521! Patience: 0/50
2024-12-20 00:06:56.448947: train_loss -0.1314
2024-12-20 00:06:56.450186: val_loss -0.1452
2024-12-20 00:06:56.450985: Pseudo dice [0.5031]
2024-12-20 00:06:56.451776: Epoch time: 443.98 s
2024-12-20 00:06:56.452605: Yayy! New best EMA pseudo Dice: 0.5031
2024-12-20 00:06:58.887136: 
2024-12-20 00:06:58.888690: Epoch 1
2024-12-20 00:06:58.889796: Current learning rate: 0.00994
2024-12-20 00:13:00.312479: Validation loss improved from -0.14521 to -0.21931! Patience: 0/50
2024-12-20 00:13:00.314362: train_loss -0.2472
2024-12-20 00:13:00.315937: val_loss -0.2193
2024-12-20 00:13:00.317383: Pseudo dice [0.5439]
2024-12-20 00:13:00.319182: Epoch time: 361.43 s
2024-12-20 00:13:00.320839: Yayy! New best EMA pseudo Dice: 0.5072
2024-12-20 00:13:02.153298: 
2024-12-20 00:13:02.155931: Epoch 2
2024-12-20 00:13:02.157602: Current learning rate: 0.00988
2024-12-20 00:19:22.662854: Validation loss did not improve from -0.21931. Patience: 1/50
2024-12-20 00:19:22.663869: train_loss -0.283
2024-12-20 00:19:22.664828: val_loss -0.2088
2024-12-20 00:19:22.665663: Pseudo dice [0.5209]
2024-12-20 00:19:22.666538: Epoch time: 380.51 s
2024-12-20 00:19:22.667248: Yayy! New best EMA pseudo Dice: 0.5086
2024-12-20 00:19:24.580679: 
2024-12-20 00:19:24.582111: Epoch 3
2024-12-20 00:19:24.583082: Current learning rate: 0.00982
2024-12-20 00:25:42.929798: Validation loss improved from -0.21931 to -0.25652! Patience: 1/50
2024-12-20 00:25:42.931100: train_loss -0.3105
2024-12-20 00:25:42.932283: val_loss -0.2565
2024-12-20 00:25:42.933263: Pseudo dice [0.5612]
2024-12-20 00:25:42.934249: Epoch time: 378.35 s
2024-12-20 00:25:42.935126: Yayy! New best EMA pseudo Dice: 0.5138
2024-12-20 00:25:44.778980: 
2024-12-20 00:25:44.780109: Epoch 4
2024-12-20 00:25:44.780833: Current learning rate: 0.00976
2024-12-20 00:31:30.934233: Validation loss improved from -0.25652 to -0.34930! Patience: 0/50
2024-12-20 00:31:30.935254: train_loss -0.3481
2024-12-20 00:31:30.936546: val_loss -0.3493
2024-12-20 00:31:30.937657: Pseudo dice [0.6144]
2024-12-20 00:31:30.938832: Epoch time: 346.16 s
2024-12-20 00:31:31.342659: Yayy! New best EMA pseudo Dice: 0.5239
2024-12-20 00:31:33.175777: 
2024-12-20 00:31:33.177084: Epoch 5
2024-12-20 00:31:33.177936: Current learning rate: 0.0097
2024-12-20 00:37:52.776926: Validation loss did not improve from -0.34930. Patience: 1/50
2024-12-20 00:37:52.777903: train_loss -0.4009
2024-12-20 00:37:52.779500: val_loss -0.3355
2024-12-20 00:37:52.780548: Pseudo dice [0.6041]
2024-12-20 00:37:52.781468: Epoch time: 379.6 s
2024-12-20 00:37:52.782330: Yayy! New best EMA pseudo Dice: 0.5319
2024-12-20 00:37:54.574038: 
2024-12-20 00:37:54.575992: Epoch 6
2024-12-20 00:37:54.577253: Current learning rate: 0.00964
2024-12-20 00:44:06.922391: Validation loss improved from -0.34930 to -0.39189! Patience: 1/50
2024-12-20 00:44:06.923285: train_loss -0.4021
2024-12-20 00:44:06.924036: val_loss -0.3919
2024-12-20 00:44:06.924729: Pseudo dice [0.6426]
2024-12-20 00:44:06.925549: Epoch time: 372.35 s
2024-12-20 00:44:06.926273: Yayy! New best EMA pseudo Dice: 0.543
2024-12-20 00:44:08.798514: 
2024-12-20 00:44:08.799678: Epoch 7
2024-12-20 00:44:08.800449: Current learning rate: 0.00958
2024-12-20 00:50:22.469381: Validation loss improved from -0.39189 to -0.39844! Patience: 0/50
2024-12-20 00:50:22.470567: train_loss -0.4222
2024-12-20 00:50:22.471728: val_loss -0.3984
2024-12-20 00:50:22.473313: Pseudo dice [0.653]
2024-12-20 00:50:22.475460: Epoch time: 373.67 s
2024-12-20 00:50:22.477566: Yayy! New best EMA pseudo Dice: 0.554
2024-12-20 00:50:24.692623: 
2024-12-20 00:50:24.693826: Epoch 8
2024-12-20 00:50:24.694653: Current learning rate: 0.00952
2024-12-20 00:56:46.902548: Validation loss did not improve from -0.39844. Patience: 1/50
2024-12-20 00:56:46.903482: train_loss -0.4411
2024-12-20 00:56:46.904435: val_loss -0.3743
2024-12-20 00:56:46.905222: Pseudo dice [0.6309]
2024-12-20 00:56:46.905993: Epoch time: 382.21 s
2024-12-20 00:56:46.906808: Yayy! New best EMA pseudo Dice: 0.5617
2024-12-20 00:56:48.821658: 
2024-12-20 00:56:48.823307: Epoch 9
2024-12-20 00:56:48.824448: Current learning rate: 0.00946
2024-12-20 01:02:58.123062: Validation loss improved from -0.39844 to -0.42852! Patience: 1/50
2024-12-20 01:02:58.124229: train_loss -0.4453
2024-12-20 01:02:58.125443: val_loss -0.4285
2024-12-20 01:02:58.126494: Pseudo dice [0.6802]
2024-12-20 01:02:58.127372: Epoch time: 369.31 s
2024-12-20 01:02:58.577270: Yayy! New best EMA pseudo Dice: 0.5735
2024-12-20 01:03:00.400256: 
2024-12-20 01:03:00.402940: Epoch 10
2024-12-20 01:03:00.404602: Current learning rate: 0.0094
2024-12-20 01:09:33.358864: Validation loss did not improve from -0.42852. Patience: 1/50
2024-12-20 01:09:33.360408: train_loss -0.4614
2024-12-20 01:09:33.362684: val_loss -0.4047
2024-12-20 01:09:33.363782: Pseudo dice [0.6555]
2024-12-20 01:09:33.365067: Epoch time: 392.96 s
2024-12-20 01:09:33.366295: Yayy! New best EMA pseudo Dice: 0.5817
2024-12-20 01:09:35.199483: 
2024-12-20 01:09:35.201842: Epoch 11
2024-12-20 01:09:35.204134: Current learning rate: 0.00934
2024-12-20 01:15:58.183219: Validation loss improved from -0.42852 to -0.43503! Patience: 1/50
2024-12-20 01:15:58.184235: train_loss -0.4814
2024-12-20 01:15:58.185171: val_loss -0.435
2024-12-20 01:15:58.186100: Pseudo dice [0.6672]
2024-12-20 01:15:58.186900: Epoch time: 382.99 s
2024-12-20 01:15:58.187597: Yayy! New best EMA pseudo Dice: 0.5903
2024-12-20 01:16:00.043706: 
2024-12-20 01:16:00.044789: Epoch 12
2024-12-20 01:16:00.045566: Current learning rate: 0.00928
2024-12-20 01:22:17.921389: Validation loss improved from -0.43503 to -0.43722! Patience: 0/50
2024-12-20 01:22:17.945515: train_loss -0.4874
2024-12-20 01:22:17.946442: val_loss -0.4372
2024-12-20 01:22:17.947173: Pseudo dice [0.6814]
2024-12-20 01:22:17.947845: Epoch time: 377.9 s
2024-12-20 01:22:17.948474: Yayy! New best EMA pseudo Dice: 0.5994
2024-12-20 01:22:19.749652: 
2024-12-20 01:22:19.750771: Epoch 13
2024-12-20 01:22:19.751472: Current learning rate: 0.00922
2024-12-20 01:28:04.112440: Validation loss did not improve from -0.43722. Patience: 1/50
2024-12-20 01:28:04.113697: train_loss -0.4915
2024-12-20 01:28:04.115171: val_loss -0.3996
2024-12-20 01:28:04.116756: Pseudo dice [0.6482]
2024-12-20 01:28:04.118810: Epoch time: 344.37 s
2024-12-20 01:28:04.120683: Yayy! New best EMA pseudo Dice: 0.6043
2024-12-20 01:28:05.956306: 
2024-12-20 01:28:05.957427: Epoch 14
2024-12-20 01:28:05.958153: Current learning rate: 0.00916
2024-12-20 01:34:27.359442: Validation loss did not improve from -0.43722. Patience: 2/50
2024-12-20 01:34:27.360681: train_loss -0.498
2024-12-20 01:34:27.361896: val_loss -0.3999
2024-12-20 01:34:27.363222: Pseudo dice [0.6502]
2024-12-20 01:34:27.364462: Epoch time: 381.41 s
2024-12-20 01:34:27.795380: Yayy! New best EMA pseudo Dice: 0.6089
2024-12-20 01:34:29.580958: 
2024-12-20 01:34:29.582228: Epoch 15
2024-12-20 01:34:29.582970: Current learning rate: 0.0091
2024-12-20 01:40:51.821903: Validation loss did not improve from -0.43722. Patience: 3/50
2024-12-20 01:40:51.822628: train_loss -0.5133
2024-12-20 01:40:51.823292: val_loss -0.4348
2024-12-20 01:40:51.823952: Pseudo dice [0.6689]
2024-12-20 01:40:51.824663: Epoch time: 382.24 s
2024-12-20 01:40:51.825537: Yayy! New best EMA pseudo Dice: 0.6149
2024-12-20 01:40:53.663931: 
2024-12-20 01:40:53.665230: Epoch 16
2024-12-20 01:40:53.666436: Current learning rate: 0.00903
2024-12-20 01:47:20.961962: Validation loss did not improve from -0.43722. Patience: 4/50
2024-12-20 01:47:20.962755: train_loss -0.5112
2024-12-20 01:47:20.963463: val_loss -0.4304
2024-12-20 01:47:20.964101: Pseudo dice [0.6664]
2024-12-20 01:47:20.964817: Epoch time: 387.3 s
2024-12-20 01:47:20.965611: Yayy! New best EMA pseudo Dice: 0.62
2024-12-20 01:47:22.847770: 
2024-12-20 01:47:22.849764: Epoch 17
2024-12-20 01:47:22.851298: Current learning rate: 0.00897
2024-12-20 01:53:15.371156: Validation loss improved from -0.43722 to -0.46882! Patience: 4/50
2024-12-20 01:53:15.372023: train_loss -0.5221
2024-12-20 01:53:15.372878: val_loss -0.4688
2024-12-20 01:53:15.373518: Pseudo dice [0.681]
2024-12-20 01:53:15.374181: Epoch time: 352.53 s
2024-12-20 01:53:15.374851: Yayy! New best EMA pseudo Dice: 0.6261
2024-12-20 01:53:17.765265: 
2024-12-20 01:53:17.766360: Epoch 18
2024-12-20 01:53:17.767289: Current learning rate: 0.00891
2024-12-20 01:58:15.224817: Validation loss did not improve from -0.46882. Patience: 1/50
2024-12-20 01:58:15.225666: train_loss -0.5286
2024-12-20 01:58:15.226530: val_loss -0.4533
2024-12-20 01:58:15.227263: Pseudo dice [0.685]
2024-12-20 01:58:15.227915: Epoch time: 297.46 s
2024-12-20 01:58:15.228616: Yayy! New best EMA pseudo Dice: 0.632
2024-12-20 01:58:17.157537: 
2024-12-20 01:58:17.159883: Epoch 19
2024-12-20 01:58:17.161655: Current learning rate: 0.00885
2024-12-20 02:03:36.579492: Validation loss did not improve from -0.46882. Patience: 2/50
2024-12-20 02:03:36.580891: train_loss -0.5334
2024-12-20 02:03:36.583095: val_loss -0.4673
2024-12-20 02:03:36.585091: Pseudo dice [0.6886]
2024-12-20 02:03:36.587386: Epoch time: 319.42 s
2024-12-20 02:03:37.012514: Yayy! New best EMA pseudo Dice: 0.6377
2024-12-20 02:03:38.872332: 
2024-12-20 02:03:38.873522: Epoch 20
2024-12-20 02:03:38.874267: Current learning rate: 0.00879
2024-12-20 02:09:41.197094: Validation loss improved from -0.46882 to -0.47569! Patience: 2/50
2024-12-20 02:09:41.198278: train_loss -0.5438
2024-12-20 02:09:41.199156: val_loss -0.4757
2024-12-20 02:09:41.199926: Pseudo dice [0.6942]
2024-12-20 02:09:41.200741: Epoch time: 362.33 s
2024-12-20 02:09:41.201561: Yayy! New best EMA pseudo Dice: 0.6433
2024-12-20 02:09:43.092021: 
2024-12-20 02:09:43.094019: Epoch 21
2024-12-20 02:09:43.095152: Current learning rate: 0.00873
2024-12-20 02:14:52.612526: Validation loss did not improve from -0.47569. Patience: 1/50
2024-12-20 02:14:52.613636: train_loss -0.556
2024-12-20 02:14:52.614413: val_loss -0.4726
2024-12-20 02:14:52.615191: Pseudo dice [0.6872]
2024-12-20 02:14:52.615975: Epoch time: 309.52 s
2024-12-20 02:14:52.616719: Yayy! New best EMA pseudo Dice: 0.6477
2024-12-20 02:14:54.455160: 
2024-12-20 02:14:54.456217: Epoch 22
2024-12-20 02:14:54.456975: Current learning rate: 0.00867
2024-12-20 02:19:15.192936: Validation loss did not improve from -0.47569. Patience: 2/50
2024-12-20 02:19:15.193909: train_loss -0.5424
2024-12-20 02:19:15.194732: val_loss -0.4347
2024-12-20 02:19:15.195431: Pseudo dice [0.6725]
2024-12-20 02:19:15.196318: Epoch time: 260.74 s
2024-12-20 02:19:15.197149: Yayy! New best EMA pseudo Dice: 0.6502
2024-12-20 02:19:17.042151: 
2024-12-20 02:19:17.043293: Epoch 23
2024-12-20 02:19:17.044085: Current learning rate: 0.00861
2024-12-20 02:24:55.693766: Validation loss did not improve from -0.47569. Patience: 3/50
2024-12-20 02:24:55.696697: train_loss -0.5613
2024-12-20 02:24:55.697589: val_loss -0.4723
2024-12-20 02:24:55.698279: Pseudo dice [0.6858]
2024-12-20 02:24:55.699060: Epoch time: 338.66 s
2024-12-20 02:24:55.699828: Yayy! New best EMA pseudo Dice: 0.6538
2024-12-20 02:24:57.610934: 
2024-12-20 02:24:57.611922: Epoch 24
2024-12-20 02:24:57.612751: Current learning rate: 0.00855
2024-12-20 02:29:57.488729: Validation loss improved from -0.47569 to -0.48977! Patience: 3/50
2024-12-20 02:29:57.489777: train_loss -0.5635
2024-12-20 02:29:57.490851: val_loss -0.4898
2024-12-20 02:29:57.491796: Pseudo dice [0.701]
2024-12-20 02:29:57.492843: Epoch time: 299.88 s
2024-12-20 02:29:57.837819: Yayy! New best EMA pseudo Dice: 0.6585
2024-12-20 02:29:59.678990: 
2024-12-20 02:29:59.681664: Epoch 25
2024-12-20 02:29:59.683493: Current learning rate: 0.00849
2024-12-20 02:35:12.837571: Validation loss did not improve from -0.48977. Patience: 1/50
2024-12-20 02:35:12.838317: train_loss -0.5689
2024-12-20 02:35:12.839395: val_loss -0.4877
2024-12-20 02:35:12.840267: Pseudo dice [0.7001]
2024-12-20 02:35:12.841156: Epoch time: 313.16 s
2024-12-20 02:35:12.841843: Yayy! New best EMA pseudo Dice: 0.6626
2024-12-20 02:35:14.694819: 
2024-12-20 02:35:14.696567: Epoch 26
2024-12-20 02:35:14.698668: Current learning rate: 0.00843
2024-12-20 02:40:43.213126: Validation loss did not improve from -0.48977. Patience: 2/50
2024-12-20 02:40:43.214227: train_loss -0.5654
2024-12-20 02:40:43.215583: val_loss -0.4816
2024-12-20 02:40:43.216972: Pseudo dice [0.693]
2024-12-20 02:40:43.218133: Epoch time: 328.52 s
2024-12-20 02:40:43.219158: Yayy! New best EMA pseudo Dice: 0.6657
2024-12-20 02:40:45.059130: 
2024-12-20 02:40:45.061223: Epoch 27
2024-12-20 02:40:45.062782: Current learning rate: 0.00836
2024-12-20 02:46:58.316936: Validation loss did not improve from -0.48977. Patience: 3/50
2024-12-20 02:46:58.318033: train_loss -0.5763
2024-12-20 02:46:58.319402: val_loss -0.466
2024-12-20 02:46:58.320385: Pseudo dice [0.6883]
2024-12-20 02:46:58.321506: Epoch time: 373.26 s
2024-12-20 02:46:58.322369: Yayy! New best EMA pseudo Dice: 0.6679
2024-12-20 02:47:00.209093: 
2024-12-20 02:47:00.210383: Epoch 28
2024-12-20 02:47:00.211113: Current learning rate: 0.0083
2024-12-20 02:52:45.890209: Validation loss improved from -0.48977 to -0.51231! Patience: 3/50
2024-12-20 02:52:45.891419: train_loss -0.5851
2024-12-20 02:52:45.892716: val_loss -0.5123
2024-12-20 02:52:45.894120: Pseudo dice [0.7056]
2024-12-20 02:52:45.894902: Epoch time: 345.68 s
2024-12-20 02:52:45.895933: Yayy! New best EMA pseudo Dice: 0.6717
2024-12-20 02:52:48.269813: 
2024-12-20 02:52:48.271133: Epoch 29
2024-12-20 02:52:48.272692: Current learning rate: 0.00824
2024-12-20 02:57:50.910281: Validation loss did not improve from -0.51231. Patience: 1/50
2024-12-20 02:57:50.911328: train_loss -0.5889
2024-12-20 02:57:50.912165: val_loss -0.5069
2024-12-20 02:57:50.912925: Pseudo dice [0.71]
2024-12-20 02:57:50.913737: Epoch time: 302.64 s
2024-12-20 02:57:51.375714: Yayy! New best EMA pseudo Dice: 0.6755
2024-12-20 02:57:53.271994: 
2024-12-20 02:57:53.274043: Epoch 30
2024-12-20 02:57:53.275412: Current learning rate: 0.00818
2024-12-20 03:03:11.184085: Validation loss did not improve from -0.51231. Patience: 2/50
2024-12-20 03:03:11.184853: train_loss -0.5908
2024-12-20 03:03:11.186069: val_loss -0.4994
2024-12-20 03:03:11.187412: Pseudo dice [0.7083]
2024-12-20 03:03:11.188767: Epoch time: 317.91 s
2024-12-20 03:03:11.190296: Yayy! New best EMA pseudo Dice: 0.6788
2024-12-20 03:03:13.123424: 
2024-12-20 03:03:13.125095: Epoch 31
2024-12-20 03:03:13.126525: Current learning rate: 0.00812
2024-12-20 03:09:12.661438: Validation loss did not improve from -0.51231. Patience: 3/50
2024-12-20 03:09:12.662349: train_loss -0.5942
2024-12-20 03:09:12.663573: val_loss -0.4372
2024-12-20 03:09:12.664817: Pseudo dice [0.6694]
2024-12-20 03:09:12.665673: Epoch time: 359.54 s
2024-12-20 03:09:14.125839: 
2024-12-20 03:09:14.127166: Epoch 32
2024-12-20 03:09:14.128050: Current learning rate: 0.00806
2024-12-20 03:13:52.051815: Validation loss did not improve from -0.51231. Patience: 4/50
2024-12-20 03:13:52.052664: train_loss -0.6021
2024-12-20 03:13:52.053530: val_loss -0.4775
2024-12-20 03:13:52.054320: Pseudo dice [0.7124]
2024-12-20 03:13:52.055015: Epoch time: 277.93 s
2024-12-20 03:13:52.055662: Yayy! New best EMA pseudo Dice: 0.6813
2024-12-20 03:13:54.018152: 
2024-12-20 03:13:54.019714: Epoch 33
2024-12-20 03:13:54.020880: Current learning rate: 0.008
2024-12-20 03:19:42.042798: Validation loss did not improve from -0.51231. Patience: 5/50
2024-12-20 03:19:42.044059: train_loss -0.6033
2024-12-20 03:19:42.045664: val_loss -0.4875
2024-12-20 03:19:42.046861: Pseudo dice [0.7026]
2024-12-20 03:19:42.048167: Epoch time: 348.03 s
2024-12-20 03:19:42.049554: Yayy! New best EMA pseudo Dice: 0.6835
2024-12-20 03:19:43.894398: 
2024-12-20 03:19:43.895536: Epoch 34
2024-12-20 03:19:43.896472: Current learning rate: 0.00793
2024-12-20 03:25:37.972278: Validation loss improved from -0.51231 to -0.51379! Patience: 5/50
2024-12-20 03:25:37.973357: train_loss -0.5987
2024-12-20 03:25:37.974521: val_loss -0.5138
2024-12-20 03:25:37.975363: Pseudo dice [0.7204]
2024-12-20 03:25:37.976153: Epoch time: 354.08 s
2024-12-20 03:25:38.357056: Yayy! New best EMA pseudo Dice: 0.6871
2024-12-20 03:25:40.173748: 
2024-12-20 03:25:40.175572: Epoch 35
2024-12-20 03:25:40.177098: Current learning rate: 0.00787
2024-12-20 03:31:13.616620: Validation loss did not improve from -0.51379. Patience: 1/50
2024-12-20 03:31:13.619613: train_loss -0.6105
2024-12-20 03:31:13.620750: val_loss -0.4853
2024-12-20 03:31:13.621551: Pseudo dice [0.705]
2024-12-20 03:31:13.622563: Epoch time: 333.45 s
2024-12-20 03:31:13.623501: Yayy! New best EMA pseudo Dice: 0.6889
2024-12-20 03:31:15.448397: 
2024-12-20 03:31:15.450946: Epoch 36
2024-12-20 03:31:15.452537: Current learning rate: 0.00781
2024-12-20 03:36:29.258066: Validation loss did not improve from -0.51379. Patience: 2/50
2024-12-20 03:36:29.258894: train_loss -0.6054
2024-12-20 03:36:29.260006: val_loss -0.5037
2024-12-20 03:36:29.260990: Pseudo dice [0.7143]
2024-12-20 03:36:29.261949: Epoch time: 313.81 s
2024-12-20 03:36:29.262983: Yayy! New best EMA pseudo Dice: 0.6915
2024-12-20 03:36:31.071753: 
2024-12-20 03:36:31.074148: Epoch 37
2024-12-20 03:36:31.076017: Current learning rate: 0.00775
2024-12-20 03:41:34.485300: Validation loss did not improve from -0.51379. Patience: 3/50
2024-12-20 03:41:34.486274: train_loss -0.6148
2024-12-20 03:41:34.487206: val_loss -0.4897
2024-12-20 03:41:34.488410: Pseudo dice [0.7063]
2024-12-20 03:41:34.489664: Epoch time: 303.42 s
2024-12-20 03:41:34.490452: Yayy! New best EMA pseudo Dice: 0.6929
2024-12-20 03:41:36.265210: 
2024-12-20 03:41:36.266510: Epoch 38
2024-12-20 03:41:36.267552: Current learning rate: 0.00769
2024-12-20 03:47:26.134530: Validation loss did not improve from -0.51379. Patience: 4/50
2024-12-20 03:47:26.135626: train_loss -0.6137
2024-12-20 03:47:26.136561: val_loss -0.4917
2024-12-20 03:47:26.137322: Pseudo dice [0.7023]
2024-12-20 03:47:26.138200: Epoch time: 349.87 s
2024-12-20 03:47:26.138918: Yayy! New best EMA pseudo Dice: 0.6939
2024-12-20 03:47:28.631810: 
2024-12-20 03:47:28.634046: Epoch 39
2024-12-20 03:47:28.635999: Current learning rate: 0.00763
2024-12-20 03:52:49.733976: Validation loss improved from -0.51379 to -0.52331! Patience: 4/50
2024-12-20 03:52:49.736414: train_loss -0.609
2024-12-20 03:52:49.738355: val_loss -0.5233
2024-12-20 03:52:49.739927: Pseudo dice [0.7266]
2024-12-20 03:52:49.741558: Epoch time: 321.11 s
2024-12-20 03:52:50.119843: Yayy! New best EMA pseudo Dice: 0.6972
2024-12-20 03:52:51.937828: 
2024-12-20 03:52:51.940637: Epoch 40
2024-12-20 03:52:51.942147: Current learning rate: 0.00756
2024-12-20 03:58:00.041460: Validation loss did not improve from -0.52331. Patience: 1/50
2024-12-20 03:58:00.042573: train_loss -0.6122
2024-12-20 03:58:00.044958: val_loss -0.5045
2024-12-20 03:58:00.046842: Pseudo dice [0.7257]
2024-12-20 03:58:00.048188: Epoch time: 308.11 s
2024-12-20 03:58:00.049237: Yayy! New best EMA pseudo Dice: 0.7
2024-12-20 03:58:01.984739: 
2024-12-20 03:58:01.985965: Epoch 41
2024-12-20 03:58:01.986738: Current learning rate: 0.0075
2024-12-20 04:03:48.422994: Validation loss did not improve from -0.52331. Patience: 2/50
2024-12-20 04:03:48.424157: train_loss -0.6193
2024-12-20 04:03:48.424937: val_loss -0.4914
2024-12-20 04:03:48.425552: Pseudo dice [0.7073]
2024-12-20 04:03:48.426236: Epoch time: 346.44 s
2024-12-20 04:03:48.426867: Yayy! New best EMA pseudo Dice: 0.7007
2024-12-20 04:03:50.138102: 
2024-12-20 04:03:50.139176: Epoch 42
2024-12-20 04:03:50.139983: Current learning rate: 0.00744
2024-12-20 04:09:36.165378: Validation loss improved from -0.52331 to -0.52385! Patience: 2/50
2024-12-20 04:09:36.166080: train_loss -0.6261
2024-12-20 04:09:36.166926: val_loss -0.5238
2024-12-20 04:09:36.167616: Pseudo dice [0.7221]
2024-12-20 04:09:36.168292: Epoch time: 346.03 s
2024-12-20 04:09:36.169078: Yayy! New best EMA pseudo Dice: 0.7029
2024-12-20 04:09:37.945295: 
2024-12-20 04:09:37.947987: Epoch 43
2024-12-20 04:09:37.950145: Current learning rate: 0.00738
2024-12-20 04:14:20.291524: Validation loss did not improve from -0.52385. Patience: 1/50
2024-12-20 04:14:20.293188: train_loss -0.6248
2024-12-20 04:14:20.294688: val_loss -0.5094
2024-12-20 04:14:20.296261: Pseudo dice [0.7193]
2024-12-20 04:14:20.297692: Epoch time: 282.35 s
2024-12-20 04:14:20.298728: Yayy! New best EMA pseudo Dice: 0.7045
2024-12-20 04:14:22.022408: 
2024-12-20 04:14:22.024487: Epoch 44
2024-12-20 04:14:22.026873: Current learning rate: 0.00732
2024-12-20 04:19:45.186658: Validation loss did not improve from -0.52385. Patience: 2/50
2024-12-20 04:19:45.187710: train_loss -0.6232
2024-12-20 04:19:45.188498: val_loss -0.4605
2024-12-20 04:19:45.189189: Pseudo dice [0.6845]
2024-12-20 04:19:45.189832: Epoch time: 323.17 s
2024-12-20 04:19:46.909567: 
2024-12-20 04:19:46.910881: Epoch 45
2024-12-20 04:19:46.911611: Current learning rate: 0.00725
2024-12-20 04:25:26.528299: Validation loss improved from -0.52385 to -0.53085! Patience: 2/50
2024-12-20 04:25:26.529140: train_loss -0.6192
2024-12-20 04:25:26.530231: val_loss -0.5308
2024-12-20 04:25:26.531346: Pseudo dice [0.7261]
2024-12-20 04:25:26.532344: Epoch time: 339.62 s
2024-12-20 04:25:26.533369: Yayy! New best EMA pseudo Dice: 0.7049
2024-12-20 04:25:28.262003: 
2024-12-20 04:25:28.263219: Epoch 46
2024-12-20 04:25:28.263920: Current learning rate: 0.00719
2024-12-20 04:31:35.165778: Validation loss did not improve from -0.53085. Patience: 1/50
2024-12-20 04:31:35.166574: train_loss -0.6395
2024-12-20 04:31:35.167615: val_loss -0.4852
2024-12-20 04:31:35.168761: Pseudo dice [0.6964]
2024-12-20 04:31:35.169739: Epoch time: 366.91 s
2024-12-20 04:31:36.690268: 
2024-12-20 04:31:36.693155: Epoch 47
2024-12-20 04:31:36.694656: Current learning rate: 0.00713
2024-12-20 04:37:22.425531: Validation loss did not improve from -0.53085. Patience: 2/50
2024-12-20 04:37:22.475232: train_loss -0.645
2024-12-20 04:37:22.477424: val_loss -0.4905
2024-12-20 04:37:22.478486: Pseudo dice [0.705]
2024-12-20 04:37:22.483874: Epoch time: 345.78 s
2024-12-20 04:37:24.645408: 
2024-12-20 04:37:24.647290: Epoch 48
2024-12-20 04:37:24.648470: Current learning rate: 0.00707
2024-12-20 04:42:24.311488: Validation loss improved from -0.53085 to -0.54820! Patience: 2/50
2024-12-20 04:42:24.312426: train_loss -0.6473
2024-12-20 04:42:24.313347: val_loss -0.5482
2024-12-20 04:42:24.314257: Pseudo dice [0.7424]
2024-12-20 04:42:24.315493: Epoch time: 299.67 s
2024-12-20 04:42:24.316609: Yayy! New best EMA pseudo Dice: 0.708
2024-12-20 04:42:26.132482: 
2024-12-20 04:42:26.134983: Epoch 49
2024-12-20 04:42:26.136950: Current learning rate: 0.007
2024-12-20 04:48:10.079029: Validation loss did not improve from -0.54820. Patience: 1/50
2024-12-20 04:48:10.080034: train_loss -0.6446
2024-12-20 04:48:10.081557: val_loss -0.5363
2024-12-20 04:48:10.082762: Pseudo dice [0.7348]
2024-12-20 04:48:10.083851: Epoch time: 343.95 s
2024-12-20 04:48:10.500326: Yayy! New best EMA pseudo Dice: 0.7106
2024-12-20 04:48:13.559174: 
2024-12-20 04:48:13.561638: Epoch 50
2024-12-20 04:48:13.563177: Current learning rate: 0.00694
2024-12-20 04:53:18.804913: Validation loss did not improve from -0.54820. Patience: 2/50
2024-12-20 04:53:18.805758: train_loss -0.651
2024-12-20 04:53:18.806559: val_loss -0.5388
2024-12-20 04:53:18.807552: Pseudo dice [0.7332]
2024-12-20 04:53:18.808336: Epoch time: 305.25 s
2024-12-20 04:53:18.809143: Yayy! New best EMA pseudo Dice: 0.7129
2024-12-20 04:53:20.662798: 
2024-12-20 04:53:20.664346: Epoch 51
2024-12-20 04:53:20.665293: Current learning rate: 0.00688
2024-12-20 04:58:32.101640: Validation loss did not improve from -0.54820. Patience: 3/50
2024-12-20 04:58:32.103039: train_loss -0.6548
2024-12-20 04:58:32.104398: val_loss -0.4835
2024-12-20 04:58:32.105729: Pseudo dice [0.7056]
2024-12-20 04:58:32.107269: Epoch time: 311.44 s
2024-12-20 04:58:33.529384: 
2024-12-20 04:58:33.530550: Epoch 52
2024-12-20 04:58:33.531331: Current learning rate: 0.00682
2024-12-20 05:04:33.381358: Validation loss did not improve from -0.54820. Patience: 4/50
2024-12-20 05:04:33.382398: train_loss -0.6517
2024-12-20 05:04:33.383291: val_loss -0.5104
2024-12-20 05:04:33.384012: Pseudo dice [0.711]
2024-12-20 05:04:33.384726: Epoch time: 359.85 s
2024-12-20 05:04:34.767400: 
2024-12-20 05:04:34.768965: Epoch 53
2024-12-20 05:04:34.769984: Current learning rate: 0.00675
2024-12-20 05:10:18.437622: Validation loss did not improve from -0.54820. Patience: 5/50
2024-12-20 05:10:18.438609: train_loss -0.657
2024-12-20 05:10:18.439429: val_loss -0.462
2024-12-20 05:10:18.440143: Pseudo dice [0.6798]
2024-12-20 05:10:18.440858: Epoch time: 343.67 s
2024-12-20 05:10:19.861984: 
2024-12-20 05:10:19.863780: Epoch 54
2024-12-20 05:10:19.865122: Current learning rate: 0.00669
2024-12-20 05:14:45.543961: Validation loss did not improve from -0.54820. Patience: 6/50
2024-12-20 05:14:45.544889: train_loss -0.6453
2024-12-20 05:14:45.546078: val_loss -0.4748
2024-12-20 05:14:45.547263: Pseudo dice [0.6991]
2024-12-20 05:14:45.548044: Epoch time: 265.68 s
2024-12-20 05:14:47.369132: 
2024-12-20 05:14:47.370545: Epoch 55
2024-12-20 05:14:47.371403: Current learning rate: 0.00663
2024-12-20 05:19:52.536918: Validation loss did not improve from -0.54820. Patience: 7/50
2024-12-20 05:19:52.537915: train_loss -0.6455
2024-12-20 05:19:52.539363: val_loss -0.5252
2024-12-20 05:19:52.540943: Pseudo dice [0.7317]
2024-12-20 05:19:52.542047: Epoch time: 305.17 s
2024-12-20 05:19:54.026550: 
2024-12-20 05:19:54.028129: Epoch 56
2024-12-20 05:19:54.029761: Current learning rate: 0.00657
2024-12-20 05:25:47.376450: Validation loss did not improve from -0.54820. Patience: 8/50
2024-12-20 05:25:47.379003: train_loss -0.654
2024-12-20 05:25:47.383300: val_loss -0.5075
2024-12-20 05:25:47.385577: Pseudo dice [0.724]
2024-12-20 05:25:47.387207: Epoch time: 353.35 s
2024-12-20 05:25:48.786251: 
2024-12-20 05:25:48.788170: Epoch 57
2024-12-20 05:25:48.790082: Current learning rate: 0.0065
2024-12-20 05:30:43.072215: Validation loss did not improve from -0.54820. Patience: 9/50
2024-12-20 05:30:43.073186: train_loss -0.65
2024-12-20 05:30:43.074260: val_loss -0.5258
2024-12-20 05:30:43.075305: Pseudo dice [0.7308]
2024-12-20 05:30:43.075984: Epoch time: 294.29 s
2024-12-20 05:30:43.076684: Yayy! New best EMA pseudo Dice: 0.7135
2024-12-20 05:30:44.934422: 
2024-12-20 05:30:44.935764: Epoch 58
2024-12-20 05:30:44.936571: Current learning rate: 0.00644
2024-12-20 05:35:48.720062: Validation loss did not improve from -0.54820. Patience: 10/50
2024-12-20 05:35:48.720893: train_loss -0.6567
2024-12-20 05:35:48.721698: val_loss -0.5307
2024-12-20 05:35:48.722543: Pseudo dice [0.7273]
2024-12-20 05:35:48.723370: Epoch time: 303.79 s
2024-12-20 05:35:48.724219: Yayy! New best EMA pseudo Dice: 0.7149
2024-12-20 05:35:50.686938: 
2024-12-20 05:35:50.688501: Epoch 59
2024-12-20 05:35:50.689572: Current learning rate: 0.00638
2024-12-20 05:41:34.094827: Validation loss did not improve from -0.54820. Patience: 11/50
2024-12-20 05:41:34.099054: train_loss -0.6708
2024-12-20 05:41:34.101454: val_loss -0.5133
2024-12-20 05:41:34.102745: Pseudo dice [0.7273]
2024-12-20 05:41:34.104212: Epoch time: 343.41 s
2024-12-20 05:41:34.588769: Yayy! New best EMA pseudo Dice: 0.7161
2024-12-20 05:41:36.463653: 
2024-12-20 05:41:36.464904: Epoch 60
2024-12-20 05:41:36.465763: Current learning rate: 0.00631
2024-12-20 05:47:15.393110: Validation loss did not improve from -0.54820. Patience: 12/50
2024-12-20 05:47:15.394888: train_loss -0.6666
2024-12-20 05:47:15.395744: val_loss -0.5232
2024-12-20 05:47:15.396388: Pseudo dice [0.7228]
2024-12-20 05:47:15.397126: Epoch time: 338.93 s
2024-12-20 05:47:15.397775: Yayy! New best EMA pseudo Dice: 0.7168
2024-12-20 05:47:17.230893: 
2024-12-20 05:47:17.232626: Epoch 61
2024-12-20 05:47:17.233911: Current learning rate: 0.00625
2024-12-20 05:52:04.198213: Validation loss did not improve from -0.54820. Patience: 13/50
2024-12-20 05:52:04.199122: train_loss -0.6658
2024-12-20 05:52:04.200388: val_loss -0.5393
2024-12-20 05:52:04.201901: Pseudo dice [0.7387]
2024-12-20 05:52:04.203261: Epoch time: 286.97 s
2024-12-20 05:52:04.205290: Yayy! New best EMA pseudo Dice: 0.719
2024-12-20 05:52:06.461059: 
2024-12-20 05:52:06.463008: Epoch 62
2024-12-20 05:52:06.464997: Current learning rate: 0.00619
2024-12-20 05:58:06.135464: Validation loss did not improve from -0.54820. Patience: 14/50
2024-12-20 05:58:06.136377: train_loss -0.6739
2024-12-20 05:58:06.137441: val_loss -0.5341
2024-12-20 05:58:06.138353: Pseudo dice [0.7363]
2024-12-20 05:58:06.139268: Epoch time: 359.68 s
2024-12-20 05:58:06.140194: Yayy! New best EMA pseudo Dice: 0.7207
2024-12-20 05:58:07.909801: 
2024-12-20 05:58:07.911463: Epoch 63
2024-12-20 05:58:07.912990: Current learning rate: 0.00612
2024-12-20 06:03:46.342640: Validation loss did not improve from -0.54820. Patience: 15/50
2024-12-20 06:03:46.343749: train_loss -0.6787
2024-12-20 06:03:46.344647: val_loss -0.5366
2024-12-20 06:03:46.345623: Pseudo dice [0.7332]
2024-12-20 06:03:46.346580: Epoch time: 338.44 s
2024-12-20 06:03:46.347736: Yayy! New best EMA pseudo Dice: 0.722
2024-12-20 06:03:48.117269: 
2024-12-20 06:03:48.118653: Epoch 64
2024-12-20 06:03:48.119708: Current learning rate: 0.00606
2024-12-20 06:09:04.625583: Validation loss did not improve from -0.54820. Patience: 16/50
2024-12-20 06:09:04.626544: train_loss -0.6843
2024-12-20 06:09:04.627529: val_loss -0.5005
2024-12-20 06:09:04.628474: Pseudo dice [0.7174]
2024-12-20 06:09:04.629523: Epoch time: 316.51 s
2024-12-20 06:09:06.434435: 
2024-12-20 06:09:06.435770: Epoch 65
2024-12-20 06:09:06.436490: Current learning rate: 0.006
2024-12-20 06:14:41.633368: Validation loss did not improve from -0.54820. Patience: 17/50
2024-12-20 06:14:41.634409: train_loss -0.6823
2024-12-20 06:14:41.635154: val_loss -0.5269
2024-12-20 06:14:41.635808: Pseudo dice [0.7271]
2024-12-20 06:14:41.636544: Epoch time: 335.2 s
2024-12-20 06:14:41.637237: Yayy! New best EMA pseudo Dice: 0.7221
2024-12-20 06:14:43.453299: 
2024-12-20 06:14:43.454513: Epoch 66
2024-12-20 06:14:43.455280: Current learning rate: 0.00593
2024-12-20 06:20:08.679383: Validation loss did not improve from -0.54820. Patience: 18/50
2024-12-20 06:20:08.680877: train_loss -0.6745
2024-12-20 06:20:08.681661: val_loss -0.5324
2024-12-20 06:20:08.682383: Pseudo dice [0.7366]
2024-12-20 06:20:08.683228: Epoch time: 325.23 s
2024-12-20 06:20:08.684225: Yayy! New best EMA pseudo Dice: 0.7235
2024-12-20 06:20:10.561754: 
2024-12-20 06:20:10.563119: Epoch 67
2024-12-20 06:20:10.563798: Current learning rate: 0.00587
2024-12-20 06:25:12.896847: Validation loss did not improve from -0.54820. Patience: 19/50
2024-12-20 06:25:12.898103: train_loss -0.6805
2024-12-20 06:25:12.899948: val_loss -0.537
2024-12-20 06:25:12.902020: Pseudo dice [0.7363]
2024-12-20 06:25:12.903678: Epoch time: 302.34 s
2024-12-20 06:25:12.905254: Yayy! New best EMA pseudo Dice: 0.7248
2024-12-20 06:25:14.703015: 
2024-12-20 06:25:14.704774: Epoch 68
2024-12-20 06:25:14.706047: Current learning rate: 0.00581
2024-12-20 06:30:48.891881: Validation loss did not improve from -0.54820. Patience: 20/50
2024-12-20 06:30:48.894405: train_loss -0.6821
2024-12-20 06:30:48.897264: val_loss -0.5243
2024-12-20 06:30:48.898544: Pseudo dice [0.7279]
2024-12-20 06:30:48.900491: Epoch time: 334.19 s
2024-12-20 06:30:48.902508: Yayy! New best EMA pseudo Dice: 0.7251
2024-12-20 06:30:50.710910: 
2024-12-20 06:30:50.712250: Epoch 69
2024-12-20 06:30:50.712929: Current learning rate: 0.00574
2024-12-20 06:35:39.083915: Validation loss did not improve from -0.54820. Patience: 21/50
2024-12-20 06:35:39.085082: train_loss -0.6881
2024-12-20 06:35:39.086170: val_loss -0.5069
2024-12-20 06:35:39.086845: Pseudo dice [0.7271]
2024-12-20 06:35:39.087507: Epoch time: 288.38 s
2024-12-20 06:35:39.467894: Yayy! New best EMA pseudo Dice: 0.7253
2024-12-20 06:35:41.226261: 
2024-12-20 06:35:41.227367: Epoch 70
2024-12-20 06:35:41.228046: Current learning rate: 0.00568
2024-12-20 06:41:32.599770: Validation loss did not improve from -0.54820. Patience: 22/50
2024-12-20 06:41:32.600943: train_loss -0.6846
2024-12-20 06:41:32.601938: val_loss -0.5336
2024-12-20 06:41:32.602661: Pseudo dice [0.7372]
2024-12-20 06:41:32.603333: Epoch time: 351.38 s
2024-12-20 06:41:32.603935: Yayy! New best EMA pseudo Dice: 0.7265
2024-12-20 06:41:34.433193: 
2024-12-20 06:41:34.435041: Epoch 71
2024-12-20 06:41:34.436013: Current learning rate: 0.00562
2024-12-20 06:46:48.388644: Validation loss did not improve from -0.54820. Patience: 23/50
2024-12-20 06:46:48.392581: train_loss -0.6744
2024-12-20 06:46:48.394693: val_loss -0.5391
2024-12-20 06:46:48.396147: Pseudo dice [0.7289]
2024-12-20 06:46:48.397614: Epoch time: 313.96 s
2024-12-20 06:46:48.398897: Yayy! New best EMA pseudo Dice: 0.7267
2024-12-20 06:46:51.581684: 
2024-12-20 06:46:51.583444: Epoch 72
2024-12-20 06:46:51.585543: Current learning rate: 0.00555
2024-12-20 06:52:01.194203: Validation loss did not improve from -0.54820. Patience: 24/50
2024-12-20 06:52:01.195451: train_loss -0.6885
2024-12-20 06:52:01.196924: val_loss -0.526
2024-12-20 06:52:01.197874: Pseudo dice [0.7295]
2024-12-20 06:52:01.198694: Epoch time: 309.62 s
2024-12-20 06:52:01.199524: Yayy! New best EMA pseudo Dice: 0.727
2024-12-20 06:52:03.054450: 
2024-12-20 06:52:03.056182: Epoch 73
2024-12-20 06:52:03.057546: Current learning rate: 0.00549
2024-12-20 06:57:41.352925: Validation loss did not improve from -0.54820. Patience: 25/50
2024-12-20 06:57:41.353914: train_loss -0.6906
2024-12-20 06:57:41.354708: val_loss -0.5196
2024-12-20 06:57:41.355486: Pseudo dice [0.723]
2024-12-20 06:57:41.356488: Epoch time: 338.3 s
2024-12-20 06:57:42.787012: 
2024-12-20 06:57:42.788289: Epoch 74
2024-12-20 06:57:42.789132: Current learning rate: 0.00542
2024-12-20 07:03:07.724674: Validation loss did not improve from -0.54820. Patience: 26/50
2024-12-20 07:03:07.726273: train_loss -0.6996
2024-12-20 07:03:07.728299: val_loss -0.5452
2024-12-20 07:03:07.730130: Pseudo dice [0.7495]
2024-12-20 07:03:07.731571: Epoch time: 324.94 s
2024-12-20 07:03:08.178726: Yayy! New best EMA pseudo Dice: 0.7289
2024-12-20 07:03:09.971337: 
2024-12-20 07:03:09.972658: Epoch 75
2024-12-20 07:03:09.973430: Current learning rate: 0.00536
2024-12-20 07:08:10.571367: Validation loss did not improve from -0.54820. Patience: 27/50
2024-12-20 07:08:10.572333: train_loss -0.6961
2024-12-20 07:08:10.573564: val_loss -0.5472
2024-12-20 07:08:10.575186: Pseudo dice [0.7367]
2024-12-20 07:08:10.576478: Epoch time: 300.6 s
2024-12-20 07:08:10.577784: Yayy! New best EMA pseudo Dice: 0.7297
2024-12-20 07:08:12.445286: 
2024-12-20 07:08:12.446343: Epoch 76
2024-12-20 07:08:12.447236: Current learning rate: 0.00529
2024-12-20 07:13:57.282527: Validation loss did not improve from -0.54820. Patience: 28/50
2024-12-20 07:13:57.283650: train_loss -0.6908
2024-12-20 07:13:57.284555: val_loss -0.5378
2024-12-20 07:13:57.285399: Pseudo dice [0.7406]
2024-12-20 07:13:57.286221: Epoch time: 344.84 s
2024-12-20 07:13:57.286958: Yayy! New best EMA pseudo Dice: 0.7308
2024-12-20 07:13:59.276736: 
2024-12-20 07:13:59.278654: Epoch 77
2024-12-20 07:13:59.280666: Current learning rate: 0.00523
2024-12-20 07:19:12.825995: Validation loss did not improve from -0.54820. Patience: 29/50
2024-12-20 07:19:12.827101: train_loss -0.6997
2024-12-20 07:19:12.829139: val_loss -0.5071
2024-12-20 07:19:12.830865: Pseudo dice [0.725]
2024-12-20 07:19:12.832432: Epoch time: 313.55 s
2024-12-20 07:19:14.338374: 
2024-12-20 07:19:14.339655: Epoch 78
2024-12-20 07:19:14.340806: Current learning rate: 0.00517
2024-12-20 07:24:48.772390: Validation loss did not improve from -0.54820. Patience: 30/50
2024-12-20 07:24:48.773480: train_loss -0.7004
2024-12-20 07:24:48.774254: val_loss -0.5289
2024-12-20 07:24:48.774891: Pseudo dice [0.7323]
2024-12-20 07:24:48.775560: Epoch time: 334.44 s
2024-12-20 07:24:50.378704: 
2024-12-20 07:24:50.379885: Epoch 79
2024-12-20 07:24:50.380824: Current learning rate: 0.0051
2024-12-20 07:29:48.863731: Validation loss did not improve from -0.54820. Patience: 31/50
2024-12-20 07:29:48.864852: train_loss -0.7034
2024-12-20 07:29:48.865757: val_loss -0.5253
2024-12-20 07:29:48.866840: Pseudo dice [0.7237]
2024-12-20 07:29:48.867987: Epoch time: 298.49 s
2024-12-20 07:29:50.804694: 
2024-12-20 07:29:50.806868: Epoch 80
2024-12-20 07:29:50.809189: Current learning rate: 0.00504
2024-12-20 07:35:32.014355: Validation loss did not improve from -0.54820. Patience: 32/50
2024-12-20 07:35:32.015454: train_loss -0.6996
2024-12-20 07:35:32.016369: val_loss -0.5301
2024-12-20 07:35:32.017205: Pseudo dice [0.7185]
2024-12-20 07:35:32.017902: Epoch time: 341.21 s
2024-12-20 07:35:33.520196: 
2024-12-20 07:35:33.521484: Epoch 81
2024-12-20 07:35:33.522656: Current learning rate: 0.00497
2024-12-20 07:41:20.589767: Validation loss did not improve from -0.54820. Patience: 33/50
2024-12-20 07:41:20.590786: train_loss -0.7061
2024-12-20 07:41:20.591562: val_loss -0.5144
2024-12-20 07:41:20.592375: Pseudo dice [0.7299]
2024-12-20 07:41:20.593081: Epoch time: 347.07 s
2024-12-20 07:41:22.040504: 
2024-12-20 07:41:22.041973: Epoch 82
2024-12-20 07:41:22.042930: Current learning rate: 0.00491
2024-12-20 07:46:20.240710: Validation loss did not improve from -0.54820. Patience: 34/50
2024-12-20 07:46:20.241712: train_loss -0.7051
2024-12-20 07:46:20.242625: val_loss -0.5172
2024-12-20 07:46:20.243329: Pseudo dice [0.733]
2024-12-20 07:46:20.244129: Epoch time: 298.2 s
2024-12-20 07:46:22.083086: 
2024-12-20 07:46:22.084476: Epoch 83
2024-12-20 07:46:22.085533: Current learning rate: 0.00484
2024-12-20 07:52:06.417729: Validation loss did not improve from -0.54820. Patience: 35/50
2024-12-20 07:52:06.422151: train_loss -0.7087
2024-12-20 07:52:06.424963: val_loss -0.5385
2024-12-20 07:52:06.426588: Pseudo dice [0.7336]
2024-12-20 07:52:06.429072: Epoch time: 344.34 s
2024-12-20 07:52:07.939120: 
2024-12-20 07:52:07.940296: Epoch 84
2024-12-20 07:52:07.941137: Current learning rate: 0.00478
2024-12-20 07:57:58.352256: Validation loss did not improve from -0.54820. Patience: 36/50
2024-12-20 07:57:58.353318: train_loss -0.7052
2024-12-20 07:57:58.354382: val_loss -0.5271
2024-12-20 07:57:58.355391: Pseudo dice [0.7356]
2024-12-20 07:57:58.356520: Epoch time: 350.42 s
2024-12-20 07:58:00.029711: 
2024-12-20 07:58:00.031451: Epoch 85
2024-12-20 07:58:00.032539: Current learning rate: 0.00471
2024-12-20 08:03:40.677990: Validation loss did not improve from -0.54820. Patience: 37/50
2024-12-20 08:03:40.679039: train_loss -0.7036
2024-12-20 08:03:40.679960: val_loss -0.5172
2024-12-20 08:03:40.680834: Pseudo dice [0.723]
2024-12-20 08:03:40.681560: Epoch time: 340.65 s
2024-12-20 08:03:42.053887: 
2024-12-20 08:03:42.055994: Epoch 86
2024-12-20 08:03:42.057550: Current learning rate: 0.00465
2024-12-20 08:08:46.571805: Validation loss did not improve from -0.54820. Patience: 38/50
2024-12-20 08:08:46.572973: train_loss -0.7051
2024-12-20 08:08:46.574736: val_loss -0.5092
2024-12-20 08:08:46.576365: Pseudo dice [0.7214]
2024-12-20 08:08:46.578201: Epoch time: 304.52 s
2024-12-20 08:08:48.005796: 
2024-12-20 08:08:48.007077: Epoch 87
2024-12-20 08:08:48.008343: Current learning rate: 0.00458
2024-12-20 08:14:01.450710: Validation loss did not improve from -0.54820. Patience: 39/50
2024-12-20 08:14:01.451611: train_loss -0.7125
2024-12-20 08:14:01.452303: val_loss -0.5109
2024-12-20 08:14:01.452959: Pseudo dice [0.729]
2024-12-20 08:14:01.453840: Epoch time: 313.45 s
2024-12-20 08:14:02.873849: 
2024-12-20 08:14:02.875356: Epoch 88
2024-12-20 08:14:02.876760: Current learning rate: 0.00452
2024-12-20 08:19:41.484429: Validation loss did not improve from -0.54820. Patience: 40/50
2024-12-20 08:19:41.485343: train_loss -0.7174
2024-12-20 08:19:41.486310: val_loss -0.5258
2024-12-20 08:19:41.487100: Pseudo dice [0.7295]
2024-12-20 08:19:41.487769: Epoch time: 338.61 s
2024-12-20 08:19:42.838955: 
2024-12-20 08:19:42.840788: Epoch 89
2024-12-20 08:19:42.842353: Current learning rate: 0.00445
2024-12-20 08:25:16.499649: Validation loss did not improve from -0.54820. Patience: 41/50
2024-12-20 08:25:16.500831: train_loss -0.7105
2024-12-20 08:25:16.501746: val_loss -0.4954
2024-12-20 08:25:16.502606: Pseudo dice [0.708]
2024-12-20 08:25:16.503446: Epoch time: 333.66 s
2024-12-20 08:25:18.354821: 
2024-12-20 08:25:18.356231: Epoch 90
2024-12-20 08:25:18.357074: Current learning rate: 0.00438
2024-12-20 08:30:07.147732: Validation loss did not improve from -0.54820. Patience: 42/50
2024-12-20 08:30:07.148813: train_loss -0.7079
2024-12-20 08:30:07.149508: val_loss -0.5234
2024-12-20 08:30:07.150205: Pseudo dice [0.7286]
2024-12-20 08:30:07.151028: Epoch time: 288.8 s
2024-12-20 08:30:08.598565: 
2024-12-20 08:30:08.599925: Epoch 91
2024-12-20 08:30:08.601036: Current learning rate: 0.00432
2024-12-20 08:35:36.782552: Validation loss did not improve from -0.54820. Patience: 43/50
2024-12-20 08:35:36.783554: train_loss -0.717
2024-12-20 08:35:36.784972: val_loss -0.5338
2024-12-20 08:35:36.785878: Pseudo dice [0.7304]
2024-12-20 08:35:36.786971: Epoch time: 328.19 s
2024-12-20 08:35:38.239568: 
2024-12-20 08:35:38.241394: Epoch 92
2024-12-20 08:35:38.242302: Current learning rate: 0.00425
2024-12-20 08:41:43.871345: Validation loss did not improve from -0.54820. Patience: 44/50
2024-12-20 08:41:43.873086: train_loss -0.7246
2024-12-20 08:41:43.874400: val_loss -0.5406
2024-12-20 08:41:43.875747: Pseudo dice [0.7391]
2024-12-20 08:41:43.877311: Epoch time: 365.63 s
2024-12-20 08:41:45.280256: 
2024-12-20 08:41:45.281342: Epoch 93
2024-12-20 08:41:45.282021: Current learning rate: 0.00419
2024-12-20 08:46:38.081090: Validation loss did not improve from -0.54820. Patience: 45/50
2024-12-20 08:46:38.082164: train_loss -0.7273
2024-12-20 08:46:38.083020: val_loss -0.5286
2024-12-20 08:46:38.083701: Pseudo dice [0.7391]
2024-12-20 08:46:38.084418: Epoch time: 292.8 s
2024-12-20 08:46:39.424042: 
2024-12-20 08:46:39.424930: Epoch 94
2024-12-20 08:46:39.425583: Current learning rate: 0.00412
2024-12-20 08:52:08.903601: Validation loss did not improve from -0.54820. Patience: 46/50
2024-12-20 08:52:08.905087: train_loss -0.7274
2024-12-20 08:52:08.908051: val_loss -0.5267
2024-12-20 08:52:08.909511: Pseudo dice [0.739]
2024-12-20 08:52:08.910805: Epoch time: 329.48 s
2024-12-20 08:52:11.539495: 
2024-12-20 08:52:11.540868: Epoch 95
2024-12-20 08:52:11.541912: Current learning rate: 0.00405
2024-12-20 08:58:00.401616: Validation loss improved from -0.54820 to -0.56773! Patience: 46/50
2024-12-20 08:58:00.404996: train_loss -0.7286
2024-12-20 08:58:00.405936: val_loss -0.5677
2024-12-20 08:58:00.406775: Pseudo dice [0.7586]
2024-12-20 08:58:00.407700: Epoch time: 348.87 s
2024-12-20 08:58:00.408600: Yayy! New best EMA pseudo Dice: 0.7333
2024-12-20 08:58:02.266817: 
2024-12-20 08:58:02.268019: Epoch 96
2024-12-20 08:58:02.269073: Current learning rate: 0.00399
2024-12-20 09:02:45.679468: Validation loss did not improve from -0.56773. Patience: 1/50
2024-12-20 09:02:45.680295: train_loss -0.7291
2024-12-20 09:02:45.681030: val_loss -0.5255
2024-12-20 09:02:45.681731: Pseudo dice [0.7371]
2024-12-20 09:02:45.682510: Epoch time: 283.41 s
2024-12-20 09:02:45.683193: Yayy! New best EMA pseudo Dice: 0.7336
2024-12-20 09:02:47.524570: 
2024-12-20 09:02:47.526560: Epoch 97
2024-12-20 09:02:47.528506: Current learning rate: 0.00392
2024-12-20 09:08:05.423676: Validation loss did not improve from -0.56773. Patience: 2/50
2024-12-20 09:08:05.424766: train_loss -0.732
2024-12-20 09:08:05.426065: val_loss -0.5257
2024-12-20 09:08:05.427211: Pseudo dice [0.7275]
2024-12-20 09:08:05.428409: Epoch time: 317.9 s
2024-12-20 09:08:06.810549: 
2024-12-20 09:08:06.811823: Epoch 98
2024-12-20 09:08:06.812638: Current learning rate: 0.00385
2024-12-20 09:13:49.218855: Validation loss did not improve from -0.56773. Patience: 3/50
2024-12-20 09:13:49.219716: train_loss -0.7268
2024-12-20 09:13:49.220420: val_loss -0.5353
2024-12-20 09:13:49.221162: Pseudo dice [0.7383]
2024-12-20 09:13:49.221912: Epoch time: 342.41 s
2024-12-20 09:13:50.676118: 
2024-12-20 09:13:50.677921: Epoch 99
2024-12-20 09:13:50.679195: Current learning rate: 0.00379
2024-12-20 09:19:36.812753: Validation loss did not improve from -0.56773. Patience: 4/50
2024-12-20 09:19:36.814526: train_loss -0.7272
2024-12-20 09:19:36.817129: val_loss -0.5328
2024-12-20 09:19:36.818869: Pseudo dice [0.7362]
2024-12-20 09:19:36.820848: Epoch time: 346.14 s
2024-12-20 09:19:37.273955: Yayy! New best EMA pseudo Dice: 0.7338
2024-12-20 09:19:39.103697: 
2024-12-20 09:19:39.105110: Epoch 100
2024-12-20 09:19:39.106615: Current learning rate: 0.00372
2024-12-20 09:24:46.862370: Validation loss did not improve from -0.56773. Patience: 5/50
2024-12-20 09:24:46.865102: train_loss -0.7335
2024-12-20 09:24:46.866268: val_loss -0.5547
2024-12-20 09:24:46.867521: Pseudo dice [0.7481]
2024-12-20 09:24:46.868635: Epoch time: 307.76 s
2024-12-20 09:24:46.869658: Yayy! New best EMA pseudo Dice: 0.7352
2024-12-20 09:24:48.744375: 
2024-12-20 09:24:48.745817: Epoch 101
2024-12-20 09:24:48.746986: Current learning rate: 0.00365
2024-12-20 09:30:03.475708: Validation loss did not improve from -0.56773. Patience: 6/50
2024-12-20 09:30:03.476632: train_loss -0.7326
2024-12-20 09:30:03.477396: val_loss -0.5344
2024-12-20 09:30:03.478014: Pseudo dice [0.7425]
2024-12-20 09:30:03.478826: Epoch time: 314.73 s
2024-12-20 09:30:03.479603: Yayy! New best EMA pseudo Dice: 0.736
2024-12-20 09:30:05.369625: 
2024-12-20 09:30:05.371046: Epoch 102
2024-12-20 09:30:05.372370: Current learning rate: 0.00359
2024-12-20 09:36:04.846969: Validation loss did not improve from -0.56773. Patience: 7/50
2024-12-20 09:36:04.847922: train_loss -0.7355
2024-12-20 09:36:04.849071: val_loss -0.492
2024-12-20 09:36:04.850007: Pseudo dice [0.7136]
2024-12-20 09:36:04.850970: Epoch time: 359.48 s
2024-12-20 09:36:06.310716: 
2024-12-20 09:36:06.312050: Epoch 103
2024-12-20 09:36:06.313001: Current learning rate: 0.00352
2024-12-20 09:41:04.316760: Validation loss did not improve from -0.56773. Patience: 8/50
2024-12-20 09:41:04.317647: train_loss -0.7407
2024-12-20 09:41:04.318653: val_loss -0.5239
2024-12-20 09:41:04.319524: Pseudo dice [0.7303]
2024-12-20 09:41:04.320412: Epoch time: 298.01 s
2024-12-20 09:41:05.709407: 
2024-12-20 09:41:05.711052: Epoch 104
2024-12-20 09:41:05.711888: Current learning rate: 0.00345
2024-12-20 09:46:15.060266: Validation loss did not improve from -0.56773. Patience: 9/50
2024-12-20 09:46:15.061671: train_loss -0.736
2024-12-20 09:46:15.063489: val_loss -0.5155
2024-12-20 09:46:15.064512: Pseudo dice [0.7307]
2024-12-20 09:46:15.065499: Epoch time: 309.35 s
2024-12-20 09:46:16.875176: 
2024-12-20 09:46:16.876093: Epoch 105
2024-12-20 09:46:16.876863: Current learning rate: 0.00338
2024-12-20 09:52:06.283003: Validation loss did not improve from -0.56773. Patience: 10/50
2024-12-20 09:52:06.283789: train_loss -0.7405
2024-12-20 09:52:06.284581: val_loss -0.519
2024-12-20 09:52:06.285196: Pseudo dice [0.7272]
2024-12-20 09:52:06.286036: Epoch time: 349.41 s
2024-12-20 09:52:08.261212: 
2024-12-20 09:52:08.263643: Epoch 106
2024-12-20 09:52:08.264602: Current learning rate: 0.00332
2024-12-20 09:58:00.124521: Validation loss did not improve from -0.56773. Patience: 11/50
2024-12-20 09:58:00.125471: train_loss -0.7358
2024-12-20 09:58:00.126344: val_loss -0.5159
2024-12-20 09:58:00.127013: Pseudo dice [0.7268]
2024-12-20 09:58:00.127702: Epoch time: 351.87 s
2024-12-20 09:58:01.532471: 
2024-12-20 09:58:01.533761: Epoch 107
2024-12-20 09:58:01.534873: Current learning rate: 0.00325
2024-12-20 10:03:23.755794: Validation loss did not improve from -0.56773. Patience: 12/50
2024-12-20 10:03:23.758426: train_loss -0.7425
2024-12-20 10:03:23.759407: val_loss -0.5397
2024-12-20 10:03:23.760267: Pseudo dice [0.7401]
2024-12-20 10:03:23.761096: Epoch time: 322.23 s
2024-12-20 10:03:25.229142: 
2024-12-20 10:03:25.230349: Epoch 108
2024-12-20 10:03:25.231140: Current learning rate: 0.00318
2024-12-20 10:08:15.305943: Validation loss did not improve from -0.56773. Patience: 13/50
2024-12-20 10:08:15.307008: train_loss -0.7423
2024-12-20 10:08:15.308203: val_loss -0.5472
2024-12-20 10:08:15.309111: Pseudo dice [0.7412]
2024-12-20 10:08:15.310138: Epoch time: 290.08 s
2024-12-20 10:08:16.740570: 
2024-12-20 10:08:16.741509: Epoch 109
2024-12-20 10:08:16.742365: Current learning rate: 0.00311
2024-12-20 10:13:53.158028: Validation loss did not improve from -0.56773. Patience: 14/50
2024-12-20 10:13:53.159070: train_loss -0.7423
2024-12-20 10:13:53.160120: val_loss -0.5311
2024-12-20 10:13:53.160899: Pseudo dice [0.7386]
2024-12-20 10:13:53.161710: Epoch time: 336.42 s
2024-12-20 10:13:55.097108: 
2024-12-20 10:13:55.098307: Epoch 110
2024-12-20 10:13:55.099271: Current learning rate: 0.00304
2024-12-20 10:19:22.682133: Validation loss did not improve from -0.56773. Patience: 15/50
2024-12-20 10:19:22.683131: train_loss -0.7402
2024-12-20 10:19:22.684118: val_loss -0.5218
2024-12-20 10:19:22.685315: Pseudo dice [0.7282]
2024-12-20 10:19:22.686575: Epoch time: 327.59 s
2024-12-20 10:19:24.080614: 
2024-12-20 10:19:24.082399: Epoch 111
2024-12-20 10:19:24.083329: Current learning rate: 0.00297
2024-12-20 10:24:56.555705: Validation loss did not improve from -0.56773. Patience: 16/50
2024-12-20 10:24:56.556846: train_loss -0.7459
2024-12-20 10:24:56.557961: val_loss -0.5247
2024-12-20 10:24:56.559878: Pseudo dice [0.724]
2024-12-20 10:24:56.560917: Epoch time: 332.48 s
2024-12-20 10:24:57.956026: 
2024-12-20 10:24:57.957558: Epoch 112
2024-12-20 10:24:57.958520: Current learning rate: 0.00291
2024-12-20 10:30:14.734391: Validation loss did not improve from -0.56773. Patience: 17/50
2024-12-20 10:30:14.735234: train_loss -0.7458
2024-12-20 10:30:14.736186: val_loss -0.5099
2024-12-20 10:30:14.736944: Pseudo dice [0.7208]
2024-12-20 10:30:14.737632: Epoch time: 316.78 s
2024-12-20 10:30:16.151401: 
2024-12-20 10:30:16.152817: Epoch 113
2024-12-20 10:30:16.154123: Current learning rate: 0.00284
2024-12-20 10:35:44.498744: Validation loss did not improve from -0.56773. Patience: 18/50
2024-12-20 10:35:44.499939: train_loss -0.7423
2024-12-20 10:35:44.501063: val_loss -0.5485
2024-12-20 10:35:44.502033: Pseudo dice [0.7451]
2024-12-20 10:35:44.503183: Epoch time: 328.35 s
2024-12-20 10:35:45.954707: 
2024-12-20 10:35:45.956601: Epoch 114
2024-12-20 10:35:45.958412: Current learning rate: 0.00277
2024-12-20 10:40:58.491001: Validation loss did not improve from -0.56773. Patience: 19/50
2024-12-20 10:40:58.492042: train_loss -0.7514
2024-12-20 10:40:58.492942: val_loss -0.5029
2024-12-20 10:40:58.493759: Pseudo dice [0.7239]
2024-12-20 10:40:58.494452: Epoch time: 312.54 s
2024-12-20 10:41:00.358491: 
2024-12-20 10:41:00.359753: Epoch 115
2024-12-20 10:41:00.360602: Current learning rate: 0.0027
2024-12-20 10:46:51.158342: Validation loss improved from -0.56773 to -0.56901! Patience: 19/50
2024-12-20 10:46:51.159209: train_loss -0.7504
2024-12-20 10:46:51.160398: val_loss -0.569
2024-12-20 10:46:51.161323: Pseudo dice [0.7545]
2024-12-20 10:46:51.162271: Epoch time: 350.8 s
2024-12-20 10:46:52.603907: 
2024-12-20 10:46:52.605605: Epoch 116
2024-12-20 10:46:52.607275: Current learning rate: 0.00263
2024-12-20 10:52:18.033688: Validation loss did not improve from -0.56901. Patience: 1/50
2024-12-20 10:52:18.034678: train_loss -0.7494
2024-12-20 10:52:18.036227: val_loss -0.5418
2024-12-20 10:52:18.037424: Pseudo dice [0.7442]
2024-12-20 10:52:18.038867: Epoch time: 325.43 s
2024-12-20 10:52:19.972867: 
2024-12-20 10:52:19.974446: Epoch 117
2024-12-20 10:52:19.975695: Current learning rate: 0.00256
2024-12-20 10:58:14.069030: Validation loss did not improve from -0.56901. Patience: 2/50
2024-12-20 10:58:14.070641: train_loss -0.7546
2024-12-20 10:58:14.071552: val_loss -0.5578
2024-12-20 10:58:14.072366: Pseudo dice [0.7529]
2024-12-20 10:58:14.073190: Epoch time: 354.1 s
2024-12-20 10:58:14.074002: Yayy! New best EMA pseudo Dice: 0.7369
2024-12-20 10:58:16.151739: 
2024-12-20 10:58:16.153719: Epoch 118
2024-12-20 10:58:16.155274: Current learning rate: 0.00249
2024-12-20 11:03:55.324880: Validation loss did not improve from -0.56901. Patience: 3/50
2024-12-20 11:03:55.325941: train_loss -0.7509
2024-12-20 11:03:55.326775: val_loss -0.5422
2024-12-20 11:03:55.327651: Pseudo dice [0.7429]
2024-12-20 11:03:55.328778: Epoch time: 339.18 s
2024-12-20 11:03:55.329677: Yayy! New best EMA pseudo Dice: 0.7375
2024-12-20 11:03:57.196108: 
2024-12-20 11:03:57.197898: Epoch 119
2024-12-20 11:03:57.198732: Current learning rate: 0.00242
2024-12-20 11:09:12.109030: Validation loss did not improve from -0.56901. Patience: 4/50
2024-12-20 11:09:12.111096: train_loss -0.7511
2024-12-20 11:09:12.113091: val_loss -0.5209
2024-12-20 11:09:12.115007: Pseudo dice [0.7264]
2024-12-20 11:09:12.117306: Epoch time: 314.92 s
2024-12-20 11:09:14.061481: 
2024-12-20 11:09:14.062381: Epoch 120
2024-12-20 11:09:14.063203: Current learning rate: 0.00235
2024-12-20 11:14:42.747157: Validation loss did not improve from -0.56901. Patience: 5/50
2024-12-20 11:14:42.748219: train_loss -0.7521
2024-12-20 11:14:42.748993: val_loss -0.5141
2024-12-20 11:14:42.749681: Pseudo dice [0.7322]
2024-12-20 11:14:42.750329: Epoch time: 328.69 s
2024-12-20 11:14:44.166114: 
2024-12-20 11:14:44.168483: Epoch 121
2024-12-20 11:14:44.170852: Current learning rate: 0.00228
2024-12-20 11:20:11.139643: Validation loss did not improve from -0.56901. Patience: 6/50
2024-12-20 11:20:11.141026: train_loss -0.7482
2024-12-20 11:20:11.142848: val_loss -0.5073
2024-12-20 11:20:11.144096: Pseudo dice [0.7206]
2024-12-20 11:20:11.145412: Epoch time: 326.98 s
2024-12-20 11:20:12.651797: 
2024-12-20 11:20:12.653065: Epoch 122
2024-12-20 11:20:12.653781: Current learning rate: 0.00221
2024-12-20 11:25:44.012582: Validation loss did not improve from -0.56901. Patience: 7/50
2024-12-20 11:25:44.014799: train_loss -0.7527
2024-12-20 11:25:44.015786: val_loss -0.5348
2024-12-20 11:25:44.016455: Pseudo dice [0.7345]
2024-12-20 11:25:44.017337: Epoch time: 331.36 s
2024-12-20 11:25:45.477067: 
2024-12-20 11:25:45.478922: Epoch 123
2024-12-20 11:25:45.480109: Current learning rate: 0.00214
2024-12-20 11:31:19.623782: Validation loss did not improve from -0.56901. Patience: 8/50
2024-12-20 11:31:19.624726: train_loss -0.7516
2024-12-20 11:31:19.625390: val_loss -0.5502
2024-12-20 11:31:19.626017: Pseudo dice [0.7444]
2024-12-20 11:31:19.626680: Epoch time: 334.15 s
2024-12-20 11:31:21.113889: 
2024-12-20 11:31:21.115195: Epoch 124
2024-12-20 11:31:21.115978: Current learning rate: 0.00207
2024-12-20 11:36:41.766678: Validation loss did not improve from -0.56901. Patience: 9/50
2024-12-20 11:36:41.767742: train_loss -0.755
2024-12-20 11:36:41.768804: val_loss -0.5067
2024-12-20 11:36:41.769933: Pseudo dice [0.7305]
2024-12-20 11:36:41.771335: Epoch time: 320.66 s
2024-12-20 11:36:43.757276: 
2024-12-20 11:36:43.758122: Epoch 125
2024-12-20 11:36:43.758895: Current learning rate: 0.00199
2024-12-20 11:42:32.457601: Validation loss did not improve from -0.56901. Patience: 10/50
2024-12-20 11:42:32.458647: train_loss -0.7567
2024-12-20 11:42:32.459751: val_loss -0.5452
2024-12-20 11:42:32.460625: Pseudo dice [0.7473]
2024-12-20 11:42:32.461551: Epoch time: 348.7 s
2024-12-20 11:42:33.924247: 
2024-12-20 11:42:33.926162: Epoch 126
2024-12-20 11:42:33.927638: Current learning rate: 0.00192
2024-12-20 11:47:40.290493: Validation loss did not improve from -0.56901. Patience: 11/50
2024-12-20 11:47:40.291337: train_loss -0.7612
2024-12-20 11:47:40.292360: val_loss -0.5149
2024-12-20 11:47:40.293175: Pseudo dice [0.7251]
2024-12-20 11:47:40.294031: Epoch time: 306.37 s
2024-12-20 11:47:41.741821: 
2024-12-20 11:47:41.742951: Epoch 127
2024-12-20 11:47:41.743711: Current learning rate: 0.00185
2024-12-20 11:53:25.314861: Validation loss did not improve from -0.56901. Patience: 12/50
2024-12-20 11:53:25.316088: train_loss -0.7608
2024-12-20 11:53:25.318110: val_loss -0.5455
2024-12-20 11:53:25.319468: Pseudo dice [0.7457]
2024-12-20 11:53:25.321241: Epoch time: 343.58 s
2024-12-20 11:53:27.163893: 
2024-12-20 11:53:27.165066: Epoch 128
2024-12-20 11:53:27.165896: Current learning rate: 0.00178
2024-12-20 11:59:35.339061: Validation loss did not improve from -0.56901. Patience: 13/50
2024-12-20 11:59:35.340921: train_loss -0.7616
2024-12-20 11:59:35.342565: val_loss -0.52
2024-12-20 11:59:35.344264: Pseudo dice [0.7347]
2024-12-20 11:59:35.346121: Epoch time: 368.18 s
2024-12-20 11:59:36.833788: 
2024-12-20 11:59:36.835058: Epoch 129
2024-12-20 11:59:36.835919: Current learning rate: 0.0017
2024-12-20 12:05:02.815489: Validation loss did not improve from -0.56901. Patience: 14/50
2024-12-20 12:05:02.816799: train_loss -0.7634
2024-12-20 12:05:02.819036: val_loss -0.523
2024-12-20 12:05:02.820784: Pseudo dice [0.7292]
2024-12-20 12:05:02.822117: Epoch time: 325.98 s
2024-12-20 12:05:04.762701: 
2024-12-20 12:05:04.763954: Epoch 130
2024-12-20 12:05:04.764936: Current learning rate: 0.00163
2024-12-20 12:10:04.303248: Validation loss did not improve from -0.56901. Patience: 15/50
2024-12-20 12:10:04.305759: train_loss -0.7587
2024-12-20 12:10:04.306929: val_loss -0.5312
2024-12-20 12:10:04.307748: Pseudo dice [0.737]
2024-12-20 12:10:04.308602: Epoch time: 299.54 s
2024-12-20 12:10:05.786241: 
2024-12-20 12:10:05.787702: Epoch 131
2024-12-20 12:10:05.789082: Current learning rate: 0.00156
2024-12-20 12:15:43.059875: Validation loss did not improve from -0.56901. Patience: 16/50
2024-12-20 12:15:43.061691: train_loss -0.7609
2024-12-20 12:15:43.062882: val_loss -0.5379
2024-12-20 12:15:43.063737: Pseudo dice [0.7485]
2024-12-20 12:15:43.064631: Epoch time: 337.28 s
2024-12-20 12:15:44.759346: 
2024-12-20 12:15:44.760589: Epoch 132
2024-12-20 12:15:44.761713: Current learning rate: 0.00148
2024-12-20 12:21:17.681693: Validation loss did not improve from -0.56901. Patience: 17/50
2024-12-20 12:21:17.682883: train_loss -0.7614
2024-12-20 12:21:17.683913: val_loss -0.522
2024-12-20 12:21:17.684855: Pseudo dice [0.7267]
2024-12-20 12:21:17.685851: Epoch time: 332.92 s
2024-12-20 12:21:19.186122: 
2024-12-20 12:21:19.187800: Epoch 133
2024-12-20 12:21:19.189440: Current learning rate: 0.00141
2024-12-20 12:27:03.740749: Validation loss did not improve from -0.56901. Patience: 18/50
2024-12-20 12:27:03.741698: train_loss -0.762
2024-12-20 12:27:03.742547: val_loss -0.5439
2024-12-20 12:27:03.743312: Pseudo dice [0.7473]
2024-12-20 12:27:03.744208: Epoch time: 344.56 s
2024-12-20 12:27:05.189419: 
2024-12-20 12:27:05.191916: Epoch 134
2024-12-20 12:27:05.193470: Current learning rate: 0.00133
2024-12-20 12:31:46.057525: Validation loss did not improve from -0.56901. Patience: 19/50
2024-12-20 12:31:46.058936: train_loss -0.7637
2024-12-20 12:31:46.060475: val_loss -0.5234
2024-12-20 12:31:46.061669: Pseudo dice [0.7308]
2024-12-20 12:31:46.062905: Epoch time: 280.87 s
2024-12-20 12:31:48.171318: 
2024-12-20 12:31:48.174058: Epoch 135
2024-12-20 12:31:48.175220: Current learning rate: 0.00126
2024-12-20 12:37:19.232151: Validation loss did not improve from -0.56901. Patience: 20/50
2024-12-20 12:37:19.233515: train_loss -0.7671
2024-12-20 12:37:19.234420: val_loss -0.5574
2024-12-20 12:37:19.235146: Pseudo dice [0.7468]
2024-12-20 12:37:19.235878: Epoch time: 331.06 s
2024-12-20 12:37:20.704107: 
2024-12-20 12:37:20.705674: Epoch 136
2024-12-20 12:37:20.706716: Current learning rate: 0.00118
2024-12-20 12:42:58.302737: Validation loss did not improve from -0.56901. Patience: 21/50
2024-12-20 12:42:58.303865: train_loss -0.7631
2024-12-20 12:42:58.304764: val_loss -0.5356
2024-12-20 12:42:58.305516: Pseudo dice [0.7362]
2024-12-20 12:42:58.306603: Epoch time: 337.6 s
2024-12-20 12:42:59.843290: 
2024-12-20 12:42:59.845333: Epoch 137
2024-12-20 12:42:59.847207: Current learning rate: 0.00111
2024-12-20 12:48:38.894228: Validation loss did not improve from -0.56901. Patience: 22/50
2024-12-20 12:48:38.895242: train_loss -0.7706
2024-12-20 12:48:38.897017: val_loss -0.513
2024-12-20 12:48:38.899102: Pseudo dice [0.7201]
2024-12-20 12:48:38.900267: Epoch time: 339.05 s
2024-12-20 12:48:40.400751: 
2024-12-20 12:48:40.402010: Epoch 138
2024-12-20 12:48:40.403122: Current learning rate: 0.00103
2024-12-20 12:54:05.198035: Validation loss did not improve from -0.56901. Patience: 23/50
2024-12-20 12:54:05.198886: train_loss -0.768
2024-12-20 12:54:05.199738: val_loss -0.5222
2024-12-20 12:54:05.200414: Pseudo dice [0.7339]
2024-12-20 12:54:05.201145: Epoch time: 324.8 s
2024-12-20 12:54:07.057224: 
2024-12-20 12:54:07.058540: Epoch 139
2024-12-20 12:54:07.059374: Current learning rate: 0.00095
2024-12-20 12:59:51.031944: Validation loss did not improve from -0.56901. Patience: 24/50
2024-12-20 12:59:51.034065: train_loss -0.7671
2024-12-20 12:59:51.035453: val_loss -0.5278
2024-12-20 12:59:51.036766: Pseudo dice [0.7505]
2024-12-20 12:59:51.037993: Epoch time: 343.98 s
2024-12-20 12:59:53.004920: 
2024-12-20 12:59:53.006592: Epoch 140
2024-12-20 12:59:53.007623: Current learning rate: 0.00087
2024-12-20 13:06:37.760079: Validation loss did not improve from -0.56901. Patience: 25/50
2024-12-20 13:06:37.761111: train_loss -0.77
2024-12-20 13:06:37.763247: val_loss -0.5136
2024-12-20 13:06:37.764357: Pseudo dice [0.7257]
2024-12-20 13:06:37.765590: Epoch time: 404.76 s
2024-12-20 13:06:39.242623: 
2024-12-20 13:06:39.244686: Epoch 141
2024-12-20 13:06:39.246166: Current learning rate: 0.00079
2024-12-20 13:12:32.025697: Validation loss did not improve from -0.56901. Patience: 26/50
2024-12-20 13:12:32.026761: train_loss -0.7675
2024-12-20 13:12:32.028129: val_loss -0.5221
2024-12-20 13:12:32.029854: Pseudo dice [0.7394]
2024-12-20 13:12:32.031341: Epoch time: 352.79 s
2024-12-20 13:12:33.596222: 
2024-12-20 13:12:33.597490: Epoch 142
2024-12-20 13:12:33.598285: Current learning rate: 0.00071
2024-12-20 13:17:23.441684: Validation loss did not improve from -0.56901. Patience: 27/50
2024-12-20 13:17:23.442814: train_loss -0.7705
2024-12-20 13:17:23.443793: val_loss -0.518
2024-12-20 13:17:23.444633: Pseudo dice [0.727]
2024-12-20 13:17:23.445510: Epoch time: 289.85 s
2024-12-20 13:17:25.131250: 
2024-12-20 13:17:25.132692: Epoch 143
2024-12-20 13:17:25.133844: Current learning rate: 0.00063
2024-12-20 13:23:03.390195: Validation loss did not improve from -0.56901. Patience: 28/50
2024-12-20 13:23:03.391227: train_loss -0.7726
2024-12-20 13:23:03.391970: val_loss -0.5369
2024-12-20 13:23:03.392760: Pseudo dice [0.7378]
2024-12-20 13:23:03.393525: Epoch time: 338.26 s
2024-12-20 13:23:04.979982: 
2024-12-20 13:23:04.982040: Epoch 144
2024-12-20 13:23:04.984364: Current learning rate: 0.00055
2024-12-20 13:28:56.968964: Validation loss did not improve from -0.56901. Patience: 29/50
2024-12-20 13:28:56.970624: train_loss -0.7747
2024-12-20 13:28:56.972086: val_loss -0.5308
2024-12-20 13:28:56.973209: Pseudo dice [0.7432]
2024-12-20 13:28:56.974383: Epoch time: 351.99 s
2024-12-20 13:28:58.839964: 
2024-12-20 13:28:58.841957: Epoch 145
2024-12-20 13:28:58.843815: Current learning rate: 0.00047
2024-12-20 13:34:05.936621: Validation loss did not improve from -0.56901. Patience: 30/50
2024-12-20 13:34:05.937507: train_loss -0.7728
2024-12-20 13:34:05.938404: val_loss -0.5378
2024-12-20 13:34:05.939190: Pseudo dice [0.7388]
2024-12-20 13:34:05.939906: Epoch time: 307.1 s
2024-12-20 13:34:07.424087: 
2024-12-20 13:34:07.424924: Epoch 146
2024-12-20 13:34:07.425714: Current learning rate: 0.00038
2024-12-20 13:40:12.530591: Validation loss did not improve from -0.56901. Patience: 31/50
2024-12-20 13:40:12.531696: train_loss -0.7699
2024-12-20 13:40:12.532804: val_loss -0.5085
2024-12-20 13:40:12.534635: Pseudo dice [0.728]
2024-12-20 13:40:12.536063: Epoch time: 365.11 s
2024-12-20 13:40:14.032567: 
2024-12-20 13:40:14.033913: Epoch 147
2024-12-20 13:40:14.034801: Current learning rate: 0.0003
2024-12-20 13:46:34.500371: Validation loss did not improve from -0.56901. Patience: 32/50
2024-12-20 13:46:34.501271: train_loss -0.776
2024-12-20 13:46:34.502461: val_loss -0.514
2024-12-20 13:46:34.503453: Pseudo dice [0.7318]
2024-12-20 13:46:34.504472: Epoch time: 380.47 s
2024-12-20 13:46:35.994320: 
2024-12-20 13:46:35.996963: Epoch 148
2024-12-20 13:46:35.998370: Current learning rate: 0.00021
2024-12-20 13:52:18.314485: Validation loss did not improve from -0.56901. Patience: 33/50
2024-12-20 13:52:18.315449: train_loss -0.7714
2024-12-20 13:52:18.317218: val_loss -0.5429
2024-12-20 13:52:18.318724: Pseudo dice [0.7477]
2024-12-20 13:52:18.320377: Epoch time: 342.32 s
2024-12-20 13:52:19.793334: 
2024-12-20 13:52:19.794800: Epoch 149
2024-12-20 13:52:19.795709: Current learning rate: 0.00011
2024-12-20 13:57:26.809768: Validation loss did not improve from -0.56901. Patience: 34/50
2024-12-20 13:57:26.810714: train_loss -0.7731
2024-12-20 13:57:26.812011: val_loss -0.5243
2024-12-20 13:57:26.813081: Pseudo dice [0.7455]
2024-12-20 13:57:26.813943: Epoch time: 307.02 s
2024-12-20 13:57:29.492824: Training done.
2024-12-20 13:57:30.132552: Using splits from existing split file: /home/gridsan/nchutisilp/datasets/nnUNet_Datasets/nnUNet_preprocessed/Dataset308_Sohee_Calcium_OCT_CrossValidation/splits_final_80.json
2024-12-20 13:57:30.145437: The split file contains 5 splits.
2024-12-20 13:57:30.146295: Desired fold for training: 3
2024-12-20 13:57:30.146924: This split has 6 training and 3 validation cases.
2024-12-20 13:57:30.147834: predicting 101-019
2024-12-20 13:57:30.207265: 101-019, shape torch.Size([1, 375, 498, 498]), rank 0
2024-12-20 14:00:16.993643: predicting 101-044
2024-12-20 14:00:17.023124: 101-044, shape torch.Size([1, 404, 498, 498]), rank 0
2024-12-20 14:02:54.496040: predicting 401-004
2024-12-20 14:02:54.523972: 401-004, shape torch.Size([1, 375, 498, 498]), rank 0
2024-12-20 14:06:01.125954: Validation complete
2024-12-20 14:06:01.126673: Mean Validation Dice:  0.7236819908758306
2024-12-19 23:59:29.618454: unpacking done...
2024-12-19 23:59:29.977694: Unable to plot network architecture: nnUNet_compile is enabled!
2024-12-19 23:59:30.136364: 
2024-12-19 23:59:30.138684: Epoch 0
2024-12-19 23:59:30.139957: Current learning rate: 0.01
2024-12-20 00:07:56.165755: Validation loss improved from 1000.00000 to -0.19562! Patience: 0/50
2024-12-20 00:07:56.166687: train_loss -0.0831
2024-12-20 00:07:56.168212: val_loss -0.1956
2024-12-20 00:07:56.169802: Pseudo dice [0.5392]
2024-12-20 00:07:56.171169: Epoch time: 506.03 s
2024-12-20 00:07:56.172557: Yayy! New best EMA pseudo Dice: 0.5392
2024-12-20 00:07:57.885781: 
2024-12-20 00:07:57.887065: Epoch 1
2024-12-20 00:07:57.888180: Current learning rate: 0.00994
2024-12-20 00:14:52.562853: Validation loss did not improve from -0.19562. Patience: 1/50
2024-12-20 00:14:52.563806: train_loss -0.2476
2024-12-20 00:14:52.565313: val_loss -0.1885
2024-12-20 00:14:52.566501: Pseudo dice [0.5137]
2024-12-20 00:14:52.567638: Epoch time: 414.68 s
2024-12-20 00:14:53.980651: 
2024-12-20 00:14:53.981718: Epoch 2
2024-12-20 00:14:53.982558: Current learning rate: 0.00988
2024-12-20 00:21:29.406586: Validation loss improved from -0.19562 to -0.28635! Patience: 1/50
2024-12-20 00:21:29.407691: train_loss -0.3052
2024-12-20 00:21:29.408600: val_loss -0.2864
2024-12-20 00:21:29.409347: Pseudo dice [0.5842]
2024-12-20 00:21:29.410071: Epoch time: 395.43 s
2024-12-20 00:21:29.410868: Yayy! New best EMA pseudo Dice: 0.5414
2024-12-20 00:21:31.373747: 
2024-12-20 00:21:31.375048: Epoch 3
2024-12-20 00:21:31.375884: Current learning rate: 0.00982
2024-12-20 00:28:39.580634: Validation loss did not improve from -0.28635. Patience: 1/50
2024-12-20 00:28:39.581497: train_loss -0.3314
2024-12-20 00:28:39.582229: val_loss -0.2442
2024-12-20 00:28:39.582883: Pseudo dice [0.5786]
2024-12-20 00:28:39.583633: Epoch time: 428.21 s
2024-12-20 00:28:39.584455: Yayy! New best EMA pseudo Dice: 0.5451
2024-12-20 00:28:41.494495: 
2024-12-20 00:28:41.495982: Epoch 4
2024-12-20 00:28:41.497005: Current learning rate: 0.00976
2024-12-20 00:35:40.660387: Validation loss improved from -0.28635 to -0.29817! Patience: 1/50
2024-12-20 00:35:40.661507: train_loss -0.3432
2024-12-20 00:35:40.663093: val_loss -0.2982
2024-12-20 00:35:40.664414: Pseudo dice [0.6213]
2024-12-20 00:35:40.665683: Epoch time: 419.17 s
2024-12-20 00:35:41.070545: Yayy! New best EMA pseudo Dice: 0.5527
2024-12-20 00:35:42.960024: 
2024-12-20 00:35:42.961440: Epoch 5
2024-12-20 00:35:42.962408: Current learning rate: 0.0097
2024-12-20 00:42:46.058854: Validation loss improved from -0.29817 to -0.32883! Patience: 0/50
2024-12-20 00:42:46.059760: train_loss -0.3954
2024-12-20 00:42:46.060525: val_loss -0.3288
2024-12-20 00:42:46.061206: Pseudo dice [0.6133]
2024-12-20 00:42:46.061974: Epoch time: 423.1 s
2024-12-20 00:42:46.062661: Yayy! New best EMA pseudo Dice: 0.5588
2024-12-20 00:42:47.791765: 
2024-12-20 00:42:47.793279: Epoch 6
2024-12-20 00:42:47.794124: Current learning rate: 0.00964
2024-12-20 00:49:56.419501: Validation loss did not improve from -0.32883. Patience: 1/50
2024-12-20 00:49:56.420424: train_loss -0.4181
2024-12-20 00:49:56.421463: val_loss -0.3257
2024-12-20 00:49:56.422523: Pseudo dice [0.6268]
2024-12-20 00:49:56.423491: Epoch time: 428.63 s
2024-12-20 00:49:56.424280: Yayy! New best EMA pseudo Dice: 0.5656
2024-12-20 00:49:58.289292: 
2024-12-20 00:49:58.290449: Epoch 7
2024-12-20 00:49:58.291243: Current learning rate: 0.00958
2024-12-20 00:57:30.079471: Validation loss did not improve from -0.32883. Patience: 2/50
2024-12-20 00:57:30.080487: train_loss -0.4324
2024-12-20 00:57:30.081388: val_loss -0.3186
2024-12-20 00:57:30.082316: Pseudo dice [0.6286]
2024-12-20 00:57:30.083188: Epoch time: 451.79 s
2024-12-20 00:57:30.083992: Yayy! New best EMA pseudo Dice: 0.5719
2024-12-20 00:57:32.028699: 
2024-12-20 00:57:32.030554: Epoch 8
2024-12-20 00:57:32.031540: Current learning rate: 0.00952
2024-12-20 01:04:59.919014: Validation loss improved from -0.32883 to -0.36035! Patience: 2/50
2024-12-20 01:04:59.930413: train_loss -0.4628
2024-12-20 01:04:59.932067: val_loss -0.3603
2024-12-20 01:04:59.933552: Pseudo dice [0.6378]
2024-12-20 01:04:59.935076: Epoch time: 447.9 s
2024-12-20 01:04:59.936629: Yayy! New best EMA pseudo Dice: 0.5785
2024-12-20 01:05:02.299560: 
2024-12-20 01:05:02.301099: Epoch 9
2024-12-20 01:05:02.302439: Current learning rate: 0.00946
2024-12-20 01:12:19.978657: Validation loss improved from -0.36035 to -0.36479! Patience: 0/50
2024-12-20 01:12:19.979596: train_loss -0.4756
2024-12-20 01:12:19.980404: val_loss -0.3648
2024-12-20 01:12:19.981059: Pseudo dice [0.6353]
2024-12-20 01:12:19.981730: Epoch time: 437.68 s
2024-12-20 01:12:20.440159: Yayy! New best EMA pseudo Dice: 0.5842
2024-12-20 01:12:22.317243: 
2024-12-20 01:12:22.318656: Epoch 10
2024-12-20 01:12:22.319393: Current learning rate: 0.0094
2024-12-20 01:19:40.103860: Validation loss did not improve from -0.36479. Patience: 1/50
2024-12-20 01:19:40.105800: train_loss -0.4678
2024-12-20 01:19:40.107381: val_loss -0.263
2024-12-20 01:19:40.108417: Pseudo dice [0.604]
2024-12-20 01:19:40.109684: Epoch time: 437.79 s
2024-12-20 01:19:40.111045: Yayy! New best EMA pseudo Dice: 0.5862
2024-12-20 01:19:41.973596: 
2024-12-20 01:19:41.974669: Epoch 11
2024-12-20 01:19:41.975708: Current learning rate: 0.00934
2024-12-20 01:27:09.136489: Validation loss improved from -0.36479 to -0.40854! Patience: 1/50
2024-12-20 01:27:09.137968: train_loss -0.4944
2024-12-20 01:27:09.138930: val_loss -0.4085
2024-12-20 01:27:09.139732: Pseudo dice [0.6705]
2024-12-20 01:27:09.140529: Epoch time: 447.17 s
2024-12-20 01:27:09.141249: Yayy! New best EMA pseudo Dice: 0.5946
2024-12-20 01:27:10.897342: 
2024-12-20 01:27:10.898995: Epoch 12
2024-12-20 01:27:10.900112: Current learning rate: 0.00928
2024-12-20 01:34:38.362470: Validation loss improved from -0.40854 to -0.41019! Patience: 0/50
2024-12-20 01:34:38.363592: train_loss -0.4972
2024-12-20 01:34:38.364825: val_loss -0.4102
2024-12-20 01:34:38.365856: Pseudo dice [0.6465]
2024-12-20 01:34:38.367135: Epoch time: 447.47 s
2024-12-20 01:34:38.368129: Yayy! New best EMA pseudo Dice: 0.5998
2024-12-20 01:34:40.462127: 
2024-12-20 01:34:40.463547: Epoch 13
2024-12-20 01:34:40.464766: Current learning rate: 0.00922
2024-12-20 01:42:03.139106: Validation loss did not improve from -0.41019. Patience: 1/50
2024-12-20 01:42:03.140030: train_loss -0.4913
2024-12-20 01:42:03.140710: val_loss -0.3648
2024-12-20 01:42:03.141354: Pseudo dice [0.6481]
2024-12-20 01:42:03.141977: Epoch time: 442.68 s
2024-12-20 01:42:03.142585: Yayy! New best EMA pseudo Dice: 0.6046
2024-12-20 01:42:05.061185: 
2024-12-20 01:42:05.062722: Epoch 14
2024-12-20 01:42:05.063622: Current learning rate: 0.00916
2024-12-20 01:48:45.509957: Validation loss did not improve from -0.41019. Patience: 2/50
2024-12-20 01:48:45.511089: train_loss -0.5213
2024-12-20 01:48:45.512403: val_loss -0.3368
2024-12-20 01:48:45.513729: Pseudo dice [0.6356]
2024-12-20 01:48:45.515028: Epoch time: 400.45 s
2024-12-20 01:48:45.918232: Yayy! New best EMA pseudo Dice: 0.6077
2024-12-20 01:48:47.823399: 
2024-12-20 01:48:47.824652: Epoch 15
2024-12-20 01:48:47.825473: Current learning rate: 0.0091
2024-12-20 01:54:55.789548: Validation loss improved from -0.41019 to -0.47098! Patience: 2/50
2024-12-20 01:54:55.790646: train_loss -0.5394
2024-12-20 01:54:55.791519: val_loss -0.471
2024-12-20 01:54:55.792172: Pseudo dice [0.6926]
2024-12-20 01:54:55.792833: Epoch time: 367.97 s
2024-12-20 01:54:55.793527: Yayy! New best EMA pseudo Dice: 0.6162
2024-12-20 01:54:57.687546: 
2024-12-20 01:54:57.688708: Epoch 16
2024-12-20 01:54:57.689521: Current learning rate: 0.00903
2024-12-20 02:01:13.302493: Validation loss did not improve from -0.47098. Patience: 1/50
2024-12-20 02:01:13.303572: train_loss -0.529
2024-12-20 02:01:13.304619: val_loss -0.3588
2024-12-20 02:01:13.305516: Pseudo dice [0.6426]
2024-12-20 02:01:13.306462: Epoch time: 375.62 s
2024-12-20 02:01:13.307395: Yayy! New best EMA pseudo Dice: 0.6188
2024-12-20 02:01:15.289702: 
2024-12-20 02:01:15.291219: Epoch 17
2024-12-20 02:01:15.292249: Current learning rate: 0.00897
2024-12-20 02:07:49.559700: Validation loss did not improve from -0.47098. Patience: 2/50
2024-12-20 02:07:49.560770: train_loss -0.5312
2024-12-20 02:07:49.562749: val_loss -0.3144
2024-12-20 02:07:49.564292: Pseudo dice [0.6287]
2024-12-20 02:07:49.565351: Epoch time: 394.27 s
2024-12-20 02:07:49.566254: Yayy! New best EMA pseudo Dice: 0.6198
2024-12-20 02:07:51.490128: 
2024-12-20 02:07:51.491633: Epoch 18
2024-12-20 02:07:51.493154: Current learning rate: 0.00891
2024-12-20 02:14:28.631727: Validation loss did not improve from -0.47098. Patience: 3/50
2024-12-20 02:14:28.633468: train_loss -0.5456
2024-12-20 02:14:28.634794: val_loss -0.4553
2024-12-20 02:14:28.635492: Pseudo dice [0.6919]
2024-12-20 02:14:28.636551: Epoch time: 397.14 s
2024-12-20 02:14:28.637175: Yayy! New best EMA pseudo Dice: 0.627
2024-12-20 02:14:30.878311: 
2024-12-20 02:14:30.879588: Epoch 19
2024-12-20 02:14:30.880589: Current learning rate: 0.00885
2024-12-20 02:20:25.464103: Validation loss did not improve from -0.47098. Patience: 4/50
2024-12-20 02:20:25.464995: train_loss -0.5392
2024-12-20 02:20:25.465896: val_loss -0.4423
2024-12-20 02:20:25.466656: Pseudo dice [0.6854]
2024-12-20 02:20:25.467364: Epoch time: 354.59 s
2024-12-20 02:20:25.904034: Yayy! New best EMA pseudo Dice: 0.6329
2024-12-20 02:20:27.829156: 
2024-12-20 02:20:27.830369: Epoch 20
2024-12-20 02:20:27.831164: Current learning rate: 0.00879
2024-12-20 02:26:45.077330: Validation loss did not improve from -0.47098. Patience: 5/50
2024-12-20 02:26:45.078349: train_loss -0.5456
2024-12-20 02:26:45.079146: val_loss -0.4047
2024-12-20 02:26:45.079871: Pseudo dice [0.6709]
2024-12-20 02:26:45.080724: Epoch time: 377.25 s
2024-12-20 02:26:45.081482: Yayy! New best EMA pseudo Dice: 0.6367
2024-12-20 02:26:47.132753: 
2024-12-20 02:26:47.134326: Epoch 21
2024-12-20 02:26:47.135967: Current learning rate: 0.00873
2024-12-20 02:33:19.788087: Validation loss did not improve from -0.47098. Patience: 6/50
2024-12-20 02:33:19.788981: train_loss -0.5488
2024-12-20 02:33:19.790020: val_loss -0.3202
2024-12-20 02:33:19.791055: Pseudo dice [0.6413]
2024-12-20 02:33:19.792099: Epoch time: 392.66 s
2024-12-20 02:33:19.793145: Yayy! New best EMA pseudo Dice: 0.6371
2024-12-20 02:33:21.547006: 
2024-12-20 02:33:21.548468: Epoch 22
2024-12-20 02:33:21.549858: Current learning rate: 0.00867
2024-12-20 02:39:41.448177: Validation loss did not improve from -0.47098. Patience: 7/50
2024-12-20 02:39:41.449237: train_loss -0.5676
2024-12-20 02:39:41.450225: val_loss -0.4524
2024-12-20 02:39:41.451137: Pseudo dice [0.6971]
2024-12-20 02:39:41.452051: Epoch time: 379.9 s
2024-12-20 02:39:41.453072: Yayy! New best EMA pseudo Dice: 0.6431
2024-12-20 02:39:43.214786: 
2024-12-20 02:39:43.215671: Epoch 23
2024-12-20 02:39:43.216386: Current learning rate: 0.00861
2024-12-20 02:46:21.835196: Validation loss did not improve from -0.47098. Patience: 8/50
2024-12-20 02:46:21.836027: train_loss -0.5787
2024-12-20 02:46:21.837021: val_loss -0.4626
2024-12-20 02:46:21.837893: Pseudo dice [0.6876]
2024-12-20 02:46:21.838817: Epoch time: 398.62 s
2024-12-20 02:46:21.839800: Yayy! New best EMA pseudo Dice: 0.6476
2024-12-20 02:46:23.654423: 
2024-12-20 02:46:23.656199: Epoch 24
2024-12-20 02:46:23.657328: Current learning rate: 0.00855
2024-12-20 02:52:57.310618: Validation loss did not improve from -0.47098. Patience: 9/50
2024-12-20 02:52:57.311998: train_loss -0.5793
2024-12-20 02:52:57.312731: val_loss -0.4323
2024-12-20 02:52:57.313461: Pseudo dice [0.6706]
2024-12-20 02:52:57.314189: Epoch time: 393.66 s
2024-12-20 02:52:57.772942: Yayy! New best EMA pseudo Dice: 0.6499
2024-12-20 02:52:59.576300: 
2024-12-20 02:52:59.577568: Epoch 25
2024-12-20 02:52:59.578185: Current learning rate: 0.00849
2024-12-20 02:59:44.656093: Validation loss did not improve from -0.47098. Patience: 10/50
2024-12-20 02:59:44.657375: train_loss -0.5734
2024-12-20 02:59:44.659116: val_loss -0.4702
2024-12-20 02:59:44.660453: Pseudo dice [0.693]
2024-12-20 02:59:44.661714: Epoch time: 405.08 s
2024-12-20 02:59:44.662830: Yayy! New best EMA pseudo Dice: 0.6542
2024-12-20 02:59:46.429513: 
2024-12-20 02:59:46.431469: Epoch 26
2024-12-20 02:59:46.432946: Current learning rate: 0.00843
2024-12-20 03:05:59.194673: Validation loss did not improve from -0.47098. Patience: 11/50
2024-12-20 03:05:59.195282: train_loss -0.576
2024-12-20 03:05:59.195909: val_loss -0.4463
2024-12-20 03:05:59.196651: Pseudo dice [0.6965]
2024-12-20 03:05:59.197287: Epoch time: 372.77 s
2024-12-20 03:05:59.197922: Yayy! New best EMA pseudo Dice: 0.6584
2024-12-20 03:06:01.000846: 
2024-12-20 03:06:01.001705: Epoch 27
2024-12-20 03:06:01.002513: Current learning rate: 0.00836
2024-12-20 03:12:29.118282: Validation loss did not improve from -0.47098. Patience: 12/50
2024-12-20 03:12:29.119053: train_loss -0.5855
2024-12-20 03:12:29.119935: val_loss -0.43
2024-12-20 03:12:29.120641: Pseudo dice [0.672]
2024-12-20 03:12:29.121369: Epoch time: 388.12 s
2024-12-20 03:12:29.122126: Yayy! New best EMA pseudo Dice: 0.6598
2024-12-20 03:12:30.945632: 
2024-12-20 03:12:30.946710: Epoch 28
2024-12-20 03:12:30.947475: Current learning rate: 0.0083
2024-12-20 03:18:59.287198: Validation loss did not improve from -0.47098. Patience: 13/50
2024-12-20 03:18:59.289243: train_loss -0.5881
2024-12-20 03:18:59.291153: val_loss -0.4342
2024-12-20 03:18:59.292353: Pseudo dice [0.6828]
2024-12-20 03:18:59.294390: Epoch time: 388.34 s
2024-12-20 03:18:59.295942: Yayy! New best EMA pseudo Dice: 0.6621
2024-12-20 03:19:01.541225: 
2024-12-20 03:19:01.542363: Epoch 29
2024-12-20 03:19:01.543103: Current learning rate: 0.00824
2024-12-20 03:25:46.422053: Validation loss did not improve from -0.47098. Patience: 14/50
2024-12-20 03:25:46.423346: train_loss -0.6019
2024-12-20 03:25:46.424392: val_loss -0.4703
2024-12-20 03:25:46.425247: Pseudo dice [0.7073]
2024-12-20 03:25:46.426153: Epoch time: 404.88 s
2024-12-20 03:25:46.789553: Yayy! New best EMA pseudo Dice: 0.6666
2024-12-20 03:25:48.638887: 
2024-12-20 03:25:48.639928: Epoch 30
2024-12-20 03:25:48.640737: Current learning rate: 0.00818
2024-12-20 03:32:05.184925: Validation loss improved from -0.47098 to -0.47710! Patience: 14/50
2024-12-20 03:32:05.185679: train_loss -0.6035
2024-12-20 03:32:05.186374: val_loss -0.4771
2024-12-20 03:32:05.187129: Pseudo dice [0.7018]
2024-12-20 03:32:05.187803: Epoch time: 376.55 s
2024-12-20 03:32:05.188413: Yayy! New best EMA pseudo Dice: 0.6701
2024-12-20 03:32:06.963965: 
2024-12-20 03:32:06.965245: Epoch 31
2024-12-20 03:32:06.966040: Current learning rate: 0.00812
2024-12-20 03:38:28.968259: Validation loss improved from -0.47710 to -0.52890! Patience: 0/50
2024-12-20 03:38:28.969177: train_loss -0.5997
2024-12-20 03:38:28.969969: val_loss -0.5289
2024-12-20 03:38:28.970755: Pseudo dice [0.733]
2024-12-20 03:38:28.971590: Epoch time: 382.01 s
2024-12-20 03:38:28.972349: Yayy! New best EMA pseudo Dice: 0.6764
2024-12-20 03:38:30.760381: 
2024-12-20 03:38:30.761650: Epoch 32
2024-12-20 03:38:30.762451: Current learning rate: 0.00806
2024-12-20 03:44:51.646416: Validation loss did not improve from -0.52890. Patience: 1/50
2024-12-20 03:44:51.647146: train_loss -0.6002
2024-12-20 03:44:51.647936: val_loss -0.4529
2024-12-20 03:44:51.648805: Pseudo dice [0.69]
2024-12-20 03:44:51.649684: Epoch time: 380.89 s
2024-12-20 03:44:51.650500: Yayy! New best EMA pseudo Dice: 0.6778
2024-12-20 03:44:53.553868: 
2024-12-20 03:44:53.555125: Epoch 33
2024-12-20 03:44:53.555954: Current learning rate: 0.008
2024-12-20 03:51:23.269441: Validation loss did not improve from -0.52890. Patience: 2/50
2024-12-20 03:51:23.270411: train_loss -0.6055
2024-12-20 03:51:23.271415: val_loss -0.4867
2024-12-20 03:51:23.272312: Pseudo dice [0.7121]
2024-12-20 03:51:23.273185: Epoch time: 389.72 s
2024-12-20 03:51:23.273872: Yayy! New best EMA pseudo Dice: 0.6812
2024-12-20 03:51:25.144752: 
2024-12-20 03:51:25.146540: Epoch 34
2024-12-20 03:51:25.147639: Current learning rate: 0.00793
2024-12-20 03:57:40.475221: Validation loss did not improve from -0.52890. Patience: 3/50
2024-12-20 03:57:40.476088: train_loss -0.6097
2024-12-20 03:57:40.477130: val_loss -0.4924
2024-12-20 03:57:40.477926: Pseudo dice [0.7127]
2024-12-20 03:57:40.478546: Epoch time: 375.33 s
2024-12-20 03:57:40.867730: Yayy! New best EMA pseudo Dice: 0.6844
2024-12-20 03:57:42.707200: 
2024-12-20 03:57:42.708218: Epoch 35
2024-12-20 03:57:42.709202: Current learning rate: 0.00787
2024-12-20 04:04:16.006251: Validation loss did not improve from -0.52890. Patience: 4/50
2024-12-20 04:04:16.007308: train_loss -0.6135
2024-12-20 04:04:16.008229: val_loss -0.5074
2024-12-20 04:04:16.009094: Pseudo dice [0.7121]
2024-12-20 04:04:16.009868: Epoch time: 393.3 s
2024-12-20 04:04:16.010604: Yayy! New best EMA pseudo Dice: 0.6871
2024-12-20 04:04:17.944122: 
2024-12-20 04:04:17.947033: Epoch 36
2024-12-20 04:04:17.949120: Current learning rate: 0.00781
2024-12-20 04:10:36.764510: Validation loss did not improve from -0.52890. Patience: 5/50
2024-12-20 04:10:36.765406: train_loss -0.6169
2024-12-20 04:10:36.766351: val_loss -0.4453
2024-12-20 04:10:36.767164: Pseudo dice [0.6855]
2024-12-20 04:10:36.768202: Epoch time: 378.82 s
2024-12-20 04:10:38.224338: 
2024-12-20 04:10:38.226050: Epoch 37
2024-12-20 04:10:38.227477: Current learning rate: 0.00775
2024-12-20 04:17:07.813595: Validation loss did not improve from -0.52890. Patience: 6/50
2024-12-20 04:17:07.814566: train_loss -0.6202
2024-12-20 04:17:07.816272: val_loss -0.5099
2024-12-20 04:17:07.817593: Pseudo dice [0.7236]
2024-12-20 04:17:07.819009: Epoch time: 389.59 s
2024-12-20 04:17:07.820203: Yayy! New best EMA pseudo Dice: 0.6906
2024-12-20 04:17:09.705583: 
2024-12-20 04:17:09.707112: Epoch 38
2024-12-20 04:17:09.708329: Current learning rate: 0.00769
2024-12-20 04:23:39.470894: Validation loss did not improve from -0.52890. Patience: 7/50
2024-12-20 04:23:39.472385: train_loss -0.6248
2024-12-20 04:23:39.474412: val_loss -0.4281
2024-12-20 04:23:39.475175: Pseudo dice [0.6799]
2024-12-20 04:23:39.476008: Epoch time: 389.77 s
2024-12-20 04:23:40.994541: 
2024-12-20 04:23:40.995520: Epoch 39
2024-12-20 04:23:40.996488: Current learning rate: 0.00763
2024-12-20 04:30:20.077345: Validation loss did not improve from -0.52890. Patience: 8/50
2024-12-20 04:30:20.078257: train_loss -0.6247
2024-12-20 04:30:20.079335: val_loss -0.4734
2024-12-20 04:30:20.080224: Pseudo dice [0.7127]
2024-12-20 04:30:20.081113: Epoch time: 399.09 s
2024-12-20 04:30:20.945596: Yayy! New best EMA pseudo Dice: 0.6919
2024-12-20 04:30:25.563161: 
2024-12-20 04:30:25.564447: Epoch 40
2024-12-20 04:30:25.565191: Current learning rate: 0.00756
2024-12-20 04:37:53.265098: Validation loss did not improve from -0.52890. Patience: 9/50
2024-12-20 04:37:53.266058: train_loss -0.6199
2024-12-20 04:37:53.266896: val_loss -0.4672
2024-12-20 04:37:53.267603: Pseudo dice [0.6934]
2024-12-20 04:37:53.268512: Epoch time: 447.7 s
2024-12-20 04:37:53.269349: Yayy! New best EMA pseudo Dice: 0.692
2024-12-20 04:37:55.339766: 
2024-12-20 04:37:55.341238: Epoch 41
2024-12-20 04:37:55.342149: Current learning rate: 0.0075
2024-12-20 04:44:08.988905: Validation loss did not improve from -0.52890. Patience: 10/50
2024-12-20 04:44:08.989915: train_loss -0.6185
2024-12-20 04:44:08.990723: val_loss -0.4579
2024-12-20 04:44:08.991509: Pseudo dice [0.6802]
2024-12-20 04:44:08.992190: Epoch time: 373.65 s
2024-12-20 04:44:10.493179: 
2024-12-20 04:44:10.494514: Epoch 42
2024-12-20 04:44:10.495387: Current learning rate: 0.00744
2024-12-20 04:50:29.461360: Validation loss did not improve from -0.52890. Patience: 11/50
2024-12-20 04:50:29.462185: train_loss -0.6158
2024-12-20 04:50:29.462964: val_loss -0.4267
2024-12-20 04:50:29.463629: Pseudo dice [0.681]
2024-12-20 04:50:29.464337: Epoch time: 378.97 s
2024-12-20 04:50:30.820663: 
2024-12-20 04:50:30.821853: Epoch 43
2024-12-20 04:50:30.822587: Current learning rate: 0.00738
2024-12-20 04:56:31.714440: Validation loss did not improve from -0.52890. Patience: 12/50
2024-12-20 04:56:31.715338: train_loss -0.631
2024-12-20 04:56:31.716324: val_loss -0.4782
2024-12-20 04:56:31.717148: Pseudo dice [0.7091]
2024-12-20 04:56:31.718014: Epoch time: 360.9 s
2024-12-20 04:56:33.089037: 
2024-12-20 04:56:33.091233: Epoch 44
2024-12-20 04:56:33.093866: Current learning rate: 0.00732
2024-12-20 05:02:42.029489: Validation loss did not improve from -0.52890. Patience: 13/50
2024-12-20 05:02:42.030371: train_loss -0.6477
2024-12-20 05:02:42.031316: val_loss -0.5028
2024-12-20 05:02:42.032206: Pseudo dice [0.7146]
2024-12-20 05:02:42.033085: Epoch time: 368.94 s
2024-12-20 05:02:42.481798: Yayy! New best EMA pseudo Dice: 0.6941
2024-12-20 05:02:44.355073: 
2024-12-20 05:02:44.356284: Epoch 45
2024-12-20 05:02:44.357231: Current learning rate: 0.00725
2024-12-20 05:09:22.788147: Validation loss did not improve from -0.52890. Patience: 14/50
2024-12-20 05:09:22.788982: train_loss -0.6378
2024-12-20 05:09:22.789787: val_loss -0.4871
2024-12-20 05:09:22.790567: Pseudo dice [0.7065]
2024-12-20 05:09:22.791408: Epoch time: 398.44 s
2024-12-20 05:09:22.792210: Yayy! New best EMA pseudo Dice: 0.6953
2024-12-20 05:09:24.676169: 
2024-12-20 05:09:24.677300: Epoch 46
2024-12-20 05:09:24.678236: Current learning rate: 0.00719
2024-12-20 05:15:46.813827: Validation loss did not improve from -0.52890. Patience: 15/50
2024-12-20 05:15:46.814952: train_loss -0.6399
2024-12-20 05:15:46.815957: val_loss -0.516
2024-12-20 05:15:46.816883: Pseudo dice [0.7217]
2024-12-20 05:15:46.817672: Epoch time: 382.14 s
2024-12-20 05:15:46.818397: Yayy! New best EMA pseudo Dice: 0.698
2024-12-20 05:15:48.757014: 
2024-12-20 05:15:48.758296: Epoch 47
2024-12-20 05:15:48.759171: Current learning rate: 0.00713
2024-12-20 05:22:16.473874: Validation loss did not improve from -0.52890. Patience: 16/50
2024-12-20 05:22:16.474806: train_loss -0.6449
2024-12-20 05:22:16.475513: val_loss -0.4686
2024-12-20 05:22:16.476122: Pseudo dice [0.7079]
2024-12-20 05:22:16.476812: Epoch time: 387.72 s
2024-12-20 05:22:16.477463: Yayy! New best EMA pseudo Dice: 0.6989
2024-12-20 05:22:18.277811: 
2024-12-20 05:22:18.279107: Epoch 48
2024-12-20 05:22:18.280033: Current learning rate: 0.00707
2024-12-20 05:28:47.631326: Validation loss did not improve from -0.52890. Patience: 17/50
2024-12-20 05:28:47.632997: train_loss -0.6427
2024-12-20 05:28:47.633875: val_loss -0.4465
2024-12-20 05:28:47.634676: Pseudo dice [0.7007]
2024-12-20 05:28:47.635484: Epoch time: 389.36 s
2024-12-20 05:28:47.636277: Yayy! New best EMA pseudo Dice: 0.6991
2024-12-20 05:28:49.433909: 
2024-12-20 05:28:49.435290: Epoch 49
2024-12-20 05:28:49.436637: Current learning rate: 0.007
2024-12-20 05:35:27.142719: Validation loss did not improve from -0.52890. Patience: 18/50
2024-12-20 05:35:27.144283: train_loss -0.6425
2024-12-20 05:35:27.145732: val_loss -0.5231
2024-12-20 05:35:27.146848: Pseudo dice [0.7335]
2024-12-20 05:35:27.148410: Epoch time: 397.71 s
2024-12-20 05:35:27.540909: Yayy! New best EMA pseudo Dice: 0.7026
2024-12-20 05:35:29.352756: 
2024-12-20 05:35:29.353897: Epoch 50
2024-12-20 05:35:29.354685: Current learning rate: 0.00694
2024-12-20 05:41:47.757764: Validation loss did not improve from -0.52890. Patience: 19/50
2024-12-20 05:41:47.759491: train_loss -0.6566
2024-12-20 05:41:47.761035: val_loss -0.4758
2024-12-20 05:41:47.762476: Pseudo dice [0.7063]
2024-12-20 05:41:47.763794: Epoch time: 378.41 s
2024-12-20 05:41:47.765289: Yayy! New best EMA pseudo Dice: 0.7029
2024-12-20 05:41:50.126894: 
2024-12-20 05:41:50.128267: Epoch 51
2024-12-20 05:41:50.129218: Current learning rate: 0.00688
2024-12-20 05:48:24.277677: Validation loss did not improve from -0.52890. Patience: 20/50
2024-12-20 05:48:24.278630: train_loss -0.6513
2024-12-20 05:48:24.279921: val_loss -0.4627
2024-12-20 05:48:24.281323: Pseudo dice [0.7025]
2024-12-20 05:48:24.282440: Epoch time: 394.15 s
2024-12-20 05:48:25.669917: 
2024-12-20 05:48:25.671221: Epoch 52
2024-12-20 05:48:25.671951: Current learning rate: 0.00682
2024-12-20 05:55:16.951129: Validation loss did not improve from -0.52890. Patience: 21/50
2024-12-20 05:55:16.951993: train_loss -0.6504
2024-12-20 05:55:16.952745: val_loss -0.4995
2024-12-20 05:55:16.953385: Pseudo dice [0.7223]
2024-12-20 05:55:16.954189: Epoch time: 411.28 s
2024-12-20 05:55:16.954892: Yayy! New best EMA pseudo Dice: 0.7048
2024-12-20 05:55:18.809995: 
2024-12-20 05:55:18.811945: Epoch 53
2024-12-20 05:55:18.812914: Current learning rate: 0.00675
2024-12-20 06:02:03.937651: Validation loss did not improve from -0.52890. Patience: 22/50
2024-12-20 06:02:03.938539: train_loss -0.6572
2024-12-20 06:02:03.939475: val_loss -0.4948
2024-12-20 06:02:03.940155: Pseudo dice [0.7186]
2024-12-20 06:02:03.940951: Epoch time: 405.13 s
2024-12-20 06:02:03.941702: Yayy! New best EMA pseudo Dice: 0.7062
2024-12-20 06:02:05.751089: 
2024-12-20 06:02:05.752507: Epoch 54
2024-12-20 06:02:05.753750: Current learning rate: 0.00669
2024-12-20 06:08:37.062269: Validation loss did not improve from -0.52890. Patience: 23/50
2024-12-20 06:08:37.063277: train_loss -0.6561
2024-12-20 06:08:37.064053: val_loss -0.4894
2024-12-20 06:08:37.064827: Pseudo dice [0.7116]
2024-12-20 06:08:37.065546: Epoch time: 391.31 s
2024-12-20 06:08:37.470386: Yayy! New best EMA pseudo Dice: 0.7067
2024-12-20 06:08:39.414082: 
2024-12-20 06:08:39.415362: Epoch 55
2024-12-20 06:08:39.416034: Current learning rate: 0.00663
2024-12-20 06:15:21.903872: Validation loss did not improve from -0.52890. Patience: 24/50
2024-12-20 06:15:21.905189: train_loss -0.6678
2024-12-20 06:15:21.907019: val_loss -0.4807
2024-12-20 06:15:21.908596: Pseudo dice [0.71]
2024-12-20 06:15:21.910354: Epoch time: 402.49 s
2024-12-20 06:15:21.911232: Yayy! New best EMA pseudo Dice: 0.7071
2024-12-20 06:15:23.755702: 
2024-12-20 06:15:23.757038: Epoch 56
2024-12-20 06:15:23.757975: Current learning rate: 0.00657
2024-12-20 06:22:08.973580: Validation loss did not improve from -0.52890. Patience: 25/50
2024-12-20 06:22:08.974471: train_loss -0.6699
2024-12-20 06:22:08.975182: val_loss -0.478
2024-12-20 06:22:08.975856: Pseudo dice [0.709]
2024-12-20 06:22:08.976542: Epoch time: 405.22 s
2024-12-20 06:22:08.977264: Yayy! New best EMA pseudo Dice: 0.7073
2024-12-20 06:22:10.895190: 
2024-12-20 06:22:10.897083: Epoch 57
2024-12-20 06:22:10.898286: Current learning rate: 0.0065
2024-12-20 06:28:54.729669: Validation loss did not improve from -0.52890. Patience: 26/50
2024-12-20 06:28:54.730598: train_loss -0.666
2024-12-20 06:28:54.731400: val_loss -0.5054
2024-12-20 06:28:54.732282: Pseudo dice [0.7255]
2024-12-20 06:28:54.733018: Epoch time: 403.84 s
2024-12-20 06:28:54.733876: Yayy! New best EMA pseudo Dice: 0.7091
2024-12-20 06:28:56.634867: 
2024-12-20 06:28:56.636302: Epoch 58
2024-12-20 06:28:56.637257: Current learning rate: 0.00644
2024-12-20 06:35:45.314630: Validation loss did not improve from -0.52890. Patience: 27/50
2024-12-20 06:35:45.316551: train_loss -0.6636
2024-12-20 06:35:45.318137: val_loss -0.5161
2024-12-20 06:35:45.319283: Pseudo dice [0.7273]
2024-12-20 06:35:45.320696: Epoch time: 408.68 s
2024-12-20 06:35:45.321683: Yayy! New best EMA pseudo Dice: 0.7109
2024-12-20 06:35:47.250935: 
2024-12-20 06:35:47.252488: Epoch 59
2024-12-20 06:35:47.253581: Current learning rate: 0.00638
2024-12-20 06:42:40.814870: Validation loss did not improve from -0.52890. Patience: 28/50
2024-12-20 06:42:40.815971: train_loss -0.6677
2024-12-20 06:42:40.817231: val_loss -0.482
2024-12-20 06:42:40.818469: Pseudo dice [0.7191]
2024-12-20 06:42:40.819823: Epoch time: 413.57 s
2024-12-20 06:42:41.191855: Yayy! New best EMA pseudo Dice: 0.7117
2024-12-20 06:42:43.118007: 
2024-12-20 06:42:43.119582: Epoch 60
2024-12-20 06:42:43.120613: Current learning rate: 0.00631
2024-12-20 06:49:14.178303: Validation loss did not improve from -0.52890. Patience: 29/50
2024-12-20 06:49:14.178975: train_loss -0.6762
2024-12-20 06:49:14.179721: val_loss -0.5096
2024-12-20 06:49:14.180428: Pseudo dice [0.7216]
2024-12-20 06:49:14.181123: Epoch time: 391.06 s
2024-12-20 06:49:14.181728: Yayy! New best EMA pseudo Dice: 0.7127
2024-12-20 06:49:16.075791: 
2024-12-20 06:49:16.077304: Epoch 61
2024-12-20 06:49:16.078446: Current learning rate: 0.00625
2024-12-20 06:55:43.320738: Validation loss did not improve from -0.52890. Patience: 30/50
2024-12-20 06:55:43.321785: train_loss -0.6709
2024-12-20 06:55:43.322959: val_loss -0.4993
2024-12-20 06:55:43.323978: Pseudo dice [0.7152]
2024-12-20 06:55:43.324991: Epoch time: 387.25 s
2024-12-20 06:55:43.325974: Yayy! New best EMA pseudo Dice: 0.713
2024-12-20 06:55:45.747025: 
2024-12-20 06:55:45.748668: Epoch 62
2024-12-20 06:55:45.750327: Current learning rate: 0.00619
2024-12-20 07:02:06.217513: Validation loss did not improve from -0.52890. Patience: 31/50
2024-12-20 07:02:06.218494: train_loss -0.6776
2024-12-20 07:02:06.219409: val_loss -0.5045
2024-12-20 07:02:06.220096: Pseudo dice [0.7195]
2024-12-20 07:02:06.220796: Epoch time: 380.47 s
2024-12-20 07:02:06.221526: Yayy! New best EMA pseudo Dice: 0.7136
2024-12-20 07:02:08.022908: 
2024-12-20 07:02:08.024101: Epoch 63
2024-12-20 07:02:08.024927: Current learning rate: 0.00612
2024-12-20 07:08:30.781172: Validation loss improved from -0.52890 to -0.55297! Patience: 31/50
2024-12-20 07:08:30.782632: train_loss -0.6835
2024-12-20 07:08:30.785279: val_loss -0.553
2024-12-20 07:08:30.786860: Pseudo dice [0.7472]
2024-12-20 07:08:30.788518: Epoch time: 382.76 s
2024-12-20 07:08:30.790504: Yayy! New best EMA pseudo Dice: 0.717
2024-12-20 07:08:32.678310: 
2024-12-20 07:08:32.679930: Epoch 64
2024-12-20 07:08:32.680942: Current learning rate: 0.00606
2024-12-20 07:15:09.189697: Validation loss did not improve from -0.55297. Patience: 1/50
2024-12-20 07:15:09.190670: train_loss -0.688
2024-12-20 07:15:09.192432: val_loss -0.5346
2024-12-20 07:15:09.194245: Pseudo dice [0.7439]
2024-12-20 07:15:09.195614: Epoch time: 396.51 s
2024-12-20 07:15:09.615113: Yayy! New best EMA pseudo Dice: 0.7197
2024-12-20 07:15:11.499380: 
2024-12-20 07:15:11.500765: Epoch 65
2024-12-20 07:15:11.501660: Current learning rate: 0.006
2024-12-20 07:21:32.266652: Validation loss did not improve from -0.55297. Patience: 2/50
2024-12-20 07:21:32.267613: train_loss -0.6846
2024-12-20 07:21:32.268424: val_loss -0.4983
2024-12-20 07:21:32.269195: Pseudo dice [0.7148]
2024-12-20 07:21:32.269897: Epoch time: 380.77 s
2024-12-20 07:21:33.749746: 
2024-12-20 07:21:33.751480: Epoch 66
2024-12-20 07:21:33.752496: Current learning rate: 0.00593
2024-12-20 07:28:00.237387: Validation loss did not improve from -0.55297. Patience: 3/50
2024-12-20 07:28:00.238394: train_loss -0.6815
2024-12-20 07:28:00.240088: val_loss -0.5058
2024-12-20 07:28:00.241735: Pseudo dice [0.723]
2024-12-20 07:28:00.242661: Epoch time: 386.49 s
2024-12-20 07:28:01.699541: 
2024-12-20 07:28:01.700943: Epoch 67
2024-12-20 07:28:01.701995: Current learning rate: 0.00587
2024-12-20 07:34:50.187414: Validation loss did not improve from -0.55297. Patience: 4/50
2024-12-20 07:34:50.188881: train_loss -0.6881
2024-12-20 07:34:50.190332: val_loss -0.5227
2024-12-20 07:34:50.191292: Pseudo dice [0.7266]
2024-12-20 07:34:50.192246: Epoch time: 408.49 s
2024-12-20 07:34:50.192995: Yayy! New best EMA pseudo Dice: 0.7203
2024-12-20 07:34:52.179835: 
2024-12-20 07:34:52.181583: Epoch 68
2024-12-20 07:34:52.183067: Current learning rate: 0.00581
2024-12-20 07:41:24.185433: Validation loss did not improve from -0.55297. Patience: 5/50
2024-12-20 07:41:24.186346: train_loss -0.6902
2024-12-20 07:41:24.187254: val_loss -0.5153
2024-12-20 07:41:24.187971: Pseudo dice [0.7282]
2024-12-20 07:41:24.188660: Epoch time: 392.01 s
2024-12-20 07:41:24.189361: Yayy! New best EMA pseudo Dice: 0.7211
2024-12-20 07:41:26.168274: 
2024-12-20 07:41:26.170679: Epoch 69
2024-12-20 07:41:26.172494: Current learning rate: 0.00574
2024-12-20 07:48:07.502286: Validation loss did not improve from -0.55297. Patience: 6/50
2024-12-20 07:48:07.503332: train_loss -0.6879
2024-12-20 07:48:07.504237: val_loss -0.5445
2024-12-20 07:48:07.505037: Pseudo dice [0.7487]
2024-12-20 07:48:07.505779: Epoch time: 401.34 s
2024-12-20 07:48:07.916543: Yayy! New best EMA pseudo Dice: 0.7238
2024-12-20 07:48:09.778375: 
2024-12-20 07:48:09.780065: Epoch 70
2024-12-20 07:48:09.781029: Current learning rate: 0.00568
2024-12-20 07:54:46.516634: Validation loss did not improve from -0.55297. Patience: 7/50
2024-12-20 07:54:46.517794: train_loss -0.6913
2024-12-20 07:54:46.519330: val_loss -0.5306
2024-12-20 07:54:46.520256: Pseudo dice [0.7423]
2024-12-20 07:54:46.521273: Epoch time: 396.74 s
2024-12-20 07:54:46.522116: Yayy! New best EMA pseudo Dice: 0.7257
2024-12-20 07:54:48.437771: 
2024-12-20 07:54:48.439670: Epoch 71
2024-12-20 07:54:48.441230: Current learning rate: 0.00562
2024-12-20 08:01:08.921139: Validation loss did not improve from -0.55297. Patience: 8/50
2024-12-20 08:01:08.921979: train_loss -0.6946
2024-12-20 08:01:08.922740: val_loss -0.4414
2024-12-20 08:01:08.923535: Pseudo dice [0.6919]
2024-12-20 08:01:08.924369: Epoch time: 380.49 s
2024-12-20 08:01:10.434470: 
2024-12-20 08:01:10.435832: Epoch 72
2024-12-20 08:01:10.436604: Current learning rate: 0.00555
2024-12-20 08:07:46.909847: Validation loss did not improve from -0.55297. Patience: 9/50
2024-12-20 08:07:46.911019: train_loss -0.6979
2024-12-20 08:07:46.911816: val_loss -0.4962
2024-12-20 08:07:46.912668: Pseudo dice [0.7219]
2024-12-20 08:07:46.913717: Epoch time: 396.48 s
2024-12-20 08:07:49.278507: 
2024-12-20 08:07:49.279951: Epoch 73
2024-12-20 08:07:49.281172: Current learning rate: 0.00549
2024-12-20 08:14:02.903728: Validation loss did not improve from -0.55297. Patience: 10/50
2024-12-20 08:14:02.904646: train_loss -0.6987
2024-12-20 08:14:02.905510: val_loss -0.539
2024-12-20 08:14:02.906223: Pseudo dice [0.7389]
2024-12-20 08:14:02.907040: Epoch time: 373.63 s
2024-12-20 08:14:04.306959: 
2024-12-20 08:14:04.309427: Epoch 74
2024-12-20 08:14:04.310949: Current learning rate: 0.00542
2024-12-20 08:20:40.436605: Validation loss did not improve from -0.55297. Patience: 11/50
2024-12-20 08:20:40.437827: train_loss -0.6995
2024-12-20 08:20:40.438597: val_loss -0.4822
2024-12-20 08:20:40.439361: Pseudo dice [0.7193]
2024-12-20 08:20:40.439927: Epoch time: 396.13 s
2024-12-20 08:20:42.272199: 
2024-12-20 08:20:42.273346: Epoch 75
2024-12-20 08:20:42.274144: Current learning rate: 0.00536
2024-12-20 08:26:53.416034: Validation loss did not improve from -0.55297. Patience: 12/50
2024-12-20 08:26:53.417092: train_loss -0.7037
2024-12-20 08:26:53.417825: val_loss -0.5196
2024-12-20 08:26:53.418432: Pseudo dice [0.7278]
2024-12-20 08:26:53.419071: Epoch time: 371.15 s
2024-12-20 08:26:54.821362: 
2024-12-20 08:26:54.823650: Epoch 76
2024-12-20 08:26:54.825502: Current learning rate: 0.00529
2024-12-20 08:33:25.678773: Validation loss did not improve from -0.55297. Patience: 13/50
2024-12-20 08:33:25.679575: train_loss -0.7059
2024-12-20 08:33:25.680488: val_loss -0.5022
2024-12-20 08:33:25.681412: Pseudo dice [0.7194]
2024-12-20 08:33:25.682362: Epoch time: 390.86 s
2024-12-20 08:33:27.104317: 
2024-12-20 08:33:27.105434: Epoch 77
2024-12-20 08:33:27.106499: Current learning rate: 0.00523
2024-12-20 08:39:56.518828: Validation loss did not improve from -0.55297. Patience: 14/50
2024-12-20 08:39:56.521448: train_loss -0.7021
2024-12-20 08:39:56.523283: val_loss -0.5074
2024-12-20 08:39:56.524005: Pseudo dice [0.7183]
2024-12-20 08:39:56.525083: Epoch time: 389.42 s
2024-12-20 08:39:58.014140: 
2024-12-20 08:39:58.015497: Epoch 78
2024-12-20 08:39:58.016296: Current learning rate: 0.00517
2024-12-20 08:46:50.100527: Validation loss did not improve from -0.55297. Patience: 15/50
2024-12-20 08:46:50.101501: train_loss -0.7095
2024-12-20 08:46:50.102425: val_loss -0.5082
2024-12-20 08:46:50.103150: Pseudo dice [0.7208]
2024-12-20 08:46:50.103873: Epoch time: 412.09 s
2024-12-20 08:46:51.582492: 
2024-12-20 08:46:51.583841: Epoch 79
2024-12-20 08:46:51.584608: Current learning rate: 0.0051
2024-12-20 08:53:14.075136: Validation loss did not improve from -0.55297. Patience: 16/50
2024-12-20 08:53:14.076215: train_loss -0.7076
2024-12-20 08:53:14.077239: val_loss -0.5273
2024-12-20 08:53:14.078277: Pseudo dice [0.732]
2024-12-20 08:53:14.079285: Epoch time: 382.5 s
2024-12-20 08:53:15.965611: 
2024-12-20 08:53:15.967708: Epoch 80
2024-12-20 08:53:15.968968: Current learning rate: 0.00504
2024-12-20 08:59:36.174088: Validation loss did not improve from -0.55297. Patience: 17/50
2024-12-20 08:59:36.174939: train_loss -0.7082
2024-12-20 08:59:36.175754: val_loss -0.5335
2024-12-20 08:59:36.176466: Pseudo dice [0.7346]
2024-12-20 08:59:36.177157: Epoch time: 380.21 s
2024-12-20 08:59:37.601202: 
2024-12-20 08:59:37.602170: Epoch 81
2024-12-20 08:59:37.602961: Current learning rate: 0.00497
2024-12-20 09:05:56.373174: Validation loss did not improve from -0.55297. Patience: 18/50
2024-12-20 09:05:56.374204: train_loss -0.7106
2024-12-20 09:05:56.375282: val_loss -0.482
2024-12-20 09:05:56.376131: Pseudo dice [0.7165]
2024-12-20 09:05:56.377048: Epoch time: 378.77 s
2024-12-20 09:05:57.873013: 
2024-12-20 09:05:57.874277: Epoch 82
2024-12-20 09:05:57.875014: Current learning rate: 0.00491
2024-12-20 09:11:46.715519: Validation loss did not improve from -0.55297. Patience: 19/50
2024-12-20 09:11:46.716435: train_loss -0.7115
2024-12-20 09:11:46.717720: val_loss -0.5403
2024-12-20 09:11:46.718704: Pseudo dice [0.7398]
2024-12-20 09:11:46.719904: Epoch time: 348.84 s
2024-12-20 09:11:48.518304: 
2024-12-20 09:11:48.519729: Epoch 83
2024-12-20 09:11:48.520768: Current learning rate: 0.00484
2024-12-20 09:17:59.365055: Validation loss did not improve from -0.55297. Patience: 20/50
2024-12-20 09:17:59.366230: train_loss -0.7145
2024-12-20 09:17:59.367395: val_loss -0.4952
2024-12-20 09:17:59.368833: Pseudo dice [0.7286]
2024-12-20 09:17:59.369818: Epoch time: 370.85 s
2024-12-20 09:17:59.370955: Yayy! New best EMA pseudo Dice: 0.7258
2024-12-20 09:18:01.113360: 
2024-12-20 09:18:01.114842: Epoch 84
2024-12-20 09:18:01.115891: Current learning rate: 0.00478
2024-12-20 09:24:40.784748: Validation loss did not improve from -0.55297. Patience: 21/50
2024-12-20 09:24:40.785652: train_loss -0.7108
2024-12-20 09:24:40.786481: val_loss -0.4635
2024-12-20 09:24:40.787218: Pseudo dice [0.7129]
2024-12-20 09:24:40.788007: Epoch time: 399.67 s
2024-12-20 09:24:42.525108: 
2024-12-20 09:24:42.526374: Epoch 85
2024-12-20 09:24:42.527138: Current learning rate: 0.00471
2024-12-20 09:31:17.276295: Validation loss did not improve from -0.55297. Patience: 22/50
2024-12-20 09:31:17.277151: train_loss -0.7174
2024-12-20 09:31:17.278026: val_loss -0.5365
2024-12-20 09:31:17.278847: Pseudo dice [0.7357]
2024-12-20 09:31:17.279697: Epoch time: 394.75 s
2024-12-20 09:31:18.622640: 
2024-12-20 09:31:18.624208: Epoch 86
2024-12-20 09:31:18.625129: Current learning rate: 0.00465
2024-12-20 09:37:42.392382: Validation loss did not improve from -0.55297. Patience: 23/50
2024-12-20 09:37:42.393395: train_loss -0.7167
2024-12-20 09:37:42.394113: val_loss -0.5336
2024-12-20 09:37:42.394759: Pseudo dice [0.7424]
2024-12-20 09:37:42.395378: Epoch time: 383.77 s
2024-12-20 09:37:42.395934: Yayy! New best EMA pseudo Dice: 0.7273
2024-12-20 09:37:44.188606: 
2024-12-20 09:37:44.189764: Epoch 87
2024-12-20 09:37:44.190523: Current learning rate: 0.00458
2024-12-20 09:44:06.549556: Validation loss did not improve from -0.55297. Patience: 24/50
2024-12-20 09:44:06.550976: train_loss -0.7226
2024-12-20 09:44:06.552434: val_loss -0.4932
2024-12-20 09:44:06.554021: Pseudo dice [0.7269]
2024-12-20 09:44:06.555215: Epoch time: 382.36 s
2024-12-20 09:44:07.889905: 
2024-12-20 09:44:07.891032: Epoch 88
2024-12-20 09:44:07.891730: Current learning rate: 0.00452
2024-12-20 09:50:40.556306: Validation loss did not improve from -0.55297. Patience: 25/50
2024-12-20 09:50:40.557853: train_loss -0.723
2024-12-20 09:50:40.558933: val_loss -0.5191
2024-12-20 09:50:40.559767: Pseudo dice [0.7396]
2024-12-20 09:50:40.560433: Epoch time: 392.67 s
2024-12-20 09:50:40.561221: Yayy! New best EMA pseudo Dice: 0.7285
2024-12-20 09:50:42.227969: 
2024-12-20 09:50:42.229347: Epoch 89
2024-12-20 09:50:42.230989: Current learning rate: 0.00445
2024-12-20 09:57:33.074744: Validation loss did not improve from -0.55297. Patience: 26/50
2024-12-20 09:57:33.076054: train_loss -0.7213
2024-12-20 09:57:33.077017: val_loss -0.5189
2024-12-20 09:57:33.077976: Pseudo dice [0.7329]
2024-12-20 09:57:33.079083: Epoch time: 410.85 s
2024-12-20 09:57:33.459726: Yayy! New best EMA pseudo Dice: 0.729
2024-12-20 09:57:35.143984: 
2024-12-20 09:57:35.145619: Epoch 90
2024-12-20 09:57:35.146824: Current learning rate: 0.00438
2024-12-20 10:04:12.116531: Validation loss did not improve from -0.55297. Patience: 27/50
2024-12-20 10:04:12.117508: train_loss -0.7244
2024-12-20 10:04:12.118397: val_loss -0.5466
2024-12-20 10:04:12.119406: Pseudo dice [0.7523]
2024-12-20 10:04:12.120333: Epoch time: 396.97 s
2024-12-20 10:04:12.121227: Yayy! New best EMA pseudo Dice: 0.7313
2024-12-20 10:04:13.877611: 
2024-12-20 10:04:13.879316: Epoch 91
2024-12-20 10:04:13.880236: Current learning rate: 0.00432
2024-12-20 10:10:43.648171: Validation loss improved from -0.55297 to -0.56662! Patience: 27/50
2024-12-20 10:10:43.649154: train_loss -0.7252
2024-12-20 10:10:43.650130: val_loss -0.5666
2024-12-20 10:10:43.650944: Pseudo dice [0.7495]
2024-12-20 10:10:43.651651: Epoch time: 389.77 s
2024-12-20 10:10:43.652608: Yayy! New best EMA pseudo Dice: 0.7331
2024-12-20 10:10:45.391045: 
2024-12-20 10:10:45.392039: Epoch 92
2024-12-20 10:10:45.392702: Current learning rate: 0.00425
2024-12-20 10:17:26.853286: Validation loss did not improve from -0.56662. Patience: 1/50
2024-12-20 10:17:26.854446: train_loss -0.7267
2024-12-20 10:17:26.855923: val_loss -0.5017
2024-12-20 10:17:26.857757: Pseudo dice [0.7246]
2024-12-20 10:17:26.859410: Epoch time: 401.46 s
2024-12-20 10:17:28.266858: 
2024-12-20 10:17:28.268356: Epoch 93
2024-12-20 10:17:28.269301: Current learning rate: 0.00419
2024-12-20 10:24:07.286289: Validation loss did not improve from -0.56662. Patience: 2/50
2024-12-20 10:24:07.287268: train_loss -0.7296
2024-12-20 10:24:07.288473: val_loss -0.5132
2024-12-20 10:24:07.289362: Pseudo dice [0.7292]
2024-12-20 10:24:07.290488: Epoch time: 399.02 s
2024-12-20 10:24:08.597172: 
2024-12-20 10:24:08.598632: Epoch 94
2024-12-20 10:24:08.599473: Current learning rate: 0.00412
2024-12-20 10:30:46.521811: Validation loss did not improve from -0.56662. Patience: 3/50
2024-12-20 10:30:46.523328: train_loss -0.7288
2024-12-20 10:30:46.525302: val_loss -0.4729
2024-12-20 10:30:46.526641: Pseudo dice [0.7138]
2024-12-20 10:30:46.528297: Epoch time: 397.93 s
2024-12-20 10:30:49.175989: 
2024-12-20 10:30:49.177703: Epoch 95
2024-12-20 10:30:49.178916: Current learning rate: 0.00405
2024-12-20 10:37:27.504388: Validation loss did not improve from -0.56662. Patience: 4/50
2024-12-20 10:37:27.505193: train_loss -0.7327
2024-12-20 10:37:27.506301: val_loss -0.473
2024-12-20 10:37:27.507066: Pseudo dice [0.7098]
2024-12-20 10:37:27.507746: Epoch time: 398.33 s
2024-12-20 10:37:28.938887: 
2024-12-20 10:37:28.941123: Epoch 96
2024-12-20 10:37:28.942765: Current learning rate: 0.00399
2024-12-20 10:44:18.971210: Validation loss did not improve from -0.56662. Patience: 5/50
2024-12-20 10:44:18.972110: train_loss -0.7363
2024-12-20 10:44:18.972999: val_loss -0.4997
2024-12-20 10:44:18.973720: Pseudo dice [0.7173]
2024-12-20 10:44:18.974417: Epoch time: 410.03 s
2024-12-20 10:44:20.355147: 
2024-12-20 10:44:20.358145: Epoch 97
2024-12-20 10:44:20.359341: Current learning rate: 0.00392
2024-12-20 10:51:44.363640: Validation loss did not improve from -0.56662. Patience: 6/50
2024-12-20 10:51:44.365560: train_loss -0.7281
2024-12-20 10:51:44.367064: val_loss -0.5429
2024-12-20 10:51:44.367704: Pseudo dice [0.7445]
2024-12-20 10:51:44.368435: Epoch time: 444.01 s
2024-12-20 10:51:45.846102: 
2024-12-20 10:51:45.847279: Epoch 98
2024-12-20 10:51:45.848182: Current learning rate: 0.00385
2024-12-20 10:58:43.377768: Validation loss did not improve from -0.56662. Patience: 7/50
2024-12-20 10:58:43.379181: train_loss -0.7351
2024-12-20 10:58:43.380370: val_loss -0.4803
2024-12-20 10:58:43.381898: Pseudo dice [0.7122]
2024-12-20 10:58:43.383373: Epoch time: 417.53 s
2024-12-20 10:58:44.851926: 
2024-12-20 10:58:44.853211: Epoch 99
2024-12-20 10:58:44.853917: Current learning rate: 0.00379
2024-12-20 11:05:14.466279: Validation loss did not improve from -0.56662. Patience: 8/50
2024-12-20 11:05:14.468316: train_loss -0.7411
2024-12-20 11:05:14.469418: val_loss -0.5306
2024-12-20 11:05:14.470262: Pseudo dice [0.7414]
2024-12-20 11:05:14.471277: Epoch time: 389.62 s
2024-12-20 11:05:16.377253: 
2024-12-20 11:05:16.379055: Epoch 100
2024-12-20 11:05:16.380157: Current learning rate: 0.00372
2024-12-20 11:11:53.929809: Validation loss did not improve from -0.56662. Patience: 9/50
2024-12-20 11:11:53.930816: train_loss -0.739
2024-12-20 11:11:53.931978: val_loss -0.5242
2024-12-20 11:11:53.933413: Pseudo dice [0.7343]
2024-12-20 11:11:53.934436: Epoch time: 397.55 s
2024-12-20 11:11:55.362575: 
2024-12-20 11:11:55.363786: Epoch 101
2024-12-20 11:11:55.364791: Current learning rate: 0.00365
2024-12-20 11:18:35.407403: Validation loss did not improve from -0.56662. Patience: 10/50
2024-12-20 11:18:35.408350: train_loss -0.7412
2024-12-20 11:18:35.409139: val_loss -0.5211
2024-12-20 11:18:35.409914: Pseudo dice [0.7303]
2024-12-20 11:18:35.410660: Epoch time: 400.05 s
2024-12-20 11:18:36.838948: 
2024-12-20 11:18:36.840208: Epoch 102
2024-12-20 11:18:36.840880: Current learning rate: 0.00359
2024-12-20 11:25:18.802073: Validation loss did not improve from -0.56662. Patience: 11/50
2024-12-20 11:25:18.803671: train_loss -0.7362
2024-12-20 11:25:18.804536: val_loss -0.518
2024-12-20 11:25:18.805191: Pseudo dice [0.7335]
2024-12-20 11:25:18.805926: Epoch time: 401.97 s
2024-12-20 11:25:20.240771: 
2024-12-20 11:25:20.243116: Epoch 103
2024-12-20 11:25:20.244665: Current learning rate: 0.00352
2024-12-20 11:32:09.133717: Validation loss did not improve from -0.56662. Patience: 12/50
2024-12-20 11:32:09.134700: train_loss -0.7351
2024-12-20 11:32:09.135666: val_loss -0.5483
2024-12-20 11:32:09.136400: Pseudo dice [0.7424]
2024-12-20 11:32:09.137063: Epoch time: 408.9 s
2024-12-20 11:32:10.566144: 
2024-12-20 11:32:10.567500: Epoch 104
2024-12-20 11:32:10.568738: Current learning rate: 0.00345
2024-12-20 11:39:13.601403: Validation loss did not improve from -0.56662. Patience: 13/50
2024-12-20 11:39:13.602360: train_loss -0.7421
2024-12-20 11:39:13.603212: val_loss -0.5016
2024-12-20 11:39:13.604345: Pseudo dice [0.72]
2024-12-20 11:39:13.605525: Epoch time: 423.04 s
2024-12-20 11:39:15.522445: 
2024-12-20 11:39:15.523898: Epoch 105
2024-12-20 11:39:15.525424: Current learning rate: 0.00338
2024-12-20 11:46:05.256333: Validation loss did not improve from -0.56662. Patience: 14/50
2024-12-20 11:46:05.257432: train_loss -0.7434
2024-12-20 11:46:05.258314: val_loss -0.5436
2024-12-20 11:46:05.259212: Pseudo dice [0.7417]
2024-12-20 11:46:05.260107: Epoch time: 409.74 s
2024-12-20 11:46:07.634051: 
2024-12-20 11:46:07.635429: Epoch 106
2024-12-20 11:46:07.636411: Current learning rate: 0.00332
2024-12-20 11:52:40.213682: Validation loss did not improve from -0.56662. Patience: 15/50
2024-12-20 11:52:40.214813: train_loss -0.7495
2024-12-20 11:52:40.216220: val_loss -0.5021
2024-12-20 11:52:40.217082: Pseudo dice [0.7223]
2024-12-20 11:52:40.217896: Epoch time: 392.58 s
2024-12-20 11:52:41.630941: 
2024-12-20 11:52:41.632321: Epoch 107
2024-12-20 11:52:41.633263: Current learning rate: 0.00325
2024-12-20 11:59:28.268621: Validation loss did not improve from -0.56662. Patience: 16/50
2024-12-20 11:59:28.271086: train_loss -0.7416
2024-12-20 11:59:28.273448: val_loss -0.5018
2024-12-20 11:59:28.275003: Pseudo dice [0.7226]
2024-12-20 11:59:28.276224: Epoch time: 406.64 s
2024-12-20 11:59:29.681420: 
2024-12-20 11:59:29.682361: Epoch 108
2024-12-20 11:59:29.683219: Current learning rate: 0.00318
2024-12-20 12:06:18.892267: Validation loss did not improve from -0.56662. Patience: 17/50
2024-12-20 12:06:18.893211: train_loss -0.7449
2024-12-20 12:06:18.894057: val_loss -0.5204
2024-12-20 12:06:18.894838: Pseudo dice [0.734]
2024-12-20 12:06:18.895473: Epoch time: 409.21 s
2024-12-20 12:06:20.352189: 
2024-12-20 12:06:20.353406: Epoch 109
2024-12-20 12:06:20.354242: Current learning rate: 0.00311
2024-12-20 12:12:49.260180: Validation loss did not improve from -0.56662. Patience: 18/50
2024-12-20 12:12:49.261786: train_loss -0.7444
2024-12-20 12:12:49.262822: val_loss -0.5087
2024-12-20 12:12:49.263462: Pseudo dice [0.731]
2024-12-20 12:12:49.264290: Epoch time: 388.91 s
2024-12-20 12:12:51.275528: 
2024-12-20 12:12:51.277694: Epoch 110
2024-12-20 12:12:51.279512: Current learning rate: 0.00304
2024-12-20 12:19:46.982881: Validation loss did not improve from -0.56662. Patience: 19/50
2024-12-20 12:19:46.984011: train_loss -0.7456
2024-12-20 12:19:46.985388: val_loss -0.5338
2024-12-20 12:19:46.986811: Pseudo dice [0.7372]
2024-12-20 12:19:46.988221: Epoch time: 415.71 s
2024-12-20 12:19:48.408974: 
2024-12-20 12:19:48.410455: Epoch 111
2024-12-20 12:19:48.411164: Current learning rate: 0.00297
2024-12-20 12:26:33.108559: Validation loss did not improve from -0.56662. Patience: 20/50
2024-12-20 12:26:33.110446: train_loss -0.7485
2024-12-20 12:26:33.111506: val_loss -0.5257
2024-12-20 12:26:33.112263: Pseudo dice [0.734]
2024-12-20 12:26:33.112958: Epoch time: 404.7 s
2024-12-20 12:26:34.496716: 
2024-12-20 12:26:34.498278: Epoch 112
2024-12-20 12:26:34.499281: Current learning rate: 0.00291
2024-12-20 12:33:26.548817: Validation loss did not improve from -0.56662. Patience: 21/50
2024-12-20 12:33:26.549867: train_loss -0.7476
2024-12-20 12:33:26.550655: val_loss -0.5257
2024-12-20 12:33:26.551437: Pseudo dice [0.7354]
2024-12-20 12:33:26.552188: Epoch time: 412.05 s
2024-12-20 12:33:27.949668: 
2024-12-20 12:33:27.950733: Epoch 113
2024-12-20 12:33:27.951458: Current learning rate: 0.00284
2024-12-20 12:40:06.363175: Validation loss did not improve from -0.56662. Patience: 22/50
2024-12-20 12:40:06.364242: train_loss -0.7515
2024-12-20 12:40:06.364947: val_loss -0.5604
2024-12-20 12:40:06.365571: Pseudo dice [0.7497]
2024-12-20 12:40:06.366212: Epoch time: 398.42 s
2024-12-20 12:40:06.366845: Yayy! New best EMA pseudo Dice: 0.7333
2024-12-20 12:40:08.298709: 
2024-12-20 12:40:08.301026: Epoch 114
2024-12-20 12:40:08.302166: Current learning rate: 0.00277
2024-12-20 12:46:49.978713: Validation loss did not improve from -0.56662. Patience: 23/50
2024-12-20 12:46:49.979715: train_loss -0.7519
2024-12-20 12:46:49.980689: val_loss -0.5336
2024-12-20 12:46:49.981476: Pseudo dice [0.7458]
2024-12-20 12:46:49.982120: Epoch time: 401.68 s
2024-12-20 12:46:50.441947: Yayy! New best EMA pseudo Dice: 0.7345
2024-12-20 12:46:52.321694: 
2024-12-20 12:46:52.323508: Epoch 115
2024-12-20 12:46:52.324905: Current learning rate: 0.0027
2024-12-20 12:54:03.032559: Validation loss did not improve from -0.56662. Patience: 24/50
2024-12-20 12:54:03.033537: train_loss -0.749
2024-12-20 12:54:03.034408: val_loss -0.5164
2024-12-20 12:54:03.035944: Pseudo dice [0.7354]
2024-12-20 12:54:03.037156: Epoch time: 430.71 s
2024-12-20 12:54:03.038364: Yayy! New best EMA pseudo Dice: 0.7346
2024-12-20 12:54:06.194480: 
2024-12-20 12:54:06.195620: Epoch 116
2024-12-20 12:54:06.196310: Current learning rate: 0.00263
2024-12-20 13:01:33.506796: Validation loss did not improve from -0.56662. Patience: 25/50
2024-12-20 13:01:33.508088: train_loss -0.7564
2024-12-20 13:01:33.509069: val_loss -0.5203
2024-12-20 13:01:33.510049: Pseudo dice [0.7334]
2024-12-20 13:01:33.511465: Epoch time: 447.32 s
2024-12-20 13:01:34.996659: 
2024-12-20 13:01:34.998032: Epoch 117
2024-12-20 13:01:34.998895: Current learning rate: 0.00256
2024-12-20 13:08:55.307160: Validation loss did not improve from -0.56662. Patience: 26/50
2024-12-20 13:08:55.329102: train_loss -0.7566
2024-12-20 13:08:55.329904: val_loss -0.5477
2024-12-20 13:08:55.330694: Pseudo dice [0.7493]
2024-12-20 13:08:55.331355: Epoch time: 440.33 s
2024-12-20 13:08:55.332066: Yayy! New best EMA pseudo Dice: 0.736
2024-12-20 13:08:57.339544: 
2024-12-20 13:08:57.340877: Epoch 118
2024-12-20 13:08:57.341536: Current learning rate: 0.00249
2024-12-20 13:15:24.643673: Validation loss did not improve from -0.56662. Patience: 27/50
2024-12-20 13:15:24.645803: train_loss -0.7588
2024-12-20 13:15:24.646792: val_loss -0.5393
2024-12-20 13:15:24.647781: Pseudo dice [0.7449]
2024-12-20 13:15:24.649055: Epoch time: 387.31 s
2024-12-20 13:15:24.650136: Yayy! New best EMA pseudo Dice: 0.7369
2024-12-20 13:15:26.676967: 
2024-12-20 13:15:26.679162: Epoch 119
2024-12-20 13:15:26.680516: Current learning rate: 0.00242
2024-12-20 13:22:07.831646: Validation loss did not improve from -0.56662. Patience: 28/50
2024-12-20 13:22:07.832882: train_loss -0.7611
2024-12-20 13:22:07.833789: val_loss -0.5584
2024-12-20 13:22:07.834638: Pseudo dice [0.7518]
2024-12-20 13:22:07.835593: Epoch time: 401.16 s
2024-12-20 13:22:08.302750: Yayy! New best EMA pseudo Dice: 0.7384
2024-12-20 13:22:10.298259: 
2024-12-20 13:22:10.299882: Epoch 120
2024-12-20 13:22:10.300992: Current learning rate: 0.00235
2024-12-20 13:28:52.551199: Validation loss did not improve from -0.56662. Patience: 29/50
2024-12-20 13:28:52.552160: train_loss -0.7592
2024-12-20 13:28:52.553503: val_loss -0.552
2024-12-20 13:28:52.554479: Pseudo dice [0.75]
2024-12-20 13:28:52.555306: Epoch time: 402.26 s
2024-12-20 13:28:52.556449: Yayy! New best EMA pseudo Dice: 0.7395
2024-12-20 13:28:54.370727: 
2024-12-20 13:28:54.372217: Epoch 121
2024-12-20 13:28:54.373476: Current learning rate: 0.00228
2024-12-20 13:35:40.765722: Validation loss did not improve from -0.56662. Patience: 30/50
2024-12-20 13:35:40.767067: train_loss -0.7583
2024-12-20 13:35:40.768050: val_loss -0.5313
2024-12-20 13:35:40.769152: Pseudo dice [0.7372]
2024-12-20 13:35:40.770254: Epoch time: 406.4 s
2024-12-20 13:35:42.219401: 
2024-12-20 13:35:42.220779: Epoch 122
2024-12-20 13:35:42.221576: Current learning rate: 0.00221
2024-12-20 13:43:18.819118: Validation loss did not improve from -0.56662. Patience: 31/50
2024-12-20 13:43:18.819768: train_loss -0.7631
2024-12-20 13:43:18.820433: val_loss -0.5293
2024-12-20 13:43:18.821117: Pseudo dice [0.7331]
2024-12-20 13:43:18.821760: Epoch time: 456.6 s
2024-12-20 13:43:20.307636: 
2024-12-20 13:43:20.308707: Epoch 123
2024-12-20 13:43:20.309523: Current learning rate: 0.00214
2024-12-20 13:49:50.865471: Validation loss did not improve from -0.56662. Patience: 32/50
2024-12-20 13:49:50.866478: train_loss -0.7602
2024-12-20 13:49:50.867360: val_loss -0.5361
2024-12-20 13:49:50.868038: Pseudo dice [0.7335]
2024-12-20 13:49:50.868692: Epoch time: 390.56 s
2024-12-20 13:49:52.303499: 
2024-12-20 13:49:52.304739: Epoch 124
2024-12-20 13:49:52.305732: Current learning rate: 0.00207
2024-12-20 13:56:13.217004: Validation loss did not improve from -0.56662. Patience: 33/50
2024-12-20 13:56:13.218424: train_loss -0.7622
2024-12-20 13:56:13.219652: val_loss -0.5233
2024-12-20 13:56:13.220846: Pseudo dice [0.7352]
2024-12-20 13:56:13.221878: Epoch time: 380.92 s
2024-12-20 13:56:15.037345: 
2024-12-20 13:56:15.038260: Epoch 125
2024-12-20 13:56:15.038942: Current learning rate: 0.00199
2024-12-20 14:03:12.917562: Validation loss did not improve from -0.56662. Patience: 34/50
2024-12-20 14:03:12.918463: train_loss -0.7655
2024-12-20 14:03:12.919401: val_loss -0.5287
2024-12-20 14:03:12.920105: Pseudo dice [0.7385]
2024-12-20 14:03:12.920886: Epoch time: 417.88 s
2024-12-20 14:03:14.284364: 
2024-12-20 14:03:14.285129: Epoch 126
2024-12-20 14:03:14.285829: Current learning rate: 0.00192
2024-12-20 14:09:23.848310: Validation loss did not improve from -0.56662. Patience: 35/50
2024-12-20 14:09:23.849361: train_loss -0.7668
2024-12-20 14:09:23.850140: val_loss -0.5273
2024-12-20 14:09:23.850942: Pseudo dice [0.7388]
2024-12-20 14:09:23.851559: Epoch time: 369.57 s
2024-12-20 14:09:25.713687: 
2024-12-20 14:09:25.714545: Epoch 127
2024-12-20 14:09:25.715144: Current learning rate: 0.00185
2024-12-20 14:15:46.474829: Validation loss did not improve from -0.56662. Patience: 36/50
2024-12-20 14:15:46.476185: train_loss -0.7666
2024-12-20 14:15:46.477447: val_loss -0.5596
2024-12-20 14:15:46.478164: Pseudo dice [0.7511]
2024-12-20 14:15:46.478729: Epoch time: 380.76 s
2024-12-20 14:15:47.873932: 
2024-12-20 14:15:47.875376: Epoch 128
2024-12-20 14:15:47.876044: Current learning rate: 0.00178
2024-12-20 14:22:00.469776: Validation loss did not improve from -0.56662. Patience: 37/50
2024-12-20 14:22:00.470876: train_loss -0.7632
2024-12-20 14:22:00.471941: val_loss -0.5578
2024-12-20 14:22:00.472557: Pseudo dice [0.7614]
2024-12-20 14:22:00.473193: Epoch time: 372.6 s
2024-12-20 14:22:00.473936: Yayy! New best EMA pseudo Dice: 0.7415
2024-12-20 14:22:02.263896: 
2024-12-20 14:22:02.264947: Epoch 129
2024-12-20 14:22:02.265624: Current learning rate: 0.0017
2024-12-20 14:28:12.721328: Validation loss did not improve from -0.56662. Patience: 38/50
2024-12-20 14:28:12.722632: train_loss -0.7663
2024-12-20 14:28:12.723739: val_loss -0.5015
2024-12-20 14:28:12.724504: Pseudo dice [0.7285]
2024-12-20 14:28:12.725360: Epoch time: 370.46 s
2024-12-20 14:28:14.458986: 
2024-12-20 14:28:14.460205: Epoch 130
2024-12-20 14:28:14.460883: Current learning rate: 0.00163
2024-12-20 14:34:33.643589: Validation loss did not improve from -0.56662. Patience: 39/50
2024-12-20 14:34:33.644261: train_loss -0.7684
2024-12-20 14:34:33.645234: val_loss -0.5376
2024-12-20 14:34:33.645989: Pseudo dice [0.7414]
2024-12-20 14:34:33.647157: Epoch time: 379.19 s
2024-12-20 14:34:35.075291: 
2024-12-20 14:34:35.080878: Epoch 131
2024-12-20 14:34:35.083665: Current learning rate: 0.00156
2024-12-20 14:40:33.656480: Validation loss did not improve from -0.56662. Patience: 40/50
2024-12-20 14:40:33.657340: train_loss -0.7679
2024-12-20 14:40:33.658096: val_loss -0.533
2024-12-20 14:40:33.658868: Pseudo dice [0.7458]
2024-12-20 14:40:33.659639: Epoch time: 358.58 s
2024-12-20 14:40:35.029601: 
2024-12-20 14:40:35.031212: Epoch 132
2024-12-20 14:40:35.032067: Current learning rate: 0.00148
2024-12-20 14:46:40.745564: Validation loss did not improve from -0.56662. Patience: 41/50
2024-12-20 14:46:40.746340: train_loss -0.7732
2024-12-20 14:46:40.747007: val_loss -0.5398
2024-12-20 14:46:40.747816: Pseudo dice [0.7473]
2024-12-20 14:46:40.748758: Epoch time: 365.72 s
2024-12-20 14:46:40.749469: Yayy! New best EMA pseudo Dice: 0.7415
2024-12-20 14:46:42.508014: 
2024-12-20 14:46:42.509018: Epoch 133
2024-12-20 14:46:42.509770: Current learning rate: 0.00141
2024-12-20 14:52:47.064624: Validation loss did not improve from -0.56662. Patience: 42/50
2024-12-20 14:52:47.065810: train_loss -0.7677
2024-12-20 14:52:47.066494: val_loss -0.5433
2024-12-20 14:52:47.067101: Pseudo dice [0.7403]
2024-12-20 14:52:47.067700: Epoch time: 364.56 s
2024-12-20 14:52:48.458708: 
2024-12-20 14:52:48.459886: Epoch 134
2024-12-20 14:52:48.460869: Current learning rate: 0.00133
2024-12-20 14:59:02.730888: Validation loss did not improve from -0.56662. Patience: 43/50
2024-12-20 14:59:02.731980: train_loss -0.769
2024-12-20 14:59:02.732828: val_loss -0.5462
2024-12-20 14:59:02.733549: Pseudo dice [0.7522]
2024-12-20 14:59:02.734322: Epoch time: 374.28 s
2024-12-20 14:59:03.141015: Yayy! New best EMA pseudo Dice: 0.7425
2024-12-20 14:59:04.949571: 
2024-12-20 14:59:04.950760: Epoch 135
2024-12-20 14:59:04.951573: Current learning rate: 0.00126
2024-12-20 15:05:00.900301: Validation loss did not improve from -0.56662. Patience: 44/50
2024-12-20 15:05:00.901293: train_loss -0.7706
2024-12-20 15:05:00.902426: val_loss -0.497
2024-12-20 15:05:00.903363: Pseudo dice [0.7296]
2024-12-20 15:05:00.904383: Epoch time: 355.95 s
2024-12-20 15:05:02.297200: 
2024-12-20 15:05:02.298475: Epoch 136
2024-12-20 15:05:02.299748: Current learning rate: 0.00118
2024-12-20 15:11:03.270727: Validation loss did not improve from -0.56662. Patience: 45/50
2024-12-20 15:11:03.334333: train_loss -0.7742
2024-12-20 15:11:03.336930: val_loss -0.5531
2024-12-20 15:11:03.338124: Pseudo dice [0.7534]
2024-12-20 15:11:03.339677: Epoch time: 361.03 s
2024-12-20 15:11:04.763454: 
2024-12-20 15:11:04.764829: Epoch 137
2024-12-20 15:11:04.765763: Current learning rate: 0.00111
2024-12-20 15:16:53.522163: Validation loss did not improve from -0.56662. Patience: 46/50
2024-12-20 15:16:53.523330: train_loss -0.7722
2024-12-20 15:16:53.524609: val_loss -0.5424
2024-12-20 15:16:53.525648: Pseudo dice [0.7486]
2024-12-20 15:16:53.526273: Epoch time: 348.76 s
2024-12-20 15:16:53.526944: Yayy! New best EMA pseudo Dice: 0.743
2024-12-20 15:16:55.931653: 
2024-12-20 15:16:55.932946: Epoch 138
2024-12-20 15:16:55.933829: Current learning rate: 0.00103
2024-12-20 15:23:14.329282: Validation loss did not improve from -0.56662. Patience: 47/50
2024-12-20 15:23:14.330147: train_loss -0.7725
2024-12-20 15:23:14.330890: val_loss -0.5317
2024-12-20 15:23:14.331503: Pseudo dice [0.735]
2024-12-20 15:23:14.332232: Epoch time: 378.4 s
2024-12-20 15:23:15.709352: 
2024-12-20 15:23:15.711068: Epoch 139
2024-12-20 15:23:15.712864: Current learning rate: 0.00095
2024-12-20 15:29:45.107143: Validation loss did not improve from -0.56662. Patience: 48/50
2024-12-20 15:29:45.107862: train_loss -0.7758
2024-12-20 15:29:45.109251: val_loss -0.5483
2024-12-20 15:29:45.110095: Pseudo dice [0.7426]
2024-12-20 15:29:45.110939: Epoch time: 389.4 s
2024-12-20 15:29:46.860860: 
2024-12-20 15:29:46.861658: Epoch 140
2024-12-20 15:29:46.862444: Current learning rate: 0.00087
2024-12-20 15:35:54.148151: Validation loss did not improve from -0.56662. Patience: 49/50
2024-12-20 15:35:54.149138: train_loss -0.7755
2024-12-20 15:35:54.150200: val_loss -0.5066
2024-12-20 15:35:54.151401: Pseudo dice [0.7273]
2024-12-20 15:35:54.152127: Epoch time: 367.29 s
2024-12-20 15:35:55.573789: 
2024-12-20 15:35:55.574797: Epoch 141
2024-12-20 15:35:55.575557: Current learning rate: 0.00079
2024-12-20 15:41:51.502339: Validation loss did not improve from -0.56662. Patience: 50/50
2024-12-20 15:41:51.503108: train_loss -0.7747
2024-12-20 15:41:51.504254: val_loss -0.5637
2024-12-20 15:41:51.505178: Pseudo dice [0.7558]
2024-12-20 15:41:51.506170: Epoch time: 355.93 s
2024-12-20 15:41:52.918101: 
2024-12-20 15:41:52.919249: Epoch 142
2024-12-20 15:41:52.920060: Current learning rate: 0.00071
2024-12-20 15:47:55.567259: Validation loss did not improve from -0.56662. Patience: 51/50
2024-12-20 15:47:55.568653: train_loss -0.7744
2024-12-20 15:47:55.569449: val_loss -0.5368
2024-12-20 15:47:55.570665: Pseudo dice [0.7453]
2024-12-20 15:47:55.571532: Epoch time: 362.65 s
2024-12-20 15:47:56.922065: 
2024-12-20 15:47:56.923535: Epoch 143
2024-12-20 15:47:56.924513: Current learning rate: 0.00063
2024-12-20 15:53:56.762359: Validation loss did not improve from -0.56662. Patience: 52/50
2024-12-20 15:53:56.763092: train_loss -0.7728
2024-12-20 15:53:56.763870: val_loss -0.5414
2024-12-20 15:53:56.764602: Pseudo dice [0.7496]
2024-12-20 15:53:56.765352: Epoch time: 359.84 s
2024-12-20 15:53:56.766304: Yayy! New best EMA pseudo Dice: 0.7433
2024-12-20 15:53:58.518913: 
2024-12-20 15:53:58.519884: Epoch 144
2024-12-20 15:53:58.520961: Current learning rate: 0.00055
2024-12-20 16:00:18.589171: Validation loss did not improve from -0.56662. Patience: 53/50
2024-12-20 16:00:18.590098: train_loss -0.7756
2024-12-20 16:00:18.590838: val_loss -0.5405
2024-12-20 16:00:18.591542: Pseudo dice [0.7455]
2024-12-20 16:00:18.592586: Epoch time: 380.07 s
2024-12-20 16:00:18.986339: Yayy! New best EMA pseudo Dice: 0.7435
2024-12-20 16:00:20.776838: 
2024-12-20 16:00:20.778087: Epoch 145
2024-12-20 16:00:20.778802: Current learning rate: 0.00047
2024-12-20 16:05:45.169155: Validation loss did not improve from -0.56662. Patience: 54/50
2024-12-20 16:05:45.169978: train_loss -0.7759
2024-12-20 16:05:45.171120: val_loss -0.5446
2024-12-20 16:05:45.171715: Pseudo dice [0.7522]
2024-12-20 16:05:45.172300: Epoch time: 324.39 s
2024-12-20 16:05:45.172891: Yayy! New best EMA pseudo Dice: 0.7444
2024-12-20 16:05:46.936714: 
2024-12-20 16:05:46.937572: Epoch 146
2024-12-20 16:05:46.938339: Current learning rate: 0.00038
2024-12-20 16:10:37.810515: Validation loss did not improve from -0.56662. Patience: 55/50
2024-12-20 16:10:37.811419: train_loss -0.7792
2024-12-20 16:10:37.812689: val_loss -0.5379
2024-12-20 16:10:37.813355: Pseudo dice [0.7478]
2024-12-20 16:10:37.814400: Epoch time: 290.88 s
2024-12-20 16:10:37.815081: Yayy! New best EMA pseudo Dice: 0.7447
2024-12-20 16:10:39.669002: 
2024-12-20 16:10:39.669781: Epoch 147
2024-12-20 16:10:39.670510: Current learning rate: 0.0003
2024-12-20 16:15:41.005127: Validation loss did not improve from -0.56662. Patience: 56/50
2024-12-20 16:15:41.008191: train_loss -0.7778
2024-12-20 16:15:41.010118: val_loss -0.5284
2024-12-20 16:15:41.011081: Pseudo dice [0.74]
2024-12-20 16:15:41.012542: Epoch time: 301.34 s
2024-12-20 16:15:42.815454: 
2024-12-20 16:15:42.816797: Epoch 148
2024-12-20 16:15:42.817540: Current learning rate: 0.00021
2024-12-20 16:20:29.512569: Validation loss did not improve from -0.56662. Patience: 57/50
2024-12-20 16:20:29.519923: train_loss -0.7814
2024-12-20 16:20:29.520822: val_loss -0.5144
2024-12-20 16:20:29.522739: Pseudo dice [0.7336]
2024-12-20 16:20:29.523409: Epoch time: 286.71 s
2024-12-20 16:20:30.915427: 
2024-12-20 16:20:30.916873: Epoch 149
2024-12-20 16:20:30.917896: Current learning rate: 0.00011
2024-12-20 16:25:35.535521: Validation loss did not improve from -0.56662. Patience: 58/50
2024-12-20 16:25:35.536615: train_loss -0.7799
2024-12-20 16:25:35.537426: val_loss -0.5284
2024-12-20 16:25:35.538159: Pseudo dice [0.7385]
2024-12-20 16:25:35.538811: Epoch time: 304.62 s
2024-12-20 16:25:37.396760: Training done.
2024-12-20 16:25:37.614545: Using splits from existing split file: /home/gridsan/nchutisilp/datasets/nnUNet_Datasets/nnUNet_preprocessed/Dataset308_Sohee_Calcium_OCT_CrossValidation/splits_final_80.json
2024-12-20 16:25:37.625699: The split file contains 5 splits.
2024-12-20 16:25:37.626570: Desired fold for training: 2
2024-12-20 16:25:37.627442: This split has 6 training and 3 validation cases.
2024-12-20 16:25:37.628628: predicting 101-045
2024-12-20 16:25:37.651198: 101-045, shape torch.Size([1, 375, 498, 498]), rank 0
2024-12-20 16:27:56.794395: predicting 401-004
2024-12-20 16:27:56.811237: 401-004, shape torch.Size([1, 375, 498, 498]), rank 0
2024-12-20 16:30:14.194853: predicting 704-003
2024-12-20 16:30:14.207192: 704-003, shape torch.Size([1, 375, 498, 498]), rank 0
2024-12-20 16:32:36.345917: Validation complete
2024-12-20 16:32:36.346478: Mean Validation Dice:  0.7300876351541526

############################
INFO: You are using the old nnU-Net default plans. We have updated our recommendations. Please consider using those instead! Read more here: https://github.com/MIC-DKFZ/nnUNet/blob/master/documentation/resenc_presets.md
############################

Using device: cuda:0

#######################################################################
Please cite the following paper when using nnU-Net:
Isensee, F., Jaeger, P. F., Kohl, S. A., Petersen, J., & Maier-Hein, K. H. (2021). nnU-Net: a self-configuring method for deep learning-based biomedical image segmentation. Nature methods, 18(2), 203-211.
#######################################################################

2024-12-20 16:32:46.064119: Using torch.compile...
/home/gridsan/nchutisilp/.local/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn("The verbose parameter is deprecated. Please use get_last_lr() "
################### Loading pretrained weights from file  /home/gridsan/nchutisilp/projects/OpenAI-CLIP/clip_pretrained_nnUNet.pt ###################
Below is the list of overlapping blocks in pretrained model and nnUNet architecture:
encoder.stages.0.0.convs.0.conv.weight shape torch.Size([32, 1, 3, 3, 3])
encoder.stages.0.0.convs.0.conv.bias shape torch.Size([32])
encoder.stages.0.0.convs.0.norm.weight shape torch.Size([32])
encoder.stages.0.0.convs.0.norm.bias shape torch.Size([32])
encoder.stages.0.0.convs.0.all_modules.0.weight shape torch.Size([32, 1, 3, 3, 3])
encoder.stages.0.0.convs.0.all_modules.0.bias shape torch.Size([32])
encoder.stages.0.0.convs.0.all_modules.1.weight shape torch.Size([32])
encoder.stages.0.0.convs.0.all_modules.1.bias shape torch.Size([32])
encoder.stages.0.0.convs.1.conv.weight shape torch.Size([32, 32, 3, 3, 3])
encoder.stages.0.0.convs.1.conv.bias shape torch.Size([32])
encoder.stages.0.0.convs.1.norm.weight shape torch.Size([32])
encoder.stages.0.0.convs.1.norm.bias shape torch.Size([32])
encoder.stages.0.0.convs.1.all_modules.0.weight shape torch.Size([32, 32, 3, 3, 3])
encoder.stages.0.0.convs.1.all_modules.0.bias shape torch.Size([32])
encoder.stages.0.0.convs.1.all_modules.1.weight shape torch.Size([32])
encoder.stages.0.0.convs.1.all_modules.1.bias shape torch.Size([32])
encoder.stages.1.0.convs.0.conv.weight shape torch.Size([64, 32, 3, 3, 3])
encoder.stages.1.0.convs.0.conv.bias shape torch.Size([64])
encoder.stages.1.0.convs.0.norm.weight shape torch.Size([64])
encoder.stages.1.0.convs.0.norm.bias shape torch.Size([64])
encoder.stages.1.0.convs.0.all_modules.0.weight shape torch.Size([64, 32, 3, 3, 3])
encoder.stages.1.0.convs.0.all_modules.0.bias shape torch.Size([64])
encoder.stages.1.0.convs.0.all_modules.1.weight shape torch.Size([64])
encoder.stages.1.0.convs.0.all_modules.1.bias shape torch.Size([64])
encoder.stages.1.0.convs.1.conv.weight shape torch.Size([64, 64, 3, 3, 3])
encoder.stages.1.0.convs.1.conv.bias shape torch.Size([64])
encoder.stages.1.0.convs.1.norm.weight shape torch.Size([64])
encoder.stages.1.0.convs.1.norm.bias shape torch.Size([64])
encoder.stages.1.0.convs.1.all_modules.0.weight shape torch.Size([64, 64, 3, 3, 3])
encoder.stages.1.0.convs.1.all_modules.0.bias shape torch.Size([64])
encoder.stages.1.0.convs.1.all_modules.1.weight shape torch.Size([64])
encoder.stages.1.0.convs.1.all_modules.1.bias shape torch.Size([64])
encoder.stages.2.0.convs.0.conv.weight shape torch.Size([128, 64, 3, 3, 3])
encoder.stages.2.0.convs.0.conv.bias shape torch.Size([128])
encoder.stages.2.0.convs.0.norm.weight shape torch.Size([128])
encoder.stages.2.0.convs.0.norm.bias shape torch.Size([128])
encoder.stages.2.0.convs.0.all_modules.0.weight shape torch.Size([128, 64, 3, 3, 3])
encoder.stages.2.0.convs.0.all_modules.0.bias shape torch.Size([128])
encoder.stages.2.0.convs.0.all_modules.1.weight shape torch.Size([128])
encoder.stages.2.0.convs.0.all_modules.1.bias shape torch.Size([128])
encoder.stages.2.0.convs.1.conv.weight shape torch.Size([128, 128, 3, 3, 3])
encoder.stages.2.0.convs.1.conv.bias shape torch.Size([128])
encoder.stages.2.0.convs.1.norm.weight shape torch.Size([128])
encoder.stages.2.0.convs.1.norm.bias shape torch.Size([128])
encoder.stages.2.0.convs.1.all_modules.0.weight shape torch.Size([128, 128, 3, 3, 3])
encoder.stages.2.0.convs.1.all_modules.0.bias shape torch.Size([128])
encoder.stages.2.0.convs.1.all_modules.1.weight shape torch.Size([128])
encoder.stages.2.0.convs.1.all_modules.1.bias shape torch.Size([128])
encoder.stages.3.0.convs.0.conv.weight shape torch.Size([256, 128, 3, 3, 3])
encoder.stages.3.0.convs.0.conv.bias shape torch.Size([256])
encoder.stages.3.0.convs.0.norm.weight shape torch.Size([256])
encoder.stages.3.0.convs.0.norm.bias shape torch.Size([256])
encoder.stages.3.0.convs.0.all_modules.0.weight shape torch.Size([256, 128, 3, 3, 3])
encoder.stages.3.0.convs.0.all_modules.0.bias shape torch.Size([256])
encoder.stages.3.0.convs.0.all_modules.1.weight shape torch.Size([256])
encoder.stages.3.0.convs.0.all_modules.1.bias shape torch.Size([256])
encoder.stages.3.0.convs.1.conv.weight shape torch.Size([256, 256, 3, 3, 3])
encoder.stages.3.0.convs.1.conv.bias shape torch.Size([256])
encoder.stages.3.0.convs.1.norm.weight shape torch.Size([256])
encoder.stages.3.0.convs.1.norm.bias shape torch.Size([256])
encoder.stages.3.0.convs.1.all_modules.0.weight shape torch.Size([256, 256, 3, 3, 3])
encoder.stages.3.0.convs.1.all_modules.0.bias shape torch.Size([256])
encoder.stages.3.0.convs.1.all_modules.1.weight shape torch.Size([256])
encoder.stages.3.0.convs.1.all_modules.1.bias shape torch.Size([256])
encoder.stages.4.0.convs.0.conv.weight shape torch.Size([320, 256, 3, 3, 3])
encoder.stages.4.0.convs.0.conv.bias shape torch.Size([320])
encoder.stages.4.0.convs.0.norm.weight shape torch.Size([320])
encoder.stages.4.0.convs.0.norm.bias shape torch.Size([320])
encoder.stages.4.0.convs.0.all_modules.0.weight shape torch.Size([320, 256, 3, 3, 3])
encoder.stages.4.0.convs.0.all_modules.0.bias shape torch.Size([320])
encoder.stages.4.0.convs.0.all_modules.1.weight shape torch.Size([320])
encoder.stages.4.0.convs.0.all_modules.1.bias shape torch.Size([320])
encoder.stages.4.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.4.0.convs.1.conv.bias shape torch.Size([320])
encoder.stages.4.0.convs.1.norm.weight shape torch.Size([320])
encoder.stages.4.0.convs.1.norm.bias shape torch.Size([320])
encoder.stages.4.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.4.0.convs.1.all_modules.0.bias shape torch.Size([320])
encoder.stages.4.0.convs.1.all_modules.1.weight shape torch.Size([320])
encoder.stages.4.0.convs.1.all_modules.1.bias shape torch.Size([320])
encoder.stages.5.0.convs.0.conv.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.5.0.convs.0.conv.bias shape torch.Size([320])
encoder.stages.5.0.convs.0.norm.weight shape torch.Size([320])
encoder.stages.5.0.convs.0.norm.bias shape torch.Size([320])
encoder.stages.5.0.convs.0.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.5.0.convs.0.all_modules.0.bias shape torch.Size([320])
encoder.stages.5.0.convs.0.all_modules.1.weight shape torch.Size([320])
encoder.stages.5.0.convs.0.all_modules.1.bias shape torch.Size([320])
encoder.stages.5.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.5.0.convs.1.conv.bias shape torch.Size([320])
encoder.stages.5.0.convs.1.norm.weight shape torch.Size([320])
encoder.stages.5.0.convs.1.norm.bias shape torch.Size([320])
encoder.stages.5.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.5.0.convs.1.all_modules.0.bias shape torch.Size([320])
encoder.stages.5.0.convs.1.all_modules.1.weight shape torch.Size([320])
encoder.stages.5.0.convs.1.all_modules.1.bias shape torch.Size([320])
decoder.encoder.stages.0.0.convs.0.conv.weight shape torch.Size([32, 1, 3, 3, 3])
decoder.encoder.stages.0.0.convs.0.conv.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.norm.weight shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.norm.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.all_modules.0.weight shape torch.Size([32, 1, 3, 3, 3])
decoder.encoder.stages.0.0.convs.0.all_modules.0.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.all_modules.1.weight shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.all_modules.1.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.conv.weight shape torch.Size([32, 32, 3, 3, 3])
decoder.encoder.stages.0.0.convs.1.conv.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.norm.weight shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.norm.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.all_modules.0.weight shape torch.Size([32, 32, 3, 3, 3])
decoder.encoder.stages.0.0.convs.1.all_modules.0.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.all_modules.1.weight shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.all_modules.1.bias shape torch.Size([32])
decoder.encoder.stages.1.0.convs.0.conv.weight shape torch.Size([64, 32, 3, 3, 3])
decoder.encoder.stages.1.0.convs.0.conv.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.norm.weight shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.norm.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.all_modules.0.weight shape torch.Size([64, 32, 3, 3, 3])
decoder.encoder.stages.1.0.convs.0.all_modules.0.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.all_modules.1.weight shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.all_modules.1.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.conv.weight shape torch.Size([64, 64, 3, 3, 3])
decoder.encoder.stages.1.0.convs.1.conv.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.norm.weight shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.norm.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.all_modules.0.weight shape torch.Size([64, 64, 3, 3, 3])
decoder.encoder.stages.1.0.convs.1.all_modules.0.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.all_modules.1.weight shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.all_modules.1.bias shape torch.Size([64])
decoder.encoder.stages.2.0.convs.0.conv.weight shape torch.Size([128, 64, 3, 3, 3])
decoder.encoder.stages.2.0.convs.0.conv.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.norm.weight shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.norm.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.all_modules.0.weight shape torch.Size([128, 64, 3, 3, 3])
decoder.encoder.stages.2.0.convs.0.all_modules.0.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.all_modules.1.weight shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.all_modules.1.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.conv.weight shape torch.Size([128, 128, 3, 3, 3])
decoder.encoder.stages.2.0.convs.1.conv.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.norm.weight shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.norm.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.all_modules.0.weight shape torch.Size([128, 128, 3, 3, 3])
decoder.encoder.stages.2.0.convs.1.all_modules.0.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.all_modules.1.weight shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.all_modules.1.bias shape torch.Size([128])
decoder.encoder.stages.3.0.convs.0.conv.weight shape torch.Size([256, 128, 3, 3, 3])
decoder.encoder.stages.3.0.convs.0.conv.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.norm.weight shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.norm.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.all_modules.0.weight shape torch.Size([256, 128, 3, 3, 3])
decoder.encoder.stages.3.0.convs.0.all_modules.0.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.all_modules.1.weight shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.all_modules.1.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.conv.weight shape torch.Size([256, 256, 3, 3, 3])
decoder.encoder.stages.3.0.convs.1.conv.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.norm.weight shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.norm.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.all_modules.0.weight shape torch.Size([256, 256, 3, 3, 3])
decoder.encoder.stages.3.0.convs.1.all_modules.0.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.all_modules.1.weight shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.all_modules.1.bias shape torch.Size([256])
decoder.encoder.stages.4.0.convs.0.conv.weight shape torch.Size([320, 256, 3, 3, 3])
decoder.encoder.stages.4.0.convs.0.conv.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.norm.weight shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.norm.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.all_modules.0.weight shape torch.Size([320, 256, 3, 3, 3])
decoder.encoder.stages.4.0.convs.0.all_modules.0.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.all_modules.1.weight shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.all_modules.1.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.4.0.convs.1.conv.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.norm.weight shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.norm.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.4.0.convs.1.all_modules.0.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.all_modules.1.weight shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.all_modules.1.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.conv.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.5.0.convs.0.conv.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.norm.weight shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.norm.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.5.0.convs.0.all_modules.0.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.all_modules.1.weight shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.all_modules.1.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.5.0.convs.1.conv.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.norm.weight shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.norm.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.5.0.convs.1.all_modules.0.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.all_modules.1.weight shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.all_modules.1.bias shape torch.Size([320])
decoder.stages.0.convs.0.conv.weight shape torch.Size([320, 640, 3, 3, 3])
decoder.stages.0.convs.0.conv.bias shape torch.Size([320])
decoder.stages.0.convs.0.norm.weight shape torch.Size([320])
decoder.stages.0.convs.0.norm.bias shape torch.Size([320])
decoder.stages.0.convs.0.all_modules.0.weight shape torch.Size([320, 640, 3, 3, 3])
decoder.stages.0.convs.0.all_modules.0.bias shape torch.Size([320])
decoder.stages.0.convs.0.all_modules.1.weight shape torch.Size([320])
decoder.stages.0.convs.0.all_modules.1.bias shape torch.Size([320])
decoder.stages.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.stages.0.convs.1.conv.bias shape torch.Size([320])
decoder.stages.0.convs.1.norm.weight shape torch.Size([320])
decoder.stages.0.convs.1.norm.bias shape torch.Size([320])
decoder.stages.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.stages.0.convs.1.all_modules.0.bias shape torch.Size([320])
decoder.stages.0.convs.1.all_modules.1.weight shape torch.Size([320])
decoder.stages.0.convs.1.all_modules.1.bias shape torch.Size([320])
decoder.stages.1.convs.0.conv.weight shape torch.Size([256, 512, 3, 3, 3])
decoder.stages.1.convs.0.conv.bias shape torch.Size([256])
decoder.stages.1.convs.0.norm.weight shape torch.Size([256])
decoder.stages.1.convs.0.norm.bias shape torch.Size([256])
decoder.stages.1.convs.0.all_modules.0.weight shape torch.Size([256, 512, 3, 3, 3])
decoder.stages.1.convs.0.all_modules.0.bias shape torch.Size([256])
decoder.stages.1.convs.0.all_modules.1.weight shape torch.Size([256])
decoder.stages.1.convs.0.all_modules.1.bias shape torch.Size([256])
decoder.stages.1.convs.1.conv.weight shape torch.Size([256, 256, 3, 3, 3])
decoder.stages.1.convs.1.conv.bias shape torch.Size([256])
decoder.stages.1.convs.1.norm.weight shape torch.Size([256])
decoder.stages.1.convs.1.norm.bias shape torch.Size([256])
decoder.stages.1.convs.1.all_modules.0.weight shape torch.Size([256, 256, 3, 3, 3])
decoder.stages.1.convs.1.all_modules.0.bias shape torch.Size([256])
decoder.stages.1.convs.1.all_modules.1.weight shape torch.Size([256])
decoder.stages.1.convs.1.all_modules.1.bias shape torch.Size([256])
decoder.stages.2.convs.0.conv.weight shape torch.Size([128, 256, 3, 3, 3])
decoder.stages.2.convs.0.conv.bias shape torch.Size([128])
decoder.stages.2.convs.0.norm.weight shape torch.Size([128])
decoder.stages.2.convs.0.norm.bias shape torch.Size([128])
decoder.stages.2.convs.0.all_modules.0.weight shape torch.Size([128, 256, 3, 3, 3])
decoder.stages.2.convs.0.all_modules.0.bias shape torch.Size([128])
decoder.stages.2.convs.0.all_modules.1.weight shape torch.Size([128])
decoder.stages.2.convs.0.all_modules.1.bias shape torch.Size([128])
decoder.stages.2.convs.1.conv.weight shape torch.Size([128, 128, 3, 3, 3])
decoder.stages.2.convs.1.conv.bias shape torch.Size([128])
decoder.stages.2.convs.1.norm.weight shape torch.Size([128])
decoder.stages.2.convs.1.norm.bias shape torch.Size([128])
decoder.stages.2.convs.1.all_modules.0.weight shape torch.Size([128, 128, 3, 3, 3])
decoder.stages.2.convs.1.all_modules.0.bias shape torch.Size([128])
decoder.stages.2.convs.1.all_modules.1.weight shape torch.Size([128])
decoder.stages.2.convs.1.all_modules.1.bias shape torch.Size([128])
decoder.stages.3.convs.0.conv.weight shape torch.Size([64, 128, 3, 3, 3])
decoder.stages.3.convs.0.conv.bias shape torch.Size([64])
decoder.stages.3.convs.0.norm.weight shape torch.Size([64])
decoder.stages.3.convs.0.norm.bias shape torch.Size([64])
decoder.stages.3.convs.0.all_modules.0.weight shape torch.Size([64, 128, 3, 3, 3])
decoder.stages.3.convs.0.all_modules.0.bias shape torch.Size([64])
decoder.stages.3.convs.0.all_modules.1.weight shape torch.Size([64])
decoder.stages.3.convs.0.all_modules.1.bias shape torch.Size([64])
decoder.stages.3.convs.1.conv.weight shape torch.Size([64, 64, 3, 3, 3])
decoder.stages.3.convs.1.conv.bias shape torch.Size([64])
decoder.stages.3.convs.1.norm.weight shape torch.Size([64])
decoder.stages.3.convs.1.norm.bias shape torch.Size([64])
decoder.stages.3.convs.1.all_modules.0.weight shape torch.Size([64, 64, 3, 3, 3])
decoder.stages.3.convs.1.all_modules.0.bias shape torch.Size([64])
decoder.stages.3.convs.1.all_modules.1.weight shape torch.Size([64])
decoder.stages.3.convs.1.all_modules.1.bias shape torch.Size([64])
decoder.stages.4.convs.0.conv.weight shape torch.Size([32, 64, 3, 3, 3])
decoder.stages.4.convs.0.conv.bias shape torch.Size([32])
decoder.stages.4.convs.0.norm.weight shape torch.Size([32])
decoder.stages.4.convs.0.norm.bias shape torch.Size([32])
decoder.stages.4.convs.0.all_modules.0.weight shape torch.Size([32, 64, 3, 3, 3])
decoder.stages.4.convs.0.all_modules.0.bias shape torch.Size([32])
decoder.stages.4.convs.0.all_modules.1.weight shape torch.Size([32])
decoder.stages.4.convs.0.all_modules.1.bias shape torch.Size([32])
decoder.stages.4.convs.1.conv.weight shape torch.Size([32, 32, 3, 3, 3])
decoder.stages.4.convs.1.conv.bias shape torch.Size([32])
decoder.stages.4.convs.1.norm.weight shape torch.Size([32])
decoder.stages.4.convs.1.norm.bias shape torch.Size([32])
decoder.stages.4.convs.1.all_modules.0.weight shape torch.Size([32, 32, 3, 3, 3])
decoder.stages.4.convs.1.all_modules.0.bias shape torch.Size([32])
decoder.stages.4.convs.1.all_modules.1.weight shape torch.Size([32])
decoder.stages.4.convs.1.all_modules.1.bias shape torch.Size([32])
decoder.transpconvs.0.weight shape torch.Size([320, 320, 1, 2, 2])
decoder.transpconvs.0.bias shape torch.Size([320])
decoder.transpconvs.1.weight shape torch.Size([320, 256, 2, 2, 2])
decoder.transpconvs.1.bias shape torch.Size([256])
decoder.transpconvs.2.weight shape torch.Size([256, 128, 2, 2, 2])
decoder.transpconvs.2.bias shape torch.Size([128])
decoder.transpconvs.3.weight shape torch.Size([128, 64, 2, 2, 2])
decoder.transpconvs.3.bias shape torch.Size([64])
decoder.transpconvs.4.weight shape torch.Size([64, 32, 2, 2, 2])
decoder.transpconvs.4.bias shape torch.Size([32])
################### Done ###################
2024-12-20 16:33:07.090557: do_dummy_2d_data_aug: True
2024-12-20 16:33:07.092974: Using splits from existing split file: /home/gridsan/nchutisilp/datasets/nnUNet_Datasets/nnUNet_preprocessed/Dataset308_Sohee_Calcium_OCT_CrossValidation/splits_final_80.json
2024-12-20 16:33:07.095534: The split file contains 5 splits.
2024-12-20 16:33:07.096728: Desired fold for training: 4
2024-12-20 16:33:07.097866: This split has 6 training and 5 validation cases.
using pin_memory on device 0
using pin_memory on device 0

This is the configuration used by this training:
Configuration name: 3d_32x160x128_b10
 {'data_identifier': 'nnUNetPlans_3d_fullres', 'preprocessor_name': 'DefaultPreprocessor', 'batch_size': 10, 'patch_size': [32, 160, 128], 'median_image_size_in_voxels': [375.0, 498.0, 498.0], 'spacing': [1.0, 1.0, 1.0], 'normalization_schemes': ['ZScoreNormalization'], 'use_mask_for_norm': [False], 'resampling_fn_data': 'resample_data_or_seg_to_shape', 'resampling_fn_seg': 'resample_data_or_seg_to_shape', 'resampling_fn_data_kwargs': {'is_seg': False, 'order': 3, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_seg_kwargs': {'is_seg': True, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_probabilities': 'resample_data_or_seg_to_shape', 'resampling_fn_probabilities_kwargs': {'is_seg': False, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'architecture': {'network_class_name': 'dynamic_network_architectures.architectures.unet.PlainConvUNet', 'arch_kwargs': {'n_stages': 6, 'features_per_stage': [32, 64, 128, 256, 320, 320], 'conv_op': 'torch.nn.modules.conv.Conv3d', 'kernel_sizes': [[3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3]], 'strides': [[1, 1, 1], [2, 2, 2], [2, 2, 2], [2, 2, 2], [2, 2, 2], [1, 2, 2]], 'n_conv_per_stage': [2, 2, 2, 2, 2, 2], 'n_conv_per_stage_decoder': [2, 2, 2, 2, 2], 'conv_bias': True, 'norm_op': 'torch.nn.modules.instancenorm.InstanceNorm3d', 'norm_op_kwargs': {'eps': 1e-05, 'affine': True}, 'dropout_op': None, 'dropout_op_kwargs': None, 'nonlin': 'torch.nn.LeakyReLU', 'nonlin_kwargs': {'inplace': True}, 'deep_supervision': True}, '_kw_requires_import': ['conv_op', 'norm_op', 'dropout_op', 'nonlin']}, 'batch_dice': True, 'inherits_from': '3d_fullres'} 

These are the global plan.json settings:
 {'dataset_name': 'Dataset308_Sohee_Calcium_OCT_CrossValidation', 'plans_name': 'nnUNetPlans', 'original_median_spacing_after_transp': [1.0, 1.0, 1.0], 'original_median_shape_after_transp': [375, 498, 498], 'image_reader_writer': 'SimpleITKIO', 'transpose_forward': [0, 1, 2], 'transpose_backward': [0, 1, 2], 'experiment_planner_used': 'ExperimentPlanner', 'label_manager': 'LabelManager', 'foreground_intensity_properties_per_channel': {'0': {'max': 0.4977976381778717, 'mean': 0.13507525622844696, 'median': 0.09599608182907104, 'min': 0.0, 'percentile_00_5': 0.014754901640117168, 'percentile_99_5': 0.4977976381778717, 'std': 0.12025152146816254}}} 

2024-12-20 16:33:34.944649: unpacking dataset...
2024-12-20 16:33:39.364984: unpacking done...
2024-12-20 16:33:39.609456: Unable to plot network architecture: nnUNet_compile is enabled!
2024-12-20 16:33:39.929098: 
2024-12-20 16:33:39.930386: Epoch 0
2024-12-20 16:33:39.932082: Current learning rate: 0.01
2024-12-20 16:38:03.806181: Validation loss improved from 1000.00000 to -0.16916! Patience: 0/50
2024-12-20 16:38:03.807023: train_loss -0.0754
2024-12-20 16:38:03.808125: val_loss -0.1692
2024-12-20 16:38:03.808880: Pseudo dice [0.524]
2024-12-20 16:38:03.809520: Epoch time: 263.88 s
2024-12-20 16:38:03.810163: Yayy! New best EMA pseudo Dice: 0.524
2024-12-20 16:38:05.867802: 
2024-12-20 16:38:05.869128: Epoch 1
2024-12-20 16:38:05.870187: Current learning rate: 0.00994
2024-12-20 16:41:43.661661: Validation loss improved from -0.16916 to -0.18754! Patience: 0/50
2024-12-20 16:41:43.663100: train_loss -0.2237
2024-12-20 16:41:43.663911: val_loss -0.1875
2024-12-20 16:41:43.664613: Pseudo dice [0.5026]
2024-12-20 16:41:43.665276: Epoch time: 217.8 s
2024-12-20 16:41:45.066172: 
2024-12-20 16:41:45.068066: Epoch 2
2024-12-20 16:41:45.068893: Current learning rate: 0.00988
2024-12-20 16:45:00.851236: Validation loss improved from -0.18754 to -0.30546! Patience: 0/50
2024-12-20 16:45:00.852247: train_loss -0.2689
2024-12-20 16:45:00.853133: val_loss -0.3055
2024-12-20 16:45:00.853797: Pseudo dice [0.6046]
2024-12-20 16:45:00.854703: Epoch time: 195.79 s
2024-12-20 16:45:00.855659: Yayy! New best EMA pseudo Dice: 0.5302
2024-12-20 16:45:02.722096: 
2024-12-20 16:45:02.724466: Epoch 3
2024-12-20 16:45:02.726071: Current learning rate: 0.00982
2024-12-20 16:47:58.176064: Validation loss did not improve from -0.30546. Patience: 1/50
2024-12-20 16:47:58.178311: train_loss -0.3212
2024-12-20 16:47:58.179356: val_loss -0.3038
2024-12-20 16:47:58.180166: Pseudo dice [0.6115]
2024-12-20 16:47:58.180951: Epoch time: 175.46 s
2024-12-20 16:47:58.181581: Yayy! New best EMA pseudo Dice: 0.5383
2024-12-20 16:47:59.979261: 
2024-12-20 16:47:59.981709: Epoch 4
2024-12-20 16:47:59.982975: Current learning rate: 0.00976
2024-12-20 16:50:54.680135: Validation loss did not improve from -0.30546. Patience: 2/50
2024-12-20 16:50:54.681267: train_loss -0.3447
2024-12-20 16:50:54.682400: val_loss -0.3042
2024-12-20 16:50:54.684374: Pseudo dice [0.6078]
2024-12-20 16:50:54.685755: Epoch time: 174.7 s
2024-12-20 16:50:55.077107: Yayy! New best EMA pseudo Dice: 0.5452
2024-12-20 16:50:56.866605: 
2024-12-20 16:50:56.868240: Epoch 5
2024-12-20 16:50:56.868878: Current learning rate: 0.0097
2024-12-20 16:53:43.323109: Validation loss improved from -0.30546 to -0.36589! Patience: 2/50
2024-12-20 16:53:43.324020: train_loss -0.3877
2024-12-20 16:53:43.324967: val_loss -0.3659
2024-12-20 16:53:43.325899: Pseudo dice [0.6429]
2024-12-20 16:53:43.326682: Epoch time: 166.46 s
2024-12-20 16:53:43.327465: Yayy! New best EMA pseudo Dice: 0.555
2024-12-20 16:53:45.122893: 
2024-12-20 16:53:45.124095: Epoch 6
2024-12-20 16:53:45.124958: Current learning rate: 0.00964
2024-12-20 16:56:30.799509: Validation loss improved from -0.36589 to -0.39597! Patience: 0/50
2024-12-20 16:56:30.800395: train_loss -0.4203
2024-12-20 16:56:30.801342: val_loss -0.396
2024-12-20 16:56:30.802119: Pseudo dice [0.6596]
2024-12-20 16:56:30.803086: Epoch time: 165.68 s
2024-12-20 16:56:30.803857: Yayy! New best EMA pseudo Dice: 0.5655
2024-12-20 16:56:32.616423: 
2024-12-20 16:56:32.617472: Epoch 7
2024-12-20 16:56:32.618405: Current learning rate: 0.00958
2024-12-20 16:59:39.150795: Validation loss improved from -0.39597 to -0.39830! Patience: 0/50
2024-12-20 16:59:39.152139: train_loss -0.4347
2024-12-20 16:59:39.152923: val_loss -0.3983
2024-12-20 16:59:39.153537: Pseudo dice [0.6544]
2024-12-20 16:59:39.154172: Epoch time: 186.54 s
2024-12-20 16:59:39.154893: Yayy! New best EMA pseudo Dice: 0.5744
2024-12-20 16:59:41.330866: 
2024-12-20 16:59:41.332025: Epoch 8
2024-12-20 16:59:41.332732: Current learning rate: 0.00952
2024-12-20 17:02:00.051653: Validation loss improved from -0.39830 to -0.41626! Patience: 0/50
2024-12-20 17:02:00.053847: train_loss -0.457
2024-12-20 17:02:00.055511: val_loss -0.4163
2024-12-20 17:02:00.056897: Pseudo dice [0.6675]
2024-12-20 17:02:00.058685: Epoch time: 138.72 s
2024-12-20 17:02:00.059988: Yayy! New best EMA pseudo Dice: 0.5837
2024-12-20 17:02:01.923090: 
2024-12-20 17:02:01.925019: Epoch 9
2024-12-20 17:02:01.925764: Current learning rate: 0.00946
2024-12-20 17:05:23.391473: Validation loss did not improve from -0.41626. Patience: 1/50
2024-12-20 17:05:23.392406: train_loss -0.4662
2024-12-20 17:05:23.393086: val_loss -0.384
2024-12-20 17:05:23.393719: Pseudo dice [0.6516]
2024-12-20 17:05:23.394662: Epoch time: 201.47 s
2024-12-20 17:05:23.784669: Yayy! New best EMA pseudo Dice: 0.5905
2024-12-20 17:05:25.501521: 
2024-12-20 17:05:25.502567: Epoch 10
2024-12-20 17:05:25.503247: Current learning rate: 0.0094
2024-12-20 17:08:30.012905: Validation loss improved from -0.41626 to -0.42603! Patience: 1/50
2024-12-20 17:08:30.013664: train_loss -0.4805
2024-12-20 17:08:30.014748: val_loss -0.426
2024-12-20 17:08:30.015623: Pseudo dice [0.6694]
2024-12-20 17:08:30.016730: Epoch time: 184.51 s
2024-12-20 17:08:30.017781: Yayy! New best EMA pseudo Dice: 0.5984
2024-12-20 17:08:31.826223: 
2024-12-20 17:08:31.827751: Epoch 11
2024-12-20 17:08:31.828796: Current learning rate: 0.00934
2024-12-20 17:11:08.243683: Validation loss did not improve from -0.42603. Patience: 1/50
2024-12-20 17:11:08.244676: train_loss -0.5021
2024-12-20 17:11:08.246045: val_loss -0.4174
2024-12-20 17:11:08.247428: Pseudo dice [0.6657]
2024-12-20 17:11:08.248635: Epoch time: 156.42 s
2024-12-20 17:11:08.249951: Yayy! New best EMA pseudo Dice: 0.6051
2024-12-20 17:11:10.034323: 
2024-12-20 17:11:10.036184: Epoch 12
2024-12-20 17:11:10.038323: Current learning rate: 0.00928
2024-12-20 17:14:03.264454: Validation loss improved from -0.42603 to -0.45713! Patience: 1/50
2024-12-20 17:14:03.265322: train_loss -0.512
2024-12-20 17:14:03.265980: val_loss -0.4571
2024-12-20 17:14:03.266898: Pseudo dice [0.6961]
2024-12-20 17:14:03.267730: Epoch time: 173.23 s
2024-12-20 17:14:03.268349: Yayy! New best EMA pseudo Dice: 0.6142
2024-12-20 17:14:05.026861: 
2024-12-20 17:14:05.027925: Epoch 13
2024-12-20 17:14:05.028637: Current learning rate: 0.00922
2024-12-20 17:17:25.171299: Validation loss did not improve from -0.45713. Patience: 1/50
2024-12-20 17:17:25.172376: train_loss -0.5145
2024-12-20 17:17:25.173458: val_loss -0.4256
2024-12-20 17:17:25.174751: Pseudo dice [0.6735]
2024-12-20 17:17:25.175777: Epoch time: 200.15 s
2024-12-20 17:17:25.177415: Yayy! New best EMA pseudo Dice: 0.6201
2024-12-20 17:17:26.918906: 
2024-12-20 17:17:26.920258: Epoch 14
2024-12-20 17:17:26.921123: Current learning rate: 0.00916
2024-12-20 17:20:34.656902: Validation loss did not improve from -0.45713. Patience: 2/50
2024-12-20 17:20:34.657836: train_loss -0.5383
2024-12-20 17:20:34.658752: val_loss -0.4154
2024-12-20 17:20:34.659673: Pseudo dice [0.6559]
2024-12-20 17:20:34.660560: Epoch time: 187.74 s
2024-12-20 17:20:35.070190: Yayy! New best EMA pseudo Dice: 0.6237
2024-12-20 17:20:36.862779: 
2024-12-20 17:20:36.864273: Epoch 15
2024-12-20 17:20:36.865272: Current learning rate: 0.0091
2024-12-20 17:23:29.977870: Validation loss did not improve from -0.45713. Patience: 3/50
2024-12-20 17:23:29.978846: train_loss -0.5364
2024-12-20 17:23:29.979761: val_loss -0.4295
2024-12-20 17:23:29.980481: Pseudo dice [0.6792]
2024-12-20 17:23:29.981285: Epoch time: 173.12 s
2024-12-20 17:23:29.981923: Yayy! New best EMA pseudo Dice: 0.6292
2024-12-20 17:23:31.869792: 
2024-12-20 17:23:31.871588: Epoch 16
2024-12-20 17:23:31.873235: Current learning rate: 0.00903
2024-12-20 17:26:07.511612: Validation loss did not improve from -0.45713. Patience: 4/50
2024-12-20 17:26:07.512496: train_loss -0.5542
2024-12-20 17:26:07.513286: val_loss -0.4391
2024-12-20 17:26:07.513963: Pseudo dice [0.6704]
2024-12-20 17:26:07.514710: Epoch time: 155.64 s
2024-12-20 17:26:07.515357: Yayy! New best EMA pseudo Dice: 0.6334
2024-12-20 17:26:09.371720: 
2024-12-20 17:26:09.372885: Epoch 17
2024-12-20 17:26:09.373880: Current learning rate: 0.00897
2024-12-20 17:28:44.997039: Validation loss improved from -0.45713 to -0.47010! Patience: 4/50
2024-12-20 17:28:44.997854: train_loss -0.5518
2024-12-20 17:28:44.998557: val_loss -0.4701
2024-12-20 17:28:44.999201: Pseudo dice [0.6963]
2024-12-20 17:28:44.999847: Epoch time: 155.63 s
2024-12-20 17:28:45.000472: Yayy! New best EMA pseudo Dice: 0.6397
2024-12-20 17:28:47.232854: 
2024-12-20 17:28:47.233850: Epoch 18
2024-12-20 17:28:47.234573: Current learning rate: 0.00891
2024-12-20 17:33:19.581835: Validation loss did not improve from -0.47010. Patience: 1/50
2024-12-20 17:33:19.582528: train_loss -0.5634
2024-12-20 17:33:19.583572: val_loss -0.4323
2024-12-20 17:33:19.584351: Pseudo dice [0.6772]
2024-12-20 17:33:19.585101: Epoch time: 272.35 s
2024-12-20 17:33:19.585820: Yayy! New best EMA pseudo Dice: 0.6434
2024-12-20 17:33:21.408623: 
2024-12-20 17:33:21.410120: Epoch 19
2024-12-20 17:33:21.411760: Current learning rate: 0.00885
2024-12-20 17:38:26.052097: Validation loss did not improve from -0.47010. Patience: 2/50
2024-12-20 17:38:26.053041: train_loss -0.5735
2024-12-20 17:38:26.053767: val_loss -0.4515
2024-12-20 17:38:26.054445: Pseudo dice [0.692]
2024-12-20 17:38:26.055188: Epoch time: 304.65 s
2024-12-20 17:38:26.447498: Yayy! New best EMA pseudo Dice: 0.6483
2024-12-20 17:38:28.376259: 
2024-12-20 17:38:28.377294: Epoch 20
2024-12-20 17:38:28.378245: Current learning rate: 0.00879
2024-12-20 17:43:51.443605: Validation loss improved from -0.47010 to -0.49136! Patience: 2/50
2024-12-20 17:43:51.468070: train_loss -0.5779
2024-12-20 17:43:51.470778: val_loss -0.4914
2024-12-20 17:43:51.471921: Pseudo dice [0.7071]
2024-12-20 17:43:51.473047: Epoch time: 323.09 s
2024-12-20 17:43:51.473925: Yayy! New best EMA pseudo Dice: 0.6542
2024-12-20 17:43:53.373711: 
2024-12-20 17:43:53.375205: Epoch 21
2024-12-20 17:43:53.376308: Current learning rate: 0.00873
2024-12-20 17:49:32.830690: Validation loss did not improve from -0.49136. Patience: 1/50
2024-12-20 17:49:32.831343: train_loss -0.5786
2024-12-20 17:49:32.832196: val_loss -0.3859
2024-12-20 17:49:32.832910: Pseudo dice [0.6476]
2024-12-20 17:49:32.833575: Epoch time: 339.46 s
2024-12-20 17:49:34.229978: 
2024-12-20 17:49:34.231437: Epoch 22
2024-12-20 17:49:34.232781: Current learning rate: 0.00867
2024-12-20 17:56:14.514203: Validation loss did not improve from -0.49136. Patience: 2/50
2024-12-20 17:56:14.515120: train_loss -0.5858
2024-12-20 17:56:14.516509: val_loss -0.4652
2024-12-20 17:56:14.517930: Pseudo dice [0.6906]
2024-12-20 17:56:14.519299: Epoch time: 400.29 s
2024-12-20 17:56:14.520621: Yayy! New best EMA pseudo Dice: 0.6572
2024-12-20 17:56:16.266568: 
2024-12-20 17:56:16.267880: Epoch 23
2024-12-20 17:56:16.268791: Current learning rate: 0.00861
2024-12-20 18:01:38.877815: Validation loss did not improve from -0.49136. Patience: 3/50
2024-12-20 18:01:38.878798: train_loss -0.5968
2024-12-20 18:01:38.879561: val_loss -0.462
2024-12-20 18:01:38.880220: Pseudo dice [0.6915]
2024-12-20 18:01:38.880937: Epoch time: 322.61 s
2024-12-20 18:01:38.881533: Yayy! New best EMA pseudo Dice: 0.6606
2024-12-20 18:01:40.606072: 
2024-12-20 18:01:40.607469: Epoch 24
2024-12-20 18:01:40.608733: Current learning rate: 0.00855
2024-12-20 18:05:54.370183: Validation loss did not improve from -0.49136. Patience: 4/50
2024-12-20 18:05:54.371128: train_loss -0.6004
2024-12-20 18:05:54.371882: val_loss -0.4135
2024-12-20 18:05:54.372617: Pseudo dice [0.6535]
2024-12-20 18:05:54.373442: Epoch time: 253.77 s
2024-12-20 18:05:56.226150: 
2024-12-20 18:05:56.227831: Epoch 25
2024-12-20 18:05:56.228634: Current learning rate: 0.00849
2024-12-20 18:10:11.143128: Validation loss did not improve from -0.49136. Patience: 5/50
2024-12-20 18:10:11.144942: train_loss -0.6014
2024-12-20 18:10:11.146345: val_loss -0.4526
2024-12-20 18:10:11.147189: Pseudo dice [0.6899]
2024-12-20 18:10:11.148186: Epoch time: 254.92 s
2024-12-20 18:10:11.149736: Yayy! New best EMA pseudo Dice: 0.6629
2024-12-20 18:10:12.958196: 
2024-12-20 18:10:12.959524: Epoch 26
2024-12-20 18:10:12.960417: Current learning rate: 0.00843
2024-12-20 18:14:07.602104: Validation loss did not improve from -0.49136. Patience: 6/50
2024-12-20 18:14:07.603853: train_loss -0.6083
2024-12-20 18:14:07.604898: val_loss -0.4568
2024-12-20 18:14:07.605614: Pseudo dice [0.6895]
2024-12-20 18:14:07.606330: Epoch time: 234.65 s
2024-12-20 18:14:07.606939: Yayy! New best EMA pseudo Dice: 0.6656
2024-12-20 18:14:09.404649: 
2024-12-20 18:14:09.406066: Epoch 27
2024-12-20 18:14:09.407156: Current learning rate: 0.00836
2024-12-20 18:18:06.590313: Validation loss did not improve from -0.49136. Patience: 7/50
2024-12-20 18:18:06.591326: train_loss -0.611
2024-12-20 18:18:06.592257: val_loss -0.4778
2024-12-20 18:18:06.592934: Pseudo dice [0.7009]
2024-12-20 18:18:06.593601: Epoch time: 237.19 s
2024-12-20 18:18:06.594312: Yayy! New best EMA pseudo Dice: 0.6691
2024-12-20 18:18:08.458922: 
2024-12-20 18:18:08.461157: Epoch 28
2024-12-20 18:18:08.463223: Current learning rate: 0.0083
2024-12-20 18:22:16.203178: Validation loss did not improve from -0.49136. Patience: 8/50
2024-12-20 18:22:16.204167: train_loss -0.6248
2024-12-20 18:22:16.205028: val_loss -0.4698
2024-12-20 18:22:16.205675: Pseudo dice [0.6948]
2024-12-20 18:22:16.206294: Epoch time: 247.75 s
2024-12-20 18:22:16.206870: Yayy! New best EMA pseudo Dice: 0.6717
2024-12-20 18:22:18.601919: 
2024-12-20 18:22:18.603190: Epoch 29
2024-12-20 18:22:18.603910: Current learning rate: 0.00824
2024-12-20 18:26:10.029645: Validation loss did not improve from -0.49136. Patience: 9/50
2024-12-20 18:26:10.030546: train_loss -0.6185
2024-12-20 18:26:10.031302: val_loss -0.4822
2024-12-20 18:26:10.032142: Pseudo dice [0.7138]
2024-12-20 18:26:10.033091: Epoch time: 231.43 s
2024-12-20 18:26:10.462751: Yayy! New best EMA pseudo Dice: 0.6759
2024-12-20 18:26:12.274892: 
2024-12-20 18:26:12.276145: Epoch 30
2024-12-20 18:26:12.276976: Current learning rate: 0.00818
2024-12-20 18:31:16.715092: Validation loss did not improve from -0.49136. Patience: 10/50
2024-12-20 18:31:16.716000: train_loss -0.6263
2024-12-20 18:31:16.716804: val_loss -0.4465
2024-12-20 18:31:16.717462: Pseudo dice [0.6849]
2024-12-20 18:31:16.718519: Epoch time: 304.44 s
2024-12-20 18:31:16.719717: Yayy! New best EMA pseudo Dice: 0.6768
2024-12-20 18:31:18.594911: 
2024-12-20 18:31:18.597040: Epoch 31
2024-12-20 18:31:18.598044: Current learning rate: 0.00812
2024-12-20 18:38:06.844620: Validation loss did not improve from -0.49136. Patience: 11/50
2024-12-20 18:38:06.848850: train_loss -0.6376
2024-12-20 18:38:06.851035: val_loss -0.4494
2024-12-20 18:38:06.852375: Pseudo dice [0.686]
2024-12-20 18:38:06.853507: Epoch time: 408.26 s
2024-12-20 18:38:06.854445: Yayy! New best EMA pseudo Dice: 0.6777
2024-12-20 18:38:08.695794: 
2024-12-20 18:38:08.696718: Epoch 32
2024-12-20 18:38:08.697727: Current learning rate: 0.00806
2024-12-20 18:43:52.871766: Validation loss did not improve from -0.49136. Patience: 12/50
2024-12-20 18:43:52.877050: train_loss -0.6313
2024-12-20 18:43:52.879238: val_loss -0.465
2024-12-20 18:43:52.880109: Pseudo dice [0.6989]
2024-12-20 18:43:52.881229: Epoch time: 344.18 s
2024-12-20 18:43:52.881986: Yayy! New best EMA pseudo Dice: 0.6798
2024-12-20 18:43:54.741089: 
2024-12-20 18:43:54.742887: Epoch 33
2024-12-20 18:43:54.743674: Current learning rate: 0.008
2024-12-20 18:49:12.706882: Validation loss did not improve from -0.49136. Patience: 13/50
2024-12-20 18:49:12.708290: train_loss -0.6298
2024-12-20 18:49:12.708964: val_loss -0.4175
2024-12-20 18:49:12.709702: Pseudo dice [0.6561]
2024-12-20 18:49:12.710501: Epoch time: 317.97 s
2024-12-20 18:49:14.159507: 
2024-12-20 18:49:14.160928: Epoch 34
2024-12-20 18:49:14.161721: Current learning rate: 0.00793
2024-12-20 18:54:41.837856: Validation loss did not improve from -0.49136. Patience: 14/50
2024-12-20 18:54:41.838769: train_loss -0.6428
2024-12-20 18:54:41.839683: val_loss -0.4263
2024-12-20 18:54:41.840441: Pseudo dice [0.6735]
2024-12-20 18:54:41.841245: Epoch time: 327.68 s
2024-12-20 18:54:43.720070: 
2024-12-20 18:54:43.721539: Epoch 35
2024-12-20 18:54:43.722375: Current learning rate: 0.00787
2024-12-20 19:00:12.962565: Validation loss did not improve from -0.49136. Patience: 15/50
2024-12-20 19:00:12.963673: train_loss -0.6502
2024-12-20 19:00:12.964755: val_loss -0.4791
2024-12-20 19:00:12.965655: Pseudo dice [0.7067]
2024-12-20 19:00:12.966666: Epoch time: 329.25 s
2024-12-20 19:00:12.968899: Yayy! New best EMA pseudo Dice: 0.68
2024-12-20 19:00:14.837988: 
2024-12-20 19:00:14.839359: Epoch 36
2024-12-20 19:00:14.840585: Current learning rate: 0.00781
2024-12-20 19:06:27.853010: Validation loss did not improve from -0.49136. Patience: 16/50
2024-12-20 19:06:27.853809: train_loss -0.6571
2024-12-20 19:06:27.854774: val_loss -0.4417
2024-12-20 19:06:27.855597: Pseudo dice [0.6775]
2024-12-20 19:06:27.856976: Epoch time: 373.02 s
2024-12-20 19:06:29.319061: 
2024-12-20 19:06:29.320444: Epoch 37
2024-12-20 19:06:29.321247: Current learning rate: 0.00775
2024-12-20 19:12:36.179536: Validation loss did not improve from -0.49136. Patience: 17/50
2024-12-20 19:12:36.181293: train_loss -0.6514
2024-12-20 19:12:36.182320: val_loss -0.4618
2024-12-20 19:12:36.182987: Pseudo dice [0.7026]
2024-12-20 19:12:36.184044: Epoch time: 366.86 s
2024-12-20 19:12:36.184726: Yayy! New best EMA pseudo Dice: 0.6821
2024-12-20 19:12:37.990335: 
2024-12-20 19:12:37.991756: Epoch 38
2024-12-20 19:12:37.993077: Current learning rate: 0.00769
2024-12-20 19:18:06.548166: Validation loss did not improve from -0.49136. Patience: 18/50
2024-12-20 19:18:06.549220: train_loss -0.6503
2024-12-20 19:18:06.550229: val_loss -0.4864
2024-12-20 19:18:06.551077: Pseudo dice [0.7144]
2024-12-20 19:18:06.551915: Epoch time: 328.56 s
2024-12-20 19:18:06.552688: Yayy! New best EMA pseudo Dice: 0.6853
2024-12-20 19:18:08.325440: 
2024-12-20 19:18:08.326957: Epoch 39
2024-12-20 19:18:08.327856: Current learning rate: 0.00763
2024-12-20 19:23:20.957455: Validation loss did not improve from -0.49136. Patience: 19/50
2024-12-20 19:23:20.958524: train_loss -0.653
2024-12-20 19:23:20.959930: val_loss -0.4887
2024-12-20 19:23:20.961033: Pseudo dice [0.7154]
2024-12-20 19:23:20.962118: Epoch time: 312.63 s
2024-12-20 19:23:21.757899: Yayy! New best EMA pseudo Dice: 0.6883
2024-12-20 19:23:23.945289: 
2024-12-20 19:23:23.947323: Epoch 40
2024-12-20 19:23:23.948613: Current learning rate: 0.00756
2024-12-20 19:28:37.252708: Validation loss did not improve from -0.49136. Patience: 20/50
2024-12-20 19:28:37.254274: train_loss -0.6591
2024-12-20 19:28:37.255349: val_loss -0.4862
2024-12-20 19:28:37.256049: Pseudo dice [0.7116]
2024-12-20 19:28:37.256947: Epoch time: 313.31 s
2024-12-20 19:28:37.257528: Yayy! New best EMA pseudo Dice: 0.6906
2024-12-20 19:28:39.127084: 
2024-12-20 19:28:39.129111: Epoch 41
2024-12-20 19:28:39.129916: Current learning rate: 0.0075
2024-12-20 19:34:51.023650: Validation loss did not improve from -0.49136. Patience: 21/50
2024-12-20 19:34:51.024701: train_loss -0.6667
2024-12-20 19:34:51.025480: val_loss -0.4794
2024-12-20 19:34:51.026318: Pseudo dice [0.6948]
2024-12-20 19:34:51.027156: Epoch time: 371.9 s
2024-12-20 19:34:51.027951: Yayy! New best EMA pseudo Dice: 0.691
2024-12-20 19:34:52.789195: 
2024-12-20 19:34:52.790654: Epoch 42
2024-12-20 19:34:52.791549: Current learning rate: 0.00744
2024-12-20 19:40:28.737138: Validation loss did not improve from -0.49136. Patience: 22/50
2024-12-20 19:40:28.737954: train_loss -0.6734
2024-12-20 19:40:28.739108: val_loss -0.446
2024-12-20 19:40:28.740151: Pseudo dice [0.6849]
2024-12-20 19:40:28.741104: Epoch time: 335.95 s
2024-12-20 19:40:30.062119: 
2024-12-20 19:40:30.063560: Epoch 43
2024-12-20 19:40:30.064586: Current learning rate: 0.00738
2024-12-20 19:45:42.920983: Validation loss improved from -0.49136 to -0.50157! Patience: 22/50
2024-12-20 19:45:42.924643: train_loss -0.6767
2024-12-20 19:45:42.925483: val_loss -0.5016
2024-12-20 19:45:42.926159: Pseudo dice [0.7198]
2024-12-20 19:45:42.927099: Epoch time: 312.86 s
2024-12-20 19:45:42.927840: Yayy! New best EMA pseudo Dice: 0.6934
2024-12-20 19:45:44.692872: 
2024-12-20 19:45:44.694440: Epoch 44
2024-12-20 19:45:44.695272: Current learning rate: 0.00732
2024-12-20 19:51:17.504973: Validation loss did not improve from -0.50157. Patience: 1/50
2024-12-20 19:51:17.506724: train_loss -0.6702
2024-12-20 19:51:17.508674: val_loss -0.4823
2024-12-20 19:51:17.509494: Pseudo dice [0.7093]
2024-12-20 19:51:17.510309: Epoch time: 332.82 s
2024-12-20 19:51:17.857673: Yayy! New best EMA pseudo Dice: 0.695
2024-12-20 19:51:19.539183: 
2024-12-20 19:51:19.540397: Epoch 45
2024-12-20 19:51:19.541150: Current learning rate: 0.00725
2024-12-20 19:56:17.777186: Validation loss did not improve from -0.50157. Patience: 2/50
2024-12-20 19:56:17.778226: train_loss -0.6708
2024-12-20 19:56:17.778898: val_loss -0.4731
2024-12-20 19:56:17.779677: Pseudo dice [0.6963]
2024-12-20 19:56:17.780260: Epoch time: 298.24 s
2024-12-20 19:56:17.781086: Yayy! New best EMA pseudo Dice: 0.6951
2024-12-20 19:56:19.489690: 
2024-12-20 19:56:19.491064: Epoch 46
2024-12-20 19:56:19.491873: Current learning rate: 0.00719
2024-12-20 20:01:25.663393: Validation loss did not improve from -0.50157. Patience: 3/50
2024-12-20 20:01:25.664220: train_loss -0.6838
2024-12-20 20:01:25.664916: val_loss -0.4761
2024-12-20 20:01:25.665540: Pseudo dice [0.7133]
2024-12-20 20:01:25.666132: Epoch time: 306.18 s
2024-12-20 20:01:25.666961: Yayy! New best EMA pseudo Dice: 0.6969
2024-12-20 20:01:27.361809: 
2024-12-20 20:01:27.363369: Epoch 47
2024-12-20 20:01:27.364045: Current learning rate: 0.00713
2024-12-20 20:07:32.968780: Validation loss did not improve from -0.50157. Patience: 4/50
2024-12-20 20:07:32.969658: train_loss -0.6822
2024-12-20 20:07:32.970645: val_loss -0.4908
2024-12-20 20:07:32.971474: Pseudo dice [0.7109]
2024-12-20 20:07:32.972187: Epoch time: 365.61 s
2024-12-20 20:07:32.973336: Yayy! New best EMA pseudo Dice: 0.6983
2024-12-20 20:07:34.685298: 
2024-12-20 20:07:34.686372: Epoch 48
2024-12-20 20:07:34.687116: Current learning rate: 0.00707
2024-12-20 20:13:50.970265: Validation loss did not improve from -0.50157. Patience: 5/50
2024-12-20 20:13:50.971502: train_loss -0.6854
2024-12-20 20:13:50.972553: val_loss -0.5012
2024-12-20 20:13:50.973894: Pseudo dice [0.7131]
2024-12-20 20:13:50.975487: Epoch time: 376.29 s
2024-12-20 20:13:50.977075: Yayy! New best EMA pseudo Dice: 0.6998
2024-12-20 20:13:52.808130: 
2024-12-20 20:13:52.809261: Epoch 49
2024-12-20 20:13:52.809948: Current learning rate: 0.007
2024-12-20 20:18:48.674158: Validation loss did not improve from -0.50157. Patience: 6/50
2024-12-20 20:18:48.675112: train_loss -0.6847
2024-12-20 20:18:48.675875: val_loss -0.4783
2024-12-20 20:18:48.676567: Pseudo dice [0.7053]
2024-12-20 20:18:48.677299: Epoch time: 295.87 s
2024-12-20 20:18:49.080824: Yayy! New best EMA pseudo Dice: 0.7004
2024-12-20 20:18:51.972213: 
2024-12-20 20:18:51.973566: Epoch 50
2024-12-20 20:18:51.974326: Current learning rate: 0.00694
2024-12-20 20:25:15.974219: Validation loss improved from -0.50157 to -0.51306! Patience: 6/50
2024-12-20 20:25:15.975172: train_loss -0.6924
2024-12-20 20:25:15.975939: val_loss -0.5131
2024-12-20 20:25:15.976660: Pseudo dice [0.7257]
2024-12-20 20:25:15.977450: Epoch time: 384.0 s
2024-12-20 20:25:15.978236: Yayy! New best EMA pseudo Dice: 0.7029
2024-12-20 20:25:17.717484: 
2024-12-20 20:25:17.719017: Epoch 51
2024-12-20 20:25:17.719776: Current learning rate: 0.00688
2024-12-20 20:31:06.779944: Validation loss did not improve from -0.51306. Patience: 1/50
2024-12-20 20:31:06.780867: train_loss -0.6904
2024-12-20 20:31:06.781907: val_loss -0.4714
2024-12-20 20:31:06.782965: Pseudo dice [0.7004]
2024-12-20 20:31:06.783916: Epoch time: 349.06 s
2024-12-20 20:31:08.146858: 
2024-12-20 20:31:08.148006: Epoch 52
2024-12-20 20:31:08.148635: Current learning rate: 0.00682
2024-12-20 20:36:07.125295: Validation loss did not improve from -0.51306. Patience: 2/50
2024-12-20 20:36:07.126230: train_loss -0.6942
2024-12-20 20:36:07.127141: val_loss -0.423
2024-12-20 20:36:07.127912: Pseudo dice [0.6746]
2024-12-20 20:36:07.128840: Epoch time: 298.98 s
2024-12-20 20:36:08.481716: 
2024-12-20 20:36:08.483352: Epoch 53
2024-12-20 20:36:08.484211: Current learning rate: 0.00675
2024-12-20 20:40:35.230005: Validation loss did not improve from -0.51306. Patience: 3/50
2024-12-20 20:40:35.230852: train_loss -0.6941
2024-12-20 20:40:35.231647: val_loss -0.4083
2024-12-20 20:40:35.232301: Pseudo dice [0.6659]
2024-12-20 20:40:35.232955: Epoch time: 266.75 s
2024-12-20 20:40:36.578807: 
2024-12-20 20:40:36.579879: Epoch 54
2024-12-20 20:40:36.580653: Current learning rate: 0.00669
2024-12-20 20:45:29.895571: Validation loss did not improve from -0.51306. Patience: 4/50
2024-12-20 20:45:29.896716: train_loss -0.6997
2024-12-20 20:45:29.898011: val_loss -0.4198
2024-12-20 20:45:29.899162: Pseudo dice [0.6793]
2024-12-20 20:45:29.899959: Epoch time: 293.32 s
2024-12-20 20:45:31.642017: 
2024-12-20 20:45:31.643668: Epoch 55
2024-12-20 20:45:31.644532: Current learning rate: 0.00663
2024-12-20 20:51:47.335822: Validation loss did not improve from -0.51306. Patience: 5/50
2024-12-20 20:51:47.339544: train_loss -0.7009
2024-12-20 20:51:47.340751: val_loss -0.4133
2024-12-20 20:51:47.341524: Pseudo dice [0.6581]
2024-12-20 20:51:47.342226: Epoch time: 375.7 s
2024-12-20 20:51:48.761285: 
2024-12-20 20:51:48.762780: Epoch 56
2024-12-20 20:51:48.764339: Current learning rate: 0.00657
2024-12-20 20:58:01.307536: Validation loss did not improve from -0.51306. Patience: 6/50
2024-12-20 20:58:01.308601: train_loss -0.7015
2024-12-20 20:58:01.310776: val_loss -0.4707
2024-12-20 20:58:01.312344: Pseudo dice [0.7061]
2024-12-20 20:58:01.314009: Epoch time: 372.55 s
2024-12-20 20:58:02.820232: 
2024-12-20 20:58:02.821891: Epoch 57
2024-12-20 20:58:02.823065: Current learning rate: 0.0065
2024-12-20 21:03:08.849796: Validation loss did not improve from -0.51306. Patience: 7/50
2024-12-20 21:03:08.851459: train_loss -0.7031
2024-12-20 21:03:08.852416: val_loss -0.4771
2024-12-20 21:03:08.853210: Pseudo dice [0.7033]
2024-12-20 21:03:08.853862: Epoch time: 306.03 s
2024-12-20 21:03:10.261519: 
2024-12-20 21:03:10.263477: Epoch 58
2024-12-20 21:03:10.264467: Current learning rate: 0.00644
2024-12-20 21:08:52.390703: Validation loss did not improve from -0.51306. Patience: 8/50
2024-12-20 21:08:52.391683: train_loss -0.707
2024-12-20 21:08:52.392750: val_loss -0.465
2024-12-20 21:08:52.393650: Pseudo dice [0.7002]
2024-12-20 21:08:52.394460: Epoch time: 342.13 s
2024-12-20 21:08:53.791913: 
2024-12-20 21:08:53.792908: Epoch 59
2024-12-20 21:08:53.793776: Current learning rate: 0.00638
2024-12-20 21:13:39.680219: Validation loss did not improve from -0.51306. Patience: 9/50
2024-12-20 21:13:39.681283: train_loss -0.7058
2024-12-20 21:13:39.681929: val_loss -0.4502
2024-12-20 21:13:39.682939: Pseudo dice [0.678]
2024-12-20 21:13:39.683690: Epoch time: 285.89 s
2024-12-20 21:13:41.509732: 
2024-12-20 21:13:41.511184: Epoch 60
2024-12-20 21:13:41.512646: Current learning rate: 0.00631
2024-12-20 21:18:55.978130: Validation loss did not improve from -0.51306. Patience: 10/50
2024-12-20 21:18:55.979222: train_loss -0.7054
2024-12-20 21:18:55.980183: val_loss -0.4945
2024-12-20 21:18:55.980870: Pseudo dice [0.7208]
2024-12-20 21:18:55.981548: Epoch time: 314.47 s
2024-12-20 21:18:58.638997: 
2024-12-20 21:18:58.640783: Epoch 61
2024-12-20 21:18:58.642417: Current learning rate: 0.00625
2024-12-20 21:24:48.651770: Validation loss did not improve from -0.51306. Patience: 11/50
2024-12-20 21:24:48.653721: train_loss -0.7016
2024-12-20 21:24:48.655246: val_loss -0.5097
2024-12-20 21:24:48.656415: Pseudo dice [0.736]
2024-12-20 21:24:48.657530: Epoch time: 350.02 s
2024-12-20 21:24:50.097420: 
2024-12-20 21:24:50.098692: Epoch 62
2024-12-20 21:24:50.099525: Current learning rate: 0.00619
2024-12-20 21:30:58.222678: Validation loss did not improve from -0.51306. Patience: 12/50
2024-12-20 21:30:58.224587: train_loss -0.717
2024-12-20 21:30:58.225535: val_loss -0.4256
2024-12-20 21:30:58.226208: Pseudo dice [0.6803]
2024-12-20 21:30:58.226859: Epoch time: 368.13 s
2024-12-20 21:30:59.720711: 
2024-12-20 21:30:59.722250: Epoch 63
2024-12-20 21:30:59.723314: Current learning rate: 0.00612
2024-12-20 21:36:23.358321: Validation loss did not improve from -0.51306. Patience: 13/50
2024-12-20 21:36:23.359377: train_loss -0.7138
2024-12-20 21:36:23.360162: val_loss -0.4602
2024-12-20 21:36:23.360969: Pseudo dice [0.6959]
2024-12-20 21:36:23.361914: Epoch time: 323.64 s
2024-12-20 21:36:24.853547: 
2024-12-20 21:36:24.854756: Epoch 64
2024-12-20 21:36:24.855467: Current learning rate: 0.00606
2024-12-20 21:41:49.143646: Validation loss improved from -0.51306 to -0.51843! Patience: 13/50
2024-12-20 21:41:49.145013: train_loss -0.713
2024-12-20 21:41:49.145896: val_loss -0.5184
2024-12-20 21:41:49.146468: Pseudo dice [0.7274]
2024-12-20 21:41:49.147143: Epoch time: 324.29 s
2024-12-20 21:41:50.989619: 
2024-12-20 21:41:50.991441: Epoch 65
2024-12-20 21:41:50.992901: Current learning rate: 0.006
2024-12-20 21:47:07.848638: Validation loss did not improve from -0.51843. Patience: 1/50
2024-12-20 21:47:07.849602: train_loss -0.7135
2024-12-20 21:47:07.850664: val_loss -0.4713
2024-12-20 21:47:07.851530: Pseudo dice [0.7056]
2024-12-20 21:47:07.852548: Epoch time: 316.86 s
2024-12-20 21:47:09.259088: 
2024-12-20 21:47:09.260256: Epoch 66
2024-12-20 21:47:09.261452: Current learning rate: 0.00593
2024-12-20 21:53:04.517341: Validation loss did not improve from -0.51843. Patience: 2/50
2024-12-20 21:53:04.521755: train_loss -0.7219
2024-12-20 21:53:04.522842: val_loss -0.4857
2024-12-20 21:53:04.523818: Pseudo dice [0.7181]
2024-12-20 21:53:04.525149: Epoch time: 355.26 s
2024-12-20 21:53:06.016648: 
2024-12-20 21:53:06.018754: Epoch 67
2024-12-20 21:53:06.019657: Current learning rate: 0.00587
2024-12-20 21:59:36.261041: Validation loss did not improve from -0.51843. Patience: 3/50
2024-12-20 21:59:36.262482: train_loss -0.7181
2024-12-20 21:59:36.263934: val_loss -0.4672
2024-12-20 21:59:36.264839: Pseudo dice [0.6993]
2024-12-20 21:59:36.265741: Epoch time: 390.25 s
2024-12-20 21:59:37.744971: 
2024-12-20 21:59:37.746369: Epoch 68
2024-12-20 21:59:37.747290: Current learning rate: 0.00581
2024-12-20 22:05:27.604083: Validation loss did not improve from -0.51843. Patience: 4/50
2024-12-20 22:05:27.605085: train_loss -0.718
2024-12-20 22:05:27.605807: val_loss -0.4732
2024-12-20 22:05:27.606434: Pseudo dice [0.709]
2024-12-20 22:05:27.607091: Epoch time: 349.86 s
2024-12-20 22:05:27.607733: Yayy! New best EMA pseudo Dice: 0.703
2024-12-20 22:05:29.441293: 
2024-12-20 22:05:29.442724: Epoch 69
2024-12-20 22:05:29.444086: Current learning rate: 0.00574
2024-12-20 22:11:13.748522: Validation loss did not improve from -0.51843. Patience: 5/50
2024-12-20 22:11:13.749577: train_loss -0.7273
2024-12-20 22:11:13.750449: val_loss -0.4378
2024-12-20 22:11:13.751243: Pseudo dice [0.6822]
2024-12-20 22:11:13.751941: Epoch time: 344.31 s
2024-12-20 22:11:15.518581: 
2024-12-20 22:11:15.519966: Epoch 70
2024-12-20 22:11:15.520733: Current learning rate: 0.00568
2024-12-20 22:16:08.442984: Validation loss did not improve from -0.51843. Patience: 6/50
2024-12-20 22:16:08.443871: train_loss -0.7273
2024-12-20 22:16:08.444725: val_loss -0.4634
2024-12-20 22:16:08.445402: Pseudo dice [0.7139]
2024-12-20 22:16:08.446008: Epoch time: 292.93 s
2024-12-20 22:16:09.889346: 
2024-12-20 22:16:09.891645: Epoch 71
2024-12-20 22:16:09.893096: Current learning rate: 0.00562
2024-12-20 22:21:37.814824: Validation loss did not improve from -0.51843. Patience: 7/50
2024-12-20 22:21:37.815783: train_loss -0.7281
2024-12-20 22:21:37.816519: val_loss -0.494
2024-12-20 22:21:37.817254: Pseudo dice [0.7208]
2024-12-20 22:21:37.817889: Epoch time: 327.93 s
2024-12-20 22:21:37.818574: Yayy! New best EMA pseudo Dice: 0.7041
2024-12-20 22:21:40.404386: 
2024-12-20 22:21:40.406730: Epoch 72
2024-12-20 22:21:40.408299: Current learning rate: 0.00555
2024-12-20 22:26:57.409819: Validation loss did not improve from -0.51843. Patience: 8/50
2024-12-20 22:26:57.410636: train_loss -0.7276
2024-12-20 22:26:57.411817: val_loss -0.4449
2024-12-20 22:26:57.412650: Pseudo dice [0.6872]
2024-12-20 22:26:57.414055: Epoch time: 317.01 s
2024-12-20 22:26:58.830916: 
2024-12-20 22:26:58.832367: Epoch 73
2024-12-20 22:26:58.833115: Current learning rate: 0.00549
2024-12-20 22:32:45.864305: Validation loss did not improve from -0.51843. Patience: 9/50
2024-12-20 22:32:45.865416: train_loss -0.7304
2024-12-20 22:32:45.866487: val_loss -0.4875
2024-12-20 22:32:45.867641: Pseudo dice [0.7178]
2024-12-20 22:32:45.868921: Epoch time: 347.04 s
2024-12-20 22:32:47.285013: 
2024-12-20 22:32:47.286978: Epoch 74
2024-12-20 22:32:47.287968: Current learning rate: 0.00542
2024-12-20 22:37:54.101006: Validation loss did not improve from -0.51843. Patience: 10/50
2024-12-20 22:37:54.102052: train_loss -0.7299
2024-12-20 22:37:54.103019: val_loss -0.4782
2024-12-20 22:37:54.104104: Pseudo dice [0.7056]
2024-12-20 22:37:54.105211: Epoch time: 306.82 s
2024-12-20 22:37:54.537945: Yayy! New best EMA pseudo Dice: 0.7041
2024-12-20 22:37:56.495404: 
2024-12-20 22:37:56.497069: Epoch 75
2024-12-20 22:37:56.497829: Current learning rate: 0.00536
2024-12-20 22:43:09.583225: Validation loss did not improve from -0.51843. Patience: 11/50
2024-12-20 22:43:09.584232: train_loss -0.7345
2024-12-20 22:43:09.585484: val_loss -0.4623
2024-12-20 22:43:09.586275: Pseudo dice [0.695]
2024-12-20 22:43:09.587176: Epoch time: 313.09 s
2024-12-20 22:43:11.004395: 
2024-12-20 22:43:11.006056: Epoch 76
2024-12-20 22:43:11.006951: Current learning rate: 0.00529
2024-12-20 22:48:14.684295: Validation loss did not improve from -0.51843. Patience: 12/50
2024-12-20 22:48:14.685787: train_loss -0.7291
2024-12-20 22:48:14.686567: val_loss -0.4554
2024-12-20 22:48:14.687170: Pseudo dice [0.7031]
2024-12-20 22:48:14.687866: Epoch time: 303.68 s
2024-12-20 22:48:16.079677: 
2024-12-20 22:48:16.081340: Epoch 77
2024-12-20 22:48:16.082861: Current learning rate: 0.00523
2024-12-20 22:52:01.765506: Validation loss did not improve from -0.51843. Patience: 13/50
2024-12-20 22:52:01.767816: train_loss -0.7369
2024-12-20 22:52:01.769037: val_loss -0.502
2024-12-20 22:52:01.769895: Pseudo dice [0.7244]
2024-12-20 22:52:01.770811: Epoch time: 225.69 s
2024-12-20 22:52:01.771851: Yayy! New best EMA pseudo Dice: 0.7053
2024-12-20 22:52:03.576271: 
2024-12-20 22:52:03.577422: Epoch 78
2024-12-20 22:52:03.578104: Current learning rate: 0.00517
2024-12-20 22:55:49.544664: Validation loss did not improve from -0.51843. Patience: 14/50
2024-12-20 22:55:49.545789: train_loss -0.7335
2024-12-20 22:55:49.546948: val_loss -0.482
2024-12-20 22:55:49.548087: Pseudo dice [0.7216]
2024-12-20 22:55:49.549288: Epoch time: 225.97 s
2024-12-20 22:55:49.550430: Yayy! New best EMA pseudo Dice: 0.7069
2024-12-20 22:55:51.454365: 
2024-12-20 22:55:51.456649: Epoch 79
2024-12-20 22:55:51.457747: Current learning rate: 0.0051
2024-12-20 22:59:33.786260: Validation loss did not improve from -0.51843. Patience: 15/50
2024-12-20 22:59:33.803724: train_loss -0.7364
2024-12-20 22:59:33.804825: val_loss -0.4856
2024-12-20 22:59:33.805509: Pseudo dice [0.7105]
2024-12-20 22:59:33.806335: Epoch time: 222.34 s
2024-12-20 22:59:34.182050: Yayy! New best EMA pseudo Dice: 0.7073
2024-12-20 22:59:36.015441: 
2024-12-20 22:59:36.017242: Epoch 80
2024-12-20 22:59:36.018372: Current learning rate: 0.00504
2024-12-20 23:03:17.036821: Validation loss did not improve from -0.51843. Patience: 16/50
2024-12-20 23:03:17.039281: train_loss -0.7376
2024-12-20 23:03:17.040948: val_loss -0.4458
2024-12-20 23:03:17.041725: Pseudo dice [0.6991]
2024-12-20 23:03:17.042654: Epoch time: 221.03 s
2024-12-20 23:03:18.560027: 
2024-12-20 23:03:18.561843: Epoch 81
2024-12-20 23:03:18.563042: Current learning rate: 0.00497
2024-12-20 23:07:05.894599: Validation loss did not improve from -0.51843. Patience: 17/50
2024-12-20 23:07:05.895679: train_loss -0.7396
2024-12-20 23:07:05.897348: val_loss -0.4887
2024-12-20 23:07:05.898585: Pseudo dice [0.7212]
2024-12-20 23:07:05.899998: Epoch time: 227.34 s
2024-12-20 23:07:05.901519: Yayy! New best EMA pseudo Dice: 0.7079
2024-12-20 23:07:08.522401: 
2024-12-20 23:07:08.523663: Epoch 82
2024-12-20 23:07:08.524549: Current learning rate: 0.00491
2024-12-20 23:11:25.147904: Validation loss did not improve from -0.51843. Patience: 18/50
2024-12-20 23:11:25.149310: train_loss -0.7438
2024-12-20 23:11:25.150362: val_loss -0.5122
2024-12-20 23:11:25.151249: Pseudo dice [0.7299]
2024-12-20 23:11:25.152510: Epoch time: 256.63 s
2024-12-20 23:11:25.153654: Yayy! New best EMA pseudo Dice: 0.7101
2024-12-20 23:11:26.998984: 
2024-12-20 23:11:27.000327: Epoch 83
2024-12-20 23:11:27.001171: Current learning rate: 0.00484
2024-12-20 23:15:58.010552: Validation loss did not improve from -0.51843. Patience: 19/50
2024-12-20 23:15:58.011788: train_loss -0.7432
2024-12-20 23:15:58.012742: val_loss -0.4432
2024-12-20 23:15:58.013575: Pseudo dice [0.6831]
2024-12-20 23:15:58.015051: Epoch time: 271.01 s
2024-12-20 23:15:59.378622: 
2024-12-20 23:15:59.380328: Epoch 84
2024-12-20 23:15:59.381703: Current learning rate: 0.00478
2024-12-20 23:20:38.203753: Validation loss did not improve from -0.51843. Patience: 20/50
2024-12-20 23:20:38.204864: train_loss -0.7472
2024-12-20 23:20:38.206690: val_loss -0.4768
2024-12-20 23:20:38.207701: Pseudo dice [0.7051]
2024-12-20 23:20:38.208775: Epoch time: 278.83 s
2024-12-20 23:20:39.961912: 
2024-12-20 23:20:39.963693: Epoch 85
2024-12-20 23:20:39.965723: Current learning rate: 0.00471
2024-12-20 23:25:51.089934: Validation loss did not improve from -0.51843. Patience: 21/50
2024-12-20 23:25:51.090652: train_loss -0.7492
2024-12-20 23:25:51.091386: val_loss -0.4568
2024-12-20 23:25:51.091976: Pseudo dice [0.7071]
2024-12-20 23:25:51.092571: Epoch time: 311.13 s
2024-12-20 23:25:52.454896: 
2024-12-20 23:25:52.456881: Epoch 86
2024-12-20 23:25:52.457968: Current learning rate: 0.00465
2024-12-20 23:32:06.660192: Validation loss did not improve from -0.51843. Patience: 22/50
2024-12-20 23:32:06.661055: train_loss -0.7496
2024-12-20 23:32:06.661770: val_loss -0.4603
2024-12-20 23:32:06.662403: Pseudo dice [0.7056]
2024-12-20 23:32:06.663163: Epoch time: 374.21 s
2024-12-20 23:32:08.038243: 
2024-12-20 23:32:08.040113: Epoch 87
2024-12-20 23:32:08.041368: Current learning rate: 0.00458
2024-12-20 23:37:51.634930: Validation loss did not improve from -0.51843. Patience: 23/50
2024-12-20 23:37:51.635804: train_loss -0.7522
2024-12-20 23:37:51.636666: val_loss -0.4788
2024-12-20 23:37:51.637676: Pseudo dice [0.7127]
2024-12-20 23:37:51.638588: Epoch time: 343.6 s
2024-12-20 23:37:53.007604: 
2024-12-20 23:37:53.009547: Epoch 88
2024-12-20 23:37:53.011699: Current learning rate: 0.00452
2024-12-20 23:43:11.340194: Validation loss did not improve from -0.51843. Patience: 24/50
2024-12-20 23:43:11.340958: train_loss -0.7552
2024-12-20 23:43:11.341916: val_loss -0.4604
2024-12-20 23:43:11.343033: Pseudo dice [0.6986]
2024-12-20 23:43:11.344263: Epoch time: 318.33 s
2024-12-20 23:43:12.782932: 
2024-12-20 23:43:12.784521: Epoch 89
2024-12-20 23:43:12.785866: Current learning rate: 0.00445
2024-12-20 23:49:27.258498: Validation loss did not improve from -0.51843. Patience: 25/50
2024-12-20 23:49:27.260082: train_loss -0.751
2024-12-20 23:49:27.261265: val_loss -0.4875
2024-12-20 23:49:27.262135: Pseudo dice [0.722]
2024-12-20 23:49:27.263552: Epoch time: 374.48 s
2024-12-20 23:49:29.095395: 
2024-12-20 23:49:29.097090: Epoch 90
2024-12-20 23:49:29.098191: Current learning rate: 0.00438
2024-12-20 23:54:50.661991: Validation loss did not improve from -0.51843. Patience: 26/50
2024-12-20 23:54:50.662761: train_loss -0.748
2024-12-20 23:54:50.664163: val_loss -0.4917
2024-12-20 23:54:50.665007: Pseudo dice [0.7253]
2024-12-20 23:54:50.665830: Epoch time: 321.57 s
2024-12-20 23:54:51.994962: 
2024-12-20 23:54:51.996140: Epoch 91
2024-12-20 23:54:51.996881: Current learning rate: 0.00432
2024-12-21 00:00:36.227054: Validation loss did not improve from -0.51843. Patience: 27/50
2024-12-21 00:00:36.228787: train_loss -0.7568
2024-12-21 00:00:36.229807: val_loss -0.492
2024-12-21 00:00:36.230923: Pseudo dice [0.7205]
2024-12-21 00:00:36.231993: Epoch time: 344.24 s
2024-12-21 00:00:36.233310: Yayy! New best EMA pseudo Dice: 0.711
2024-12-21 00:00:38.019006: 
2024-12-21 00:00:38.020037: Epoch 92
2024-12-21 00:00:38.020754: Current learning rate: 0.00425
2024-12-21 00:06:02.432988: Validation loss did not improve from -0.51843. Patience: 28/50
2024-12-21 00:06:02.436754: train_loss -0.7553
2024-12-21 00:06:02.438305: val_loss -0.4593
2024-12-21 00:06:02.439279: Pseudo dice [0.7116]
2024-12-21 00:06:02.440407: Epoch time: 324.42 s
2024-12-21 00:06:02.441832: Yayy! New best EMA pseudo Dice: 0.711
2024-12-21 00:06:04.717922: 
2024-12-21 00:06:04.719552: Epoch 93
2024-12-21 00:06:04.720590: Current learning rate: 0.00419
2024-12-21 00:11:49.978311: Validation loss did not improve from -0.51843. Patience: 29/50
2024-12-21 00:11:49.979497: train_loss -0.7578
2024-12-21 00:11:49.981179: val_loss -0.4576
2024-12-21 00:11:49.981849: Pseudo dice [0.7013]
2024-12-21 00:11:49.982592: Epoch time: 345.26 s
2024-12-21 00:11:51.382009: 
2024-12-21 00:11:51.383453: Epoch 94
2024-12-21 00:11:51.384553: Current learning rate: 0.00412
2024-12-21 00:17:31.133709: Validation loss did not improve from -0.51843. Patience: 30/50
2024-12-21 00:17:31.135280: train_loss -0.761
2024-12-21 00:17:31.136578: val_loss -0.4737
2024-12-21 00:17:31.137769: Pseudo dice [0.7126]
2024-12-21 00:17:31.138984: Epoch time: 339.76 s
2024-12-21 00:17:32.889035: 
2024-12-21 00:17:32.890244: Epoch 95
2024-12-21 00:17:32.891089: Current learning rate: 0.00405
2024-12-21 00:22:48.198614: Validation loss did not improve from -0.51843. Patience: 31/50
2024-12-21 00:22:48.199617: train_loss -0.7605
2024-12-21 00:22:48.200947: val_loss -0.5081
2024-12-21 00:22:48.202085: Pseudo dice [0.73]
2024-12-21 00:22:48.203284: Epoch time: 315.31 s
2024-12-21 00:22:48.204473: Yayy! New best EMA pseudo Dice: 0.7123
2024-12-21 00:22:50.017536: 
2024-12-21 00:22:50.019257: Epoch 96
2024-12-21 00:22:50.020525: Current learning rate: 0.00399
2024-12-21 00:28:40.011089: Validation loss did not improve from -0.51843. Patience: 32/50
2024-12-21 00:28:40.011946: train_loss -0.7607
2024-12-21 00:28:40.012848: val_loss -0.4931
2024-12-21 00:28:40.013760: Pseudo dice [0.7298]
2024-12-21 00:28:40.014565: Epoch time: 350.0 s
2024-12-21 00:28:40.015262: Yayy! New best EMA pseudo Dice: 0.714
2024-12-21 00:28:41.808999: 
2024-12-21 00:28:41.810331: Epoch 97
2024-12-21 00:28:41.811118: Current learning rate: 0.00392
2024-12-21 00:34:21.661227: Validation loss did not improve from -0.51843. Patience: 33/50
2024-12-21 00:34:21.662781: train_loss -0.7616
2024-12-21 00:34:21.663878: val_loss -0.4826
2024-12-21 00:34:21.664809: Pseudo dice [0.7255]
2024-12-21 00:34:21.666007: Epoch time: 339.86 s
2024-12-21 00:34:21.666715: Yayy! New best EMA pseudo Dice: 0.7152
2024-12-21 00:34:23.495893: 
2024-12-21 00:34:23.497333: Epoch 98
2024-12-21 00:34:23.498602: Current learning rate: 0.00385
2024-12-21 00:40:28.789611: Validation loss did not improve from -0.51843. Patience: 34/50
2024-12-21 00:40:28.790611: train_loss -0.7627
2024-12-21 00:40:28.791458: val_loss -0.4793
2024-12-21 00:40:28.792275: Pseudo dice [0.7197]
2024-12-21 00:40:28.793462: Epoch time: 365.3 s
2024-12-21 00:40:28.794110: Yayy! New best EMA pseudo Dice: 0.7156
2024-12-21 00:40:30.627023: 
2024-12-21 00:40:30.628428: Epoch 99
2024-12-21 00:40:30.629172: Current learning rate: 0.00379
2024-12-21 00:46:01.237256: Validation loss did not improve from -0.51843. Patience: 35/50
2024-12-21 00:46:01.238277: train_loss -0.7624
2024-12-21 00:46:01.239171: val_loss -0.4589
2024-12-21 00:46:01.239837: Pseudo dice [0.7034]
2024-12-21 00:46:01.240484: Epoch time: 330.61 s
2024-12-21 00:46:03.136680: 
2024-12-21 00:46:03.138924: Epoch 100
2024-12-21 00:46:03.139983: Current learning rate: 0.00372
2024-12-21 00:51:34.312748: Validation loss did not improve from -0.51843. Patience: 36/50
2024-12-21 00:51:34.314009: train_loss -0.7668
2024-12-21 00:51:34.315473: val_loss -0.4905
2024-12-21 00:51:34.316700: Pseudo dice [0.7261]
2024-12-21 00:51:34.317954: Epoch time: 331.18 s
2024-12-21 00:51:35.715503: 
2024-12-21 00:51:35.717001: Epoch 101
2024-12-21 00:51:35.717899: Current learning rate: 0.00365
2024-12-21 00:57:32.767577: Validation loss did not improve from -0.51843. Patience: 37/50
2024-12-21 00:57:32.768513: train_loss -0.7656
2024-12-21 00:57:32.769938: val_loss -0.4832
2024-12-21 00:57:32.771300: Pseudo dice [0.7194]
2024-12-21 00:57:32.772447: Epoch time: 357.05 s
2024-12-21 00:57:32.773303: Yayy! New best EMA pseudo Dice: 0.716
2024-12-21 00:57:34.581649: 
2024-12-21 00:57:34.582909: Epoch 102
2024-12-21 00:57:34.584056: Current learning rate: 0.00359
2024-12-21 01:02:39.603286: Validation loss did not improve from -0.51843. Patience: 38/50
2024-12-21 01:02:39.604200: train_loss -0.7628
2024-12-21 01:02:39.604918: val_loss -0.4814
2024-12-21 01:02:39.605640: Pseudo dice [0.7202]
2024-12-21 01:02:39.606267: Epoch time: 305.02 s
2024-12-21 01:02:39.606931: Yayy! New best EMA pseudo Dice: 0.7164
2024-12-21 01:02:41.484598: 
2024-12-21 01:02:41.486069: Epoch 103
2024-12-21 01:02:41.487175: Current learning rate: 0.00352
2024-12-21 01:08:59.365343: Validation loss did not improve from -0.51843. Patience: 39/50
2024-12-21 01:08:59.369398: train_loss -0.7687
2024-12-21 01:08:59.370745: val_loss -0.4643
2024-12-21 01:08:59.371634: Pseudo dice [0.7136]
2024-12-21 01:08:59.372824: Epoch time: 377.89 s
2024-12-21 01:09:01.149136: 
2024-12-21 01:09:01.150955: Epoch 104
2024-12-21 01:09:01.152374: Current learning rate: 0.00345
2024-12-21 01:14:13.192382: Validation loss did not improve from -0.51843. Patience: 40/50
2024-12-21 01:14:13.194591: train_loss -0.7654
2024-12-21 01:14:13.196805: val_loss -0.438
2024-12-21 01:14:13.198046: Pseudo dice [0.6836]
2024-12-21 01:14:13.199663: Epoch time: 312.05 s
2024-12-21 01:14:15.091107: 
2024-12-21 01:14:15.092645: Epoch 105
2024-12-21 01:14:15.093528: Current learning rate: 0.00338
2024-12-21 01:20:19.396342: Validation loss did not improve from -0.51843. Patience: 41/50
2024-12-21 01:20:19.397592: train_loss -0.7657
2024-12-21 01:20:19.398628: val_loss -0.4749
2024-12-21 01:20:19.399694: Pseudo dice [0.7151]
2024-12-21 01:20:19.400611: Epoch time: 364.31 s
2024-12-21 01:20:20.827413: 
2024-12-21 01:20:20.829549: Epoch 106
2024-12-21 01:20:20.830410: Current learning rate: 0.00332
2024-12-21 01:25:54.782995: Validation loss did not improve from -0.51843. Patience: 42/50
2024-12-21 01:25:54.784625: train_loss -0.7727
2024-12-21 01:25:54.785704: val_loss -0.4416
2024-12-21 01:25:54.786515: Pseudo dice [0.6902]
2024-12-21 01:25:54.787157: Epoch time: 333.96 s
2024-12-21 01:25:56.237422: 
2024-12-21 01:25:56.238845: Epoch 107
2024-12-21 01:25:56.239712: Current learning rate: 0.00325
2024-12-21 01:31:02.389887: Validation loss did not improve from -0.51843. Patience: 43/50
2024-12-21 01:31:02.391644: train_loss -0.767
2024-12-21 01:31:02.392660: val_loss -0.4765
2024-12-21 01:31:02.393369: Pseudo dice [0.7184]
2024-12-21 01:31:02.393980: Epoch time: 306.16 s
2024-12-21 01:31:03.834686: 
2024-12-21 01:31:03.836533: Epoch 108
2024-12-21 01:31:03.837471: Current learning rate: 0.00318
2024-12-21 01:36:49.472139: Validation loss did not improve from -0.51843. Patience: 44/50
2024-12-21 01:36:49.473707: train_loss -0.7691
2024-12-21 01:36:49.474725: val_loss -0.4901
2024-12-21 01:36:49.475530: Pseudo dice [0.7269]
2024-12-21 01:36:49.476756: Epoch time: 345.64 s
2024-12-21 01:36:50.969151: 
2024-12-21 01:36:50.970238: Epoch 109
2024-12-21 01:36:50.971379: Current learning rate: 0.00311
2024-12-21 01:42:41.871923: Validation loss did not improve from -0.51843. Patience: 45/50
2024-12-21 01:42:41.873007: train_loss -0.776
2024-12-21 01:42:41.873750: val_loss -0.4811
2024-12-21 01:42:41.874547: Pseudo dice [0.7097]
2024-12-21 01:42:41.875221: Epoch time: 350.91 s
2024-12-21 01:42:43.723483: 
2024-12-21 01:42:43.724243: Epoch 110
2024-12-21 01:42:43.725110: Current learning rate: 0.00304
2024-12-21 01:48:32.089664: Validation loss did not improve from -0.51843. Patience: 46/50
2024-12-21 01:48:32.092168: train_loss -0.7738
2024-12-21 01:48:32.093479: val_loss -0.4976
2024-12-21 01:48:32.095033: Pseudo dice [0.7192]
2024-12-21 01:48:32.096658: Epoch time: 348.37 s
2024-12-21 01:48:33.494755: 
2024-12-21 01:48:33.496005: Epoch 111
2024-12-21 01:48:33.497164: Current learning rate: 0.00297
2024-12-21 01:54:16.228925: Validation loss did not improve from -0.51843. Patience: 47/50
2024-12-21 01:54:16.230333: train_loss -0.7758
2024-12-21 01:54:16.230982: val_loss -0.4628
2024-12-21 01:54:16.231667: Pseudo dice [0.7119]
2024-12-21 01:54:16.232365: Epoch time: 342.74 s
2024-12-21 01:54:17.731410: 
2024-12-21 01:54:17.733554: Epoch 112
2024-12-21 01:54:17.735201: Current learning rate: 0.00291
2024-12-21 01:59:39.046278: Validation loss did not improve from -0.51843. Patience: 48/50
2024-12-21 01:59:39.047283: train_loss -0.7757
2024-12-21 01:59:39.048144: val_loss -0.4688
2024-12-21 01:59:39.048821: Pseudo dice [0.7058]
2024-12-21 01:59:39.049436: Epoch time: 321.32 s
2024-12-21 01:59:40.489030: 
2024-12-21 01:59:40.490923: Epoch 113
2024-12-21 01:59:40.492292: Current learning rate: 0.00284
2024-12-21 02:05:14.826089: Validation loss did not improve from -0.51843. Patience: 49/50
2024-12-21 02:05:14.827198: train_loss -0.775
2024-12-21 02:05:14.828382: val_loss -0.4449
2024-12-21 02:05:14.829914: Pseudo dice [0.6933]
2024-12-21 02:05:14.831223: Epoch time: 334.34 s
2024-12-21 02:05:16.310214: 
2024-12-21 02:05:16.311612: Epoch 114
2024-12-21 02:05:16.312983: Current learning rate: 0.00277
2024-12-21 02:11:02.325673: Validation loss did not improve from -0.51843. Patience: 50/50
2024-12-21 02:11:02.326879: train_loss -0.7738
2024-12-21 02:11:02.328431: val_loss -0.4394
2024-12-21 02:11:02.329320: Pseudo dice [0.7048]
2024-12-21 02:11:02.330335: Epoch time: 346.02 s
2024-12-21 02:11:04.822984: 
2024-12-21 02:11:04.824332: Epoch 115
2024-12-21 02:11:04.825258: Current learning rate: 0.0027
2024-12-21 02:16:53.565148: Validation loss did not improve from -0.51843. Patience: 51/50
2024-12-21 02:16:53.571210: train_loss -0.7714
2024-12-21 02:16:53.572703: val_loss -0.4741
2024-12-21 02:16:53.573812: Pseudo dice [0.721]
2024-12-21 02:16:53.575150: Epoch time: 348.75 s
2024-12-21 02:16:55.075174: 
2024-12-21 02:16:55.076584: Epoch 116
2024-12-21 02:16:55.077448: Current learning rate: 0.00263
2024-12-21 02:23:11.435719: Validation loss did not improve from -0.51843. Patience: 52/50
2024-12-21 02:23:11.437057: train_loss -0.7772
2024-12-21 02:23:11.439383: val_loss -0.4824
2024-12-21 02:23:11.440777: Pseudo dice [0.7149]
2024-12-21 02:23:11.442440: Epoch time: 376.36 s
2024-12-21 02:23:12.908836: 
2024-12-21 02:23:12.909683: Epoch 117
2024-12-21 02:23:12.910450: Current learning rate: 0.00256
2024-12-21 02:29:22.176390: Validation loss did not improve from -0.51843. Patience: 53/50
2024-12-21 02:29:22.177412: train_loss -0.776
2024-12-21 02:29:22.178839: val_loss -0.4435
2024-12-21 02:29:22.179758: Pseudo dice [0.701]
2024-12-21 02:29:22.180849: Epoch time: 369.27 s
2024-12-21 02:29:23.665673: 
2024-12-21 02:29:23.668012: Epoch 118
2024-12-21 02:29:23.669963: Current learning rate: 0.00249
2024-12-21 02:35:15.237215: Validation loss did not improve from -0.51843. Patience: 54/50
2024-12-21 02:35:15.238194: train_loss -0.7769
2024-12-21 02:35:15.239433: val_loss -0.4627
2024-12-21 02:35:15.240640: Pseudo dice [0.7047]
2024-12-21 02:35:15.241797: Epoch time: 351.57 s
2024-12-21 02:35:16.702776: 
2024-12-21 02:35:16.704402: Epoch 119
2024-12-21 02:35:16.705148: Current learning rate: 0.00242
2024-12-21 02:40:38.269578: Validation loss did not improve from -0.51843. Patience: 55/50
2024-12-21 02:40:38.270906: train_loss -0.7801
2024-12-21 02:40:38.271657: val_loss -0.4667
2024-12-21 02:40:38.272390: Pseudo dice [0.7159]
2024-12-21 02:40:38.273366: Epoch time: 321.57 s
2024-12-21 02:40:40.267628: 
2024-12-21 02:40:40.268708: Epoch 120
2024-12-21 02:40:40.269474: Current learning rate: 0.00235
2024-12-21 02:44:17.497155: Validation loss did not improve from -0.51843. Patience: 56/50
2024-12-21 02:44:17.498473: train_loss -0.7812
2024-12-21 02:44:17.499549: val_loss -0.447
2024-12-21 02:44:17.500846: Pseudo dice [0.7031]
2024-12-21 02:44:17.501938: Epoch time: 217.23 s
2024-12-21 02:44:18.962114: 
2024-12-21 02:44:18.963071: Epoch 121
2024-12-21 02:44:18.963739: Current learning rate: 0.00228
2024-12-21 02:47:35.377398: Validation loss did not improve from -0.51843. Patience: 57/50
2024-12-21 02:47:35.379405: train_loss -0.7812
2024-12-21 02:47:35.381027: val_loss -0.488
2024-12-21 02:47:35.382132: Pseudo dice [0.7224]
2024-12-21 02:47:35.383310: Epoch time: 196.42 s
2024-12-21 02:47:36.854934: 
2024-12-21 02:47:36.856582: Epoch 122
2024-12-21 02:47:36.857639: Current learning rate: 0.00221
2024-12-21 02:50:32.625345: Validation loss did not improve from -0.51843. Patience: 58/50
2024-12-21 02:50:32.626265: train_loss -0.7819
2024-12-21 02:50:32.627172: val_loss -0.4775
2024-12-21 02:50:32.628018: Pseudo dice [0.7097]
2024-12-21 02:50:32.628903: Epoch time: 175.77 s
2024-12-21 02:50:34.103733: 
2024-12-21 02:50:34.105841: Epoch 123
2024-12-21 02:50:34.108189: Current learning rate: 0.00214
2024-12-21 02:53:37.570296: Validation loss improved from -0.51843 to -0.52339! Patience: 58/50
2024-12-21 02:53:37.571051: train_loss -0.7798
2024-12-21 02:53:37.572083: val_loss -0.5234
2024-12-21 02:53:37.573378: Pseudo dice [0.7437]
2024-12-21 02:53:37.574251: Epoch time: 183.47 s
2024-12-21 02:53:39.066422: 
2024-12-21 02:53:39.068013: Epoch 124
2024-12-21 02:53:39.069293: Current learning rate: 0.00207
2024-12-21 02:56:26.792517: Validation loss did not improve from -0.52339. Patience: 1/50
2024-12-21 02:56:26.793406: train_loss -0.7813
2024-12-21 02:56:26.794346: val_loss -0.4263
2024-12-21 02:56:26.795216: Pseudo dice [0.6866]
2024-12-21 02:56:26.796133: Epoch time: 167.73 s
2024-12-21 02:56:28.611212: 
2024-12-21 02:56:28.612583: Epoch 125
2024-12-21 02:56:28.613438: Current learning rate: 0.00199
2024-12-21 02:59:07.371610: Validation loss did not improve from -0.52339. Patience: 2/50
2024-12-21 02:59:07.372875: train_loss -0.7845
2024-12-21 02:59:07.374062: val_loss -0.4714
2024-12-21 02:59:07.375242: Pseudo dice [0.712]
2024-12-21 02:59:07.376284: Epoch time: 158.76 s
2024-12-21 02:59:09.558470: 
2024-12-21 02:59:09.559891: Epoch 126
2024-12-21 02:59:09.561038: Current learning rate: 0.00192
2024-12-21 03:01:43.160428: Validation loss did not improve from -0.52339. Patience: 3/50
2024-12-21 03:01:43.161387: train_loss -0.7856
2024-12-21 03:01:43.162251: val_loss -0.4824
2024-12-21 03:01:43.163035: Pseudo dice [0.7181]
2024-12-21 03:01:43.164226: Epoch time: 153.6 s
2024-12-21 03:01:44.614340: 
2024-12-21 03:01:44.616243: Epoch 127
2024-12-21 03:01:44.617224: Current learning rate: 0.00185
2024-12-21 03:04:33.901073: Validation loss did not improve from -0.52339. Patience: 4/50
2024-12-21 03:04:33.901729: train_loss -0.788
2024-12-21 03:04:33.902380: val_loss -0.4764
2024-12-21 03:04:33.902987: Pseudo dice [0.7221]
2024-12-21 03:04:33.903595: Epoch time: 169.29 s
2024-12-21 03:04:35.448111: 
2024-12-21 03:04:35.450038: Epoch 128
2024-12-21 03:04:35.451108: Current learning rate: 0.00178
2024-12-21 03:07:44.785250: Validation loss did not improve from -0.52339. Patience: 5/50
2024-12-21 03:07:44.786963: train_loss -0.7873
2024-12-21 03:07:44.788242: val_loss -0.4635
2024-12-21 03:07:44.789068: Pseudo dice [0.7195]
2024-12-21 03:07:44.789761: Epoch time: 189.34 s
2024-12-21 03:07:46.213950: 
2024-12-21 03:07:46.214997: Epoch 129
2024-12-21 03:07:46.215881: Current learning rate: 0.0017
2024-12-21 03:11:02.907814: Validation loss did not improve from -0.52339. Patience: 6/50
2024-12-21 03:11:02.908666: train_loss -0.7882
2024-12-21 03:11:02.909674: val_loss -0.4416
2024-12-21 03:11:02.910654: Pseudo dice [0.6941]
2024-12-21 03:11:02.911642: Epoch time: 196.7 s
2024-12-21 03:11:04.690580: 
2024-12-21 03:11:04.691930: Epoch 130
2024-12-21 03:11:04.693074: Current learning rate: 0.00163
2024-12-21 03:14:17.881502: Validation loss did not improve from -0.52339. Patience: 7/50
2024-12-21 03:14:17.882544: train_loss -0.7906
2024-12-21 03:14:17.883655: val_loss -0.4521
2024-12-21 03:14:17.885043: Pseudo dice [0.7045]
2024-12-21 03:14:17.886247: Epoch time: 193.19 s
2024-12-21 03:14:19.377196: 
2024-12-21 03:14:19.378421: Epoch 131
2024-12-21 03:14:19.379270: Current learning rate: 0.00156
2024-12-21 03:17:32.204598: Validation loss did not improve from -0.52339. Patience: 8/50
2024-12-21 03:17:32.208257: train_loss -0.7866
2024-12-21 03:17:32.209767: val_loss -0.468
2024-12-21 03:17:32.210624: Pseudo dice [0.7081]
2024-12-21 03:17:32.211673: Epoch time: 192.83 s
2024-12-21 03:17:33.656492: 
2024-12-21 03:17:33.658058: Epoch 132
2024-12-21 03:17:33.659069: Current learning rate: 0.00148
2024-12-21 03:20:44.560483: Validation loss did not improve from -0.52339. Patience: 9/50
2024-12-21 03:20:44.561426: train_loss -0.7872
2024-12-21 03:20:44.562527: val_loss -0.4929
2024-12-21 03:20:44.563661: Pseudo dice [0.7162]
2024-12-21 03:20:44.564667: Epoch time: 190.91 s
2024-12-21 03:20:46.063829: 
2024-12-21 03:20:46.065365: Epoch 133
2024-12-21 03:20:46.066166: Current learning rate: 0.00141
2024-12-21 03:23:54.185898: Validation loss did not improve from -0.52339. Patience: 10/50
2024-12-21 03:23:54.187220: train_loss -0.7886
2024-12-21 03:23:54.188769: val_loss -0.4642
2024-12-21 03:23:54.189586: Pseudo dice [0.7124]
2024-12-21 03:23:54.190459: Epoch time: 188.12 s
2024-12-21 03:23:55.620610: 
2024-12-21 03:23:55.621512: Epoch 134
2024-12-21 03:23:55.622266: Current learning rate: 0.00133
2024-12-21 03:27:03.100815: Validation loss did not improve from -0.52339. Patience: 11/50
2024-12-21 03:27:03.101928: train_loss -0.7849
2024-12-21 03:27:03.102763: val_loss -0.4913
2024-12-21 03:27:03.103377: Pseudo dice [0.7244]
2024-12-21 03:27:03.104030: Epoch time: 187.48 s
2024-12-21 03:27:04.979504: 
2024-12-21 03:27:04.980914: Epoch 135
2024-12-21 03:27:04.981895: Current learning rate: 0.00126
2024-12-21 03:30:17.754961: Validation loss did not improve from -0.52339. Patience: 12/50
2024-12-21 03:30:17.756497: train_loss -0.7897
2024-12-21 03:30:17.758441: val_loss -0.4767
2024-12-21 03:30:17.759809: Pseudo dice [0.7248]
2024-12-21 03:30:17.761206: Epoch time: 192.78 s
2024-12-21 03:30:19.263858: 
2024-12-21 03:30:19.266593: Epoch 136
2024-12-21 03:30:19.268066: Current learning rate: 0.00118
2024-12-21 03:33:30.594142: Validation loss did not improve from -0.52339. Patience: 13/50
2024-12-21 03:33:30.595764: train_loss -0.7899
2024-12-21 03:33:30.596620: val_loss -0.5044
2024-12-21 03:33:30.597428: Pseudo dice [0.7314]
2024-12-21 03:33:30.598285: Epoch time: 191.33 s
2024-12-21 03:33:32.047929: 
2024-12-21 03:33:32.049476: Epoch 137
2024-12-21 03:33:32.050566: Current learning rate: 0.00111
2024-12-21 03:36:43.534026: Validation loss did not improve from -0.52339. Patience: 14/50
2024-12-21 03:36:43.535136: train_loss -0.7899
2024-12-21 03:36:43.536361: val_loss -0.4568
2024-12-21 03:36:43.537467: Pseudo dice [0.7075]
2024-12-21 03:36:43.538799: Epoch time: 191.49 s
2024-12-21 03:36:44.978071: 
2024-12-21 03:36:44.981299: Epoch 138
2024-12-21 03:36:44.983549: Current learning rate: 0.00103
2024-12-21 03:39:53.579234: Validation loss did not improve from -0.52339. Patience: 15/50
2024-12-21 03:39:53.580595: train_loss -0.7914
2024-12-21 03:39:53.582394: val_loss -0.4599
2024-12-21 03:39:53.583852: Pseudo dice [0.7086]
2024-12-21 03:39:53.585428: Epoch time: 188.61 s
2024-12-21 03:39:55.047856: 
2024-12-21 03:39:55.049079: Epoch 139
2024-12-21 03:39:55.049799: Current learning rate: 0.00095
2024-12-21 03:43:08.467333: Validation loss did not improve from -0.52339. Patience: 16/50
2024-12-21 03:43:08.469667: train_loss -0.7895
2024-12-21 03:43:08.471220: val_loss -0.4373
2024-12-21 03:43:08.472560: Pseudo dice [0.7012]
2024-12-21 03:43:08.473557: Epoch time: 193.42 s
2024-12-21 03:43:10.392386: 
2024-12-21 03:43:10.393902: Epoch 140
2024-12-21 03:43:10.394911: Current learning rate: 0.00087
2024-12-21 03:46:24.838954: Validation loss did not improve from -0.52339. Patience: 17/50
2024-12-21 03:46:24.840244: train_loss -0.7936
2024-12-21 03:46:24.841627: val_loss -0.4611
2024-12-21 03:46:24.842395: Pseudo dice [0.7139]
2024-12-21 03:46:24.843422: Epoch time: 194.45 s
2024-12-21 03:46:26.320909: 
2024-12-21 03:46:26.322194: Epoch 141
2024-12-21 03:46:26.323006: Current learning rate: 0.00079
2024-12-21 03:49:17.813767: Validation loss did not improve from -0.52339. Patience: 18/50
2024-12-21 03:49:17.814581: train_loss -0.7914
2024-12-21 03:49:17.815329: val_loss -0.4676
2024-12-21 03:49:17.816125: Pseudo dice [0.7103]
2024-12-21 03:49:17.817200: Epoch time: 171.5 s
2024-12-21 03:49:19.299335: 
2024-12-21 03:49:19.301024: Epoch 142
2024-12-21 03:49:19.302172: Current learning rate: 0.00071
2024-12-21 03:52:11.075370: Validation loss did not improve from -0.52339. Patience: 19/50
2024-12-21 03:52:11.076465: train_loss -0.7979
2024-12-21 03:52:11.078481: val_loss -0.4756
2024-12-21 03:52:11.079976: Pseudo dice [0.7159]
2024-12-21 03:52:11.081367: Epoch time: 171.78 s
2024-12-21 03:52:12.524197: 
2024-12-21 03:52:12.525215: Epoch 143
2024-12-21 03:52:12.525898: Current learning rate: 0.00063
2024-12-21 03:55:14.937565: Validation loss did not improve from -0.52339. Patience: 20/50
2024-12-21 03:55:14.938763: train_loss -0.7927
2024-12-21 03:55:14.939725: val_loss -0.4613
2024-12-21 03:55:14.940966: Pseudo dice [0.7099]
2024-12-21 03:55:14.942199: Epoch time: 182.42 s
2024-12-21 03:55:16.390740: 
2024-12-21 03:55:16.392375: Epoch 144
2024-12-21 03:55:16.393727: Current learning rate: 0.00055
2024-12-21 03:58:10.311763: Validation loss did not improve from -0.52339. Patience: 21/50
2024-12-21 03:58:10.312840: train_loss -0.7933
2024-12-21 03:58:10.314232: val_loss -0.4741
2024-12-21 03:58:10.315299: Pseudo dice [0.7111]
2024-12-21 03:58:10.316276: Epoch time: 173.93 s
2024-12-21 03:58:12.161650: 
2024-12-21 03:58:12.162981: Epoch 145
2024-12-21 03:58:12.163652: Current learning rate: 0.00047
2024-12-21 04:01:07.938493: Validation loss did not improve from -0.52339. Patience: 22/50
2024-12-21 04:01:07.939482: train_loss -0.7944
2024-12-21 04:01:07.940204: val_loss -0.4805
2024-12-21 04:01:07.940876: Pseudo dice [0.7246]
2024-12-21 04:01:07.941686: Epoch time: 175.78 s
2024-12-21 04:01:09.377789: 
2024-12-21 04:01:09.379262: Epoch 146
2024-12-21 04:01:09.380065: Current learning rate: 0.00038
2024-12-21 04:03:56.196538: Validation loss did not improve from -0.52339. Patience: 23/50
2024-12-21 04:03:56.197601: train_loss -0.7919
2024-12-21 04:03:56.198470: val_loss -0.4306
2024-12-21 04:03:56.199341: Pseudo dice [0.7065]
2024-12-21 04:03:56.200194: Epoch time: 166.82 s
2024-12-21 04:03:58.052687: 
2024-12-21 04:03:58.054548: Epoch 147
2024-12-21 04:03:58.056501: Current learning rate: 0.0003
2024-12-21 04:08:11.315549: Validation loss did not improve from -0.52339. Patience: 24/50
2024-12-21 04:08:11.317007: train_loss -0.7944
2024-12-21 04:08:11.317846: val_loss -0.4673
2024-12-21 04:08:11.318685: Pseudo dice [0.7174]
2024-12-21 04:08:11.319493: Epoch time: 253.26 s
2024-12-21 04:08:12.801351: 
2024-12-21 04:08:12.803227: Epoch 148
2024-12-21 04:08:12.804214: Current learning rate: 0.00021
2024-12-21 04:12:46.313598: Validation loss did not improve from -0.52339. Patience: 25/50
2024-12-21 04:12:46.314482: train_loss -0.7937
2024-12-21 04:12:46.315263: val_loss -0.4616
2024-12-21 04:12:46.316087: Pseudo dice [0.7079]
2024-12-21 04:12:46.317166: Epoch time: 273.52 s
2024-12-21 04:12:47.768634: 
2024-12-21 04:12:47.769848: Epoch 149
2024-12-21 04:12:47.770621: Current learning rate: 0.00011
2024-12-21 04:17:34.061124: Validation loss did not improve from -0.52339. Patience: 26/50
2024-12-21 04:17:34.062130: train_loss -0.7955
2024-12-21 04:17:34.062955: val_loss -0.4751
2024-12-21 04:17:34.063712: Pseudo dice [0.712]
2024-12-21 04:17:34.064454: Epoch time: 286.29 s
2024-12-21 04:17:35.846915: Training done.
2024-12-21 04:17:36.073967: Using splits from existing split file: /home/gridsan/nchutisilp/datasets/nnUNet_Datasets/nnUNet_preprocessed/Dataset308_Sohee_Calcium_OCT_CrossValidation/splits_final_80.json
2024-12-21 04:17:36.089767: The split file contains 5 splits.
2024-12-21 04:17:36.091962: Desired fold for training: 4
2024-12-21 04:17:36.093596: This split has 6 training and 5 validation cases.
2024-12-21 04:17:36.095288: predicting 101-044
2024-12-21 04:17:36.172568: 101-044, shape torch.Size([1, 404, 498, 498]), rank 0
2024-12-21 04:19:30.850760: predicting 401-004
2024-12-21 04:19:30.931012: 401-004, shape torch.Size([1, 375, 498, 498]), rank 0
2024-12-21 04:21:23.208836: predicting 701-013
2024-12-21 04:21:23.226457: 701-013, shape torch.Size([1, 375, 498, 498]), rank 0
2024-12-21 04:24:02.063526: predicting 704-003
2024-12-21 04:24:02.080954: 704-003, shape torch.Size([1, 375, 498, 498]), rank 0
2024-12-21 04:25:59.268760: predicting 706-005
2024-12-21 04:25:59.303180: 706-005, shape torch.Size([1, 375, 498, 498]), rank 0
2024-12-21 04:28:21.135335: Validation complete
2024-12-21 04:28:21.136129: Mean Validation Dice:  0.6986091993076289
