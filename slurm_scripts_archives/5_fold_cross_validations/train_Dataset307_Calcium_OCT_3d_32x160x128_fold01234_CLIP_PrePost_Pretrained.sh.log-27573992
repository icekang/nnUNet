
CommandNotFoundError: Your shell has not been properly configured to use 'conda activate'.
To initialize your shell, run

    $ conda init <SHELL_NAME>

Currently supported shells are:
  - bash
  - fish
  - tcsh
  - xonsh
  - zsh
  - powershell

See 'conda init --help' for more information and options.

IMPORTANT: You may need to close and restart your shell after running 'conda init'.


/home/gridsan/nchutisilp/.conda/envs/nnunet/bin/python
Starting training with CONFIG=3d_32x160x128_b10, DATASET_ID=309, TRAINER=nnUNetTrainer

############################
INFO: You are using the old nnU-Net default plans. We have updated our recommendations. Please consider using those instead! Read more here: https://github.com/MIC-DKFZ/nnUNet/blob/master/documentation/resenc_presets.md
############################

Using device: cuda:0

#######################################################################
Please cite the following paper when using nnU-Net:
Isensee, F., Jaeger, P. F., Kohl, S. A., Petersen, J., & Maier-Hein, K. H. (2021). nnU-Net: a self-configuring method for deep learning-based biomedical image segmentation. Nature methods, 18(2), 203-211.
#######################################################################

2024-12-08 16:21:51.123562: Using torch.compile...

############################
INFO: You are using the old nnU-Net default plans. We have updated our recommendations. Please consider using those instead! Read more here: https://github.com/MIC-DKFZ/nnUNet/blob/master/documentation/resenc_presets.md
############################

Using device: cuda:0

#######################################################################
Please cite the following paper when using nnU-Net:
Isensee, F., Jaeger, P. F., Kohl, S. A., Petersen, J., & Maier-Hein, K. H. (2021). nnU-Net: a self-configuring method for deep learning-based biomedical image segmentation. Nature methods, 18(2), 203-211.
#######################################################################

2024-12-08 16:21:51.117349: Using torch.compile...
/home/gridsan/nchutisilp/.local/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn("The verbose parameter is deprecated. Please use get_last_lr() "
/home/gridsan/nchutisilp/.local/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn("The verbose parameter is deprecated. Please use get_last_lr() "
################### Loading pretrained weights from file  /home/gridsan/nchutisilp/projects/OpenAI-CLIP/clip_preivl_poststent_pretrained_nnUNet.pt ###################
Below is the list of overlapping blocks in pretrained model and nnUNet architecture:
encoder.stages.0.0.convs.0.conv.weight shape torch.Size([32, 1, 3, 3, 3])
encoder.stages.0.0.convs.0.conv.bias shape torch.Size([32])
encoder.stages.0.0.convs.0.norm.weight shape torch.Size([32])
encoder.stages.0.0.convs.0.norm.bias shape torch.Size([32])
encoder.stages.0.0.convs.0.all_modules.0.weight shape torch.Size([32, 1, 3, 3, 3])
encoder.stages.0.0.convs.0.all_modules.0.bias shape torch.Size([32])
encoder.stages.0.0.convs.0.all_modules.1.weight shape torch.Size([32])
encoder.stages.0.0.convs.0.all_modules.1.bias shape torch.Size([32])
encoder.stages.0.0.convs.1.conv.weight shape torch.Size([32, 32, 3, 3, 3])
encoder.stages.0.0.convs.1.conv.bias shape torch.Size([32])
encoder.stages.0.0.convs.1.norm.weight shape torch.Size([32])
encoder.stages.0.0.convs.1.norm.bias shape torch.Size([32])
encoder.stages.0.0.convs.1.all_modules.0.weight shape torch.Size([32, 32, 3, 3, 3])
encoder.stages.0.0.convs.1.all_modules.0.bias shape torch.Size([32])
encoder.stages.0.0.convs.1.all_modules.1.weight shape torch.Size([32])
encoder.stages.0.0.convs.1.all_modules.1.bias shape torch.Size([32])
encoder.stages.1.0.convs.0.conv.weight shape torch.Size([64, 32, 3, 3, 3])
encoder.stages.1.0.convs.0.conv.bias shape torch.Size([64])
encoder.stages.1.0.convs.0.norm.weight shape torch.Size([64])
encoder.stages.1.0.convs.0.norm.bias shape torch.Size([64])
encoder.stages.1.0.convs.0.all_modules.0.weight shape torch.Size([64, 32, 3, 3, 3])
encoder.stages.1.0.convs.0.all_modules.0.bias shape torch.Size([64])
encoder.stages.1.0.convs.0.all_modules.1.weight shape torch.Size([64])
encoder.stages.1.0.convs.0.all_modules.1.bias shape torch.Size([64])
encoder.stages.1.0.convs.1.conv.weight shape torch.Size([64, 64, 3, 3, 3])
encoder.stages.1.0.convs.1.conv.bias shape torch.Size([64])
encoder.stages.1.0.convs.1.norm.weight shape torch.Size([64])
encoder.stages.1.0.convs.1.norm.bias shape torch.Size([64])
encoder.stages.1.0.convs.1.all_modules.0.weight shape torch.Size([64, 64, 3, 3, 3])
encoder.stages.1.0.convs.1.all_modules.0.bias shape torch.Size([64])
encoder.stages.1.0.convs.1.all_modules.1.weight shape torch.Size([64])
encoder.stages.1.0.convs.1.all_modules.1.bias shape torch.Size([64])
encoder.stages.2.0.convs.0.conv.weight shape torch.Size([128, 64, 3, 3, 3])
encoder.stages.2.0.convs.0.conv.bias shape torch.Size([128])
encoder.stages.2.0.convs.0.norm.weight shape torch.Size([128])
encoder.stages.2.0.convs.0.norm.bias shape torch.Size([128])
encoder.stages.2.0.convs.0.all_modules.0.weight shape torch.Size([128, 64, 3, 3, 3])
encoder.stages.2.0.convs.0.all_modules.0.bias shape torch.Size([128])
encoder.stages.2.0.convs.0.all_modules.1.weight shape torch.Size([128])
encoder.stages.2.0.convs.0.all_modules.1.bias shape torch.Size([128])
encoder.stages.2.0.convs.1.conv.weight shape torch.Size([128, 128, 3, 3, 3])
encoder.stages.2.0.convs.1.conv.bias shape torch.Size([128])
encoder.stages.2.0.convs.1.norm.weight shape torch.Size([128])
encoder.stages.2.0.convs.1.norm.bias shape torch.Size([128])
encoder.stages.2.0.convs.1.all_modules.0.weight shape torch.Size([128, 128, 3, 3, 3])
encoder.stages.2.0.convs.1.all_modules.0.bias shape torch.Size([128])
encoder.stages.2.0.convs.1.all_modules.1.weight shape torch.Size([128])
encoder.stages.2.0.convs.1.all_modules.1.bias shape torch.Size([128])
encoder.stages.3.0.convs.0.conv.weight shape torch.Size([256, 128, 3, 3, 3])
encoder.stages.3.0.convs.0.conv.bias shape torch.Size([256])
encoder.stages.3.0.convs.0.norm.weight shape torch.Size([256])
encoder.stages.3.0.convs.0.norm.bias shape torch.Size([256])
encoder.stages.3.0.convs.0.all_modules.0.weight shape torch.Size([256, 128, 3, 3, 3])
encoder.stages.3.0.convs.0.all_modules.0.bias shape torch.Size([256])
encoder.stages.3.0.convs.0.all_modules.1.weight shape torch.Size([256])
encoder.stages.3.0.convs.0.all_modules.1.bias shape torch.Size([256])
encoder.stages.3.0.convs.1.conv.weight shape torch.Size([256, 256, 3, 3, 3])
encoder.stages.3.0.convs.1.conv.bias shape torch.Size([256])
encoder.stages.3.0.convs.1.norm.weight shape torch.Size([256])
encoder.stages.3.0.convs.1.norm.bias shape torch.Size([256])
encoder.stages.3.0.convs.1.all_modules.0.weight shape torch.Size([256, 256, 3, 3, 3])
encoder.stages.3.0.convs.1.all_modules.0.bias shape torch.Size([256])
encoder.stages.3.0.convs.1.all_modules.1.weight shape torch.Size([256])
encoder.stages.3.0.convs.1.all_modules.1.bias shape torch.Size([256])
encoder.stages.4.0.convs.0.conv.weight shape torch.Size([320, 256, 3, 3, 3])
encoder.stages.4.0.convs.0.conv.bias shape torch.Size([320])
encoder.stages.4.0.convs.0.norm.weight shape torch.Size([320])
encoder.stages.4.0.convs.0.norm.bias shape torch.Size([320])
encoder.stages.4.0.convs.0.all_modules.0.weight shape torch.Size([320, 256, 3, 3, 3])
encoder.stages.4.0.convs.0.all_modules.0.bias shape torch.Size([320])
encoder.stages.4.0.convs.0.all_modules.1.weight shape torch.Size([320])
encoder.stages.4.0.convs.0.all_modules.1.bias shape torch.Size([320])
encoder.stages.4.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.4.0.convs.1.conv.bias shape torch.Size([320])
encoder.stages.4.0.convs.1.norm.weight shape torch.Size([320])
encoder.stages.4.0.convs.1.norm.bias shape torch.Size([320])
encoder.stages.4.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.4.0.convs.1.all_modules.0.bias shape torch.Size([320])
encoder.stages.4.0.convs.1.all_modules.1.weight shape torch.Size([320])
encoder.stages.4.0.convs.1.all_modules.1.bias shape torch.Size([320])
encoder.stages.5.0.convs.0.conv.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.5.0.convs.0.conv.bias shape torch.Size([320])
encoder.stages.5.0.convs.0.norm.weight shape torch.Size([320])
encoder.stages.5.0.convs.0.norm.bias shape torch.Size([320])
encoder.stages.5.0.convs.0.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.5.0.convs.0.all_modules.0.bias shape torch.Size([320])
encoder.stages.5.0.convs.0.all_modules.1.weight shape torch.Size([320])
encoder.stages.5.0.convs.0.all_modules.1.bias shape torch.Size([320])
encoder.stages.5.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.5.0.convs.1.conv.bias shape torch.Size([320])
encoder.stages.5.0.convs.1.norm.weight shape torch.Size([320])
encoder.stages.5.0.convs.1.norm.bias shape torch.Size([320])
encoder.stages.5.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.5.0.convs.1.all_modules.0.bias shape torch.Size([320])
encoder.stages.5.0.convs.1.all_modules.1.weight shape torch.Size([320])
encoder.stages.5.0.convs.1.all_modules.1.bias shape torch.Size([320])
decoder.encoder.stages.0.0.convs.0.conv.weight shape torch.Size([32, 1, 3, 3, 3])
decoder.encoder.stages.0.0.convs.0.conv.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.norm.weight shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.norm.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.all_modules.0.weight shape torch.Size([32, 1, 3, 3, 3])
decoder.encoder.stages.0.0.convs.0.all_modules.0.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.all_modules.1.weight shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.all_modules.1.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.conv.weight shape torch.Size([32, 32, 3, 3, 3])
decoder.encoder.stages.0.0.convs.1.conv.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.norm.weight shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.norm.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.all_modules.0.weight shape torch.Size([32, 32, 3, 3, 3])
decoder.encoder.stages.0.0.convs.1.all_modules.0.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.all_modules.1.weight shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.all_modules.1.bias shape torch.Size([32])
decoder.encoder.stages.1.0.convs.0.conv.weight shape torch.Size([64, 32, 3, 3, 3])
decoder.encoder.stages.1.0.convs.0.conv.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.norm.weight shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.norm.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.all_modules.0.weight shape torch.Size([64, 32, 3, 3, 3])
decoder.encoder.stages.1.0.convs.0.all_modules.0.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.all_modules.1.weight shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.all_modules.1.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.conv.weight shape torch.Size([64, 64, 3, 3, 3])
decoder.encoder.stages.1.0.convs.1.conv.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.norm.weight shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.norm.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.all_modules.0.weight shape torch.Size([64, 64, 3, 3, 3])
decoder.encoder.stages.1.0.convs.1.all_modules.0.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.all_modules.1.weight shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.all_modules.1.bias shape torch.Size([64])
decoder.encoder.stages.2.0.convs.0.conv.weight shape torch.Size([128, 64, 3, 3, 3])
decoder.encoder.stages.2.0.convs.0.conv.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.norm.weight shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.norm.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.all_modules.0.weight shape torch.Size([128, 64, 3, 3, 3])
decoder.encoder.stages.2.0.convs.0.all_modules.0.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.all_modules.1.weight shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.all_modules.1.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.conv.weight shape torch.Size([128, 128, 3, 3, 3])
decoder.encoder.stages.2.0.convs.1.conv.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.norm.weight shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.norm.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.all_modules.0.weight shape torch.Size([128, 128, 3, 3, 3])
decoder.encoder.stages.2.0.convs.1.all_modules.0.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.all_modules.1.weight shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.all_modules.1.bias shape torch.Size([128])
decoder.encoder.stages.3.0.convs.0.conv.weight shape torch.Size([256, 128, 3, 3, 3])
decoder.encoder.stages.3.0.convs.0.conv.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.norm.weight shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.norm.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.all_modules.0.weight shape torch.Size([256, 128, 3, 3, 3])
decoder.encoder.stages.3.0.convs.0.all_modules.0.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.all_modules.1.weight shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.all_modules.1.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.conv.weight shape torch.Size([256, 256, 3, 3, 3])
decoder.encoder.stages.3.0.convs.1.conv.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.norm.weight shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.norm.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.all_modules.0.weight shape torch.Size([256, 256, 3, 3, 3])
decoder.encoder.stages.3.0.convs.1.all_modules.0.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.all_modules.1.weight shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.all_modules.1.bias shape torch.Size([256])
decoder.encoder.stages.4.0.convs.0.conv.weight shape torch.Size([320, 256, 3, 3, 3])
decoder.encoder.stages.4.0.convs.0.conv.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.norm.weight shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.norm.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.all_modules.0.weight shape torch.Size([320, 256, 3, 3, 3])
decoder.encoder.stages.4.0.convs.0.all_modules.0.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.all_modules.1.weight shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.all_modules.1.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.4.0.convs.1.conv.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.norm.weight shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.norm.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.4.0.convs.1.all_modules.0.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.all_modules.1.weight shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.all_modules.1.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.conv.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.5.0.convs.0.conv.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.norm.weight shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.norm.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.5.0.convs.0.all_modules.0.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.all_modules.1.weight shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.all_modules.1.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.5.0.convs.1.conv.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.norm.weight shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.norm.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.5.0.convs.1.all_modules.0.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.all_modules.1.weight shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.all_modules.1.bias shape torch.Size([320])
decoder.stages.0.convs.0.conv.weight shape torch.Size([320, 640, 3, 3, 3])
decoder.stages.0.convs.0.conv.bias shape torch.Size([320])
decoder.stages.0.convs.0.norm.weight shape torch.Size([320])
decoder.stages.0.convs.0.norm.bias shape torch.Size([320])
decoder.stages.0.convs.0.all_modules.0.weight shape torch.Size([320, 640, 3, 3, 3])
decoder.stages.0.convs.0.all_modules.0.bias shape torch.Size([320])
decoder.stages.0.convs.0.all_modules.1.weight shape torch.Size([320])
decoder.stages.0.convs.0.all_modules.1.bias shape torch.Size([320])
decoder.stages.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.stages.0.convs.1.conv.bias shape torch.Size([320])
decoder.stages.0.convs.1.norm.weight shape torch.Size([320])
decoder.stages.0.convs.1.norm.bias shape torch.Size([320])
decoder.stages.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.stages.0.convs.1.all_modules.0.bias shape torch.Size([320])
decoder.stages.0.convs.1.all_modules.1.weight shape torch.Size([320])
decoder.stages.0.convs.1.all_modules.1.bias shape torch.Size([320])
decoder.stages.1.convs.0.conv.weight shape torch.Size([256, 512, 3, 3, 3])
decoder.stages.1.convs.0.conv.bias shape torch.Size([256])
decoder.stages.1.convs.0.norm.weight shape torch.Size([256])
decoder.stages.1.convs.0.norm.bias shape torch.Size([256])
decoder.stages.1.convs.0.all_modules.0.weight shape torch.Size([256, 512, 3, 3, 3])
decoder.stages.1.convs.0.all_modules.0.bias shape torch.Size([256])
decoder.stages.1.convs.0.all_modules.1.weight shape torch.Size([256])
decoder.stages.1.convs.0.all_modules.1.bias shape torch.Size([256])
decoder.stages.1.convs.1.conv.weight shape torch.Size([256, 256, 3, 3, 3])
decoder.stages.1.convs.1.conv.bias shape torch.Size([256])
decoder.stages.1.convs.1.norm.weight shape torch.Size([256])
decoder.stages.1.convs.1.norm.bias shape torch.Size([256])
decoder.stages.1.convs.1.all_modules.0.weight shape torch.Size([256, 256, 3, 3, 3])
decoder.stages.1.convs.1.all_modules.0.bias shape torch.Size([256])
decoder.stages.1.convs.1.all_modules.1.weight shape torch.Size([256])
decoder.stages.1.convs.1.all_modules.1.bias shape torch.Size([256])
decoder.stages.2.convs.0.conv.weight shape torch.Size([128, 256, 3, 3, 3])
decoder.stages.2.convs.0.conv.bias shape torch.Size([128])
decoder.stages.2.convs.0.norm.weight shape torch.Size([128])
decoder.stages.2.convs.0.norm.bias shape torch.Size([128])
decoder.stages.2.convs.0.all_modules.0.weight shape torch.Size([128, 256, 3, 3, 3])
decoder.stages.2.convs.0.all_modules.0.bias shape torch.Size([128])
decoder.stages.2.convs.0.all_modules.1.weight shape torch.Size([128])
decoder.stages.2.convs.0.all_modules.1.bias shape torch.Size([128])
decoder.stages.2.convs.1.conv.weight shape torch.Size([128, 128, 3, 3, 3])
decoder.stages.2.convs.1.conv.bias shape torch.Size([128])
decoder.stages.2.convs.1.norm.weight shape torch.Size([128])
decoder.stages.2.convs.1.norm.bias shape torch.Size([128])
decoder.stages.2.convs.1.all_modules.0.weight shape torch.Size([128, 128, 3, 3, 3])
decoder.stages.2.convs.1.all_modules.0.bias shape torch.Size([128])
decoder.stages.2.convs.1.all_modules.1.weight shape torch.Size([128])
decoder.stages.2.convs.1.all_modules.1.bias shape torch.Size([128])
decoder.stages.3.convs.0.conv.weight shape torch.Size([64, 128, 3, 3, 3])
decoder.stages.3.convs.0.conv.bias shape torch.Size([64])
decoder.stages.3.convs.0.norm.weight shape torch.Size([64])
decoder.stages.3.convs.0.norm.bias shape torch.Size([64])
decoder.stages.3.convs.0.all_modules.0.weight shape torch.Size([64, 128, 3, 3, 3])
decoder.stages.3.convs.0.all_modules.0.bias shape torch.Size([64])
decoder.stages.3.convs.0.all_modules.1.weight shape torch.Size([64])
decoder.stages.3.convs.0.all_modules.1.bias shape torch.Size([64])
decoder.stages.3.convs.1.conv.weight shape torch.Size([64, 64, 3, 3, 3])
decoder.stages.3.convs.1.conv.bias shape torch.Size([64])
decoder.stages.3.convs.1.norm.weight shape torch.Size([64])
decoder.stages.3.convs.1.norm.bias shape torch.Size([64])
decoder.stages.3.convs.1.all_modules.0.weight shape torch.Size([64, 64, 3, 3, 3])
decoder.stages.3.convs.1.all_modules.0.bias shape torch.Size([64])
decoder.stages.3.convs.1.all_modules.1.weight shape torch.Size([64])
decoder.stages.3.convs.1.all_modules.1.bias shape torch.Size([64])
decoder.stages.4.convs.0.conv.weight shape torch.Size([32, 64, 3, 3, 3])
decoder.stages.4.convs.0.conv.bias shape torch.Size([32])
decoder.stages.4.convs.0.norm.weight shape torch.Size([32])
decoder.stages.4.convs.0.norm.bias shape torch.Size([32])
decoder.stages.4.convs.0.all_modules.0.weight shape torch.Size([32, 64, 3, 3, 3])
decoder.stages.4.convs.0.all_modules.0.bias shape torch.Size([32])
decoder.stages.4.convs.0.all_modules.1.weight shape torch.Size([32])
decoder.stages.4.convs.0.all_modules.1.bias shape torch.Size([32])
decoder.stages.4.convs.1.conv.weight shape torch.Size([32, 32, 3, 3, 3])
decoder.stages.4.convs.1.conv.bias shape torch.Size([32])
decoder.stages.4.convs.1.norm.weight shape torch.Size([32])
decoder.stages.4.convs.1.norm.bias shape torch.Size([32])
decoder.stages.4.convs.1.all_modules.0.weight shape torch.Size([32, 32, 3, 3, 3])
decoder.stages.4.convs.1.all_modules.0.bias shape torch.Size([32])
decoder.stages.4.convs.1.all_modules.1.weight shape torch.Size([32])
decoder.stages.4.convs.1.all_modules.1.bias shape torch.Size([32])
decoder.transpconvs.0.weight shape torch.Size([320, 320, 1, 2, 2])
decoder.transpconvs.0.bias shape torch.Size([320])
decoder.transpconvs.1.weight shape torch.Size([320, 256, 2, 2, 2])
decoder.transpconvs.1.bias shape torch.Size([256])
decoder.transpconvs.2.weight shape torch.Size([256, 128, 2, 2, 2])
decoder.transpconvs.2.bias shape torch.Size([128])
decoder.transpconvs.3.weight shape torch.Size([128, 64, 2, 2, 2])
decoder.transpconvs.3.bias shape torch.Size([64])
decoder.transpconvs.4.weight shape torch.Size([64, 32, 2, 2, 2])
decoder.transpconvs.4.bias shape torch.Size([32])
################### Done ###################
2024-12-08 16:22:06.396044: do_dummy_2d_data_aug: True
2024-12-08 16:22:06.399419: Using splits from existing split file: /home/gridsan/nchutisilp/datasets/nnUNet_Datasets/nnUNet_preprocessed/Dataset309_Sohee_Calcium_OCT_CrossValidation/splits_final.json
2024-12-08 16:22:06.401823: The split file contains 5 splits.
2024-12-08 16:22:06.403004: Desired fold for training: 1
2024-12-08 16:22:06.404161: This split has 6 training and 2 validation cases.
################### Loading pretrained weights from file  /home/gridsan/nchutisilp/projects/OpenAI-CLIP/clip_preivl_poststent_pretrained_nnUNet.pt ###################
Below is the list of overlapping blocks in pretrained model and nnUNet architecture:
encoder.stages.0.0.convs.0.conv.weight shape torch.Size([32, 1, 3, 3, 3])
encoder.stages.0.0.convs.0.conv.bias shape torch.Size([32])
encoder.stages.0.0.convs.0.norm.weight shape torch.Size([32])
encoder.stages.0.0.convs.0.norm.bias shape torch.Size([32])
encoder.stages.0.0.convs.0.all_modules.0.weight shape torch.Size([32, 1, 3, 3, 3])
encoder.stages.0.0.convs.0.all_modules.0.bias shape torch.Size([32])
encoder.stages.0.0.convs.0.all_modules.1.weight shape torch.Size([32])
encoder.stages.0.0.convs.0.all_modules.1.bias shape torch.Size([32])
encoder.stages.0.0.convs.1.conv.weight shape torch.Size([32, 32, 3, 3, 3])
encoder.stages.0.0.convs.1.conv.bias shape torch.Size([32])
encoder.stages.0.0.convs.1.norm.weight shape torch.Size([32])
encoder.stages.0.0.convs.1.norm.bias shape torch.Size([32])
encoder.stages.0.0.convs.1.all_modules.0.weight shape torch.Size([32, 32, 3, 3, 3])
encoder.stages.0.0.convs.1.all_modules.0.bias shape torch.Size([32])
encoder.stages.0.0.convs.1.all_modules.1.weight shape torch.Size([32])
encoder.stages.0.0.convs.1.all_modules.1.bias shape torch.Size([32])
encoder.stages.1.0.convs.0.conv.weight shape torch.Size([64, 32, 3, 3, 3])
encoder.stages.1.0.convs.0.conv.bias shape torch.Size([64])
encoder.stages.1.0.convs.0.norm.weight shape torch.Size([64])
encoder.stages.1.0.convs.0.norm.bias shape torch.Size([64])
encoder.stages.1.0.convs.0.all_modules.0.weight shape torch.Size([64, 32, 3, 3, 3])
encoder.stages.1.0.convs.0.all_modules.0.bias shape torch.Size([64])
encoder.stages.1.0.convs.0.all_modules.1.weight shape torch.Size([64])
encoder.stages.1.0.convs.0.all_modules.1.bias shape torch.Size([64])
encoder.stages.1.0.convs.1.conv.weight shape torch.Size([64, 64, 3, 3, 3])
encoder.stages.1.0.convs.1.conv.bias shape torch.Size([64])
encoder.stages.1.0.convs.1.norm.weight shape torch.Size([64])
encoder.stages.1.0.convs.1.norm.bias shape torch.Size([64])
encoder.stages.1.0.convs.1.all_modules.0.weight shape torch.Size([64, 64, 3, 3, 3])
encoder.stages.1.0.convs.1.all_modules.0.bias shape torch.Size([64])
encoder.stages.1.0.convs.1.all_modules.1.weight shape torch.Size([64])
encoder.stages.1.0.convs.1.all_modules.1.bias shape torch.Size([64])
encoder.stages.2.0.convs.0.conv.weight shape torch.Size([128, 64, 3, 3, 3])
encoder.stages.2.0.convs.0.conv.bias shape torch.Size([128])
encoder.stages.2.0.convs.0.norm.weight shape torch.Size([128])
encoder.stages.2.0.convs.0.norm.bias shape torch.Size([128])
encoder.stages.2.0.convs.0.all_modules.0.weight shape torch.Size([128, 64, 3, 3, 3])
encoder.stages.2.0.convs.0.all_modules.0.bias shape torch.Size([128])
encoder.stages.2.0.convs.0.all_modules.1.weight shape torch.Size([128])
encoder.stages.2.0.convs.0.all_modules.1.bias shape torch.Size([128])
encoder.stages.2.0.convs.1.conv.weight shape torch.Size([128, 128, 3, 3, 3])
encoder.stages.2.0.convs.1.conv.bias shape torch.Size([128])
encoder.stages.2.0.convs.1.norm.weight shape torch.Size([128])
encoder.stages.2.0.convs.1.norm.bias shape torch.Size([128])
encoder.stages.2.0.convs.1.all_modules.0.weight shape torch.Size([128, 128, 3, 3, 3])
encoder.stages.2.0.convs.1.all_modules.0.bias shape torch.Size([128])
encoder.stages.2.0.convs.1.all_modules.1.weight shape torch.Size([128])
encoder.stages.2.0.convs.1.all_modules.1.bias shape torch.Size([128])
encoder.stages.3.0.convs.0.conv.weight shape torch.Size([256, 128, 3, 3, 3])
encoder.stages.3.0.convs.0.conv.bias shape torch.Size([256])
encoder.stages.3.0.convs.0.norm.weight shape torch.Size([256])
encoder.stages.3.0.convs.0.norm.bias shape torch.Size([256])
encoder.stages.3.0.convs.0.all_modules.0.weight shape torch.Size([256, 128, 3, 3, 3])
encoder.stages.3.0.convs.0.all_modules.0.bias shape torch.Size([256])
encoder.stages.3.0.convs.0.all_modules.1.weight shape torch.Size([256])
encoder.stages.3.0.convs.0.all_modules.1.bias shape torch.Size([256])
encoder.stages.3.0.convs.1.conv.weight shape torch.Size([256, 256, 3, 3, 3])
encoder.stages.3.0.convs.1.conv.bias shape torch.Size([256])
encoder.stages.3.0.convs.1.norm.weight shape torch.Size([256])
encoder.stages.3.0.convs.1.norm.bias shape torch.Size([256])
encoder.stages.3.0.convs.1.all_modules.0.weight shape torch.Size([256, 256, 3, 3, 3])
encoder.stages.3.0.convs.1.all_modules.0.bias shape torch.Size([256])
encoder.stages.3.0.convs.1.all_modules.1.weight shape torch.Size([256])
encoder.stages.3.0.convs.1.all_modules.1.bias shape torch.Size([256])
encoder.stages.4.0.convs.0.conv.weight shape torch.Size([320, 256, 3, 3, 3])
encoder.stages.4.0.convs.0.conv.bias shape torch.Size([320])
encoder.stages.4.0.convs.0.norm.weight shape torch.Size([320])
encoder.stages.4.0.convs.0.norm.bias shape torch.Size([320])
encoder.stages.4.0.convs.0.all_modules.0.weight shape torch.Size([320, 256, 3, 3, 3])
encoder.stages.4.0.convs.0.all_modules.0.bias shape torch.Size([320])
encoder.stages.4.0.convs.0.all_modules.1.weight shape torch.Size([320])
encoder.stages.4.0.convs.0.all_modules.1.bias shape torch.Size([320])
encoder.stages.4.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.4.0.convs.1.conv.bias shape torch.Size([320])
encoder.stages.4.0.convs.1.norm.weight shape torch.Size([320])
encoder.stages.4.0.convs.1.norm.bias shape torch.Size([320])
encoder.stages.4.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.4.0.convs.1.all_modules.0.bias shape torch.Size([320])
encoder.stages.4.0.convs.1.all_modules.1.weight shape torch.Size([320])
encoder.stages.4.0.convs.1.all_modules.1.bias shape torch.Size([320])
encoder.stages.5.0.convs.0.conv.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.5.0.convs.0.conv.bias shape torch.Size([320])
encoder.stages.5.0.convs.0.norm.weight shape torch.Size([320])
encoder.stages.5.0.convs.0.norm.bias shape torch.Size([320])
encoder.stages.5.0.convs.0.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.5.0.convs.0.all_modules.0.bias shape torch.Size([320])
encoder.stages.5.0.convs.0.all_modules.1.weight shape torch.Size([320])
encoder.stages.5.0.convs.0.all_modules.1.bias shape torch.Size([320])
encoder.stages.5.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.5.0.convs.1.conv.bias shape torch.Size([320])
encoder.stages.5.0.convs.1.norm.weight shape torch.Size([320])
encoder.stages.5.0.convs.1.norm.bias shape torch.Size([320])
encoder.stages.5.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.5.0.convs.1.all_modules.0.bias shape torch.Size([320])
encoder.stages.5.0.convs.1.all_modules.1.weight shape torch.Size([320])
encoder.stages.5.0.convs.1.all_modules.1.bias shape torch.Size([320])
decoder.encoder.stages.0.0.convs.0.conv.weight shape torch.Size([32, 1, 3, 3, 3])
decoder.encoder.stages.0.0.convs.0.conv.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.norm.weight shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.norm.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.all_modules.0.weight shape torch.Size([32, 1, 3, 3, 3])
decoder.encoder.stages.0.0.convs.0.all_modules.0.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.all_modules.1.weight shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.all_modules.1.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.conv.weight shape torch.Size([32, 32, 3, 3, 3])
decoder.encoder.stages.0.0.convs.1.conv.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.norm.weight shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.norm.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.all_modules.0.weight shape torch.Size([32, 32, 3, 3, 3])
decoder.encoder.stages.0.0.convs.1.all_modules.0.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.all_modules.1.weight shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.all_modules.1.bias shape torch.Size([32])
decoder.encoder.stages.1.0.convs.0.conv.weight shape torch.Size([64, 32, 3, 3, 3])
decoder.encoder.stages.1.0.convs.0.conv.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.norm.weight shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.norm.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.all_modules.0.weight shape torch.Size([64, 32, 3, 3, 3])
decoder.encoder.stages.1.0.convs.0.all_modules.0.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.all_modules.1.weight shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.all_modules.1.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.conv.weight shape torch.Size([64, 64, 3, 3, 3])
decoder.encoder.stages.1.0.convs.1.conv.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.norm.weight shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.norm.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.all_modules.0.weight shape torch.Size([64, 64, 3, 3, 3])
decoder.encoder.stages.1.0.convs.1.all_modules.0.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.all_modules.1.weight shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.all_modules.1.bias shape torch.Size([64])
decoder.encoder.stages.2.0.convs.0.conv.weight shape torch.Size([128, 64, 3, 3, 3])
decoder.encoder.stages.2.0.convs.0.conv.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.norm.weight shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.norm.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.all_modules.0.weight shape torch.Size([128, 64, 3, 3, 3])
decoder.encoder.stages.2.0.convs.0.all_modules.0.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.all_modules.1.weight shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.all_modules.1.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.conv.weight shape torch.Size([128, 128, 3, 3, 3])
decoder.encoder.stages.2.0.convs.1.conv.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.norm.weight shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.norm.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.all_modules.0.weight shape torch.Size([128, 128, 3, 3, 3])
decoder.encoder.stages.2.0.convs.1.all_modules.0.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.all_modules.1.weight shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.all_modules.1.bias shape torch.Size([128])
decoder.encoder.stages.3.0.convs.0.conv.weight shape torch.Size([256, 128, 3, 3, 3])
decoder.encoder.stages.3.0.convs.0.conv.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.norm.weight shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.norm.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.all_modules.0.weight shape torch.Size([256, 128, 3, 3, 3])
decoder.encoder.stages.3.0.convs.0.all_modules.0.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.all_modules.1.weight shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.all_modules.1.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.conv.weight shape torch.Size([256, 256, 3, 3, 3])
decoder.encoder.stages.3.0.convs.1.conv.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.norm.weight shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.norm.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.all_modules.0.weight shape torch.Size([256, 256, 3, 3, 3])
decoder.encoder.stages.3.0.convs.1.all_modules.0.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.all_modules.1.weight shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.all_modules.1.bias shape torch.Size([256])
decoder.encoder.stages.4.0.convs.0.conv.weight shape torch.Size([320, 256, 3, 3, 3])
decoder.encoder.stages.4.0.convs.0.conv.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.norm.weight shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.norm.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.all_modules.0.weight shape torch.Size([320, 256, 3, 3, 3])
decoder.encoder.stages.4.0.convs.0.all_modules.0.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.all_modules.1.weight shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.all_modules.1.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.4.0.convs.1.conv.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.norm.weight shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.norm.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.4.0.convs.1.all_modules.0.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.all_modules.1.weight shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.all_modules.1.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.conv.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.5.0.convs.0.conv.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.norm.weight shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.norm.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.5.0.convs.0.all_modules.0.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.all_modules.1.weight shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.all_modules.1.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.5.0.convs.1.conv.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.norm.weight shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.norm.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.5.0.convs.1.all_modules.0.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.all_modules.1.weight shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.all_modules.1.bias shape torch.Size([320])
decoder.stages.0.convs.0.conv.weight shape torch.Size([320, 640, 3, 3, 3])
decoder.stages.0.convs.0.conv.bias shape torch.Size([320])
decoder.stages.0.convs.0.norm.weight shape torch.Size([320])
decoder.stages.0.convs.0.norm.bias shape torch.Size([320])
decoder.stages.0.convs.0.all_modules.0.weight shape torch.Size([320, 640, 3, 3, 3])
decoder.stages.0.convs.0.all_modules.0.bias shape torch.Size([320])
decoder.stages.0.convs.0.all_modules.1.weight shape torch.Size([320])
decoder.stages.0.convs.0.all_modules.1.bias shape torch.Size([320])
decoder.stages.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.stages.0.convs.1.conv.bias shape torch.Size([320])
decoder.stages.0.convs.1.norm.weight shape torch.Size([320])
decoder.stages.0.convs.1.norm.bias shape torch.Size([320])
decoder.stages.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.stages.0.convs.1.all_modules.0.bias shape torch.Size([320])
decoder.stages.0.convs.1.all_modules.1.weight shape torch.Size([320])
decoder.stages.0.convs.1.all_modules.1.bias shape torch.Size([320])
decoder.stages.1.convs.0.conv.weight shape torch.Size([256, 512, 3, 3, 3])
decoder.stages.1.convs.0.conv.bias shape torch.Size([256])
decoder.stages.1.convs.0.norm.weight shape torch.Size([256])
decoder.stages.1.convs.0.norm.bias shape torch.Size([256])
decoder.stages.1.convs.0.all_modules.0.weight shape torch.Size([256, 512, 3, 3, 3])
decoder.stages.1.convs.0.all_modules.0.bias shape torch.Size([256])
decoder.stages.1.convs.0.all_modules.1.weight shape torch.Size([256])
decoder.stages.1.convs.0.all_modules.1.bias shape torch.Size([256])
decoder.stages.1.convs.1.conv.weight shape torch.Size([256, 256, 3, 3, 3])
decoder.stages.1.convs.1.conv.bias shape torch.Size([256])
decoder.stages.1.convs.1.norm.weight shape torch.Size([256])
decoder.stages.1.convs.1.norm.bias shape torch.Size([256])
decoder.stages.1.convs.1.all_modules.0.weight shape torch.Size([256, 256, 3, 3, 3])
decoder.stages.1.convs.1.all_modules.0.bias shape torch.Size([256])
decoder.stages.1.convs.1.all_modules.1.weight shape torch.Size([256])
decoder.stages.1.convs.1.all_modules.1.bias shape torch.Size([256])
decoder.stages.2.convs.0.conv.weight shape torch.Size([128, 256, 3, 3, 3])
decoder.stages.2.convs.0.conv.bias shape torch.Size([128])
decoder.stages.2.convs.0.norm.weight shape torch.Size([128])
decoder.stages.2.convs.0.norm.bias shape torch.Size([128])
decoder.stages.2.convs.0.all_modules.0.weight shape torch.Size([128, 256, 3, 3, 3])
decoder.stages.2.convs.0.all_modules.0.bias shape torch.Size([128])
decoder.stages.2.convs.0.all_modules.1.weight shape torch.Size([128])
decoder.stages.2.convs.0.all_modules.1.bias shape torch.Size([128])
decoder.stages.2.convs.1.conv.weight shape torch.Size([128, 128, 3, 3, 3])
decoder.stages.2.convs.1.conv.bias shape torch.Size([128])
decoder.stages.2.convs.1.norm.weight shape torch.Size([128])
decoder.stages.2.convs.1.norm.bias shape torch.Size([128])
decoder.stages.2.convs.1.all_modules.0.weight shape torch.Size([128, 128, 3, 3, 3])
decoder.stages.2.convs.1.all_modules.0.bias shape torch.Size([128])
decoder.stages.2.convs.1.all_modules.1.weight shape torch.Size([128])
decoder.stages.2.convs.1.all_modules.1.bias shape torch.Size([128])
decoder.stages.3.convs.0.conv.weight shape torch.Size([64, 128, 3, 3, 3])
decoder.stages.3.convs.0.conv.bias shape torch.Size([64])
decoder.stages.3.convs.0.norm.weight shape torch.Size([64])
decoder.stages.3.convs.0.norm.bias shape torch.Size([64])
decoder.stages.3.convs.0.all_modules.0.weight shape torch.Size([64, 128, 3, 3, 3])
decoder.stages.3.convs.0.all_modules.0.bias shape torch.Size([64])
decoder.stages.3.convs.0.all_modules.1.weight shape torch.Size([64])
decoder.stages.3.convs.0.all_modules.1.bias shape torch.Size([64])
decoder.stages.3.convs.1.conv.weight shape torch.Size([64, 64, 3, 3, 3])
decoder.stages.3.convs.1.conv.bias shape torch.Size([64])
decoder.stages.3.convs.1.norm.weight shape torch.Size([64])
decoder.stages.3.convs.1.norm.bias shape torch.Size([64])
decoder.stages.3.convs.1.all_modules.0.weight shape torch.Size([64, 64, 3, 3, 3])
decoder.stages.3.convs.1.all_modules.0.bias shape torch.Size([64])
decoder.stages.3.convs.1.all_modules.1.weight shape torch.Size([64])
decoder.stages.3.convs.1.all_modules.1.bias shape torch.Size([64])
decoder.stages.4.convs.0.conv.weight shape torch.Size([32, 64, 3, 3, 3])
decoder.stages.4.convs.0.conv.bias shape torch.Size([32])
decoder.stages.4.convs.0.norm.weight shape torch.Size([32])
decoder.stages.4.convs.0.norm.bias shape torch.Size([32])
decoder.stages.4.convs.0.all_modules.0.weight shape torch.Size([32, 64, 3, 3, 3])
decoder.stages.4.convs.0.all_modules.0.bias shape torch.Size([32])
decoder.stages.4.convs.0.all_modules.1.weight shape torch.Size([32])
decoder.stages.4.convs.0.all_modules.1.bias shape torch.Size([32])
decoder.stages.4.convs.1.conv.weight shape torch.Size([32, 32, 3, 3, 3])
decoder.stages.4.convs.1.conv.bias shape torch.Size([32])
decoder.stages.4.convs.1.norm.weight shape torch.Size([32])
decoder.stages.4.convs.1.norm.bias shape torch.Size([32])
decoder.stages.4.convs.1.all_modules.0.weight shape torch.Size([32, 32, 3, 3, 3])
decoder.stages.4.convs.1.all_modules.0.bias shape torch.Size([32])
decoder.stages.4.convs.1.all_modules.1.weight shape torch.Size([32])
decoder.stages.4.convs.1.all_modules.1.bias shape torch.Size([32])
decoder.transpconvs.0.weight shape torch.Size([320, 320, 1, 2, 2])
decoder.transpconvs.0.bias shape torch.Size([320])
decoder.transpconvs.1.weight shape torch.Size([320, 256, 2, 2, 2])
decoder.transpconvs.1.bias shape torch.Size([256])
decoder.transpconvs.2.weight shape torch.Size([256, 128, 2, 2, 2])
decoder.transpconvs.2.bias shape torch.Size([128])
decoder.transpconvs.3.weight shape torch.Size([128, 64, 2, 2, 2])
decoder.transpconvs.3.bias shape torch.Size([64])
decoder.transpconvs.4.weight shape torch.Size([64, 32, 2, 2, 2])
decoder.transpconvs.4.bias shape torch.Size([32])
################### Done ###################
2024-12-08 16:22:06.396064: do_dummy_2d_data_aug: True
2024-12-08 16:22:06.399433: Using splits from existing split file: /home/gridsan/nchutisilp/datasets/nnUNet_Datasets/nnUNet_preprocessed/Dataset309_Sohee_Calcium_OCT_CrossValidation/splits_final.json
2024-12-08 16:22:06.402142: The split file contains 5 splits.
2024-12-08 16:22:06.403503: Desired fold for training: 0
2024-12-08 16:22:06.404408: This split has 6 training and 2 validation cases.
using pin_memory on device 0
using pin_memory on device 0

This is the configuration used by this training:
Configuration name: 3d_32x160x128_b10
 {'data_identifier': 'nnUNetPlans_3d_fullres', 'preprocessor_name': 'DefaultPreprocessor', 'batch_size': 10, 'patch_size': [32, 160, 128], 'median_image_size_in_voxels': [375.0, 498.0, 498.0], 'spacing': [1.0, 1.0, 1.0], 'normalization_schemes': ['ZScoreNormalization'], 'use_mask_for_norm': [False], 'resampling_fn_data': 'resample_data_or_seg_to_shape', 'resampling_fn_seg': 'resample_data_or_seg_to_shape', 'resampling_fn_data_kwargs': {'is_seg': False, 'order': 3, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_seg_kwargs': {'is_seg': True, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_probabilities': 'resample_data_or_seg_to_shape', 'resampling_fn_probabilities_kwargs': {'is_seg': False, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'architecture': {'network_class_name': 'dynamic_network_architectures.architectures.unet.PlainConvUNet', 'arch_kwargs': {'n_stages': 6, 'features_per_stage': [32, 64, 128, 256, 320, 320], 'conv_op': 'torch.nn.modules.conv.Conv3d', 'kernel_sizes': [[3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3]], 'strides': [[1, 1, 1], [2, 2, 2], [2, 2, 2], [2, 2, 2], [2, 2, 2], [1, 2, 2]], 'n_conv_per_stage': [2, 2, 2, 2, 2, 2], 'n_conv_per_stage_decoder': [2, 2, 2, 2, 2], 'conv_bias': True, 'norm_op': 'torch.nn.modules.instancenorm.InstanceNorm3d', 'norm_op_kwargs': {'eps': 1e-05, 'affine': True}, 'dropout_op': None, 'dropout_op_kwargs': None, 'nonlin': 'torch.nn.LeakyReLU', 'nonlin_kwargs': {'inplace': True}, 'deep_supervision': True}, '_kw_requires_import': ['conv_op', 'norm_op', 'dropout_op', 'nonlin']}, 'batch_dice': True, 'inherits_from': '3d_fullres'} 

These are the global plan.json settings:
 {'dataset_name': 'Dataset309_Sohee_Calcium_OCT_CrossValidation', 'plans_name': 'nnUNetPlans', 'original_median_spacing_after_transp': [1.0, 1.0, 1.0], 'original_median_shape_after_transp': [375, 498, 498], 'image_reader_writer': 'SimpleITKIO', 'transpose_forward': [0, 1, 2], 'transpose_backward': [0, 1, 2], 'experiment_planner_used': 'ExperimentPlanner', 'label_manager': 'LabelManager', 'foreground_intensity_properties_per_channel': {'0': {'max': 0.4977976381778717, 'mean': 0.13507525622844696, 'median': 0.09599608182907104, 'min': 0.0, 'percentile_00_5': 0.014754901640117168, 'percentile_99_5': 0.4977976381778717, 'std': 0.12025152146816254}}} 

2024-12-08 16:22:15.929169: unpacking dataset...
using pin_memory on device 0
using pin_memory on device 0

This is the configuration used by this training:
Configuration name: 3d_32x160x128_b10
 {'data_identifier': 'nnUNetPlans_3d_fullres', 'preprocessor_name': 'DefaultPreprocessor', 'batch_size': 10, 'patch_size': [32, 160, 128], 'median_image_size_in_voxels': [375.0, 498.0, 498.0], 'spacing': [1.0, 1.0, 1.0], 'normalization_schemes': ['ZScoreNormalization'], 'use_mask_for_norm': [False], 'resampling_fn_data': 'resample_data_or_seg_to_shape', 'resampling_fn_seg': 'resample_data_or_seg_to_shape', 'resampling_fn_data_kwargs': {'is_seg': False, 'order': 3, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_seg_kwargs': {'is_seg': True, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_probabilities': 'resample_data_or_seg_to_shape', 'resampling_fn_probabilities_kwargs': {'is_seg': False, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'architecture': {'network_class_name': 'dynamic_network_architectures.architectures.unet.PlainConvUNet', 'arch_kwargs': {'n_stages': 6, 'features_per_stage': [32, 64, 128, 256, 320, 320], 'conv_op': 'torch.nn.modules.conv.Conv3d', 'kernel_sizes': [[3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3]], 'strides': [[1, 1, 1], [2, 2, 2], [2, 2, 2], [2, 2, 2], [2, 2, 2], [1, 2, 2]], 'n_conv_per_stage': [2, 2, 2, 2, 2, 2], 'n_conv_per_stage_decoder': [2, 2, 2, 2, 2], 'conv_bias': True, 'norm_op': 'torch.nn.modules.instancenorm.InstanceNorm3d', 'norm_op_kwargs': {'eps': 1e-05, 'affine': True}, 'dropout_op': None, 'dropout_op_kwargs': None, 'nonlin': 'torch.nn.LeakyReLU', 'nonlin_kwargs': {'inplace': True}, 'deep_supervision': True}, '_kw_requires_import': ['conv_op', 'norm_op', 'dropout_op', 'nonlin']}, 'batch_dice': True, 'inherits_from': '3d_fullres'} 

These are the global plan.json settings:
 {'dataset_name': 'Dataset309_Sohee_Calcium_OCT_CrossValidation', 'plans_name': 'nnUNetPlans', 'original_median_spacing_after_transp': [1.0, 1.0, 1.0], 'original_median_shape_after_transp': [375, 498, 498], 'image_reader_writer': 'SimpleITKIO', 'transpose_forward': [0, 1, 2], 'transpose_backward': [0, 1, 2], 'experiment_planner_used': 'ExperimentPlanner', 'label_manager': 'LabelManager', 'foreground_intensity_properties_per_channel': {'0': {'max': 0.4977976381778717, 'mean': 0.13507525622844696, 'median': 0.09599608182907104, 'min': 0.0, 'percentile_00_5': 0.014754901640117168, 'percentile_99_5': 0.4977976381778717, 'std': 0.12025152146816254}}} 

2024-12-08 16:22:17.174784: unpacking dataset...
2024-12-08 16:22:22.010443: unpacking done...
2024-12-08 16:22:22.110557: Unable to plot network architecture: nnUNet_compile is enabled!
2024-12-08 16:22:22.801913: 
2024-12-08 16:22:22.803379: Epoch 0
2024-12-08 16:22:22.804939: Current learning rate: 0.01
2024-12-08 16:25:51.793326: Validation loss improved from 1000.00000 to -0.18420! Patience: 0/50
2024-12-08 16:25:51.831556: train_loss -0.071
2024-12-08 16:25:51.841430: val_loss -0.1842
2024-12-08 16:25:51.842331: Pseudo dice [0.5774]
2024-12-08 16:25:51.843475: Epoch time: 208.99 s
2024-12-08 16:25:51.844727: Yayy! New best EMA pseudo Dice: 0.5774
2024-12-08 16:25:54.415987: 
2024-12-08 16:25:54.417294: Epoch 1
2024-12-08 16:25:54.418447: Current learning rate: 0.00999
2024-12-08 16:27:22.665370: Validation loss improved from -0.18420 to -0.24914! Patience: 0/50
2024-12-08 16:27:22.666435: train_loss -0.2476
2024-12-08 16:27:22.667386: val_loss -0.2491
2024-12-08 16:27:22.668319: Pseudo dice [0.6026]
2024-12-08 16:27:22.669169: Epoch time: 88.25 s
2024-12-08 16:27:22.669942: Yayy! New best EMA pseudo Dice: 0.5799
2024-12-08 16:27:24.261083: 
2024-12-08 16:27:24.262633: Epoch 2
2024-12-08 16:27:24.263491: Current learning rate: 0.00998
2024-12-08 16:28:52.947822: Validation loss did not improve from -0.24914. Patience: 1/50
2024-12-08 16:28:52.948741: train_loss -0.2851
2024-12-08 16:28:52.949755: val_loss -0.2247
2024-12-08 16:28:52.950488: Pseudo dice [0.5732]
2024-12-08 16:28:52.951261: Epoch time: 88.69 s
2024-12-08 16:28:54.262463: 
2024-12-08 16:28:54.263814: Epoch 3
2024-12-08 16:28:54.264933: Current learning rate: 0.00997
2024-12-08 16:30:23.293207: Validation loss improved from -0.24914 to -0.33768! Patience: 1/50
2024-12-08 16:30:23.294055: train_loss -0.3274
2024-12-08 16:30:23.294966: val_loss -0.3377
2024-12-08 16:30:23.295732: Pseudo dice [0.6573]
2024-12-08 16:30:23.296427: Epoch time: 89.03 s
2024-12-08 16:30:23.297111: Yayy! New best EMA pseudo Dice: 0.587
2024-12-08 16:30:24.876254: 
2024-12-08 16:30:24.878109: Epoch 4
2024-12-08 16:30:24.878888: Current learning rate: 0.00996
2024-12-08 16:31:54.117865: Validation loss did not improve from -0.33768. Patience: 1/50
2024-12-08 16:31:54.119177: train_loss -0.3572
2024-12-08 16:31:54.120060: val_loss -0.3162
2024-12-08 16:31:54.120840: Pseudo dice [0.6445]
2024-12-08 16:31:54.121648: Epoch time: 89.24 s
2024-12-08 16:31:54.447610: Yayy! New best EMA pseudo Dice: 0.5928
2024-12-08 16:31:56.084672: 
2024-12-08 16:31:56.086016: Epoch 5
2024-12-08 16:31:56.086929: Current learning rate: 0.00995
2024-12-08 16:33:25.429232: Validation loss improved from -0.33768 to -0.38247! Patience: 1/50
2024-12-08 16:33:25.430367: train_loss -0.3735
2024-12-08 16:33:25.431078: val_loss -0.3825
2024-12-08 16:33:25.431791: Pseudo dice [0.6868]
2024-12-08 16:33:25.432438: Epoch time: 89.35 s
2024-12-08 16:33:25.433081: Yayy! New best EMA pseudo Dice: 0.6022
2024-12-08 16:33:27.022053: 
2024-12-08 16:33:27.023515: Epoch 6
2024-12-08 16:33:27.024246: Current learning rate: 0.00995
2024-12-08 16:34:55.878236: Validation loss did not improve from -0.38247. Patience: 1/50
2024-12-08 16:34:55.879117: train_loss -0.4084
2024-12-08 16:34:55.879907: val_loss -0.3529
2024-12-08 16:34:55.880605: Pseudo dice [0.6657]
2024-12-08 16:34:55.881283: Epoch time: 88.86 s
2024-12-08 16:34:55.882004: Yayy! New best EMA pseudo Dice: 0.6085
2024-12-08 16:34:57.519984: 
2024-12-08 16:34:57.521568: Epoch 7
2024-12-08 16:34:57.522930: Current learning rate: 0.00994
2024-12-08 16:36:25.420508: Validation loss did not improve from -0.38247. Patience: 2/50
2024-12-08 16:36:25.421503: train_loss -0.4186
2024-12-08 16:36:25.422337: val_loss -0.3114
2024-12-08 16:36:25.423210: Pseudo dice [0.6166]
2024-12-08 16:36:25.423917: Epoch time: 87.9 s
2024-12-08 16:36:25.424569: Yayy! New best EMA pseudo Dice: 0.6093
2024-12-08 16:36:27.449598: 
2024-12-08 16:36:27.451481: Epoch 8
2024-12-08 16:36:27.452331: Current learning rate: 0.00993
2024-12-08 16:37:57.122146: Validation loss improved from -0.38247 to -0.38467! Patience: 2/50
2024-12-08 16:37:57.123241: train_loss -0.4407
2024-12-08 16:37:57.124300: val_loss -0.3847
2024-12-08 16:37:57.124984: Pseudo dice [0.6758]
2024-12-08 16:37:57.125616: Epoch time: 89.67 s
2024-12-08 16:37:57.126246: Yayy! New best EMA pseudo Dice: 0.616
2024-12-08 16:37:58.805260: 
2024-12-08 16:37:58.806973: Epoch 9
2024-12-08 16:37:58.807717: Current learning rate: 0.00992
2024-12-08 16:39:28.405116: Validation loss improved from -0.38467 to -0.40746! Patience: 0/50
2024-12-08 16:39:28.406243: train_loss -0.4378
2024-12-08 16:39:28.407131: val_loss -0.4075
2024-12-08 16:39:28.407833: Pseudo dice [0.6958]
2024-12-08 16:39:28.408462: Epoch time: 89.6 s
2024-12-08 16:39:28.767647: Yayy! New best EMA pseudo Dice: 0.624
2024-12-08 16:39:30.360911: 
2024-12-08 16:39:30.362267: Epoch 10
2024-12-08 16:39:30.362898: Current learning rate: 0.00991
2024-12-08 16:40:59.987216: Validation loss did not improve from -0.40746. Patience: 1/50
2024-12-08 16:40:59.988538: train_loss -0.4652
2024-12-08 16:40:59.989614: val_loss -0.3929
2024-12-08 16:40:59.990267: Pseudo dice [0.6833]
2024-12-08 16:40:59.990890: Epoch time: 89.63 s
2024-12-08 16:40:59.991581: Yayy! New best EMA pseudo Dice: 0.6299
2024-12-08 16:41:01.639524: 
2024-12-08 16:41:01.641020: Epoch 11
2024-12-08 16:41:01.641924: Current learning rate: 0.0099
2024-12-08 16:42:31.220435: Validation loss improved from -0.40746 to -0.41332! Patience: 1/50
2024-12-08 16:42:31.221528: train_loss -0.4725
2024-12-08 16:42:31.222603: val_loss -0.4133
2024-12-08 16:42:31.223244: Pseudo dice [0.7119]
2024-12-08 16:42:31.223916: Epoch time: 89.58 s
2024-12-08 16:42:31.224686: Yayy! New best EMA pseudo Dice: 0.6381
2024-12-08 16:42:32.802666: 
2024-12-08 16:42:32.803983: Epoch 12
2024-12-08 16:42:32.804766: Current learning rate: 0.00989
2024-12-08 16:44:00.976022: Validation loss improved from -0.41332 to -0.41426! Patience: 0/50
2024-12-08 16:44:00.977235: train_loss -0.4784
2024-12-08 16:44:00.978188: val_loss -0.4143
2024-12-08 16:44:00.979048: Pseudo dice [0.7083]
2024-12-08 16:44:00.979952: Epoch time: 88.18 s
2024-12-08 16:44:00.981230: Yayy! New best EMA pseudo Dice: 0.6451
2024-12-08 16:44:02.576624: 
2024-12-08 16:44:02.578421: Epoch 13
2024-12-08 16:44:02.579177: Current learning rate: 0.00988
2024-12-08 16:45:30.835290: Validation loss improved from -0.41426 to -0.44635! Patience: 0/50
2024-12-08 16:45:30.836497: train_loss -0.4911
2024-12-08 16:45:30.837862: val_loss -0.4464
2024-12-08 16:45:30.838689: Pseudo dice [0.717]
2024-12-08 16:45:30.839616: Epoch time: 88.26 s
2024-12-08 16:45:30.840624: Yayy! New best EMA pseudo Dice: 0.6523
2024-12-08 16:45:32.458087: 
2024-12-08 16:45:32.459698: Epoch 14
2024-12-08 16:45:32.460684: Current learning rate: 0.00987
2024-12-08 16:47:00.682502: Validation loss did not improve from -0.44635. Patience: 1/50
2024-12-08 16:47:00.683775: train_loss -0.5099
2024-12-08 16:47:00.684993: val_loss -0.4245
2024-12-08 16:47:00.685650: Pseudo dice [0.6989]
2024-12-08 16:47:00.686375: Epoch time: 88.23 s
2024-12-08 16:47:01.065709: Yayy! New best EMA pseudo Dice: 0.657
2024-12-08 16:47:02.657582: 
2024-12-08 16:47:02.659568: Epoch 15
2024-12-08 16:47:02.660700: Current learning rate: 0.00986
2024-12-08 16:48:30.927507: Validation loss did not improve from -0.44635. Patience: 2/50
2024-12-08 16:48:30.928764: train_loss -0.518
2024-12-08 16:48:30.929868: val_loss -0.4339
2024-12-08 16:48:30.930707: Pseudo dice [0.7092]
2024-12-08 16:48:30.931477: Epoch time: 88.27 s
2024-12-08 16:48:30.932342: Yayy! New best EMA pseudo Dice: 0.6622
2024-12-08 16:48:32.639090: 
2024-12-08 16:48:32.640703: Epoch 16
2024-12-08 16:48:32.641933: Current learning rate: 0.00986
2024-12-08 16:50:01.619436: Validation loss improved from -0.44635 to -0.46496! Patience: 2/50
2024-12-08 16:50:01.620533: train_loss -0.5173
2024-12-08 16:50:01.621480: val_loss -0.465
2024-12-08 16:50:01.622148: Pseudo dice [0.7225]
2024-12-08 16:50:01.622837: Epoch time: 88.98 s
2024-12-08 16:50:01.623408: Yayy! New best EMA pseudo Dice: 0.6682
2024-12-08 16:50:03.301770: 
2024-12-08 16:50:03.302896: Epoch 17
2024-12-08 16:50:03.303723: Current learning rate: 0.00985
2024-12-08 16:51:31.876454: Validation loss improved from -0.46496 to -0.49220! Patience: 0/50
2024-12-08 16:51:31.877548: train_loss -0.5231
2024-12-08 16:51:31.878274: val_loss -0.4922
2024-12-08 16:51:31.878915: Pseudo dice [0.7491]
2024-12-08 16:51:31.879578: Epoch time: 88.58 s
2024-12-08 16:51:31.880235: Yayy! New best EMA pseudo Dice: 0.6763
2024-12-08 16:51:33.549258: 
2024-12-08 16:51:33.550727: Epoch 18
2024-12-08 16:51:33.551528: Current learning rate: 0.00984
2024-12-08 16:53:02.048607: Validation loss did not improve from -0.49220. Patience: 1/50
2024-12-08 16:53:02.049549: train_loss -0.5225
2024-12-08 16:53:02.050445: val_loss -0.482
2024-12-08 16:53:02.051102: Pseudo dice [0.7339]
2024-12-08 16:53:02.051984: Epoch time: 88.5 s
2024-12-08 16:53:02.052858: Yayy! New best EMA pseudo Dice: 0.6821
2024-12-08 16:53:04.059602: 
2024-12-08 16:53:04.061113: Epoch 19
2024-12-08 16:53:04.061969: Current learning rate: 0.00983
2024-12-08 16:54:32.820710: Validation loss did not improve from -0.49220. Patience: 2/50
2024-12-08 16:54:32.821431: train_loss -0.5271
2024-12-08 16:54:32.822353: val_loss -0.4914
2024-12-08 16:54:32.823077: Pseudo dice [0.7437]
2024-12-08 16:54:32.824069: Epoch time: 88.76 s
2024-12-08 16:54:33.166068: Yayy! New best EMA pseudo Dice: 0.6882
2024-12-08 16:54:34.780537: 
2024-12-08 16:54:34.782532: Epoch 20
2024-12-08 16:54:34.783394: Current learning rate: 0.00982
2024-12-08 16:56:03.594342: Validation loss did not improve from -0.49220. Patience: 3/50
2024-12-08 16:56:03.595098: train_loss -0.5332
2024-12-08 16:56:03.596094: val_loss -0.485
2024-12-08 16:56:03.596972: Pseudo dice [0.743]
2024-12-08 16:56:03.597706: Epoch time: 88.82 s
2024-12-08 16:56:03.598459: Yayy! New best EMA pseudo Dice: 0.6937
2024-12-08 16:56:05.222035: 
2024-12-08 16:56:05.223601: Epoch 21
2024-12-08 16:56:05.224646: Current learning rate: 0.00981
2024-12-08 16:57:33.606290: Validation loss did not improve from -0.49220. Patience: 4/50
2024-12-08 16:57:33.607566: train_loss -0.54
2024-12-08 16:57:33.608630: val_loss -0.4762
2024-12-08 16:57:33.609307: Pseudo dice [0.7249]
2024-12-08 16:57:33.610062: Epoch time: 88.39 s
2024-12-08 16:57:33.610777: Yayy! New best EMA pseudo Dice: 0.6968
2024-12-08 16:57:35.182767: 
2024-12-08 16:57:35.184740: Epoch 22
2024-12-08 16:57:35.185428: Current learning rate: 0.0098
2024-12-08 16:59:03.715794: Validation loss did not improve from -0.49220. Patience: 5/50
2024-12-08 16:59:03.716850: train_loss -0.5449
2024-12-08 16:59:03.717701: val_loss -0.4856
2024-12-08 16:59:03.718358: Pseudo dice [0.7416]
2024-12-08 16:59:03.718966: Epoch time: 88.54 s
2024-12-08 16:59:03.719553: Yayy! New best EMA pseudo Dice: 0.7013
2024-12-08 16:59:05.276382: 
2024-12-08 16:59:05.277625: Epoch 23
2024-12-08 16:59:05.278410: Current learning rate: 0.00979
2024-12-08 17:00:33.819962: Validation loss did not improve from -0.49220. Patience: 6/50
2024-12-08 17:00:33.820915: train_loss -0.547
2024-12-08 17:00:33.821877: val_loss -0.4308
2024-12-08 17:00:33.822872: Pseudo dice [0.7199]
2024-12-08 17:00:33.823780: Epoch time: 88.55 s
2024-12-08 17:00:33.824637: Yayy! New best EMA pseudo Dice: 0.7032
2024-12-08 17:00:35.368133: 
2024-12-08 17:00:35.370309: Epoch 24
2024-12-08 17:00:35.371427: Current learning rate: 0.00978
2024-12-08 17:02:03.966602: Validation loss did not improve from -0.49220. Patience: 7/50
2024-12-08 17:02:03.967664: train_loss -0.5613
2024-12-08 17:02:03.968445: val_loss -0.4561
2024-12-08 17:02:03.969214: Pseudo dice [0.7216]
2024-12-08 17:02:03.969939: Epoch time: 88.6 s
2024-12-08 17:02:04.307266: Yayy! New best EMA pseudo Dice: 0.705
2024-12-08 17:02:05.900637: 
2024-12-08 17:02:05.902027: Epoch 25
2024-12-08 17:02:05.902756: Current learning rate: 0.00977
2024-12-08 17:03:35.071074: Validation loss did not improve from -0.49220. Patience: 8/50
2024-12-08 17:03:35.072266: train_loss -0.5669
2024-12-08 17:03:35.073308: val_loss -0.4402
2024-12-08 17:03:35.074036: Pseudo dice [0.7253]
2024-12-08 17:03:35.074847: Epoch time: 89.17 s
2024-12-08 17:03:35.075640: Yayy! New best EMA pseudo Dice: 0.707
2024-12-08 17:03:36.636128: 
2024-12-08 17:03:36.637459: Epoch 26
2024-12-08 17:03:36.638308: Current learning rate: 0.00977
2024-12-08 17:05:05.239289: Validation loss did not improve from -0.49220. Patience: 9/50
2024-12-08 17:05:05.240574: train_loss -0.5585
2024-12-08 17:05:05.241611: val_loss -0.4856
2024-12-08 17:05:05.242454: Pseudo dice [0.7313]
2024-12-08 17:05:05.243165: Epoch time: 88.61 s
2024-12-08 17:05:05.243975: Yayy! New best EMA pseudo Dice: 0.7095
2024-12-08 17:05:06.844848: 
2024-12-08 17:05:06.846568: Epoch 27
2024-12-08 17:05:06.847349: Current learning rate: 0.00976
2024-12-08 17:06:35.368257: Validation loss did not improve from -0.49220. Patience: 10/50
2024-12-08 17:06:35.369319: train_loss -0.5748
2024-12-08 17:06:35.370263: val_loss -0.4871
2024-12-08 17:06:35.371017: Pseudo dice [0.7378]
2024-12-08 17:06:35.371647: Epoch time: 88.53 s
2024-12-08 17:06:35.372400: Yayy! New best EMA pseudo Dice: 0.7123
2024-12-08 17:06:36.941232: 
2024-12-08 17:06:36.943052: Epoch 28
2024-12-08 17:06:36.943829: Current learning rate: 0.00975
2024-12-08 17:08:05.384984: Validation loss did not improve from -0.49220. Patience: 11/50
2024-12-08 17:08:05.385889: train_loss -0.5774
2024-12-08 17:08:05.386849: val_loss -0.4904
2024-12-08 17:08:05.387611: Pseudo dice [0.7487]
2024-12-08 17:08:05.388308: Epoch time: 88.45 s
2024-12-08 17:08:05.389015: Yayy! New best EMA pseudo Dice: 0.7159
2024-12-08 17:08:07.362734: 
2024-12-08 17:08:07.364596: Epoch 29
2024-12-08 17:08:07.365401: Current learning rate: 0.00974
2024-12-08 17:09:35.875214: Validation loss improved from -0.49220 to -0.51100! Patience: 11/50
2024-12-08 17:09:35.876126: train_loss -0.5801
2024-12-08 17:09:35.877038: val_loss -0.511
2024-12-08 17:09:35.877842: Pseudo dice [0.7604]
2024-12-08 17:09:35.878628: Epoch time: 88.51 s
2024-12-08 17:09:36.227661: Yayy! New best EMA pseudo Dice: 0.7204
2024-12-08 17:09:37.875336: 
2024-12-08 17:09:37.876977: Epoch 30
2024-12-08 17:09:37.877784: Current learning rate: 0.00973
2024-12-08 17:11:06.207463: Validation loss did not improve from -0.51100. Patience: 1/50
2024-12-08 17:11:06.208484: train_loss -0.5829
2024-12-08 17:11:06.209445: val_loss -0.4507
2024-12-08 17:11:06.210199: Pseudo dice [0.7279]
2024-12-08 17:11:06.210943: Epoch time: 88.33 s
2024-12-08 17:11:06.211591: Yayy! New best EMA pseudo Dice: 0.7211
2024-12-08 17:11:07.818984: 
2024-12-08 17:11:07.820482: Epoch 31
2024-12-08 17:11:07.821357: Current learning rate: 0.00972
2024-12-08 17:12:36.127781: Validation loss did not improve from -0.51100. Patience: 2/50
2024-12-08 17:12:36.128718: train_loss -0.5789
2024-12-08 17:12:36.129675: val_loss -0.458
2024-12-08 17:12:36.130468: Pseudo dice [0.7367]
2024-12-08 17:12:36.131104: Epoch time: 88.31 s
2024-12-08 17:12:36.131861: Yayy! New best EMA pseudo Dice: 0.7227
2024-12-08 17:12:37.749665: 
2024-12-08 17:12:37.751047: Epoch 32
2024-12-08 17:12:37.751781: Current learning rate: 0.00971
2024-12-08 17:14:07.370800: Validation loss did not improve from -0.51100. Patience: 3/50
2024-12-08 17:14:07.372045: train_loss -0.5772
2024-12-08 17:14:07.373162: val_loss -0.4836
2024-12-08 17:14:07.374541: Pseudo dice [0.7415]
2024-12-08 17:14:07.375593: Epoch time: 89.62 s
2024-12-08 17:14:07.376638: Yayy! New best EMA pseudo Dice: 0.7246
2024-12-08 17:14:09.022255: 
2024-12-08 17:14:09.023657: Epoch 33
2024-12-08 17:14:09.024776: Current learning rate: 0.0097
2024-12-08 17:15:38.636291: Validation loss improved from -0.51100 to -0.51429! Patience: 3/50
2024-12-08 17:15:38.637444: train_loss -0.5832
2024-12-08 17:15:38.638456: val_loss -0.5143
2024-12-08 17:15:38.639237: Pseudo dice [0.7588]
2024-12-08 17:15:38.639984: Epoch time: 89.62 s
2024-12-08 17:15:38.640645: Yayy! New best EMA pseudo Dice: 0.728
2024-12-08 17:15:40.263322: 
2024-12-08 17:15:40.264891: Epoch 34
2024-12-08 17:15:40.265628: Current learning rate: 0.00969
2024-12-08 17:17:09.819932: Validation loss did not improve from -0.51429. Patience: 1/50
2024-12-08 17:17:09.820766: train_loss -0.5926
2024-12-08 17:17:09.821863: val_loss -0.4954
2024-12-08 17:17:09.822863: Pseudo dice [0.7452]
2024-12-08 17:17:09.823847: Epoch time: 89.56 s
2024-12-08 17:17:10.189451: Yayy! New best EMA pseudo Dice: 0.7297
2024-12-08 17:17:11.786936: 
2024-12-08 17:17:11.788331: Epoch 35
2024-12-08 17:17:11.789386: Current learning rate: 0.00968
2024-12-08 17:18:41.325987: Validation loss did not improve from -0.51429. Patience: 2/50
2024-12-08 17:18:41.327100: train_loss -0.59
2024-12-08 17:18:41.328064: val_loss -0.4763
2024-12-08 17:18:41.328983: Pseudo dice [0.7369]
2024-12-08 17:18:41.329769: Epoch time: 89.54 s
2024-12-08 17:18:41.330518: Yayy! New best EMA pseudo Dice: 0.7304
2024-12-08 17:18:42.971159: 
2024-12-08 17:18:42.972780: Epoch 36
2024-12-08 17:18:42.973731: Current learning rate: 0.00968
2024-12-08 17:20:12.604096: Validation loss did not improve from -0.51429. Patience: 3/50
2024-12-08 17:20:12.605648: train_loss -0.6094
2024-12-08 17:20:12.607451: val_loss -0.5019
2024-12-08 17:20:12.608428: Pseudo dice [0.7474]
2024-12-08 17:20:12.609282: Epoch time: 89.64 s
2024-12-08 17:20:12.610094: Yayy! New best EMA pseudo Dice: 0.7321
2024-12-08 17:20:14.229812: 
2024-12-08 17:20:14.231650: Epoch 37
2024-12-08 17:20:14.232695: Current learning rate: 0.00967
2024-12-08 17:21:43.660581: Validation loss did not improve from -0.51429. Patience: 4/50
2024-12-08 17:21:43.661711: train_loss -0.6058
2024-12-08 17:21:43.662591: val_loss -0.5134
2024-12-08 17:21:43.663241: Pseudo dice [0.7637]
2024-12-08 17:21:43.664054: Epoch time: 89.43 s
2024-12-08 17:21:43.664745: Yayy! New best EMA pseudo Dice: 0.7353
2024-12-08 17:21:45.299899: 
2024-12-08 17:21:45.301460: Epoch 38
2024-12-08 17:21:45.302219: Current learning rate: 0.00966
2024-12-08 17:23:14.673147: Validation loss did not improve from -0.51429. Patience: 5/50
2024-12-08 17:23:14.673946: train_loss -0.6092
2024-12-08 17:23:14.674683: val_loss -0.4955
2024-12-08 17:23:14.675376: Pseudo dice [0.7459]
2024-12-08 17:23:14.675993: Epoch time: 89.37 s
2024-12-08 17:23:14.676649: Yayy! New best EMA pseudo Dice: 0.7363
2024-12-08 17:23:16.664927: 
2024-12-08 17:23:16.666666: Epoch 39
2024-12-08 17:23:16.667791: Current learning rate: 0.00965
2024-12-08 17:24:45.928521: Validation loss did not improve from -0.51429. Patience: 6/50
2024-12-08 17:24:45.929562: train_loss -0.6143
2024-12-08 17:24:45.930531: val_loss -0.4335
2024-12-08 17:24:45.931290: Pseudo dice [0.7189]
2024-12-08 17:24:45.931919: Epoch time: 89.27 s
2024-12-08 17:24:47.563249: 
2024-12-08 17:24:47.564929: Epoch 40
2024-12-08 17:24:47.565610: Current learning rate: 0.00964
2024-12-08 17:26:16.861922: Validation loss did not improve from -0.51429. Patience: 7/50
2024-12-08 17:26:16.862782: train_loss -0.6212
2024-12-08 17:26:16.863644: val_loss -0.4427
2024-12-08 17:26:16.864410: Pseudo dice [0.7187]
2024-12-08 17:26:16.865083: Epoch time: 89.3 s
2024-12-08 17:26:18.185941: 
2024-12-08 17:26:18.187883: Epoch 41
2024-12-08 17:26:18.188775: Current learning rate: 0.00963
2024-12-08 17:27:47.749956: Validation loss did not improve from -0.51429. Patience: 8/50
2024-12-08 17:27:47.751214: train_loss -0.6156
2024-12-08 17:27:47.752378: val_loss -0.5041
2024-12-08 17:27:47.753276: Pseudo dice [0.7561]
2024-12-08 17:27:47.754237: Epoch time: 89.57 s
2024-12-08 17:27:48.994395: 
2024-12-08 17:27:48.995674: Epoch 42
2024-12-08 17:27:48.996446: Current learning rate: 0.00962
2024-12-08 17:29:18.389157: Validation loss did not improve from -0.51429. Patience: 9/50
2024-12-08 17:29:18.390048: train_loss -0.6234
2024-12-08 17:29:18.390787: val_loss -0.4511
2024-12-08 17:29:18.391413: Pseudo dice [0.7289]
2024-12-08 17:29:18.392109: Epoch time: 89.4 s
2024-12-08 17:29:19.620393: 
2024-12-08 17:29:19.622404: Epoch 43
2024-12-08 17:29:19.623273: Current learning rate: 0.00961
2024-12-08 17:30:49.067577: Validation loss did not improve from -0.51429. Patience: 10/50
2024-12-08 17:30:49.068827: train_loss -0.6313
2024-12-08 17:30:49.069896: val_loss -0.501
2024-12-08 17:30:49.070664: Pseudo dice [0.7632]
2024-12-08 17:30:49.071318: Epoch time: 89.45 s
2024-12-08 17:30:49.072107: Yayy! New best EMA pseudo Dice: 0.7375
2024-12-08 17:30:50.623183: 
2024-12-08 17:30:50.625097: Epoch 44
2024-12-08 17:30:50.625977: Current learning rate: 0.0096
2024-12-08 17:32:20.045747: Validation loss did not improve from -0.51429. Patience: 11/50
2024-12-08 17:32:20.046629: train_loss -0.6257
2024-12-08 17:32:20.047533: val_loss -0.509
2024-12-08 17:32:20.048354: Pseudo dice [0.7449]
2024-12-08 17:32:20.049105: Epoch time: 89.42 s
2024-12-08 17:32:20.367741: Yayy! New best EMA pseudo Dice: 0.7383
2024-12-08 17:32:21.976825: 
2024-12-08 17:32:21.978695: Epoch 45
2024-12-08 17:32:21.979397: Current learning rate: 0.00959
2024-12-08 17:33:51.309922: Validation loss did not improve from -0.51429. Patience: 12/50
2024-12-08 17:33:51.310864: train_loss -0.6387
2024-12-08 17:33:51.311626: val_loss -0.5083
2024-12-08 17:33:51.312351: Pseudo dice [0.7433]
2024-12-08 17:33:51.312991: Epoch time: 89.33 s
2024-12-08 17:33:51.313654: Yayy! New best EMA pseudo Dice: 0.7388
2024-12-08 17:33:52.950186: 
2024-12-08 17:33:52.951536: Epoch 46
2024-12-08 17:33:52.952529: Current learning rate: 0.00959
2024-12-08 17:35:22.448275: Validation loss improved from -0.51429 to -0.53766! Patience: 12/50
2024-12-08 17:35:22.449196: train_loss -0.6237
2024-12-08 17:35:22.450278: val_loss -0.5377
2024-12-08 17:35:22.451264: Pseudo dice [0.7659]
2024-12-08 17:35:22.452246: Epoch time: 89.5 s
2024-12-08 17:35:22.452945: Yayy! New best EMA pseudo Dice: 0.7415
2024-12-08 17:35:24.009026: 
2024-12-08 17:35:24.010811: Epoch 47
2024-12-08 17:35:24.011628: Current learning rate: 0.00958
2024-12-08 17:36:53.430445: Validation loss did not improve from -0.53766. Patience: 1/50
2024-12-08 17:36:53.431443: train_loss -0.6339
2024-12-08 17:36:53.432355: val_loss -0.4983
2024-12-08 17:36:53.433187: Pseudo dice [0.7446]
2024-12-08 17:36:53.434014: Epoch time: 89.42 s
2024-12-08 17:36:53.434668: Yayy! New best EMA pseudo Dice: 0.7418
2024-12-08 17:36:55.032444: 
2024-12-08 17:36:55.034862: Epoch 48
2024-12-08 17:36:55.035949: Current learning rate: 0.00957
2024-12-08 17:38:24.449760: Validation loss did not improve from -0.53766. Patience: 2/50
2024-12-08 17:38:24.450799: train_loss -0.6362
2024-12-08 17:38:24.451696: val_loss -0.5178
2024-12-08 17:38:24.452352: Pseudo dice [0.7678]
2024-12-08 17:38:24.453089: Epoch time: 89.42 s
2024-12-08 17:38:24.453716: Yayy! New best EMA pseudo Dice: 0.7444
2024-12-08 17:38:26.034750: 
2024-12-08 17:38:26.036546: Epoch 49
2024-12-08 17:38:26.037304: Current learning rate: 0.00956
2024-12-08 17:39:55.428963: Validation loss did not improve from -0.53766. Patience: 3/50
2024-12-08 17:39:55.429670: train_loss -0.6391
2024-12-08 17:39:55.430497: val_loss -0.5345
2024-12-08 17:39:55.431225: Pseudo dice [0.7699]
2024-12-08 17:39:55.431930: Epoch time: 89.4 s
2024-12-08 17:39:55.798644: Yayy! New best EMA pseudo Dice: 0.7469
2024-12-08 17:39:57.773407: 
2024-12-08 17:39:57.774950: Epoch 50
2024-12-08 17:39:57.775672: Current learning rate: 0.00955
2024-12-08 17:41:26.969620: Validation loss did not improve from -0.53766. Patience: 4/50
2024-12-08 17:41:26.970650: train_loss -0.6294
2024-12-08 17:41:26.971599: val_loss -0.5154
2024-12-08 17:41:26.972595: Pseudo dice [0.7694]
2024-12-08 17:41:26.973443: Epoch time: 89.2 s
2024-12-08 17:41:26.974365: Yayy! New best EMA pseudo Dice: 0.7492
2024-12-08 17:41:28.585019: 
2024-12-08 17:41:28.586486: Epoch 51
2024-12-08 17:41:28.587312: Current learning rate: 0.00954
2024-12-08 17:42:57.958829: Validation loss improved from -0.53766 to -0.54321! Patience: 4/50
2024-12-08 17:42:57.960129: train_loss -0.6345
2024-12-08 17:42:57.960871: val_loss -0.5432
2024-12-08 17:42:57.961503: Pseudo dice [0.7741]
2024-12-08 17:42:57.962145: Epoch time: 89.38 s
2024-12-08 17:42:57.962933: Yayy! New best EMA pseudo Dice: 0.7517
2024-12-08 17:42:59.562348: 
2024-12-08 17:42:59.563769: Epoch 52
2024-12-08 17:42:59.564474: Current learning rate: 0.00953
2024-12-08 17:44:28.955418: Validation loss did not improve from -0.54321. Patience: 1/50
2024-12-08 17:44:28.956649: train_loss -0.64
2024-12-08 17:44:28.957822: val_loss -0.4843
2024-12-08 17:44:28.958670: Pseudo dice [0.7335]
2024-12-08 17:44:28.959528: Epoch time: 89.4 s
2024-12-08 17:44:30.208624: 
2024-12-08 17:44:30.210315: Epoch 53
2024-12-08 17:44:30.211115: Current learning rate: 0.00952
2024-12-08 17:45:59.720859: Validation loss did not improve from -0.54321. Patience: 2/50
2024-12-08 17:45:59.722020: train_loss -0.643
2024-12-08 17:45:59.722867: val_loss -0.5184
2024-12-08 17:45:59.723583: Pseudo dice [0.7772]
2024-12-08 17:45:59.724327: Epoch time: 89.51 s
2024-12-08 17:45:59.724962: Yayy! New best EMA pseudo Dice: 0.7526
2024-12-08 17:46:01.305973: 
2024-12-08 17:46:01.307457: Epoch 54
2024-12-08 17:46:01.308118: Current learning rate: 0.00951
2024-12-08 17:47:30.908097: Validation loss did not improve from -0.54321. Patience: 3/50
2024-12-08 17:47:30.909263: train_loss -0.65
2024-12-08 17:47:30.910161: val_loss -0.4877
2024-12-08 17:47:30.910829: Pseudo dice [0.7501]
2024-12-08 17:47:30.911510: Epoch time: 89.6 s
2024-12-08 17:47:32.517148: 
2024-12-08 17:47:32.519046: Epoch 55
2024-12-08 17:47:32.519990: Current learning rate: 0.0095
2024-12-08 17:49:02.053831: Validation loss did not improve from -0.54321. Patience: 4/50
2024-12-08 17:49:02.055147: train_loss -0.6574
2024-12-08 17:49:02.056041: val_loss -0.4708
2024-12-08 17:49:02.056767: Pseudo dice [0.7378]
2024-12-08 17:49:02.057531: Epoch time: 89.54 s
2024-12-08 17:49:03.285821: 
2024-12-08 17:49:03.287688: Epoch 56
2024-12-08 17:49:03.288415: Current learning rate: 0.00949
2024-12-08 17:50:32.806946: Validation loss improved from -0.54321 to -0.55785! Patience: 4/50
2024-12-08 17:50:32.807926: train_loss -0.6487
2024-12-08 17:50:32.809127: val_loss -0.5578
2024-12-08 17:50:32.810115: Pseudo dice [0.7838]
2024-12-08 17:50:32.811025: Epoch time: 89.52 s
2024-12-08 17:50:32.811906: Yayy! New best EMA pseudo Dice: 0.7542
2024-12-08 17:50:34.446432: 
2024-12-08 17:50:34.448558: Epoch 57
2024-12-08 17:50:34.449755: Current learning rate: 0.00949
2024-12-08 17:52:04.025649: Validation loss did not improve from -0.55785. Patience: 1/50
2024-12-08 17:52:04.026715: train_loss -0.6541
2024-12-08 17:52:04.027570: val_loss -0.5276
2024-12-08 17:52:04.028394: Pseudo dice [0.762]
2024-12-08 17:52:04.029130: Epoch time: 89.58 s
2024-12-08 17:52:04.029752: Yayy! New best EMA pseudo Dice: 0.755
2024-12-08 17:52:05.632155: 
2024-12-08 17:52:05.633869: Epoch 58
2024-12-08 17:52:05.634546: Current learning rate: 0.00948
2024-12-08 17:53:35.137505: Validation loss did not improve from -0.55785. Patience: 2/50
2024-12-08 17:53:35.138561: train_loss -0.6568
2024-12-08 17:53:35.139432: val_loss -0.4865
2024-12-08 17:53:35.140146: Pseudo dice [0.7374]
2024-12-08 17:53:35.140842: Epoch time: 89.51 s
2024-12-08 17:53:36.402792: 
2024-12-08 17:53:36.404381: Epoch 59
2024-12-08 17:53:36.405154: Current learning rate: 0.00947
2024-12-08 17:55:05.909060: Validation loss did not improve from -0.55785. Patience: 3/50
2024-12-08 17:55:05.910334: train_loss -0.6555
2024-12-08 17:55:05.911087: val_loss -0.4831
2024-12-08 17:55:05.911739: Pseudo dice [0.7469]
2024-12-08 17:55:05.912439: Epoch time: 89.51 s
2024-12-08 17:55:07.955142: 
2024-12-08 17:55:07.956818: Epoch 60
2024-12-08 17:55:07.957608: Current learning rate: 0.00946
2024-12-08 17:56:37.162743: Validation loss did not improve from -0.55785. Patience: 4/50
2024-12-08 17:56:37.163899: train_loss -0.6526
2024-12-08 17:56:37.164920: val_loss -0.4922
2024-12-08 17:56:37.165801: Pseudo dice [0.7505]
2024-12-08 17:56:37.166632: Epoch time: 89.21 s
2024-12-08 17:56:38.376554: 
2024-12-08 17:56:38.378046: Epoch 61
2024-12-08 17:56:38.378871: Current learning rate: 0.00945
2024-12-08 17:58:07.582510: Validation loss did not improve from -0.55785. Patience: 5/50
2024-12-08 17:58:07.583589: train_loss -0.6582
2024-12-08 17:58:07.584810: val_loss -0.5331
2024-12-08 17:58:07.585485: Pseudo dice [0.7665]
2024-12-08 17:58:07.586139: Epoch time: 89.21 s
2024-12-08 17:58:08.848718: 
2024-12-08 17:58:08.851202: Epoch 62
2024-12-08 17:58:08.852475: Current learning rate: 0.00944
2024-12-08 17:59:38.095059: Validation loss did not improve from -0.55785. Patience: 6/50
2024-12-08 17:59:38.096228: train_loss -0.6659
2024-12-08 17:59:38.097136: val_loss -0.5169
2024-12-08 17:59:38.097764: Pseudo dice [0.7572]
2024-12-08 17:59:38.098451: Epoch time: 89.25 s
2024-12-08 17:59:39.331650: 
2024-12-08 17:59:39.333486: Epoch 63
2024-12-08 17:59:39.334135: Current learning rate: 0.00943
2024-12-08 18:01:08.543248: Validation loss did not improve from -0.55785. Patience: 7/50
2024-12-08 18:01:08.544172: train_loss -0.6692
2024-12-08 18:01:08.545204: val_loss -0.5439
2024-12-08 18:01:08.546051: Pseudo dice [0.7783]
2024-12-08 18:01:08.546928: Epoch time: 89.21 s
2024-12-08 18:01:08.547891: Yayy! New best EMA pseudo Dice: 0.7565
2024-12-08 18:01:10.239668: 
2024-12-08 18:01:10.241341: Epoch 64
2024-12-08 18:01:10.242149: Current learning rate: 0.00942
2024-12-08 18:02:39.435852: Validation loss did not improve from -0.55785. Patience: 8/50
2024-12-08 18:02:39.436752: train_loss -0.6647
2024-12-08 18:02:39.437666: val_loss -0.5406
2024-12-08 18:02:39.438305: Pseudo dice [0.7684]
2024-12-08 18:02:39.438925: Epoch time: 89.2 s
2024-12-08 18:02:39.787187: Yayy! New best EMA pseudo Dice: 0.7577
2024-12-08 18:02:41.415020: 
2024-12-08 18:02:41.416536: Epoch 65
2024-12-08 18:02:41.417366: Current learning rate: 0.00941
2024-12-08 18:04:10.646949: Validation loss did not improve from -0.55785. Patience: 9/50
2024-12-08 18:04:10.647683: train_loss -0.6657
2024-12-08 18:04:10.648825: val_loss -0.5405
2024-12-08 18:04:10.649796: Pseudo dice [0.7773]
2024-12-08 18:04:10.650413: Epoch time: 89.23 s
2024-12-08 18:04:10.651085: Yayy! New best EMA pseudo Dice: 0.7597
2024-12-08 18:04:12.334572: 
2024-12-08 18:04:12.336012: Epoch 66
2024-12-08 18:04:12.336781: Current learning rate: 0.0094
2024-12-08 18:05:41.634408: Validation loss did not improve from -0.55785. Patience: 10/50
2024-12-08 18:05:41.635257: train_loss -0.6636
2024-12-08 18:05:41.636137: val_loss -0.4533
2024-12-08 18:05:41.636769: Pseudo dice [0.7188]
2024-12-08 18:05:41.637456: Epoch time: 89.3 s
2024-12-08 18:05:42.881519: 
2024-12-08 18:05:42.882297: Epoch 67
2024-12-08 18:05:42.883415: Current learning rate: 0.00939
2024-12-08 18:07:12.187796: Validation loss did not improve from -0.55785. Patience: 11/50
2024-12-08 18:07:12.189269: train_loss -0.6624
2024-12-08 18:07:12.190241: val_loss -0.5311
2024-12-08 18:07:12.191007: Pseudo dice [0.7618]
2024-12-08 18:07:12.191727: Epoch time: 89.31 s
2024-12-08 18:07:13.475365: 
2024-12-08 18:07:13.476911: Epoch 68
2024-12-08 18:07:13.477764: Current learning rate: 0.00939
2024-12-08 18:08:42.907308: Validation loss did not improve from -0.55785. Patience: 12/50
2024-12-08 18:08:42.908328: train_loss -0.6642
2024-12-08 18:08:42.909204: val_loss -0.3806
2024-12-08 18:08:42.909855: Pseudo dice [0.6833]
2024-12-08 18:08:42.910511: Epoch time: 89.43 s
2024-12-08 18:08:44.182046: 
2024-12-08 18:08:44.183827: Epoch 69
2024-12-08 18:08:44.184510: Current learning rate: 0.00938
2024-12-08 18:10:13.631538: Validation loss did not improve from -0.55785. Patience: 13/50
2024-12-08 18:10:13.632872: train_loss -0.6718
2024-12-08 18:10:13.633731: val_loss -0.4976
2024-12-08 18:10:13.634385: Pseudo dice [0.7544]
2024-12-08 18:10:13.634990: Epoch time: 89.45 s
2024-12-08 18:10:15.290825: 
2024-12-08 18:10:15.292053: Epoch 70
2024-12-08 18:10:15.292823: Current learning rate: 0.00937
2024-12-08 18:11:44.713259: Validation loss did not improve from -0.55785. Patience: 14/50
2024-12-08 18:11:44.714450: train_loss -0.6858
2024-12-08 18:11:44.715390: val_loss -0.5276
2024-12-08 18:11:44.716211: Pseudo dice [0.7667]
2024-12-08 18:11:44.717106: Epoch time: 89.42 s
2024-12-08 18:11:46.373982: 
2024-12-08 18:11:46.375739: Epoch 71
2024-12-08 18:11:46.376531: Current learning rate: 0.00936
2024-12-08 18:13:15.865120: Validation loss improved from -0.55785 to -0.56386! Patience: 14/50
2024-12-08 18:13:15.866725: train_loss -0.6798
2024-12-08 18:13:15.867961: val_loss -0.5639
2024-12-08 18:13:15.868766: Pseudo dice [0.7832]
2024-12-08 18:13:15.869511: Epoch time: 89.49 s
2024-12-08 18:13:17.148724: 
2024-12-08 18:13:17.151020: Epoch 72
2024-12-08 18:13:17.152417: Current learning rate: 0.00935
2024-12-08 18:14:45.243634: Validation loss did not improve from -0.56386. Patience: 1/50
2024-12-08 18:14:45.244353: train_loss -0.6821
2024-12-08 18:14:45.245233: val_loss -0.5421
2024-12-08 18:14:45.246027: Pseudo dice [0.778]
2024-12-08 18:14:45.246871: Epoch time: 88.1 s
2024-12-08 18:14:46.533152: 
2024-12-08 18:14:46.534829: Epoch 73
2024-12-08 18:14:46.535837: Current learning rate: 0.00934
2024-12-08 18:16:14.736480: Validation loss did not improve from -0.56386. Patience: 2/50
2024-12-08 18:16:14.737667: train_loss -0.6824
2024-12-08 18:16:14.738499: val_loss -0.5129
2024-12-08 18:16:14.739179: Pseudo dice [0.7563]
2024-12-08 18:16:14.739825: Epoch time: 88.21 s
2024-12-08 18:16:16.019157: 
2024-12-08 18:16:16.020658: Epoch 74
2024-12-08 18:16:16.021301: Current learning rate: 0.00933
2024-12-08 18:17:44.176226: Validation loss did not improve from -0.56386. Patience: 3/50
2024-12-08 18:17:44.177248: train_loss -0.6819
2024-12-08 18:17:44.178119: val_loss -0.5357
2024-12-08 18:17:44.178745: Pseudo dice [0.7732]
2024-12-08 18:17:44.179556: Epoch time: 88.16 s
2024-12-08 18:17:45.887764: 
2024-12-08 18:17:45.889337: Epoch 75
2024-12-08 18:17:45.890611: Current learning rate: 0.00932
2024-12-08 18:19:15.234949: Validation loss did not improve from -0.56386. Patience: 4/50
2024-12-08 18:19:15.235936: train_loss -0.6806
2024-12-08 18:19:15.236789: val_loss -0.4992
2024-12-08 18:19:15.237600: Pseudo dice [0.7454]
2024-12-08 18:19:15.238354: Epoch time: 89.35 s
2024-12-08 18:19:16.494527: 
2024-12-08 18:19:16.497006: Epoch 76
2024-12-08 18:19:16.498279: Current learning rate: 0.00931
2024-12-08 18:20:44.867851: Validation loss did not improve from -0.56386. Patience: 5/50
2024-12-08 18:20:44.868930: train_loss -0.691
2024-12-08 18:20:44.869918: val_loss -0.5454
2024-12-08 18:20:44.870746: Pseudo dice [0.7762]
2024-12-08 18:20:44.871561: Epoch time: 88.38 s
2024-12-08 18:20:46.157460: 
2024-12-08 18:20:46.159370: Epoch 77
2024-12-08 18:20:46.160143: Current learning rate: 0.0093
2024-12-08 18:22:15.895621: Validation loss did not improve from -0.56386. Patience: 6/50
2024-12-08 18:22:15.896793: train_loss -0.684
2024-12-08 18:22:15.897625: val_loss -0.5587
2024-12-08 18:22:15.898446: Pseudo dice [0.783]
2024-12-08 18:22:15.899243: Epoch time: 89.74 s
2024-12-08 18:22:15.900125: Yayy! New best EMA pseudo Dice: 0.7614
2024-12-08 18:22:17.560977: 
2024-12-08 18:22:17.562760: Epoch 78
2024-12-08 18:22:17.563672: Current learning rate: 0.0093
2024-12-08 18:23:47.414829: Validation loss did not improve from -0.56386. Patience: 7/50
2024-12-08 18:23:47.416087: train_loss -0.6815
2024-12-08 18:23:47.417111: val_loss -0.5608
2024-12-08 18:23:47.417912: Pseudo dice [0.787]
2024-12-08 18:23:47.418675: Epoch time: 89.86 s
2024-12-08 18:23:47.419445: Yayy! New best EMA pseudo Dice: 0.764
2024-12-08 18:23:49.119578: 
2024-12-08 18:23:49.121088: Epoch 79
2024-12-08 18:23:49.122146: Current learning rate: 0.00929
2024-12-08 18:25:18.216327: Validation loss did not improve from -0.56386. Patience: 8/50
2024-12-08 18:25:18.217454: train_loss -0.6826
2024-12-08 18:25:18.218563: val_loss -0.5211
2024-12-08 18:25:18.219267: Pseudo dice [0.7678]
2024-12-08 18:25:18.219930: Epoch time: 89.1 s
2024-12-08 18:25:18.592334: Yayy! New best EMA pseudo Dice: 0.7643
2024-12-08 18:25:20.236689: 
2024-12-08 18:25:20.238254: Epoch 80
2024-12-08 18:25:20.239027: Current learning rate: 0.00928
2024-12-08 18:26:50.016278: Validation loss did not improve from -0.56386. Patience: 9/50
2024-12-08 18:26:50.017395: train_loss -0.6897
2024-12-08 18:26:50.018206: val_loss -0.5178
2024-12-08 18:26:50.018968: Pseudo dice [0.7574]
2024-12-08 18:26:50.019824: Epoch time: 89.78 s
2024-12-08 18:26:51.679023: 
2024-12-08 18:26:51.680828: Epoch 81
2024-12-08 18:26:51.681723: Current learning rate: 0.00927
2024-12-08 18:28:21.422585: Validation loss did not improve from -0.56386. Patience: 10/50
2024-12-08 18:28:21.423681: train_loss -0.6978
2024-12-08 18:28:21.424788: val_loss -0.5307
2024-12-08 18:28:21.425774: Pseudo dice [0.7628]
2024-12-08 18:28:21.426709: Epoch time: 89.75 s
2024-12-08 18:28:22.719395: 
2024-12-08 18:28:22.721064: Epoch 82
2024-12-08 18:28:22.722014: Current learning rate: 0.00926
2024-12-08 18:29:52.392982: Validation loss did not improve from -0.56386. Patience: 11/50
2024-12-08 18:29:52.394084: train_loss -0.6976
2024-12-08 18:29:52.395204: val_loss -0.5256
2024-12-08 18:29:52.396064: Pseudo dice [0.7753]
2024-12-08 18:29:52.396897: Epoch time: 89.68 s
2024-12-08 18:29:52.397636: Yayy! New best EMA pseudo Dice: 0.7647
2024-12-08 18:29:53.957935: 
2024-12-08 18:29:53.959305: Epoch 83
2024-12-08 18:29:53.960368: Current learning rate: 0.00925
2024-12-08 18:31:23.735619: Validation loss did not improve from -0.56386. Patience: 12/50
2024-12-08 18:31:23.736313: train_loss -0.6888
2024-12-08 18:31:23.737302: val_loss -0.5128
2024-12-08 18:31:23.738243: Pseudo dice [0.7611]
2024-12-08 18:31:23.739153: Epoch time: 89.78 s
2024-12-08 18:31:25.002175: 
2024-12-08 18:31:25.003659: Epoch 84
2024-12-08 18:31:25.004486: Current learning rate: 0.00924
2024-12-08 18:32:54.826049: Validation loss did not improve from -0.56386. Patience: 13/50
2024-12-08 18:32:54.828660: train_loss -0.6963
2024-12-08 18:32:54.829543: val_loss -0.5353
2024-12-08 18:32:54.830300: Pseudo dice [0.7713]
2024-12-08 18:32:54.831073: Epoch time: 89.83 s
2024-12-08 18:32:55.189319: Yayy! New best EMA pseudo Dice: 0.7651
2024-12-08 18:32:56.786337: 
2024-12-08 18:32:56.787672: Epoch 85
2024-12-08 18:32:56.788421: Current learning rate: 0.00923
2024-12-08 18:34:26.323716: Validation loss did not improve from -0.56386. Patience: 14/50
2024-12-08 18:34:26.324844: train_loss -0.7
2024-12-08 18:34:26.325789: val_loss -0.5253
2024-12-08 18:34:26.326424: Pseudo dice [0.7737]
2024-12-08 18:34:26.327066: Epoch time: 89.54 s
2024-12-08 18:34:26.327652: Yayy! New best EMA pseudo Dice: 0.7659
2024-12-08 18:34:27.888324: 
2024-12-08 18:34:27.889963: Epoch 86
2024-12-08 18:34:27.890714: Current learning rate: 0.00922
2024-12-08 18:35:57.444932: Validation loss did not improve from -0.56386. Patience: 15/50
2024-12-08 18:35:57.446053: train_loss -0.6996
2024-12-08 18:35:57.446916: val_loss -0.4224
2024-12-08 18:35:57.447687: Pseudo dice [0.7025]
2024-12-08 18:35:57.448420: Epoch time: 89.56 s
2024-12-08 18:35:58.670401: 
2024-12-08 18:35:58.671571: Epoch 87
2024-12-08 18:35:58.672502: Current learning rate: 0.00921
2024-12-08 18:37:28.264660: Validation loss did not improve from -0.56386. Patience: 16/50
2024-12-08 18:37:28.265753: train_loss -0.6987
2024-12-08 18:37:28.266870: val_loss -0.5339
2024-12-08 18:37:28.267675: Pseudo dice [0.7699]
2024-12-08 18:37:28.268423: Epoch time: 89.6 s
2024-12-08 18:37:29.497949: 
2024-12-08 18:37:29.499567: Epoch 88
2024-12-08 18:37:29.500349: Current learning rate: 0.0092
2024-12-08 18:38:59.127588: Validation loss did not improve from -0.56386. Patience: 17/50
2024-12-08 18:38:59.128375: train_loss -0.6983
2024-12-08 18:38:59.129068: val_loss -0.5167
2024-12-08 18:38:59.129689: Pseudo dice [0.7589]
2024-12-08 18:38:59.130324: Epoch time: 89.63 s
2024-12-08 18:39:00.332012: 
2024-12-08 18:39:00.333445: Epoch 89
2024-12-08 18:39:00.334072: Current learning rate: 0.0092
2024-12-08 18:40:29.983093: Validation loss did not improve from -0.56386. Patience: 18/50
2024-12-08 18:40:29.984242: train_loss -0.7118
2024-12-08 18:40:29.985315: val_loss -0.5436
2024-12-08 18:40:29.986170: Pseudo dice [0.7765]
2024-12-08 18:40:29.987225: Epoch time: 89.65 s
2024-12-08 18:40:31.597221: 
2024-12-08 18:40:31.598946: Epoch 90
2024-12-08 18:40:31.600027: Current learning rate: 0.00919
2024-12-08 18:42:01.258685: Validation loss did not improve from -0.56386. Patience: 19/50
2024-12-08 18:42:01.259981: train_loss -0.7093
2024-12-08 18:42:01.260865: val_loss -0.5347
2024-12-08 18:42:01.261669: Pseudo dice [0.7741]
2024-12-08 18:42:01.262362: Epoch time: 89.66 s
2024-12-08 18:42:02.504629: 
2024-12-08 18:42:02.506341: Epoch 91
2024-12-08 18:42:02.507159: Current learning rate: 0.00918
2024-12-08 18:43:32.226505: Validation loss did not improve from -0.56386. Patience: 20/50
2024-12-08 18:43:32.227597: train_loss -0.7017
2024-12-08 18:43:32.228366: val_loss -0.4706
2024-12-08 18:43:32.229065: Pseudo dice [0.7445]
2024-12-08 18:43:32.229800: Epoch time: 89.72 s
2024-12-08 18:43:33.837075: 
2024-12-08 18:43:33.838768: Epoch 92
2024-12-08 18:43:33.839482: Current learning rate: 0.00917
2024-12-08 18:45:03.803383: Validation loss did not improve from -0.56386. Patience: 21/50
2024-12-08 18:45:03.804846: train_loss -0.7099
2024-12-08 18:45:03.806145: val_loss -0.4978
2024-12-08 18:45:03.806937: Pseudo dice [0.7511]
2024-12-08 18:45:03.807751: Epoch time: 89.97 s
2024-12-08 18:45:05.021482: 
2024-12-08 18:45:05.023023: Epoch 93
2024-12-08 18:45:05.023791: Current learning rate: 0.00916
2024-12-08 18:46:34.831492: Validation loss did not improve from -0.56386. Patience: 22/50
2024-12-08 18:46:34.832531: train_loss -0.708
2024-12-08 18:46:34.833309: val_loss -0.5309
2024-12-08 18:46:34.834212: Pseudo dice [0.7719]
2024-12-08 18:46:34.834994: Epoch time: 89.81 s
2024-12-08 18:46:36.041502: 
2024-12-08 18:46:36.042936: Epoch 94
2024-12-08 18:46:36.043624: Current learning rate: 0.00915
2024-12-08 18:48:05.787449: Validation loss did not improve from -0.56386. Patience: 23/50
2024-12-08 18:48:05.788311: train_loss -0.7089
2024-12-08 18:48:05.789107: val_loss -0.5116
2024-12-08 18:48:05.789835: Pseudo dice [0.7627]
2024-12-08 18:48:05.790594: Epoch time: 89.75 s
2024-12-08 18:48:07.405135: 
2024-12-08 18:48:07.407263: Epoch 95
2024-12-08 18:48:07.408020: Current learning rate: 0.00914
2024-12-08 18:49:37.116087: Validation loss did not improve from -0.56386. Patience: 24/50
2024-12-08 18:49:37.117267: train_loss -0.7029
2024-12-08 18:49:37.118169: val_loss -0.4802
2024-12-08 18:49:37.118871: Pseudo dice [0.7425]
2024-12-08 18:49:37.119632: Epoch time: 89.71 s
2024-12-08 18:49:38.341434: 
2024-12-08 18:49:38.343156: Epoch 96
2024-12-08 18:49:38.343884: Current learning rate: 0.00913
2024-12-08 18:51:08.067139: Validation loss did not improve from -0.56386. Patience: 25/50
2024-12-08 18:51:08.068133: train_loss -0.7094
2024-12-08 18:51:08.069146: val_loss -0.4402
2024-12-08 18:51:08.069865: Pseudo dice [0.7174]
2024-12-08 18:51:08.070657: Epoch time: 89.73 s
2024-12-08 18:51:09.312958: 
2024-12-08 18:51:09.315110: Epoch 97
2024-12-08 18:51:09.316277: Current learning rate: 0.00912
2024-12-08 18:52:39.086590: Validation loss did not improve from -0.56386. Patience: 26/50
2024-12-08 18:52:39.087977: train_loss -0.7
2024-12-08 18:52:39.088853: val_loss -0.513
2024-12-08 18:52:39.089535: Pseudo dice [0.7572]
2024-12-08 18:52:39.090241: Epoch time: 89.78 s
2024-12-08 18:52:40.357566: 
2024-12-08 18:52:40.359362: Epoch 98
2024-12-08 18:52:40.360145: Current learning rate: 0.00911
2024-12-08 18:54:09.613048: Validation loss did not improve from -0.56386. Patience: 27/50
2024-12-08 18:54:09.614423: train_loss -0.6944
2024-12-08 18:54:09.615694: val_loss -0.521
2024-12-08 18:54:09.616472: Pseudo dice [0.7603]
2024-12-08 18:54:09.617345: Epoch time: 89.26 s
2024-12-08 18:54:10.876032: 
2024-12-08 18:54:10.877745: Epoch 99
2024-12-08 18:54:10.878736: Current learning rate: 0.0091
2024-12-08 18:55:39.445267: Validation loss did not improve from -0.56386. Patience: 28/50
2024-12-08 18:55:39.446437: train_loss -0.7075
2024-12-08 18:55:39.447400: val_loss -0.5312
2024-12-08 18:55:39.448207: Pseudo dice [0.7726]
2024-12-08 18:55:39.448896: Epoch time: 88.57 s
2024-12-08 18:55:41.048167: 
2024-12-08 18:55:41.049687: Epoch 100
2024-12-08 18:55:41.050421: Current learning rate: 0.0091
2024-12-08 18:57:09.641243: Validation loss did not improve from -0.56386. Patience: 29/50
2024-12-08 18:57:09.642570: train_loss -0.71
2024-12-08 18:57:09.643493: val_loss -0.5624
2024-12-08 18:57:09.644319: Pseudo dice [0.7817]
2024-12-08 18:57:09.645073: Epoch time: 88.6 s
2024-12-08 18:57:10.901426: 
2024-12-08 18:57:10.903271: Epoch 101
2024-12-08 18:57:10.904190: Current learning rate: 0.00909
2024-12-08 18:58:39.608499: Validation loss did not improve from -0.56386. Patience: 30/50
2024-12-08 18:58:39.609411: train_loss -0.7125
2024-12-08 18:58:39.610218: val_loss -0.5158
2024-12-08 18:58:39.610960: Pseudo dice [0.7636]
2024-12-08 18:58:39.611635: Epoch time: 88.71 s
2024-12-08 18:58:40.888004: 
2024-12-08 18:58:40.889643: Epoch 102
2024-12-08 18:58:40.890399: Current learning rate: 0.00908
2024-12-08 19:00:09.666493: Validation loss did not improve from -0.56386. Patience: 31/50
2024-12-08 19:00:09.667833: train_loss -0.708
2024-12-08 19:00:09.668817: val_loss -0.5247
2024-12-08 19:00:09.669613: Pseudo dice [0.7655]
2024-12-08 19:00:09.670449: Epoch time: 88.78 s
2024-12-08 19:00:11.284651: 
2024-12-08 19:00:11.286141: Epoch 103
2024-12-08 19:00:11.286777: Current learning rate: 0.00907
2024-12-08 19:01:40.091697: Validation loss did not improve from -0.56386. Patience: 32/50
2024-12-08 19:01:40.092938: train_loss -0.7126
2024-12-08 19:01:40.094281: val_loss -0.4163
2024-12-08 19:01:40.095291: Pseudo dice [0.7191]
2024-12-08 19:01:40.096334: Epoch time: 88.81 s
2024-12-08 19:01:41.305973: 
2024-12-08 19:01:41.307853: Epoch 104
2024-12-08 19:01:41.308843: Current learning rate: 0.00906
2024-12-08 19:03:10.108974: Validation loss did not improve from -0.56386. Patience: 33/50
2024-12-08 19:03:10.110266: train_loss -0.7164
2024-12-08 19:03:10.111227: val_loss -0.537
2024-12-08 19:03:10.112049: Pseudo dice [0.7764]
2024-12-08 19:03:10.112853: Epoch time: 88.81 s
2024-12-08 19:03:11.723604: 
2024-12-08 19:03:11.725815: Epoch 105
2024-12-08 19:03:11.726808: Current learning rate: 0.00905
2024-12-08 19:04:40.434350: Validation loss did not improve from -0.56386. Patience: 34/50
2024-12-08 19:04:40.435606: train_loss -0.7203
2024-12-08 19:04:40.436409: val_loss -0.5501
2024-12-08 19:04:40.437258: Pseudo dice [0.7763]
2024-12-08 19:04:40.437991: Epoch time: 88.71 s
2024-12-08 19:04:41.671332: 
2024-12-08 19:04:41.672991: Epoch 106
2024-12-08 19:04:41.673884: Current learning rate: 0.00904
2024-12-08 19:06:10.308459: Validation loss did not improve from -0.56386. Patience: 35/50
2024-12-08 19:06:10.309415: train_loss -0.7216
2024-12-08 19:06:10.310450: val_loss -0.4524
2024-12-08 19:06:10.311283: Pseudo dice [0.7385]
2024-12-08 19:06:10.312021: Epoch time: 88.64 s
2024-12-08 19:06:11.543732: 
2024-12-08 19:06:11.545402: Epoch 107
2024-12-08 19:06:11.546366: Current learning rate: 0.00903
2024-12-08 19:07:40.230357: Validation loss did not improve from -0.56386. Patience: 36/50
2024-12-08 19:07:40.231318: train_loss -0.7164
2024-12-08 19:07:40.232124: val_loss -0.4735
2024-12-08 19:07:40.232766: Pseudo dice [0.7381]
2024-12-08 19:07:40.233513: Epoch time: 88.69 s
2024-12-08 19:07:41.519425: 
2024-12-08 19:07:41.520839: Epoch 108
2024-12-08 19:07:41.521851: Current learning rate: 0.00902
2024-12-08 19:09:10.111978: Validation loss did not improve from -0.56386. Patience: 37/50
2024-12-08 19:09:10.112732: train_loss -0.7186
2024-12-08 19:09:10.113576: val_loss -0.5375
2024-12-08 19:09:10.114542: Pseudo dice [0.7705]
2024-12-08 19:09:10.115395: Epoch time: 88.59 s
2024-12-08 19:09:11.349827: 
2024-12-08 19:09:11.351187: Epoch 109
2024-12-08 19:09:11.352113: Current learning rate: 0.00901
2024-12-08 19:10:40.053051: Validation loss did not improve from -0.56386. Patience: 38/50
2024-12-08 19:10:40.054358: train_loss -0.7221
2024-12-08 19:10:40.055607: val_loss -0.4528
2024-12-08 19:10:40.056377: Pseudo dice [0.7441]
2024-12-08 19:10:40.057182: Epoch time: 88.71 s
2024-12-08 19:10:41.630684: 
2024-12-08 19:10:41.632810: Epoch 110
2024-12-08 19:10:41.633803: Current learning rate: 0.009
2024-12-08 19:12:10.231052: Validation loss did not improve from -0.56386. Patience: 39/50
2024-12-08 19:12:10.232245: train_loss -0.7156
2024-12-08 19:12:10.233211: val_loss -0.4954
2024-12-08 19:12:10.234238: Pseudo dice [0.7508]
2024-12-08 19:12:10.235553: Epoch time: 88.6 s
2024-12-08 19:12:11.481947: 
2024-12-08 19:12:11.483917: Epoch 111
2024-12-08 19:12:11.484928: Current learning rate: 0.009
2024-12-08 19:13:40.082610: Validation loss did not improve from -0.56386. Patience: 40/50
2024-12-08 19:13:40.083761: train_loss -0.7209
2024-12-08 19:13:40.084663: val_loss -0.4051
2024-12-08 19:13:40.085499: Pseudo dice [0.6944]
2024-12-08 19:13:40.086188: Epoch time: 88.6 s
2024-12-08 19:13:41.402091: 
2024-12-08 19:13:41.403950: Epoch 112
2024-12-08 19:13:41.404942: Current learning rate: 0.00899
2024-12-08 19:15:10.125233: Validation loss did not improve from -0.56386. Patience: 41/50
2024-12-08 19:15:10.126210: train_loss -0.7243
2024-12-08 19:15:10.127060: val_loss -0.5317
2024-12-08 19:15:10.127743: Pseudo dice [0.7765]
2024-12-08 19:15:10.128493: Epoch time: 88.73 s
2024-12-08 19:15:11.343868: 
2024-12-08 19:15:11.345522: Epoch 113
2024-12-08 19:15:11.346326: Current learning rate: 0.00898
2024-12-08 19:16:39.930670: Validation loss did not improve from -0.56386. Patience: 42/50
2024-12-08 19:16:39.931865: train_loss -0.726
2024-12-08 19:16:39.932824: val_loss -0.5177
2024-12-08 19:16:39.933585: Pseudo dice [0.7751]
2024-12-08 19:16:39.934299: Epoch time: 88.59 s
2024-12-08 19:16:41.188156: 
2024-12-08 19:16:41.189599: Epoch 114
2024-12-08 19:16:41.190346: Current learning rate: 0.00897
2024-12-08 19:18:09.716334: Validation loss did not improve from -0.56386. Patience: 43/50
2024-12-08 19:18:09.717375: train_loss -0.7272
2024-12-08 19:18:09.718222: val_loss -0.5211
2024-12-08 19:18:09.718838: Pseudo dice [0.7715]
2024-12-08 19:18:09.719509: Epoch time: 88.53 s
2024-12-08 19:18:11.692530: 
2024-12-08 19:18:11.694628: Epoch 115
2024-12-08 19:18:11.695334: Current learning rate: 0.00896
2024-12-08 19:19:41.367037: Validation loss did not improve from -0.56386. Patience: 44/50
2024-12-08 19:19:41.368038: train_loss -0.7341
2024-12-08 19:19:41.368937: val_loss -0.4527
2024-12-08 19:19:41.369636: Pseudo dice [0.7273]
2024-12-08 19:19:41.370367: Epoch time: 89.68 s
2024-12-08 19:19:42.606201: 
2024-12-08 19:19:42.608142: Epoch 116
2024-12-08 19:19:42.608919: Current learning rate: 0.00895
2024-12-08 19:21:11.085691: Validation loss did not improve from -0.56386. Patience: 45/50
2024-12-08 19:21:11.086704: train_loss -0.729
2024-12-08 19:21:11.087731: val_loss -0.5087
2024-12-08 19:21:11.088640: Pseudo dice [0.7628]
2024-12-08 19:21:11.089416: Epoch time: 88.48 s
2024-12-08 19:21:12.337446: 
2024-12-08 19:21:12.338500: Epoch 117
2024-12-08 19:21:12.339353: Current learning rate: 0.00894
2024-12-08 19:22:40.778894: Validation loss did not improve from -0.56386. Patience: 46/50
2024-12-08 19:22:40.780294: train_loss -0.7281
2024-12-08 19:22:40.781243: val_loss -0.4935
2024-12-08 19:22:40.781916: Pseudo dice [0.7548]
2024-12-08 19:22:40.782610: Epoch time: 88.44 s
2024-12-08 19:22:42.015307: 
2024-12-08 19:22:42.017308: Epoch 118
2024-12-08 19:22:42.018063: Current learning rate: 0.00893
2024-12-08 19:24:10.494733: Validation loss did not improve from -0.56386. Patience: 47/50
2024-12-08 19:24:10.495759: train_loss -0.7279
2024-12-08 19:24:10.496493: val_loss -0.5575
2024-12-08 19:24:10.497271: Pseudo dice [0.7843]
2024-12-08 19:24:10.498060: Epoch time: 88.48 s
2024-12-08 19:24:11.763272: 
2024-12-08 19:24:11.764714: Epoch 119
2024-12-08 19:24:11.765743: Current learning rate: 0.00892
2024-12-08 19:25:41.639529: Validation loss did not improve from -0.56386. Patience: 48/50
2024-12-08 19:25:41.640722: train_loss -0.7214
2024-12-08 19:25:41.641450: val_loss -0.493
2024-12-08 19:25:41.642509: Pseudo dice [0.7499]
2024-12-08 19:25:41.643620: Epoch time: 89.88 s
2024-12-08 19:25:43.244714: 
2024-12-08 19:25:43.246231: Epoch 120
2024-12-08 19:25:43.247121: Current learning rate: 0.00891
2024-12-08 19:27:13.035607: Validation loss did not improve from -0.56386. Patience: 49/50
2024-12-08 19:27:13.036550: train_loss -0.7309
2024-12-08 19:27:13.037349: val_loss -0.522
2024-12-08 19:27:13.038142: Pseudo dice [0.7675]
2024-12-08 19:27:13.038815: Epoch time: 89.79 s
2024-12-08 19:27:14.307160: 
2024-12-08 19:27:14.309023: Epoch 121
2024-12-08 19:27:14.309922: Current learning rate: 0.0089
2024-12-08 19:28:44.069444: Validation loss did not improve from -0.56386. Patience: 50/50
2024-12-08 19:28:44.070464: train_loss -0.7362
2024-12-08 19:28:44.071541: val_loss -0.5255
2024-12-08 19:28:44.072530: Pseudo dice [0.7723]
2024-12-08 19:28:44.073302: Epoch time: 89.76 s
2024-12-08 19:28:45.369574: Patience reached. Stopping training.
2024-12-08 19:28:45.786592: Training done.
2024-12-08 19:28:45.980444: Using splits from existing split file: /home/gridsan/nchutisilp/datasets/nnUNet_Datasets/nnUNet_preprocessed/Dataset309_Sohee_Calcium_OCT_CrossValidation/splits_final.json
2024-12-08 19:28:45.982548: The split file contains 5 splits.
2024-12-08 19:28:45.983548: Desired fold for training: 0
2024-12-08 19:28:45.984242: This split has 6 training and 2 validation cases.
2024-12-08 19:28:45.985253: predicting 106-002
2024-12-08 19:28:45.996172: 106-002, shape torch.Size([1, 539, 498, 498]), rank 0
2024-12-08 19:31:21.965152: predicting 706-005
2024-12-08 19:31:21.993500: 706-005, shape torch.Size([1, 375, 498, 498]), rank 0
2024-12-08 19:33:13.941109: Validation complete
2024-12-08 19:33:13.942480: Mean Validation Dice:  0.7790772881702488
2024-12-08 16:22:22.010113: unpacking done...
2024-12-08 16:22:22.112808: Unable to plot network architecture: nnUNet_compile is enabled!
2024-12-08 16:22:22.802149: 
2024-12-08 16:22:22.803450: Epoch 0
2024-12-08 16:22:22.805207: Current learning rate: 0.01
2024-12-08 16:25:51.796755: Validation loss improved from 1000.00000 to -0.12505! Patience: 0/50
2024-12-08 16:25:51.831478: train_loss -0.1167
2024-12-08 16:25:51.841359: val_loss -0.1251
2024-12-08 16:25:51.842435: Pseudo dice [0.4581]
2024-12-08 16:25:51.843436: Epoch time: 209.0 s
2024-12-08 16:25:51.844421: Yayy! New best EMA pseudo Dice: 0.4581
2024-12-08 16:25:54.407285: 
2024-12-08 16:25:54.408956: Epoch 1
2024-12-08 16:25:54.409892: Current learning rate: 0.00999
2024-12-08 16:27:22.119646: Validation loss improved from -0.12505 to -0.17659! Patience: 0/50
2024-12-08 16:27:22.122012: train_loss -0.2636
2024-12-08 16:27:22.123160: val_loss -0.1766
2024-12-08 16:27:22.124127: Pseudo dice [0.5145]
2024-12-08 16:27:22.124835: Epoch time: 87.72 s
2024-12-08 16:27:22.125743: Yayy! New best EMA pseudo Dice: 0.4637
2024-12-08 16:27:23.713231: 
2024-12-08 16:27:23.715536: Epoch 2
2024-12-08 16:27:23.716882: Current learning rate: 0.00998
2024-12-08 16:28:51.862564: Validation loss improved from -0.17659 to -0.21429! Patience: 0/50
2024-12-08 16:28:51.863629: train_loss -0.311
2024-12-08 16:28:51.864588: val_loss -0.2143
2024-12-08 16:28:51.865383: Pseudo dice [0.5493]
2024-12-08 16:28:51.866184: Epoch time: 88.15 s
2024-12-08 16:28:51.866926: Yayy! New best EMA pseudo Dice: 0.4723
2024-12-08 16:28:53.485434: 
2024-12-08 16:28:53.487542: Epoch 3
2024-12-08 16:28:53.488497: Current learning rate: 0.00997
2024-12-08 16:30:21.553473: Validation loss improved from -0.21429 to -0.25550! Patience: 0/50
2024-12-08 16:30:21.554447: train_loss -0.3514
2024-12-08 16:30:21.555419: val_loss -0.2555
2024-12-08 16:30:21.556162: Pseudo dice [0.5641]
2024-12-08 16:30:21.556873: Epoch time: 88.07 s
2024-12-08 16:30:21.557552: Yayy! New best EMA pseudo Dice: 0.4815
2024-12-08 16:30:23.131986: 
2024-12-08 16:30:23.133416: Epoch 4
2024-12-08 16:30:23.134285: Current learning rate: 0.00996
2024-12-08 16:31:50.769690: Validation loss improved from -0.25550 to -0.30786! Patience: 0/50
2024-12-08 16:31:50.770580: train_loss -0.3671
2024-12-08 16:31:50.771411: val_loss -0.3079
2024-12-08 16:31:50.772214: Pseudo dice [0.6049]
2024-12-08 16:31:50.772993: Epoch time: 87.64 s
2024-12-08 16:31:51.080579: Yayy! New best EMA pseudo Dice: 0.4938
2024-12-08 16:31:52.689901: 
2024-12-08 16:31:52.691529: Epoch 5
2024-12-08 16:31:52.692702: Current learning rate: 0.00995
2024-12-08 16:33:20.538719: Validation loss improved from -0.30786 to -0.33514! Patience: 0/50
2024-12-08 16:33:20.539994: train_loss -0.3964
2024-12-08 16:33:20.540817: val_loss -0.3351
2024-12-08 16:33:20.541566: Pseudo dice [0.6172]
2024-12-08 16:33:20.542319: Epoch time: 87.85 s
2024-12-08 16:33:20.543046: Yayy! New best EMA pseudo Dice: 0.5061
2024-12-08 16:33:22.108794: 
2024-12-08 16:33:22.110801: Epoch 6
2024-12-08 16:33:22.111880: Current learning rate: 0.00995
2024-12-08 16:34:49.709500: Validation loss improved from -0.33514 to -0.35739! Patience: 0/50
2024-12-08 16:34:49.710410: train_loss -0.4077
2024-12-08 16:34:49.711184: val_loss -0.3574
2024-12-08 16:34:49.711919: Pseudo dice [0.627]
2024-12-08 16:34:49.712738: Epoch time: 87.6 s
2024-12-08 16:34:49.713531: Yayy! New best EMA pseudo Dice: 0.5182
2024-12-08 16:34:51.286834: 
2024-12-08 16:34:51.288772: Epoch 7
2024-12-08 16:34:51.289592: Current learning rate: 0.00994
2024-12-08 16:36:18.911485: Validation loss improved from -0.35739 to -0.38606! Patience: 0/50
2024-12-08 16:36:18.912570: train_loss -0.433
2024-12-08 16:36:18.913536: val_loss -0.3861
2024-12-08 16:36:18.914305: Pseudo dice [0.6313]
2024-12-08 16:36:18.915083: Epoch time: 87.63 s
2024-12-08 16:36:18.915891: Yayy! New best EMA pseudo Dice: 0.5295
2024-12-08 16:36:20.883194: 
2024-12-08 16:36:20.884595: Epoch 8
2024-12-08 16:36:20.885714: Current learning rate: 0.00993
2024-12-08 16:37:48.725924: Validation loss did not improve from -0.38606. Patience: 1/50
2024-12-08 16:37:48.727217: train_loss -0.4339
2024-12-08 16:37:48.728802: val_loss -0.3689
2024-12-08 16:37:48.729678: Pseudo dice [0.631]
2024-12-08 16:37:48.730554: Epoch time: 87.85 s
2024-12-08 16:37:48.731548: Yayy! New best EMA pseudo Dice: 0.5397
2024-12-08 16:37:50.367141: 
2024-12-08 16:37:50.369124: Epoch 9
2024-12-08 16:37:50.370069: Current learning rate: 0.00992
2024-12-08 16:39:18.125122: Validation loss did not improve from -0.38606. Patience: 2/50
2024-12-08 16:39:18.126343: train_loss -0.4474
2024-12-08 16:39:18.127376: val_loss -0.3596
2024-12-08 16:39:18.128421: Pseudo dice [0.6296]
2024-12-08 16:39:18.129418: Epoch time: 87.76 s
2024-12-08 16:39:18.466565: Yayy! New best EMA pseudo Dice: 0.5487
2024-12-08 16:39:20.047041: 
2024-12-08 16:39:20.048738: Epoch 10
2024-12-08 16:39:20.049679: Current learning rate: 0.00991
2024-12-08 16:40:48.006356: Validation loss improved from -0.38606 to -0.43408! Patience: 2/50
2024-12-08 16:40:48.007436: train_loss -0.4563
2024-12-08 16:40:48.008264: val_loss -0.4341
2024-12-08 16:40:48.008911: Pseudo dice [0.6742]
2024-12-08 16:40:48.009598: Epoch time: 87.96 s
2024-12-08 16:40:48.010346: Yayy! New best EMA pseudo Dice: 0.5612
2024-12-08 16:40:49.549420: 
2024-12-08 16:40:49.550762: Epoch 11
2024-12-08 16:40:49.551656: Current learning rate: 0.0099
2024-12-08 16:42:17.262761: Validation loss did not improve from -0.43408. Patience: 1/50
2024-12-08 16:42:17.263777: train_loss -0.4747
2024-12-08 16:42:17.264789: val_loss -0.3888
2024-12-08 16:42:17.265566: Pseudo dice [0.6463]
2024-12-08 16:42:17.266380: Epoch time: 87.72 s
2024-12-08 16:42:17.267205: Yayy! New best EMA pseudo Dice: 0.5697
2024-12-08 16:42:18.823130: 
2024-12-08 16:42:18.825085: Epoch 12
2024-12-08 16:42:18.826308: Current learning rate: 0.00989
2024-12-08 16:43:46.616567: Validation loss did not improve from -0.43408. Patience: 2/50
2024-12-08 16:43:46.617539: train_loss -0.491
2024-12-08 16:43:46.618371: val_loss -0.4273
2024-12-08 16:43:46.619084: Pseudo dice [0.6654]
2024-12-08 16:43:46.619856: Epoch time: 87.8 s
2024-12-08 16:43:46.620554: Yayy! New best EMA pseudo Dice: 0.5793
2024-12-08 16:43:48.250716: 
2024-12-08 16:43:48.252393: Epoch 13
2024-12-08 16:43:48.253338: Current learning rate: 0.00988
2024-12-08 16:45:16.090362: Validation loss did not improve from -0.43408. Patience: 3/50
2024-12-08 16:45:16.091429: train_loss -0.4935
2024-12-08 16:45:16.092313: val_loss -0.4123
2024-12-08 16:45:16.093079: Pseudo dice [0.6599]
2024-12-08 16:45:16.093840: Epoch time: 87.84 s
2024-12-08 16:45:16.094563: Yayy! New best EMA pseudo Dice: 0.5874
2024-12-08 16:45:17.692513: 
2024-12-08 16:45:17.693855: Epoch 14
2024-12-08 16:45:17.694972: Current learning rate: 0.00987
2024-12-08 16:46:45.524039: Validation loss did not improve from -0.43408. Patience: 4/50
2024-12-08 16:46:45.525275: train_loss -0.4953
2024-12-08 16:46:45.526148: val_loss -0.4079
2024-12-08 16:46:45.526857: Pseudo dice [0.6537]
2024-12-08 16:46:45.527533: Epoch time: 87.83 s
2024-12-08 16:46:45.867779: Yayy! New best EMA pseudo Dice: 0.594
2024-12-08 16:46:47.473837: 
2024-12-08 16:46:47.475544: Epoch 15
2024-12-08 16:46:47.476459: Current learning rate: 0.00986
2024-12-08 16:48:15.307527: Validation loss did not improve from -0.43408. Patience: 5/50
2024-12-08 16:48:15.308627: train_loss -0.5045
2024-12-08 16:48:15.309531: val_loss -0.3964
2024-12-08 16:48:15.310440: Pseudo dice [0.6498]
2024-12-08 16:48:15.311395: Epoch time: 87.84 s
2024-12-08 16:48:15.312298: Yayy! New best EMA pseudo Dice: 0.5996
2024-12-08 16:48:16.953182: 
2024-12-08 16:48:16.955341: Epoch 16
2024-12-08 16:48:16.956467: Current learning rate: 0.00986
2024-12-08 16:49:44.940372: Validation loss improved from -0.43408 to -0.46449! Patience: 5/50
2024-12-08 16:49:44.941383: train_loss -0.5174
2024-12-08 16:49:44.942341: val_loss -0.4645
2024-12-08 16:49:44.943136: Pseudo dice [0.6871]
2024-12-08 16:49:44.943829: Epoch time: 87.99 s
2024-12-08 16:49:44.944611: Yayy! New best EMA pseudo Dice: 0.6083
2024-12-08 16:49:46.548690: 
2024-12-08 16:49:46.550811: Epoch 17
2024-12-08 16:49:46.551693: Current learning rate: 0.00985
2024-12-08 16:51:14.539886: Validation loss improved from -0.46449 to -0.47605! Patience: 0/50
2024-12-08 16:51:14.541208: train_loss -0.5254
2024-12-08 16:51:14.542202: val_loss -0.4761
2024-12-08 16:51:14.542926: Pseudo dice [0.6983]
2024-12-08 16:51:14.543940: Epoch time: 87.99 s
2024-12-08 16:51:14.544695: Yayy! New best EMA pseudo Dice: 0.6173
2024-12-08 16:51:16.153785: 
2024-12-08 16:51:16.155274: Epoch 18
2024-12-08 16:51:16.156617: Current learning rate: 0.00984
2024-12-08 16:52:44.140658: Validation loss did not improve from -0.47605. Patience: 1/50
2024-12-08 16:52:44.141447: train_loss -0.5109
2024-12-08 16:52:44.142337: val_loss -0.4291
2024-12-08 16:52:44.143109: Pseudo dice [0.6742]
2024-12-08 16:52:44.144045: Epoch time: 87.99 s
2024-12-08 16:52:44.144815: Yayy! New best EMA pseudo Dice: 0.623
2024-12-08 16:52:46.106986: 
2024-12-08 16:52:46.108464: Epoch 19
2024-12-08 16:52:46.109934: Current learning rate: 0.00983
2024-12-08 16:54:14.238884: Validation loss did not improve from -0.47605. Patience: 2/50
2024-12-08 16:54:14.240249: train_loss -0.5222
2024-12-08 16:54:14.241203: val_loss -0.475
2024-12-08 16:54:14.242133: Pseudo dice [0.6959]
2024-12-08 16:54:14.242970: Epoch time: 88.13 s
2024-12-08 16:54:14.584905: Yayy! New best EMA pseudo Dice: 0.6303
2024-12-08 16:54:16.214733: 
2024-12-08 16:54:16.216356: Epoch 20
2024-12-08 16:54:16.217311: Current learning rate: 0.00982
2024-12-08 16:55:44.344591: Validation loss did not improve from -0.47605. Patience: 3/50
2024-12-08 16:55:44.345615: train_loss -0.5394
2024-12-08 16:55:44.346655: val_loss -0.4281
2024-12-08 16:55:44.347473: Pseudo dice [0.6688]
2024-12-08 16:55:44.348126: Epoch time: 88.13 s
2024-12-08 16:55:44.348873: Yayy! New best EMA pseudo Dice: 0.6342
2024-12-08 16:55:45.983523: 
2024-12-08 16:55:45.985229: Epoch 21
2024-12-08 16:55:45.986099: Current learning rate: 0.00981
2024-12-08 16:57:13.832518: Validation loss improved from -0.47605 to -0.48128! Patience: 3/50
2024-12-08 16:57:13.833699: train_loss -0.5382
2024-12-08 16:57:13.834590: val_loss -0.4813
2024-12-08 16:57:13.835253: Pseudo dice [0.7048]
2024-12-08 16:57:13.835966: Epoch time: 87.85 s
2024-12-08 16:57:13.836621: Yayy! New best EMA pseudo Dice: 0.6412
2024-12-08 16:57:15.369040: 
2024-12-08 16:57:15.370532: Epoch 22
2024-12-08 16:57:15.371671: Current learning rate: 0.0098
2024-12-08 16:58:43.204679: Validation loss did not improve from -0.48128. Patience: 1/50
2024-12-08 16:58:43.205493: train_loss -0.5444
2024-12-08 16:58:43.206330: val_loss -0.4685
2024-12-08 16:58:43.207056: Pseudo dice [0.7016]
2024-12-08 16:58:43.207853: Epoch time: 87.84 s
2024-12-08 16:58:43.208633: Yayy! New best EMA pseudo Dice: 0.6473
2024-12-08 16:58:44.725087: 
2024-12-08 16:58:44.727075: Epoch 23
2024-12-08 16:58:44.728011: Current learning rate: 0.00979
2024-12-08 17:00:12.607089: Validation loss improved from -0.48128 to -0.51224! Patience: 1/50
2024-12-08 17:00:12.608245: train_loss -0.5574
2024-12-08 17:00:12.609232: val_loss -0.5122
2024-12-08 17:00:12.610071: Pseudo dice [0.7227]
2024-12-08 17:00:12.610777: Epoch time: 87.88 s
2024-12-08 17:00:12.611508: Yayy! New best EMA pseudo Dice: 0.6548
2024-12-08 17:00:14.170210: 
2024-12-08 17:00:14.171915: Epoch 24
2024-12-08 17:00:14.172962: Current learning rate: 0.00978
2024-12-08 17:01:42.222565: Validation loss did not improve from -0.51224. Patience: 1/50
2024-12-08 17:01:42.223862: train_loss -0.5437
2024-12-08 17:01:42.225335: val_loss -0.4804
2024-12-08 17:01:42.226213: Pseudo dice [0.6917]
2024-12-08 17:01:42.227024: Epoch time: 88.05 s
2024-12-08 17:01:42.582597: Yayy! New best EMA pseudo Dice: 0.6585
2024-12-08 17:01:44.148490: 
2024-12-08 17:01:44.150230: Epoch 25
2024-12-08 17:01:44.150990: Current learning rate: 0.00977
2024-12-08 17:03:12.436874: Validation loss did not improve from -0.51224. Patience: 2/50
2024-12-08 17:03:12.437975: train_loss -0.5474
2024-12-08 17:03:12.438899: val_loss -0.4885
2024-12-08 17:03:12.439790: Pseudo dice [0.7064]
2024-12-08 17:03:12.440500: Epoch time: 88.29 s
2024-12-08 17:03:12.441307: Yayy! New best EMA pseudo Dice: 0.6633
2024-12-08 17:03:13.998187: 
2024-12-08 17:03:13.999931: Epoch 26
2024-12-08 17:03:14.001024: Current learning rate: 0.00977
2024-12-08 17:04:41.879460: Validation loss did not improve from -0.51224. Patience: 3/50
2024-12-08 17:04:41.880693: train_loss -0.5607
2024-12-08 17:04:41.881636: val_loss -0.4947
2024-12-08 17:04:41.882456: Pseudo dice [0.7083]
2024-12-08 17:04:41.883204: Epoch time: 87.88 s
2024-12-08 17:04:41.884239: Yayy! New best EMA pseudo Dice: 0.6678
2024-12-08 17:04:43.420732: 
2024-12-08 17:04:43.422012: Epoch 27
2024-12-08 17:04:43.423027: Current learning rate: 0.00976
2024-12-08 17:06:11.243761: Validation loss did not improve from -0.51224. Patience: 4/50
2024-12-08 17:06:11.245154: train_loss -0.5598
2024-12-08 17:06:11.246245: val_loss -0.4247
2024-12-08 17:06:11.247050: Pseudo dice [0.6773]
2024-12-08 17:06:11.247817: Epoch time: 87.83 s
2024-12-08 17:06:11.248596: Yayy! New best EMA pseudo Dice: 0.6687
2024-12-08 17:06:12.822591: 
2024-12-08 17:06:12.824159: Epoch 28
2024-12-08 17:06:12.825229: Current learning rate: 0.00975
2024-12-08 17:07:40.714632: Validation loss did not improve from -0.51224. Patience: 5/50
2024-12-08 17:07:40.715675: train_loss -0.5556
2024-12-08 17:07:40.716596: val_loss -0.4902
2024-12-08 17:07:40.717351: Pseudo dice [0.7096]
2024-12-08 17:07:40.718053: Epoch time: 87.89 s
2024-12-08 17:07:40.718719: Yayy! New best EMA pseudo Dice: 0.6728
2024-12-08 17:07:42.584028: 
2024-12-08 17:07:42.585855: Epoch 29
2024-12-08 17:07:42.586908: Current learning rate: 0.00974
2024-12-08 17:09:10.662591: Validation loss did not improve from -0.51224. Patience: 6/50
2024-12-08 17:09:10.663691: train_loss -0.5667
2024-12-08 17:09:10.664678: val_loss -0.4668
2024-12-08 17:09:10.665491: Pseudo dice [0.6946]
2024-12-08 17:09:10.666274: Epoch time: 88.08 s
2024-12-08 17:09:11.014768: Yayy! New best EMA pseudo Dice: 0.675
2024-12-08 17:09:12.588610: 
2024-12-08 17:09:12.590313: Epoch 30
2024-12-08 17:09:12.591306: Current learning rate: 0.00973
2024-12-08 17:10:40.431913: Validation loss did not improve from -0.51224. Patience: 7/50
2024-12-08 17:10:40.433159: train_loss -0.5682
2024-12-08 17:10:40.434067: val_loss -0.4974
2024-12-08 17:10:40.434942: Pseudo dice [0.7068]
2024-12-08 17:10:40.435868: Epoch time: 87.85 s
2024-12-08 17:10:40.436563: Yayy! New best EMA pseudo Dice: 0.6782
2024-12-08 17:10:41.976445: 
2024-12-08 17:10:41.978157: Epoch 31
2024-12-08 17:10:41.978880: Current learning rate: 0.00972
2024-12-08 17:12:09.718910: Validation loss did not improve from -0.51224. Patience: 8/50
2024-12-08 17:12:09.719725: train_loss -0.5871
2024-12-08 17:12:09.720521: val_loss -0.4936
2024-12-08 17:12:09.721260: Pseudo dice [0.7056]
2024-12-08 17:12:09.722104: Epoch time: 87.74 s
2024-12-08 17:12:09.722949: Yayy! New best EMA pseudo Dice: 0.6809
2024-12-08 17:12:11.314850: 
2024-12-08 17:12:11.316335: Epoch 32
2024-12-08 17:12:11.317461: Current learning rate: 0.00971
2024-12-08 17:13:39.056465: Validation loss did not improve from -0.51224. Patience: 9/50
2024-12-08 17:13:39.057482: train_loss -0.5806
2024-12-08 17:13:39.058282: val_loss -0.4751
2024-12-08 17:13:39.059064: Pseudo dice [0.7081]
2024-12-08 17:13:39.059955: Epoch time: 87.74 s
2024-12-08 17:13:39.060667: Yayy! New best EMA pseudo Dice: 0.6836
2024-12-08 17:13:40.668110: 
2024-12-08 17:13:40.670081: Epoch 33
2024-12-08 17:13:40.670849: Current learning rate: 0.0097
2024-12-08 17:15:09.093021: Validation loss did not improve from -0.51224. Patience: 10/50
2024-12-08 17:15:09.094129: train_loss -0.5808
2024-12-08 17:15:09.095149: val_loss -0.5044
2024-12-08 17:15:09.095904: Pseudo dice [0.7198]
2024-12-08 17:15:09.096748: Epoch time: 88.43 s
2024-12-08 17:15:09.097566: Yayy! New best EMA pseudo Dice: 0.6873
2024-12-08 17:15:10.652084: 
2024-12-08 17:15:10.654276: Epoch 34
2024-12-08 17:15:10.655160: Current learning rate: 0.00969
2024-12-08 17:16:38.945942: Validation loss improved from -0.51224 to -0.51449! Patience: 10/50
2024-12-08 17:16:38.947405: train_loss -0.5894
2024-12-08 17:16:38.948436: val_loss -0.5145
2024-12-08 17:16:38.949217: Pseudo dice [0.7185]
2024-12-08 17:16:38.950009: Epoch time: 88.3 s
2024-12-08 17:16:39.280435: Yayy! New best EMA pseudo Dice: 0.6904
2024-12-08 17:16:40.850341: 
2024-12-08 17:16:40.852272: Epoch 35
2024-12-08 17:16:40.853148: Current learning rate: 0.00968
2024-12-08 17:18:09.133732: Validation loss did not improve from -0.51449. Patience: 1/50
2024-12-08 17:18:09.134711: train_loss -0.594
2024-12-08 17:18:09.135662: val_loss -0.5011
2024-12-08 17:18:09.136331: Pseudo dice [0.715]
2024-12-08 17:18:09.137067: Epoch time: 88.29 s
2024-12-08 17:18:09.137675: Yayy! New best EMA pseudo Dice: 0.6928
2024-12-08 17:18:10.746895: 
2024-12-08 17:18:10.748629: Epoch 36
2024-12-08 17:18:10.749702: Current learning rate: 0.00968
2024-12-08 17:19:39.054698: Validation loss did not improve from -0.51449. Patience: 2/50
2024-12-08 17:19:39.055616: train_loss -0.6016
2024-12-08 17:19:39.056848: val_loss -0.4868
2024-12-08 17:19:39.057739: Pseudo dice [0.709]
2024-12-08 17:19:39.058421: Epoch time: 88.31 s
2024-12-08 17:19:39.059323: Yayy! New best EMA pseudo Dice: 0.6945
2024-12-08 17:19:40.833239: 
2024-12-08 17:19:40.834678: Epoch 37
2024-12-08 17:19:40.835618: Current learning rate: 0.00967
2024-12-08 17:21:09.126979: Validation loss did not improve from -0.51449. Patience: 3/50
2024-12-08 17:21:09.128007: train_loss -0.6088
2024-12-08 17:21:09.128989: val_loss -0.47
2024-12-08 17:21:09.129905: Pseudo dice [0.6911]
2024-12-08 17:21:09.130600: Epoch time: 88.3 s
2024-12-08 17:21:10.431960: 
2024-12-08 17:21:10.433414: Epoch 38
2024-12-08 17:21:10.434158: Current learning rate: 0.00966
2024-12-08 17:22:38.805797: Validation loss improved from -0.51449 to -0.53298! Patience: 3/50
2024-12-08 17:22:38.806897: train_loss -0.6026
2024-12-08 17:22:38.807807: val_loss -0.533
2024-12-08 17:22:38.808507: Pseudo dice [0.7287]
2024-12-08 17:22:38.809309: Epoch time: 88.38 s
2024-12-08 17:22:38.809987: Yayy! New best EMA pseudo Dice: 0.6976
2024-12-08 17:22:40.393370: 
2024-12-08 17:22:40.395026: Epoch 39
2024-12-08 17:22:40.396214: Current learning rate: 0.00965
2024-12-08 17:24:08.667906: Validation loss did not improve from -0.53298. Patience: 1/50
2024-12-08 17:24:08.668812: train_loss -0.613
2024-12-08 17:24:08.669900: val_loss -0.4889
2024-12-08 17:24:08.670563: Pseudo dice [0.6992]
2024-12-08 17:24:08.671309: Epoch time: 88.28 s
2024-12-08 17:24:09.344491: Yayy! New best EMA pseudo Dice: 0.6977
2024-12-08 17:24:10.950168: 
2024-12-08 17:24:10.951537: Epoch 40
2024-12-08 17:24:10.952629: Current learning rate: 0.00964
2024-12-08 17:25:39.174792: Validation loss did not improve from -0.53298. Patience: 2/50
2024-12-08 17:25:39.175939: train_loss -0.607
2024-12-08 17:25:39.177377: val_loss -0.5265
2024-12-08 17:25:39.178318: Pseudo dice [0.7305]
2024-12-08 17:25:39.179339: Epoch time: 88.23 s
2024-12-08 17:25:39.180610: Yayy! New best EMA pseudo Dice: 0.701
2024-12-08 17:25:40.809379: 
2024-12-08 17:25:40.810879: Epoch 41
2024-12-08 17:25:40.811786: Current learning rate: 0.00963
2024-12-08 17:27:09.046550: Validation loss did not improve from -0.53298. Patience: 3/50
2024-12-08 17:27:09.047772: train_loss -0.6072
2024-12-08 17:27:09.049663: val_loss -0.4809
2024-12-08 17:27:09.050669: Pseudo dice [0.7024]
2024-12-08 17:27:09.052050: Epoch time: 88.24 s
2024-12-08 17:27:09.052895: Yayy! New best EMA pseudo Dice: 0.7012
2024-12-08 17:27:10.599678: 
2024-12-08 17:27:10.601477: Epoch 42
2024-12-08 17:27:10.602697: Current learning rate: 0.00962
2024-12-08 17:28:38.678552: Validation loss did not improve from -0.53298. Patience: 4/50
2024-12-08 17:28:38.680221: train_loss -0.6071
2024-12-08 17:28:38.681118: val_loss -0.5168
2024-12-08 17:28:38.681974: Pseudo dice [0.7149]
2024-12-08 17:28:38.682720: Epoch time: 88.08 s
2024-12-08 17:28:38.683421: Yayy! New best EMA pseudo Dice: 0.7025
2024-12-08 17:28:40.272361: 
2024-12-08 17:28:40.274144: Epoch 43
2024-12-08 17:28:40.275069: Current learning rate: 0.00961
2024-12-08 17:30:08.357808: Validation loss did not improve from -0.53298. Patience: 5/50
2024-12-08 17:30:08.358942: train_loss -0.6012
2024-12-08 17:30:08.359984: val_loss -0.5071
2024-12-08 17:30:08.360662: Pseudo dice [0.7125]
2024-12-08 17:30:08.361580: Epoch time: 88.09 s
2024-12-08 17:30:08.362315: Yayy! New best EMA pseudo Dice: 0.7035
2024-12-08 17:30:09.918013: 
2024-12-08 17:30:09.919947: Epoch 44
2024-12-08 17:30:09.921128: Current learning rate: 0.0096
2024-12-08 17:31:38.012464: Validation loss did not improve from -0.53298. Patience: 6/50
2024-12-08 17:31:38.013637: train_loss -0.6185
2024-12-08 17:31:38.014697: val_loss -0.5032
2024-12-08 17:31:38.015463: Pseudo dice [0.7109]
2024-12-08 17:31:38.016336: Epoch time: 88.1 s
2024-12-08 17:31:38.331143: Yayy! New best EMA pseudo Dice: 0.7043
2024-12-08 17:31:39.908525: 
2024-12-08 17:31:39.910320: Epoch 45
2024-12-08 17:31:39.911077: Current learning rate: 0.00959
2024-12-08 17:33:08.005352: Validation loss did not improve from -0.53298. Patience: 7/50
2024-12-08 17:33:08.006619: train_loss -0.6235
2024-12-08 17:33:08.007617: val_loss -0.4512
2024-12-08 17:33:08.008569: Pseudo dice [0.6885]
2024-12-08 17:33:08.009336: Epoch time: 88.1 s
2024-12-08 17:33:09.252529: 
2024-12-08 17:33:09.254136: Epoch 46
2024-12-08 17:33:09.254927: Current learning rate: 0.00959
2024-12-08 17:34:37.253531: Validation loss did not improve from -0.53298. Patience: 8/50
2024-12-08 17:34:37.254685: train_loss -0.6354
2024-12-08 17:34:37.255613: val_loss -0.4994
2024-12-08 17:34:37.256414: Pseudo dice [0.7135]
2024-12-08 17:34:37.257171: Epoch time: 88.0 s
2024-12-08 17:34:38.459570: 
2024-12-08 17:34:38.460900: Epoch 47
2024-12-08 17:34:38.461722: Current learning rate: 0.00958
2024-12-08 17:36:06.559134: Validation loss did not improve from -0.53298. Patience: 9/50
2024-12-08 17:36:06.560349: train_loss -0.6255
2024-12-08 17:36:06.561215: val_loss -0.5046
2024-12-08 17:36:06.561883: Pseudo dice [0.7103]
2024-12-08 17:36:06.562643: Epoch time: 88.1 s
2024-12-08 17:36:06.563295: Yayy! New best EMA pseudo Dice: 0.7044
2024-12-08 17:36:08.106768: 
2024-12-08 17:36:08.108299: Epoch 48
2024-12-08 17:36:08.109374: Current learning rate: 0.00957
2024-12-08 17:37:36.164782: Validation loss did not improve from -0.53298. Patience: 10/50
2024-12-08 17:37:36.165858: train_loss -0.6343
2024-12-08 17:37:36.166882: val_loss -0.5196
2024-12-08 17:37:36.167626: Pseudo dice [0.726]
2024-12-08 17:37:36.168367: Epoch time: 88.06 s
2024-12-08 17:37:36.169172: Yayy! New best EMA pseudo Dice: 0.7066
2024-12-08 17:37:37.755879: 
2024-12-08 17:37:37.757673: Epoch 49
2024-12-08 17:37:37.758909: Current learning rate: 0.00956
2024-12-08 17:39:05.877743: Validation loss did not improve from -0.53298. Patience: 11/50
2024-12-08 17:39:05.878845: train_loss -0.6378
2024-12-08 17:39:05.879829: val_loss -0.5202
2024-12-08 17:39:05.880545: Pseudo dice [0.7276]
2024-12-08 17:39:05.881324: Epoch time: 88.12 s
2024-12-08 17:39:06.221817: Yayy! New best EMA pseudo Dice: 0.7087
2024-12-08 17:39:08.298222: 
2024-12-08 17:39:08.299913: Epoch 50
2024-12-08 17:39:08.300689: Current learning rate: 0.00955
2024-12-08 17:40:36.556138: Validation loss did not improve from -0.53298. Patience: 12/50
2024-12-08 17:40:36.557624: train_loss -0.6335
2024-12-08 17:40:36.558765: val_loss -0.4906
2024-12-08 17:40:36.559543: Pseudo dice [0.7128]
2024-12-08 17:40:36.560304: Epoch time: 88.26 s
2024-12-08 17:40:36.560985: Yayy! New best EMA pseudo Dice: 0.7091
2024-12-08 17:40:38.126306: 
2024-12-08 17:40:38.127680: Epoch 51
2024-12-08 17:40:38.128444: Current learning rate: 0.00954
2024-12-08 17:42:06.457450: Validation loss did not improve from -0.53298. Patience: 13/50
2024-12-08 17:42:06.458667: train_loss -0.6248
2024-12-08 17:42:06.459698: val_loss -0.5234
2024-12-08 17:42:06.460410: Pseudo dice [0.737]
2024-12-08 17:42:06.461175: Epoch time: 88.33 s
2024-12-08 17:42:06.461921: Yayy! New best EMA pseudo Dice: 0.7119
2024-12-08 17:42:08.046992: 
2024-12-08 17:42:08.048956: Epoch 52
2024-12-08 17:42:08.050227: Current learning rate: 0.00953
2024-12-08 17:43:36.293080: Validation loss did not improve from -0.53298. Patience: 14/50
2024-12-08 17:43:36.293935: train_loss -0.6331
2024-12-08 17:43:36.294826: val_loss -0.5295
2024-12-08 17:43:36.295595: Pseudo dice [0.7333]
2024-12-08 17:43:36.296322: Epoch time: 88.25 s
2024-12-08 17:43:36.297044: Yayy! New best EMA pseudo Dice: 0.714
2024-12-08 17:43:37.900482: 
2024-12-08 17:43:37.902178: Epoch 53
2024-12-08 17:43:37.903127: Current learning rate: 0.00952
2024-12-08 17:45:06.243140: Validation loss improved from -0.53298 to -0.54907! Patience: 14/50
2024-12-08 17:45:06.244326: train_loss -0.6362
2024-12-08 17:45:06.245216: val_loss -0.5491
2024-12-08 17:45:06.245958: Pseudo dice [0.7425]
2024-12-08 17:45:06.246666: Epoch time: 88.34 s
2024-12-08 17:45:06.247301: Yayy! New best EMA pseudo Dice: 0.7169
2024-12-08 17:45:07.828140: 
2024-12-08 17:45:07.829917: Epoch 54
2024-12-08 17:45:07.830704: Current learning rate: 0.00951
2024-12-08 17:46:36.129329: Validation loss did not improve from -0.54907. Patience: 1/50
2024-12-08 17:46:36.130419: train_loss -0.6344
2024-12-08 17:46:36.131211: val_loss -0.4902
2024-12-08 17:46:36.132023: Pseudo dice [0.715]
2024-12-08 17:46:36.132654: Epoch time: 88.3 s
2024-12-08 17:46:37.726164: 
2024-12-08 17:46:37.728404: Epoch 55
2024-12-08 17:46:37.729383: Current learning rate: 0.0095
2024-12-08 17:48:06.095284: Validation loss did not improve from -0.54907. Patience: 2/50
2024-12-08 17:48:06.096481: train_loss -0.6405
2024-12-08 17:48:06.097351: val_loss -0.5316
2024-12-08 17:48:06.098062: Pseudo dice [0.7264]
2024-12-08 17:48:06.098845: Epoch time: 88.37 s
2024-12-08 17:48:06.099613: Yayy! New best EMA pseudo Dice: 0.7177
2024-12-08 17:48:07.674492: 
2024-12-08 17:48:07.676260: Epoch 56
2024-12-08 17:48:07.677210: Current learning rate: 0.00949
2024-12-08 17:49:35.934812: Validation loss did not improve from -0.54907. Patience: 3/50
2024-12-08 17:49:35.935730: train_loss -0.6377
2024-12-08 17:49:35.936501: val_loss -0.5069
2024-12-08 17:49:35.937274: Pseudo dice [0.7257]
2024-12-08 17:49:35.938107: Epoch time: 88.26 s
2024-12-08 17:49:35.938723: Yayy! New best EMA pseudo Dice: 0.7185
2024-12-08 17:49:37.549926: 
2024-12-08 17:49:37.551532: Epoch 57
2024-12-08 17:49:37.552361: Current learning rate: 0.00949
2024-12-08 17:51:05.728989: Validation loss did not improve from -0.54907. Patience: 4/50
2024-12-08 17:51:05.730366: train_loss -0.6217
2024-12-08 17:51:05.731494: val_loss -0.4633
2024-12-08 17:51:05.732211: Pseudo dice [0.6985]
2024-12-08 17:51:05.732966: Epoch time: 88.18 s
2024-12-08 17:51:06.957331: 
2024-12-08 17:51:06.958974: Epoch 58
2024-12-08 17:51:06.959865: Current learning rate: 0.00948
2024-12-08 17:52:34.962451: Validation loss did not improve from -0.54907. Patience: 5/50
2024-12-08 17:52:34.963624: train_loss -0.6428
2024-12-08 17:52:34.964542: val_loss -0.4856
2024-12-08 17:52:34.965370: Pseudo dice [0.7035]
2024-12-08 17:52:34.966306: Epoch time: 88.01 s
2024-12-08 17:52:36.237761: 
2024-12-08 17:52:36.239865: Epoch 59
2024-12-08 17:52:36.240745: Current learning rate: 0.00947
2024-12-08 17:54:04.262191: Validation loss did not improve from -0.54907. Patience: 6/50
2024-12-08 17:54:04.263489: train_loss -0.6491
2024-12-08 17:54:04.264775: val_loss -0.5335
2024-12-08 17:54:04.265533: Pseudo dice [0.7371]
2024-12-08 17:54:04.266245: Epoch time: 88.03 s
2024-12-08 17:54:05.889070: 
2024-12-08 17:54:05.890831: Epoch 60
2024-12-08 17:54:05.892231: Current learning rate: 0.00946
2024-12-08 17:55:33.925967: Validation loss did not improve from -0.54907. Patience: 7/50
2024-12-08 17:55:33.927191: train_loss -0.6523
2024-12-08 17:55:33.928308: val_loss -0.5047
2024-12-08 17:55:33.929223: Pseudo dice [0.7117]
2024-12-08 17:55:33.930226: Epoch time: 88.04 s
2024-12-08 17:55:35.542959: 
2024-12-08 17:55:35.544990: Epoch 61
2024-12-08 17:55:35.545888: Current learning rate: 0.00945
2024-12-08 17:57:03.482259: Validation loss improved from -0.54907 to -0.56662! Patience: 7/50
2024-12-08 17:57:03.483475: train_loss -0.6475
2024-12-08 17:57:03.484388: val_loss -0.5666
2024-12-08 17:57:03.485174: Pseudo dice [0.7484]
2024-12-08 17:57:03.485936: Epoch time: 87.94 s
2024-12-08 17:57:03.486732: Yayy! New best EMA pseudo Dice: 0.72
2024-12-08 17:57:05.100644: 
2024-12-08 17:57:05.102294: Epoch 62
2024-12-08 17:57:05.103165: Current learning rate: 0.00944
2024-12-08 17:58:33.035601: Validation loss did not improve from -0.56662. Patience: 1/50
2024-12-08 17:58:33.036649: train_loss -0.6521
2024-12-08 17:58:33.037583: val_loss -0.5302
2024-12-08 17:58:33.038357: Pseudo dice [0.7257]
2024-12-08 17:58:33.039129: Epoch time: 87.94 s
2024-12-08 17:58:33.039910: Yayy! New best EMA pseudo Dice: 0.7205
2024-12-08 17:58:34.660466: 
2024-12-08 17:58:34.662229: Epoch 63
2024-12-08 17:58:34.662955: Current learning rate: 0.00943
2024-12-08 18:00:02.691077: Validation loss did not improve from -0.56662. Patience: 2/50
2024-12-08 18:00:02.692472: train_loss -0.649
2024-12-08 18:00:02.693368: val_loss -0.517
2024-12-08 18:00:02.694225: Pseudo dice [0.7201]
2024-12-08 18:00:02.694975: Epoch time: 88.03 s
2024-12-08 18:00:03.968508: 
2024-12-08 18:00:03.969619: Epoch 64
2024-12-08 18:00:03.970425: Current learning rate: 0.00942
2024-12-08 18:01:31.777631: Validation loss did not improve from -0.56662. Patience: 3/50
2024-12-08 18:01:31.778853: train_loss -0.6593
2024-12-08 18:01:31.779959: val_loss -0.5368
2024-12-08 18:01:31.780777: Pseudo dice [0.7357]
2024-12-08 18:01:31.781604: Epoch time: 87.81 s
2024-12-08 18:01:32.125550: Yayy! New best EMA pseudo Dice: 0.722
2024-12-08 18:01:33.735054: 
2024-12-08 18:01:33.737055: Epoch 65
2024-12-08 18:01:33.738338: Current learning rate: 0.00941
2024-12-08 18:03:01.860708: Validation loss did not improve from -0.56662. Patience: 4/50
2024-12-08 18:03:01.861702: train_loss -0.6555
2024-12-08 18:03:01.862802: val_loss -0.5278
2024-12-08 18:03:01.863476: Pseudo dice [0.7224]
2024-12-08 18:03:01.864232: Epoch time: 88.13 s
2024-12-08 18:03:01.864870: Yayy! New best EMA pseudo Dice: 0.7221
2024-12-08 18:03:03.453892: 
2024-12-08 18:03:03.455856: Epoch 66
2024-12-08 18:03:03.456869: Current learning rate: 0.0094
2024-12-08 18:04:31.653663: Validation loss did not improve from -0.56662. Patience: 5/50
2024-12-08 18:04:31.654898: train_loss -0.6639
2024-12-08 18:04:31.655754: val_loss -0.5541
2024-12-08 18:04:31.656436: Pseudo dice [0.7455]
2024-12-08 18:04:31.657151: Epoch time: 88.2 s
2024-12-08 18:04:31.657819: Yayy! New best EMA pseudo Dice: 0.7244
2024-12-08 18:04:33.241739: 
2024-12-08 18:04:33.243423: Epoch 67
2024-12-08 18:04:33.244094: Current learning rate: 0.00939
2024-12-08 18:06:01.230932: Validation loss did not improve from -0.56662. Patience: 6/50
2024-12-08 18:06:01.232241: train_loss -0.6689
2024-12-08 18:06:01.233648: val_loss -0.5472
2024-12-08 18:06:01.234367: Pseudo dice [0.733]
2024-12-08 18:06:01.235064: Epoch time: 87.99 s
2024-12-08 18:06:01.236107: Yayy! New best EMA pseudo Dice: 0.7253
2024-12-08 18:06:02.846426: 
2024-12-08 18:06:02.848379: Epoch 68
2024-12-08 18:06:02.849576: Current learning rate: 0.00939
2024-12-08 18:07:31.043859: Validation loss did not improve from -0.56662. Patience: 7/50
2024-12-08 18:07:31.045148: train_loss -0.6622
2024-12-08 18:07:31.046126: val_loss -0.4983
2024-12-08 18:07:31.046888: Pseudo dice [0.7165]
2024-12-08 18:07:31.047773: Epoch time: 88.2 s
2024-12-08 18:07:32.313089: 
2024-12-08 18:07:32.314275: Epoch 69
2024-12-08 18:07:32.315016: Current learning rate: 0.00938
2024-12-08 18:09:00.422250: Validation loss did not improve from -0.56662. Patience: 8/50
2024-12-08 18:09:00.423401: train_loss -0.6681
2024-12-08 18:09:00.424559: val_loss -0.5387
2024-12-08 18:09:00.425547: Pseudo dice [0.734]
2024-12-08 18:09:00.426419: Epoch time: 88.11 s
2024-12-08 18:09:00.800155: Yayy! New best EMA pseudo Dice: 0.7253
2024-12-08 18:09:02.438333: 
2024-12-08 18:09:02.440039: Epoch 70
2024-12-08 18:09:02.441410: Current learning rate: 0.00937
2024-12-08 18:10:30.559933: Validation loss improved from -0.56662 to -0.56848! Patience: 8/50
2024-12-08 18:10:30.560939: train_loss -0.6614
2024-12-08 18:10:30.562166: val_loss -0.5685
2024-12-08 18:10:30.562934: Pseudo dice [0.7521]
2024-12-08 18:10:30.563622: Epoch time: 88.12 s
2024-12-08 18:10:30.564327: Yayy! New best EMA pseudo Dice: 0.728
2024-12-08 18:10:32.510623: 
2024-12-08 18:10:32.511939: Epoch 71
2024-12-08 18:10:32.512764: Current learning rate: 0.00936
2024-12-08 18:12:00.385964: Validation loss did not improve from -0.56848. Patience: 1/50
2024-12-08 18:12:00.387006: train_loss -0.6698
2024-12-08 18:12:00.387977: val_loss -0.5428
2024-12-08 18:12:00.388863: Pseudo dice [0.7423]
2024-12-08 18:12:00.389716: Epoch time: 87.88 s
2024-12-08 18:12:00.390475: Yayy! New best EMA pseudo Dice: 0.7294
2024-12-08 18:12:02.053662: 
2024-12-08 18:12:02.055785: Epoch 72
2024-12-08 18:12:02.057017: Current learning rate: 0.00935
2024-12-08 18:13:29.756811: Validation loss did not improve from -0.56848. Patience: 2/50
2024-12-08 18:13:29.757816: train_loss -0.6721
2024-12-08 18:13:29.758671: val_loss -0.513
2024-12-08 18:13:29.759365: Pseudo dice [0.7231]
2024-12-08 18:13:29.760207: Epoch time: 87.71 s
2024-12-08 18:13:31.061228: 
2024-12-08 18:13:31.063001: Epoch 73
2024-12-08 18:13:31.063857: Current learning rate: 0.00934
2024-12-08 18:14:58.546558: Validation loss did not improve from -0.56848. Patience: 3/50
2024-12-08 18:14:58.547611: train_loss -0.6697
2024-12-08 18:14:58.548396: val_loss -0.5174
2024-12-08 18:14:58.549121: Pseudo dice [0.7335]
2024-12-08 18:14:58.549872: Epoch time: 87.49 s
2024-12-08 18:14:59.826743: 
2024-12-08 18:14:59.828552: Epoch 74
2024-12-08 18:14:59.829385: Current learning rate: 0.00933
2024-12-08 18:16:27.344897: Validation loss did not improve from -0.56848. Patience: 4/50
2024-12-08 18:16:27.346370: train_loss -0.6702
2024-12-08 18:16:27.347447: val_loss -0.529
2024-12-08 18:16:27.348222: Pseudo dice [0.7289]
2024-12-08 18:16:27.348906: Epoch time: 87.52 s
2024-12-08 18:16:28.990928: 
2024-12-08 18:16:28.992789: Epoch 75
2024-12-08 18:16:28.993685: Current learning rate: 0.00932
2024-12-08 18:17:56.801697: Validation loss did not improve from -0.56848. Patience: 5/50
2024-12-08 18:17:56.803053: train_loss -0.6726
2024-12-08 18:17:56.804036: val_loss -0.5237
2024-12-08 18:17:56.804796: Pseudo dice [0.7325]
2024-12-08 18:17:56.805583: Epoch time: 87.81 s
2024-12-08 18:17:56.806404: Yayy! New best EMA pseudo Dice: 0.7296
2024-12-08 18:17:58.440102: 
2024-12-08 18:17:58.441774: Epoch 76
2024-12-08 18:17:58.442576: Current learning rate: 0.00931
2024-12-08 18:19:26.132733: Validation loss did not improve from -0.56848. Patience: 6/50
2024-12-08 18:19:26.133972: train_loss -0.6696
2024-12-08 18:19:26.134947: val_loss -0.5094
2024-12-08 18:19:26.135832: Pseudo dice [0.7172]
2024-12-08 18:19:26.136585: Epoch time: 87.69 s
2024-12-08 18:19:27.449043: 
2024-12-08 18:19:27.450716: Epoch 77
2024-12-08 18:19:27.451516: Current learning rate: 0.0093
2024-12-08 18:20:55.207186: Validation loss did not improve from -0.56848. Patience: 7/50
2024-12-08 18:20:55.208452: train_loss -0.6718
2024-12-08 18:20:55.209696: val_loss -0.5252
2024-12-08 18:20:55.210609: Pseudo dice [0.7336]
2024-12-08 18:20:55.211514: Epoch time: 87.76 s
2024-12-08 18:20:56.505888: 
2024-12-08 18:20:56.507850: Epoch 78
2024-12-08 18:20:56.509129: Current learning rate: 0.0093
2024-12-08 18:22:24.210713: Validation loss did not improve from -0.56848. Patience: 8/50
2024-12-08 18:22:24.212016: train_loss -0.6734
2024-12-08 18:22:24.213178: val_loss -0.5341
2024-12-08 18:22:24.213978: Pseudo dice [0.7387]
2024-12-08 18:22:24.214814: Epoch time: 87.71 s
2024-12-08 18:22:24.215479: Yayy! New best EMA pseudo Dice: 0.7298
2024-12-08 18:22:25.908937: 
2024-12-08 18:22:25.910876: Epoch 79
2024-12-08 18:22:25.912023: Current learning rate: 0.00929
2024-12-08 18:23:53.605257: Validation loss did not improve from -0.56848. Patience: 9/50
2024-12-08 18:23:53.605958: train_loss -0.6781
2024-12-08 18:23:53.606812: val_loss -0.5203
2024-12-08 18:23:53.607861: Pseudo dice [0.7267]
2024-12-08 18:23:53.608836: Epoch time: 87.7 s
2024-12-08 18:23:55.265325: 
2024-12-08 18:23:55.267321: Epoch 80
2024-12-08 18:23:55.268803: Current learning rate: 0.00928
2024-12-08 18:25:22.981667: Validation loss did not improve from -0.56848. Patience: 10/50
2024-12-08 18:25:22.982876: train_loss -0.6711
2024-12-08 18:25:22.984011: val_loss -0.5183
2024-12-08 18:25:22.984803: Pseudo dice [0.7285]
2024-12-08 18:25:22.985527: Epoch time: 87.72 s
2024-12-08 18:25:24.295890: 
2024-12-08 18:25:24.297392: Epoch 81
2024-12-08 18:25:24.298458: Current learning rate: 0.00927
2024-12-08 18:26:51.981152: Validation loss did not improve from -0.56848. Patience: 11/50
2024-12-08 18:26:51.982448: train_loss -0.6702
2024-12-08 18:26:51.983476: val_loss -0.558
2024-12-08 18:26:51.984176: Pseudo dice [0.7496]
2024-12-08 18:26:51.984917: Epoch time: 87.69 s
2024-12-08 18:26:51.985644: Yayy! New best EMA pseudo Dice: 0.7314
2024-12-08 18:26:54.030461: 
2024-12-08 18:26:54.031628: Epoch 82
2024-12-08 18:26:54.032344: Current learning rate: 0.00926
2024-12-08 18:28:22.321240: Validation loss did not improve from -0.56848. Patience: 12/50
2024-12-08 18:28:22.322536: train_loss -0.6801
2024-12-08 18:28:22.323721: val_loss -0.5267
2024-12-08 18:28:22.324718: Pseudo dice [0.7431]
2024-12-08 18:28:22.325617: Epoch time: 88.29 s
2024-12-08 18:28:22.326591: Yayy! New best EMA pseudo Dice: 0.7326
2024-12-08 18:28:23.915922: 
2024-12-08 18:28:23.917975: Epoch 83
2024-12-08 18:28:23.918873: Current learning rate: 0.00925
2024-12-08 18:29:52.353027: Validation loss did not improve from -0.56848. Patience: 13/50
2024-12-08 18:29:52.354757: train_loss -0.6651
2024-12-08 18:29:52.356447: val_loss -0.5225
2024-12-08 18:29:52.357683: Pseudo dice [0.7343]
2024-12-08 18:29:52.359193: Epoch time: 88.44 s
2024-12-08 18:29:52.360285: Yayy! New best EMA pseudo Dice: 0.7328
2024-12-08 18:29:53.926346: 
2024-12-08 18:29:53.928047: Epoch 84
2024-12-08 18:29:53.929112: Current learning rate: 0.00924
2024-12-08 18:31:22.422233: Validation loss did not improve from -0.56848. Patience: 14/50
2024-12-08 18:31:22.426923: train_loss -0.6909
2024-12-08 18:31:22.428723: val_loss -0.5333
2024-12-08 18:31:22.429382: Pseudo dice [0.7308]
2024-12-08 18:31:22.430102: Epoch time: 88.5 s
2024-12-08 18:31:24.068778: 
2024-12-08 18:31:24.070942: Epoch 85
2024-12-08 18:31:24.072152: Current learning rate: 0.00923
2024-12-08 18:32:52.593969: Validation loss did not improve from -0.56848. Patience: 15/50
2024-12-08 18:32:52.598595: train_loss -0.6943
2024-12-08 18:32:52.601566: val_loss -0.539
2024-12-08 18:32:52.602605: Pseudo dice [0.7367]
2024-12-08 18:32:52.603928: Epoch time: 88.53 s
2024-12-08 18:32:52.605277: Yayy! New best EMA pseudo Dice: 0.733
2024-12-08 18:32:54.346557: 
2024-12-08 18:32:54.348047: Epoch 86
2024-12-08 18:32:54.348808: Current learning rate: 0.00922
2024-12-08 18:34:22.761375: Validation loss did not improve from -0.56848. Patience: 16/50
2024-12-08 18:34:22.762430: train_loss -0.6889
2024-12-08 18:34:22.763396: val_loss -0.5465
2024-12-08 18:34:22.764150: Pseudo dice [0.7426]
2024-12-08 18:34:22.764834: Epoch time: 88.42 s
2024-12-08 18:34:22.765412: Yayy! New best EMA pseudo Dice: 0.734
2024-12-08 18:34:24.421380: 
2024-12-08 18:34:24.422829: Epoch 87
2024-12-08 18:34:24.423602: Current learning rate: 0.00921
2024-12-08 18:35:52.938250: Validation loss did not improve from -0.56848. Patience: 17/50
2024-12-08 18:35:52.939420: train_loss -0.6888
2024-12-08 18:35:52.940551: val_loss -0.5287
2024-12-08 18:35:52.941534: Pseudo dice [0.7357]
2024-12-08 18:35:52.942552: Epoch time: 88.52 s
2024-12-08 18:35:52.943515: Yayy! New best EMA pseudo Dice: 0.7341
2024-12-08 18:35:54.586912: 
2024-12-08 18:35:54.588409: Epoch 88
2024-12-08 18:35:54.589274: Current learning rate: 0.0092
2024-12-08 18:37:22.872648: Validation loss did not improve from -0.56848. Patience: 18/50
2024-12-08 18:37:22.873879: train_loss -0.6931
2024-12-08 18:37:22.874784: val_loss -0.5118
2024-12-08 18:37:22.875502: Pseudo dice [0.7387]
2024-12-08 18:37:22.876359: Epoch time: 88.29 s
2024-12-08 18:37:22.877019: Yayy! New best EMA pseudo Dice: 0.7346
2024-12-08 18:37:24.486935: 
2024-12-08 18:37:24.488778: Epoch 89
2024-12-08 18:37:24.489752: Current learning rate: 0.0092
2024-12-08 18:38:52.735279: Validation loss improved from -0.56848 to -0.57523! Patience: 18/50
2024-12-08 18:38:52.736558: train_loss -0.6882
2024-12-08 18:38:52.737564: val_loss -0.5752
2024-12-08 18:38:52.738343: Pseudo dice [0.7573]
2024-12-08 18:38:52.739171: Epoch time: 88.25 s
2024-12-08 18:38:53.059888: Yayy! New best EMA pseudo Dice: 0.7369
2024-12-08 18:38:54.648091: 
2024-12-08 18:38:54.649185: Epoch 90
2024-12-08 18:38:54.649987: Current learning rate: 0.00919
2024-12-08 18:40:22.924581: Validation loss did not improve from -0.57523. Patience: 1/50
2024-12-08 18:40:22.925434: train_loss -0.6907
2024-12-08 18:40:22.926311: val_loss -0.523
2024-12-08 18:40:22.927154: Pseudo dice [0.7346]
2024-12-08 18:40:22.927815: Epoch time: 88.28 s
2024-12-08 18:40:24.214410: 
2024-12-08 18:40:24.215929: Epoch 91
2024-12-08 18:40:24.216808: Current learning rate: 0.00918
2024-12-08 18:41:52.591773: Validation loss did not improve from -0.57523. Patience: 2/50
2024-12-08 18:41:52.592996: train_loss -0.6965
2024-12-08 18:41:52.594129: val_loss -0.5443
2024-12-08 18:41:52.594958: Pseudo dice [0.7442]
2024-12-08 18:41:52.595827: Epoch time: 88.38 s
2024-12-08 18:41:52.596510: Yayy! New best EMA pseudo Dice: 0.7374
2024-12-08 18:41:54.534387: 
2024-12-08 18:41:54.536318: Epoch 92
2024-12-08 18:41:54.537301: Current learning rate: 0.00917
2024-12-08 18:43:22.828666: Validation loss did not improve from -0.57523. Patience: 3/50
2024-12-08 18:43:22.829836: train_loss -0.6934
2024-12-08 18:43:22.830719: val_loss -0.5596
2024-12-08 18:43:22.831436: Pseudo dice [0.7499]
2024-12-08 18:43:22.832121: Epoch time: 88.3 s
2024-12-08 18:43:22.832890: Yayy! New best EMA pseudo Dice: 0.7386
2024-12-08 18:43:24.447154: 
2024-12-08 18:43:24.448933: Epoch 93
2024-12-08 18:43:24.449908: Current learning rate: 0.00916
2024-12-08 18:44:52.491367: Validation loss did not improve from -0.57523. Patience: 4/50
2024-12-08 18:44:52.492464: train_loss -0.7004
2024-12-08 18:44:52.493482: val_loss -0.5488
2024-12-08 18:44:52.494393: Pseudo dice [0.7446]
2024-12-08 18:44:52.495132: Epoch time: 88.05 s
2024-12-08 18:44:52.495852: Yayy! New best EMA pseudo Dice: 0.7392
2024-12-08 18:44:54.109987: 
2024-12-08 18:44:54.111112: Epoch 94
2024-12-08 18:44:54.112168: Current learning rate: 0.00915
2024-12-08 18:46:22.163100: Validation loss did not improve from -0.57523. Patience: 5/50
2024-12-08 18:46:22.163984: train_loss -0.6999
2024-12-08 18:46:22.164901: val_loss -0.5693
2024-12-08 18:46:22.165675: Pseudo dice [0.7568]
2024-12-08 18:46:22.166433: Epoch time: 88.06 s
2024-12-08 18:46:22.514945: Yayy! New best EMA pseudo Dice: 0.741
2024-12-08 18:46:24.088830: 
2024-12-08 18:46:24.090872: Epoch 95
2024-12-08 18:46:24.091932: Current learning rate: 0.00914
2024-12-08 18:47:51.903232: Validation loss did not improve from -0.57523. Patience: 6/50
2024-12-08 18:47:51.904477: train_loss -0.6966
2024-12-08 18:47:51.905202: val_loss -0.5496
2024-12-08 18:47:51.905866: Pseudo dice [0.7397]
2024-12-08 18:47:51.906628: Epoch time: 87.82 s
2024-12-08 18:47:53.135675: 
2024-12-08 18:47:53.137482: Epoch 96
2024-12-08 18:47:53.138815: Current learning rate: 0.00913
2024-12-08 18:49:21.119949: Validation loss did not improve from -0.57523. Patience: 7/50
2024-12-08 18:49:21.120872: train_loss -0.6961
2024-12-08 18:49:21.121896: val_loss -0.5731
2024-12-08 18:49:21.122670: Pseudo dice [0.7566]
2024-12-08 18:49:21.123415: Epoch time: 87.99 s
2024-12-08 18:49:21.124168: Yayy! New best EMA pseudo Dice: 0.7424
2024-12-08 18:49:22.746316: 
2024-12-08 18:49:22.747825: Epoch 97
2024-12-08 18:49:22.748977: Current learning rate: 0.00912
2024-12-08 18:50:50.539663: Validation loss did not improve from -0.57523. Patience: 8/50
2024-12-08 18:50:50.540761: train_loss -0.7022
2024-12-08 18:50:50.541762: val_loss -0.5437
2024-12-08 18:50:50.542653: Pseudo dice [0.7408]
2024-12-08 18:50:50.543554: Epoch time: 87.8 s
2024-12-08 18:50:51.823571: 
2024-12-08 18:50:51.825760: Epoch 98
2024-12-08 18:50:51.826846: Current learning rate: 0.00911
2024-12-08 18:52:19.523226: Validation loss did not improve from -0.57523. Patience: 9/50
2024-12-08 18:52:19.524493: train_loss -0.701
2024-12-08 18:52:19.525487: val_loss -0.5673
2024-12-08 18:52:19.526329: Pseudo dice [0.7526]
2024-12-08 18:52:19.527056: Epoch time: 87.7 s
2024-12-08 18:52:19.527761: Yayy! New best EMA pseudo Dice: 0.7433
2024-12-08 18:52:21.116902: 
2024-12-08 18:52:21.118544: Epoch 99
2024-12-08 18:52:21.119795: Current learning rate: 0.0091
2024-12-08 18:53:48.671618: Validation loss did not improve from -0.57523. Patience: 10/50
2024-12-08 18:53:48.672992: train_loss -0.7017
2024-12-08 18:53:48.674296: val_loss -0.5383
2024-12-08 18:53:48.675173: Pseudo dice [0.739]
2024-12-08 18:53:48.676166: Epoch time: 87.56 s
2024-12-08 18:53:50.271296: 
2024-12-08 18:53:50.273263: Epoch 100
2024-12-08 18:53:50.274423: Current learning rate: 0.0091
2024-12-08 18:55:17.892396: Validation loss did not improve from -0.57523. Patience: 11/50
2024-12-08 18:55:17.893656: train_loss -0.6878
2024-12-08 18:55:17.894749: val_loss -0.5106
2024-12-08 18:55:17.895667: Pseudo dice [0.7279]
2024-12-08 18:55:17.896537: Epoch time: 87.62 s
2024-12-08 18:55:19.167114: 
2024-12-08 18:55:19.168607: Epoch 101
2024-12-08 18:55:19.169673: Current learning rate: 0.00909
2024-12-08 18:56:46.802088: Validation loss did not improve from -0.57523. Patience: 12/50
2024-12-08 18:56:46.803408: train_loss -0.6971
2024-12-08 18:56:46.804435: val_loss -0.5465
2024-12-08 18:56:46.805234: Pseudo dice [0.7485]
2024-12-08 18:56:46.806119: Epoch time: 87.64 s
2024-12-08 18:56:48.062672: 
2024-12-08 18:56:48.063781: Epoch 102
2024-12-08 18:56:48.064778: Current learning rate: 0.00908
2024-12-08 18:58:15.636858: Validation loss improved from -0.57523 to -0.57778! Patience: 12/50
2024-12-08 18:58:15.637769: train_loss -0.7035
2024-12-08 18:58:15.638634: val_loss -0.5778
2024-12-08 18:58:15.639740: Pseudo dice [0.7599]
2024-12-08 18:58:15.640629: Epoch time: 87.58 s
2024-12-08 18:58:15.641725: Yayy! New best EMA pseudo Dice: 0.7439
2024-12-08 18:58:17.569222: 
2024-12-08 18:58:17.571180: Epoch 103
2024-12-08 18:58:17.572004: Current learning rate: 0.00907
2024-12-08 18:59:45.145282: Validation loss did not improve from -0.57778. Patience: 1/50
2024-12-08 18:59:45.146316: train_loss -0.7141
2024-12-08 18:59:45.147279: val_loss -0.5694
2024-12-08 18:59:45.147955: Pseudo dice [0.7627]
2024-12-08 18:59:45.148674: Epoch time: 87.58 s
2024-12-08 18:59:45.149337: Yayy! New best EMA pseudo Dice: 0.7457
2024-12-08 18:59:46.770988: 
2024-12-08 18:59:46.772411: Epoch 104
2024-12-08 18:59:46.773511: Current learning rate: 0.00906
2024-12-08 19:01:14.310108: Validation loss did not improve from -0.57778. Patience: 2/50
2024-12-08 19:01:14.311286: train_loss -0.7127
2024-12-08 19:01:14.312314: val_loss -0.5554
2024-12-08 19:01:14.312979: Pseudo dice [0.7458]
2024-12-08 19:01:14.313703: Epoch time: 87.54 s
2024-12-08 19:01:14.660599: Yayy! New best EMA pseudo Dice: 0.7458
2024-12-08 19:01:16.292448: 
2024-12-08 19:01:16.295079: Epoch 105
2024-12-08 19:01:16.296010: Current learning rate: 0.00905
2024-12-08 19:02:43.902986: Validation loss did not improve from -0.57778. Patience: 3/50
2024-12-08 19:02:43.903795: train_loss -0.708
2024-12-08 19:02:43.904824: val_loss -0.5491
2024-12-08 19:02:43.905522: Pseudo dice [0.7461]
2024-12-08 19:02:43.906284: Epoch time: 87.61 s
2024-12-08 19:02:43.907051: Yayy! New best EMA pseudo Dice: 0.7458
2024-12-08 19:02:45.480310: 
2024-12-08 19:02:45.481815: Epoch 106
2024-12-08 19:02:45.482831: Current learning rate: 0.00904
2024-12-08 19:04:13.069716: Validation loss did not improve from -0.57778. Patience: 4/50
2024-12-08 19:04:13.070687: train_loss -0.7032
2024-12-08 19:04:13.071669: val_loss -0.5375
2024-12-08 19:04:13.072390: Pseudo dice [0.7376]
2024-12-08 19:04:13.073303: Epoch time: 87.59 s
2024-12-08 19:04:14.396031: 
2024-12-08 19:04:14.398490: Epoch 107
2024-12-08 19:04:14.399465: Current learning rate: 0.00903
2024-12-08 19:05:42.240829: Validation loss did not improve from -0.57778. Patience: 5/50
2024-12-08 19:05:42.241916: train_loss -0.7141
2024-12-08 19:05:42.242936: val_loss -0.5509
2024-12-08 19:05:42.243783: Pseudo dice [0.7433]
2024-12-08 19:05:42.244612: Epoch time: 87.85 s
2024-12-08 19:05:43.485711: 
2024-12-08 19:05:43.487516: Epoch 108
2024-12-08 19:05:43.488650: Current learning rate: 0.00902
2024-12-08 19:07:11.394984: Validation loss did not improve from -0.57778. Patience: 6/50
2024-12-08 19:07:11.396153: train_loss -0.7034
2024-12-08 19:07:11.397134: val_loss -0.5426
2024-12-08 19:07:11.397903: Pseudo dice [0.7422]
2024-12-08 19:07:11.398711: Epoch time: 87.91 s
2024-12-08 19:07:12.679646: 
2024-12-08 19:07:12.681133: Epoch 109
2024-12-08 19:07:12.682083: Current learning rate: 0.00901
2024-12-08 19:08:40.595261: Validation loss did not improve from -0.57778. Patience: 7/50
2024-12-08 19:08:40.596214: train_loss -0.7017
2024-12-08 19:08:40.597072: val_loss -0.5563
2024-12-08 19:08:40.597835: Pseudo dice [0.7487]
2024-12-08 19:08:40.598567: Epoch time: 87.92 s
2024-12-08 19:08:42.207665: 
2024-12-08 19:08:42.209393: Epoch 110
2024-12-08 19:08:42.210250: Current learning rate: 0.009
2024-12-08 19:10:10.071751: Validation loss did not improve from -0.57778. Patience: 8/50
2024-12-08 19:10:10.072715: train_loss -0.7079
2024-12-08 19:10:10.073632: val_loss -0.5376
2024-12-08 19:10:10.074419: Pseudo dice [0.7365]
2024-12-08 19:10:10.075296: Epoch time: 87.87 s
2024-12-08 19:10:11.357815: 
2024-12-08 19:10:11.359681: Epoch 111
2024-12-08 19:10:11.360639: Current learning rate: 0.009
2024-12-08 19:11:39.294896: Validation loss did not improve from -0.57778. Patience: 9/50
2024-12-08 19:11:39.295905: train_loss -0.7043
2024-12-08 19:11:39.296836: val_loss -0.5203
2024-12-08 19:11:39.297615: Pseudo dice [0.7274]
2024-12-08 19:11:39.298455: Epoch time: 87.94 s
2024-12-08 19:11:40.562800: 
2024-12-08 19:11:40.564571: Epoch 112
2024-12-08 19:11:40.565312: Current learning rate: 0.00899
2024-12-08 19:13:08.501085: Validation loss did not improve from -0.57778. Patience: 10/50
2024-12-08 19:13:08.502248: train_loss -0.7004
2024-12-08 19:13:08.503284: val_loss -0.5373
2024-12-08 19:13:08.504188: Pseudo dice [0.737]
2024-12-08 19:13:08.505113: Epoch time: 87.94 s
2024-12-08 19:13:09.759719: 
2024-12-08 19:13:09.761375: Epoch 113
2024-12-08 19:13:09.762239: Current learning rate: 0.00898
2024-12-08 19:14:37.591300: Validation loss did not improve from -0.57778. Patience: 11/50
2024-12-08 19:14:37.592903: train_loss -0.7173
2024-12-08 19:14:37.593846: val_loss -0.5625
2024-12-08 19:14:37.594612: Pseudo dice [0.751]
2024-12-08 19:14:37.595255: Epoch time: 87.83 s
2024-12-08 19:14:39.227636: 
2024-12-08 19:14:39.228935: Epoch 114
2024-12-08 19:14:39.229947: Current learning rate: 0.00897
2024-12-08 19:16:06.917082: Validation loss did not improve from -0.57778. Patience: 12/50
2024-12-08 19:16:06.918187: train_loss -0.7226
2024-12-08 19:16:06.919203: val_loss -0.4959
2024-12-08 19:16:06.919974: Pseudo dice [0.7148]
2024-12-08 19:16:06.920715: Epoch time: 87.69 s
2024-12-08 19:16:09.516640: 
2024-12-08 19:16:09.519650: Epoch 115
2024-12-08 19:16:09.520658: Current learning rate: 0.00896
2024-12-08 19:17:37.420435: Validation loss did not improve from -0.57778. Patience: 13/50
2024-12-08 19:17:37.421457: train_loss -0.7178
2024-12-08 19:17:37.422545: val_loss -0.5397
2024-12-08 19:17:37.423357: Pseudo dice [0.7418]
2024-12-08 19:17:37.423998: Epoch time: 87.91 s
2024-12-08 19:17:38.702805: 
2024-12-08 19:17:38.704523: Epoch 116
2024-12-08 19:17:38.705474: Current learning rate: 0.00895
2024-12-08 19:19:06.648084: Validation loss did not improve from -0.57778. Patience: 14/50
2024-12-08 19:19:06.648980: train_loss -0.715
2024-12-08 19:19:06.650136: val_loss -0.5552
2024-12-08 19:19:06.651068: Pseudo dice [0.7531]
2024-12-08 19:19:06.651898: Epoch time: 87.95 s
2024-12-08 19:19:07.960315: 
2024-12-08 19:19:07.962834: Epoch 117
2024-12-08 19:19:07.964427: Current learning rate: 0.00894
2024-12-08 19:20:35.989824: Validation loss did not improve from -0.57778. Patience: 15/50
2024-12-08 19:20:35.991139: train_loss -0.718
2024-12-08 19:20:35.992448: val_loss -0.5414
2024-12-08 19:20:35.993405: Pseudo dice [0.7402]
2024-12-08 19:20:35.994280: Epoch time: 88.03 s
2024-12-08 19:20:37.281472: 
2024-12-08 19:20:37.282983: Epoch 118
2024-12-08 19:20:37.283859: Current learning rate: 0.00893
2024-12-08 19:22:05.232230: Validation loss did not improve from -0.57778. Patience: 16/50
2024-12-08 19:22:05.233584: train_loss -0.7229
2024-12-08 19:22:05.234792: val_loss -0.5628
2024-12-08 19:22:05.235573: Pseudo dice [0.7522]
2024-12-08 19:22:05.236349: Epoch time: 87.95 s
2024-12-08 19:22:06.562553: 
2024-12-08 19:22:06.564278: Epoch 119
2024-12-08 19:22:06.565158: Current learning rate: 0.00892
2024-12-08 19:23:34.469695: Validation loss did not improve from -0.57778. Patience: 17/50
2024-12-08 19:23:34.470840: train_loss -0.724
2024-12-08 19:23:34.471861: val_loss -0.5573
2024-12-08 19:23:34.472671: Pseudo dice [0.7513]
2024-12-08 19:23:34.473404: Epoch time: 87.91 s
2024-12-08 19:23:36.100728: 
2024-12-08 19:23:36.102174: Epoch 120
2024-12-08 19:23:36.103107: Current learning rate: 0.00891
2024-12-08 19:25:04.076480: Validation loss did not improve from -0.57778. Patience: 18/50
2024-12-08 19:25:04.077532: train_loss -0.7182
2024-12-08 19:25:04.078547: val_loss -0.5188
2024-12-08 19:25:04.079273: Pseudo dice [0.7393]
2024-12-08 19:25:04.080104: Epoch time: 87.98 s
2024-12-08 19:25:05.409708: 
2024-12-08 19:25:05.411157: Epoch 121
2024-12-08 19:25:05.412078: Current learning rate: 0.0089
2024-12-08 19:26:33.302520: Validation loss did not improve from -0.57778. Patience: 19/50
2024-12-08 19:26:33.303696: train_loss -0.7259
2024-12-08 19:26:33.304779: val_loss -0.556
2024-12-08 19:26:33.305647: Pseudo dice [0.7518]
2024-12-08 19:26:33.306449: Epoch time: 87.9 s
2024-12-08 19:26:34.631505: 
2024-12-08 19:26:34.633771: Epoch 122
2024-12-08 19:26:34.634661: Current learning rate: 0.00889
2024-12-08 19:28:02.455036: Validation loss did not improve from -0.57778. Patience: 20/50
2024-12-08 19:28:02.456143: train_loss -0.7254
2024-12-08 19:28:02.456998: val_loss -0.5681
2024-12-08 19:28:02.457937: Pseudo dice [0.7622]
2024-12-08 19:28:02.458687: Epoch time: 87.83 s
2024-12-08 19:28:03.775101: 
2024-12-08 19:28:03.776948: Epoch 123
2024-12-08 19:28:03.778132: Current learning rate: 0.00889
2024-12-08 19:29:31.864789: Validation loss did not improve from -0.57778. Patience: 21/50
2024-12-08 19:29:31.865798: train_loss -0.7283
2024-12-08 19:29:31.866734: val_loss -0.5371
2024-12-08 19:29:31.867395: Pseudo dice [0.7438]
2024-12-08 19:29:31.868080: Epoch time: 88.09 s
2024-12-08 19:29:33.194555: 
2024-12-08 19:29:33.196274: Epoch 124
2024-12-08 19:29:33.197163: Current learning rate: 0.00888
2024-12-08 19:31:01.027924: Validation loss did not improve from -0.57778. Patience: 22/50
2024-12-08 19:31:01.029095: train_loss -0.7226
2024-12-08 19:31:01.030005: val_loss -0.5424
2024-12-08 19:31:01.030737: Pseudo dice [0.7445]
2024-12-08 19:31:01.031601: Epoch time: 87.84 s
2024-12-08 19:31:02.942489: 
2024-12-08 19:31:02.944766: Epoch 125
2024-12-08 19:31:02.945850: Current learning rate: 0.00887
2024-12-08 19:32:30.479304: Validation loss did not improve from -0.57778. Patience: 23/50
2024-12-08 19:32:30.480481: train_loss -0.7023
2024-12-08 19:32:30.481405: val_loss -0.5155
2024-12-08 19:32:30.482203: Pseudo dice [0.7269]
2024-12-08 19:32:30.482974: Epoch time: 87.54 s
2024-12-08 19:32:31.713627: 
2024-12-08 19:32:31.715543: Epoch 126
2024-12-08 19:32:31.716795: Current learning rate: 0.00886
2024-12-08 19:33:59.334184: Validation loss did not improve from -0.57778. Patience: 24/50
2024-12-08 19:33:59.335325: train_loss -0.7169
2024-12-08 19:33:59.336202: val_loss -0.5707
2024-12-08 19:33:59.336966: Pseudo dice [0.7524]
2024-12-08 19:33:59.337634: Epoch time: 87.62 s
2024-12-08 19:34:00.569576: 
2024-12-08 19:34:00.571330: Epoch 127
2024-12-08 19:34:00.572841: Current learning rate: 0.00885
2024-12-08 19:35:28.071459: Validation loss did not improve from -0.57778. Patience: 25/50
2024-12-08 19:35:28.072696: train_loss -0.7297
2024-12-08 19:35:28.073829: val_loss -0.5643
2024-12-08 19:35:28.074662: Pseudo dice [0.7484]
2024-12-08 19:35:28.075668: Epoch time: 87.5 s
2024-12-08 19:35:29.284307: 
2024-12-08 19:35:29.286560: Epoch 128
2024-12-08 19:35:29.287487: Current learning rate: 0.00884
2024-12-08 19:36:56.686203: Validation loss did not improve from -0.57778. Patience: 26/50
2024-12-08 19:36:56.688040: train_loss -0.7358
2024-12-08 19:36:56.689752: val_loss -0.5643
2024-12-08 19:36:56.690537: Pseudo dice [0.7403]
2024-12-08 19:36:56.691211: Epoch time: 87.4 s
2024-12-08 19:36:57.909679: 
2024-12-08 19:36:57.911789: Epoch 129
2024-12-08 19:36:57.912896: Current learning rate: 0.00883
2024-12-08 19:38:25.414419: Validation loss did not improve from -0.57778. Patience: 27/50
2024-12-08 19:38:25.415581: train_loss -0.7294
2024-12-08 19:38:25.416850: val_loss -0.5755
2024-12-08 19:38:25.417796: Pseudo dice [0.7579]
2024-12-08 19:38:25.418905: Epoch time: 87.51 s
2024-12-08 19:38:26.983919: 
2024-12-08 19:38:26.985451: Epoch 130
2024-12-08 19:38:26.986400: Current learning rate: 0.00882
2024-12-08 19:39:54.405271: Validation loss did not improve from -0.57778. Patience: 28/50
2024-12-08 19:39:54.406967: train_loss -0.7308
2024-12-08 19:39:54.408973: val_loss -0.5657
2024-12-08 19:39:54.410274: Pseudo dice [0.7524]
2024-12-08 19:39:54.411576: Epoch time: 87.42 s
2024-12-08 19:39:54.412853: Yayy! New best EMA pseudo Dice: 0.7464
2024-12-08 19:39:56.054149: 
2024-12-08 19:39:56.056443: Epoch 131
2024-12-08 19:39:56.057731: Current learning rate: 0.00881
2024-12-08 19:41:23.616580: Validation loss did not improve from -0.57778. Patience: 29/50
2024-12-08 19:41:23.617649: train_loss -0.7368
2024-12-08 19:41:23.618653: val_loss -0.5419
2024-12-08 19:41:23.619440: Pseudo dice [0.7457]
2024-12-08 19:41:23.620464: Epoch time: 87.56 s
2024-12-08 19:41:24.858948: 
2024-12-08 19:41:24.860885: Epoch 132
2024-12-08 19:41:24.861974: Current learning rate: 0.0088
2024-12-08 19:42:52.360184: Validation loss did not improve from -0.57778. Patience: 30/50
2024-12-08 19:42:52.361261: train_loss -0.7369
2024-12-08 19:42:52.362279: val_loss -0.5744
2024-12-08 19:42:52.363303: Pseudo dice [0.7572]
2024-12-08 19:42:52.364314: Epoch time: 87.5 s
2024-12-08 19:42:52.365008: Yayy! New best EMA pseudo Dice: 0.7474
2024-12-08 19:42:54.016167: 
2024-12-08 19:42:54.018469: Epoch 133
2024-12-08 19:42:54.019339: Current learning rate: 0.00879
2024-12-08 19:44:21.735536: Validation loss did not improve from -0.57778. Patience: 31/50
2024-12-08 19:44:21.736577: train_loss -0.7358
2024-12-08 19:44:21.737742: val_loss -0.5521
2024-12-08 19:44:21.738821: Pseudo dice [0.7502]
2024-12-08 19:44:21.739890: Epoch time: 87.72 s
2024-12-08 19:44:21.740963: Yayy! New best EMA pseudo Dice: 0.7477
2024-12-08 19:44:23.317480: 
2024-12-08 19:44:23.319498: Epoch 134
2024-12-08 19:44:23.320461: Current learning rate: 0.00879
2024-12-08 19:45:51.028563: Validation loss did not improve from -0.57778. Patience: 32/50
2024-12-08 19:45:51.029521: train_loss -0.7345
2024-12-08 19:45:51.030679: val_loss -0.5232
2024-12-08 19:45:51.031474: Pseudo dice [0.7293]
2024-12-08 19:45:51.032331: Epoch time: 87.71 s
2024-12-08 19:45:52.612016: 
2024-12-08 19:45:52.614496: Epoch 135
2024-12-08 19:45:52.615543: Current learning rate: 0.00878
2024-12-08 19:47:20.103688: Validation loss did not improve from -0.57778. Patience: 33/50
2024-12-08 19:47:20.104910: train_loss -0.7357
2024-12-08 19:47:20.106237: val_loss -0.5309
2024-12-08 19:47:20.107415: Pseudo dice [0.7376]
2024-12-08 19:47:20.108366: Epoch time: 87.49 s
2024-12-08 19:47:21.692883: 
2024-12-08 19:47:21.695090: Epoch 136
2024-12-08 19:47:21.696221: Current learning rate: 0.00877
2024-12-08 19:48:49.240140: Validation loss did not improve from -0.57778. Patience: 34/50
2024-12-08 19:48:49.241405: train_loss -0.7325
2024-12-08 19:48:49.242532: val_loss -0.5288
2024-12-08 19:48:49.243537: Pseudo dice [0.7273]
2024-12-08 19:48:49.244633: Epoch time: 87.55 s
2024-12-08 19:48:50.497031: 
2024-12-08 19:48:50.499077: Epoch 137
2024-12-08 19:48:50.500445: Current learning rate: 0.00876
2024-12-08 19:50:18.007191: Validation loss did not improve from -0.57778. Patience: 35/50
2024-12-08 19:50:18.007906: train_loss -0.7299
2024-12-08 19:50:18.008804: val_loss -0.5563
2024-12-08 19:50:18.009515: Pseudo dice [0.7558]
2024-12-08 19:50:18.010163: Epoch time: 87.51 s
2024-12-08 19:50:19.264921: 
2024-12-08 19:50:19.267186: Epoch 138
2024-12-08 19:50:19.268103: Current learning rate: 0.00875
2024-12-08 19:51:46.710236: Validation loss did not improve from -0.57778. Patience: 36/50
2024-12-08 19:51:46.711320: train_loss -0.7358
2024-12-08 19:51:46.712363: val_loss -0.5229
2024-12-08 19:51:46.713213: Pseudo dice [0.7347]
2024-12-08 19:51:46.713968: Epoch time: 87.45 s
2024-12-08 19:51:47.956785: 
2024-12-08 19:51:47.958452: Epoch 139
2024-12-08 19:51:47.959603: Current learning rate: 0.00874
2024-12-08 19:53:15.466184: Validation loss did not improve from -0.57778. Patience: 37/50
2024-12-08 19:53:15.467082: train_loss -0.7422
2024-12-08 19:53:15.468137: val_loss -0.5541
2024-12-08 19:53:15.468943: Pseudo dice [0.7555]
2024-12-08 19:53:15.469971: Epoch time: 87.51 s
2024-12-08 19:53:17.075217: 
2024-12-08 19:53:17.077582: Epoch 140
2024-12-08 19:53:17.078793: Current learning rate: 0.00873
2024-12-08 19:54:44.521536: Validation loss did not improve from -0.57778. Patience: 38/50
2024-12-08 19:54:44.523021: train_loss -0.7372
2024-12-08 19:54:44.524678: val_loss -0.5331
2024-12-08 19:54:44.526129: Pseudo dice [0.7387]
2024-12-08 19:54:44.526881: Epoch time: 87.45 s
2024-12-08 19:54:45.775597: 
2024-12-08 19:54:45.777322: Epoch 141
2024-12-08 19:54:45.778459: Current learning rate: 0.00872
2024-12-08 19:56:13.252296: Validation loss did not improve from -0.57778. Patience: 39/50
2024-12-08 19:56:13.253645: train_loss -0.7375
2024-12-08 19:56:13.255254: val_loss -0.5668
2024-12-08 19:56:13.256200: Pseudo dice [0.7614]
2024-12-08 19:56:13.257145: Epoch time: 87.48 s
2024-12-08 19:56:14.491681: 
2024-12-08 19:56:14.494911: Epoch 142
2024-12-08 19:56:14.496870: Current learning rate: 0.00871
2024-12-08 19:57:41.970732: Validation loss did not improve from -0.57778. Patience: 40/50
2024-12-08 19:57:41.971627: train_loss -0.7383
2024-12-08 19:57:41.973115: val_loss -0.5579
2024-12-08 19:57:41.974399: Pseudo dice [0.7554]
2024-12-08 19:57:41.975405: Epoch time: 87.48 s
2024-12-08 19:57:43.209638: 
2024-12-08 19:57:43.211567: Epoch 143
2024-12-08 19:57:43.212756: Current learning rate: 0.0087
2024-12-08 19:59:10.845329: Validation loss did not improve from -0.57778. Patience: 41/50
2024-12-08 19:59:10.846767: train_loss -0.7341
2024-12-08 19:59:10.848282: val_loss -0.5149
2024-12-08 19:59:10.849201: Pseudo dice [0.7288]
2024-12-08 19:59:10.849885: Epoch time: 87.64 s
2024-12-08 19:59:12.082505: 
2024-12-08 19:59:12.084609: Epoch 144
2024-12-08 19:59:12.085952: Current learning rate: 0.00869
2024-12-08 20:00:39.698942: Validation loss did not improve from -0.57778. Patience: 42/50
2024-12-08 20:00:39.700251: train_loss -0.7324
2024-12-08 20:00:39.701369: val_loss -0.5076
2024-12-08 20:00:39.702264: Pseudo dice [0.7298]
2024-12-08 20:00:39.703248: Epoch time: 87.62 s
2024-12-08 20:00:41.291182: 
2024-12-08 20:00:41.292916: Epoch 145
2024-12-08 20:00:41.294153: Current learning rate: 0.00868
2024-12-08 20:02:08.815775: Validation loss did not improve from -0.57778. Patience: 43/50
2024-12-08 20:02:08.817236: train_loss -0.7363
2024-12-08 20:02:08.818457: val_loss -0.5473
2024-12-08 20:02:08.819470: Pseudo dice [0.7489]
2024-12-08 20:02:08.820295: Epoch time: 87.53 s
2024-12-08 20:02:10.395159: 
2024-12-08 20:02:10.397107: Epoch 146
2024-12-08 20:02:10.398320: Current learning rate: 0.00868
2024-12-08 20:03:37.870077: Validation loss did not improve from -0.57778. Patience: 44/50
2024-12-08 20:03:37.870750: train_loss -0.7416
2024-12-08 20:03:37.871780: val_loss -0.5494
2024-12-08 20:03:37.872604: Pseudo dice [0.7497]
2024-12-08 20:03:37.873316: Epoch time: 87.48 s
2024-12-08 20:03:39.144788: 
2024-12-08 20:03:39.146958: Epoch 147
2024-12-08 20:03:39.147882: Current learning rate: 0.00867
2024-12-08 20:05:06.683752: Validation loss did not improve from -0.57778. Patience: 45/50
2024-12-08 20:05:06.684940: train_loss -0.7417
2024-12-08 20:05:06.685979: val_loss -0.5524
2024-12-08 20:05:06.687019: Pseudo dice [0.7427]
2024-12-08 20:05:06.687983: Epoch time: 87.54 s
2024-12-08 20:05:07.929410: 
2024-12-08 20:05:07.931409: Epoch 148
2024-12-08 20:05:07.932299: Current learning rate: 0.00866
2024-12-08 20:06:35.486231: Validation loss did not improve from -0.57778. Patience: 46/50
2024-12-08 20:06:35.487243: train_loss -0.7408
2024-12-08 20:06:35.488541: val_loss -0.5522
2024-12-08 20:06:35.489373: Pseudo dice [0.7502]
2024-12-08 20:06:35.490234: Epoch time: 87.56 s
2024-12-08 20:06:36.734992: 
2024-12-08 20:06:36.737330: Epoch 149
2024-12-08 20:06:36.738551: Current learning rate: 0.00865
2024-12-08 20:08:04.302549: Validation loss did not improve from -0.57778. Patience: 47/50
2024-12-08 20:08:04.303831: train_loss -0.7302
2024-12-08 20:08:04.304713: val_loss -0.5488
2024-12-08 20:08:04.305571: Pseudo dice [0.7425]
2024-12-08 20:08:04.306520: Epoch time: 87.57 s
2024-12-08 20:08:05.864622: 
2024-12-08 20:08:05.866725: Epoch 150
2024-12-08 20:08:05.867889: Current learning rate: 0.00864
2024-12-08 20:09:33.459188: Validation loss did not improve from -0.57778. Patience: 48/50
2024-12-08 20:09:33.460378: train_loss -0.7324
2024-12-08 20:09:33.461150: val_loss -0.5277
2024-12-08 20:09:33.461807: Pseudo dice [0.7428]
2024-12-08 20:09:33.462689: Epoch time: 87.6 s
2024-12-08 20:09:34.728120: 
2024-12-08 20:09:34.730339: Epoch 151
2024-12-08 20:09:34.731787: Current learning rate: 0.00863
2024-12-08 20:11:02.446316: Validation loss improved from -0.57778 to -0.57870! Patience: 48/50
2024-12-08 20:11:02.447592: train_loss -0.7387
2024-12-08 20:11:02.448811: val_loss -0.5787
2024-12-08 20:11:02.449582: Pseudo dice [0.7605]
2024-12-08 20:11:02.450477: Epoch time: 87.72 s
2024-12-08 20:11:03.715002: 
2024-12-08 20:11:03.717540: Epoch 152
2024-12-08 20:11:03.718866: Current learning rate: 0.00862
2024-12-08 20:12:31.458420: Validation loss did not improve from -0.57870. Patience: 1/50
2024-12-08 20:12:31.459416: train_loss -0.7398
2024-12-08 20:12:31.460544: val_loss -0.5303
2024-12-08 20:12:31.461477: Pseudo dice [0.741]
2024-12-08 20:12:31.462424: Epoch time: 87.75 s
2024-12-08 20:12:32.720948: 
2024-12-08 20:12:32.723443: Epoch 153
2024-12-08 20:12:32.724679: Current learning rate: 0.00861
2024-12-08 20:14:00.349341: Validation loss did not improve from -0.57870. Patience: 2/50
2024-12-08 20:14:00.350363: train_loss -0.7415
2024-12-08 20:14:00.351626: val_loss -0.5373
2024-12-08 20:14:00.352394: Pseudo dice [0.7423]
2024-12-08 20:14:00.353298: Epoch time: 87.63 s
2024-12-08 20:14:01.615805: 
2024-12-08 20:14:01.617793: Epoch 154
2024-12-08 20:14:01.619084: Current learning rate: 0.0086
2024-12-08 20:15:29.175659: Validation loss did not improve from -0.57870. Patience: 3/50
2024-12-08 20:15:29.176911: train_loss -0.7351
2024-12-08 20:15:29.178256: val_loss -0.5367
2024-12-08 20:15:29.179724: Pseudo dice [0.7398]
2024-12-08 20:15:29.180923: Epoch time: 87.56 s
2024-12-08 20:15:30.790000: 
2024-12-08 20:15:30.791260: Epoch 155
2024-12-08 20:15:30.792503: Current learning rate: 0.00859
2024-12-08 20:16:58.358805: Validation loss did not improve from -0.57870. Patience: 4/50
2024-12-08 20:16:58.360183: train_loss -0.7446
2024-12-08 20:16:58.361464: val_loss -0.5318
2024-12-08 20:16:58.362464: Pseudo dice [0.7386]
2024-12-08 20:16:58.363408: Epoch time: 87.57 s
2024-12-08 20:16:59.641402: 
2024-12-08 20:16:59.643764: Epoch 156
2024-12-08 20:16:59.644850: Current learning rate: 0.00858
2024-12-08 20:18:27.218904: Validation loss did not improve from -0.57870. Patience: 5/50
2024-12-08 20:18:27.220258: train_loss -0.7396
2024-12-08 20:18:27.221036: val_loss -0.5682
2024-12-08 20:18:27.221861: Pseudo dice [0.7581]
2024-12-08 20:18:27.222667: Epoch time: 87.58 s
2024-12-08 20:18:28.830549: 
2024-12-08 20:18:28.832268: Epoch 157
2024-12-08 20:18:28.833226: Current learning rate: 0.00858
2024-12-08 20:19:56.397266: Validation loss did not improve from -0.57870. Patience: 6/50
2024-12-08 20:19:56.398533: train_loss -0.7461
2024-12-08 20:19:56.399731: val_loss -0.5219
2024-12-08 20:19:56.400729: Pseudo dice [0.7302]
2024-12-08 20:19:56.401601: Epoch time: 87.57 s
2024-12-08 20:19:57.677685: 
2024-12-08 20:19:57.679665: Epoch 158
2024-12-08 20:19:57.680695: Current learning rate: 0.00857
2024-12-08 20:21:25.319333: Validation loss did not improve from -0.57870. Patience: 7/50
2024-12-08 20:21:25.320530: train_loss -0.7476
2024-12-08 20:21:25.321506: val_loss -0.5468
2024-12-08 20:21:25.322629: Pseudo dice [0.7517]
2024-12-08 20:21:25.323648: Epoch time: 87.64 s
2024-12-08 20:21:26.841594: 
2024-12-08 20:21:26.843587: Epoch 159
2024-12-08 20:21:26.845114: Current learning rate: 0.00856
2024-12-08 20:22:54.361175: Validation loss did not improve from -0.57870. Patience: 8/50
2024-12-08 20:22:54.362345: train_loss -0.7535
2024-12-08 20:22:54.363467: val_loss -0.549
2024-12-08 20:22:54.364334: Pseudo dice [0.7415]
2024-12-08 20:22:54.365118: Epoch time: 87.52 s
2024-12-08 20:22:55.979198: 
2024-12-08 20:22:55.981499: Epoch 160
2024-12-08 20:22:55.983275: Current learning rate: 0.00855
2024-12-08 20:24:23.438030: Validation loss improved from -0.57870 to -0.59805! Patience: 8/50
2024-12-08 20:24:23.439099: train_loss -0.7493
2024-12-08 20:24:23.440500: val_loss -0.5981
2024-12-08 20:24:23.441328: Pseudo dice [0.7832]
2024-12-08 20:24:23.442108: Epoch time: 87.46 s
2024-12-08 20:24:23.442948: Yayy! New best EMA pseudo Dice: 0.7483
2024-12-08 20:24:25.113127: 
2024-12-08 20:24:25.116246: Epoch 161
2024-12-08 20:24:25.117347: Current learning rate: 0.00854
2024-12-08 20:25:52.734231: Validation loss did not improve from -0.59805. Patience: 1/50
2024-12-08 20:25:52.736647: train_loss -0.752
2024-12-08 20:25:52.738539: val_loss -0.528
2024-12-08 20:25:52.739419: Pseudo dice [0.7423]
2024-12-08 20:25:52.740635: Epoch time: 87.62 s
2024-12-08 20:25:54.012748: 
2024-12-08 20:25:54.014834: Epoch 162
2024-12-08 20:25:54.016005: Current learning rate: 0.00853
2024-12-08 20:27:21.594247: Validation loss did not improve from -0.59805. Patience: 2/50
2024-12-08 20:27:21.595738: train_loss -0.7459
2024-12-08 20:27:21.596867: val_loss -0.5487
2024-12-08 20:27:21.597686: Pseudo dice [0.7522]
2024-12-08 20:27:21.598542: Epoch time: 87.58 s
2024-12-08 20:27:22.853817: 
2024-12-08 20:27:22.855435: Epoch 163
2024-12-08 20:27:22.856508: Current learning rate: 0.00852
2024-12-08 20:28:50.480825: Validation loss did not improve from -0.59805. Patience: 3/50
2024-12-08 20:28:50.481690: train_loss -0.7547
2024-12-08 20:28:50.482645: val_loss -0.5311
2024-12-08 20:28:50.483551: Pseudo dice [0.7425]
2024-12-08 20:28:50.484721: Epoch time: 87.63 s
2024-12-08 20:28:51.789279: 
2024-12-08 20:28:51.791643: Epoch 164
2024-12-08 20:28:51.792856: Current learning rate: 0.00851
2024-12-08 20:30:19.497208: Validation loss did not improve from -0.59805. Patience: 4/50
2024-12-08 20:30:19.498285: train_loss -0.7547
2024-12-08 20:30:19.499451: val_loss -0.4943
2024-12-08 20:30:19.500434: Pseudo dice [0.7151]
2024-12-08 20:30:19.501252: Epoch time: 87.71 s
2024-12-08 20:30:21.083231: 
2024-12-08 20:30:21.085249: Epoch 165
2024-12-08 20:30:21.086383: Current learning rate: 0.0085
2024-12-08 20:31:48.717351: Validation loss did not improve from -0.59805. Patience: 5/50
2024-12-08 20:31:48.718882: train_loss -0.7477
2024-12-08 20:31:48.720054: val_loss -0.5244
2024-12-08 20:31:48.721005: Pseudo dice [0.7388]
2024-12-08 20:31:48.722110: Epoch time: 87.64 s
2024-12-08 20:31:49.934187: 
2024-12-08 20:31:49.936472: Epoch 166
2024-12-08 20:31:49.937830: Current learning rate: 0.00849
2024-12-08 20:33:17.577877: Validation loss did not improve from -0.59805. Patience: 6/50
2024-12-08 20:33:17.579126: train_loss -0.7472
2024-12-08 20:33:17.580567: val_loss -0.544
2024-12-08 20:33:17.581811: Pseudo dice [0.7464]
2024-12-08 20:33:17.582843: Epoch time: 87.65 s
2024-12-08 20:33:19.134698: 
2024-12-08 20:33:19.136464: Epoch 167
2024-12-08 20:33:19.137521: Current learning rate: 0.00848
2024-12-08 20:34:46.832681: Validation loss did not improve from -0.59805. Patience: 7/50
2024-12-08 20:34:46.833679: train_loss -0.7542
2024-12-08 20:34:46.834656: val_loss -0.5456
2024-12-08 20:34:46.835563: Pseudo dice [0.7508]
2024-12-08 20:34:46.836527: Epoch time: 87.7 s
2024-12-08 20:34:48.079304: 
2024-12-08 20:34:48.081297: Epoch 168
2024-12-08 20:34:48.082780: Current learning rate: 0.00847
2024-12-08 20:36:15.700958: Validation loss did not improve from -0.59805. Patience: 8/50
2024-12-08 20:36:15.702224: train_loss -0.7549
2024-12-08 20:36:15.703483: val_loss -0.5705
2024-12-08 20:36:15.704403: Pseudo dice [0.756]
2024-12-08 20:36:15.705425: Epoch time: 87.62 s
2024-12-08 20:36:16.953724: 
2024-12-08 20:36:16.955581: Epoch 169
2024-12-08 20:36:16.956643: Current learning rate: 0.00847
2024-12-08 20:37:44.736719: Validation loss did not improve from -0.59805. Patience: 9/50
2024-12-08 20:37:44.738024: train_loss -0.7529
2024-12-08 20:37:44.739463: val_loss -0.5776
2024-12-08 20:37:44.740726: Pseudo dice [0.7651]
2024-12-08 20:37:44.741804: Epoch time: 87.79 s
2024-12-08 20:37:46.319721: 
2024-12-08 20:37:46.321784: Epoch 170
2024-12-08 20:37:46.322694: Current learning rate: 0.00846
2024-12-08 20:39:15.155969: Validation loss did not improve from -0.59805. Patience: 10/50
2024-12-08 20:39:15.161558: train_loss -0.7589
2024-12-08 20:39:15.165636: val_loss -0.5621
2024-12-08 20:39:15.166911: Pseudo dice [0.7599]
2024-12-08 20:39:15.169315: Epoch time: 88.84 s
2024-12-08 20:39:15.171072: Yayy! New best EMA pseudo Dice: 0.749
2024-12-08 20:39:16.944629: 
2024-12-08 20:39:16.946406: Epoch 171
2024-12-08 20:39:16.947607: Current learning rate: 0.00845
2024-12-08 20:40:44.938221: Validation loss did not improve from -0.59805. Patience: 11/50
2024-12-08 20:40:44.939356: train_loss -0.7584
2024-12-08 20:40:44.940320: val_loss -0.5255
2024-12-08 20:40:44.940999: Pseudo dice [0.7369]
2024-12-08 20:40:44.941952: Epoch time: 88.0 s
2024-12-08 20:40:46.182443: 
2024-12-08 20:40:46.184546: Epoch 172
2024-12-08 20:40:46.185662: Current learning rate: 0.00844
2024-12-08 20:42:14.268449: Validation loss did not improve from -0.59805. Patience: 12/50
2024-12-08 20:42:14.271106: train_loss -0.7578
2024-12-08 20:42:14.272375: val_loss -0.5533
2024-12-08 20:42:14.273380: Pseudo dice [0.7583]
2024-12-08 20:42:14.274459: Epoch time: 88.09 s
2024-12-08 20:42:15.585407: 
2024-12-08 20:42:15.587545: Epoch 173
2024-12-08 20:42:15.588659: Current learning rate: 0.00843
2024-12-08 20:43:43.475746: Validation loss did not improve from -0.59805. Patience: 13/50
2024-12-08 20:43:43.476786: train_loss -0.7567
2024-12-08 20:43:43.477912: val_loss -0.5619
2024-12-08 20:43:43.478871: Pseudo dice [0.7567]
2024-12-08 20:43:43.479989: Epoch time: 87.89 s
2024-12-08 20:43:43.480798: Yayy! New best EMA pseudo Dice: 0.7496
2024-12-08 20:43:45.123529: 
2024-12-08 20:43:45.125635: Epoch 174
2024-12-08 20:43:45.127109: Current learning rate: 0.00842
2024-12-08 20:45:12.959131: Validation loss did not improve from -0.59805. Patience: 14/50
2024-12-08 20:45:12.960227: train_loss -0.7633
2024-12-08 20:45:12.961265: val_loss -0.5345
2024-12-08 20:45:12.962153: Pseudo dice [0.7491]
2024-12-08 20:45:12.963108: Epoch time: 87.84 s
2024-12-08 20:45:14.566730: 
2024-12-08 20:45:14.569008: Epoch 175
2024-12-08 20:45:14.570363: Current learning rate: 0.00841
2024-12-08 20:46:42.251120: Validation loss did not improve from -0.59805. Patience: 15/50
2024-12-08 20:46:42.252363: train_loss -0.7624
2024-12-08 20:46:42.253856: val_loss -0.5457
2024-12-08 20:46:42.254914: Pseudo dice [0.744]
2024-12-08 20:46:42.255919: Epoch time: 87.69 s
2024-12-08 20:46:43.517762: 
2024-12-08 20:46:43.520237: Epoch 176
2024-12-08 20:46:43.521857: Current learning rate: 0.0084
2024-12-08 20:48:11.326854: Validation loss improved from -0.59805 to -0.59929! Patience: 15/50
2024-12-08 20:48:11.327951: train_loss -0.7592
2024-12-08 20:48:11.329018: val_loss -0.5993
2024-12-08 20:48:11.330051: Pseudo dice [0.7714]
2024-12-08 20:48:11.330859: Epoch time: 87.81 s
2024-12-08 20:48:11.331567: Yayy! New best EMA pseudo Dice: 0.7512
2024-12-08 20:48:14.339630: 
2024-12-08 20:48:14.341800: Epoch 177
2024-12-08 20:48:14.343024: Current learning rate: 0.00839
2024-12-08 20:49:42.405375: Validation loss did not improve from -0.59929. Patience: 1/50
2024-12-08 20:49:42.406662: train_loss -0.7577
2024-12-08 20:49:42.407837: val_loss -0.5328
2024-12-08 20:49:42.408705: Pseudo dice [0.7433]
2024-12-08 20:49:42.409462: Epoch time: 88.07 s
2024-12-08 20:49:43.652051: 
2024-12-08 20:49:43.654130: Epoch 178
2024-12-08 20:49:43.655614: Current learning rate: 0.00838
2024-12-08 20:51:11.233963: Validation loss did not improve from -0.59929. Patience: 2/50
2024-12-08 20:51:11.235408: train_loss -0.756
2024-12-08 20:51:11.236447: val_loss -0.5234
2024-12-08 20:51:11.237288: Pseudo dice [0.7402]
2024-12-08 20:51:11.238062: Epoch time: 87.58 s
2024-12-08 20:51:12.474700: 
2024-12-08 20:51:12.476694: Epoch 179
2024-12-08 20:51:12.477745: Current learning rate: 0.00837
2024-12-08 20:52:40.036587: Validation loss did not improve from -0.59929. Patience: 3/50
2024-12-08 20:52:40.037635: train_loss -0.7595
2024-12-08 20:52:40.038958: val_loss -0.5601
2024-12-08 20:52:40.039903: Pseudo dice [0.7625]
2024-12-08 20:52:40.040941: Epoch time: 87.56 s
2024-12-08 20:52:41.623899: 
2024-12-08 20:52:41.626053: Epoch 180
2024-12-08 20:52:41.627484: Current learning rate: 0.00836
2024-12-08 20:54:09.121509: Validation loss did not improve from -0.59929. Patience: 4/50
2024-12-08 20:54:09.122662: train_loss -0.7605
2024-12-08 20:54:09.124174: val_loss -0.5463
2024-12-08 20:54:09.125174: Pseudo dice [0.743]
2024-12-08 20:54:09.126390: Epoch time: 87.5 s
2024-12-08 20:54:10.380040: 
2024-12-08 20:54:10.382313: Epoch 181
2024-12-08 20:54:10.383389: Current learning rate: 0.00836
2024-12-08 20:55:37.838582: Validation loss did not improve from -0.59929. Patience: 5/50
2024-12-08 20:55:37.839958: train_loss -0.7509
2024-12-08 20:55:37.841276: val_loss -0.533
2024-12-08 20:55:37.842255: Pseudo dice [0.744]
2024-12-08 20:55:37.843076: Epoch time: 87.46 s
2024-12-08 20:55:39.092513: 
2024-12-08 20:55:39.094855: Epoch 182
2024-12-08 20:55:39.096009: Current learning rate: 0.00835
2024-12-08 20:57:06.272894: Validation loss did not improve from -0.59929. Patience: 6/50
2024-12-08 20:57:06.274304: train_loss -0.754
2024-12-08 20:57:06.275564: val_loss -0.5497
2024-12-08 20:57:06.276334: Pseudo dice [0.7475]
2024-12-08 20:57:06.277164: Epoch time: 87.18 s
2024-12-08 20:57:07.507174: 
2024-12-08 20:57:07.508955: Epoch 183
2024-12-08 20:57:07.510063: Current learning rate: 0.00834
2024-12-08 20:58:34.574514: Validation loss did not improve from -0.59929. Patience: 7/50
2024-12-08 20:58:34.575780: train_loss -0.7616
2024-12-08 20:58:34.576972: val_loss -0.548
2024-12-08 20:58:34.577716: Pseudo dice [0.7475]
2024-12-08 20:58:34.578461: Epoch time: 87.07 s
2024-12-08 20:58:35.819697: 
2024-12-08 20:58:35.821224: Epoch 184
2024-12-08 20:58:35.822322: Current learning rate: 0.00833
2024-12-08 21:00:02.987579: Validation loss did not improve from -0.59929. Patience: 8/50
2024-12-08 21:00:02.988271: train_loss -0.7578
2024-12-08 21:00:02.989156: val_loss -0.5747
2024-12-08 21:00:02.989919: Pseudo dice [0.7661]
2024-12-08 21:00:02.990659: Epoch time: 87.17 s
2024-12-08 21:00:04.592037: 
2024-12-08 21:00:04.594130: Epoch 185
2024-12-08 21:00:04.595526: Current learning rate: 0.00832
2024-12-08 21:01:31.762996: Validation loss did not improve from -0.59929. Patience: 9/50
2024-12-08 21:01:31.764471: train_loss -0.759
2024-12-08 21:01:31.765728: val_loss -0.5296
2024-12-08 21:01:31.766668: Pseudo dice [0.7403]
2024-12-08 21:01:31.767462: Epoch time: 87.17 s
2024-12-08 21:01:33.071016: 
2024-12-08 21:01:33.072465: Epoch 186
2024-12-08 21:01:33.073829: Current learning rate: 0.00831
2024-12-08 21:03:00.257056: Validation loss did not improve from -0.59929. Patience: 10/50
2024-12-08 21:03:00.258428: train_loss -0.7605
2024-12-08 21:03:00.259628: val_loss -0.5454
2024-12-08 21:03:00.260652: Pseudo dice [0.746]
2024-12-08 21:03:00.261693: Epoch time: 87.19 s
2024-12-08 21:03:01.531023: 
2024-12-08 21:03:01.533398: Epoch 187
2024-12-08 21:03:01.535000: Current learning rate: 0.0083
2024-12-08 21:04:28.680951: Validation loss did not improve from -0.59929. Patience: 11/50
2024-12-08 21:04:28.682065: train_loss -0.7657
2024-12-08 21:04:28.683574: val_loss -0.5352
2024-12-08 21:04:28.684884: Pseudo dice [0.7417]
2024-12-08 21:04:28.686207: Epoch time: 87.15 s
2024-12-08 21:04:30.273830: 
2024-12-08 21:04:30.275789: Epoch 188
2024-12-08 21:04:30.276935: Current learning rate: 0.00829
2024-12-08 21:05:57.427934: Validation loss did not improve from -0.59929. Patience: 12/50
2024-12-08 21:05:57.429145: train_loss -0.7611
2024-12-08 21:05:57.430001: val_loss -0.5561
2024-12-08 21:05:57.430636: Pseudo dice [0.7538]
2024-12-08 21:05:57.431400: Epoch time: 87.16 s
2024-12-08 21:05:58.688379: 
2024-12-08 21:05:58.689902: Epoch 189
2024-12-08 21:05:58.691120: Current learning rate: 0.00828
2024-12-08 21:07:25.898983: Validation loss did not improve from -0.59929. Patience: 13/50
2024-12-08 21:07:25.900235: train_loss -0.764
2024-12-08 21:07:25.901398: val_loss -0.5456
2024-12-08 21:07:25.902331: Pseudo dice [0.7555]
2024-12-08 21:07:25.903301: Epoch time: 87.21 s
2024-12-08 21:07:27.507746: 
2024-12-08 21:07:27.509422: Epoch 190
2024-12-08 21:07:27.510237: Current learning rate: 0.00827
2024-12-08 21:08:54.964365: Validation loss did not improve from -0.59929. Patience: 14/50
2024-12-08 21:08:54.965693: train_loss -0.7459
2024-12-08 21:08:54.966945: val_loss -0.457
2024-12-08 21:08:54.967828: Pseudo dice [0.7062]
2024-12-08 21:08:54.968714: Epoch time: 87.46 s
2024-12-08 21:08:56.226583: 
2024-12-08 21:08:56.228817: Epoch 191
2024-12-08 21:08:56.230405: Current learning rate: 0.00826
2024-12-08 21:10:23.729908: Validation loss did not improve from -0.59929. Patience: 15/50
2024-12-08 21:10:23.730729: train_loss -0.7313
2024-12-08 21:10:23.731657: val_loss -0.5336
2024-12-08 21:10:23.732679: Pseudo dice [0.7414]
2024-12-08 21:10:23.733342: Epoch time: 87.51 s
2024-12-08 21:10:25.003079: 
2024-12-08 21:10:25.005172: Epoch 192
2024-12-08 21:10:25.006349: Current learning rate: 0.00825
2024-12-08 21:11:52.521373: Validation loss did not improve from -0.59929. Patience: 16/50
2024-12-08 21:11:52.522814: train_loss -0.7526
2024-12-08 21:11:52.523995: val_loss -0.5378
2024-12-08 21:11:52.525323: Pseudo dice [0.7401]
2024-12-08 21:11:52.526098: Epoch time: 87.52 s
2024-12-08 21:11:53.847791: 
2024-12-08 21:11:53.849422: Epoch 193
2024-12-08 21:11:53.850732: Current learning rate: 0.00824
2024-12-08 21:13:21.313154: Validation loss did not improve from -0.59929. Patience: 17/50
2024-12-08 21:13:21.314001: train_loss -0.753
2024-12-08 21:13:21.314998: val_loss -0.5685
2024-12-08 21:13:21.315972: Pseudo dice [0.76]
2024-12-08 21:13:21.316907: Epoch time: 87.47 s
2024-12-08 21:13:22.602084: 
2024-12-08 21:13:22.603844: Epoch 194
2024-12-08 21:13:22.604728: Current learning rate: 0.00824
2024-12-08 21:14:49.960557: Validation loss did not improve from -0.59929. Patience: 18/50
2024-12-08 21:14:49.961664: train_loss -0.7645
2024-12-08 21:14:49.962784: val_loss -0.5438
2024-12-08 21:14:49.963795: Pseudo dice [0.7467]
2024-12-08 21:14:49.964621: Epoch time: 87.36 s
2024-12-08 21:14:51.601380: 
2024-12-08 21:14:51.603426: Epoch 195
2024-12-08 21:14:51.604592: Current learning rate: 0.00823
2024-12-08 21:16:19.013364: Validation loss did not improve from -0.59929. Patience: 19/50
2024-12-08 21:16:19.014503: train_loss -0.7667
2024-12-08 21:16:19.015413: val_loss -0.5545
2024-12-08 21:16:19.016063: Pseudo dice [0.7595]
2024-12-08 21:16:19.016790: Epoch time: 87.41 s
2024-12-08 21:16:20.282462: 
2024-12-08 21:16:20.285072: Epoch 196
2024-12-08 21:16:20.286118: Current learning rate: 0.00822
2024-12-08 21:17:47.781762: Validation loss did not improve from -0.59929. Patience: 20/50
2024-12-08 21:17:47.782944: train_loss -0.7688
2024-12-08 21:17:47.784338: val_loss -0.547
2024-12-08 21:17:47.785573: Pseudo dice [0.7479]
2024-12-08 21:17:47.786897: Epoch time: 87.5 s
2024-12-08 21:17:49.406028: 
2024-12-08 21:17:49.408037: Epoch 197
2024-12-08 21:17:49.409075: Current learning rate: 0.00821
2024-12-08 21:19:16.846569: Validation loss did not improve from -0.59929. Patience: 21/50
2024-12-08 21:19:16.847765: train_loss -0.7666
2024-12-08 21:19:16.849165: val_loss -0.5419
2024-12-08 21:19:16.850233: Pseudo dice [0.7496]
2024-12-08 21:19:16.851345: Epoch time: 87.44 s
2024-12-08 21:19:18.141186: 
2024-12-08 21:19:18.143795: Epoch 198
2024-12-08 21:19:18.145133: Current learning rate: 0.0082
2024-12-08 21:20:45.792128: Validation loss did not improve from -0.59929. Patience: 22/50
2024-12-08 21:20:45.793670: train_loss -0.7652
2024-12-08 21:20:45.794913: val_loss -0.5604
2024-12-08 21:20:45.795706: Pseudo dice [0.7569]
2024-12-08 21:20:45.796432: Epoch time: 87.65 s
2024-12-08 21:20:47.080905: 
2024-12-08 21:20:47.082788: Epoch 199
2024-12-08 21:20:47.084331: Current learning rate: 0.00819
2024-12-08 21:22:14.720110: Validation loss did not improve from -0.59929. Patience: 23/50
2024-12-08 21:22:14.721243: train_loss -0.7652
2024-12-08 21:22:14.722281: val_loss -0.5474
2024-12-08 21:22:14.723154: Pseudo dice [0.7489]
2024-12-08 21:22:14.724188: Epoch time: 87.64 s
2024-12-08 21:22:16.366696: 
2024-12-08 21:22:16.369093: Epoch 200
2024-12-08 21:22:16.370521: Current learning rate: 0.00818
2024-12-08 21:23:43.975476: Validation loss did not improve from -0.59929. Patience: 24/50
2024-12-08 21:23:43.976882: train_loss -0.7592
2024-12-08 21:23:43.978272: val_loss -0.5526
2024-12-08 21:23:43.979233: Pseudo dice [0.751]
2024-12-08 21:23:43.980297: Epoch time: 87.61 s
2024-12-08 21:23:45.274438: 
2024-12-08 21:23:45.276631: Epoch 201
2024-12-08 21:23:45.278248: Current learning rate: 0.00817
2024-12-08 21:25:12.934477: Validation loss did not improve from -0.59929. Patience: 25/50
2024-12-08 21:25:12.935856: train_loss -0.7527
2024-12-08 21:25:12.937191: val_loss -0.5285
2024-12-08 21:25:12.938190: Pseudo dice [0.7459]
2024-12-08 21:25:12.939014: Epoch time: 87.66 s
2024-12-08 21:25:14.232068: 
2024-12-08 21:25:14.234193: Epoch 202
2024-12-08 21:25:14.235317: Current learning rate: 0.00816
2024-12-08 21:26:41.706171: Validation loss did not improve from -0.59929. Patience: 26/50
2024-12-08 21:26:41.707413: train_loss -0.764
2024-12-08 21:26:41.708617: val_loss -0.5396
2024-12-08 21:26:41.709535: Pseudo dice [0.7429]
2024-12-08 21:26:41.710630: Epoch time: 87.48 s
2024-12-08 21:26:42.986108: 
2024-12-08 21:26:42.988310: Epoch 203
2024-12-08 21:26:42.989593: Current learning rate: 0.00815
2024-12-08 21:28:10.359903: Validation loss did not improve from -0.59929. Patience: 27/50
2024-12-08 21:28:10.361142: train_loss -0.7684
2024-12-08 21:28:10.362505: val_loss -0.5236
2024-12-08 21:28:10.363566: Pseudo dice [0.7344]
2024-12-08 21:28:10.364648: Epoch time: 87.38 s
2024-12-08 21:28:11.640196: 
2024-12-08 21:28:11.642341: Epoch 204
2024-12-08 21:28:11.643563: Current learning rate: 0.00814
2024-12-08 21:29:39.053627: Validation loss did not improve from -0.59929. Patience: 28/50
2024-12-08 21:29:39.057188: train_loss -0.7627
2024-12-08 21:29:39.058679: val_loss -0.5326
2024-12-08 21:29:39.059467: Pseudo dice [0.7453]
2024-12-08 21:29:39.060382: Epoch time: 87.42 s
2024-12-08 21:29:40.694667: 
2024-12-08 21:29:40.697121: Epoch 205
2024-12-08 21:29:40.698555: Current learning rate: 0.00813
2024-12-08 21:31:08.062718: Validation loss did not improve from -0.59929. Patience: 29/50
2024-12-08 21:31:08.063620: train_loss -0.7623
2024-12-08 21:31:08.064746: val_loss -0.5335
2024-12-08 21:31:08.065623: Pseudo dice [0.735]
2024-12-08 21:31:08.066645: Epoch time: 87.37 s
2024-12-08 21:31:09.253238: 
2024-12-08 21:31:09.255111: Epoch 206
2024-12-08 21:31:09.256336: Current learning rate: 0.00813
2024-12-08 21:32:36.654333: Validation loss did not improve from -0.59929. Patience: 30/50
2024-12-08 21:32:36.655427: train_loss -0.7627
2024-12-08 21:32:36.656380: val_loss -0.5424
2024-12-08 21:32:36.657783: Pseudo dice [0.742]
2024-12-08 21:32:36.658885: Epoch time: 87.4 s
2024-12-08 21:32:37.885292: 
2024-12-08 21:32:37.887422: Epoch 207
2024-12-08 21:32:37.889194: Current learning rate: 0.00812
2024-12-08 21:34:05.276564: Validation loss did not improve from -0.59929. Patience: 31/50
2024-12-08 21:34:05.277793: train_loss -0.7709
2024-12-08 21:34:05.279121: val_loss -0.5804
2024-12-08 21:34:05.280087: Pseudo dice [0.758]
2024-12-08 21:34:05.280885: Epoch time: 87.39 s
2024-12-08 21:34:06.922066: 
2024-12-08 21:34:06.924230: Epoch 208
2024-12-08 21:34:06.925447: Current learning rate: 0.00811
2024-12-08 21:35:34.820750: Validation loss did not improve from -0.59929. Patience: 32/50
2024-12-08 21:35:34.821908: train_loss -0.7698
2024-12-08 21:35:34.823049: val_loss -0.5226
2024-12-08 21:35:34.823742: Pseudo dice [0.7315]
2024-12-08 21:35:34.824439: Epoch time: 87.9 s
2024-12-08 21:35:36.069555: 
2024-12-08 21:35:36.071562: Epoch 209
2024-12-08 21:35:36.072656: Current learning rate: 0.0081
2024-12-08 21:37:03.445822: Validation loss did not improve from -0.59929. Patience: 33/50
2024-12-08 21:37:03.446959: train_loss -0.7693
2024-12-08 21:37:03.448067: val_loss -0.5564
2024-12-08 21:37:03.449002: Pseudo dice [0.7464]
2024-12-08 21:37:03.450042: Epoch time: 87.38 s
2024-12-08 21:37:05.001704: 
2024-12-08 21:37:05.003492: Epoch 210
2024-12-08 21:37:05.004875: Current learning rate: 0.00809
2024-12-08 21:38:32.571814: Validation loss did not improve from -0.59929. Patience: 34/50
2024-12-08 21:38:32.573281: train_loss -0.7671
2024-12-08 21:38:32.574478: val_loss -0.5634
2024-12-08 21:38:32.575581: Pseudo dice [0.7529]
2024-12-08 21:38:32.576626: Epoch time: 87.57 s
2024-12-08 21:38:33.777133: 
2024-12-08 21:38:33.778817: Epoch 211
2024-12-08 21:38:33.779835: Current learning rate: 0.00808
2024-12-08 21:40:03.477625: Validation loss did not improve from -0.59929. Patience: 35/50
2024-12-08 21:40:03.480765: train_loss -0.7697
2024-12-08 21:40:03.483475: val_loss -0.5339
2024-12-08 21:40:03.484273: Pseudo dice [0.742]
2024-12-08 21:40:03.485315: Epoch time: 89.7 s
2024-12-08 21:40:04.849909: 
2024-12-08 21:40:04.851957: Epoch 212
2024-12-08 21:40:04.853247: Current learning rate: 0.00807
2024-12-08 21:41:32.631433: Validation loss did not improve from -0.59929. Patience: 36/50
2024-12-08 21:41:32.636134: train_loss -0.7638
2024-12-08 21:41:32.638809: val_loss -0.5498
2024-12-08 21:41:32.640146: Pseudo dice [0.7539]
2024-12-08 21:41:32.641332: Epoch time: 87.79 s
2024-12-08 21:41:33.861468: 
2024-12-08 21:41:33.863717: Epoch 213
2024-12-08 21:41:33.864759: Current learning rate: 0.00806
2024-12-08 21:43:01.531993: Validation loss did not improve from -0.59929. Patience: 37/50
2024-12-08 21:43:01.533232: train_loss -0.7677
2024-12-08 21:43:01.535200: val_loss -0.5431
2024-12-08 21:43:01.535951: Pseudo dice [0.7449]
2024-12-08 21:43:01.536716: Epoch time: 87.67 s
2024-12-08 21:43:02.735892: 
2024-12-08 21:43:02.738006: Epoch 214
2024-12-08 21:43:02.738963: Current learning rate: 0.00805
2024-12-08 21:44:30.572302: Validation loss did not improve from -0.59929. Patience: 38/50
2024-12-08 21:44:30.575053: train_loss -0.7647
2024-12-08 21:44:30.576372: val_loss -0.5594
2024-12-08 21:44:30.577088: Pseudo dice [0.7579]
2024-12-08 21:44:30.578683: Epoch time: 87.84 s
2024-12-08 21:44:32.196782: 
2024-12-08 21:44:32.199053: Epoch 215
2024-12-08 21:44:32.200539: Current learning rate: 0.00804
2024-12-08 21:45:59.781694: Validation loss did not improve from -0.59929. Patience: 39/50
2024-12-08 21:45:59.782884: train_loss -0.7668
2024-12-08 21:45:59.783825: val_loss -0.534
2024-12-08 21:45:59.784507: Pseudo dice [0.7348]
2024-12-08 21:45:59.785343: Epoch time: 87.59 s
2024-12-08 21:46:01.000463: 
2024-12-08 21:46:01.002436: Epoch 216
2024-12-08 21:46:01.003350: Current learning rate: 0.00803
2024-12-08 21:47:28.574999: Validation loss did not improve from -0.59929. Patience: 40/50
2024-12-08 21:47:28.576382: train_loss -0.7666
2024-12-08 21:47:28.577485: val_loss -0.5499
2024-12-08 21:47:28.578294: Pseudo dice [0.7548]
2024-12-08 21:47:28.579117: Epoch time: 87.58 s
2024-12-08 21:47:29.784182: 
2024-12-08 21:47:29.786860: Epoch 217
2024-12-08 21:47:29.788334: Current learning rate: 0.00802
2024-12-08 21:48:57.394183: Validation loss did not improve from -0.59929. Patience: 41/50
2024-12-08 21:48:57.395637: train_loss -0.7694
2024-12-08 21:48:57.397095: val_loss -0.5727
2024-12-08 21:48:57.397937: Pseudo dice [0.7577]
2024-12-08 21:48:57.398818: Epoch time: 87.61 s
2024-12-08 21:48:58.587793: 
2024-12-08 21:48:58.589987: Epoch 218
2024-12-08 21:48:58.591207: Current learning rate: 0.00801
2024-12-08 21:50:26.182385: Validation loss did not improve from -0.59929. Patience: 42/50
2024-12-08 21:50:26.183721: train_loss -0.7707
2024-12-08 21:50:26.184733: val_loss -0.531
2024-12-08 21:50:26.185419: Pseudo dice [0.7437]
2024-12-08 21:50:26.186243: Epoch time: 87.6 s
2024-12-08 21:50:28.242705: 
2024-12-08 21:50:28.244982: Epoch 219
2024-12-08 21:50:28.246279: Current learning rate: 0.00801
2024-12-08 21:51:56.250564: Validation loss did not improve from -0.59929. Patience: 43/50
2024-12-08 21:51:56.251612: train_loss -0.7584
2024-12-08 21:51:56.252501: val_loss -0.5178
2024-12-08 21:51:56.253626: Pseudo dice [0.7288]
2024-12-08 21:51:56.254544: Epoch time: 88.01 s
2024-12-08 21:51:57.807477: 
2024-12-08 21:51:57.809475: Epoch 220
2024-12-08 21:51:57.810910: Current learning rate: 0.008
2024-12-08 21:53:25.681237: Validation loss did not improve from -0.59929. Patience: 44/50
2024-12-08 21:53:25.682468: train_loss -0.77
2024-12-08 21:53:25.683532: val_loss -0.5463
2024-12-08 21:53:25.684381: Pseudo dice [0.7404]
2024-12-08 21:53:25.685291: Epoch time: 87.88 s
2024-12-08 21:53:26.864199: 
2024-12-08 21:53:26.866091: Epoch 221
2024-12-08 21:53:26.867188: Current learning rate: 0.00799
2024-12-08 21:54:54.171132: Validation loss did not improve from -0.59929. Patience: 45/50
2024-12-08 21:54:54.172495: train_loss -0.7615
2024-12-08 21:54:54.173889: val_loss -0.5366
2024-12-08 21:54:54.175151: Pseudo dice [0.7424]
2024-12-08 21:54:54.176255: Epoch time: 87.31 s
2024-12-08 21:54:55.376662: 
2024-12-08 21:54:55.378661: Epoch 222
2024-12-08 21:54:55.379797: Current learning rate: 0.00798
2024-12-08 21:56:22.652424: Validation loss did not improve from -0.59929. Patience: 46/50
2024-12-08 21:56:22.653542: train_loss -0.7684
2024-12-08 21:56:22.654423: val_loss -0.548
2024-12-08 21:56:22.655240: Pseudo dice [0.7473]
2024-12-08 21:56:22.656434: Epoch time: 87.28 s
2024-12-08 21:56:23.843913: 
2024-12-08 21:56:23.846072: Epoch 223
2024-12-08 21:56:23.847154: Current learning rate: 0.00797
2024-12-08 21:57:51.158785: Validation loss did not improve from -0.59929. Patience: 47/50
2024-12-08 21:57:51.160016: train_loss -0.7713
2024-12-08 21:57:51.161103: val_loss -0.5505
2024-12-08 21:57:51.162035: Pseudo dice [0.75]
2024-12-08 21:57:51.163159: Epoch time: 87.32 s
2024-12-08 21:57:52.346109: 
2024-12-08 21:57:52.348027: Epoch 224
2024-12-08 21:57:52.348793: Current learning rate: 0.00796
2024-12-08 21:59:19.606415: Validation loss did not improve from -0.59929. Patience: 48/50
2024-12-08 21:59:19.607787: train_loss -0.7764
2024-12-08 21:59:19.609029: val_loss -0.5639
2024-12-08 21:59:19.609872: Pseudo dice [0.762]
2024-12-08 21:59:19.610668: Epoch time: 87.26 s
2024-12-08 21:59:21.143487: 
2024-12-08 21:59:21.145831: Epoch 225
2024-12-08 21:59:21.147185: Current learning rate: 0.00795
2024-12-08 22:00:48.447003: Validation loss did not improve from -0.59929. Patience: 49/50
2024-12-08 22:00:48.447776: train_loss -0.7716
2024-12-08 22:00:48.448922: val_loss -0.5581
2024-12-08 22:00:48.450190: Pseudo dice [0.753]
2024-12-08 22:00:48.451406: Epoch time: 87.31 s
2024-12-08 22:00:49.710413: 
2024-12-08 22:00:49.714242: Epoch 226
2024-12-08 22:00:49.716217: Current learning rate: 0.00794
2024-12-08 22:02:17.160075: Validation loss did not improve from -0.59929. Patience: 50/50
2024-12-08 22:02:17.161341: train_loss -0.7728
2024-12-08 22:02:17.162442: val_loss -0.5121
2024-12-08 22:02:17.163366: Pseudo dice [0.7273]
2024-12-08 22:02:17.164259: Epoch time: 87.45 s
2024-12-08 22:02:18.375521: Patience reached. Stopping training.
2024-12-08 22:02:18.750338: Training done.
2024-12-08 22:02:19.056428: Using splits from existing split file: /home/gridsan/nchutisilp/datasets/nnUNet_Datasets/nnUNet_preprocessed/Dataset309_Sohee_Calcium_OCT_CrossValidation/splits_final.json
2024-12-08 22:02:19.059541: The split file contains 5 splits.
2024-12-08 22:02:19.061120: Desired fold for training: 1
2024-12-08 22:02:19.062178: This split has 6 training and 2 validation cases.
2024-12-08 22:02:19.063324: predicting 101-019
2024-12-08 22:02:19.091259: 101-019, shape torch.Size([1, 375, 498, 498]), rank 0
2024-12-08 22:04:03.052380: predicting 401-004
2024-12-08 22:04:03.074990: 401-004, shape torch.Size([1, 375, 498, 498]), rank 0
2024-12-08 22:05:56.553457: Validation complete
2024-12-08 22:05:56.554413: Mean Validation Dice:  0.7251662810809496

############################
INFO: You are using the old nnU-Net default plans. We have updated our recommendations. Please consider using those instead! Read more here: https://github.com/MIC-DKFZ/nnUNet/blob/master/documentation/resenc_presets.md
############################

Using device: cuda:0

#######################################################################
Please cite the following paper when using nnU-Net:
Isensee, F., Jaeger, P. F., Kohl, S. A., Petersen, J., & Maier-Hein, K. H. (2021). nnU-Net: a self-configuring method for deep learning-based biomedical image segmentation. Nature methods, 18(2), 203-211.
#######################################################################

2024-12-08 22:06:07.012898: Using torch.compile...

############################
INFO: You are using the old nnU-Net default plans. We have updated our recommendations. Please consider using those instead! Read more here: https://github.com/MIC-DKFZ/nnUNet/blob/master/documentation/resenc_presets.md
############################

Using device: cuda:0

#######################################################################
Please cite the following paper when using nnU-Net:
Isensee, F., Jaeger, P. F., Kohl, S. A., Petersen, J., & Maier-Hein, K. H. (2021). nnU-Net: a self-configuring method for deep learning-based biomedical image segmentation. Nature methods, 18(2), 203-211.
#######################################################################

2024-12-08 22:06:07.012565: Using torch.compile...
/home/gridsan/nchutisilp/.local/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn("The verbose parameter is deprecated. Please use get_last_lr() "
/home/gridsan/nchutisilp/.local/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn("The verbose parameter is deprecated. Please use get_last_lr() "
################### Loading pretrained weights from file  /home/gridsan/nchutisilp/projects/OpenAI-CLIP/clip_preivl_poststent_pretrained_nnUNet.pt ###################
Below is the list of overlapping blocks in pretrained model and nnUNet architecture:
encoder.stages.0.0.convs.0.conv.weight shape torch.Size([32, 1, 3, 3, 3])
encoder.stages.0.0.convs.0.conv.bias shape torch.Size([32])
encoder.stages.0.0.convs.0.norm.weight shape torch.Size([32])
encoder.stages.0.0.convs.0.norm.bias shape torch.Size([32])
encoder.stages.0.0.convs.0.all_modules.0.weight shape torch.Size([32, 1, 3, 3, 3])
encoder.stages.0.0.convs.0.all_modules.0.bias shape torch.Size([32])
encoder.stages.0.0.convs.0.all_modules.1.weight shape torch.Size([32])
encoder.stages.0.0.convs.0.all_modules.1.bias shape torch.Size([32])
encoder.stages.0.0.convs.1.conv.weight shape torch.Size([32, 32, 3, 3, 3])
encoder.stages.0.0.convs.1.conv.bias shape torch.Size([32])
encoder.stages.0.0.convs.1.norm.weight shape torch.Size([32])
encoder.stages.0.0.convs.1.norm.bias shape torch.Size([32])
encoder.stages.0.0.convs.1.all_modules.0.weight shape torch.Size([32, 32, 3, 3, 3])
encoder.stages.0.0.convs.1.all_modules.0.bias shape torch.Size([32])
encoder.stages.0.0.convs.1.all_modules.1.weight shape torch.Size([32])
encoder.stages.0.0.convs.1.all_modules.1.bias shape torch.Size([32])
encoder.stages.1.0.convs.0.conv.weight shape torch.Size([64, 32, 3, 3, 3])
encoder.stages.1.0.convs.0.conv.bias shape torch.Size([64])
encoder.stages.1.0.convs.0.norm.weight shape torch.Size([64])
encoder.stages.1.0.convs.0.norm.bias shape torch.Size([64])
encoder.stages.1.0.convs.0.all_modules.0.weight shape torch.Size([64, 32, 3, 3, 3])
encoder.stages.1.0.convs.0.all_modules.0.bias shape torch.Size([64])
encoder.stages.1.0.convs.0.all_modules.1.weight shape torch.Size([64])
encoder.stages.1.0.convs.0.all_modules.1.bias shape torch.Size([64])
encoder.stages.1.0.convs.1.conv.weight shape torch.Size([64, 64, 3, 3, 3])
encoder.stages.1.0.convs.1.conv.bias shape torch.Size([64])
encoder.stages.1.0.convs.1.norm.weight shape torch.Size([64])
encoder.stages.1.0.convs.1.norm.bias shape torch.Size([64])
encoder.stages.1.0.convs.1.all_modules.0.weight shape torch.Size([64, 64, 3, 3, 3])
encoder.stages.1.0.convs.1.all_modules.0.bias shape torch.Size([64])
encoder.stages.1.0.convs.1.all_modules.1.weight shape torch.Size([64])
encoder.stages.1.0.convs.1.all_modules.1.bias shape torch.Size([64])
encoder.stages.2.0.convs.0.conv.weight shape torch.Size([128, 64, 3, 3, 3])
encoder.stages.2.0.convs.0.conv.bias shape torch.Size([128])
encoder.stages.2.0.convs.0.norm.weight shape torch.Size([128])
encoder.stages.2.0.convs.0.norm.bias shape torch.Size([128])
encoder.stages.2.0.convs.0.all_modules.0.weight shape torch.Size([128, 64, 3, 3, 3])
encoder.stages.2.0.convs.0.all_modules.0.bias shape torch.Size([128])
encoder.stages.2.0.convs.0.all_modules.1.weight shape torch.Size([128])
encoder.stages.2.0.convs.0.all_modules.1.bias shape torch.Size([128])
encoder.stages.2.0.convs.1.conv.weight shape torch.Size([128, 128, 3, 3, 3])
encoder.stages.2.0.convs.1.conv.bias shape torch.Size([128])
encoder.stages.2.0.convs.1.norm.weight shape torch.Size([128])
encoder.stages.2.0.convs.1.norm.bias shape torch.Size([128])
encoder.stages.2.0.convs.1.all_modules.0.weight shape torch.Size([128, 128, 3, 3, 3])
encoder.stages.2.0.convs.1.all_modules.0.bias shape torch.Size([128])
encoder.stages.2.0.convs.1.all_modules.1.weight shape torch.Size([128])
encoder.stages.2.0.convs.1.all_modules.1.bias shape torch.Size([128])
encoder.stages.3.0.convs.0.conv.weight shape torch.Size([256, 128, 3, 3, 3])
encoder.stages.3.0.convs.0.conv.bias shape torch.Size([256])
encoder.stages.3.0.convs.0.norm.weight shape torch.Size([256])
encoder.stages.3.0.convs.0.norm.bias shape torch.Size([256])
encoder.stages.3.0.convs.0.all_modules.0.weight shape torch.Size([256, 128, 3, 3, 3])
encoder.stages.3.0.convs.0.all_modules.0.bias shape torch.Size([256])
encoder.stages.3.0.convs.0.all_modules.1.weight shape torch.Size([256])
encoder.stages.3.0.convs.0.all_modules.1.bias shape torch.Size([256])
encoder.stages.3.0.convs.1.conv.weight shape torch.Size([256, 256, 3, 3, 3])
encoder.stages.3.0.convs.1.conv.bias shape torch.Size([256])
encoder.stages.3.0.convs.1.norm.weight shape torch.Size([256])
encoder.stages.3.0.convs.1.norm.bias shape torch.Size([256])
encoder.stages.3.0.convs.1.all_modules.0.weight shape torch.Size([256, 256, 3, 3, 3])
encoder.stages.3.0.convs.1.all_modules.0.bias shape torch.Size([256])
encoder.stages.3.0.convs.1.all_modules.1.weight shape torch.Size([256])
encoder.stages.3.0.convs.1.all_modules.1.bias shape torch.Size([256])
encoder.stages.4.0.convs.0.conv.weight shape torch.Size([320, 256, 3, 3, 3])
encoder.stages.4.0.convs.0.conv.bias shape torch.Size([320])
encoder.stages.4.0.convs.0.norm.weight shape torch.Size([320])
encoder.stages.4.0.convs.0.norm.bias shape torch.Size([320])
encoder.stages.4.0.convs.0.all_modules.0.weight shape torch.Size([320, 256, 3, 3, 3])
encoder.stages.4.0.convs.0.all_modules.0.bias shape torch.Size([320])
encoder.stages.4.0.convs.0.all_modules.1.weight shape torch.Size([320])
encoder.stages.4.0.convs.0.all_modules.1.bias shape torch.Size([320])
encoder.stages.4.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.4.0.convs.1.conv.bias shape torch.Size([320])
encoder.stages.4.0.convs.1.norm.weight shape torch.Size([320])
encoder.stages.4.0.convs.1.norm.bias shape torch.Size([320])
encoder.stages.4.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.4.0.convs.1.all_modules.0.bias shape torch.Size([320])
encoder.stages.4.0.convs.1.all_modules.1.weight shape torch.Size([320])
encoder.stages.4.0.convs.1.all_modules.1.bias shape torch.Size([320])
encoder.stages.5.0.convs.0.conv.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.5.0.convs.0.conv.bias shape torch.Size([320])
encoder.stages.5.0.convs.0.norm.weight shape torch.Size([320])
encoder.stages.5.0.convs.0.norm.bias shape torch.Size([320])
encoder.stages.5.0.convs.0.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.5.0.convs.0.all_modules.0.bias shape torch.Size([320])
encoder.stages.5.0.convs.0.all_modules.1.weight shape torch.Size([320])
encoder.stages.5.0.convs.0.all_modules.1.bias shape torch.Size([320])
encoder.stages.5.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.5.0.convs.1.conv.bias shape torch.Size([320])
encoder.stages.5.0.convs.1.norm.weight shape torch.Size([320])
encoder.stages.5.0.convs.1.norm.bias shape torch.Size([320])
encoder.stages.5.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.5.0.convs.1.all_modules.0.bias shape torch.Size([320])
encoder.stages.5.0.convs.1.all_modules.1.weight shape torch.Size([320])
encoder.stages.5.0.convs.1.all_modules.1.bias shape torch.Size([320])
decoder.encoder.stages.0.0.convs.0.conv.weight shape torch.Size([32, 1, 3, 3, 3])
decoder.encoder.stages.0.0.convs.0.conv.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.norm.weight shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.norm.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.all_modules.0.weight shape torch.Size([32, 1, 3, 3, 3])
decoder.encoder.stages.0.0.convs.0.all_modules.0.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.all_modules.1.weight shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.all_modules.1.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.conv.weight shape torch.Size([32, 32, 3, 3, 3])
decoder.encoder.stages.0.0.convs.1.conv.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.norm.weight shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.norm.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.all_modules.0.weight shape torch.Size([32, 32, 3, 3, 3])
decoder.encoder.stages.0.0.convs.1.all_modules.0.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.all_modules.1.weight shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.all_modules.1.bias shape torch.Size([32])
decoder.encoder.stages.1.0.convs.0.conv.weight shape torch.Size([64, 32, 3, 3, 3])
decoder.encoder.stages.1.0.convs.0.conv.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.norm.weight shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.norm.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.all_modules.0.weight shape torch.Size([64, 32, 3, 3, 3])
decoder.encoder.stages.1.0.convs.0.all_modules.0.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.all_modules.1.weight shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.all_modules.1.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.conv.weight shape torch.Size([64, 64, 3, 3, 3])
decoder.encoder.stages.1.0.convs.1.conv.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.norm.weight shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.norm.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.all_modules.0.weight shape torch.Size([64, 64, 3, 3, 3])
decoder.encoder.stages.1.0.convs.1.all_modules.0.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.all_modules.1.weight shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.all_modules.1.bias shape torch.Size([64])
decoder.encoder.stages.2.0.convs.0.conv.weight shape torch.Size([128, 64, 3, 3, 3])
decoder.encoder.stages.2.0.convs.0.conv.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.norm.weight shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.norm.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.all_modules.0.weight shape torch.Size([128, 64, 3, 3, 3])
decoder.encoder.stages.2.0.convs.0.all_modules.0.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.all_modules.1.weight shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.all_modules.1.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.conv.weight shape torch.Size([128, 128, 3, 3, 3])
decoder.encoder.stages.2.0.convs.1.conv.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.norm.weight shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.norm.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.all_modules.0.weight shape torch.Size([128, 128, 3, 3, 3])
decoder.encoder.stages.2.0.convs.1.all_modules.0.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.all_modules.1.weight shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.all_modules.1.bias shape torch.Size([128])
decoder.encoder.stages.3.0.convs.0.conv.weight shape torch.Size([256, 128, 3, 3, 3])
decoder.encoder.stages.3.0.convs.0.conv.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.norm.weight shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.norm.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.all_modules.0.weight shape torch.Size([256, 128, 3, 3, 3])
decoder.encoder.stages.3.0.convs.0.all_modules.0.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.all_modules.1.weight shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.all_modules.1.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.conv.weight shape torch.Size([256, 256, 3, 3, 3])
decoder.encoder.stages.3.0.convs.1.conv.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.norm.weight shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.norm.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.all_modules.0.weight shape torch.Size([256, 256, 3, 3, 3])
decoder.encoder.stages.3.0.convs.1.all_modules.0.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.all_modules.1.weight shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.all_modules.1.bias shape torch.Size([256])
decoder.encoder.stages.4.0.convs.0.conv.weight shape torch.Size([320, 256, 3, 3, 3])
decoder.encoder.stages.4.0.convs.0.conv.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.norm.weight shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.norm.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.all_modules.0.weight shape torch.Size([320, 256, 3, 3, 3])
decoder.encoder.stages.4.0.convs.0.all_modules.0.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.all_modules.1.weight shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.all_modules.1.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.4.0.convs.1.conv.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.norm.weight shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.norm.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.4.0.convs.1.all_modules.0.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.all_modules.1.weight shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.all_modules.1.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.conv.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.5.0.convs.0.conv.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.norm.weight shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.norm.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.5.0.convs.0.all_modules.0.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.all_modules.1.weight shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.all_modules.1.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.5.0.convs.1.conv.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.norm.weight shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.norm.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.5.0.convs.1.all_modules.0.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.all_modules.1.weight shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.all_modules.1.bias shape torch.Size([320])
decoder.stages.0.convs.0.conv.weight shape torch.Size([320, 640, 3, 3, 3])
decoder.stages.0.convs.0.conv.bias shape torch.Size([320])
decoder.stages.0.convs.0.norm.weight shape torch.Size([320])
decoder.stages.0.convs.0.norm.bias shape torch.Size([320])
decoder.stages.0.convs.0.all_modules.0.weight shape torch.Size([320, 640, 3, 3, 3])
decoder.stages.0.convs.0.all_modules.0.bias shape torch.Size([320])
decoder.stages.0.convs.0.all_modules.1.weight shape torch.Size([320])
decoder.stages.0.convs.0.all_modules.1.bias shape torch.Size([320])
decoder.stages.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.stages.0.convs.1.conv.bias shape torch.Size([320])
decoder.stages.0.convs.1.norm.weight shape torch.Size([320])
decoder.stages.0.convs.1.norm.bias shape torch.Size([320])
decoder.stages.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.stages.0.convs.1.all_modules.0.bias shape torch.Size([320])
decoder.stages.0.convs.1.all_modules.1.weight shape torch.Size([320])
decoder.stages.0.convs.1.all_modules.1.bias shape torch.Size([320])
decoder.stages.1.convs.0.conv.weight shape torch.Size([256, 512, 3, 3, 3])
decoder.stages.1.convs.0.conv.bias shape torch.Size([256])
decoder.stages.1.convs.0.norm.weight shape torch.Size([256])
decoder.stages.1.convs.0.norm.bias shape torch.Size([256])
decoder.stages.1.convs.0.all_modules.0.weight shape torch.Size([256, 512, 3, 3, 3])
decoder.stages.1.convs.0.all_modules.0.bias shape torch.Size([256])
decoder.stages.1.convs.0.all_modules.1.weight shape torch.Size([256])
decoder.stages.1.convs.0.all_modules.1.bias shape torch.Size([256])
decoder.stages.1.convs.1.conv.weight shape torch.Size([256, 256, 3, 3, 3])
decoder.stages.1.convs.1.conv.bias shape torch.Size([256])
decoder.stages.1.convs.1.norm.weight shape torch.Size([256])
decoder.stages.1.convs.1.norm.bias shape torch.Size([256])
decoder.stages.1.convs.1.all_modules.0.weight shape torch.Size([256, 256, 3, 3, 3])
decoder.stages.1.convs.1.all_modules.0.bias shape torch.Size([256])
decoder.stages.1.convs.1.all_modules.1.weight shape torch.Size([256])
decoder.stages.1.convs.1.all_modules.1.bias shape torch.Size([256])
decoder.stages.2.convs.0.conv.weight shape torch.Size([128, 256, 3, 3, 3])
decoder.stages.2.convs.0.conv.bias shape torch.Size([128])
decoder.stages.2.convs.0.norm.weight shape torch.Size([128])
decoder.stages.2.convs.0.norm.bias shape torch.Size([128])
decoder.stages.2.convs.0.all_modules.0.weight shape torch.Size([128, 256, 3, 3, 3])
decoder.stages.2.convs.0.all_modules.0.bias shape torch.Size([128])
decoder.stages.2.convs.0.all_modules.1.weight shape torch.Size([128])
decoder.stages.2.convs.0.all_modules.1.bias shape torch.Size([128])
decoder.stages.2.convs.1.conv.weight shape torch.Size([128, 128, 3, 3, 3])
decoder.stages.2.convs.1.conv.bias shape torch.Size([128])
decoder.stages.2.convs.1.norm.weight shape torch.Size([128])
decoder.stages.2.convs.1.norm.bias shape torch.Size([128])
decoder.stages.2.convs.1.all_modules.0.weight shape torch.Size([128, 128, 3, 3, 3])
decoder.stages.2.convs.1.all_modules.0.bias shape torch.Size([128])
decoder.stages.2.convs.1.all_modules.1.weight shape torch.Size([128])
decoder.stages.2.convs.1.all_modules.1.bias shape torch.Size([128])
decoder.stages.3.convs.0.conv.weight shape torch.Size([64, 128, 3, 3, 3])
decoder.stages.3.convs.0.conv.bias shape torch.Size([64])
decoder.stages.3.convs.0.norm.weight shape torch.Size([64])
decoder.stages.3.convs.0.norm.bias shape torch.Size([64])
decoder.stages.3.convs.0.all_modules.0.weight shape torch.Size([64, 128, 3, 3, 3])
decoder.stages.3.convs.0.all_modules.0.bias shape torch.Size([64])
decoder.stages.3.convs.0.all_modules.1.weight shape torch.Size([64])
decoder.stages.3.convs.0.all_modules.1.bias shape torch.Size([64])
decoder.stages.3.convs.1.conv.weight shape torch.Size([64, 64, 3, 3, 3])
decoder.stages.3.convs.1.conv.bias shape torch.Size([64])
decoder.stages.3.convs.1.norm.weight shape torch.Size([64])
decoder.stages.3.convs.1.norm.bias shape torch.Size([64])
decoder.stages.3.convs.1.all_modules.0.weight shape torch.Size([64, 64, 3, 3, 3])
decoder.stages.3.convs.1.all_modules.0.bias shape torch.Size([64])
decoder.stages.3.convs.1.all_modules.1.weight shape torch.Size([64])
decoder.stages.3.convs.1.all_modules.1.bias shape torch.Size([64])
decoder.stages.4.convs.0.conv.weight shape torch.Size([32, 64, 3, 3, 3])
decoder.stages.4.convs.0.conv.bias shape torch.Size([32])
decoder.stages.4.convs.0.norm.weight shape torch.Size([32])
decoder.stages.4.convs.0.norm.bias shape torch.Size([32])
decoder.stages.4.convs.0.all_modules.0.weight shape torch.Size([32, 64, 3, 3, 3])
decoder.stages.4.convs.0.all_modules.0.bias shape torch.Size([32])
decoder.stages.4.convs.0.all_modules.1.weight shape torch.Size([32])
decoder.stages.4.convs.0.all_modules.1.bias shape torch.Size([32])
decoder.stages.4.convs.1.conv.weight shape torch.Size([32, 32, 3, 3, 3])
decoder.stages.4.convs.1.conv.bias shape torch.Size([32])
decoder.stages.4.convs.1.norm.weight shape torch.Size([32])
decoder.stages.4.convs.1.norm.bias shape torch.Size([32])
decoder.stages.4.convs.1.all_modules.0.weight shape torch.Size([32, 32, 3, 3, 3])
decoder.stages.4.convs.1.all_modules.0.bias shape torch.Size([32])
decoder.stages.4.convs.1.all_modules.1.weight shape torch.Size([32])
decoder.stages.4.convs.1.all_modules.1.bias shape torch.Size([32])
decoder.transpconvs.0.weight shape torch.Size([320, 320, 1, 2, 2])
decoder.transpconvs.0.bias shape torch.Size([320])
decoder.transpconvs.1.weight shape torch.Size([320, 256, 2, 2, 2])
decoder.transpconvs.1.bias shape torch.Size([256])
decoder.transpconvs.2.weight shape torch.Size([256, 128, 2, 2, 2])
decoder.transpconvs.2.bias shape torch.Size([128])
decoder.transpconvs.3.weight shape torch.Size([128, 64, 2, 2, 2])
decoder.transpconvs.3.bias shape torch.Size([64])
decoder.transpconvs.4.weight shape torch.Size([64, 32, 2, 2, 2])
decoder.transpconvs.4.bias shape torch.Size([32])
################### Done ###################
2024-12-08 22:06:11.336386: do_dummy_2d_data_aug: True
2024-12-08 22:06:11.338912: Using splits from existing split file: /home/gridsan/nchutisilp/datasets/nnUNet_Datasets/nnUNet_preprocessed/Dataset309_Sohee_Calcium_OCT_CrossValidation/splits_final.json
2024-12-08 22:06:11.340789: The split file contains 5 splits.
2024-12-08 22:06:11.341883: Desired fold for training: 2
2024-12-08 22:06:11.342889: This split has 6 training and 2 validation cases.
################### Loading pretrained weights from file  /home/gridsan/nchutisilp/projects/OpenAI-CLIP/clip_preivl_poststent_pretrained_nnUNet.pt ###################
Below is the list of overlapping blocks in pretrained model and nnUNet architecture:
encoder.stages.0.0.convs.0.conv.weight shape torch.Size([32, 1, 3, 3, 3])
encoder.stages.0.0.convs.0.conv.bias shape torch.Size([32])
encoder.stages.0.0.convs.0.norm.weight shape torch.Size([32])
encoder.stages.0.0.convs.0.norm.bias shape torch.Size([32])
encoder.stages.0.0.convs.0.all_modules.0.weight shape torch.Size([32, 1, 3, 3, 3])
encoder.stages.0.0.convs.0.all_modules.0.bias shape torch.Size([32])
encoder.stages.0.0.convs.0.all_modules.1.weight shape torch.Size([32])
encoder.stages.0.0.convs.0.all_modules.1.bias shape torch.Size([32])
encoder.stages.0.0.convs.1.conv.weight shape torch.Size([32, 32, 3, 3, 3])
encoder.stages.0.0.convs.1.conv.bias shape torch.Size([32])
encoder.stages.0.0.convs.1.norm.weight shape torch.Size([32])
encoder.stages.0.0.convs.1.norm.bias shape torch.Size([32])
encoder.stages.0.0.convs.1.all_modules.0.weight shape torch.Size([32, 32, 3, 3, 3])
encoder.stages.0.0.convs.1.all_modules.0.bias shape torch.Size([32])
encoder.stages.0.0.convs.1.all_modules.1.weight shape torch.Size([32])
encoder.stages.0.0.convs.1.all_modules.1.bias shape torch.Size([32])
encoder.stages.1.0.convs.0.conv.weight shape torch.Size([64, 32, 3, 3, 3])
encoder.stages.1.0.convs.0.conv.bias shape torch.Size([64])
encoder.stages.1.0.convs.0.norm.weight shape torch.Size([64])
encoder.stages.1.0.convs.0.norm.bias shape torch.Size([64])
encoder.stages.1.0.convs.0.all_modules.0.weight shape torch.Size([64, 32, 3, 3, 3])
encoder.stages.1.0.convs.0.all_modules.0.bias shape torch.Size([64])
encoder.stages.1.0.convs.0.all_modules.1.weight shape torch.Size([64])
encoder.stages.1.0.convs.0.all_modules.1.bias shape torch.Size([64])
encoder.stages.1.0.convs.1.conv.weight shape torch.Size([64, 64, 3, 3, 3])
encoder.stages.1.0.convs.1.conv.bias shape torch.Size([64])
encoder.stages.1.0.convs.1.norm.weight shape torch.Size([64])
encoder.stages.1.0.convs.1.norm.bias shape torch.Size([64])
encoder.stages.1.0.convs.1.all_modules.0.weight shape torch.Size([64, 64, 3, 3, 3])
encoder.stages.1.0.convs.1.all_modules.0.bias shape torch.Size([64])
encoder.stages.1.0.convs.1.all_modules.1.weight shape torch.Size([64])
encoder.stages.1.0.convs.1.all_modules.1.bias shape torch.Size([64])
encoder.stages.2.0.convs.0.conv.weight shape torch.Size([128, 64, 3, 3, 3])
encoder.stages.2.0.convs.0.conv.bias shape torch.Size([128])
encoder.stages.2.0.convs.0.norm.weight shape torch.Size([128])
encoder.stages.2.0.convs.0.norm.bias shape torch.Size([128])
encoder.stages.2.0.convs.0.all_modules.0.weight shape torch.Size([128, 64, 3, 3, 3])
encoder.stages.2.0.convs.0.all_modules.0.bias shape torch.Size([128])
encoder.stages.2.0.convs.0.all_modules.1.weight shape torch.Size([128])
encoder.stages.2.0.convs.0.all_modules.1.bias shape torch.Size([128])
encoder.stages.2.0.convs.1.conv.weight shape torch.Size([128, 128, 3, 3, 3])
encoder.stages.2.0.convs.1.conv.bias shape torch.Size([128])
encoder.stages.2.0.convs.1.norm.weight shape torch.Size([128])
encoder.stages.2.0.convs.1.norm.bias shape torch.Size([128])
encoder.stages.2.0.convs.1.all_modules.0.weight shape torch.Size([128, 128, 3, 3, 3])
encoder.stages.2.0.convs.1.all_modules.0.bias shape torch.Size([128])
encoder.stages.2.0.convs.1.all_modules.1.weight shape torch.Size([128])
encoder.stages.2.0.convs.1.all_modules.1.bias shape torch.Size([128])
encoder.stages.3.0.convs.0.conv.weight shape torch.Size([256, 128, 3, 3, 3])
encoder.stages.3.0.convs.0.conv.bias shape torch.Size([256])
encoder.stages.3.0.convs.0.norm.weight shape torch.Size([256])
encoder.stages.3.0.convs.0.norm.bias shape torch.Size([256])
encoder.stages.3.0.convs.0.all_modules.0.weight shape torch.Size([256, 128, 3, 3, 3])
encoder.stages.3.0.convs.0.all_modules.0.bias shape torch.Size([256])
encoder.stages.3.0.convs.0.all_modules.1.weight shape torch.Size([256])
encoder.stages.3.0.convs.0.all_modules.1.bias shape torch.Size([256])
encoder.stages.3.0.convs.1.conv.weight shape torch.Size([256, 256, 3, 3, 3])
encoder.stages.3.0.convs.1.conv.bias shape torch.Size([256])
encoder.stages.3.0.convs.1.norm.weight shape torch.Size([256])
encoder.stages.3.0.convs.1.norm.bias shape torch.Size([256])
encoder.stages.3.0.convs.1.all_modules.0.weight shape torch.Size([256, 256, 3, 3, 3])
encoder.stages.3.0.convs.1.all_modules.0.bias shape torch.Size([256])
encoder.stages.3.0.convs.1.all_modules.1.weight shape torch.Size([256])
encoder.stages.3.0.convs.1.all_modules.1.bias shape torch.Size([256])
encoder.stages.4.0.convs.0.conv.weight shape torch.Size([320, 256, 3, 3, 3])
encoder.stages.4.0.convs.0.conv.bias shape torch.Size([320])
encoder.stages.4.0.convs.0.norm.weight shape torch.Size([320])
encoder.stages.4.0.convs.0.norm.bias shape torch.Size([320])
encoder.stages.4.0.convs.0.all_modules.0.weight shape torch.Size([320, 256, 3, 3, 3])
encoder.stages.4.0.convs.0.all_modules.0.bias shape torch.Size([320])
encoder.stages.4.0.convs.0.all_modules.1.weight shape torch.Size([320])
encoder.stages.4.0.convs.0.all_modules.1.bias shape torch.Size([320])
encoder.stages.4.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.4.0.convs.1.conv.bias shape torch.Size([320])
encoder.stages.4.0.convs.1.norm.weight shape torch.Size([320])
encoder.stages.4.0.convs.1.norm.bias shape torch.Size([320])
encoder.stages.4.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.4.0.convs.1.all_modules.0.bias shape torch.Size([320])
encoder.stages.4.0.convs.1.all_modules.1.weight shape torch.Size([320])
encoder.stages.4.0.convs.1.all_modules.1.bias shape torch.Size([320])
encoder.stages.5.0.convs.0.conv.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.5.0.convs.0.conv.bias shape torch.Size([320])
encoder.stages.5.0.convs.0.norm.weight shape torch.Size([320])
encoder.stages.5.0.convs.0.norm.bias shape torch.Size([320])
encoder.stages.5.0.convs.0.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.5.0.convs.0.all_modules.0.bias shape torch.Size([320])
encoder.stages.5.0.convs.0.all_modules.1.weight shape torch.Size([320])
encoder.stages.5.0.convs.0.all_modules.1.bias shape torch.Size([320])
encoder.stages.5.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.5.0.convs.1.conv.bias shape torch.Size([320])
encoder.stages.5.0.convs.1.norm.weight shape torch.Size([320])
encoder.stages.5.0.convs.1.norm.bias shape torch.Size([320])
encoder.stages.5.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.5.0.convs.1.all_modules.0.bias shape torch.Size([320])
encoder.stages.5.0.convs.1.all_modules.1.weight shape torch.Size([320])
encoder.stages.5.0.convs.1.all_modules.1.bias shape torch.Size([320])
decoder.encoder.stages.0.0.convs.0.conv.weight shape torch.Size([32, 1, 3, 3, 3])
decoder.encoder.stages.0.0.convs.0.conv.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.norm.weight shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.norm.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.all_modules.0.weight shape torch.Size([32, 1, 3, 3, 3])
decoder.encoder.stages.0.0.convs.0.all_modules.0.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.all_modules.1.weight shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.all_modules.1.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.conv.weight shape torch.Size([32, 32, 3, 3, 3])
decoder.encoder.stages.0.0.convs.1.conv.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.norm.weight shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.norm.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.all_modules.0.weight shape torch.Size([32, 32, 3, 3, 3])
decoder.encoder.stages.0.0.convs.1.all_modules.0.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.all_modules.1.weight shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.all_modules.1.bias shape torch.Size([32])
decoder.encoder.stages.1.0.convs.0.conv.weight shape torch.Size([64, 32, 3, 3, 3])
decoder.encoder.stages.1.0.convs.0.conv.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.norm.weight shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.norm.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.all_modules.0.weight shape torch.Size([64, 32, 3, 3, 3])
decoder.encoder.stages.1.0.convs.0.all_modules.0.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.all_modules.1.weight shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.all_modules.1.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.conv.weight shape torch.Size([64, 64, 3, 3, 3])
decoder.encoder.stages.1.0.convs.1.conv.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.norm.weight shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.norm.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.all_modules.0.weight shape torch.Size([64, 64, 3, 3, 3])
decoder.encoder.stages.1.0.convs.1.all_modules.0.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.all_modules.1.weight shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.all_modules.1.bias shape torch.Size([64])
decoder.encoder.stages.2.0.convs.0.conv.weight shape torch.Size([128, 64, 3, 3, 3])
decoder.encoder.stages.2.0.convs.0.conv.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.norm.weight shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.norm.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.all_modules.0.weight shape torch.Size([128, 64, 3, 3, 3])
decoder.encoder.stages.2.0.convs.0.all_modules.0.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.all_modules.1.weight shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.all_modules.1.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.conv.weight shape torch.Size([128, 128, 3, 3, 3])
decoder.encoder.stages.2.0.convs.1.conv.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.norm.weight shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.norm.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.all_modules.0.weight shape torch.Size([128, 128, 3, 3, 3])
decoder.encoder.stages.2.0.convs.1.all_modules.0.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.all_modules.1.weight shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.all_modules.1.bias shape torch.Size([128])
decoder.encoder.stages.3.0.convs.0.conv.weight shape torch.Size([256, 128, 3, 3, 3])
decoder.encoder.stages.3.0.convs.0.conv.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.norm.weight shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.norm.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.all_modules.0.weight shape torch.Size([256, 128, 3, 3, 3])
decoder.encoder.stages.3.0.convs.0.all_modules.0.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.all_modules.1.weight shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.all_modules.1.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.conv.weight shape torch.Size([256, 256, 3, 3, 3])
decoder.encoder.stages.3.0.convs.1.conv.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.norm.weight shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.norm.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.all_modules.0.weight shape torch.Size([256, 256, 3, 3, 3])
decoder.encoder.stages.3.0.convs.1.all_modules.0.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.all_modules.1.weight shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.all_modules.1.bias shape torch.Size([256])
decoder.encoder.stages.4.0.convs.0.conv.weight shape torch.Size([320, 256, 3, 3, 3])
decoder.encoder.stages.4.0.convs.0.conv.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.norm.weight shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.norm.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.all_modules.0.weight shape torch.Size([320, 256, 3, 3, 3])
decoder.encoder.stages.4.0.convs.0.all_modules.0.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.all_modules.1.weight shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.all_modules.1.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.4.0.convs.1.conv.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.norm.weight shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.norm.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.4.0.convs.1.all_modules.0.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.all_modules.1.weight shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.all_modules.1.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.conv.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.5.0.convs.0.conv.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.norm.weight shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.norm.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.5.0.convs.0.all_modules.0.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.all_modules.1.weight shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.all_modules.1.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.5.0.convs.1.conv.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.norm.weight shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.norm.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.5.0.convs.1.all_modules.0.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.all_modules.1.weight shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.all_modules.1.bias shape torch.Size([320])
decoder.stages.0.convs.0.conv.weight shape torch.Size([320, 640, 3, 3, 3])
decoder.stages.0.convs.0.conv.bias shape torch.Size([320])
decoder.stages.0.convs.0.norm.weight shape torch.Size([320])
decoder.stages.0.convs.0.norm.bias shape torch.Size([320])
decoder.stages.0.convs.0.all_modules.0.weight shape torch.Size([320, 640, 3, 3, 3])
decoder.stages.0.convs.0.all_modules.0.bias shape torch.Size([320])
decoder.stages.0.convs.0.all_modules.1.weight shape torch.Size([320])
decoder.stages.0.convs.0.all_modules.1.bias shape torch.Size([320])
decoder.stages.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.stages.0.convs.1.conv.bias shape torch.Size([320])
decoder.stages.0.convs.1.norm.weight shape torch.Size([320])
decoder.stages.0.convs.1.norm.bias shape torch.Size([320])
decoder.stages.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.stages.0.convs.1.all_modules.0.bias shape torch.Size([320])
decoder.stages.0.convs.1.all_modules.1.weight shape torch.Size([320])
decoder.stages.0.convs.1.all_modules.1.bias shape torch.Size([320])
decoder.stages.1.convs.0.conv.weight shape torch.Size([256, 512, 3, 3, 3])
decoder.stages.1.convs.0.conv.bias shape torch.Size([256])
decoder.stages.1.convs.0.norm.weight shape torch.Size([256])
decoder.stages.1.convs.0.norm.bias shape torch.Size([256])
decoder.stages.1.convs.0.all_modules.0.weight shape torch.Size([256, 512, 3, 3, 3])
decoder.stages.1.convs.0.all_modules.0.bias shape torch.Size([256])
decoder.stages.1.convs.0.all_modules.1.weight shape torch.Size([256])
decoder.stages.1.convs.0.all_modules.1.bias shape torch.Size([256])
decoder.stages.1.convs.1.conv.weight shape torch.Size([256, 256, 3, 3, 3])
decoder.stages.1.convs.1.conv.bias shape torch.Size([256])
decoder.stages.1.convs.1.norm.weight shape torch.Size([256])
decoder.stages.1.convs.1.norm.bias shape torch.Size([256])
decoder.stages.1.convs.1.all_modules.0.weight shape torch.Size([256, 256, 3, 3, 3])
decoder.stages.1.convs.1.all_modules.0.bias shape torch.Size([256])
decoder.stages.1.convs.1.all_modules.1.weight shape torch.Size([256])
decoder.stages.1.convs.1.all_modules.1.bias shape torch.Size([256])
decoder.stages.2.convs.0.conv.weight shape torch.Size([128, 256, 3, 3, 3])
decoder.stages.2.convs.0.conv.bias shape torch.Size([128])
decoder.stages.2.convs.0.norm.weight shape torch.Size([128])
decoder.stages.2.convs.0.norm.bias shape torch.Size([128])
decoder.stages.2.convs.0.all_modules.0.weight shape torch.Size([128, 256, 3, 3, 3])
decoder.stages.2.convs.0.all_modules.0.bias shape torch.Size([128])
decoder.stages.2.convs.0.all_modules.1.weight shape torch.Size([128])
decoder.stages.2.convs.0.all_modules.1.bias shape torch.Size([128])
decoder.stages.2.convs.1.conv.weight shape torch.Size([128, 128, 3, 3, 3])
decoder.stages.2.convs.1.conv.bias shape torch.Size([128])
decoder.stages.2.convs.1.norm.weight shape torch.Size([128])
decoder.stages.2.convs.1.norm.bias shape torch.Size([128])
decoder.stages.2.convs.1.all_modules.0.weight shape torch.Size([128, 128, 3, 3, 3])
decoder.stages.2.convs.1.all_modules.0.bias shape torch.Size([128])
decoder.stages.2.convs.1.all_modules.1.weight shape torch.Size([128])
decoder.stages.2.convs.1.all_modules.1.bias shape torch.Size([128])
decoder.stages.3.convs.0.conv.weight shape torch.Size([64, 128, 3, 3, 3])
decoder.stages.3.convs.0.conv.bias shape torch.Size([64])
decoder.stages.3.convs.0.norm.weight shape torch.Size([64])
decoder.stages.3.convs.0.norm.bias shape torch.Size([64])
decoder.stages.3.convs.0.all_modules.0.weight shape torch.Size([64, 128, 3, 3, 3])
decoder.stages.3.convs.0.all_modules.0.bias shape torch.Size([64])
decoder.stages.3.convs.0.all_modules.1.weight shape torch.Size([64])
decoder.stages.3.convs.0.all_modules.1.bias shape torch.Size([64])
decoder.stages.3.convs.1.conv.weight shape torch.Size([64, 64, 3, 3, 3])
decoder.stages.3.convs.1.conv.bias shape torch.Size([64])
decoder.stages.3.convs.1.norm.weight shape torch.Size([64])
decoder.stages.3.convs.1.norm.bias shape torch.Size([64])
decoder.stages.3.convs.1.all_modules.0.weight shape torch.Size([64, 64, 3, 3, 3])
decoder.stages.3.convs.1.all_modules.0.bias shape torch.Size([64])
decoder.stages.3.convs.1.all_modules.1.weight shape torch.Size([64])
decoder.stages.3.convs.1.all_modules.1.bias shape torch.Size([64])
decoder.stages.4.convs.0.conv.weight shape torch.Size([32, 64, 3, 3, 3])
decoder.stages.4.convs.0.conv.bias shape torch.Size([32])
decoder.stages.4.convs.0.norm.weight shape torch.Size([32])
decoder.stages.4.convs.0.norm.bias shape torch.Size([32])
decoder.stages.4.convs.0.all_modules.0.weight shape torch.Size([32, 64, 3, 3, 3])
decoder.stages.4.convs.0.all_modules.0.bias shape torch.Size([32])
decoder.stages.4.convs.0.all_modules.1.weight shape torch.Size([32])
decoder.stages.4.convs.0.all_modules.1.bias shape torch.Size([32])
decoder.stages.4.convs.1.conv.weight shape torch.Size([32, 32, 3, 3, 3])
decoder.stages.4.convs.1.conv.bias shape torch.Size([32])
decoder.stages.4.convs.1.norm.weight shape torch.Size([32])
decoder.stages.4.convs.1.norm.bias shape torch.Size([32])
decoder.stages.4.convs.1.all_modules.0.weight shape torch.Size([32, 32, 3, 3, 3])
decoder.stages.4.convs.1.all_modules.0.bias shape torch.Size([32])
decoder.stages.4.convs.1.all_modules.1.weight shape torch.Size([32])
decoder.stages.4.convs.1.all_modules.1.bias shape torch.Size([32])
decoder.transpconvs.0.weight shape torch.Size([320, 320, 1, 2, 2])
decoder.transpconvs.0.bias shape torch.Size([320])
decoder.transpconvs.1.weight shape torch.Size([320, 256, 2, 2, 2])
decoder.transpconvs.1.bias shape torch.Size([256])
decoder.transpconvs.2.weight shape torch.Size([256, 128, 2, 2, 2])
decoder.transpconvs.2.bias shape torch.Size([128])
decoder.transpconvs.3.weight shape torch.Size([128, 64, 2, 2, 2])
decoder.transpconvs.3.bias shape torch.Size([64])
decoder.transpconvs.4.weight shape torch.Size([64, 32, 2, 2, 2])
decoder.transpconvs.4.bias shape torch.Size([32])
################### Done ###################
2024-12-08 22:06:11.362748: do_dummy_2d_data_aug: True
2024-12-08 22:06:11.364561: Using splits from existing split file: /home/gridsan/nchutisilp/datasets/nnUNet_Datasets/nnUNet_preprocessed/Dataset309_Sohee_Calcium_OCT_CrossValidation/splits_final.json
2024-12-08 22:06:11.366236: The split file contains 5 splits.
2024-12-08 22:06:11.367146: Desired fold for training: 3
2024-12-08 22:06:11.368018: This split has 7 training and 1 validation cases.
using pin_memory on device 0
using pin_memory on device 0
using pin_memory on device 0

This is the configuration used by this training:
Configuration name: 3d_32x160x128_b10
 {'data_identifier': 'nnUNetPlans_3d_fullres', 'preprocessor_name': 'DefaultPreprocessor', 'batch_size': 10, 'patch_size': [32, 160, 128], 'median_image_size_in_voxels': [375.0, 498.0, 498.0], 'spacing': [1.0, 1.0, 1.0], 'normalization_schemes': ['ZScoreNormalization'], 'use_mask_for_norm': [False], 'resampling_fn_data': 'resample_data_or_seg_to_shape', 'resampling_fn_seg': 'resample_data_or_seg_to_shape', 'resampling_fn_data_kwargs': {'is_seg': False, 'order': 3, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_seg_kwargs': {'is_seg': True, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_probabilities': 'resample_data_or_seg_to_shape', 'resampling_fn_probabilities_kwargs': {'is_seg': False, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'architecture': {'network_class_name': 'dynamic_network_architectures.architectures.unet.PlainConvUNet', 'arch_kwargs': {'n_stages': 6, 'features_per_stage': [32, 64, 128, 256, 320, 320], 'conv_op': 'torch.nn.modules.conv.Conv3d', 'kernel_sizes': [[3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3]], 'strides': [[1, 1, 1], [2, 2, 2], [2, 2, 2], [2, 2, 2], [2, 2, 2], [1, 2, 2]], 'n_conv_per_stage': [2, 2, 2, 2, 2, 2], 'n_conv_per_stage_decoder': [2, 2, 2, 2, 2], 'conv_bias': True, 'norm_op': 'torch.nn.modules.instancenorm.InstanceNorm3d', 'norm_op_kwargs': {'eps': 1e-05, 'affine': True}, 'dropout_op': None, 'dropout_op_kwargs': None, 'nonlin': 'torch.nn.LeakyReLU', 'nonlin_kwargs': {'inplace': True}, 'deep_supervision': True}, '_kw_requires_import': ['conv_op', 'norm_op', 'dropout_op', 'nonlin']}, 'batch_dice': True, 'inherits_from': '3d_fullres'} 

These are the global plan.json settings:
 {'dataset_name': 'Dataset309_Sohee_Calcium_OCT_CrossValidation', 'plans_name': 'nnUNetPlans', 'original_median_spacing_after_transp': [1.0, 1.0, 1.0], 'original_median_shape_after_transp': [375, 498, 498], 'image_reader_writer': 'SimpleITKIO', 'transpose_forward': [0, 1, 2], 'transpose_backward': [0, 1, 2], 'experiment_planner_used': 'ExperimentPlanner', 'label_manager': 'LabelManager', 'foreground_intensity_properties_per_channel': {'0': {'max': 0.4977976381778717, 'mean': 0.13507525622844696, 'median': 0.09599608182907104, 'min': 0.0, 'percentile_00_5': 0.014754901640117168, 'percentile_99_5': 0.4977976381778717, 'std': 0.12025152146816254}}} 

2024-12-08 22:06:13.704005: unpacking dataset...
using pin_memory on device 0

This is the configuration used by this training:
Configuration name: 3d_32x160x128_b10
 {'data_identifier': 'nnUNetPlans_3d_fullres', 'preprocessor_name': 'DefaultPreprocessor', 'batch_size': 10, 'patch_size': [32, 160, 128], 'median_image_size_in_voxels': [375.0, 498.0, 498.0], 'spacing': [1.0, 1.0, 1.0], 'normalization_schemes': ['ZScoreNormalization'], 'use_mask_for_norm': [False], 'resampling_fn_data': 'resample_data_or_seg_to_shape', 'resampling_fn_seg': 'resample_data_or_seg_to_shape', 'resampling_fn_data_kwargs': {'is_seg': False, 'order': 3, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_seg_kwargs': {'is_seg': True, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_probabilities': 'resample_data_or_seg_to_shape', 'resampling_fn_probabilities_kwargs': {'is_seg': False, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'architecture': {'network_class_name': 'dynamic_network_architectures.architectures.unet.PlainConvUNet', 'arch_kwargs': {'n_stages': 6, 'features_per_stage': [32, 64, 128, 256, 320, 320], 'conv_op': 'torch.nn.modules.conv.Conv3d', 'kernel_sizes': [[3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3]], 'strides': [[1, 1, 1], [2, 2, 2], [2, 2, 2], [2, 2, 2], [2, 2, 2], [1, 2, 2]], 'n_conv_per_stage': [2, 2, 2, 2, 2, 2], 'n_conv_per_stage_decoder': [2, 2, 2, 2, 2], 'conv_bias': True, 'norm_op': 'torch.nn.modules.instancenorm.InstanceNorm3d', 'norm_op_kwargs': {'eps': 1e-05, 'affine': True}, 'dropout_op': None, 'dropout_op_kwargs': None, 'nonlin': 'torch.nn.LeakyReLU', 'nonlin_kwargs': {'inplace': True}, 'deep_supervision': True}, '_kw_requires_import': ['conv_op', 'norm_op', 'dropout_op', 'nonlin']}, 'batch_dice': True, 'inherits_from': '3d_fullres'} 

These are the global plan.json settings:
 {'dataset_name': 'Dataset309_Sohee_Calcium_OCT_CrossValidation', 'plans_name': 'nnUNetPlans', 'original_median_spacing_after_transp': [1.0, 1.0, 1.0], 'original_median_shape_after_transp': [375, 498, 498], 'image_reader_writer': 'SimpleITKIO', 'transpose_forward': [0, 1, 2], 'transpose_backward': [0, 1, 2], 'experiment_planner_used': 'ExperimentPlanner', 'label_manager': 'LabelManager', 'foreground_intensity_properties_per_channel': {'0': {'max': 0.4977976381778717, 'mean': 0.13507525622844696, 'median': 0.09599608182907104, 'min': 0.0, 'percentile_00_5': 0.014754901640117168, 'percentile_99_5': 0.4977976381778717, 'std': 0.12025152146816254}}} 

2024-12-08 22:06:14.454313: unpacking dataset...
2024-12-08 22:06:18.225188: unpacking done...
2024-12-08 22:06:18.448553: Unable to plot network architecture: nnUNet_compile is enabled!
2024-12-08 22:06:18.497194: 
2024-12-08 22:06:18.498775: Epoch 0
2024-12-08 22:06:18.499593: Current learning rate: 0.01
2024-12-08 22:08:30.532815: Validation loss improved from 1000.00000 to -0.12847! Patience: 0/50
2024-12-08 22:08:30.534657: train_loss -0.0729
2024-12-08 22:08:30.535612: val_loss -0.1285
2024-12-08 22:08:30.536402: Pseudo dice [0.5069]
2024-12-08 22:08:30.537191: Epoch time: 132.04 s
2024-12-08 22:08:30.537895: Yayy! New best EMA pseudo Dice: 0.5069
2024-12-08 22:08:32.047604: 
2024-12-08 22:08:32.049181: Epoch 1
2024-12-08 22:08:32.050062: Current learning rate: 0.00999
2024-12-08 22:09:59.747374: Validation loss improved from -0.12847 to -0.22438! Patience: 0/50
2024-12-08 22:09:59.748713: train_loss -0.2205
2024-12-08 22:09:59.750222: val_loss -0.2244
2024-12-08 22:09:59.750966: Pseudo dice [0.5472]
2024-12-08 22:09:59.751733: Epoch time: 87.7 s
2024-12-08 22:09:59.752455: Yayy! New best EMA pseudo Dice: 0.5109
2024-12-08 22:10:01.339536: 
2024-12-08 22:10:01.341307: Epoch 2
2024-12-08 22:10:01.342115: Current learning rate: 0.00998
2024-12-08 22:11:29.446037: Validation loss did not improve from -0.22438. Patience: 1/50
2024-12-08 22:11:29.447320: train_loss -0.2969
2024-12-08 22:11:29.448571: val_loss -0.1514
2024-12-08 22:11:29.449548: Pseudo dice [0.4815]
2024-12-08 22:11:29.450596: Epoch time: 88.11 s
2024-12-08 22:11:30.739225: 
2024-12-08 22:11:30.741203: Epoch 3
2024-12-08 22:11:30.742435: Current learning rate: 0.00997
2024-12-08 22:12:58.942570: Validation loss improved from -0.22438 to -0.28280! Patience: 1/50
2024-12-08 22:12:58.943869: train_loss -0.2954
2024-12-08 22:12:58.944858: val_loss -0.2828
2024-12-08 22:12:58.945737: Pseudo dice [0.6009]
2024-12-08 22:12:58.946657: Epoch time: 88.21 s
2024-12-08 22:12:58.947529: Yayy! New best EMA pseudo Dice: 0.5173
2024-12-08 22:13:00.596842: 
2024-12-08 22:13:00.598649: Epoch 4
2024-12-08 22:13:00.599621: Current learning rate: 0.00996
2024-12-08 22:14:28.725636: Validation loss did not improve from -0.28280. Patience: 1/50
2024-12-08 22:14:28.726551: train_loss -0.3265
2024-12-08 22:14:28.727386: val_loss -0.2395
2024-12-08 22:14:28.728114: Pseudo dice [0.5749]
2024-12-08 22:14:28.728919: Epoch time: 88.13 s
2024-12-08 22:14:29.090028: Yayy! New best EMA pseudo Dice: 0.523
2024-12-08 22:14:30.720305: 
2024-12-08 22:14:30.721997: Epoch 5
2024-12-08 22:14:30.723473: Current learning rate: 0.00995
2024-12-08 22:15:58.721667: Validation loss improved from -0.28280 to -0.30375! Patience: 1/50
2024-12-08 22:15:58.722590: train_loss -0.3791
2024-12-08 22:15:58.723701: val_loss -0.3037
2024-12-08 22:15:58.724600: Pseudo dice [0.6161]
2024-12-08 22:15:58.725394: Epoch time: 88.0 s
2024-12-08 22:15:58.726203: Yayy! New best EMA pseudo Dice: 0.5323
2024-12-08 22:16:00.331079: 
2024-12-08 22:16:00.332458: Epoch 6
2024-12-08 22:16:00.333270: Current learning rate: 0.00995
2024-12-08 22:17:28.097857: Validation loss did not improve from -0.30375. Patience: 1/50
2024-12-08 22:17:28.099336: train_loss -0.3938
2024-12-08 22:17:28.100467: val_loss -0.2556
2024-12-08 22:17:28.101352: Pseudo dice [0.5796]
2024-12-08 22:17:28.102113: Epoch time: 87.77 s
2024-12-08 22:17:28.103070: Yayy! New best EMA pseudo Dice: 0.5371
2024-12-08 22:17:29.686148: 
2024-12-08 22:17:29.687608: Epoch 7
2024-12-08 22:17:29.688555: Current learning rate: 0.00994
2024-12-08 22:18:57.245612: Validation loss improved from -0.30375 to -0.35863! Patience: 1/50
2024-12-08 22:18:57.246550: train_loss -0.4066
2024-12-08 22:18:57.247623: val_loss -0.3586
2024-12-08 22:18:57.248636: Pseudo dice [0.6549]
2024-12-08 22:18:57.249574: Epoch time: 87.56 s
2024-12-08 22:18:57.250459: Yayy! New best EMA pseudo Dice: 0.5488
2024-12-08 22:18:59.291532: 
2024-12-08 22:18:59.292925: Epoch 8
2024-12-08 22:18:59.293655: Current learning rate: 0.00993
2024-12-08 22:20:27.226070: Validation loss did not improve from -0.35863. Patience: 1/50
2024-12-08 22:20:27.227155: train_loss -0.4412
2024-12-08 22:20:27.228138: val_loss -0.2038
2024-12-08 22:20:27.228980: Pseudo dice [0.5452]
2024-12-08 22:20:27.229917: Epoch time: 87.94 s
2024-12-08 22:20:28.554057: 
2024-12-08 22:20:28.555243: Epoch 9
2024-12-08 22:20:28.556276: Current learning rate: 0.00992
2024-12-08 22:21:56.419941: Validation loss did not improve from -0.35863. Patience: 2/50
2024-12-08 22:21:56.421037: train_loss -0.4353
2024-12-08 22:21:56.422200: val_loss -0.3436
2024-12-08 22:21:56.423020: Pseudo dice [0.6348]
2024-12-08 22:21:56.423877: Epoch time: 87.87 s
2024-12-08 22:21:56.785204: Yayy! New best EMA pseudo Dice: 0.5571
2024-12-08 22:21:58.400528: 
2024-12-08 22:21:58.401833: Epoch 10
2024-12-08 22:21:58.402611: Current learning rate: 0.00991
2024-12-08 22:23:26.538609: Validation loss did not improve from -0.35863. Patience: 3/50
2024-12-08 22:23:26.539624: train_loss -0.459
2024-12-08 22:23:26.540623: val_loss -0.2969
2024-12-08 22:23:26.541301: Pseudo dice [0.6132]
2024-12-08 22:23:26.542225: Epoch time: 88.14 s
2024-12-08 22:23:26.543050: Yayy! New best EMA pseudo Dice: 0.5627
2024-12-08 22:23:28.156708: 
2024-12-08 22:23:28.158085: Epoch 11
2024-12-08 22:23:28.158969: Current learning rate: 0.0099
2024-12-08 22:24:56.374425: Validation loss did not improve from -0.35863. Patience: 4/50
2024-12-08 22:24:56.375751: train_loss -0.4886
2024-12-08 22:24:56.376804: val_loss -0.3188
2024-12-08 22:24:56.377632: Pseudo dice [0.6179]
2024-12-08 22:24:56.378386: Epoch time: 88.22 s
2024-12-08 22:24:56.379036: Yayy! New best EMA pseudo Dice: 0.5682
2024-12-08 22:24:57.961298: 
2024-12-08 22:24:57.963070: Epoch 12
2024-12-08 22:24:57.964156: Current learning rate: 0.00989
2024-12-08 22:26:25.871661: Validation loss did not improve from -0.35863. Patience: 5/50
2024-12-08 22:26:25.873106: train_loss -0.4664
2024-12-08 22:26:25.874177: val_loss -0.2128
2024-12-08 22:26:25.875068: Pseudo dice [0.5447]
2024-12-08 22:26:25.876033: Epoch time: 87.91 s
2024-12-08 22:26:27.150836: 
2024-12-08 22:26:27.152461: Epoch 13
2024-12-08 22:26:27.153340: Current learning rate: 0.00988
2024-12-08 22:27:55.080201: Validation loss improved from -0.35863 to -0.35884! Patience: 5/50
2024-12-08 22:27:55.081318: train_loss -0.4828
2024-12-08 22:27:55.082419: val_loss -0.3588
2024-12-08 22:27:55.083227: Pseudo dice [0.6379]
2024-12-08 22:27:55.084196: Epoch time: 87.93 s
2024-12-08 22:27:55.085003: Yayy! New best EMA pseudo Dice: 0.5731
2024-12-08 22:27:56.732692: 
2024-12-08 22:27:56.734786: Epoch 14
2024-12-08 22:27:56.736066: Current learning rate: 0.00987
2024-12-08 22:29:24.669917: Validation loss did not improve from -0.35884. Patience: 1/50
2024-12-08 22:29:24.670896: train_loss -0.5036
2024-12-08 22:29:24.671870: val_loss -0.3515
2024-12-08 22:29:24.672601: Pseudo dice [0.6536]
2024-12-08 22:29:24.673355: Epoch time: 87.94 s
2024-12-08 22:29:25.038789: Yayy! New best EMA pseudo Dice: 0.5811
2024-12-08 22:29:26.656543: 
2024-12-08 22:29:26.658261: Epoch 15
2024-12-08 22:29:26.659065: Current learning rate: 0.00986
2024-12-08 22:30:54.582404: Validation loss did not improve from -0.35884. Patience: 2/50
2024-12-08 22:30:54.583674: train_loss -0.5187
2024-12-08 22:30:54.584892: val_loss -0.2914
2024-12-08 22:30:54.585785: Pseudo dice [0.6044]
2024-12-08 22:30:54.586536: Epoch time: 87.93 s
2024-12-08 22:30:54.587336: Yayy! New best EMA pseudo Dice: 0.5835
2024-12-08 22:30:56.256128: 
2024-12-08 22:30:56.257690: Epoch 16
2024-12-08 22:30:56.258455: Current learning rate: 0.00986
2024-12-08 22:32:24.277790: Validation loss improved from -0.35884 to -0.43137! Patience: 2/50
2024-12-08 22:32:24.279231: train_loss -0.5176
2024-12-08 22:32:24.280532: val_loss -0.4314
2024-12-08 22:32:24.281641: Pseudo dice [0.6951]
2024-12-08 22:32:24.282726: Epoch time: 88.02 s
2024-12-08 22:32:24.283809: Yayy! New best EMA pseudo Dice: 0.5946
2024-12-08 22:32:25.951001: 
2024-12-08 22:32:25.952841: Epoch 17
2024-12-08 22:32:25.953659: Current learning rate: 0.00985
2024-12-08 22:33:54.056830: Validation loss did not improve from -0.43137. Patience: 1/50
2024-12-08 22:33:54.058013: train_loss -0.5119
2024-12-08 22:33:54.059055: val_loss -0.3749
2024-12-08 22:33:54.059807: Pseudo dice [0.6597]
2024-12-08 22:33:54.060560: Epoch time: 88.11 s
2024-12-08 22:33:54.061394: Yayy! New best EMA pseudo Dice: 0.6011
2024-12-08 22:33:55.669032: 
2024-12-08 22:33:55.670423: Epoch 18
2024-12-08 22:33:55.671150: Current learning rate: 0.00984
2024-12-08 22:35:23.588403: Validation loss did not improve from -0.43137. Patience: 2/50
2024-12-08 22:35:23.589541: train_loss -0.5248
2024-12-08 22:35:23.590524: val_loss -0.33
2024-12-08 22:35:23.591317: Pseudo dice [0.634]
2024-12-08 22:35:23.592178: Epoch time: 87.92 s
2024-12-08 22:35:23.592864: Yayy! New best EMA pseudo Dice: 0.6044
2024-12-08 22:35:25.637956: 
2024-12-08 22:35:25.639942: Epoch 19
2024-12-08 22:35:25.640945: Current learning rate: 0.00983
2024-12-08 22:36:53.690739: Validation loss did not improve from -0.43137. Patience: 3/50
2024-12-08 22:36:53.691854: train_loss -0.5364
2024-12-08 22:36:53.692864: val_loss -0.3689
2024-12-08 22:36:53.693975: Pseudo dice [0.6587]
2024-12-08 22:36:53.694786: Epoch time: 88.05 s
2024-12-08 22:36:54.056373: Yayy! New best EMA pseudo Dice: 0.6098
2024-12-08 22:36:55.720390: 
2024-12-08 22:36:55.721942: Epoch 20
2024-12-08 22:36:55.722858: Current learning rate: 0.00982
2024-12-08 22:38:23.924232: Validation loss did not improve from -0.43137. Patience: 4/50
2024-12-08 22:38:23.925469: train_loss -0.536
2024-12-08 22:38:23.926468: val_loss -0.3851
2024-12-08 22:38:23.927375: Pseudo dice [0.6698]
2024-12-08 22:38:23.928350: Epoch time: 88.21 s
2024-12-08 22:38:23.929168: Yayy! New best EMA pseudo Dice: 0.6158
2024-12-08 22:38:25.614716: 
2024-12-08 22:38:25.618168: Epoch 21
2024-12-08 22:38:25.620265: Current learning rate: 0.00981
2024-12-08 22:39:53.248085: Validation loss did not improve from -0.43137. Patience: 5/50
2024-12-08 22:39:53.249075: train_loss -0.5457
2024-12-08 22:39:53.250071: val_loss -0.3336
2024-12-08 22:39:53.250797: Pseudo dice [0.6426]
2024-12-08 22:39:53.251590: Epoch time: 87.64 s
2024-12-08 22:39:53.252265: Yayy! New best EMA pseudo Dice: 0.6185
2024-12-08 22:39:54.833090: 
2024-12-08 22:39:54.834673: Epoch 22
2024-12-08 22:39:54.835349: Current learning rate: 0.0098
2024-12-08 22:41:22.700625: Validation loss did not improve from -0.43137. Patience: 6/50
2024-12-08 22:41:22.701970: train_loss -0.5614
2024-12-08 22:41:22.703099: val_loss -0.3768
2024-12-08 22:41:22.704141: Pseudo dice [0.6588]
2024-12-08 22:41:22.705115: Epoch time: 87.87 s
2024-12-08 22:41:22.705886: Yayy! New best EMA pseudo Dice: 0.6225
2024-12-08 22:41:24.282288: 
2024-12-08 22:41:24.284663: Epoch 23
2024-12-08 22:41:24.285629: Current learning rate: 0.00979
2024-12-08 22:42:52.193295: Validation loss did not improve from -0.43137. Patience: 7/50
2024-12-08 22:42:52.195218: train_loss -0.5445
2024-12-08 22:42:52.196476: val_loss -0.3552
2024-12-08 22:42:52.197331: Pseudo dice [0.6437]
2024-12-08 22:42:52.198217: Epoch time: 87.91 s
2024-12-08 22:42:52.198943: Yayy! New best EMA pseudo Dice: 0.6247
2024-12-08 22:42:53.744975: 
2024-12-08 22:42:53.746382: Epoch 24
2024-12-08 22:42:53.747408: Current learning rate: 0.00978
2024-12-08 22:44:21.888973: Validation loss did not improve from -0.43137. Patience: 8/50
2024-12-08 22:44:21.890431: train_loss -0.5664
2024-12-08 22:44:21.891300: val_loss -0.3927
2024-12-08 22:44:21.892095: Pseudo dice [0.6763]
2024-12-08 22:44:21.892923: Epoch time: 88.15 s
2024-12-08 22:44:22.256462: Yayy! New best EMA pseudo Dice: 0.6298
2024-12-08 22:44:23.824453: 
2024-12-08 22:44:23.826334: Epoch 25
2024-12-08 22:44:23.827449: Current learning rate: 0.00977
2024-12-08 22:45:52.057458: Validation loss did not improve from -0.43137. Patience: 9/50
2024-12-08 22:45:52.058863: train_loss -0.5536
2024-12-08 22:45:52.059644: val_loss -0.3085
2024-12-08 22:45:52.060321: Pseudo dice [0.6052]
2024-12-08 22:45:52.060955: Epoch time: 88.24 s
2024-12-08 22:45:53.264132: 
2024-12-08 22:45:53.265582: Epoch 26
2024-12-08 22:45:53.266394: Current learning rate: 0.00977
2024-12-08 22:47:21.170270: Validation loss did not improve from -0.43137. Patience: 10/50
2024-12-08 22:47:21.171455: train_loss -0.5594
2024-12-08 22:47:21.172380: val_loss -0.4202
2024-12-08 22:47:21.173166: Pseudo dice [0.68]
2024-12-08 22:47:21.174056: Epoch time: 87.91 s
2024-12-08 22:47:21.174759: Yayy! New best EMA pseudo Dice: 0.6326
2024-12-08 22:47:22.777194: 
2024-12-08 22:47:22.779211: Epoch 27
2024-12-08 22:47:22.780361: Current learning rate: 0.00976
2024-12-08 22:48:50.596963: Validation loss did not improve from -0.43137. Patience: 11/50
2024-12-08 22:48:50.598174: train_loss -0.5704
2024-12-08 22:48:50.599170: val_loss -0.3794
2024-12-08 22:48:50.600002: Pseudo dice [0.6701]
2024-12-08 22:48:50.600831: Epoch time: 87.82 s
2024-12-08 22:48:50.601592: Yayy! New best EMA pseudo Dice: 0.6364
2024-12-08 22:48:52.159836: 
2024-12-08 22:48:52.161319: Epoch 28
2024-12-08 22:48:52.162153: Current learning rate: 0.00975
2024-12-08 22:50:20.095298: Validation loss did not improve from -0.43137. Patience: 12/50
2024-12-08 22:50:20.096413: train_loss -0.564
2024-12-08 22:50:20.097366: val_loss -0.3767
2024-12-08 22:50:20.098209: Pseudo dice [0.6591]
2024-12-08 22:50:20.098968: Epoch time: 87.94 s
2024-12-08 22:50:20.099579: Yayy! New best EMA pseudo Dice: 0.6386
2024-12-08 22:50:21.708571: 
2024-12-08 22:50:21.710418: Epoch 29
2024-12-08 22:50:21.711214: Current learning rate: 0.00974
2024-12-08 22:51:49.706267: Validation loss did not improve from -0.43137. Patience: 13/50
2024-12-08 22:51:49.707458: train_loss -0.5686
2024-12-08 22:51:49.708657: val_loss -0.2827
2024-12-08 22:51:49.709618: Pseudo dice [0.592]
2024-12-08 22:51:49.710510: Epoch time: 88.0 s
2024-12-08 22:51:51.780443: 
2024-12-08 22:51:51.782021: Epoch 30
2024-12-08 22:51:51.783579: Current learning rate: 0.00973
2024-12-08 22:53:19.771106: Validation loss did not improve from -0.43137. Patience: 14/50
2024-12-08 22:53:19.772050: train_loss -0.5769
2024-12-08 22:53:19.773026: val_loss -0.3855
2024-12-08 22:53:19.774007: Pseudo dice [0.6754]
2024-12-08 22:53:19.775026: Epoch time: 87.99 s
2024-12-08 22:53:21.022894: 
2024-12-08 22:53:21.024796: Epoch 31
2024-12-08 22:53:21.025728: Current learning rate: 0.00972
2024-12-08 22:54:49.020185: Validation loss did not improve from -0.43137. Patience: 15/50
2024-12-08 22:54:49.021011: train_loss -0.577
2024-12-08 22:54:49.021818: val_loss -0.3286
2024-12-08 22:54:49.022515: Pseudo dice [0.6285]
2024-12-08 22:54:49.023179: Epoch time: 88.0 s
2024-12-08 22:54:50.328479: 
2024-12-08 22:54:50.329740: Epoch 32
2024-12-08 22:54:50.330439: Current learning rate: 0.00971
2024-12-08 22:56:18.397354: Validation loss did not improve from -0.43137. Patience: 16/50
2024-12-08 22:56:18.398401: train_loss -0.5824
2024-12-08 22:56:18.399388: val_loss -0.2932
2024-12-08 22:56:18.400199: Pseudo dice [0.6004]
2024-12-08 22:56:18.401053: Epoch time: 88.07 s
2024-12-08 22:56:19.622015: 
2024-12-08 22:56:19.623619: Epoch 33
2024-12-08 22:56:19.624523: Current learning rate: 0.0097
2024-12-08 22:57:47.652177: Validation loss did not improve from -0.43137. Patience: 17/50
2024-12-08 22:57:47.653545: train_loss -0.5886
2024-12-08 22:57:47.654839: val_loss -0.42
2024-12-08 22:57:47.655855: Pseudo dice [0.6838]
2024-12-08 22:57:47.656832: Epoch time: 88.03 s
2024-12-08 22:57:48.883054: 
2024-12-08 22:57:48.884807: Epoch 34
2024-12-08 22:57:48.885688: Current learning rate: 0.00969
2024-12-08 22:59:16.641390: Validation loss did not improve from -0.43137. Patience: 18/50
2024-12-08 22:59:16.642859: train_loss -0.5999
2024-12-08 22:59:16.644348: val_loss -0.3868
2024-12-08 22:59:16.645334: Pseudo dice [0.6675]
2024-12-08 22:59:16.646147: Epoch time: 87.76 s
2024-12-08 22:59:17.003850: Yayy! New best EMA pseudo Dice: 0.6414
2024-12-08 22:59:18.578381: 
2024-12-08 22:59:18.580007: Epoch 35
2024-12-08 22:59:18.580695: Current learning rate: 0.00968
2024-12-08 23:00:46.369315: Validation loss did not improve from -0.43137. Patience: 19/50
2024-12-08 23:00:46.370506: train_loss -0.5867
2024-12-08 23:00:46.371532: val_loss -0.3377
2024-12-08 23:00:46.372311: Pseudo dice [0.6552]
2024-12-08 23:00:46.373346: Epoch time: 87.79 s
2024-12-08 23:00:46.374343: Yayy! New best EMA pseudo Dice: 0.6428
2024-12-08 23:00:48.033010: 
2024-12-08 23:00:48.034885: Epoch 36
2024-12-08 23:00:48.035768: Current learning rate: 0.00968
2024-12-08 23:02:15.763530: Validation loss did not improve from -0.43137. Patience: 20/50
2024-12-08 23:02:15.764647: train_loss -0.6071
2024-12-08 23:02:15.765611: val_loss -0.2997
2024-12-08 23:02:15.766427: Pseudo dice [0.604]
2024-12-08 23:02:15.767190: Epoch time: 87.73 s
2024-12-08 23:02:17.104557: 
2024-12-08 23:02:17.106178: Epoch 37
2024-12-08 23:02:17.106866: Current learning rate: 0.00967
2024-12-08 23:03:44.863911: Validation loss did not improve from -0.43137. Patience: 21/50
2024-12-08 23:03:44.865049: train_loss -0.5996
2024-12-08 23:03:44.865929: val_loss -0.3774
2024-12-08 23:03:44.866677: Pseudo dice [0.6745]
2024-12-08 23:03:44.867452: Epoch time: 87.76 s
2024-12-08 23:03:46.128531: 
2024-12-08 23:03:46.130314: Epoch 38
2024-12-08 23:03:46.131168: Current learning rate: 0.00966
2024-12-08 23:05:13.880257: Validation loss did not improve from -0.43137. Patience: 22/50
2024-12-08 23:05:13.881301: train_loss -0.5973
2024-12-08 23:05:13.882324: val_loss -0.2988
2024-12-08 23:05:13.883320: Pseudo dice [0.6306]
2024-12-08 23:05:13.884250: Epoch time: 87.75 s
2024-12-08 23:05:15.200288: 
2024-12-08 23:05:15.201797: Epoch 39
2024-12-08 23:05:15.202871: Current learning rate: 0.00965
2024-12-08 23:06:42.910787: Validation loss did not improve from -0.43137. Patience: 23/50
2024-12-08 23:06:42.912194: train_loss -0.6055
2024-12-08 23:06:42.913401: val_loss -0.3331
2024-12-08 23:06:42.914314: Pseudo dice [0.6389]
2024-12-08 23:06:42.915281: Epoch time: 87.71 s
2024-12-08 23:06:44.588771: 
2024-12-08 23:06:44.590514: Epoch 40
2024-12-08 23:06:44.591456: Current learning rate: 0.00964
2024-12-08 23:08:12.261049: Validation loss did not improve from -0.43137. Patience: 24/50
2024-12-08 23:08:12.262424: train_loss -0.6
2024-12-08 23:08:12.263392: val_loss -0.2988
2024-12-08 23:08:12.264156: Pseudo dice [0.6159]
2024-12-08 23:08:12.265014: Epoch time: 87.67 s
2024-12-08 23:08:13.968617: 
2024-12-08 23:08:13.970584: Epoch 41
2024-12-08 23:08:13.971404: Current learning rate: 0.00963
2024-12-08 23:09:41.619066: Validation loss did not improve from -0.43137. Patience: 25/50
2024-12-08 23:09:41.620098: train_loss -0.613
2024-12-08 23:09:41.620923: val_loss -0.2459
2024-12-08 23:09:41.621681: Pseudo dice [0.5785]
2024-12-08 23:09:41.622545: Epoch time: 87.65 s
2024-12-08 23:09:42.833271: 
2024-12-08 23:09:42.834990: Epoch 42
2024-12-08 23:09:42.836002: Current learning rate: 0.00962
2024-12-08 23:11:10.925371: Validation loss improved from -0.43137 to -0.44677! Patience: 25/50
2024-12-08 23:11:10.926759: train_loss -0.6147
2024-12-08 23:11:10.927749: val_loss -0.4468
2024-12-08 23:11:10.928620: Pseudo dice [0.6943]
2024-12-08 23:11:10.929312: Epoch time: 88.09 s
2024-12-08 23:11:12.156915: 
2024-12-08 23:11:12.158650: Epoch 43
2024-12-08 23:11:12.159488: Current learning rate: 0.00961
2024-12-08 23:12:40.193133: Validation loss did not improve from -0.44677. Patience: 1/50
2024-12-08 23:12:40.194178: train_loss -0.6136
2024-12-08 23:12:40.194970: val_loss -0.2214
2024-12-08 23:12:40.195835: Pseudo dice [0.5684]
2024-12-08 23:12:40.196623: Epoch time: 88.04 s
2024-12-08 23:12:41.443868: 
2024-12-08 23:12:41.445968: Epoch 44
2024-12-08 23:12:41.446825: Current learning rate: 0.0096
2024-12-08 23:14:09.480093: Validation loss did not improve from -0.44677. Patience: 2/50
2024-12-08 23:14:09.481196: train_loss -0.6178
2024-12-08 23:14:09.482594: val_loss -0.387
2024-12-08 23:14:09.483855: Pseudo dice [0.6638]
2024-12-08 23:14:09.484881: Epoch time: 88.04 s
2024-12-08 23:14:11.052430: 
2024-12-08 23:14:11.054215: Epoch 45
2024-12-08 23:14:11.055187: Current learning rate: 0.00959
2024-12-08 23:15:38.980284: Validation loss did not improve from -0.44677. Patience: 3/50
2024-12-08 23:15:38.981444: train_loss -0.6204
2024-12-08 23:15:38.982480: val_loss -0.1162
2024-12-08 23:15:38.983259: Pseudo dice [0.5114]
2024-12-08 23:15:38.984131: Epoch time: 87.93 s
2024-12-08 23:15:40.226620: 
2024-12-08 23:15:40.228329: Epoch 46
2024-12-08 23:15:40.229120: Current learning rate: 0.00959
2024-12-08 23:17:08.034832: Validation loss did not improve from -0.44677. Patience: 4/50
2024-12-08 23:17:08.036067: train_loss -0.6206
2024-12-08 23:17:08.036956: val_loss -0.1225
2024-12-08 23:17:08.037762: Pseudo dice [0.5492]
2024-12-08 23:17:08.038640: Epoch time: 87.81 s
2024-12-08 23:17:09.256780: 
2024-12-08 23:17:09.258963: Epoch 47
2024-12-08 23:17:09.259743: Current learning rate: 0.00958
2024-12-08 23:18:36.999218: Validation loss did not improve from -0.44677. Patience: 5/50
2024-12-08 23:18:37.000393: train_loss -0.6215
2024-12-08 23:18:37.001450: val_loss -0.2626
2024-12-08 23:18:37.002313: Pseudo dice [0.5727]
2024-12-08 23:18:37.003264: Epoch time: 87.74 s
2024-12-08 23:18:38.188239: 
2024-12-08 23:18:38.189791: Epoch 48
2024-12-08 23:18:38.190654: Current learning rate: 0.00957
2024-12-08 23:20:05.980272: Validation loss did not improve from -0.44677. Patience: 6/50
2024-12-08 23:20:05.981452: train_loss -0.6214
2024-12-08 23:20:05.982296: val_loss -0.3356
2024-12-08 23:20:05.983074: Pseudo dice [0.6461]
2024-12-08 23:20:05.983936: Epoch time: 87.79 s
2024-12-08 23:20:07.210947: 
2024-12-08 23:20:07.212580: Epoch 49
2024-12-08 23:20:07.213537: Current learning rate: 0.00956
2024-12-08 23:21:34.994210: Validation loss did not improve from -0.44677. Patience: 7/50
2024-12-08 23:21:34.995467: train_loss -0.6337
2024-12-08 23:21:34.996425: val_loss -0.4027
2024-12-08 23:21:34.997241: Pseudo dice [0.6956]
2024-12-08 23:21:34.997970: Epoch time: 87.79 s
2024-12-08 23:21:36.623896: 
2024-12-08 23:21:36.625483: Epoch 50
2024-12-08 23:21:36.626384: Current learning rate: 0.00955
2024-12-08 23:23:04.828030: Validation loss did not improve from -0.44677. Patience: 8/50
2024-12-08 23:23:04.829093: train_loss -0.6325
2024-12-08 23:23:04.830061: val_loss -0.3325
2024-12-08 23:23:04.830780: Pseudo dice [0.6476]
2024-12-08 23:23:04.831496: Epoch time: 88.21 s
2024-12-08 23:23:06.059268: 
2024-12-08 23:23:06.060900: Epoch 51
2024-12-08 23:23:06.062096: Current learning rate: 0.00954
2024-12-08 23:24:34.043960: Validation loss did not improve from -0.44677. Patience: 9/50
2024-12-08 23:24:34.044826: train_loss -0.6338
2024-12-08 23:24:34.045852: val_loss -0.3871
2024-12-08 23:24:34.046911: Pseudo dice [0.6675]
2024-12-08 23:24:34.048124: Epoch time: 87.99 s
2024-12-08 23:24:35.602718: 
2024-12-08 23:24:35.604341: Epoch 52
2024-12-08 23:24:35.605405: Current learning rate: 0.00953
2024-12-08 23:26:03.630767: Validation loss did not improve from -0.44677. Patience: 10/50
2024-12-08 23:26:03.631712: train_loss -0.6227
2024-12-08 23:26:03.632774: val_loss -0.3834
2024-12-08 23:26:03.633722: Pseudo dice [0.6662]
2024-12-08 23:26:03.634466: Epoch time: 88.03 s
2024-12-08 23:26:04.874310: 
2024-12-08 23:26:04.875849: Epoch 53
2024-12-08 23:26:04.876735: Current learning rate: 0.00952
2024-12-08 23:27:32.971289: Validation loss did not improve from -0.44677. Patience: 11/50
2024-12-08 23:27:32.972338: train_loss -0.6221
2024-12-08 23:27:32.973269: val_loss -0.3286
2024-12-08 23:27:32.973996: Pseudo dice [0.6391]
2024-12-08 23:27:32.974783: Epoch time: 88.1 s
2024-12-08 23:27:34.177789: 
2024-12-08 23:27:34.178927: Epoch 54
2024-12-08 23:27:34.179648: Current learning rate: 0.00951
2024-12-08 23:29:02.275791: Validation loss did not improve from -0.44677. Patience: 12/50
2024-12-08 23:29:02.276892: train_loss -0.6421
2024-12-08 23:29:02.277845: val_loss -0.3512
2024-12-08 23:29:02.278552: Pseudo dice [0.615]
2024-12-08 23:29:02.279250: Epoch time: 88.1 s
2024-12-08 23:29:03.838955: 
2024-12-08 23:29:03.840483: Epoch 55
2024-12-08 23:29:03.841197: Current learning rate: 0.0095
2024-12-08 23:30:31.987777: Validation loss improved from -0.44677 to -0.45421! Patience: 12/50
2024-12-08 23:30:31.988823: train_loss -0.6531
2024-12-08 23:30:31.989727: val_loss -0.4542
2024-12-08 23:30:31.990559: Pseudo dice [0.7087]
2024-12-08 23:30:31.991757: Epoch time: 88.15 s
2024-12-08 23:30:33.187534: 
2024-12-08 23:30:33.189031: Epoch 56
2024-12-08 23:30:33.189997: Current learning rate: 0.00949
2024-12-08 23:32:01.213006: Validation loss did not improve from -0.45421. Patience: 1/50
2024-12-08 23:32:01.213807: train_loss -0.6411
2024-12-08 23:32:01.214895: val_loss -0.342
2024-12-08 23:32:01.215830: Pseudo dice [0.6435]
2024-12-08 23:32:01.216733: Epoch time: 88.03 s
2024-12-08 23:32:02.422382: 
2024-12-08 23:32:02.423961: Epoch 57
2024-12-08 23:32:02.424838: Current learning rate: 0.00949
2024-12-08 23:33:30.469932: Validation loss did not improve from -0.45421. Patience: 2/50
2024-12-08 23:33:30.471003: train_loss -0.6463
2024-12-08 23:33:30.472248: val_loss -0.3261
2024-12-08 23:33:30.473232: Pseudo dice [0.6139]
2024-12-08 23:33:30.474164: Epoch time: 88.05 s
2024-12-08 23:33:31.717422: 
2024-12-08 23:33:31.719511: Epoch 58
2024-12-08 23:33:31.720502: Current learning rate: 0.00948
2024-12-08 23:34:59.719271: Validation loss did not improve from -0.45421. Patience: 3/50
2024-12-08 23:34:59.720356: train_loss -0.6511
2024-12-08 23:34:59.721361: val_loss -0.3702
2024-12-08 23:34:59.722438: Pseudo dice [0.6702]
2024-12-08 23:34:59.723404: Epoch time: 88.0 s
2024-12-08 23:35:00.961132: 
2024-12-08 23:35:00.963192: Epoch 59
2024-12-08 23:35:00.963940: Current learning rate: 0.00947
2024-12-08 23:36:28.958906: Validation loss did not improve from -0.45421. Patience: 4/50
2024-12-08 23:36:28.960247: train_loss -0.6553
2024-12-08 23:36:28.961094: val_loss -0.2469
2024-12-08 23:36:28.961797: Pseudo dice [0.5919]
2024-12-08 23:36:28.962479: Epoch time: 88.0 s
2024-12-08 23:36:30.561084: 
2024-12-08 23:36:30.563029: Epoch 60
2024-12-08 23:36:30.564106: Current learning rate: 0.00946
2024-12-08 23:37:58.537133: Validation loss did not improve from -0.45421. Patience: 5/50
2024-12-08 23:37:58.538532: train_loss -0.6516
2024-12-08 23:37:58.539457: val_loss -0.3962
2024-12-08 23:37:58.540235: Pseudo dice [0.6738]
2024-12-08 23:37:58.541006: Epoch time: 87.98 s
2024-12-08 23:37:59.782894: 
2024-12-08 23:37:59.784560: Epoch 61
2024-12-08 23:37:59.785460: Current learning rate: 0.00945
2024-12-08 23:39:27.794368: Validation loss did not improve from -0.45421. Patience: 6/50
2024-12-08 23:39:27.795481: train_loss -0.656
2024-12-08 23:39:27.796387: val_loss -0.1589
2024-12-08 23:39:27.797285: Pseudo dice [0.5453]
2024-12-08 23:39:27.798168: Epoch time: 88.01 s
2024-12-08 23:39:29.059697: 
2024-12-08 23:39:29.061232: Epoch 62
2024-12-08 23:39:29.061982: Current learning rate: 0.00944
2024-12-08 23:40:57.061866: Validation loss did not improve from -0.45421. Patience: 7/50
2024-12-08 23:40:57.062999: train_loss -0.651
2024-12-08 23:40:57.064054: val_loss -0.3572
2024-12-08 23:40:57.064787: Pseudo dice [0.6579]
2024-12-08 23:40:57.065624: Epoch time: 88.0 s
2024-12-08 23:40:58.698859: 
2024-12-08 23:40:58.700432: Epoch 63
2024-12-08 23:40:58.701436: Current learning rate: 0.00943
2024-12-08 23:42:26.682872: Validation loss did not improve from -0.45421. Patience: 8/50
2024-12-08 23:42:26.684089: train_loss -0.6493
2024-12-08 23:42:26.684963: val_loss -0.3491
2024-12-08 23:42:26.685817: Pseudo dice [0.6445]
2024-12-08 23:42:26.686535: Epoch time: 87.99 s
2024-12-08 23:42:27.955733: 
2024-12-08 23:42:27.957572: Epoch 64
2024-12-08 23:42:27.958459: Current learning rate: 0.00942
2024-12-08 23:43:56.112385: Validation loss did not improve from -0.45421. Patience: 9/50
2024-12-08 23:43:56.113410: train_loss -0.6556
2024-12-08 23:43:56.114718: val_loss -0.3977
2024-12-08 23:43:56.115732: Pseudo dice [0.667]
2024-12-08 23:43:56.116631: Epoch time: 88.16 s
2024-12-08 23:43:57.694022: 
2024-12-08 23:43:57.696109: Epoch 65
2024-12-08 23:43:57.696995: Current learning rate: 0.00941
2024-12-08 23:45:25.797545: Validation loss did not improve from -0.45421. Patience: 10/50
2024-12-08 23:45:25.798927: train_loss -0.651
2024-12-08 23:45:25.800052: val_loss -0.3026
2024-12-08 23:45:25.800923: Pseudo dice [0.6117]
2024-12-08 23:45:25.801808: Epoch time: 88.11 s
2024-12-08 23:45:27.024871: 
2024-12-08 23:45:27.026523: Epoch 66
2024-12-08 23:45:27.027508: Current learning rate: 0.0094
2024-12-08 23:46:55.243062: Validation loss did not improve from -0.45421. Patience: 11/50
2024-12-08 23:46:55.244494: train_loss -0.6625
2024-12-08 23:46:55.245564: val_loss -0.4
2024-12-08 23:46:55.246603: Pseudo dice [0.673]
2024-12-08 23:46:55.247455: Epoch time: 88.22 s
2024-12-08 23:46:56.508488: 
2024-12-08 23:46:56.510370: Epoch 67
2024-12-08 23:46:56.511301: Current learning rate: 0.00939
2024-12-08 23:48:24.666286: Validation loss did not improve from -0.45421. Patience: 12/50
2024-12-08 23:48:24.667386: train_loss -0.6684
2024-12-08 23:48:24.668280: val_loss -0.2701
2024-12-08 23:48:24.669003: Pseudo dice [0.5938]
2024-12-08 23:48:24.669909: Epoch time: 88.16 s
2024-12-08 23:48:25.911285: 
2024-12-08 23:48:25.913032: Epoch 68
2024-12-08 23:48:25.913921: Current learning rate: 0.00939
2024-12-08 23:49:54.087650: Validation loss improved from -0.45421 to -0.48316! Patience: 12/50
2024-12-08 23:49:54.088955: train_loss -0.6644
2024-12-08 23:49:54.090034: val_loss -0.4832
2024-12-08 23:49:54.090912: Pseudo dice [0.7125]
2024-12-08 23:49:54.091708: Epoch time: 88.18 s
2024-12-08 23:49:55.394367: 
2024-12-08 23:49:55.396178: Epoch 69
2024-12-08 23:49:55.397022: Current learning rate: 0.00938
2024-12-08 23:51:23.533419: Validation loss did not improve from -0.48316. Patience: 1/50
2024-12-08 23:51:23.534608: train_loss -0.6646
2024-12-08 23:51:23.535728: val_loss -0.1667
2024-12-08 23:51:23.536657: Pseudo dice [0.5644]
2024-12-08 23:51:23.537497: Epoch time: 88.14 s
2024-12-08 23:51:25.146670: 
2024-12-08 23:51:25.148560: Epoch 70
2024-12-08 23:51:25.149400: Current learning rate: 0.00937
2024-12-08 23:52:53.344672: Validation loss did not improve from -0.48316. Patience: 2/50
2024-12-08 23:52:53.345802: train_loss -0.6568
2024-12-08 23:52:53.346560: val_loss -0.2804
2024-12-08 23:52:53.347251: Pseudo dice [0.5882]
2024-12-08 23:52:53.347965: Epoch time: 88.2 s
2024-12-08 23:52:54.575876: 
2024-12-08 23:52:54.577714: Epoch 71
2024-12-08 23:52:54.578429: Current learning rate: 0.00936
2024-12-08 23:54:22.790715: Validation loss did not improve from -0.48316. Patience: 3/50
2024-12-08 23:54:22.791806: train_loss -0.6671
2024-12-08 23:54:22.792781: val_loss -0.3244
2024-12-08 23:54:22.793698: Pseudo dice [0.6343]
2024-12-08 23:54:22.794394: Epoch time: 88.22 s
2024-12-08 23:54:24.030940: 
2024-12-08 23:54:24.032574: Epoch 72
2024-12-08 23:54:24.033362: Current learning rate: 0.00935
2024-12-08 23:55:52.323741: Validation loss did not improve from -0.48316. Patience: 4/50
2024-12-08 23:55:52.325006: train_loss -0.6655
2024-12-08 23:55:52.326299: val_loss -0.2531
2024-12-08 23:55:52.327401: Pseudo dice [0.6053]
2024-12-08 23:55:52.328526: Epoch time: 88.29 s
2024-12-08 23:55:53.587498: 
2024-12-08 23:55:53.589483: Epoch 73
2024-12-08 23:55:53.590426: Current learning rate: 0.00934
2024-12-08 23:57:21.689710: Validation loss did not improve from -0.48316. Patience: 5/50
2024-12-08 23:57:21.690882: train_loss -0.6634
2024-12-08 23:57:21.691916: val_loss -0.33
2024-12-08 23:57:21.692666: Pseudo dice [0.6441]
2024-12-08 23:57:21.693395: Epoch time: 88.1 s
2024-12-08 23:57:23.325378: 
2024-12-08 23:57:23.327679: Epoch 74
2024-12-08 23:57:23.328699: Current learning rate: 0.00933
2024-12-08 23:58:51.870797: Validation loss did not improve from -0.48316. Patience: 6/50
2024-12-08 23:58:51.871973: train_loss -0.6707
2024-12-08 23:58:51.872909: val_loss -0.2574
2024-12-08 23:58:51.873653: Pseudo dice [0.6039]
2024-12-08 23:58:51.874659: Epoch time: 88.55 s
2024-12-08 23:58:53.524529: 
2024-12-08 23:58:53.525707: Epoch 75
2024-12-08 23:58:53.526490: Current learning rate: 0.00932
2024-12-09 00:00:21.875884: Validation loss did not improve from -0.48316. Patience: 7/50
2024-12-09 00:00:21.876903: train_loss -0.6726
2024-12-09 00:00:21.877831: val_loss -0.3638
2024-12-09 00:00:21.878566: Pseudo dice [0.6678]
2024-12-09 00:00:21.879215: Epoch time: 88.35 s
2024-12-09 00:00:23.190199: 
2024-12-09 00:00:23.191497: Epoch 76
2024-12-09 00:00:23.192279: Current learning rate: 0.00931
2024-12-09 00:01:51.410374: Validation loss did not improve from -0.48316. Patience: 8/50
2024-12-09 00:01:51.411223: train_loss -0.6755
2024-12-09 00:01:51.412178: val_loss -0.3632
2024-12-09 00:01:51.412992: Pseudo dice [0.658]
2024-12-09 00:01:51.413877: Epoch time: 88.22 s
2024-12-09 00:01:52.660350: 
2024-12-09 00:01:52.662292: Epoch 77
2024-12-09 00:01:52.663234: Current learning rate: 0.0093
2024-12-09 00:03:20.949492: Validation loss did not improve from -0.48316. Patience: 9/50
2024-12-09 00:03:20.950423: train_loss -0.6664
2024-12-09 00:03:20.951354: val_loss -0.2716
2024-12-09 00:03:20.952362: Pseudo dice [0.6224]
2024-12-09 00:03:20.953288: Epoch time: 88.29 s
2024-12-09 00:03:22.332620: 
2024-12-09 00:03:22.334503: Epoch 78
2024-12-09 00:03:22.335634: Current learning rate: 0.0093
2024-12-09 00:04:50.667975: Validation loss did not improve from -0.48316. Patience: 10/50
2024-12-09 00:04:50.669496: train_loss -0.6776
2024-12-09 00:04:50.670436: val_loss -0.3726
2024-12-09 00:04:50.671275: Pseudo dice [0.6627]
2024-12-09 00:04:50.672180: Epoch time: 88.34 s
2024-12-09 00:04:52.028775: 
2024-12-09 00:04:52.030269: Epoch 79
2024-12-09 00:04:52.030977: Current learning rate: 0.00929
2024-12-09 00:06:20.515718: Validation loss did not improve from -0.48316. Patience: 11/50
2024-12-09 00:06:20.516895: train_loss -0.6808
2024-12-09 00:06:20.517776: val_loss -0.306
2024-12-09 00:06:20.518524: Pseudo dice [0.6168]
2024-12-09 00:06:20.519205: Epoch time: 88.49 s
2024-12-09 00:06:22.153492: 
2024-12-09 00:06:22.155273: Epoch 80
2024-12-09 00:06:22.156078: Current learning rate: 0.00928
2024-12-09 00:07:50.602474: Validation loss did not improve from -0.48316. Patience: 12/50
2024-12-09 00:07:50.603769: train_loss -0.6853
2024-12-09 00:07:50.605037: val_loss -0.3413
2024-12-09 00:07:50.605947: Pseudo dice [0.6401]
2024-12-09 00:07:50.606927: Epoch time: 88.45 s
2024-12-09 00:07:51.891274: 
2024-12-09 00:07:51.892951: Epoch 81
2024-12-09 00:07:51.893946: Current learning rate: 0.00927
2024-12-09 00:09:19.971770: Validation loss did not improve from -0.48316. Patience: 13/50
2024-12-09 00:09:19.972898: train_loss -0.6664
2024-12-09 00:09:19.973847: val_loss -0.375
2024-12-09 00:09:19.974716: Pseudo dice [0.6691]
2024-12-09 00:09:19.975573: Epoch time: 88.08 s
2024-12-09 00:09:21.425943: 
2024-12-09 00:09:21.427927: Epoch 82
2024-12-09 00:09:21.428860: Current learning rate: 0.00926
2024-12-09 00:10:49.456305: Validation loss did not improve from -0.48316. Patience: 14/50
2024-12-09 00:10:49.457481: train_loss -0.6755
2024-12-09 00:10:49.458522: val_loss -0.2914
2024-12-09 00:10:49.459233: Pseudo dice [0.6297]
2024-12-09 00:10:49.460023: Epoch time: 88.03 s
2024-12-09 00:10:50.648201: 
2024-12-09 00:10:50.649723: Epoch 83
2024-12-09 00:10:50.650458: Current learning rate: 0.00925
2024-12-09 00:12:18.638799: Validation loss did not improve from -0.48316. Patience: 15/50
2024-12-09 00:12:18.640047: train_loss -0.6861
2024-12-09 00:12:18.640975: val_loss -0.4468
2024-12-09 00:12:18.641748: Pseudo dice [0.7093]
2024-12-09 00:12:18.642601: Epoch time: 87.99 s
2024-12-09 00:12:18.643375: Yayy! New best EMA pseudo Dice: 0.6442
2024-12-09 00:12:20.196340: 
2024-12-09 00:12:20.197480: Epoch 84
2024-12-09 00:12:20.198146: Current learning rate: 0.00924
2024-12-09 00:13:48.212745: Validation loss did not improve from -0.48316. Patience: 16/50
2024-12-09 00:13:48.213983: train_loss -0.6892
2024-12-09 00:13:48.215057: val_loss -0.3097
2024-12-09 00:13:48.216109: Pseudo dice [0.6208]
2024-12-09 00:13:48.216880: Epoch time: 88.02 s
2024-12-09 00:13:50.087940: 
2024-12-09 00:13:50.089439: Epoch 85
2024-12-09 00:13:50.090418: Current learning rate: 0.00923
2024-12-09 00:15:17.869604: Validation loss did not improve from -0.48316. Patience: 17/50
2024-12-09 00:15:17.870724: train_loss -0.6823
2024-12-09 00:15:17.871636: val_loss -0.3642
2024-12-09 00:15:17.872495: Pseudo dice [0.6687]
2024-12-09 00:15:17.873278: Epoch time: 87.78 s
2024-12-09 00:15:17.874066: Yayy! New best EMA pseudo Dice: 0.6445
2024-12-09 00:15:19.458849: 
2024-12-09 00:15:19.460737: Epoch 86
2024-12-09 00:15:19.461609: Current learning rate: 0.00922
2024-12-09 00:16:47.345000: Validation loss did not improve from -0.48316. Patience: 18/50
2024-12-09 00:16:47.348389: train_loss -0.6869
2024-12-09 00:16:47.349247: val_loss -0.3673
2024-12-09 00:16:47.350024: Pseudo dice [0.6462]
2024-12-09 00:16:47.350746: Epoch time: 87.89 s
2024-12-09 00:16:47.351481: Yayy! New best EMA pseudo Dice: 0.6447
2024-12-09 00:16:48.956665: 
2024-12-09 00:16:48.958745: Epoch 87
2024-12-09 00:16:48.959715: Current learning rate: 0.00921
2024-12-09 00:18:16.813688: Validation loss did not improve from -0.48316. Patience: 19/50
2024-12-09 00:18:16.814615: train_loss -0.6834
2024-12-09 00:18:16.815619: val_loss -0.4288
2024-12-09 00:18:16.816312: Pseudo dice [0.6964]
2024-12-09 00:18:16.817135: Epoch time: 87.86 s
2024-12-09 00:18:16.817896: Yayy! New best EMA pseudo Dice: 0.6498
2024-12-09 00:18:18.373585: 
2024-12-09 00:18:18.375181: Epoch 88
2024-12-09 00:18:18.375988: Current learning rate: 0.0092
2024-12-09 00:19:46.219941: Validation loss did not improve from -0.48316. Patience: 20/50
2024-12-09 00:19:46.220901: train_loss -0.6907
2024-12-09 00:19:46.221909: val_loss -0.3986
2024-12-09 00:19:46.222909: Pseudo dice [0.6845]
2024-12-09 00:19:46.223798: Epoch time: 87.85 s
2024-12-09 00:19:46.224850: Yayy! New best EMA pseudo Dice: 0.6533
2024-12-09 00:19:47.780320: 
2024-12-09 00:19:47.781311: Epoch 89
2024-12-09 00:19:47.782146: Current learning rate: 0.0092
2024-12-09 00:21:15.551682: Validation loss did not improve from -0.48316. Patience: 21/50
2024-12-09 00:21:15.552593: train_loss -0.6885
2024-12-09 00:21:15.553484: val_loss -0.2705
2024-12-09 00:21:15.554280: Pseudo dice [0.6147]
2024-12-09 00:21:15.555072: Epoch time: 87.77 s
2024-12-09 00:21:17.089273: 
2024-12-09 00:21:17.090255: Epoch 90
2024-12-09 00:21:17.091111: Current learning rate: 0.00919
2024-12-09 00:22:44.819031: Validation loss did not improve from -0.48316. Patience: 22/50
2024-12-09 00:22:44.820427: train_loss -0.6855
2024-12-09 00:22:44.821347: val_loss -0.1755
2024-12-09 00:22:44.822274: Pseudo dice [0.5433]
2024-12-09 00:22:44.823168: Epoch time: 87.73 s
2024-12-09 00:22:46.040540: 
2024-12-09 00:22:46.041729: Epoch 91
2024-12-09 00:22:46.042537: Current learning rate: 0.00918
2024-12-09 00:24:13.710652: Validation loss did not improve from -0.48316. Patience: 23/50
2024-12-09 00:24:13.711748: train_loss -0.6883
2024-12-09 00:24:13.712785: val_loss -0.4547
2024-12-09 00:24:13.713752: Pseudo dice [0.715]
2024-12-09 00:24:13.714648: Epoch time: 87.67 s
2024-12-09 00:24:15.023884: 
2024-12-09 00:24:15.026327: Epoch 92
2024-12-09 00:24:15.028123: Current learning rate: 0.00917
2024-12-09 00:25:43.394412: Validation loss did not improve from -0.48316. Patience: 24/50
2024-12-09 00:25:43.395736: train_loss -0.6808
2024-12-09 00:25:43.396744: val_loss -0.4636
2024-12-09 00:25:43.397614: Pseudo dice [0.7108]
2024-12-09 00:25:43.398428: Epoch time: 88.38 s
2024-12-09 00:25:44.600894: 
2024-12-09 00:25:44.602657: Epoch 93
2024-12-09 00:25:44.603551: Current learning rate: 0.00916
2024-12-09 00:27:12.975723: Validation loss did not improve from -0.48316. Patience: 25/50
2024-12-09 00:27:12.978816: train_loss -0.6889
2024-12-09 00:27:12.980031: val_loss -0.3646
2024-12-09 00:27:12.980857: Pseudo dice [0.6655]
2024-12-09 00:27:12.981881: Epoch time: 88.38 s
2024-12-09 00:27:12.982809: Yayy! New best EMA pseudo Dice: 0.6541
2024-12-09 00:27:14.553013: 
2024-12-09 00:27:14.554971: Epoch 94
2024-12-09 00:27:14.556154: Current learning rate: 0.00915
2024-12-09 00:28:43.151992: Validation loss did not improve from -0.48316. Patience: 26/50
2024-12-09 00:28:43.153111: train_loss -0.6878
2024-12-09 00:28:43.154124: val_loss -0.3113
2024-12-09 00:28:43.154980: Pseudo dice [0.6507]
2024-12-09 00:28:43.155748: Epoch time: 88.6 s
2024-12-09 00:28:45.021418: 
2024-12-09 00:28:45.023054: Epoch 95
2024-12-09 00:28:45.023786: Current learning rate: 0.00914
2024-12-09 00:30:13.580590: Validation loss did not improve from -0.48316. Patience: 27/50
2024-12-09 00:30:13.581846: train_loss -0.6968
2024-12-09 00:30:13.582953: val_loss -0.2876
2024-12-09 00:30:13.583936: Pseudo dice [0.607]
2024-12-09 00:30:13.584654: Epoch time: 88.56 s
2024-12-09 00:30:15.118901: 
2024-12-09 00:30:15.120616: Epoch 96
2024-12-09 00:30:15.121465: Current learning rate: 0.00913
2024-12-09 00:31:43.497788: Validation loss did not improve from -0.48316. Patience: 28/50
2024-12-09 00:31:43.498971: train_loss -0.6918
2024-12-09 00:31:43.500004: val_loss -0.337
2024-12-09 00:31:43.500888: Pseudo dice [0.659]
2024-12-09 00:31:43.501738: Epoch time: 88.38 s
2024-12-09 00:31:44.823617: 
2024-12-09 00:31:44.825185: Epoch 97
2024-12-09 00:31:44.826006: Current learning rate: 0.00912
2024-12-09 00:33:13.212872: Validation loss did not improve from -0.48316. Patience: 29/50
2024-12-09 00:33:13.214224: train_loss -0.7015
2024-12-09 00:33:13.215443: val_loss -0.4673
2024-12-09 00:33:13.216385: Pseudo dice [0.7118]
2024-12-09 00:33:13.217265: Epoch time: 88.39 s
2024-12-09 00:33:13.218369: Yayy! New best EMA pseudo Dice: 0.6563
2024-12-09 00:33:14.753284: 
2024-12-09 00:33:14.755143: Epoch 98
2024-12-09 00:33:14.756221: Current learning rate: 0.00911
2024-12-09 00:34:43.142189: Validation loss did not improve from -0.48316. Patience: 30/50
2024-12-09 00:34:43.143249: train_loss -0.6932
2024-12-09 00:34:43.144345: val_loss -0.4682
2024-12-09 00:34:43.145365: Pseudo dice [0.7133]
2024-12-09 00:34:43.146237: Epoch time: 88.39 s
2024-12-09 00:34:43.147238: Yayy! New best EMA pseudo Dice: 0.662
2024-12-09 00:34:44.778242: 
2024-12-09 00:34:44.779831: Epoch 99
2024-12-09 00:34:44.780680: Current learning rate: 0.0091
2024-12-09 00:36:13.189085: Validation loss did not improve from -0.48316. Patience: 31/50
2024-12-09 00:36:13.190240: train_loss -0.6993
2024-12-09 00:36:13.191074: val_loss -0.4179
2024-12-09 00:36:13.191804: Pseudo dice [0.6943]
2024-12-09 00:36:13.192542: Epoch time: 88.41 s
2024-12-09 00:36:13.548130: Yayy! New best EMA pseudo Dice: 0.6652
2024-12-09 00:36:15.143684: 
2024-12-09 00:36:15.145609: Epoch 100
2024-12-09 00:36:15.146764: Current learning rate: 0.0091
2024-12-09 00:37:43.519925: Validation loss did not improve from -0.48316. Patience: 32/50
2024-12-09 00:37:43.521392: train_loss -0.6898
2024-12-09 00:37:43.522851: val_loss -0.3241
2024-12-09 00:37:43.523743: Pseudo dice [0.6385]
2024-12-09 00:37:43.524557: Epoch time: 88.38 s
2024-12-09 00:37:44.768021: 
2024-12-09 00:37:44.769672: Epoch 101
2024-12-09 00:37:44.770545: Current learning rate: 0.00909
2024-12-09 00:39:13.215123: Validation loss did not improve from -0.48316. Patience: 33/50
2024-12-09 00:39:13.216892: train_loss -0.6807
2024-12-09 00:39:13.218008: val_loss -0.314
2024-12-09 00:39:13.219285: Pseudo dice [0.6085]
2024-12-09 00:39:13.220567: Epoch time: 88.45 s
2024-12-09 00:39:14.438951: 
2024-12-09 00:39:14.440713: Epoch 102
2024-12-09 00:39:14.441625: Current learning rate: 0.00908
2024-12-09 00:40:42.716171: Validation loss did not improve from -0.48316. Patience: 34/50
2024-12-09 00:40:42.717289: train_loss -0.6834
2024-12-09 00:40:42.718650: val_loss -0.3456
2024-12-09 00:40:42.719416: Pseudo dice [0.6481]
2024-12-09 00:40:42.720399: Epoch time: 88.28 s
2024-12-09 00:40:43.946636: 
2024-12-09 00:40:43.948500: Epoch 103
2024-12-09 00:40:43.949634: Current learning rate: 0.00907
2024-12-09 00:42:12.218994: Validation loss did not improve from -0.48316. Patience: 35/50
2024-12-09 00:42:12.220333: train_loss -0.6864
2024-12-09 00:42:12.221376: val_loss -0.4048
2024-12-09 00:42:12.222399: Pseudo dice [0.6932]
2024-12-09 00:42:12.223316: Epoch time: 88.27 s
2024-12-09 00:42:13.434743: 
2024-12-09 00:42:13.436913: Epoch 104
2024-12-09 00:42:13.437806: Current learning rate: 0.00906
2024-12-09 00:43:41.577179: Validation loss did not improve from -0.48316. Patience: 36/50
2024-12-09 00:43:41.578361: train_loss -0.6956
2024-12-09 00:43:41.579648: val_loss -0.3544
2024-12-09 00:43:41.580579: Pseudo dice [0.6638]
2024-12-09 00:43:41.581334: Epoch time: 88.15 s
2024-12-09 00:43:43.142052: 
2024-12-09 00:43:43.143991: Epoch 105
2024-12-09 00:43:43.145053: Current learning rate: 0.00905
2024-12-09 00:45:11.230564: Validation loss did not improve from -0.48316. Patience: 37/50
2024-12-09 00:45:11.231460: train_loss -0.7005
2024-12-09 00:45:11.232341: val_loss -0.3124
2024-12-09 00:45:11.233128: Pseudo dice [0.6461]
2024-12-09 00:45:11.233967: Epoch time: 88.09 s
2024-12-09 00:45:12.426754: 
2024-12-09 00:45:12.428005: Epoch 106
2024-12-09 00:45:12.428756: Current learning rate: 0.00904
2024-12-09 00:46:39.913572: Validation loss did not improve from -0.48316. Patience: 38/50
2024-12-09 00:46:39.914567: train_loss -0.7026
2024-12-09 00:46:39.915491: val_loss -0.0991
2024-12-09 00:46:39.916246: Pseudo dice [0.5346]
2024-12-09 00:46:39.917133: Epoch time: 87.49 s
2024-12-09 00:46:41.589882: 
2024-12-09 00:46:41.591347: Epoch 107
2024-12-09 00:46:41.592557: Current learning rate: 0.00903
2024-12-09 00:48:09.628767: Validation loss did not improve from -0.48316. Patience: 39/50
2024-12-09 00:48:09.629578: train_loss -0.7074
2024-12-09 00:48:09.630665: val_loss -0.3703
2024-12-09 00:48:09.631565: Pseudo dice [0.6728]
2024-12-09 00:48:09.632449: Epoch time: 88.04 s
2024-12-09 00:48:10.871676: 
2024-12-09 00:48:10.873072: Epoch 108
2024-12-09 00:48:10.873966: Current learning rate: 0.00902
2024-12-09 00:49:38.930804: Validation loss did not improve from -0.48316. Patience: 40/50
2024-12-09 00:49:38.931705: train_loss -0.7099
2024-12-09 00:49:38.933070: val_loss -0.3453
2024-12-09 00:49:38.934071: Pseudo dice [0.6653]
2024-12-09 00:49:38.934941: Epoch time: 88.06 s
2024-12-09 00:49:40.215505: 
2024-12-09 00:49:40.217106: Epoch 109
2024-12-09 00:49:40.217883: Current learning rate: 0.00901
2024-12-09 00:51:08.286924: Validation loss did not improve from -0.48316. Patience: 41/50
2024-12-09 00:51:08.288048: train_loss -0.7059
2024-12-09 00:51:08.288942: val_loss -0.3017
2024-12-09 00:51:08.289754: Pseudo dice [0.6312]
2024-12-09 00:51:08.290485: Epoch time: 88.07 s
2024-12-09 00:51:09.875548: 
2024-12-09 00:51:09.877504: Epoch 110
2024-12-09 00:51:09.878361: Current learning rate: 0.009
2024-12-09 00:52:38.081197: Validation loss did not improve from -0.48316. Patience: 42/50
2024-12-09 00:52:38.082268: train_loss -0.7009
2024-12-09 00:52:38.083183: val_loss -0.4046
2024-12-09 00:52:38.084229: Pseudo dice [0.6833]
2024-12-09 00:52:38.085073: Epoch time: 88.21 s
2024-12-09 00:52:39.324664: 
2024-12-09 00:52:39.326345: Epoch 111
2024-12-09 00:52:39.327208: Current learning rate: 0.009
2024-12-09 00:54:07.463975: Validation loss did not improve from -0.48316. Patience: 43/50
2024-12-09 00:54:07.465103: train_loss -0.7056
2024-12-09 00:54:07.466002: val_loss -0.2804
2024-12-09 00:54:07.466686: Pseudo dice [0.6054]
2024-12-09 00:54:07.467353: Epoch time: 88.14 s
2024-12-09 00:54:08.709712: 
2024-12-09 00:54:08.711027: Epoch 112
2024-12-09 00:54:08.711850: Current learning rate: 0.00899
2024-12-09 00:55:36.931996: Validation loss did not improve from -0.48316. Patience: 44/50
2024-12-09 00:55:36.933645: train_loss -0.7116
2024-12-09 00:55:36.934944: val_loss -0.349
2024-12-09 00:55:36.935649: Pseudo dice [0.6587]
2024-12-09 00:55:36.936572: Epoch time: 88.22 s
2024-12-09 00:55:38.166168: 
2024-12-09 00:55:38.167155: Epoch 113
2024-12-09 00:55:38.167864: Current learning rate: 0.00898
2024-12-09 00:57:06.338096: Validation loss did not improve from -0.48316. Patience: 45/50
2024-12-09 00:57:06.339451: train_loss -0.7067
2024-12-09 00:57:06.340371: val_loss -0.3075
2024-12-09 00:57:06.341123: Pseudo dice [0.6137]
2024-12-09 00:57:06.341905: Epoch time: 88.17 s
2024-12-09 00:57:07.596947: 
2024-12-09 00:57:07.598329: Epoch 114
2024-12-09 00:57:07.599075: Current learning rate: 0.00897
2024-12-09 00:58:35.801312: Validation loss did not improve from -0.48316. Patience: 46/50
2024-12-09 00:58:35.802144: train_loss -0.7175
2024-12-09 00:58:35.802988: val_loss -0.3313
2024-12-09 00:58:35.803637: Pseudo dice [0.6397]
2024-12-09 00:58:35.804305: Epoch time: 88.21 s
2024-12-09 00:58:37.433397: 
2024-12-09 00:58:37.435056: Epoch 115
2024-12-09 00:58:37.436099: Current learning rate: 0.00896
2024-12-09 01:00:05.720036: Validation loss did not improve from -0.48316. Patience: 47/50
2024-12-09 01:00:05.721080: train_loss -0.7114
2024-12-09 01:00:05.722108: val_loss -0.4083
2024-12-09 01:00:05.722925: Pseudo dice [0.697]
2024-12-09 01:00:05.723591: Epoch time: 88.29 s
2024-12-09 01:00:06.955842: 
2024-12-09 01:00:06.957335: Epoch 116
2024-12-09 01:00:06.958081: Current learning rate: 0.00895
2024-12-09 01:01:35.170917: Validation loss did not improve from -0.48316. Patience: 48/50
2024-12-09 01:01:35.171932: train_loss -0.7156
2024-12-09 01:01:35.173232: val_loss -0.3209
2024-12-09 01:01:35.174284: Pseudo dice [0.6466]
2024-12-09 01:01:35.175280: Epoch time: 88.22 s
2024-12-09 01:01:36.404187: 
2024-12-09 01:01:36.405608: Epoch 117
2024-12-09 01:01:36.406678: Current learning rate: 0.00894
2024-12-09 01:03:04.581869: Validation loss did not improve from -0.48316. Patience: 49/50
2024-12-09 01:03:04.583240: train_loss -0.7098
2024-12-09 01:03:04.584525: val_loss -0.3393
2024-12-09 01:03:04.585571: Pseudo dice [0.6293]
2024-12-09 01:03:04.586401: Epoch time: 88.18 s
2024-12-09 01:03:06.204004: 
2024-12-09 01:03:06.205732: Epoch 118
2024-12-09 01:03:06.206560: Current learning rate: 0.00893
2024-12-09 01:04:34.374211: Validation loss did not improve from -0.48316. Patience: 50/50
2024-12-09 01:04:34.375249: train_loss -0.7044
2024-12-09 01:04:34.376671: val_loss -0.3795
2024-12-09 01:04:34.377416: Pseudo dice [0.6684]
2024-12-09 01:04:34.378198: Epoch time: 88.17 s
2024-12-09 01:04:35.616216: Patience reached. Stopping training.
2024-12-09 01:04:36.048826: Training done.
2024-12-09 01:04:36.287760: Using splits from existing split file: /home/gridsan/nchutisilp/datasets/nnUNet_Datasets/nnUNet_preprocessed/Dataset309_Sohee_Calcium_OCT_CrossValidation/splits_final.json
2024-12-09 01:04:36.297734: The split file contains 5 splits.
2024-12-09 01:04:36.298963: Desired fold for training: 3
2024-12-09 01:04:36.300000: This split has 7 training and 1 validation cases.
2024-12-09 01:04:36.301157: predicting 701-013
2024-12-09 01:04:36.336400: 701-013, shape torch.Size([1, 375, 498, 498]), rank 0
2024-12-09 01:06:43.662418: Validation complete
2024-12-09 01:06:43.663484: Mean Validation Dice:  0.6774025467358696
2024-12-08 22:06:18.799147: unpacking done...
2024-12-08 22:06:18.808183: Unable to plot network architecture: nnUNet_compile is enabled!
2024-12-08 22:06:18.868418: 
2024-12-08 22:06:18.870234: Epoch 0
2024-12-08 22:06:18.872560: Current learning rate: 0.01
2024-12-08 22:08:30.502070: Validation loss improved from 1000.00000 to -0.25466! Patience: 0/50
2024-12-08 22:08:30.503498: train_loss -0.0993
2024-12-08 22:08:30.504590: val_loss -0.2547
2024-12-08 22:08:30.505253: Pseudo dice [0.5594]
2024-12-08 22:08:30.505826: Epoch time: 131.64 s
2024-12-08 22:08:30.506650: Yayy! New best EMA pseudo Dice: 0.5594
2024-12-08 22:08:32.028762: 
2024-12-08 22:08:32.030740: Epoch 1
2024-12-08 22:08:32.031618: Current learning rate: 0.00999
2024-12-08 22:09:59.604405: Validation loss improved from -0.25466 to -0.31670! Patience: 0/50
2024-12-08 22:09:59.605556: train_loss -0.2199
2024-12-08 22:09:59.606493: val_loss -0.3167
2024-12-08 22:09:59.607268: Pseudo dice [0.5967]
2024-12-08 22:09:59.607958: Epoch time: 87.58 s
2024-12-08 22:09:59.608536: Yayy! New best EMA pseudo Dice: 0.5631
2024-12-08 22:10:01.192998: 
2024-12-08 22:10:01.195068: Epoch 2
2024-12-08 22:10:01.196271: Current learning rate: 0.00998
2024-12-08 22:11:29.297320: Validation loss improved from -0.31670 to -0.33392! Patience: 0/50
2024-12-08 22:11:29.298249: train_loss -0.2703
2024-12-08 22:11:29.299225: val_loss -0.3339
2024-12-08 22:11:29.300066: Pseudo dice [0.6174]
2024-12-08 22:11:29.300862: Epoch time: 88.11 s
2024-12-08 22:11:29.301656: Yayy! New best EMA pseudo Dice: 0.5686
2024-12-08 22:11:31.004907: 
2024-12-08 22:11:31.006639: Epoch 3
2024-12-08 22:11:31.007515: Current learning rate: 0.00997
2024-12-08 22:12:59.528378: Validation loss improved from -0.33392 to -0.35841! Patience: 0/50
2024-12-08 22:12:59.529689: train_loss -0.3134
2024-12-08 22:12:59.530630: val_loss -0.3584
2024-12-08 22:12:59.531668: Pseudo dice [0.6157]
2024-12-08 22:12:59.532688: Epoch time: 88.53 s
2024-12-08 22:12:59.533672: Yayy! New best EMA pseudo Dice: 0.5733
2024-12-08 22:13:01.128011: 
2024-12-08 22:13:01.129710: Epoch 4
2024-12-08 22:13:01.130708: Current learning rate: 0.00996
2024-12-08 22:14:29.662918: Validation loss improved from -0.35841 to -0.39053! Patience: 0/50
2024-12-08 22:14:29.663888: train_loss -0.3356
2024-12-08 22:14:29.664970: val_loss -0.3905
2024-12-08 22:14:29.665892: Pseudo dice [0.6546]
2024-12-08 22:14:29.667257: Epoch time: 88.54 s
2024-12-08 22:14:30.040666: Yayy! New best EMA pseudo Dice: 0.5814
2024-12-08 22:14:31.725838: 
2024-12-08 22:14:31.727738: Epoch 5
2024-12-08 22:14:31.728877: Current learning rate: 0.00995
2024-12-08 22:16:00.248279: Validation loss improved from -0.39053 to -0.42714! Patience: 0/50
2024-12-08 22:16:00.249365: train_loss -0.3645
2024-12-08 22:16:00.250185: val_loss -0.4271
2024-12-08 22:16:00.251166: Pseudo dice [0.6613]
2024-12-08 22:16:00.251985: Epoch time: 88.52 s
2024-12-08 22:16:00.252641: Yayy! New best EMA pseudo Dice: 0.5894
2024-12-08 22:16:01.805932: 
2024-12-08 22:16:01.807360: Epoch 6
2024-12-08 22:16:01.808119: Current learning rate: 0.00995
2024-12-08 22:17:30.391501: Validation loss improved from -0.42714 to -0.42956! Patience: 0/50
2024-12-08 22:17:30.392404: train_loss -0.4003
2024-12-08 22:17:30.393218: val_loss -0.4296
2024-12-08 22:17:30.393941: Pseudo dice [0.6701]
2024-12-08 22:17:30.394749: Epoch time: 88.59 s
2024-12-08 22:17:30.395576: Yayy! New best EMA pseudo Dice: 0.5975
2024-12-08 22:17:32.039976: 
2024-12-08 22:17:32.041354: Epoch 7
2024-12-08 22:17:32.042229: Current learning rate: 0.00994
2024-12-08 22:19:00.685824: Validation loss did not improve from -0.42956. Patience: 1/50
2024-12-08 22:19:00.686929: train_loss -0.4024
2024-12-08 22:19:00.687689: val_loss -0.4292
2024-12-08 22:19:00.688327: Pseudo dice [0.6601]
2024-12-08 22:19:00.689079: Epoch time: 88.65 s
2024-12-08 22:19:00.689708: Yayy! New best EMA pseudo Dice: 0.6037
2024-12-08 22:19:02.733151: 
2024-12-08 22:19:02.734886: Epoch 8
2024-12-08 22:19:02.735701: Current learning rate: 0.00993
2024-12-08 22:20:31.562595: Validation loss improved from -0.42956 to -0.49013! Patience: 1/50
2024-12-08 22:20:31.563926: train_loss -0.437
2024-12-08 22:20:31.564850: val_loss -0.4901
2024-12-08 22:20:31.565619: Pseudo dice [0.6984]
2024-12-08 22:20:31.566282: Epoch time: 88.83 s
2024-12-08 22:20:31.566931: Yayy! New best EMA pseudo Dice: 0.6132
2024-12-08 22:20:33.214118: 
2024-12-08 22:20:33.215656: Epoch 9
2024-12-08 22:20:33.216404: Current learning rate: 0.00992
2024-12-08 22:22:01.976873: Validation loss did not improve from -0.49013. Patience: 1/50
2024-12-08 22:22:01.978188: train_loss -0.4597
2024-12-08 22:22:01.979035: val_loss -0.4619
2024-12-08 22:22:01.979843: Pseudo dice [0.6911]
2024-12-08 22:22:01.980571: Epoch time: 88.76 s
2024-12-08 22:22:02.335769: Yayy! New best EMA pseudo Dice: 0.621
2024-12-08 22:22:03.955887: 
2024-12-08 22:22:03.957739: Epoch 10
2024-12-08 22:22:03.958636: Current learning rate: 0.00991
2024-12-08 22:23:32.713735: Validation loss did not improve from -0.49013. Patience: 2/50
2024-12-08 22:23:32.714812: train_loss -0.4554
2024-12-08 22:23:32.715705: val_loss -0.4438
2024-12-08 22:23:32.716408: Pseudo dice [0.6739]
2024-12-08 22:23:32.717106: Epoch time: 88.76 s
2024-12-08 22:23:32.717859: Yayy! New best EMA pseudo Dice: 0.6263
2024-12-08 22:23:34.307265: 
2024-12-08 22:23:34.308127: Epoch 11
2024-12-08 22:23:34.308740: Current learning rate: 0.0099
2024-12-08 22:25:03.112951: Validation loss did not improve from -0.49013. Patience: 3/50
2024-12-08 22:25:03.114095: train_loss -0.4633
2024-12-08 22:25:03.115141: val_loss -0.4358
2024-12-08 22:25:03.115851: Pseudo dice [0.6618]
2024-12-08 22:25:03.116650: Epoch time: 88.81 s
2024-12-08 22:25:03.117400: Yayy! New best EMA pseudo Dice: 0.6298
2024-12-08 22:25:04.753356: 
2024-12-08 22:25:04.755208: Epoch 12
2024-12-08 22:25:04.755889: Current learning rate: 0.00989
2024-12-08 22:26:33.265355: Validation loss did not improve from -0.49013. Patience: 4/50
2024-12-08 22:26:33.266320: train_loss -0.4743
2024-12-08 22:26:33.267053: val_loss -0.4834
2024-12-08 22:26:33.267766: Pseudo dice [0.7001]
2024-12-08 22:26:33.268436: Epoch time: 88.51 s
2024-12-08 22:26:33.269055: Yayy! New best EMA pseudo Dice: 0.6368
2024-12-08 22:26:34.870566: 
2024-12-08 22:26:34.872083: Epoch 13
2024-12-08 22:26:34.872920: Current learning rate: 0.00988
2024-12-08 22:28:03.029873: Validation loss improved from -0.49013 to -0.50829! Patience: 4/50
2024-12-08 22:28:03.030927: train_loss -0.5003
2024-12-08 22:28:03.031827: val_loss -0.5083
2024-12-08 22:28:03.032718: Pseudo dice [0.7175]
2024-12-08 22:28:03.033429: Epoch time: 88.16 s
2024-12-08 22:28:03.034203: Yayy! New best EMA pseudo Dice: 0.6449
2024-12-08 22:28:04.658778: 
2024-12-08 22:28:04.660586: Epoch 14
2024-12-08 22:28:04.661366: Current learning rate: 0.00987
2024-12-08 22:29:32.536287: Validation loss did not improve from -0.50829. Patience: 1/50
2024-12-08 22:29:32.537229: train_loss -0.4934
2024-12-08 22:29:32.538041: val_loss -0.472
2024-12-08 22:29:32.538681: Pseudo dice [0.6879]
2024-12-08 22:29:32.539356: Epoch time: 87.88 s
2024-12-08 22:29:32.889867: Yayy! New best EMA pseudo Dice: 0.6492
2024-12-08 22:29:34.497224: 
2024-12-08 22:29:34.498981: Epoch 15
2024-12-08 22:29:34.499903: Current learning rate: 0.00986
2024-12-08 22:31:02.408792: Validation loss did not improve from -0.50829. Patience: 2/50
2024-12-08 22:31:02.409767: train_loss -0.5038
2024-12-08 22:31:02.410794: val_loss -0.4741
2024-12-08 22:31:02.411512: Pseudo dice [0.7044]
2024-12-08 22:31:02.412211: Epoch time: 87.91 s
2024-12-08 22:31:02.413113: Yayy! New best EMA pseudo Dice: 0.6547
2024-12-08 22:31:04.073136: 
2024-12-08 22:31:04.075186: Epoch 16
2024-12-08 22:31:04.076241: Current learning rate: 0.00986
2024-12-08 22:32:32.015063: Validation loss did not improve from -0.50829. Patience: 3/50
2024-12-08 22:32:32.016069: train_loss -0.5093
2024-12-08 22:32:32.017018: val_loss -0.5015
2024-12-08 22:32:32.017756: Pseudo dice [0.7062]
2024-12-08 22:32:32.018533: Epoch time: 87.94 s
2024-12-08 22:32:32.019312: Yayy! New best EMA pseudo Dice: 0.6599
2024-12-08 22:32:33.667742: 
2024-12-08 22:32:33.669864: Epoch 17
2024-12-08 22:32:33.670730: Current learning rate: 0.00985
2024-12-08 22:34:02.292095: Validation loss did not improve from -0.50829. Patience: 4/50
2024-12-08 22:34:02.292991: train_loss -0.5142
2024-12-08 22:34:02.294097: val_loss -0.4519
2024-12-08 22:34:02.294854: Pseudo dice [0.6802]
2024-12-08 22:34:02.295566: Epoch time: 88.63 s
2024-12-08 22:34:02.296331: Yayy! New best EMA pseudo Dice: 0.6619
2024-12-08 22:34:04.405156: 
2024-12-08 22:34:04.407130: Epoch 18
2024-12-08 22:34:04.408124: Current learning rate: 0.00984
2024-12-08 22:35:33.329014: Validation loss improved from -0.50829 to -0.52408! Patience: 4/50
2024-12-08 22:35:33.330288: train_loss -0.5109
2024-12-08 22:35:33.331156: val_loss -0.5241
2024-12-08 22:35:33.331760: Pseudo dice [0.7221]
2024-12-08 22:35:33.332503: Epoch time: 88.93 s
2024-12-08 22:35:33.333173: Yayy! New best EMA pseudo Dice: 0.6679
2024-12-08 22:35:34.965808: 
2024-12-08 22:35:34.967487: Epoch 19
2024-12-08 22:35:34.968362: Current learning rate: 0.00983
2024-12-08 22:37:03.988113: Validation loss did not improve from -0.52408. Patience: 1/50
2024-12-08 22:37:03.989206: train_loss -0.5255
2024-12-08 22:37:03.990151: val_loss -0.4841
2024-12-08 22:37:03.990912: Pseudo dice [0.7012]
2024-12-08 22:37:03.991715: Epoch time: 89.02 s
2024-12-08 22:37:04.347670: Yayy! New best EMA pseudo Dice: 0.6712
2024-12-08 22:37:05.989135: 
2024-12-08 22:37:05.990800: Epoch 20
2024-12-08 22:37:05.991611: Current learning rate: 0.00982
2024-12-08 22:38:35.102801: Validation loss did not improve from -0.52408. Patience: 2/50
2024-12-08 22:38:35.104299: train_loss -0.5359
2024-12-08 22:38:35.105149: val_loss -0.4965
2024-12-08 22:38:35.105968: Pseudo dice [0.7108]
2024-12-08 22:38:35.106868: Epoch time: 89.12 s
2024-12-08 22:38:35.107660: Yayy! New best EMA pseudo Dice: 0.6752
2024-12-08 22:38:36.798258: 
2024-12-08 22:38:36.800193: Epoch 21
2024-12-08 22:38:36.801275: Current learning rate: 0.00981
2024-12-08 22:40:05.874659: Validation loss did not improve from -0.52408. Patience: 3/50
2024-12-08 22:40:05.875517: train_loss -0.5381
2024-12-08 22:40:05.876367: val_loss -0.5056
2024-12-08 22:40:05.877136: Pseudo dice [0.7044]
2024-12-08 22:40:05.877942: Epoch time: 89.08 s
2024-12-08 22:40:05.878675: Yayy! New best EMA pseudo Dice: 0.6781
2024-12-08 22:40:07.454814: 
2024-12-08 22:40:07.456566: Epoch 22
2024-12-08 22:40:07.457660: Current learning rate: 0.0098
2024-12-08 22:41:36.491410: Validation loss did not improve from -0.52408. Patience: 4/50
2024-12-08 22:41:36.492857: train_loss -0.5349
2024-12-08 22:41:36.493685: val_loss -0.5083
2024-12-08 22:41:36.494380: Pseudo dice [0.7043]
2024-12-08 22:41:36.495064: Epoch time: 89.04 s
2024-12-08 22:41:36.495697: Yayy! New best EMA pseudo Dice: 0.6807
2024-12-08 22:41:38.075078: 
2024-12-08 22:41:38.077025: Epoch 23
2024-12-08 22:41:38.077864: Current learning rate: 0.00979
2024-12-08 22:43:07.021009: Validation loss improved from -0.52408 to -0.54066! Patience: 4/50
2024-12-08 22:43:07.021896: train_loss -0.5631
2024-12-08 22:43:07.022661: val_loss -0.5407
2024-12-08 22:43:07.023476: Pseudo dice [0.738]
2024-12-08 22:43:07.024327: Epoch time: 88.95 s
2024-12-08 22:43:07.025179: Yayy! New best EMA pseudo Dice: 0.6865
2024-12-08 22:43:08.561219: 
2024-12-08 22:43:08.563078: Epoch 24
2024-12-08 22:43:08.563800: Current learning rate: 0.00978
2024-12-08 22:44:37.772687: Validation loss did not improve from -0.54066. Patience: 1/50
2024-12-08 22:44:37.774044: train_loss -0.5495
2024-12-08 22:44:37.775010: val_loss -0.4798
2024-12-08 22:44:37.775696: Pseudo dice [0.6926]
2024-12-08 22:44:37.776579: Epoch time: 89.21 s
2024-12-08 22:44:38.139139: Yayy! New best EMA pseudo Dice: 0.6871
2024-12-08 22:44:39.728868: 
2024-12-08 22:44:39.730661: Epoch 25
2024-12-08 22:44:39.731366: Current learning rate: 0.00977
2024-12-08 22:46:08.917159: Validation loss did not improve from -0.54066. Patience: 2/50
2024-12-08 22:46:08.918022: train_loss -0.5501
2024-12-08 22:46:08.918839: val_loss -0.5353
2024-12-08 22:46:08.919575: Pseudo dice [0.7338]
2024-12-08 22:46:08.920296: Epoch time: 89.19 s
2024-12-08 22:46:08.920890: Yayy! New best EMA pseudo Dice: 0.6918
2024-12-08 22:46:10.494134: 
2024-12-08 22:46:10.495420: Epoch 26
2024-12-08 22:46:10.496226: Current learning rate: 0.00977
2024-12-08 22:47:39.690047: Validation loss improved from -0.54066 to -0.54149! Patience: 2/50
2024-12-08 22:47:39.690830: train_loss -0.558
2024-12-08 22:47:39.691568: val_loss -0.5415
2024-12-08 22:47:39.692279: Pseudo dice [0.7379]
2024-12-08 22:47:39.692891: Epoch time: 89.2 s
2024-12-08 22:47:39.693670: Yayy! New best EMA pseudo Dice: 0.6964
2024-12-08 22:47:41.273712: 
2024-12-08 22:47:41.275385: Epoch 27
2024-12-08 22:47:41.276243: Current learning rate: 0.00976
2024-12-08 22:49:10.402401: Validation loss improved from -0.54149 to -0.54484! Patience: 0/50
2024-12-08 22:49:10.403149: train_loss -0.5612
2024-12-08 22:49:10.403835: val_loss -0.5448
2024-12-08 22:49:10.404591: Pseudo dice [0.7349]
2024-12-08 22:49:10.405274: Epoch time: 89.13 s
2024-12-08 22:49:10.405940: Yayy! New best EMA pseudo Dice: 0.7002
2024-12-08 22:49:12.017784: 
2024-12-08 22:49:12.018642: Epoch 28
2024-12-08 22:49:12.019280: Current learning rate: 0.00975
2024-12-08 22:50:41.269900: Validation loss improved from -0.54484 to -0.56026! Patience: 0/50
2024-12-08 22:50:41.270860: train_loss -0.5664
2024-12-08 22:50:41.271730: val_loss -0.5603
2024-12-08 22:50:41.272406: Pseudo dice [0.7453]
2024-12-08 22:50:41.272977: Epoch time: 89.25 s
2024-12-08 22:50:41.273573: Yayy! New best EMA pseudo Dice: 0.7047
2024-12-08 22:50:43.306230: 
2024-12-08 22:50:43.307855: Epoch 29
2024-12-08 22:50:43.308695: Current learning rate: 0.00974
2024-12-08 22:52:12.579342: Validation loss did not improve from -0.56026. Patience: 1/50
2024-12-08 22:52:12.580489: train_loss -0.5824
2024-12-08 22:52:12.581392: val_loss -0.5238
2024-12-08 22:52:12.582169: Pseudo dice [0.7305]
2024-12-08 22:52:12.582789: Epoch time: 89.28 s
2024-12-08 22:52:12.953257: Yayy! New best EMA pseudo Dice: 0.7073
2024-12-08 22:52:14.593909: 
2024-12-08 22:52:14.595708: Epoch 30
2024-12-08 22:52:14.596473: Current learning rate: 0.00973
2024-12-08 22:53:43.863608: Validation loss improved from -0.56026 to -0.56946! Patience: 1/50
2024-12-08 22:53:43.865090: train_loss -0.5807
2024-12-08 22:53:43.866101: val_loss -0.5695
2024-12-08 22:53:43.866828: Pseudo dice [0.7476]
2024-12-08 22:53:43.867442: Epoch time: 89.27 s
2024-12-08 22:53:43.868161: Yayy! New best EMA pseudo Dice: 0.7113
2024-12-08 22:53:45.487579: 
2024-12-08 22:53:45.488890: Epoch 31
2024-12-08 22:53:45.489972: Current learning rate: 0.00972
2024-12-08 22:55:14.676165: Validation loss did not improve from -0.56946. Patience: 1/50
2024-12-08 22:55:14.677075: train_loss -0.5806
2024-12-08 22:55:14.678091: val_loss -0.5576
2024-12-08 22:55:14.678867: Pseudo dice [0.7455]
2024-12-08 22:55:14.679579: Epoch time: 89.19 s
2024-12-08 22:55:14.680245: Yayy! New best EMA pseudo Dice: 0.7148
2024-12-08 22:55:16.296640: 
2024-12-08 22:55:16.298399: Epoch 32
2024-12-08 22:55:16.299203: Current learning rate: 0.00971
2024-12-08 22:56:45.581210: Validation loss improved from -0.56946 to -0.58689! Patience: 1/50
2024-12-08 22:56:45.582070: train_loss -0.5831
2024-12-08 22:56:45.583134: val_loss -0.5869
2024-12-08 22:56:45.584122: Pseudo dice [0.7633]
2024-12-08 22:56:45.584851: Epoch time: 89.29 s
2024-12-08 22:56:45.585630: Yayy! New best EMA pseudo Dice: 0.7196
2024-12-08 22:56:47.223037: 
2024-12-08 22:56:47.224605: Epoch 33
2024-12-08 22:56:47.225564: Current learning rate: 0.0097
2024-12-08 22:58:16.595454: Validation loss did not improve from -0.58689. Patience: 1/50
2024-12-08 22:58:16.596655: train_loss -0.582
2024-12-08 22:58:16.597495: val_loss -0.5425
2024-12-08 22:58:16.598243: Pseudo dice [0.7358]
2024-12-08 22:58:16.598943: Epoch time: 89.37 s
2024-12-08 22:58:16.599627: Yayy! New best EMA pseudo Dice: 0.7212
2024-12-08 22:58:18.224344: 
2024-12-08 22:58:18.225229: Epoch 34
2024-12-08 22:58:18.226064: Current learning rate: 0.00969
2024-12-08 22:59:47.619018: Validation loss did not improve from -0.58689. Patience: 2/50
2024-12-08 22:59:47.620333: train_loss -0.5823
2024-12-08 22:59:47.621412: val_loss -0.5396
2024-12-08 22:59:47.622168: Pseudo dice [0.7294]
2024-12-08 22:59:47.622937: Epoch time: 89.4 s
2024-12-08 22:59:47.997226: Yayy! New best EMA pseudo Dice: 0.722
2024-12-08 22:59:49.643146: 
2024-12-08 22:59:49.644749: Epoch 35
2024-12-08 22:59:49.645541: Current learning rate: 0.00968
2024-12-08 23:01:19.021959: Validation loss did not improve from -0.58689. Patience: 3/50
2024-12-08 23:01:19.022961: train_loss -0.5813
2024-12-08 23:01:19.023964: val_loss -0.5219
2024-12-08 23:01:19.024736: Pseudo dice [0.7235]
2024-12-08 23:01:19.025508: Epoch time: 89.38 s
2024-12-08 23:01:19.026241: Yayy! New best EMA pseudo Dice: 0.7222
2024-12-08 23:01:20.653225: 
2024-12-08 23:01:20.655389: Epoch 36
2024-12-08 23:01:20.656411: Current learning rate: 0.00968
2024-12-08 23:02:50.045542: Validation loss did not improve from -0.58689. Patience: 4/50
2024-12-08 23:02:50.046544: train_loss -0.5904
2024-12-08 23:02:50.047384: val_loss -0.5832
2024-12-08 23:02:50.048148: Pseudo dice [0.7605]
2024-12-08 23:02:50.048838: Epoch time: 89.39 s
2024-12-08 23:02:50.049592: Yayy! New best EMA pseudo Dice: 0.726
2024-12-08 23:02:51.666337: 
2024-12-08 23:02:51.668120: Epoch 37
2024-12-08 23:02:51.669048: Current learning rate: 0.00967
2024-12-08 23:04:21.208238: Validation loss did not improve from -0.58689. Patience: 5/50
2024-12-08 23:04:21.209207: train_loss -0.5942
2024-12-08 23:04:21.210412: val_loss -0.5655
2024-12-08 23:04:21.211287: Pseudo dice [0.749]
2024-12-08 23:04:21.212248: Epoch time: 89.54 s
2024-12-08 23:04:21.213253: Yayy! New best EMA pseudo Dice: 0.7283
2024-12-08 23:04:22.897478: 
2024-12-08 23:04:22.899359: Epoch 38
2024-12-08 23:04:22.900125: Current learning rate: 0.00966
2024-12-08 23:05:52.485249: Validation loss did not improve from -0.58689. Patience: 6/50
2024-12-08 23:05:52.486366: train_loss -0.6015
2024-12-08 23:05:52.487224: val_loss -0.5655
2024-12-08 23:05:52.487882: Pseudo dice [0.7432]
2024-12-08 23:05:52.488569: Epoch time: 89.59 s
2024-12-08 23:05:52.489246: Yayy! New best EMA pseudo Dice: 0.7298
2024-12-08 23:05:54.626497: 
2024-12-08 23:05:54.628768: Epoch 39
2024-12-08 23:05:54.629492: Current learning rate: 0.00965
2024-12-08 23:07:24.025552: Validation loss did not improve from -0.58689. Patience: 7/50
2024-12-08 23:07:24.026807: train_loss -0.586
2024-12-08 23:07:24.028039: val_loss -0.5008
2024-12-08 23:07:24.028712: Pseudo dice [0.7056]
2024-12-08 23:07:24.029467: Epoch time: 89.4 s
2024-12-08 23:07:25.741453: 
2024-12-08 23:07:25.743072: Epoch 40
2024-12-08 23:07:25.743901: Current learning rate: 0.00964
2024-12-08 23:08:54.973089: Validation loss did not improve from -0.58689. Patience: 8/50
2024-12-08 23:08:54.974103: train_loss -0.6088
2024-12-08 23:08:54.974950: val_loss -0.5382
2024-12-08 23:08:54.975682: Pseudo dice [0.7405]
2024-12-08 23:08:54.976326: Epoch time: 89.23 s
2024-12-08 23:08:56.282712: 
2024-12-08 23:08:56.284447: Epoch 41
2024-12-08 23:08:56.285339: Current learning rate: 0.00963
2024-12-08 23:10:25.492239: Validation loss did not improve from -0.58689. Patience: 9/50
2024-12-08 23:10:25.493102: train_loss -0.6099
2024-12-08 23:10:25.493911: val_loss -0.5218
2024-12-08 23:10:25.494576: Pseudo dice [0.7167]
2024-12-08 23:10:25.495270: Epoch time: 89.21 s
2024-12-08 23:10:26.744093: 
2024-12-08 23:10:26.745462: Epoch 42
2024-12-08 23:10:26.746288: Current learning rate: 0.00962
2024-12-08 23:11:56.035177: Validation loss did not improve from -0.58689. Patience: 10/50
2024-12-08 23:11:56.039859: train_loss -0.5983
2024-12-08 23:11:56.042596: val_loss -0.569
2024-12-08 23:11:56.043628: Pseudo dice [0.7455]
2024-12-08 23:11:56.044899: Epoch time: 89.3 s
2024-12-08 23:11:57.323308: 
2024-12-08 23:11:57.325187: Epoch 43
2024-12-08 23:11:57.326029: Current learning rate: 0.00961
2024-12-08 23:13:26.538118: Validation loss did not improve from -0.58689. Patience: 11/50
2024-12-08 23:13:26.539080: train_loss -0.6144
2024-12-08 23:13:26.540069: val_loss -0.5686
2024-12-08 23:13:26.540723: Pseudo dice [0.7467]
2024-12-08 23:13:26.541332: Epoch time: 89.22 s
2024-12-08 23:13:26.542013: Yayy! New best EMA pseudo Dice: 0.731
2024-12-08 23:13:28.111513: 
2024-12-08 23:13:28.113364: Epoch 44
2024-12-08 23:13:28.114220: Current learning rate: 0.0096
2024-12-08 23:14:56.969339: Validation loss did not improve from -0.58689. Patience: 12/50
2024-12-08 23:14:56.970457: train_loss -0.6219
2024-12-08 23:14:56.971232: val_loss -0.5564
2024-12-08 23:14:56.971940: Pseudo dice [0.7451]
2024-12-08 23:14:56.972684: Epoch time: 88.86 s
2024-12-08 23:14:57.344426: Yayy! New best EMA pseudo Dice: 0.7324
2024-12-08 23:14:58.952747: 
2024-12-08 23:14:58.954999: Epoch 45
2024-12-08 23:14:58.956008: Current learning rate: 0.00959
2024-12-08 23:16:27.767872: Validation loss did not improve from -0.58689. Patience: 13/50
2024-12-08 23:16:27.768793: train_loss -0.6237
2024-12-08 23:16:27.769712: val_loss -0.5841
2024-12-08 23:16:27.770408: Pseudo dice [0.7563]
2024-12-08 23:16:27.771180: Epoch time: 88.82 s
2024-12-08 23:16:27.771798: Yayy! New best EMA pseudo Dice: 0.7348
2024-12-08 23:16:29.453339: 
2024-12-08 23:16:29.455277: Epoch 46
2024-12-08 23:16:29.456094: Current learning rate: 0.00959
2024-12-08 23:17:58.103017: Validation loss did not improve from -0.58689. Patience: 14/50
2024-12-08 23:17:58.104342: train_loss -0.6269
2024-12-08 23:17:58.105328: val_loss -0.582
2024-12-08 23:17:58.106095: Pseudo dice [0.7531]
2024-12-08 23:17:58.106811: Epoch time: 88.65 s
2024-12-08 23:17:58.107500: Yayy! New best EMA pseudo Dice: 0.7366
2024-12-08 23:17:59.793242: 
2024-12-08 23:17:59.795216: Epoch 47
2024-12-08 23:17:59.796356: Current learning rate: 0.00958
2024-12-08 23:19:28.413409: Validation loss did not improve from -0.58689. Patience: 15/50
2024-12-08 23:19:28.414468: train_loss -0.6233
2024-12-08 23:19:28.415207: val_loss -0.5665
2024-12-08 23:19:28.415889: Pseudo dice [0.7446]
2024-12-08 23:19:28.416815: Epoch time: 88.62 s
2024-12-08 23:19:28.417529: Yayy! New best EMA pseudo Dice: 0.7374
2024-12-08 23:19:30.035377: 
2024-12-08 23:19:30.036904: Epoch 48
2024-12-08 23:19:30.037568: Current learning rate: 0.00957
2024-12-08 23:20:58.570401: Validation loss did not improve from -0.58689. Patience: 16/50
2024-12-08 23:20:58.571453: train_loss -0.6274
2024-12-08 23:20:58.572579: val_loss -0.5611
2024-12-08 23:20:58.573660: Pseudo dice [0.7527]
2024-12-08 23:20:58.574492: Epoch time: 88.54 s
2024-12-08 23:20:58.575473: Yayy! New best EMA pseudo Dice: 0.739
2024-12-08 23:21:00.193935: 
2024-12-08 23:21:00.195799: Epoch 49
2024-12-08 23:21:00.196744: Current learning rate: 0.00956
2024-12-08 23:22:28.766962: Validation loss did not improve from -0.58689. Patience: 17/50
2024-12-08 23:22:28.768111: train_loss -0.6298
2024-12-08 23:22:28.769185: val_loss -0.5623
2024-12-08 23:22:28.770067: Pseudo dice [0.7438]
2024-12-08 23:22:28.770888: Epoch time: 88.58 s
2024-12-08 23:22:29.138169: Yayy! New best EMA pseudo Dice: 0.7395
2024-12-08 23:22:31.556865: 
2024-12-08 23:22:31.558614: Epoch 50
2024-12-08 23:22:31.559680: Current learning rate: 0.00955
2024-12-08 23:23:59.856957: Validation loss improved from -0.58689 to -0.60612! Patience: 17/50
2024-12-08 23:23:59.858198: train_loss -0.6308
2024-12-08 23:23:59.859449: val_loss -0.6061
2024-12-08 23:23:59.860487: Pseudo dice [0.7715]
2024-12-08 23:23:59.861596: Epoch time: 88.3 s
2024-12-08 23:23:59.862805: Yayy! New best EMA pseudo Dice: 0.7427
2024-12-08 23:24:01.416017: 
2024-12-08 23:24:01.417748: Epoch 51
2024-12-08 23:24:01.418574: Current learning rate: 0.00954
2024-12-08 23:25:29.668023: Validation loss did not improve from -0.60612. Patience: 1/50
2024-12-08 23:25:29.670495: train_loss -0.6313
2024-12-08 23:25:29.671378: val_loss -0.5555
2024-12-08 23:25:29.672117: Pseudo dice [0.7403]
2024-12-08 23:25:29.673053: Epoch time: 88.26 s
2024-12-08 23:25:30.910956: 
2024-12-08 23:25:30.912539: Epoch 52
2024-12-08 23:25:30.913298: Current learning rate: 0.00953
2024-12-08 23:26:59.469849: Validation loss did not improve from -0.60612. Patience: 2/50
2024-12-08 23:26:59.471069: train_loss -0.6362
2024-12-08 23:26:59.472229: val_loss -0.5726
2024-12-08 23:26:59.473037: Pseudo dice [0.7568]
2024-12-08 23:26:59.473904: Epoch time: 88.56 s
2024-12-08 23:26:59.474705: Yayy! New best EMA pseudo Dice: 0.7439
2024-12-08 23:27:01.013723: 
2024-12-08 23:27:01.015533: Epoch 53
2024-12-08 23:27:01.016341: Current learning rate: 0.00952
2024-12-08 23:28:29.657182: Validation loss did not improve from -0.60612. Patience: 3/50
2024-12-08 23:28:29.658349: train_loss -0.643
2024-12-08 23:28:29.659208: val_loss -0.5544
2024-12-08 23:28:29.659968: Pseudo dice [0.7384]
2024-12-08 23:28:29.660789: Epoch time: 88.65 s
2024-12-08 23:28:30.837806: 
2024-12-08 23:28:30.840054: Epoch 54
2024-12-08 23:28:30.840878: Current learning rate: 0.00951
2024-12-08 23:29:59.603078: Validation loss did not improve from -0.60612. Patience: 4/50
2024-12-08 23:29:59.604030: train_loss -0.6479
2024-12-08 23:29:59.604887: val_loss -0.5907
2024-12-08 23:29:59.605539: Pseudo dice [0.7609]
2024-12-08 23:29:59.606327: Epoch time: 88.77 s
2024-12-08 23:29:59.970387: Yayy! New best EMA pseudo Dice: 0.7451
2024-12-08 23:30:01.505720: 
2024-12-08 23:30:01.506979: Epoch 55
2024-12-08 23:30:01.507913: Current learning rate: 0.0095
2024-12-08 23:31:30.243541: Validation loss did not improve from -0.60612. Patience: 5/50
2024-12-08 23:31:30.244971: train_loss -0.6373
2024-12-08 23:31:30.245878: val_loss -0.5585
2024-12-08 23:31:30.246677: Pseudo dice [0.7379]
2024-12-08 23:31:30.247426: Epoch time: 88.74 s
2024-12-08 23:31:31.454470: 
2024-12-08 23:31:31.456408: Epoch 56
2024-12-08 23:31:31.457327: Current learning rate: 0.00949
2024-12-08 23:33:00.264981: Validation loss did not improve from -0.60612. Patience: 6/50
2024-12-08 23:33:00.265758: train_loss -0.6385
2024-12-08 23:33:00.266499: val_loss -0.576
2024-12-08 23:33:00.267165: Pseudo dice [0.758]
2024-12-08 23:33:00.267839: Epoch time: 88.81 s
2024-12-08 23:33:00.268471: Yayy! New best EMA pseudo Dice: 0.7457
2024-12-08 23:33:01.808478: 
2024-12-08 23:33:01.809415: Epoch 57
2024-12-08 23:33:01.810314: Current learning rate: 0.00949
2024-12-08 23:34:30.666218: Validation loss did not improve from -0.60612. Patience: 7/50
2024-12-08 23:34:30.667455: train_loss -0.6305
2024-12-08 23:34:30.668520: val_loss -0.5692
2024-12-08 23:34:30.669379: Pseudo dice [0.7487]
2024-12-08 23:34:30.670104: Epoch time: 88.86 s
2024-12-08 23:34:30.670756: Yayy! New best EMA pseudo Dice: 0.746
2024-12-08 23:34:32.252838: 
2024-12-08 23:34:32.254376: Epoch 58
2024-12-08 23:34:32.255255: Current learning rate: 0.00948
2024-12-08 23:36:01.104441: Validation loss did not improve from -0.60612. Patience: 8/50
2024-12-08 23:36:01.105919: train_loss -0.6397
2024-12-08 23:36:01.107205: val_loss -0.5654
2024-12-08 23:36:01.107958: Pseudo dice [0.7514]
2024-12-08 23:36:01.108684: Epoch time: 88.85 s
2024-12-08 23:36:01.109442: Yayy! New best EMA pseudo Dice: 0.7466
2024-12-08 23:36:02.668006: 
2024-12-08 23:36:02.669724: Epoch 59
2024-12-08 23:36:02.670640: Current learning rate: 0.00947
2024-12-08 23:37:31.619438: Validation loss did not improve from -0.60612. Patience: 9/50
2024-12-08 23:37:31.620598: train_loss -0.6427
2024-12-08 23:37:31.621412: val_loss -0.5669
2024-12-08 23:37:31.622134: Pseudo dice [0.7516]
2024-12-08 23:37:31.623069: Epoch time: 88.95 s
2024-12-08 23:37:31.964348: Yayy! New best EMA pseudo Dice: 0.7471
2024-12-08 23:37:33.535403: 
2024-12-08 23:37:33.536324: Epoch 60
2024-12-08 23:37:33.537200: Current learning rate: 0.00946
2024-12-08 23:39:02.533890: Validation loss did not improve from -0.60612. Patience: 10/50
2024-12-08 23:39:02.535200: train_loss -0.6528
2024-12-08 23:39:02.536227: val_loss -0.591
2024-12-08 23:39:02.536892: Pseudo dice [0.7606]
2024-12-08 23:39:02.537551: Epoch time: 89.0 s
2024-12-08 23:39:02.538288: Yayy! New best EMA pseudo Dice: 0.7484
2024-12-08 23:39:04.518055: 
2024-12-08 23:39:04.519713: Epoch 61
2024-12-08 23:39:04.520602: Current learning rate: 0.00945
2024-12-08 23:40:33.667335: Validation loss did not improve from -0.60612. Patience: 11/50
2024-12-08 23:40:33.668284: train_loss -0.653
2024-12-08 23:40:33.669208: val_loss -0.5987
2024-12-08 23:40:33.670112: Pseudo dice [0.7625]
2024-12-08 23:40:33.671012: Epoch time: 89.15 s
2024-12-08 23:40:33.671860: Yayy! New best EMA pseudo Dice: 0.7498
2024-12-08 23:40:35.224354: 
2024-12-08 23:40:35.225828: Epoch 62
2024-12-08 23:40:35.226629: Current learning rate: 0.00944
2024-12-08 23:42:04.426357: Validation loss did not improve from -0.60612. Patience: 12/50
2024-12-08 23:42:04.427721: train_loss -0.6589
2024-12-08 23:42:04.428548: val_loss -0.5898
2024-12-08 23:42:04.429298: Pseudo dice [0.7685]
2024-12-08 23:42:04.430078: Epoch time: 89.2 s
2024-12-08 23:42:04.430839: Yayy! New best EMA pseudo Dice: 0.7517
2024-12-08 23:42:06.066989: 
2024-12-08 23:42:06.069052: Epoch 63
2024-12-08 23:42:06.069765: Current learning rate: 0.00943
2024-12-08 23:43:35.285900: Validation loss did not improve from -0.60612. Patience: 13/50
2024-12-08 23:43:35.287022: train_loss -0.6591
2024-12-08 23:43:35.288383: val_loss -0.6016
2024-12-08 23:43:35.289148: Pseudo dice [0.768]
2024-12-08 23:43:35.290041: Epoch time: 89.22 s
2024-12-08 23:43:35.290784: Yayy! New best EMA pseudo Dice: 0.7533
2024-12-08 23:43:36.951886: 
2024-12-08 23:43:36.953731: Epoch 64
2024-12-08 23:43:36.954578: Current learning rate: 0.00942
2024-12-08 23:45:06.275131: Validation loss did not improve from -0.60612. Patience: 14/50
2024-12-08 23:45:06.276423: train_loss -0.6585
2024-12-08 23:45:06.277325: val_loss -0.5936
2024-12-08 23:45:06.278049: Pseudo dice [0.7657]
2024-12-08 23:45:06.278804: Epoch time: 89.33 s
2024-12-08 23:45:06.629881: Yayy! New best EMA pseudo Dice: 0.7546
2024-12-08 23:45:08.181396: 
2024-12-08 23:45:08.183074: Epoch 65
2024-12-08 23:45:08.184045: Current learning rate: 0.00941
2024-12-08 23:46:37.304257: Validation loss did not improve from -0.60612. Patience: 15/50
2024-12-08 23:46:37.305615: train_loss -0.6635
2024-12-08 23:46:37.306678: val_loss -0.5737
2024-12-08 23:46:37.307471: Pseudo dice [0.7547]
2024-12-08 23:46:37.308182: Epoch time: 89.13 s
2024-12-08 23:46:37.308899: Yayy! New best EMA pseudo Dice: 0.7546
2024-12-08 23:46:38.907331: 
2024-12-08 23:46:38.909355: Epoch 66
2024-12-08 23:46:38.910290: Current learning rate: 0.0094
2024-12-08 23:48:08.017441: Validation loss did not improve from -0.60612. Patience: 16/50
2024-12-08 23:48:08.018552: train_loss -0.6619
2024-12-08 23:48:08.019416: val_loss -0.5892
2024-12-08 23:48:08.020396: Pseudo dice [0.7596]
2024-12-08 23:48:08.021215: Epoch time: 89.11 s
2024-12-08 23:48:08.022000: Yayy! New best EMA pseudo Dice: 0.7551
2024-12-08 23:48:09.573477: 
2024-12-08 23:48:09.574967: Epoch 67
2024-12-08 23:48:09.575813: Current learning rate: 0.00939
2024-12-08 23:49:38.753390: Validation loss did not improve from -0.60612. Patience: 17/50
2024-12-08 23:49:38.754675: train_loss -0.6583
2024-12-08 23:49:38.756022: val_loss -0.5707
2024-12-08 23:49:38.756909: Pseudo dice [0.7503]
2024-12-08 23:49:38.757699: Epoch time: 89.18 s
2024-12-08 23:49:40.042564: 
2024-12-08 23:49:40.044716: Epoch 68
2024-12-08 23:49:40.045819: Current learning rate: 0.00939
2024-12-08 23:51:09.255970: Validation loss did not improve from -0.60612. Patience: 18/50
2024-12-08 23:51:09.256985: train_loss -0.6569
2024-12-08 23:51:09.257920: val_loss -0.582
2024-12-08 23:51:09.258659: Pseudo dice [0.7616]
2024-12-08 23:51:09.259366: Epoch time: 89.22 s
2024-12-08 23:51:09.260152: Yayy! New best EMA pseudo Dice: 0.7553
2024-12-08 23:51:10.841729: 
2024-12-08 23:51:10.843604: Epoch 69
2024-12-08 23:51:10.844447: Current learning rate: 0.00938
2024-12-08 23:52:39.759350: Validation loss improved from -0.60612 to -0.60998! Patience: 18/50
2024-12-08 23:52:39.760191: train_loss -0.6546
2024-12-08 23:52:39.761029: val_loss -0.61
2024-12-08 23:52:39.761859: Pseudo dice [0.7731]
2024-12-08 23:52:39.762723: Epoch time: 88.92 s
2024-12-08 23:52:40.114970: Yayy! New best EMA pseudo Dice: 0.7571
2024-12-08 23:52:41.698877: 
2024-12-08 23:52:41.700814: Epoch 70
2024-12-08 23:52:41.701519: Current learning rate: 0.00937
2024-12-08 23:54:10.613562: Validation loss did not improve from -0.60998. Patience: 1/50
2024-12-08 23:54:10.614691: train_loss -0.6566
2024-12-08 23:54:10.615602: val_loss -0.5844
2024-12-08 23:54:10.616385: Pseudo dice [0.7623]
2024-12-08 23:54:10.617079: Epoch time: 88.92 s
2024-12-08 23:54:10.617801: Yayy! New best EMA pseudo Dice: 0.7576
2024-12-08 23:54:12.195329: 
2024-12-08 23:54:12.196913: Epoch 71
2024-12-08 23:54:12.197781: Current learning rate: 0.00936
2024-12-08 23:55:41.124034: Validation loss did not improve from -0.60998. Patience: 2/50
2024-12-08 23:55:41.124836: train_loss -0.6655
2024-12-08 23:55:41.125589: val_loss -0.5599
2024-12-08 23:55:41.126194: Pseudo dice [0.7542]
2024-12-08 23:55:41.126873: Epoch time: 88.93 s
2024-12-08 23:55:42.680374: 
2024-12-08 23:55:42.682092: Epoch 72
2024-12-08 23:55:42.683024: Current learning rate: 0.00935
2024-12-08 23:57:11.247946: Validation loss did not improve from -0.60998. Patience: 3/50
2024-12-08 23:57:11.248976: train_loss -0.6715
2024-12-08 23:57:11.250080: val_loss -0.5691
2024-12-08 23:57:11.251006: Pseudo dice [0.7514]
2024-12-08 23:57:11.252012: Epoch time: 88.57 s
2024-12-08 23:57:12.508286: 
2024-12-08 23:57:12.510256: Epoch 73
2024-12-08 23:57:12.511025: Current learning rate: 0.00934
2024-12-08 23:58:40.936512: Validation loss improved from -0.60998 to -0.61522! Patience: 3/50
2024-12-08 23:58:40.937351: train_loss -0.6673
2024-12-08 23:58:40.938505: val_loss -0.6152
2024-12-08 23:58:40.939534: Pseudo dice [0.7763]
2024-12-08 23:58:40.940496: Epoch time: 88.43 s
2024-12-08 23:58:40.941464: Yayy! New best EMA pseudo Dice: 0.7586
2024-12-08 23:58:42.565588: 
2024-12-08 23:58:42.567651: Epoch 74
2024-12-08 23:58:42.568441: Current learning rate: 0.00933
2024-12-09 00:00:10.587424: Validation loss did not improve from -0.61522. Patience: 1/50
2024-12-09 00:00:10.588725: train_loss -0.6699
2024-12-09 00:00:10.589913: val_loss -0.5742
2024-12-09 00:00:10.590951: Pseudo dice [0.758]
2024-12-09 00:00:10.591827: Epoch time: 88.02 s
2024-12-09 00:00:12.170820: 
2024-12-09 00:00:12.172380: Epoch 75
2024-12-09 00:00:12.173378: Current learning rate: 0.00932
2024-12-09 00:01:39.864489: Validation loss did not improve from -0.61522. Patience: 2/50
2024-12-09 00:01:39.865828: train_loss -0.6738
2024-12-09 00:01:39.866667: val_loss -0.5899
2024-12-09 00:01:39.867508: Pseudo dice [0.7681]
2024-12-09 00:01:39.868211: Epoch time: 87.7 s
2024-12-09 00:01:39.868970: Yayy! New best EMA pseudo Dice: 0.7595
2024-12-09 00:01:41.456260: 
2024-12-09 00:01:41.457820: Epoch 76
2024-12-09 00:01:41.458542: Current learning rate: 0.00931
2024-12-09 00:03:09.075048: Validation loss did not improve from -0.61522. Patience: 3/50
2024-12-09 00:03:09.076008: train_loss -0.6767
2024-12-09 00:03:09.076765: val_loss -0.5749
2024-12-09 00:03:09.077454: Pseudo dice [0.7631]
2024-12-09 00:03:09.078185: Epoch time: 87.62 s
2024-12-09 00:03:09.078846: Yayy! New best EMA pseudo Dice: 0.7599
2024-12-09 00:03:10.669973: 
2024-12-09 00:03:10.671233: Epoch 77
2024-12-09 00:03:10.672046: Current learning rate: 0.0093
2024-12-09 00:04:38.695297: Validation loss did not improve from -0.61522. Patience: 4/50
2024-12-09 00:04:38.696404: train_loss -0.672
2024-12-09 00:04:38.697289: val_loss -0.5826
2024-12-09 00:04:38.697953: Pseudo dice [0.7545]
2024-12-09 00:04:38.698635: Epoch time: 88.03 s
2024-12-09 00:04:39.984890: 
2024-12-09 00:04:39.987382: Epoch 78
2024-12-09 00:04:39.988106: Current learning rate: 0.0093
2024-12-09 00:06:08.203711: Validation loss did not improve from -0.61522. Patience: 5/50
2024-12-09 00:06:08.204847: train_loss -0.6815
2024-12-09 00:06:08.205864: val_loss -0.5598
2024-12-09 00:06:08.206684: Pseudo dice [0.7429]
2024-12-09 00:06:08.207403: Epoch time: 88.22 s
2024-12-09 00:06:09.529613: 
2024-12-09 00:06:09.531096: Epoch 79
2024-12-09 00:06:09.531843: Current learning rate: 0.00929
2024-12-09 00:07:38.091272: Validation loss did not improve from -0.61522. Patience: 6/50
2024-12-09 00:07:38.092141: train_loss -0.6825
2024-12-09 00:07:38.092886: val_loss -0.6089
2024-12-09 00:07:38.093598: Pseudo dice [0.773]
2024-12-09 00:07:38.094311: Epoch time: 88.56 s
2024-12-09 00:07:39.754584: 
2024-12-09 00:07:39.756011: Epoch 80
2024-12-09 00:07:39.756739: Current learning rate: 0.00928
2024-12-09 00:09:08.306230: Validation loss did not improve from -0.61522. Patience: 7/50
2024-12-09 00:09:08.307478: train_loss -0.6803
2024-12-09 00:09:08.308644: val_loss -0.5858
2024-12-09 00:09:08.309550: Pseudo dice [0.7603]
2024-12-09 00:09:08.310655: Epoch time: 88.55 s
2024-12-09 00:09:09.630410: 
2024-12-09 00:09:09.632353: Epoch 81
2024-12-09 00:09:09.633067: Current learning rate: 0.00927
2024-12-09 00:10:38.185869: Validation loss did not improve from -0.61522. Patience: 8/50
2024-12-09 00:10:38.187129: train_loss -0.6845
2024-12-09 00:10:38.188388: val_loss -0.6082
2024-12-09 00:10:38.189390: Pseudo dice [0.7659]
2024-12-09 00:10:38.190393: Epoch time: 88.56 s
2024-12-09 00:10:38.191431: Yayy! New best EMA pseudo Dice: 0.76
2024-12-09 00:10:39.828036: 
2024-12-09 00:10:39.829874: Epoch 82
2024-12-09 00:10:39.830946: Current learning rate: 0.00926
2024-12-09 00:12:08.541604: Validation loss did not improve from -0.61522. Patience: 9/50
2024-12-09 00:12:08.542470: train_loss -0.6906
2024-12-09 00:12:08.543549: val_loss -0.5916
2024-12-09 00:12:08.544479: Pseudo dice [0.7625]
2024-12-09 00:12:08.545514: Epoch time: 88.72 s
2024-12-09 00:12:08.546461: Yayy! New best EMA pseudo Dice: 0.7602
2024-12-09 00:12:10.422859: 
2024-12-09 00:12:10.424505: Epoch 83
2024-12-09 00:12:10.425547: Current learning rate: 0.00925
2024-12-09 00:13:39.178751: Validation loss did not improve from -0.61522. Patience: 10/50
2024-12-09 00:13:39.181295: train_loss -0.6883
2024-12-09 00:13:39.182333: val_loss -0.5984
2024-12-09 00:13:39.183110: Pseudo dice [0.7624]
2024-12-09 00:13:39.184317: Epoch time: 88.76 s
2024-12-09 00:13:39.185512: Yayy! New best EMA pseudo Dice: 0.7605
2024-12-09 00:13:40.749167: 
2024-12-09 00:13:40.750653: Epoch 84
2024-12-09 00:13:40.751662: Current learning rate: 0.00924
2024-12-09 00:15:09.434401: Validation loss did not improve from -0.61522. Patience: 11/50
2024-12-09 00:15:09.435683: train_loss -0.6944
2024-12-09 00:15:09.436676: val_loss -0.592
2024-12-09 00:15:09.437433: Pseudo dice [0.7675]
2024-12-09 00:15:09.438265: Epoch time: 88.69 s
2024-12-09 00:15:09.787980: Yayy! New best EMA pseudo Dice: 0.7612
2024-12-09 00:15:11.300413: 
2024-12-09 00:15:11.301695: Epoch 85
2024-12-09 00:15:11.302493: Current learning rate: 0.00923
2024-12-09 00:16:40.280460: Validation loss did not improve from -0.61522. Patience: 12/50
2024-12-09 00:16:40.300672: train_loss -0.6936
2024-12-09 00:16:40.302956: val_loss -0.5964
2024-12-09 00:16:40.304045: Pseudo dice [0.769]
2024-12-09 00:16:40.305786: Epoch time: 89.0 s
2024-12-09 00:16:40.307091: Yayy! New best EMA pseudo Dice: 0.7619
2024-12-09 00:16:41.964057: 
2024-12-09 00:16:41.965116: Epoch 86
2024-12-09 00:16:41.966007: Current learning rate: 0.00922
2024-12-09 00:18:10.503794: Validation loss did not improve from -0.61522. Patience: 13/50
2024-12-09 00:18:10.504747: train_loss -0.6852
2024-12-09 00:18:10.505656: val_loss -0.591
2024-12-09 00:18:10.506409: Pseudo dice [0.7618]
2024-12-09 00:18:10.507234: Epoch time: 88.54 s
2024-12-09 00:18:11.685075: 
2024-12-09 00:18:11.686658: Epoch 87
2024-12-09 00:18:11.687359: Current learning rate: 0.00921
2024-12-09 00:19:40.187130: Validation loss did not improve from -0.61522. Patience: 14/50
2024-12-09 00:19:40.188285: train_loss -0.6935
2024-12-09 00:19:40.189130: val_loss -0.5907
2024-12-09 00:19:40.189882: Pseudo dice [0.7527]
2024-12-09 00:19:40.190545: Epoch time: 88.5 s
2024-12-09 00:19:41.413196: 
2024-12-09 00:19:41.414691: Epoch 88
2024-12-09 00:19:41.415707: Current learning rate: 0.0092
2024-12-09 00:21:09.850393: Validation loss did not improve from -0.61522. Patience: 15/50
2024-12-09 00:21:09.851572: train_loss -0.6873
2024-12-09 00:21:09.852573: val_loss -0.6045
2024-12-09 00:21:09.853477: Pseudo dice [0.7689]
2024-12-09 00:21:09.854480: Epoch time: 88.44 s
2024-12-09 00:21:11.128642: 
2024-12-09 00:21:11.130223: Epoch 89
2024-12-09 00:21:11.131137: Current learning rate: 0.0092
2024-12-09 00:22:39.457653: Validation loss did not improve from -0.61522. Patience: 16/50
2024-12-09 00:22:39.458766: train_loss -0.6938
2024-12-09 00:22:39.459726: val_loss -0.5893
2024-12-09 00:22:39.460522: Pseudo dice [0.7696]
2024-12-09 00:22:39.461240: Epoch time: 88.33 s
2024-12-09 00:22:39.809974: Yayy! New best EMA pseudo Dice: 0.7626
2024-12-09 00:22:41.397053: 
2024-12-09 00:22:41.398613: Epoch 90
2024-12-09 00:22:41.399586: Current learning rate: 0.00919
2024-12-09 00:24:09.498473: Validation loss did not improve from -0.61522. Patience: 17/50
2024-12-09 00:24:09.499429: train_loss -0.6953
2024-12-09 00:24:09.500373: val_loss -0.5854
2024-12-09 00:24:09.501162: Pseudo dice [0.7599]
2024-12-09 00:24:09.501900: Epoch time: 88.1 s
2024-12-09 00:24:10.793983: 
2024-12-09 00:24:10.795752: Epoch 91
2024-12-09 00:24:10.796734: Current learning rate: 0.00918
2024-12-09 00:25:38.998660: Validation loss did not improve from -0.61522. Patience: 18/50
2024-12-09 00:25:38.999572: train_loss -0.6973
2024-12-09 00:25:39.000567: val_loss -0.5973
2024-12-09 00:25:39.001332: Pseudo dice [0.7716]
2024-12-09 00:25:39.002012: Epoch time: 88.21 s
2024-12-09 00:25:39.002666: Yayy! New best EMA pseudo Dice: 0.7632
2024-12-09 00:25:40.591251: 
2024-12-09 00:25:40.592765: Epoch 92
2024-12-09 00:25:40.593516: Current learning rate: 0.00917
2024-12-09 00:27:08.647596: Validation loss did not improve from -0.61522. Patience: 19/50
2024-12-09 00:27:08.648675: train_loss -0.7049
2024-12-09 00:27:08.649460: val_loss -0.5942
2024-12-09 00:27:08.654394: Pseudo dice [0.7718]
2024-12-09 00:27:08.655291: Epoch time: 88.06 s
2024-12-09 00:27:08.656070: Yayy! New best EMA pseudo Dice: 0.7641
2024-12-09 00:27:10.258361: 
2024-12-09 00:27:10.259711: Epoch 93
2024-12-09 00:27:10.260556: Current learning rate: 0.00916
2024-12-09 00:28:38.477023: Validation loss did not improve from -0.61522. Patience: 20/50
2024-12-09 00:28:38.477991: train_loss -0.7017
2024-12-09 00:28:38.478869: val_loss -0.5864
2024-12-09 00:28:38.479582: Pseudo dice [0.7619]
2024-12-09 00:28:38.480384: Epoch time: 88.22 s
2024-12-09 00:28:40.161332: 
2024-12-09 00:28:40.162528: Epoch 94
2024-12-09 00:28:40.163406: Current learning rate: 0.00915
2024-12-09 00:30:08.367369: Validation loss improved from -0.61522 to -0.61912! Patience: 20/50
2024-12-09 00:30:08.368609: train_loss -0.6959
2024-12-09 00:30:08.369417: val_loss -0.6191
2024-12-09 00:30:08.370226: Pseudo dice [0.7878]
2024-12-09 00:30:08.370923: Epoch time: 88.21 s
2024-12-09 00:30:08.768525: Yayy! New best EMA pseudo Dice: 0.7663
2024-12-09 00:30:10.316734: 
2024-12-09 00:30:10.318408: Epoch 95
2024-12-09 00:30:10.319196: Current learning rate: 0.00914
2024-12-09 00:31:38.814907: Validation loss did not improve from -0.61912. Patience: 1/50
2024-12-09 00:31:38.815823: train_loss -0.6949
2024-12-09 00:31:38.816755: val_loss -0.609
2024-12-09 00:31:38.817633: Pseudo dice [0.7834]
2024-12-09 00:31:38.818530: Epoch time: 88.5 s
2024-12-09 00:31:38.819308: Yayy! New best EMA pseudo Dice: 0.768
2024-12-09 00:31:40.349005: 
2024-12-09 00:31:40.350673: Epoch 96
2024-12-09 00:31:40.351492: Current learning rate: 0.00913
2024-12-09 00:33:08.864984: Validation loss did not improve from -0.61912. Patience: 2/50
2024-12-09 00:33:08.866066: train_loss -0.6962
2024-12-09 00:33:08.867127: val_loss -0.6154
2024-12-09 00:33:08.867909: Pseudo dice [0.7833]
2024-12-09 00:33:08.868788: Epoch time: 88.52 s
2024-12-09 00:33:08.869596: Yayy! New best EMA pseudo Dice: 0.7695
2024-12-09 00:33:10.477721: 
2024-12-09 00:33:10.479667: Epoch 97
2024-12-09 00:33:10.480696: Current learning rate: 0.00912
2024-12-09 00:34:39.547297: Validation loss improved from -0.61912 to -0.62749! Patience: 2/50
2024-12-09 00:34:39.548341: train_loss -0.7054
2024-12-09 00:34:39.549539: val_loss -0.6275
2024-12-09 00:34:39.550389: Pseudo dice [0.7864]
2024-12-09 00:34:39.551237: Epoch time: 89.07 s
2024-12-09 00:34:39.552020: Yayy! New best EMA pseudo Dice: 0.7712
2024-12-09 00:34:41.218728: 
2024-12-09 00:34:41.220642: Epoch 98
2024-12-09 00:34:41.221557: Current learning rate: 0.00911
2024-12-09 00:36:09.936067: Validation loss did not improve from -0.62749. Patience: 1/50
2024-12-09 00:36:09.938170: train_loss -0.7084
2024-12-09 00:36:09.939255: val_loss -0.6114
2024-12-09 00:36:09.939965: Pseudo dice [0.7833]
2024-12-09 00:36:09.940627: Epoch time: 88.72 s
2024-12-09 00:36:09.941298: Yayy! New best EMA pseudo Dice: 0.7724
2024-12-09 00:36:11.644238: 
2024-12-09 00:36:11.645628: Epoch 99
2024-12-09 00:36:11.646284: Current learning rate: 0.0091
2024-12-09 00:37:40.387683: Validation loss did not improve from -0.62749. Patience: 2/50
2024-12-09 00:37:40.388463: train_loss -0.7018
2024-12-09 00:37:40.389195: val_loss -0.6102
2024-12-09 00:37:40.389941: Pseudo dice [0.7808]
2024-12-09 00:37:40.390590: Epoch time: 88.75 s
2024-12-09 00:37:40.779097: Yayy! New best EMA pseudo Dice: 0.7732
2024-12-09 00:37:42.427656: 
2024-12-09 00:37:42.429363: Epoch 100
2024-12-09 00:37:42.430242: Current learning rate: 0.0091
2024-12-09 00:39:11.013862: Validation loss did not improve from -0.62749. Patience: 3/50
2024-12-09 00:39:11.015054: train_loss -0.7048
2024-12-09 00:39:11.015957: val_loss -0.5989
2024-12-09 00:39:11.016607: Pseudo dice [0.774]
2024-12-09 00:39:11.017318: Epoch time: 88.59 s
2024-12-09 00:39:11.017979: Yayy! New best EMA pseudo Dice: 0.7733
2024-12-09 00:39:12.659350: 
2024-12-09 00:39:12.660989: Epoch 101
2024-12-09 00:39:12.662013: Current learning rate: 0.00909
2024-12-09 00:40:41.144764: Validation loss did not improve from -0.62749. Patience: 4/50
2024-12-09 00:40:41.145819: train_loss -0.706
2024-12-09 00:40:41.146621: val_loss -0.6014
2024-12-09 00:40:41.147326: Pseudo dice [0.7661]
2024-12-09 00:40:41.147991: Epoch time: 88.49 s
2024-12-09 00:40:42.411482: 
2024-12-09 00:40:42.413202: Epoch 102
2024-12-09 00:40:42.413964: Current learning rate: 0.00908
2024-12-09 00:42:10.980026: Validation loss did not improve from -0.62749. Patience: 5/50
2024-12-09 00:42:10.980880: train_loss -0.6961
2024-12-09 00:42:10.981786: val_loss -0.6154
2024-12-09 00:42:10.982487: Pseudo dice [0.7838]
2024-12-09 00:42:10.983174: Epoch time: 88.57 s
2024-12-09 00:42:10.983933: Yayy! New best EMA pseudo Dice: 0.7737
2024-12-09 00:42:12.593163: 
2024-12-09 00:42:12.595100: Epoch 103
2024-12-09 00:42:12.596264: Current learning rate: 0.00907
2024-12-09 00:43:41.179629: Validation loss did not improve from -0.62749. Patience: 6/50
2024-12-09 00:43:41.180557: train_loss -0.6988
2024-12-09 00:43:41.181332: val_loss -0.6104
2024-12-09 00:43:41.182154: Pseudo dice [0.7732]
2024-12-09 00:43:41.182899: Epoch time: 88.59 s
2024-12-09 00:43:42.391551: 
2024-12-09 00:43:42.393235: Epoch 104
2024-12-09 00:43:42.394176: Current learning rate: 0.00906
2024-12-09 00:45:10.961822: Validation loss did not improve from -0.62749. Patience: 7/50
2024-12-09 00:45:10.962917: train_loss -0.7046
2024-12-09 00:45:10.963728: val_loss -0.5919
2024-12-09 00:45:10.964508: Pseudo dice [0.7643]
2024-12-09 00:45:10.965233: Epoch time: 88.57 s
2024-12-09 00:45:12.869215: 
2024-12-09 00:45:12.870746: Epoch 105
2024-12-09 00:45:12.871819: Current learning rate: 0.00905
2024-12-09 00:46:41.265810: Validation loss did not improve from -0.62749. Patience: 8/50
2024-12-09 00:46:41.266928: train_loss -0.707
2024-12-09 00:46:41.267862: val_loss -0.5853
2024-12-09 00:46:41.268907: Pseudo dice [0.7579]
2024-12-09 00:46:41.269668: Epoch time: 88.4 s
2024-12-09 00:46:42.459379: 
2024-12-09 00:46:42.461039: Epoch 106
2024-12-09 00:46:42.462134: Current learning rate: 0.00904
2024-12-09 00:48:10.927409: Validation loss did not improve from -0.62749. Patience: 9/50
2024-12-09 00:48:10.928885: train_loss -0.7003
2024-12-09 00:48:10.929988: val_loss -0.6066
2024-12-09 00:48:10.930863: Pseudo dice [0.7795]
2024-12-09 00:48:10.931908: Epoch time: 88.47 s
2024-12-09 00:48:12.128032: 
2024-12-09 00:48:12.129794: Epoch 107
2024-12-09 00:48:12.131027: Current learning rate: 0.00903
2024-12-09 00:49:40.548344: Validation loss did not improve from -0.62749. Patience: 10/50
2024-12-09 00:49:40.549611: train_loss -0.7106
2024-12-09 00:49:40.550585: val_loss -0.5543
2024-12-09 00:49:40.551401: Pseudo dice [0.7423]
2024-12-09 00:49:40.552117: Epoch time: 88.42 s
2024-12-09 00:49:41.755431: 
2024-12-09 00:49:41.757036: Epoch 108
2024-12-09 00:49:41.758038: Current learning rate: 0.00902
2024-12-09 00:51:10.151777: Validation loss did not improve from -0.62749. Patience: 11/50
2024-12-09 00:51:10.152991: train_loss -0.7069
2024-12-09 00:51:10.153974: val_loss -0.6097
2024-12-09 00:51:10.154675: Pseudo dice [0.7798]
2024-12-09 00:51:10.155689: Epoch time: 88.4 s
2024-12-09 00:51:11.400660: 
2024-12-09 00:51:11.402255: Epoch 109
2024-12-09 00:51:11.403169: Current learning rate: 0.00901
2024-12-09 00:52:39.947379: Validation loss did not improve from -0.62749. Patience: 12/50
2024-12-09 00:52:39.948624: train_loss -0.7048
2024-12-09 00:52:39.949714: val_loss -0.6047
2024-12-09 00:52:39.950476: Pseudo dice [0.7774]
2024-12-09 00:52:39.951295: Epoch time: 88.55 s
2024-12-09 00:52:41.554649: 
2024-12-09 00:52:41.556451: Epoch 110
2024-12-09 00:52:41.557537: Current learning rate: 0.009
2024-12-09 00:54:10.057446: Validation loss did not improve from -0.62749. Patience: 13/50
2024-12-09 00:54:10.058613: train_loss -0.7075
2024-12-09 00:54:10.059870: val_loss -0.6142
2024-12-09 00:54:10.061194: Pseudo dice [0.7806]
2024-12-09 00:54:10.062469: Epoch time: 88.5 s
2024-12-09 00:54:11.245830: 
2024-12-09 00:54:11.247444: Epoch 111
2024-12-09 00:54:11.248598: Current learning rate: 0.009
2024-12-09 00:55:39.635984: Validation loss did not improve from -0.62749. Patience: 14/50
2024-12-09 00:55:39.637255: train_loss -0.7172
2024-12-09 00:55:39.638224: val_loss -0.5934
2024-12-09 00:55:39.639064: Pseudo dice [0.7681]
2024-12-09 00:55:39.640077: Epoch time: 88.39 s
2024-12-09 00:55:40.886639: 
2024-12-09 00:55:40.888428: Epoch 112
2024-12-09 00:55:40.889349: Current learning rate: 0.00899
2024-12-09 00:57:09.314189: Validation loss did not improve from -0.62749. Patience: 15/50
2024-12-09 00:57:09.315489: train_loss -0.7163
2024-12-09 00:57:09.316341: val_loss -0.624
2024-12-09 00:57:09.317030: Pseudo dice [0.7799]
2024-12-09 00:57:09.317734: Epoch time: 88.43 s
2024-12-09 00:57:10.519139: 
2024-12-09 00:57:10.520891: Epoch 113
2024-12-09 00:57:10.521677: Current learning rate: 0.00898
2024-12-09 00:58:38.993278: Validation loss did not improve from -0.62749. Patience: 16/50
2024-12-09 00:58:38.994486: train_loss -0.7142
2024-12-09 00:58:38.995362: val_loss -0.6085
2024-12-09 00:58:38.996041: Pseudo dice [0.7759]
2024-12-09 00:58:38.996801: Epoch time: 88.48 s
2024-12-09 00:58:40.193962: 
2024-12-09 00:58:40.195497: Epoch 114
2024-12-09 00:58:40.196266: Current learning rate: 0.00897
2024-12-09 01:00:08.732351: Validation loss did not improve from -0.62749. Patience: 17/50
2024-12-09 01:00:08.733460: train_loss -0.7017
2024-12-09 01:00:08.734625: val_loss -0.5977
2024-12-09 01:00:08.735552: Pseudo dice [0.7683]
2024-12-09 01:00:08.736792: Epoch time: 88.54 s
2024-12-09 01:00:10.293399: 
2024-12-09 01:00:10.294766: Epoch 115
2024-12-09 01:00:10.295501: Current learning rate: 0.00896
2024-12-09 01:01:38.933999: Validation loss did not improve from -0.62749. Patience: 18/50
2024-12-09 01:01:38.934875: train_loss -0.7103
2024-12-09 01:01:38.935606: val_loss -0.571
2024-12-09 01:01:38.936291: Pseudo dice [0.7571]
2024-12-09 01:01:38.936939: Epoch time: 88.64 s
2024-12-09 01:01:40.623938: 
2024-12-09 01:01:40.625587: Epoch 116
2024-12-09 01:01:40.626301: Current learning rate: 0.00895
2024-12-09 01:03:09.336568: Validation loss did not improve from -0.62749. Patience: 19/50
2024-12-09 01:03:09.337283: train_loss -0.7169
2024-12-09 01:03:09.338146: val_loss -0.589
2024-12-09 01:03:09.339437: Pseudo dice [0.7692]
2024-12-09 01:03:09.340301: Epoch time: 88.71 s
2024-12-09 01:03:10.545355: 
2024-12-09 01:03:10.547023: Epoch 117
2024-12-09 01:03:10.547735: Current learning rate: 0.00894
2024-12-09 01:04:39.251759: Validation loss did not improve from -0.62749. Patience: 20/50
2024-12-09 01:04:39.253099: train_loss -0.7165
2024-12-09 01:04:39.254481: val_loss -0.6098
2024-12-09 01:04:39.255792: Pseudo dice [0.7748]
2024-12-09 01:04:39.257036: Epoch time: 88.71 s
2024-12-09 01:04:40.491249: 
2024-12-09 01:04:40.493325: Epoch 118
2024-12-09 01:04:40.494661: Current learning rate: 0.00893
2024-12-09 01:06:09.011691: Validation loss did not improve from -0.62749. Patience: 21/50
2024-12-09 01:06:09.013155: train_loss -0.717
2024-12-09 01:06:09.014321: val_loss -0.6046
2024-12-09 01:06:09.015028: Pseudo dice [0.7775]
2024-12-09 01:06:09.015762: Epoch time: 88.52 s
2024-12-09 01:06:10.230299: 
2024-12-09 01:06:10.232120: Epoch 119
2024-12-09 01:06:10.232933: Current learning rate: 0.00892
2024-12-09 01:07:38.566743: Validation loss did not improve from -0.62749. Patience: 22/50
2024-12-09 01:07:38.567875: train_loss -0.7156
2024-12-09 01:07:38.568809: val_loss -0.5984
2024-12-09 01:07:38.569612: Pseudo dice [0.7657]
2024-12-09 01:07:38.570388: Epoch time: 88.34 s
2024-12-09 01:07:40.136298: 
2024-12-09 01:07:40.139326: Epoch 120
2024-12-09 01:07:40.140535: Current learning rate: 0.00891
2024-12-09 01:09:07.618281: Validation loss did not improve from -0.62749. Patience: 23/50
2024-12-09 01:09:07.619321: train_loss -0.7135
2024-12-09 01:09:07.620683: val_loss -0.5967
2024-12-09 01:09:07.621680: Pseudo dice [0.7761]
2024-12-09 01:09:07.622681: Epoch time: 87.48 s
2024-12-09 01:09:08.807055: 
2024-12-09 01:09:08.809143: Epoch 121
2024-12-09 01:09:08.810498: Current learning rate: 0.0089
2024-12-09 01:10:35.616930: Validation loss did not improve from -0.62749. Patience: 24/50
2024-12-09 01:10:35.618018: train_loss -0.7252
2024-12-09 01:10:35.619162: val_loss -0.615
2024-12-09 01:10:35.620216: Pseudo dice [0.7808]
2024-12-09 01:10:35.621205: Epoch time: 86.81 s
2024-12-09 01:10:36.790086: 
2024-12-09 01:10:36.791606: Epoch 122
2024-12-09 01:10:36.792776: Current learning rate: 0.00889
2024-12-09 01:12:03.348829: Validation loss did not improve from -0.62749. Patience: 25/50
2024-12-09 01:12:03.350374: train_loss -0.7247
2024-12-09 01:12:03.351276: val_loss -0.6244
2024-12-09 01:12:03.352343: Pseudo dice [0.7855]
2024-12-09 01:12:03.353089: Epoch time: 86.56 s
2024-12-09 01:12:03.353936: Yayy! New best EMA pseudo Dice: 0.7738
2024-12-09 01:12:04.902164: 
2024-12-09 01:12:04.904334: Epoch 123
2024-12-09 01:12:04.905194: Current learning rate: 0.00889
2024-12-09 01:13:31.287252: Validation loss did not improve from -0.62749. Patience: 26/50
2024-12-09 01:13:31.288296: train_loss -0.724
2024-12-09 01:13:31.289286: val_loss -0.5937
2024-12-09 01:13:31.290234: Pseudo dice [0.7662]
2024-12-09 01:13:31.291508: Epoch time: 86.39 s
2024-12-09 01:13:32.451977: 
2024-12-09 01:13:32.453747: Epoch 124
2024-12-09 01:13:32.454692: Current learning rate: 0.00888
2024-12-09 01:14:58.803526: Validation loss did not improve from -0.62749. Patience: 27/50
2024-12-09 01:14:58.804780: train_loss -0.7298
2024-12-09 01:14:58.805776: val_loss -0.6096
2024-12-09 01:14:58.806543: Pseudo dice [0.7843]
2024-12-09 01:14:58.807195: Epoch time: 86.35 s
2024-12-09 01:14:59.148295: Yayy! New best EMA pseudo Dice: 0.7742
2024-12-09 01:15:00.656282: 
2024-12-09 01:15:00.658429: Epoch 125
2024-12-09 01:15:00.659638: Current learning rate: 0.00887
2024-12-09 01:16:27.075305: Validation loss did not improve from -0.62749. Patience: 28/50
2024-12-09 01:16:27.076370: train_loss -0.7277
2024-12-09 01:16:27.077354: val_loss -0.5905
2024-12-09 01:16:27.078098: Pseudo dice [0.7647]
2024-12-09 01:16:27.078819: Epoch time: 86.42 s
2024-12-09 01:16:28.247661: 
2024-12-09 01:16:28.249882: Epoch 126
2024-12-09 01:16:28.251135: Current learning rate: 0.00886
2024-12-09 01:17:54.702611: Validation loss did not improve from -0.62749. Patience: 29/50
2024-12-09 01:17:54.703722: train_loss -0.7243
2024-12-09 01:17:54.705278: val_loss -0.6198
2024-12-09 01:17:54.706334: Pseudo dice [0.7887]
2024-12-09 01:17:54.707649: Epoch time: 86.46 s
2024-12-09 01:17:54.708774: Yayy! New best EMA pseudo Dice: 0.7748
2024-12-09 01:17:56.239668: 
2024-12-09 01:17:56.242230: Epoch 127
2024-12-09 01:17:56.244209: Current learning rate: 0.00885
2024-12-09 01:19:22.799644: Validation loss did not improve from -0.62749. Patience: 30/50
2024-12-09 01:19:22.800451: train_loss -0.7215
2024-12-09 01:19:22.801366: val_loss -0.6063
2024-12-09 01:19:22.801979: Pseudo dice [0.773]
2024-12-09 01:19:22.803020: Epoch time: 86.56 s
2024-12-09 01:19:24.331097: 
2024-12-09 01:19:24.333025: Epoch 128
2024-12-09 01:19:24.334008: Current learning rate: 0.00884
2024-12-09 01:20:50.956173: Validation loss did not improve from -0.62749. Patience: 31/50
2024-12-09 01:20:50.957433: train_loss -0.7239
2024-12-09 01:20:50.958693: val_loss -0.5889
2024-12-09 01:20:50.960195: Pseudo dice [0.7621]
2024-12-09 01:20:50.961000: Epoch time: 86.63 s
2024-12-09 01:20:52.157750: 
2024-12-09 01:20:52.159459: Epoch 129
2024-12-09 01:20:52.160542: Current learning rate: 0.00883
2024-12-09 01:22:18.627614: Validation loss did not improve from -0.62749. Patience: 32/50
2024-12-09 01:22:18.628449: train_loss -0.7242
2024-12-09 01:22:18.629671: val_loss -0.6029
2024-12-09 01:22:18.630417: Pseudo dice [0.7737]
2024-12-09 01:22:18.631374: Epoch time: 86.47 s
2024-12-09 01:22:20.166111: 
2024-12-09 01:22:20.168562: Epoch 130
2024-12-09 01:22:20.169623: Current learning rate: 0.00882
2024-12-09 01:23:46.806343: Validation loss did not improve from -0.62749. Patience: 33/50
2024-12-09 01:23:46.807257: train_loss -0.7306
2024-12-09 01:23:46.808223: val_loss -0.5991
2024-12-09 01:23:46.809192: Pseudo dice [0.7783]
2024-12-09 01:23:46.810051: Epoch time: 86.64 s
2024-12-09 01:23:48.016484: 
2024-12-09 01:23:48.018848: Epoch 131
2024-12-09 01:23:48.020321: Current learning rate: 0.00881
2024-12-09 01:25:14.639065: Validation loss did not improve from -0.62749. Patience: 34/50
2024-12-09 01:25:14.640178: train_loss -0.7304
2024-12-09 01:25:14.641163: val_loss -0.625
2024-12-09 01:25:14.641887: Pseudo dice [0.7907]
2024-12-09 01:25:14.643018: Epoch time: 86.62 s
2024-12-09 01:25:14.643876: Yayy! New best EMA pseudo Dice: 0.7756
2024-12-09 01:25:16.194270: 
2024-12-09 01:25:16.196182: Epoch 132
2024-12-09 01:25:16.197363: Current learning rate: 0.0088
2024-12-09 01:26:42.755116: Validation loss did not improve from -0.62749. Patience: 35/50
2024-12-09 01:26:42.756254: train_loss -0.7335
2024-12-09 01:26:42.757646: val_loss -0.5933
2024-12-09 01:26:42.758857: Pseudo dice [0.7677]
2024-12-09 01:26:42.760216: Epoch time: 86.56 s
2024-12-09 01:26:43.957373: 
2024-12-09 01:26:43.959448: Epoch 133
2024-12-09 01:26:43.960723: Current learning rate: 0.00879
2024-12-09 01:28:10.839729: Validation loss did not improve from -0.62749. Patience: 36/50
2024-12-09 01:28:10.840707: train_loss -0.7295
2024-12-09 01:28:10.841698: val_loss -0.6166
2024-12-09 01:28:10.842806: Pseudo dice [0.7843]
2024-12-09 01:28:10.844260: Epoch time: 86.88 s
2024-12-09 01:28:10.845609: Yayy! New best EMA pseudo Dice: 0.7757
2024-12-09 01:28:12.398912: 
2024-12-09 01:28:12.401279: Epoch 134
2024-12-09 01:28:12.402449: Current learning rate: 0.00879
2024-12-09 01:29:39.181726: Validation loss did not improve from -0.62749. Patience: 37/50
2024-12-09 01:29:39.182909: train_loss -0.7264
2024-12-09 01:29:39.184441: val_loss -0.5896
2024-12-09 01:29:39.185618: Pseudo dice [0.77]
2024-12-09 01:29:39.186928: Epoch time: 86.78 s
2024-12-09 01:29:40.763734: 
2024-12-09 01:29:40.765616: Epoch 135
2024-12-09 01:29:40.766745: Current learning rate: 0.00878
2024-12-09 01:31:07.246251: Validation loss did not improve from -0.62749. Patience: 38/50
2024-12-09 01:31:07.247192: train_loss -0.7207
2024-12-09 01:31:07.248369: val_loss -0.5757
2024-12-09 01:31:07.249009: Pseudo dice [0.7609]
2024-12-09 01:31:07.249696: Epoch time: 86.48 s
2024-12-09 01:31:08.478392: 
2024-12-09 01:31:08.479988: Epoch 136
2024-12-09 01:31:08.481282: Current learning rate: 0.00877
2024-12-09 01:32:34.921309: Validation loss did not improve from -0.62749. Patience: 39/50
2024-12-09 01:32:34.922644: train_loss -0.7344
2024-12-09 01:32:34.923683: val_loss -0.5875
2024-12-09 01:32:34.924918: Pseudo dice [0.7715]
2024-12-09 01:32:34.925942: Epoch time: 86.45 s
2024-12-09 01:32:36.140120: 
2024-12-09 01:32:36.141648: Epoch 137
2024-12-09 01:32:36.142959: Current learning rate: 0.00876
2024-12-09 01:34:02.674200: Validation loss did not improve from -0.62749. Patience: 40/50
2024-12-09 01:34:02.675061: train_loss -0.7363
2024-12-09 01:34:02.675828: val_loss -0.6009
2024-12-09 01:34:02.676474: Pseudo dice [0.7707]
2024-12-09 01:34:02.677087: Epoch time: 86.54 s
2024-12-09 01:34:04.225050: 
2024-12-09 01:34:04.226785: Epoch 138
2024-12-09 01:34:04.227826: Current learning rate: 0.00875
2024-12-09 01:35:30.726723: Validation loss did not improve from -0.62749. Patience: 41/50
2024-12-09 01:35:30.727607: train_loss -0.7365
2024-12-09 01:35:30.729007: val_loss -0.6246
2024-12-09 01:35:30.730470: Pseudo dice [0.7853]
2024-12-09 01:35:30.731360: Epoch time: 86.5 s
2024-12-09 01:35:31.942413: 
2024-12-09 01:35:31.944585: Epoch 139
2024-12-09 01:35:31.945850: Current learning rate: 0.00874
2024-12-09 01:36:58.418478: Validation loss did not improve from -0.62749. Patience: 42/50
2024-12-09 01:36:58.420080: train_loss -0.7366
2024-12-09 01:36:58.421357: val_loss -0.6272
2024-12-09 01:36:58.422226: Pseudo dice [0.7973]
2024-12-09 01:36:58.423230: Epoch time: 86.48 s
2024-12-09 01:36:58.792053: Yayy! New best EMA pseudo Dice: 0.7767
2024-12-09 01:37:00.352138: 
2024-12-09 01:37:00.354494: Epoch 140
2024-12-09 01:37:00.355450: Current learning rate: 0.00873
2024-12-09 01:38:26.865937: Validation loss did not improve from -0.62749. Patience: 43/50
2024-12-09 01:38:26.866892: train_loss -0.733
2024-12-09 01:38:26.867985: val_loss -0.6015
2024-12-09 01:38:26.868590: Pseudo dice [0.7724]
2024-12-09 01:38:26.869560: Epoch time: 86.52 s
2024-12-09 01:38:28.066903: 
2024-12-09 01:38:28.068609: Epoch 141
2024-12-09 01:38:28.069422: Current learning rate: 0.00872
2024-12-09 01:39:54.507014: Validation loss did not improve from -0.62749. Patience: 44/50
2024-12-09 01:39:54.507794: train_loss -0.7361
2024-12-09 01:39:54.508498: val_loss -0.6078
2024-12-09 01:39:54.509615: Pseudo dice [0.7824]
2024-12-09 01:39:54.510617: Epoch time: 86.44 s
2024-12-09 01:39:54.511754: Yayy! New best EMA pseudo Dice: 0.7769
2024-12-09 01:39:56.081983: 
2024-12-09 01:39:56.083895: Epoch 142
2024-12-09 01:39:56.084748: Current learning rate: 0.00871
2024-12-09 01:41:22.622353: Validation loss did not improve from -0.62749. Patience: 45/50
2024-12-09 01:41:22.623557: train_loss -0.7437
2024-12-09 01:41:22.624796: val_loss -0.6069
2024-12-09 01:41:22.625782: Pseudo dice [0.778]
2024-12-09 01:41:22.626535: Epoch time: 86.54 s
2024-12-09 01:41:22.627527: Yayy! New best EMA pseudo Dice: 0.777
2024-12-09 01:41:24.150108: 
2024-12-09 01:41:24.151753: Epoch 143
2024-12-09 01:41:24.152754: Current learning rate: 0.0087
2024-12-09 01:42:50.868133: Validation loss did not improve from -0.62749. Patience: 46/50
2024-12-09 01:42:50.869689: train_loss -0.7404
2024-12-09 01:42:50.870864: val_loss -0.6161
2024-12-09 01:42:50.871949: Pseudo dice [0.7749]
2024-12-09 01:42:50.872548: Epoch time: 86.72 s
2024-12-09 01:42:52.060294: 
2024-12-09 01:42:52.062016: Epoch 144
2024-12-09 01:42:52.063355: Current learning rate: 0.00869
2024-12-09 01:44:18.827799: Validation loss did not improve from -0.62749. Patience: 47/50
2024-12-09 01:44:18.829077: train_loss -0.7401
2024-12-09 01:44:18.830062: val_loss -0.6135
2024-12-09 01:44:18.831155: Pseudo dice [0.7815]
2024-12-09 01:44:18.832150: Epoch time: 86.77 s
2024-12-09 01:44:19.190121: Yayy! New best EMA pseudo Dice: 0.7773
2024-12-09 01:44:20.752133: 
2024-12-09 01:44:20.753820: Epoch 145
2024-12-09 01:44:20.754590: Current learning rate: 0.00868
2024-12-09 01:45:47.244365: Validation loss did not improve from -0.62749. Patience: 48/50
2024-12-09 01:45:47.245695: train_loss -0.7436
2024-12-09 01:45:47.247296: val_loss -0.5985
2024-12-09 01:45:47.248116: Pseudo dice [0.774]
2024-12-09 01:45:47.249434: Epoch time: 86.49 s
2024-12-09 01:45:48.446404: 
2024-12-09 01:45:48.447864: Epoch 146
2024-12-09 01:45:48.448977: Current learning rate: 0.00868
2024-12-09 01:47:15.056205: Validation loss improved from -0.62749 to -0.62829! Patience: 48/50
2024-12-09 01:47:15.057406: train_loss -0.7441
2024-12-09 01:47:15.058453: val_loss -0.6283
2024-12-09 01:47:15.059228: Pseudo dice [0.7883]
2024-12-09 01:47:15.059988: Epoch time: 86.61 s
2024-12-09 01:47:15.060750: Yayy! New best EMA pseudo Dice: 0.7781
2024-12-09 01:47:16.617867: 
2024-12-09 01:47:16.619203: Epoch 147
2024-12-09 01:47:16.620120: Current learning rate: 0.00867
2024-12-09 01:48:43.143281: Validation loss did not improve from -0.62829. Patience: 1/50
2024-12-09 01:48:43.144811: train_loss -0.7423
2024-12-09 01:48:43.146021: val_loss -0.5884
2024-12-09 01:48:43.146768: Pseudo dice [0.7656]
2024-12-09 01:48:43.148194: Epoch time: 86.53 s
2024-12-09 01:48:44.345176: 
2024-12-09 01:48:44.346953: Epoch 148
2024-12-09 01:48:44.348177: Current learning rate: 0.00866
2024-12-09 01:50:10.679097: Validation loss did not improve from -0.62829. Patience: 2/50
2024-12-09 01:50:10.680171: train_loss -0.7453
2024-12-09 01:50:10.681242: val_loss -0.585
2024-12-09 01:50:10.682188: Pseudo dice [0.7639]
2024-12-09 01:50:10.682861: Epoch time: 86.34 s
2024-12-09 01:50:12.214159: 
2024-12-09 01:50:12.216063: Epoch 149
2024-12-09 01:50:12.217318: Current learning rate: 0.00865
2024-12-09 01:51:38.566794: Validation loss did not improve from -0.62829. Patience: 3/50
2024-12-09 01:51:38.568304: train_loss -0.7307
2024-12-09 01:51:38.569516: val_loss -0.5627
2024-12-09 01:51:38.570459: Pseudo dice [0.7548]
2024-12-09 01:51:38.571477: Epoch time: 86.35 s
2024-12-09 01:51:40.148993: 
2024-12-09 01:51:40.150961: Epoch 150
2024-12-09 01:51:40.152231: Current learning rate: 0.00864
2024-12-09 01:53:06.285100: Validation loss did not improve from -0.62829. Patience: 4/50
2024-12-09 01:53:06.286239: train_loss -0.7417
2024-12-09 01:53:06.287489: val_loss -0.6018
2024-12-09 01:53:06.288709: Pseudo dice [0.7656]
2024-12-09 01:53:06.289846: Epoch time: 86.14 s
2024-12-09 01:53:07.494765: 
2024-12-09 01:53:07.496720: Epoch 151
2024-12-09 01:53:07.498041: Current learning rate: 0.00863
2024-12-09 01:54:33.722355: Validation loss did not improve from -0.62829. Patience: 5/50
2024-12-09 01:54:33.723569: train_loss -0.7427
2024-12-09 01:54:33.724749: val_loss -0.5955
2024-12-09 01:54:33.725587: Pseudo dice [0.7677]
2024-12-09 01:54:33.726679: Epoch time: 86.23 s
2024-12-09 01:54:34.928354: 
2024-12-09 01:54:34.929782: Epoch 152
2024-12-09 01:54:34.930687: Current learning rate: 0.00862
2024-12-09 01:56:01.121158: Validation loss did not improve from -0.62829. Patience: 6/50
2024-12-09 01:56:01.122311: train_loss -0.739
2024-12-09 01:56:01.123284: val_loss -0.6122
2024-12-09 01:56:01.124139: Pseudo dice [0.7766]
2024-12-09 01:56:01.125054: Epoch time: 86.2 s
2024-12-09 01:56:02.374871: 
2024-12-09 01:56:02.376714: Epoch 153
2024-12-09 01:56:02.377976: Current learning rate: 0.00861
2024-12-09 01:57:28.742487: Validation loss did not improve from -0.62829. Patience: 7/50
2024-12-09 01:57:28.743523: train_loss -0.7339
2024-12-09 01:57:28.744754: val_loss -0.5831
2024-12-09 01:57:28.745733: Pseudo dice [0.7521]
2024-12-09 01:57:28.746734: Epoch time: 86.37 s
2024-12-09 01:57:29.992582: 
2024-12-09 01:57:29.994751: Epoch 154
2024-12-09 01:57:29.996192: Current learning rate: 0.0086
2024-12-09 01:58:56.093359: Validation loss did not improve from -0.62829. Patience: 8/50
2024-12-09 01:58:56.094485: train_loss -0.7348
2024-12-09 01:58:56.095417: val_loss -0.5936
2024-12-09 01:58:56.096585: Pseudo dice [0.7718]
2024-12-09 01:58:56.097433: Epoch time: 86.1 s
2024-12-09 01:58:57.694439: 
2024-12-09 01:58:57.696463: Epoch 155
2024-12-09 01:58:57.697761: Current learning rate: 0.00859
2024-12-09 02:00:26.011090: Validation loss did not improve from -0.62829. Patience: 9/50
2024-12-09 02:00:26.014597: train_loss -0.7337
2024-12-09 02:00:26.016070: val_loss -0.5727
2024-12-09 02:00:26.016925: Pseudo dice [0.7545]
2024-12-09 02:00:26.018301: Epoch time: 88.32 s
2024-12-09 02:00:27.267252: 
2024-12-09 02:00:27.269089: Epoch 156
2024-12-09 02:00:27.270356: Current learning rate: 0.00858
2024-12-09 02:01:53.890372: Validation loss did not improve from -0.62829. Patience: 10/50
2024-12-09 02:01:53.891247: train_loss -0.736
2024-12-09 02:01:53.892426: val_loss -0.6132
2024-12-09 02:01:53.893144: Pseudo dice [0.7796]
2024-12-09 02:01:53.894515: Epoch time: 86.63 s
2024-12-09 02:01:55.145290: 
2024-12-09 02:01:55.147618: Epoch 157
2024-12-09 02:01:55.148988: Current learning rate: 0.00858
2024-12-09 02:03:21.260015: Validation loss did not improve from -0.62829. Patience: 11/50
2024-12-09 02:03:21.260665: train_loss -0.7378
2024-12-09 02:03:21.261607: val_loss -0.5831
2024-12-09 02:03:21.262613: Pseudo dice [0.7688]
2024-12-09 02:03:21.263479: Epoch time: 86.12 s
2024-12-09 02:03:22.486269: 
2024-12-09 02:03:22.487813: Epoch 158
2024-12-09 02:03:22.488646: Current learning rate: 0.00857
2024-12-09 02:04:48.799478: Validation loss did not improve from -0.62829. Patience: 12/50
2024-12-09 02:04:48.801137: train_loss -0.7331
2024-12-09 02:04:48.802324: val_loss -0.6044
2024-12-09 02:04:48.803008: Pseudo dice [0.7747]
2024-12-09 02:04:48.803873: Epoch time: 86.32 s
2024-12-09 02:04:50.371695: 
2024-12-09 02:04:50.373566: Epoch 159
2024-12-09 02:04:50.374439: Current learning rate: 0.00856
2024-12-09 02:06:16.724937: Validation loss did not improve from -0.62829. Patience: 13/50
2024-12-09 02:06:16.725935: train_loss -0.7377
2024-12-09 02:06:16.726698: val_loss -0.6112
2024-12-09 02:06:16.727753: Pseudo dice [0.7798]
2024-12-09 02:06:16.728402: Epoch time: 86.36 s
2024-12-09 02:06:18.262227: 
2024-12-09 02:06:18.264358: Epoch 160
2024-12-09 02:06:18.265117: Current learning rate: 0.00855
2024-12-09 02:07:44.626424: Validation loss did not improve from -0.62829. Patience: 14/50
2024-12-09 02:07:44.627223: train_loss -0.7397
2024-12-09 02:07:44.628020: val_loss -0.6112
2024-12-09 02:07:44.628819: Pseudo dice [0.7822]
2024-12-09 02:07:44.629539: Epoch time: 86.37 s
2024-12-09 02:07:45.842614: 
2024-12-09 02:07:45.844818: Epoch 161
2024-12-09 02:07:45.845658: Current learning rate: 0.00854
2024-12-09 02:09:12.338957: Validation loss did not improve from -0.62829. Patience: 15/50
2024-12-09 02:09:12.340022: train_loss -0.7452
2024-12-09 02:09:12.340773: val_loss -0.6142
2024-12-09 02:09:12.341717: Pseudo dice [0.7879]
2024-12-09 02:09:12.342792: Epoch time: 86.5 s
2024-12-09 02:09:13.582205: 
2024-12-09 02:09:13.584563: Epoch 162
2024-12-09 02:09:13.585783: Current learning rate: 0.00853
2024-12-09 02:10:39.943867: Validation loss improved from -0.62829 to -0.63346! Patience: 15/50
2024-12-09 02:10:39.945369: train_loss -0.745
2024-12-09 02:10:39.946568: val_loss -0.6335
2024-12-09 02:10:39.947631: Pseudo dice [0.7932]
2024-12-09 02:10:39.948781: Epoch time: 86.36 s
2024-12-09 02:10:41.142926: 
2024-12-09 02:10:41.145181: Epoch 163
2024-12-09 02:10:41.146291: Current learning rate: 0.00852
2024-12-09 02:12:07.612480: Validation loss did not improve from -0.63346. Patience: 1/50
2024-12-09 02:12:07.617345: train_loss -0.7458
2024-12-09 02:12:07.618927: val_loss -0.6234
2024-12-09 02:12:07.619746: Pseudo dice [0.7861]
2024-12-09 02:12:07.620837: Epoch time: 86.47 s
2024-12-09 02:12:08.897540: 
2024-12-09 02:12:08.899131: Epoch 164
2024-12-09 02:12:08.900584: Current learning rate: 0.00851
2024-12-09 02:13:35.306723: Validation loss did not improve from -0.63346. Patience: 2/50
2024-12-09 02:13:35.307747: train_loss -0.7525
2024-12-09 02:13:35.308641: val_loss -0.5849
2024-12-09 02:13:35.309585: Pseudo dice [0.7616]
2024-12-09 02:13:35.310485: Epoch time: 86.41 s
2024-12-09 02:13:36.816324: 
2024-12-09 02:13:36.818665: Epoch 165
2024-12-09 02:13:36.819980: Current learning rate: 0.0085
2024-12-09 02:15:03.085835: Validation loss did not improve from -0.63346. Patience: 3/50
2024-12-09 02:15:03.086821: train_loss -0.7576
2024-12-09 02:15:03.088085: val_loss -0.6147
2024-12-09 02:15:03.089285: Pseudo dice [0.7786]
2024-12-09 02:15:03.090454: Epoch time: 86.27 s
2024-12-09 02:15:04.271188: 
2024-12-09 02:15:04.273347: Epoch 166
2024-12-09 02:15:04.274535: Current learning rate: 0.00849
2024-12-09 02:16:30.748595: Validation loss did not improve from -0.63346. Patience: 4/50
2024-12-09 02:16:30.749744: train_loss -0.7584
2024-12-09 02:16:30.751151: val_loss -0.6153
2024-12-09 02:16:30.752356: Pseudo dice [0.7887]
2024-12-09 02:16:30.753242: Epoch time: 86.48 s
2024-12-09 02:16:31.943086: 
2024-12-09 02:16:31.945276: Epoch 167
2024-12-09 02:16:31.946666: Current learning rate: 0.00848
2024-12-09 02:17:58.262989: Validation loss did not improve from -0.63346. Patience: 5/50
2024-12-09 02:17:58.263836: train_loss -0.7448
2024-12-09 02:17:58.264870: val_loss -0.6027
2024-12-09 02:17:58.265600: Pseudo dice [0.7718]
2024-12-09 02:17:58.266668: Epoch time: 86.32 s
2024-12-09 02:17:59.454332: 
2024-12-09 02:17:59.456094: Epoch 168
2024-12-09 02:17:59.457187: Current learning rate: 0.00847
2024-12-09 02:19:25.846735: Validation loss did not improve from -0.63346. Patience: 6/50
2024-12-09 02:19:25.847624: train_loss -0.7445
2024-12-09 02:19:25.848517: val_loss -0.5955
2024-12-09 02:19:25.849425: Pseudo dice [0.769]
2024-12-09 02:19:25.850233: Epoch time: 86.39 s
2024-12-09 02:19:27.047008: 
2024-12-09 02:19:27.048804: Epoch 169
2024-12-09 02:19:27.050096: Current learning rate: 0.00847
2024-12-09 02:20:53.368823: Validation loss did not improve from -0.63346. Patience: 7/50
2024-12-09 02:20:53.370054: train_loss -0.7457
2024-12-09 02:20:53.370994: val_loss -0.6301
2024-12-09 02:20:53.371938: Pseudo dice [0.793]
2024-12-09 02:20:53.372976: Epoch time: 86.32 s
2024-12-09 02:20:55.401011: 
2024-12-09 02:20:55.403141: Epoch 170
2024-12-09 02:20:55.404404: Current learning rate: 0.00846
2024-12-09 02:22:21.912711: Validation loss did not improve from -0.63346. Patience: 8/50
2024-12-09 02:22:21.914114: train_loss -0.7439
2024-12-09 02:22:21.915115: val_loss -0.6302
2024-12-09 02:22:21.916173: Pseudo dice [0.7916]
2024-12-09 02:22:21.916840: Epoch time: 86.51 s
2024-12-09 02:22:21.917726: Yayy! New best EMA pseudo Dice: 0.7789
2024-12-09 02:22:23.445940: 
2024-12-09 02:22:23.447993: Epoch 171
2024-12-09 02:22:23.449098: Current learning rate: 0.00845
2024-12-09 02:23:49.728100: Validation loss did not improve from -0.63346. Patience: 9/50
2024-12-09 02:23:49.728989: train_loss -0.7452
2024-12-09 02:23:49.730235: val_loss -0.6027
2024-12-09 02:23:49.730857: Pseudo dice [0.7736]
2024-12-09 02:23:49.731563: Epoch time: 86.28 s
2024-12-09 02:23:50.946973: 
2024-12-09 02:23:50.948545: Epoch 172
2024-12-09 02:23:50.949698: Current learning rate: 0.00844
2024-12-09 02:25:17.187475: Validation loss did not improve from -0.63346. Patience: 10/50
2024-12-09 02:25:17.188727: train_loss -0.7485
2024-12-09 02:25:17.189690: val_loss -0.624
2024-12-09 02:25:17.190519: Pseudo dice [0.7866]
2024-12-09 02:25:17.191295: Epoch time: 86.24 s
2024-12-09 02:25:17.192285: Yayy! New best EMA pseudo Dice: 0.7792
2024-12-09 02:25:18.786443: 
2024-12-09 02:25:18.788728: Epoch 173
2024-12-09 02:25:18.789922: Current learning rate: 0.00843
2024-12-09 02:26:45.028460: Validation loss did not improve from -0.63346. Patience: 11/50
2024-12-09 02:26:45.029683: train_loss -0.7512
2024-12-09 02:26:45.030447: val_loss -0.5881
2024-12-09 02:26:45.031125: Pseudo dice [0.7703]
2024-12-09 02:26:45.031776: Epoch time: 86.24 s
2024-12-09 02:26:46.266661: 
2024-12-09 02:26:46.268709: Epoch 174
2024-12-09 02:26:46.269820: Current learning rate: 0.00842
2024-12-09 02:28:12.773222: Validation loss did not improve from -0.63346. Patience: 12/50
2024-12-09 02:28:12.774259: train_loss -0.7525
2024-12-09 02:28:12.775711: val_loss -0.5962
2024-12-09 02:28:12.776949: Pseudo dice [0.7742]
2024-12-09 02:28:12.778029: Epoch time: 86.51 s
2024-12-09 02:28:14.357526: 
2024-12-09 02:28:14.359157: Epoch 175
2024-12-09 02:28:14.360003: Current learning rate: 0.00841
2024-12-09 02:29:40.724156: Validation loss did not improve from -0.63346. Patience: 13/50
2024-12-09 02:29:40.725613: train_loss -0.7551
2024-12-09 02:29:40.726738: val_loss -0.5941
2024-12-09 02:29:40.727679: Pseudo dice [0.7724]
2024-12-09 02:29:40.728420: Epoch time: 86.37 s
2024-12-09 02:29:41.949869: 
2024-12-09 02:29:41.951847: Epoch 176
2024-12-09 02:29:41.953024: Current learning rate: 0.0084
2024-12-09 02:31:08.429042: Validation loss did not improve from -0.63346. Patience: 14/50
2024-12-09 02:31:08.430169: train_loss -0.755
2024-12-09 02:31:08.431048: val_loss -0.597
2024-12-09 02:31:08.431723: Pseudo dice [0.7714]
2024-12-09 02:31:08.432386: Epoch time: 86.48 s
2024-12-09 02:31:09.646958: 
2024-12-09 02:31:09.648411: Epoch 177
2024-12-09 02:31:09.649371: Current learning rate: 0.00839
2024-12-09 02:32:36.147175: Validation loss did not improve from -0.63346. Patience: 15/50
2024-12-09 02:32:36.148491: train_loss -0.7625
2024-12-09 02:32:36.150240: val_loss -0.5972
2024-12-09 02:32:36.151557: Pseudo dice [0.7752]
2024-12-09 02:32:36.152919: Epoch time: 86.5 s
2024-12-09 02:32:37.372743: 
2024-12-09 02:32:37.375566: Epoch 178
2024-12-09 02:32:37.377776: Current learning rate: 0.00838
2024-12-09 02:34:03.648332: Validation loss did not improve from -0.63346. Patience: 16/50
2024-12-09 02:34:03.649023: train_loss -0.7549
2024-12-09 02:34:03.649983: val_loss -0.6259
2024-12-09 02:34:03.650663: Pseudo dice [0.7849]
2024-12-09 02:34:03.651467: Epoch time: 86.28 s
2024-12-09 02:34:04.877896: 
2024-12-09 02:34:04.880369: Epoch 179
2024-12-09 02:34:04.882126: Current learning rate: 0.00837
2024-12-09 02:35:31.221186: Validation loss did not improve from -0.63346. Patience: 17/50
2024-12-09 02:35:31.222391: train_loss -0.7533
2024-12-09 02:35:31.223420: val_loss -0.6104
2024-12-09 02:35:31.224239: Pseudo dice [0.7755]
2024-12-09 02:35:31.225114: Epoch time: 86.35 s
2024-12-09 02:35:33.120957: 
2024-12-09 02:35:33.123234: Epoch 180
2024-12-09 02:35:33.124247: Current learning rate: 0.00836
2024-12-09 02:36:59.310666: Validation loss did not improve from -0.63346. Patience: 18/50
2024-12-09 02:36:59.311881: train_loss -0.7564
2024-12-09 02:36:59.312872: val_loss -0.6143
2024-12-09 02:36:59.313657: Pseudo dice [0.7769]
2024-12-09 02:36:59.314420: Epoch time: 86.19 s
2024-12-09 02:37:00.538428: 
2024-12-09 02:37:00.540318: Epoch 181
2024-12-09 02:37:00.541204: Current learning rate: 0.00836
2024-12-09 02:38:26.854887: Validation loss did not improve from -0.63346. Patience: 19/50
2024-12-09 02:38:26.857102: train_loss -0.758
2024-12-09 02:38:26.858697: val_loss -0.6031
2024-12-09 02:38:26.859624: Pseudo dice [0.7794]
2024-12-09 02:38:26.860723: Epoch time: 86.32 s
2024-12-09 02:38:28.061286: 
2024-12-09 02:38:28.062828: Epoch 182
2024-12-09 02:38:28.064157: Current learning rate: 0.00835
2024-12-09 02:39:54.324350: Validation loss did not improve from -0.63346. Patience: 20/50
2024-12-09 02:39:54.325610: train_loss -0.7635
2024-12-09 02:39:54.327037: val_loss -0.6083
2024-12-09 02:39:54.328336: Pseudo dice [0.7788]
2024-12-09 02:39:54.329469: Epoch time: 86.27 s
2024-12-09 02:39:55.545413: 
2024-12-09 02:39:55.547036: Epoch 183
2024-12-09 02:39:55.548037: Current learning rate: 0.00834
2024-12-09 02:41:21.742845: Validation loss did not improve from -0.63346. Patience: 21/50
2024-12-09 02:41:21.744500: train_loss -0.7595
2024-12-09 02:41:21.746136: val_loss -0.6106
2024-12-09 02:41:21.746894: Pseudo dice [0.7807]
2024-12-09 02:41:21.747969: Epoch time: 86.2 s
2024-12-09 02:41:22.960218: 
2024-12-09 02:41:22.962267: Epoch 184
2024-12-09 02:41:22.963209: Current learning rate: 0.00833
2024-12-09 02:42:49.110337: Validation loss did not improve from -0.63346. Patience: 22/50
2024-12-09 02:42:49.111894: train_loss -0.7618
2024-12-09 02:42:49.113713: val_loss -0.6001
2024-12-09 02:42:49.114564: Pseudo dice [0.779]
2024-12-09 02:42:49.115624: Epoch time: 86.15 s
2024-12-09 02:42:50.609121: 
2024-12-09 02:42:50.610996: Epoch 185
2024-12-09 02:42:50.612454: Current learning rate: 0.00832
2024-12-09 02:44:16.727722: Validation loss did not improve from -0.63346. Patience: 23/50
2024-12-09 02:44:16.728810: train_loss -0.7557
2024-12-09 02:44:16.729748: val_loss -0.602
2024-12-09 02:44:16.730494: Pseudo dice [0.7733]
2024-12-09 02:44:16.731188: Epoch time: 86.12 s
2024-12-09 02:44:17.941665: 
2024-12-09 02:44:17.944639: Epoch 186
2024-12-09 02:44:17.945983: Current learning rate: 0.00831
2024-12-09 02:45:44.225686: Validation loss did not improve from -0.63346. Patience: 24/50
2024-12-09 02:45:44.226936: train_loss -0.755
2024-12-09 02:45:44.228246: val_loss -0.6222
2024-12-09 02:45:44.228927: Pseudo dice [0.7853]
2024-12-09 02:45:44.229751: Epoch time: 86.29 s
2024-12-09 02:45:45.419796: 
2024-12-09 02:45:45.422251: Epoch 187
2024-12-09 02:45:45.423276: Current learning rate: 0.0083
2024-12-09 02:47:11.728356: Validation loss did not improve from -0.63346. Patience: 25/50
2024-12-09 02:47:11.729851: train_loss -0.7279
2024-12-09 02:47:11.731321: val_loss -0.5498
2024-12-09 02:47:11.732807: Pseudo dice [0.7383]
2024-12-09 02:47:11.733813: Epoch time: 86.31 s
2024-12-09 02:47:12.907959: 
2024-12-09 02:47:12.910158: Epoch 188
2024-12-09 02:47:12.911562: Current learning rate: 0.00829
2024-12-09 02:48:39.200715: Validation loss did not improve from -0.63346. Patience: 26/50
2024-12-09 02:48:39.201786: train_loss -0.7342
2024-12-09 02:48:39.202838: val_loss -0.5816
2024-12-09 02:48:39.203722: Pseudo dice [0.7634]
2024-12-09 02:48:39.204666: Epoch time: 86.29 s
2024-12-09 02:48:40.418284: 
2024-12-09 02:48:40.420791: Epoch 189
2024-12-09 02:48:40.421621: Current learning rate: 0.00828
2024-12-09 02:50:06.846295: Validation loss did not improve from -0.63346. Patience: 27/50
2024-12-09 02:50:06.847708: train_loss -0.7371
2024-12-09 02:50:06.849174: val_loss -0.6071
2024-12-09 02:50:06.850357: Pseudo dice [0.78]
2024-12-09 02:50:06.851388: Epoch time: 86.43 s
2024-12-09 02:50:08.654028: 
2024-12-09 02:50:08.656252: Epoch 190
2024-12-09 02:50:08.657541: Current learning rate: 0.00827
2024-12-09 02:51:35.161592: Validation loss did not improve from -0.63346. Patience: 28/50
2024-12-09 02:51:35.162695: train_loss -0.7425
2024-12-09 02:51:35.163663: val_loss -0.6151
2024-12-09 02:51:35.164440: Pseudo dice [0.7774]
2024-12-09 02:51:35.165041: Epoch time: 86.51 s
2024-12-09 02:51:36.709125: 
2024-12-09 02:51:36.711201: Epoch 191
2024-12-09 02:51:36.712368: Current learning rate: 0.00826
2024-12-09 02:53:03.098097: Validation loss did not improve from -0.63346. Patience: 29/50
2024-12-09 02:53:03.099295: train_loss -0.7506
2024-12-09 02:53:03.100552: val_loss -0.5721
2024-12-09 02:53:03.101846: Pseudo dice [0.7528]
2024-12-09 02:53:03.102659: Epoch time: 86.39 s
2024-12-09 02:53:04.310228: 
2024-12-09 02:53:04.312023: Epoch 192
2024-12-09 02:53:04.312986: Current learning rate: 0.00825
2024-12-09 02:54:30.668843: Validation loss did not improve from -0.63346. Patience: 30/50
2024-12-09 02:54:30.670170: train_loss -0.7516
2024-12-09 02:54:30.671216: val_loss -0.5788
2024-12-09 02:54:30.672390: Pseudo dice [0.7605]
2024-12-09 02:54:30.673370: Epoch time: 86.36 s
2024-12-09 02:54:31.900877: 
2024-12-09 02:54:31.903143: Epoch 193
2024-12-09 02:54:31.904057: Current learning rate: 0.00824
2024-12-09 02:55:58.356753: Validation loss did not improve from -0.63346. Patience: 31/50
2024-12-09 02:55:58.357639: train_loss -0.753
2024-12-09 02:55:58.358499: val_loss -0.5788
2024-12-09 02:55:58.359347: Pseudo dice [0.7628]
2024-12-09 02:55:58.360572: Epoch time: 86.46 s
2024-12-09 02:55:59.586343: 
2024-12-09 02:55:59.589448: Epoch 194
2024-12-09 02:55:59.591483: Current learning rate: 0.00824
2024-12-09 02:57:26.062311: Validation loss did not improve from -0.63346. Patience: 32/50
2024-12-09 02:57:26.063766: train_loss -0.7609
2024-12-09 02:57:26.065177: val_loss -0.624
2024-12-09 02:57:26.066153: Pseudo dice [0.7872]
2024-12-09 02:57:26.067541: Epoch time: 86.48 s
2024-12-09 02:57:27.660699: 
2024-12-09 02:57:27.662606: Epoch 195
2024-12-09 02:57:27.664029: Current learning rate: 0.00823
2024-12-09 02:58:54.169477: Validation loss did not improve from -0.63346. Patience: 33/50
2024-12-09 02:58:54.170386: train_loss -0.7608
2024-12-09 02:58:54.171781: val_loss -0.5695
2024-12-09 02:58:54.172628: Pseudo dice [0.7669]
2024-12-09 02:58:54.173729: Epoch time: 86.51 s
2024-12-09 02:58:55.416813: 
2024-12-09 02:58:55.419117: Epoch 196
2024-12-09 02:58:55.420206: Current learning rate: 0.00822
2024-12-09 03:00:21.904938: Validation loss did not improve from -0.63346. Patience: 34/50
2024-12-09 03:00:21.906137: train_loss -0.7584
2024-12-09 03:00:21.907112: val_loss -0.6086
2024-12-09 03:00:21.908180: Pseudo dice [0.7783]
2024-12-09 03:00:21.908927: Epoch time: 86.49 s
2024-12-09 03:00:23.137989: 
2024-12-09 03:00:23.139825: Epoch 197
2024-12-09 03:00:23.140909: Current learning rate: 0.00821
2024-12-09 03:01:49.688203: Validation loss did not improve from -0.63346. Patience: 35/50
2024-12-09 03:01:49.689018: train_loss -0.7655
2024-12-09 03:01:49.689890: val_loss -0.5945
2024-12-09 03:01:49.691062: Pseudo dice [0.7665]
2024-12-09 03:01:49.691655: Epoch time: 86.55 s
2024-12-09 03:01:50.934424: 
2024-12-09 03:01:50.936675: Epoch 198
2024-12-09 03:01:50.937334: Current learning rate: 0.0082
2024-12-09 03:03:17.476414: Validation loss did not improve from -0.63346. Patience: 36/50
2024-12-09 03:03:17.477753: train_loss -0.764
2024-12-09 03:03:17.478869: val_loss -0.6021
2024-12-09 03:03:17.479664: Pseudo dice [0.777]
2024-12-09 03:03:17.480523: Epoch time: 86.54 s
2024-12-09 03:03:18.708981: 
2024-12-09 03:03:18.711024: Epoch 199
2024-12-09 03:03:18.712118: Current learning rate: 0.00819
2024-12-09 03:04:45.269529: Validation loss did not improve from -0.63346. Patience: 37/50
2024-12-09 03:04:45.270849: train_loss -0.7581
2024-12-09 03:04:45.272003: val_loss -0.6218
2024-12-09 03:04:45.272973: Pseudo dice [0.7836]
2024-12-09 03:04:45.273667: Epoch time: 86.56 s
2024-12-09 03:04:46.841881: 
2024-12-09 03:04:46.844539: Epoch 200
2024-12-09 03:04:46.845833: Current learning rate: 0.00818
2024-12-09 03:06:13.037244: Validation loss did not improve from -0.63346. Patience: 38/50
2024-12-09 03:06:13.038628: train_loss -0.7657
2024-12-09 03:06:13.040318: val_loss -0.5764
2024-12-09 03:06:13.041374: Pseudo dice [0.7533]
2024-12-09 03:06:13.042696: Epoch time: 86.2 s
2024-12-09 03:06:14.636491: 
2024-12-09 03:06:14.639092: Epoch 201
2024-12-09 03:06:14.640059: Current learning rate: 0.00817
2024-12-09 03:07:40.953487: Validation loss did not improve from -0.63346. Patience: 39/50
2024-12-09 03:07:40.954976: train_loss -0.7592
2024-12-09 03:07:40.956473: val_loss -0.5953
2024-12-09 03:07:40.957542: Pseudo dice [0.7705]
2024-12-09 03:07:40.958478: Epoch time: 86.32 s
2024-12-09 03:07:42.208961: 
2024-12-09 03:07:42.210843: Epoch 202
2024-12-09 03:07:42.212222: Current learning rate: 0.00816
2024-12-09 03:09:08.449076: Validation loss did not improve from -0.63346. Patience: 40/50
2024-12-09 03:09:08.450395: train_loss -0.7634
2024-12-09 03:09:08.451702: val_loss -0.6
2024-12-09 03:09:08.452308: Pseudo dice [0.7765]
2024-12-09 03:09:08.453314: Epoch time: 86.24 s
2024-12-09 03:09:09.683987: 
2024-12-09 03:09:09.685095: Epoch 203
2024-12-09 03:09:09.686116: Current learning rate: 0.00815
2024-12-09 03:10:35.985943: Validation loss did not improve from -0.63346. Patience: 41/50
2024-12-09 03:10:35.987303: train_loss -0.7689
2024-12-09 03:10:35.988390: val_loss -0.6014
2024-12-09 03:10:35.989469: Pseudo dice [0.7724]
2024-12-09 03:10:35.990525: Epoch time: 86.3 s
2024-12-09 03:10:37.209610: 
2024-12-09 03:10:37.211957: Epoch 204
2024-12-09 03:10:37.213147: Current learning rate: 0.00814
2024-12-09 03:12:03.469325: Validation loss did not improve from -0.63346. Patience: 42/50
2024-12-09 03:12:03.470643: train_loss -0.7692
2024-12-09 03:12:03.471969: val_loss -0.5899
2024-12-09 03:12:03.472973: Pseudo dice [0.7653]
2024-12-09 03:12:03.473893: Epoch time: 86.26 s
2024-12-09 03:12:05.071779: 
2024-12-09 03:12:05.074196: Epoch 205
2024-12-09 03:12:05.075423: Current learning rate: 0.00813
2024-12-09 03:13:31.317627: Validation loss did not improve from -0.63346. Patience: 43/50
2024-12-09 03:13:31.318462: train_loss -0.7685
2024-12-09 03:13:31.319166: val_loss -0.6008
2024-12-09 03:13:31.319794: Pseudo dice [0.7764]
2024-12-09 03:13:31.320422: Epoch time: 86.25 s
2024-12-09 03:13:32.474785: 
2024-12-09 03:13:32.477330: Epoch 206
2024-12-09 03:13:32.478053: Current learning rate: 0.00813
2024-12-09 03:14:58.735255: Validation loss did not improve from -0.63346. Patience: 44/50
2024-12-09 03:14:58.736746: train_loss -0.766
2024-12-09 03:14:58.738215: val_loss -0.6211
2024-12-09 03:14:58.739195: Pseudo dice [0.7858]
2024-12-09 03:14:58.740384: Epoch time: 86.26 s
2024-12-09 03:14:59.990972: 
2024-12-09 03:14:59.992916: Epoch 207
2024-12-09 03:14:59.994154: Current learning rate: 0.00812
2024-12-09 03:16:26.585738: Validation loss did not improve from -0.63346. Patience: 45/50
2024-12-09 03:16:26.586776: train_loss -0.7698
2024-12-09 03:16:26.589549: val_loss -0.5852
2024-12-09 03:16:26.590374: Pseudo dice [0.7717]
2024-12-09 03:16:26.591522: Epoch time: 86.6 s
2024-12-09 03:16:27.793447: 
2024-12-09 03:16:27.795128: Epoch 208
2024-12-09 03:16:27.796120: Current learning rate: 0.00811
2024-12-09 03:17:54.260270: Validation loss did not improve from -0.63346. Patience: 46/50
2024-12-09 03:17:54.265437: train_loss -0.7713
2024-12-09 03:17:54.266904: val_loss -0.6042
2024-12-09 03:17:54.268170: Pseudo dice [0.779]
2024-12-09 03:17:54.269608: Epoch time: 86.47 s
2024-12-09 03:17:55.500937: 
2024-12-09 03:17:55.502919: Epoch 209
2024-12-09 03:17:55.503687: Current learning rate: 0.0081
2024-12-09 03:19:22.024240: Validation loss did not improve from -0.63346. Patience: 47/50
2024-12-09 03:19:22.025498: train_loss -0.7713
2024-12-09 03:19:22.026998: val_loss -0.6226
2024-12-09 03:19:22.027957: Pseudo dice [0.7915]
2024-12-09 03:19:22.028964: Epoch time: 86.53 s
2024-12-09 03:19:23.538992: 
2024-12-09 03:19:23.541192: Epoch 210
2024-12-09 03:19:23.542587: Current learning rate: 0.00809
2024-12-09 03:20:49.941466: Validation loss did not improve from -0.63346. Patience: 48/50
2024-12-09 03:20:49.942891: train_loss -0.7694
2024-12-09 03:20:49.943953: val_loss -0.6093
2024-12-09 03:20:49.945124: Pseudo dice [0.7737]
2024-12-09 03:20:49.946263: Epoch time: 86.4 s
2024-12-09 03:20:51.102377: 
2024-12-09 03:20:51.104096: Epoch 211
2024-12-09 03:20:51.104939: Current learning rate: 0.00808
2024-12-09 03:22:17.503274: Validation loss did not improve from -0.63346. Patience: 49/50
2024-12-09 03:22:17.504537: train_loss -0.7596
2024-12-09 03:22:17.505861: val_loss -0.608
2024-12-09 03:22:17.506602: Pseudo dice [0.7763]
2024-12-09 03:22:17.507242: Epoch time: 86.4 s
2024-12-09 03:22:19.325290: 
2024-12-09 03:22:19.327723: Epoch 212
2024-12-09 03:22:19.328863: Current learning rate: 0.00807
2024-12-09 03:23:45.860487: Validation loss did not improve from -0.63346. Patience: 50/50
2024-12-09 03:23:45.861585: train_loss -0.7664
2024-12-09 03:23:45.863072: val_loss -0.6204
2024-12-09 03:23:45.864202: Pseudo dice [0.7908]
2024-12-09 03:23:45.865244: Epoch time: 86.54 s
2024-12-09 03:23:47.034691: Patience reached. Stopping training.
2024-12-09 03:23:47.412153: Training done.
2024-12-09 03:23:47.553649: Using splits from existing split file: /home/gridsan/nchutisilp/datasets/nnUNet_Datasets/nnUNet_preprocessed/Dataset309_Sohee_Calcium_OCT_CrossValidation/splits_final.json
2024-12-09 03:23:47.555392: The split file contains 5 splits.
2024-12-09 03:23:47.556089: Desired fold for training: 2
2024-12-09 03:23:47.556626: This split has 6 training and 2 validation cases.
2024-12-09 03:23:47.557322: predicting 101-044
2024-12-09 03:23:47.569693: 101-044, shape torch.Size([1, 404, 498, 498]), rank 0
2024-12-09 03:25:33.402185: predicting 704-003
2024-12-09 03:25:33.428398: 704-003, shape torch.Size([1, 375, 498, 498]), rank 0
2024-12-09 03:27:23.610458: Validation complete
2024-12-09 03:27:23.611181: Mean Validation Dice:  0.7621469115918889

############################
INFO: You are using the old nnU-Net default plans. We have updated our recommendations. Please consider using those instead! Read more here: https://github.com/MIC-DKFZ/nnUNet/blob/master/documentation/resenc_presets.md
############################

Using device: cuda:0

#######################################################################
Please cite the following paper when using nnU-Net:
Isensee, F., Jaeger, P. F., Kohl, S. A., Petersen, J., & Maier-Hein, K. H. (2021). nnU-Net: a self-configuring method for deep learning-based biomedical image segmentation. Nature methods, 18(2), 203-211.
#######################################################################

2024-12-09 03:27:31.505877: Using torch.compile...
/home/gridsan/nchutisilp/.local/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn("The verbose parameter is deprecated. Please use get_last_lr() "
################### Loading pretrained weights from file  /home/gridsan/nchutisilp/projects/OpenAI-CLIP/clip_preivl_poststent_pretrained_nnUNet.pt ###################
Below is the list of overlapping blocks in pretrained model and nnUNet architecture:
encoder.stages.0.0.convs.0.conv.weight shape torch.Size([32, 1, 3, 3, 3])
encoder.stages.0.0.convs.0.conv.bias shape torch.Size([32])
encoder.stages.0.0.convs.0.norm.weight shape torch.Size([32])
encoder.stages.0.0.convs.0.norm.bias shape torch.Size([32])
encoder.stages.0.0.convs.0.all_modules.0.weight shape torch.Size([32, 1, 3, 3, 3])
encoder.stages.0.0.convs.0.all_modules.0.bias shape torch.Size([32])
encoder.stages.0.0.convs.0.all_modules.1.weight shape torch.Size([32])
encoder.stages.0.0.convs.0.all_modules.1.bias shape torch.Size([32])
encoder.stages.0.0.convs.1.conv.weight shape torch.Size([32, 32, 3, 3, 3])
encoder.stages.0.0.convs.1.conv.bias shape torch.Size([32])
encoder.stages.0.0.convs.1.norm.weight shape torch.Size([32])
encoder.stages.0.0.convs.1.norm.bias shape torch.Size([32])
encoder.stages.0.0.convs.1.all_modules.0.weight shape torch.Size([32, 32, 3, 3, 3])
encoder.stages.0.0.convs.1.all_modules.0.bias shape torch.Size([32])
encoder.stages.0.0.convs.1.all_modules.1.weight shape torch.Size([32])
encoder.stages.0.0.convs.1.all_modules.1.bias shape torch.Size([32])
encoder.stages.1.0.convs.0.conv.weight shape torch.Size([64, 32, 3, 3, 3])
encoder.stages.1.0.convs.0.conv.bias shape torch.Size([64])
encoder.stages.1.0.convs.0.norm.weight shape torch.Size([64])
encoder.stages.1.0.convs.0.norm.bias shape torch.Size([64])
encoder.stages.1.0.convs.0.all_modules.0.weight shape torch.Size([64, 32, 3, 3, 3])
encoder.stages.1.0.convs.0.all_modules.0.bias shape torch.Size([64])
encoder.stages.1.0.convs.0.all_modules.1.weight shape torch.Size([64])
encoder.stages.1.0.convs.0.all_modules.1.bias shape torch.Size([64])
encoder.stages.1.0.convs.1.conv.weight shape torch.Size([64, 64, 3, 3, 3])
encoder.stages.1.0.convs.1.conv.bias shape torch.Size([64])
encoder.stages.1.0.convs.1.norm.weight shape torch.Size([64])
encoder.stages.1.0.convs.1.norm.bias shape torch.Size([64])
encoder.stages.1.0.convs.1.all_modules.0.weight shape torch.Size([64, 64, 3, 3, 3])
encoder.stages.1.0.convs.1.all_modules.0.bias shape torch.Size([64])
encoder.stages.1.0.convs.1.all_modules.1.weight shape torch.Size([64])
encoder.stages.1.0.convs.1.all_modules.1.bias shape torch.Size([64])
encoder.stages.2.0.convs.0.conv.weight shape torch.Size([128, 64, 3, 3, 3])
encoder.stages.2.0.convs.0.conv.bias shape torch.Size([128])
encoder.stages.2.0.convs.0.norm.weight shape torch.Size([128])
encoder.stages.2.0.convs.0.norm.bias shape torch.Size([128])
encoder.stages.2.0.convs.0.all_modules.0.weight shape torch.Size([128, 64, 3, 3, 3])
encoder.stages.2.0.convs.0.all_modules.0.bias shape torch.Size([128])
encoder.stages.2.0.convs.0.all_modules.1.weight shape torch.Size([128])
encoder.stages.2.0.convs.0.all_modules.1.bias shape torch.Size([128])
encoder.stages.2.0.convs.1.conv.weight shape torch.Size([128, 128, 3, 3, 3])
encoder.stages.2.0.convs.1.conv.bias shape torch.Size([128])
encoder.stages.2.0.convs.1.norm.weight shape torch.Size([128])
encoder.stages.2.0.convs.1.norm.bias shape torch.Size([128])
encoder.stages.2.0.convs.1.all_modules.0.weight shape torch.Size([128, 128, 3, 3, 3])
encoder.stages.2.0.convs.1.all_modules.0.bias shape torch.Size([128])
encoder.stages.2.0.convs.1.all_modules.1.weight shape torch.Size([128])
encoder.stages.2.0.convs.1.all_modules.1.bias shape torch.Size([128])
encoder.stages.3.0.convs.0.conv.weight shape torch.Size([256, 128, 3, 3, 3])
encoder.stages.3.0.convs.0.conv.bias shape torch.Size([256])
encoder.stages.3.0.convs.0.norm.weight shape torch.Size([256])
encoder.stages.3.0.convs.0.norm.bias shape torch.Size([256])
encoder.stages.3.0.convs.0.all_modules.0.weight shape torch.Size([256, 128, 3, 3, 3])
encoder.stages.3.0.convs.0.all_modules.0.bias shape torch.Size([256])
encoder.stages.3.0.convs.0.all_modules.1.weight shape torch.Size([256])
encoder.stages.3.0.convs.0.all_modules.1.bias shape torch.Size([256])
encoder.stages.3.0.convs.1.conv.weight shape torch.Size([256, 256, 3, 3, 3])
encoder.stages.3.0.convs.1.conv.bias shape torch.Size([256])
encoder.stages.3.0.convs.1.norm.weight shape torch.Size([256])
encoder.stages.3.0.convs.1.norm.bias shape torch.Size([256])
encoder.stages.3.0.convs.1.all_modules.0.weight shape torch.Size([256, 256, 3, 3, 3])
encoder.stages.3.0.convs.1.all_modules.0.bias shape torch.Size([256])
encoder.stages.3.0.convs.1.all_modules.1.weight shape torch.Size([256])
encoder.stages.3.0.convs.1.all_modules.1.bias shape torch.Size([256])
encoder.stages.4.0.convs.0.conv.weight shape torch.Size([320, 256, 3, 3, 3])
encoder.stages.4.0.convs.0.conv.bias shape torch.Size([320])
encoder.stages.4.0.convs.0.norm.weight shape torch.Size([320])
encoder.stages.4.0.convs.0.norm.bias shape torch.Size([320])
encoder.stages.4.0.convs.0.all_modules.0.weight shape torch.Size([320, 256, 3, 3, 3])
encoder.stages.4.0.convs.0.all_modules.0.bias shape torch.Size([320])
encoder.stages.4.0.convs.0.all_modules.1.weight shape torch.Size([320])
encoder.stages.4.0.convs.0.all_modules.1.bias shape torch.Size([320])
encoder.stages.4.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.4.0.convs.1.conv.bias shape torch.Size([320])
encoder.stages.4.0.convs.1.norm.weight shape torch.Size([320])
encoder.stages.4.0.convs.1.norm.bias shape torch.Size([320])
encoder.stages.4.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.4.0.convs.1.all_modules.0.bias shape torch.Size([320])
encoder.stages.4.0.convs.1.all_modules.1.weight shape torch.Size([320])
encoder.stages.4.0.convs.1.all_modules.1.bias shape torch.Size([320])
encoder.stages.5.0.convs.0.conv.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.5.0.convs.0.conv.bias shape torch.Size([320])
encoder.stages.5.0.convs.0.norm.weight shape torch.Size([320])
encoder.stages.5.0.convs.0.norm.bias shape torch.Size([320])
encoder.stages.5.0.convs.0.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.5.0.convs.0.all_modules.0.bias shape torch.Size([320])
encoder.stages.5.0.convs.0.all_modules.1.weight shape torch.Size([320])
encoder.stages.5.0.convs.0.all_modules.1.bias shape torch.Size([320])
encoder.stages.5.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.5.0.convs.1.conv.bias shape torch.Size([320])
encoder.stages.5.0.convs.1.norm.weight shape torch.Size([320])
encoder.stages.5.0.convs.1.norm.bias shape torch.Size([320])
encoder.stages.5.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.5.0.convs.1.all_modules.0.bias shape torch.Size([320])
encoder.stages.5.0.convs.1.all_modules.1.weight shape torch.Size([320])
encoder.stages.5.0.convs.1.all_modules.1.bias shape torch.Size([320])
decoder.encoder.stages.0.0.convs.0.conv.weight shape torch.Size([32, 1, 3, 3, 3])
decoder.encoder.stages.0.0.convs.0.conv.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.norm.weight shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.norm.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.all_modules.0.weight shape torch.Size([32, 1, 3, 3, 3])
decoder.encoder.stages.0.0.convs.0.all_modules.0.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.all_modules.1.weight shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.all_modules.1.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.conv.weight shape torch.Size([32, 32, 3, 3, 3])
decoder.encoder.stages.0.0.convs.1.conv.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.norm.weight shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.norm.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.all_modules.0.weight shape torch.Size([32, 32, 3, 3, 3])
decoder.encoder.stages.0.0.convs.1.all_modules.0.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.all_modules.1.weight shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.all_modules.1.bias shape torch.Size([32])
decoder.encoder.stages.1.0.convs.0.conv.weight shape torch.Size([64, 32, 3, 3, 3])
decoder.encoder.stages.1.0.convs.0.conv.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.norm.weight shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.norm.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.all_modules.0.weight shape torch.Size([64, 32, 3, 3, 3])
decoder.encoder.stages.1.0.convs.0.all_modules.0.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.all_modules.1.weight shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.all_modules.1.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.conv.weight shape torch.Size([64, 64, 3, 3, 3])
decoder.encoder.stages.1.0.convs.1.conv.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.norm.weight shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.norm.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.all_modules.0.weight shape torch.Size([64, 64, 3, 3, 3])
decoder.encoder.stages.1.0.convs.1.all_modules.0.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.all_modules.1.weight shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.all_modules.1.bias shape torch.Size([64])
decoder.encoder.stages.2.0.convs.0.conv.weight shape torch.Size([128, 64, 3, 3, 3])
decoder.encoder.stages.2.0.convs.0.conv.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.norm.weight shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.norm.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.all_modules.0.weight shape torch.Size([128, 64, 3, 3, 3])
decoder.encoder.stages.2.0.convs.0.all_modules.0.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.all_modules.1.weight shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.all_modules.1.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.conv.weight shape torch.Size([128, 128, 3, 3, 3])
decoder.encoder.stages.2.0.convs.1.conv.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.norm.weight shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.norm.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.all_modules.0.weight shape torch.Size([128, 128, 3, 3, 3])
decoder.encoder.stages.2.0.convs.1.all_modules.0.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.all_modules.1.weight shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.all_modules.1.bias shape torch.Size([128])
decoder.encoder.stages.3.0.convs.0.conv.weight shape torch.Size([256, 128, 3, 3, 3])
decoder.encoder.stages.3.0.convs.0.conv.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.norm.weight shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.norm.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.all_modules.0.weight shape torch.Size([256, 128, 3, 3, 3])
decoder.encoder.stages.3.0.convs.0.all_modules.0.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.all_modules.1.weight shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.all_modules.1.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.conv.weight shape torch.Size([256, 256, 3, 3, 3])
decoder.encoder.stages.3.0.convs.1.conv.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.norm.weight shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.norm.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.all_modules.0.weight shape torch.Size([256, 256, 3, 3, 3])
decoder.encoder.stages.3.0.convs.1.all_modules.0.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.all_modules.1.weight shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.all_modules.1.bias shape torch.Size([256])
decoder.encoder.stages.4.0.convs.0.conv.weight shape torch.Size([320, 256, 3, 3, 3])
decoder.encoder.stages.4.0.convs.0.conv.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.norm.weight shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.norm.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.all_modules.0.weight shape torch.Size([320, 256, 3, 3, 3])
decoder.encoder.stages.4.0.convs.0.all_modules.0.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.all_modules.1.weight shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.all_modules.1.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.4.0.convs.1.conv.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.norm.weight shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.norm.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.4.0.convs.1.all_modules.0.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.all_modules.1.weight shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.all_modules.1.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.conv.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.5.0.convs.0.conv.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.norm.weight shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.norm.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.5.0.convs.0.all_modules.0.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.all_modules.1.weight shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.all_modules.1.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.5.0.convs.1.conv.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.norm.weight shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.norm.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.5.0.convs.1.all_modules.0.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.all_modules.1.weight shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.all_modules.1.bias shape torch.Size([320])
decoder.stages.0.convs.0.conv.weight shape torch.Size([320, 640, 3, 3, 3])
decoder.stages.0.convs.0.conv.bias shape torch.Size([320])
decoder.stages.0.convs.0.norm.weight shape torch.Size([320])
decoder.stages.0.convs.0.norm.bias shape torch.Size([320])
decoder.stages.0.convs.0.all_modules.0.weight shape torch.Size([320, 640, 3, 3, 3])
decoder.stages.0.convs.0.all_modules.0.bias shape torch.Size([320])
decoder.stages.0.convs.0.all_modules.1.weight shape torch.Size([320])
decoder.stages.0.convs.0.all_modules.1.bias shape torch.Size([320])
decoder.stages.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.stages.0.convs.1.conv.bias shape torch.Size([320])
decoder.stages.0.convs.1.norm.weight shape torch.Size([320])
decoder.stages.0.convs.1.norm.bias shape torch.Size([320])
decoder.stages.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.stages.0.convs.1.all_modules.0.bias shape torch.Size([320])
decoder.stages.0.convs.1.all_modules.1.weight shape torch.Size([320])
decoder.stages.0.convs.1.all_modules.1.bias shape torch.Size([320])
decoder.stages.1.convs.0.conv.weight shape torch.Size([256, 512, 3, 3, 3])
decoder.stages.1.convs.0.conv.bias shape torch.Size([256])
decoder.stages.1.convs.0.norm.weight shape torch.Size([256])
decoder.stages.1.convs.0.norm.bias shape torch.Size([256])
decoder.stages.1.convs.0.all_modules.0.weight shape torch.Size([256, 512, 3, 3, 3])
decoder.stages.1.convs.0.all_modules.0.bias shape torch.Size([256])
decoder.stages.1.convs.0.all_modules.1.weight shape torch.Size([256])
decoder.stages.1.convs.0.all_modules.1.bias shape torch.Size([256])
decoder.stages.1.convs.1.conv.weight shape torch.Size([256, 256, 3, 3, 3])
decoder.stages.1.convs.1.conv.bias shape torch.Size([256])
decoder.stages.1.convs.1.norm.weight shape torch.Size([256])
decoder.stages.1.convs.1.norm.bias shape torch.Size([256])
decoder.stages.1.convs.1.all_modules.0.weight shape torch.Size([256, 256, 3, 3, 3])
decoder.stages.1.convs.1.all_modules.0.bias shape torch.Size([256])
decoder.stages.1.convs.1.all_modules.1.weight shape torch.Size([256])
decoder.stages.1.convs.1.all_modules.1.bias shape torch.Size([256])
decoder.stages.2.convs.0.conv.weight shape torch.Size([128, 256, 3, 3, 3])
decoder.stages.2.convs.0.conv.bias shape torch.Size([128])
decoder.stages.2.convs.0.norm.weight shape torch.Size([128])
decoder.stages.2.convs.0.norm.bias shape torch.Size([128])
decoder.stages.2.convs.0.all_modules.0.weight shape torch.Size([128, 256, 3, 3, 3])
decoder.stages.2.convs.0.all_modules.0.bias shape torch.Size([128])
decoder.stages.2.convs.0.all_modules.1.weight shape torch.Size([128])
decoder.stages.2.convs.0.all_modules.1.bias shape torch.Size([128])
decoder.stages.2.convs.1.conv.weight shape torch.Size([128, 128, 3, 3, 3])
decoder.stages.2.convs.1.conv.bias shape torch.Size([128])
decoder.stages.2.convs.1.norm.weight shape torch.Size([128])
decoder.stages.2.convs.1.norm.bias shape torch.Size([128])
decoder.stages.2.convs.1.all_modules.0.weight shape torch.Size([128, 128, 3, 3, 3])
decoder.stages.2.convs.1.all_modules.0.bias shape torch.Size([128])
decoder.stages.2.convs.1.all_modules.1.weight shape torch.Size([128])
decoder.stages.2.convs.1.all_modules.1.bias shape torch.Size([128])
decoder.stages.3.convs.0.conv.weight shape torch.Size([64, 128, 3, 3, 3])
decoder.stages.3.convs.0.conv.bias shape torch.Size([64])
decoder.stages.3.convs.0.norm.weight shape torch.Size([64])
decoder.stages.3.convs.0.norm.bias shape torch.Size([64])
decoder.stages.3.convs.0.all_modules.0.weight shape torch.Size([64, 128, 3, 3, 3])
decoder.stages.3.convs.0.all_modules.0.bias shape torch.Size([64])
decoder.stages.3.convs.0.all_modules.1.weight shape torch.Size([64])
decoder.stages.3.convs.0.all_modules.1.bias shape torch.Size([64])
decoder.stages.3.convs.1.conv.weight shape torch.Size([64, 64, 3, 3, 3])
decoder.stages.3.convs.1.conv.bias shape torch.Size([64])
decoder.stages.3.convs.1.norm.weight shape torch.Size([64])
decoder.stages.3.convs.1.norm.bias shape torch.Size([64])
decoder.stages.3.convs.1.all_modules.0.weight shape torch.Size([64, 64, 3, 3, 3])
decoder.stages.3.convs.1.all_modules.0.bias shape torch.Size([64])
decoder.stages.3.convs.1.all_modules.1.weight shape torch.Size([64])
decoder.stages.3.convs.1.all_modules.1.bias shape torch.Size([64])
decoder.stages.4.convs.0.conv.weight shape torch.Size([32, 64, 3, 3, 3])
decoder.stages.4.convs.0.conv.bias shape torch.Size([32])
decoder.stages.4.convs.0.norm.weight shape torch.Size([32])
decoder.stages.4.convs.0.norm.bias shape torch.Size([32])
decoder.stages.4.convs.0.all_modules.0.weight shape torch.Size([32, 64, 3, 3, 3])
decoder.stages.4.convs.0.all_modules.0.bias shape torch.Size([32])
decoder.stages.4.convs.0.all_modules.1.weight shape torch.Size([32])
decoder.stages.4.convs.0.all_modules.1.bias shape torch.Size([32])
decoder.stages.4.convs.1.conv.weight shape torch.Size([32, 32, 3, 3, 3])
decoder.stages.4.convs.1.conv.bias shape torch.Size([32])
decoder.stages.4.convs.1.norm.weight shape torch.Size([32])
decoder.stages.4.convs.1.norm.bias shape torch.Size([32])
decoder.stages.4.convs.1.all_modules.0.weight shape torch.Size([32, 32, 3, 3, 3])
decoder.stages.4.convs.1.all_modules.0.bias shape torch.Size([32])
decoder.stages.4.convs.1.all_modules.1.weight shape torch.Size([32])
decoder.stages.4.convs.1.all_modules.1.bias shape torch.Size([32])
decoder.transpconvs.0.weight shape torch.Size([320, 320, 1, 2, 2])
decoder.transpconvs.0.bias shape torch.Size([320])
decoder.transpconvs.1.weight shape torch.Size([320, 256, 2, 2, 2])
decoder.transpconvs.1.bias shape torch.Size([256])
decoder.transpconvs.2.weight shape torch.Size([256, 128, 2, 2, 2])
decoder.transpconvs.2.bias shape torch.Size([128])
decoder.transpconvs.3.weight shape torch.Size([128, 64, 2, 2, 2])
decoder.transpconvs.3.bias shape torch.Size([64])
decoder.transpconvs.4.weight shape torch.Size([64, 32, 2, 2, 2])
decoder.transpconvs.4.bias shape torch.Size([32])
################### Done ###################
2024-12-09 03:27:34.498746: do_dummy_2d_data_aug: True
2024-12-09 03:27:34.501366: Using splits from existing split file: /home/gridsan/nchutisilp/datasets/nnUNet_Datasets/nnUNet_preprocessed/Dataset309_Sohee_Calcium_OCT_CrossValidation/splits_final.json
2024-12-09 03:27:34.503483: The split file contains 5 splits.
2024-12-09 03:27:34.504685: Desired fold for training: 4
2024-12-09 03:27:34.505702: This split has 7 training and 1 validation cases.
using pin_memory on device 0
using pin_memory on device 0

This is the configuration used by this training:
Configuration name: 3d_32x160x128_b10
 {'data_identifier': 'nnUNetPlans_3d_fullres', 'preprocessor_name': 'DefaultPreprocessor', 'batch_size': 10, 'patch_size': [32, 160, 128], 'median_image_size_in_voxels': [375.0, 498.0, 498.0], 'spacing': [1.0, 1.0, 1.0], 'normalization_schemes': ['ZScoreNormalization'], 'use_mask_for_norm': [False], 'resampling_fn_data': 'resample_data_or_seg_to_shape', 'resampling_fn_seg': 'resample_data_or_seg_to_shape', 'resampling_fn_data_kwargs': {'is_seg': False, 'order': 3, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_seg_kwargs': {'is_seg': True, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_probabilities': 'resample_data_or_seg_to_shape', 'resampling_fn_probabilities_kwargs': {'is_seg': False, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'architecture': {'network_class_name': 'dynamic_network_architectures.architectures.unet.PlainConvUNet', 'arch_kwargs': {'n_stages': 6, 'features_per_stage': [32, 64, 128, 256, 320, 320], 'conv_op': 'torch.nn.modules.conv.Conv3d', 'kernel_sizes': [[3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3]], 'strides': [[1, 1, 1], [2, 2, 2], [2, 2, 2], [2, 2, 2], [2, 2, 2], [1, 2, 2]], 'n_conv_per_stage': [2, 2, 2, 2, 2, 2], 'n_conv_per_stage_decoder': [2, 2, 2, 2, 2], 'conv_bias': True, 'norm_op': 'torch.nn.modules.instancenorm.InstanceNorm3d', 'norm_op_kwargs': {'eps': 1e-05, 'affine': True}, 'dropout_op': None, 'dropout_op_kwargs': None, 'nonlin': 'torch.nn.LeakyReLU', 'nonlin_kwargs': {'inplace': True}, 'deep_supervision': True}, '_kw_requires_import': ['conv_op', 'norm_op', 'dropout_op', 'nonlin']}, 'batch_dice': True, 'inherits_from': '3d_fullres'} 

These are the global plan.json settings:
 {'dataset_name': 'Dataset309_Sohee_Calcium_OCT_CrossValidation', 'plans_name': 'nnUNetPlans', 'original_median_spacing_after_transp': [1.0, 1.0, 1.0], 'original_median_shape_after_transp': [375, 498, 498], 'image_reader_writer': 'SimpleITKIO', 'transpose_forward': [0, 1, 2], 'transpose_backward': [0, 1, 2], 'experiment_planner_used': 'ExperimentPlanner', 'label_manager': 'LabelManager', 'foreground_intensity_properties_per_channel': {'0': {'max': 0.4977976381778717, 'mean': 0.13507525622844696, 'median': 0.09599608182907104, 'min': 0.0, 'percentile_00_5': 0.014754901640117168, 'percentile_99_5': 0.4977976381778717, 'std': 0.12025152146816254}}} 

2024-12-09 03:27:36.715712: unpacking dataset...
2024-12-09 03:27:40.959340: unpacking done...
2024-12-09 03:27:41.104597: Unable to plot network architecture: nnUNet_compile is enabled!
2024-12-09 03:27:41.151649: 
2024-12-09 03:27:41.153893: Epoch 0
2024-12-09 03:27:41.155360: Current learning rate: 0.01
2024-12-09 03:29:51.539335: Validation loss improved from 1000.00000 to -0.23899! Patience: 0/50
2024-12-09 03:29:51.540538: train_loss -0.0978
2024-12-09 03:29:51.541771: val_loss -0.239
2024-12-09 03:29:51.542745: Pseudo dice [0.5717]
2024-12-09 03:29:51.543388: Epoch time: 130.39 s
2024-12-09 03:29:51.544081: Yayy! New best EMA pseudo Dice: 0.5717
2024-12-09 03:29:52.998190: 
2024-12-09 03:29:52.999978: Epoch 1
2024-12-09 03:29:53.001453: Current learning rate: 0.00999
2024-12-09 03:31:19.246078: Validation loss improved from -0.23899 to -0.25383! Patience: 0/50
2024-12-09 03:31:19.247317: train_loss -0.231
2024-12-09 03:31:19.248287: val_loss -0.2538
2024-12-09 03:31:19.249153: Pseudo dice [0.584]
2024-12-09 03:31:19.250162: Epoch time: 86.25 s
2024-12-09 03:31:19.251261: Yayy! New best EMA pseudo Dice: 0.5729
2024-12-09 03:31:20.755274: 
2024-12-09 03:31:20.757344: Epoch 2
2024-12-09 03:31:20.758545: Current learning rate: 0.00998
2024-12-09 03:32:47.031595: Validation loss improved from -0.25383 to -0.26657! Patience: 0/50
2024-12-09 03:32:47.033020: train_loss -0.2714
2024-12-09 03:32:47.034379: val_loss -0.2666
2024-12-09 03:32:47.035111: Pseudo dice [0.582]
2024-12-09 03:32:47.035851: Epoch time: 86.28 s
2024-12-09 03:32:47.036604: Yayy! New best EMA pseudo Dice: 0.5738
2024-12-09 03:32:48.602822: 
2024-12-09 03:32:48.605678: Epoch 3
2024-12-09 03:32:48.607206: Current learning rate: 0.00997
2024-12-09 03:34:14.790880: Validation loss improved from -0.26657 to -0.31405! Patience: 0/50
2024-12-09 03:34:14.791855: train_loss -0.3234
2024-12-09 03:34:14.792862: val_loss -0.314
2024-12-09 03:34:14.794050: Pseudo dice [0.6119]
2024-12-09 03:34:14.795185: Epoch time: 86.19 s
2024-12-09 03:34:14.796066: Yayy! New best EMA pseudo Dice: 0.5776
2024-12-09 03:34:16.301199: 
2024-12-09 03:34:16.302940: Epoch 4
2024-12-09 03:34:16.304337: Current learning rate: 0.00996
2024-12-09 03:35:42.249965: Validation loss did not improve from -0.31405. Patience: 1/50
2024-12-09 03:35:42.251121: train_loss -0.3486
2024-12-09 03:35:42.252146: val_loss -0.2855
2024-12-09 03:35:42.253070: Pseudo dice [0.5872]
2024-12-09 03:35:42.254372: Epoch time: 85.95 s
2024-12-09 03:35:42.548269: Yayy! New best EMA pseudo Dice: 0.5786
2024-12-09 03:35:44.152097: 
2024-12-09 03:35:44.154669: Epoch 5
2024-12-09 03:35:44.156034: Current learning rate: 0.00995
2024-12-09 03:37:10.142761: Validation loss did not improve from -0.31405. Patience: 2/50
2024-12-09 03:37:10.144024: train_loss -0.3862
2024-12-09 03:37:10.145093: val_loss -0.308
2024-12-09 03:37:10.146309: Pseudo dice [0.6316]
2024-12-09 03:37:10.147180: Epoch time: 85.99 s
2024-12-09 03:37:10.148056: Yayy! New best EMA pseudo Dice: 0.5839
2024-12-09 03:37:11.632535: 
2024-12-09 03:37:11.635171: Epoch 6
2024-12-09 03:37:11.636783: Current learning rate: 0.00995
2024-12-09 03:38:37.633284: Validation loss improved from -0.31405 to -0.34534! Patience: 2/50
2024-12-09 03:38:37.634702: train_loss -0.4153
2024-12-09 03:38:37.636031: val_loss -0.3453
2024-12-09 03:38:37.636922: Pseudo dice [0.6502]
2024-12-09 03:38:37.637842: Epoch time: 86.0 s
2024-12-09 03:38:37.639024: Yayy! New best EMA pseudo Dice: 0.5905
2024-12-09 03:38:39.125412: 
2024-12-09 03:38:39.127829: Epoch 7
2024-12-09 03:38:39.128923: Current learning rate: 0.00994
2024-12-09 03:40:05.108025: Validation loss improved from -0.34534 to -0.40673! Patience: 0/50
2024-12-09 03:40:05.108990: train_loss -0.431
2024-12-09 03:40:05.109890: val_loss -0.4067
2024-12-09 03:40:05.110569: Pseudo dice [0.6611]
2024-12-09 03:40:05.111249: Epoch time: 85.98 s
2024-12-09 03:40:05.111905: Yayy! New best EMA pseudo Dice: 0.5976
2024-12-09 03:40:06.604064: 
2024-12-09 03:40:06.606908: Epoch 8
2024-12-09 03:40:06.608345: Current learning rate: 0.00993
2024-12-09 03:41:32.617781: Validation loss did not improve from -0.40673. Patience: 1/50
2024-12-09 03:41:32.619204: train_loss -0.4469
2024-12-09 03:41:32.620616: val_loss -0.3864
2024-12-09 03:41:32.621781: Pseudo dice [0.6517]
2024-12-09 03:41:32.622678: Epoch time: 86.02 s
2024-12-09 03:41:32.623682: Yayy! New best EMA pseudo Dice: 0.603
2024-12-09 03:41:34.427704: 
2024-12-09 03:41:34.429543: Epoch 9
2024-12-09 03:41:34.431006: Current learning rate: 0.00992
2024-12-09 03:43:00.532284: Validation loss did not improve from -0.40673. Patience: 2/50
2024-12-09 03:43:00.533797: train_loss -0.4611
2024-12-09 03:43:00.535085: val_loss -0.3917
2024-12-09 03:43:00.536117: Pseudo dice [0.6565]
2024-12-09 03:43:00.536867: Epoch time: 86.11 s
2024-12-09 03:43:00.863815: Yayy! New best EMA pseudo Dice: 0.6083
2024-12-09 03:43:02.306100: 
2024-12-09 03:43:02.307916: Epoch 10
2024-12-09 03:43:02.308923: Current learning rate: 0.00991
2024-12-09 03:44:28.283094: Validation loss improved from -0.40673 to -0.43394! Patience: 2/50
2024-12-09 03:44:28.284639: train_loss -0.4616
2024-12-09 03:44:28.285826: val_loss -0.4339
2024-12-09 03:44:28.287041: Pseudo dice [0.6823]
2024-12-09 03:44:28.287977: Epoch time: 85.98 s
2024-12-09 03:44:28.288859: Yayy! New best EMA pseudo Dice: 0.6157
2024-12-09 03:44:29.731669: 
2024-12-09 03:44:29.734810: Epoch 11
2024-12-09 03:44:29.736146: Current learning rate: 0.0099
2024-12-09 03:45:55.721317: Validation loss improved from -0.43394 to -0.44883! Patience: 0/50
2024-12-09 03:45:55.722729: train_loss -0.4795
2024-12-09 03:45:55.723510: val_loss -0.4488
2024-12-09 03:45:55.724206: Pseudo dice [0.6891]
2024-12-09 03:45:55.724872: Epoch time: 85.99 s
2024-12-09 03:45:55.725614: Yayy! New best EMA pseudo Dice: 0.6231
2024-12-09 03:45:57.188737: 
2024-12-09 03:45:57.190738: Epoch 12
2024-12-09 03:45:57.191909: Current learning rate: 0.00989
2024-12-09 03:47:23.279868: Validation loss did not improve from -0.44883. Patience: 1/50
2024-12-09 03:47:23.280653: train_loss -0.504
2024-12-09 03:47:23.281484: val_loss -0.4383
2024-12-09 03:47:23.282368: Pseudo dice [0.6862]
2024-12-09 03:47:23.283346: Epoch time: 86.09 s
2024-12-09 03:47:23.283973: Yayy! New best EMA pseudo Dice: 0.6294
2024-12-09 03:47:24.809395: 
2024-12-09 03:47:24.811229: Epoch 13
2024-12-09 03:47:24.812386: Current learning rate: 0.00988
2024-12-09 03:48:50.798901: Validation loss did not improve from -0.44883. Patience: 2/50
2024-12-09 03:48:50.800080: train_loss -0.5041
2024-12-09 03:48:50.801085: val_loss -0.4237
2024-12-09 03:48:50.801989: Pseudo dice [0.6855]
2024-12-09 03:48:50.802645: Epoch time: 85.99 s
2024-12-09 03:48:50.803403: Yayy! New best EMA pseudo Dice: 0.635
2024-12-09 03:48:52.302680: 
2024-12-09 03:48:52.304017: Epoch 14
2024-12-09 03:48:52.305003: Current learning rate: 0.00987
2024-12-09 03:50:18.290155: Validation loss did not improve from -0.44883. Patience: 3/50
2024-12-09 03:50:18.291692: train_loss -0.5186
2024-12-09 03:50:18.292933: val_loss -0.4328
2024-12-09 03:50:18.293858: Pseudo dice [0.6823]
2024-12-09 03:50:18.294606: Epoch time: 85.99 s
2024-12-09 03:50:18.629052: Yayy! New best EMA pseudo Dice: 0.6397
2024-12-09 03:50:20.127585: 
2024-12-09 03:50:20.129796: Epoch 15
2024-12-09 03:50:20.130933: Current learning rate: 0.00986
2024-12-09 03:51:46.109599: Validation loss improved from -0.44883 to -0.46154! Patience: 3/50
2024-12-09 03:51:46.110592: train_loss -0.5153
2024-12-09 03:51:46.111742: val_loss -0.4615
2024-12-09 03:51:46.112605: Pseudo dice [0.7003]
2024-12-09 03:51:46.113549: Epoch time: 85.98 s
2024-12-09 03:51:46.114518: Yayy! New best EMA pseudo Dice: 0.6458
2024-12-09 03:51:47.664453: 
2024-12-09 03:51:47.666447: Epoch 16
2024-12-09 03:51:47.667493: Current learning rate: 0.00986
2024-12-09 03:53:13.673308: Validation loss did not improve from -0.46154. Patience: 1/50
2024-12-09 03:53:13.674565: train_loss -0.5118
2024-12-09 03:53:13.675514: val_loss -0.4443
2024-12-09 03:53:13.676350: Pseudo dice [0.6877]
2024-12-09 03:53:13.677268: Epoch time: 86.01 s
2024-12-09 03:53:13.678111: Yayy! New best EMA pseudo Dice: 0.65
2024-12-09 03:53:15.211420: 
2024-12-09 03:53:15.213198: Epoch 17
2024-12-09 03:53:15.214451: Current learning rate: 0.00985
2024-12-09 03:54:41.181507: Validation loss did not improve from -0.46154. Patience: 2/50
2024-12-09 03:54:41.182854: train_loss -0.5341
2024-12-09 03:54:41.184086: val_loss -0.4273
2024-12-09 03:54:41.184968: Pseudo dice [0.688]
2024-12-09 03:54:41.186012: Epoch time: 85.97 s
2024-12-09 03:54:41.186884: Yayy! New best EMA pseudo Dice: 0.6538
2024-12-09 03:54:42.722733: 
2024-12-09 03:54:42.725014: Epoch 18
2024-12-09 03:54:42.726059: Current learning rate: 0.00984
2024-12-09 03:56:08.719632: Validation loss did not improve from -0.46154. Patience: 3/50
2024-12-09 03:56:08.720732: train_loss -0.5242
2024-12-09 03:56:08.721942: val_loss -0.4563
2024-12-09 03:56:08.722847: Pseudo dice [0.6988]
2024-12-09 03:56:08.723816: Epoch time: 86.0 s
2024-12-09 03:56:08.724689: Yayy! New best EMA pseudo Dice: 0.6583
2024-12-09 03:56:10.570015: 
2024-12-09 03:56:10.572147: Epoch 19
2024-12-09 03:56:10.573244: Current learning rate: 0.00983
2024-12-09 03:57:36.681233: Validation loss did not improve from -0.46154. Patience: 4/50
2024-12-09 03:57:36.682451: train_loss -0.5434
2024-12-09 03:57:36.683964: val_loss -0.3956
2024-12-09 03:57:36.685044: Pseudo dice [0.6724]
2024-12-09 03:57:36.685929: Epoch time: 86.11 s
2024-12-09 03:57:37.024350: Yayy! New best EMA pseudo Dice: 0.6597
2024-12-09 03:57:38.532067: 
2024-12-09 03:57:38.533744: Epoch 20
2024-12-09 03:57:38.534982: Current learning rate: 0.00982
2024-12-09 03:59:04.504731: Validation loss did not improve from -0.46154. Patience: 5/50
2024-12-09 03:59:04.506270: train_loss -0.5449
2024-12-09 03:59:04.507907: val_loss -0.4111
2024-12-09 03:59:04.508869: Pseudo dice [0.68]
2024-12-09 03:59:04.509718: Epoch time: 85.98 s
2024-12-09 03:59:04.510598: Yayy! New best EMA pseudo Dice: 0.6617
2024-12-09 03:59:06.071381: 
2024-12-09 03:59:06.073949: Epoch 21
2024-12-09 03:59:06.074947: Current learning rate: 0.00981
2024-12-09 04:00:32.077333: Validation loss did not improve from -0.46154. Patience: 6/50
2024-12-09 04:00:32.078747: train_loss -0.5422
2024-12-09 04:00:32.080249: val_loss -0.4604
2024-12-09 04:00:32.081333: Pseudo dice [0.7004]
2024-12-09 04:00:32.082380: Epoch time: 86.01 s
2024-12-09 04:00:32.083357: Yayy! New best EMA pseudo Dice: 0.6656
2024-12-09 04:00:33.545407: 
2024-12-09 04:00:33.547968: Epoch 22
2024-12-09 04:00:33.549316: Current learning rate: 0.0098
2024-12-09 04:01:59.623071: Validation loss improved from -0.46154 to -0.50383! Patience: 6/50
2024-12-09 04:01:59.624114: train_loss -0.5574
2024-12-09 04:01:59.625401: val_loss -0.5038
2024-12-09 04:01:59.626394: Pseudo dice [0.7133]
2024-12-09 04:01:59.627344: Epoch time: 86.08 s
2024-12-09 04:01:59.628165: Yayy! New best EMA pseudo Dice: 0.6704
2024-12-09 04:02:01.054984: 
2024-12-09 04:02:01.056429: Epoch 23
2024-12-09 04:02:01.057620: Current learning rate: 0.00979
2024-12-09 04:03:27.025855: Validation loss did not improve from -0.50383. Patience: 1/50
2024-12-09 04:03:27.027315: train_loss -0.5628
2024-12-09 04:03:27.028740: val_loss -0.4489
2024-12-09 04:03:27.029926: Pseudo dice [0.6997]
2024-12-09 04:03:27.030646: Epoch time: 85.97 s
2024-12-09 04:03:27.031727: Yayy! New best EMA pseudo Dice: 0.6733
2024-12-09 04:03:28.546787: 
2024-12-09 04:03:28.549489: Epoch 24
2024-12-09 04:03:28.551378: Current learning rate: 0.00978
2024-12-09 04:04:54.542673: Validation loss did not improve from -0.50383. Patience: 2/50
2024-12-09 04:04:54.544029: train_loss -0.5735
2024-12-09 04:04:54.545301: val_loss -0.4849
2024-12-09 04:04:54.546342: Pseudo dice [0.7166]
2024-12-09 04:04:54.547324: Epoch time: 86.0 s
2024-12-09 04:04:54.881705: Yayy! New best EMA pseudo Dice: 0.6776
2024-12-09 04:04:56.348293: 
2024-12-09 04:04:56.350548: Epoch 25
2024-12-09 04:04:56.351790: Current learning rate: 0.00977
2024-12-09 04:06:22.363567: Validation loss did not improve from -0.50383. Patience: 3/50
2024-12-09 04:06:22.364796: train_loss -0.5654
2024-12-09 04:06:22.365720: val_loss -0.4593
2024-12-09 04:06:22.366790: Pseudo dice [0.6994]
2024-12-09 04:06:22.367670: Epoch time: 86.02 s
2024-12-09 04:06:22.368598: Yayy! New best EMA pseudo Dice: 0.6798
2024-12-09 04:06:23.844121: 
2024-12-09 04:06:23.846644: Epoch 26
2024-12-09 04:06:23.848151: Current learning rate: 0.00977
2024-12-09 04:07:49.897825: Validation loss did not improve from -0.50383. Patience: 4/50
2024-12-09 04:07:49.899104: train_loss -0.5663
2024-12-09 04:07:49.900304: val_loss -0.4727
2024-12-09 04:07:49.901089: Pseudo dice [0.7039]
2024-12-09 04:07:49.902118: Epoch time: 86.06 s
2024-12-09 04:07:49.902955: Yayy! New best EMA pseudo Dice: 0.6822
2024-12-09 04:07:51.365699: 
2024-12-09 04:07:51.367672: Epoch 27
2024-12-09 04:07:51.368895: Current learning rate: 0.00976
2024-12-09 04:09:17.339960: Validation loss did not improve from -0.50383. Patience: 5/50
2024-12-09 04:09:17.340973: train_loss -0.575
2024-12-09 04:09:17.342352: val_loss -0.4974
2024-12-09 04:09:17.343495: Pseudo dice [0.7271]
2024-12-09 04:09:17.344723: Epoch time: 85.98 s
2024-12-09 04:09:17.345755: Yayy! New best EMA pseudo Dice: 0.6867
2024-12-09 04:09:18.820152: 
2024-12-09 04:09:18.822235: Epoch 28
2024-12-09 04:09:18.823902: Current learning rate: 0.00975
2024-12-09 04:10:44.814585: Validation loss improved from -0.50383 to -0.50809! Patience: 5/50
2024-12-09 04:10:44.815692: train_loss -0.5888
2024-12-09 04:10:44.816941: val_loss -0.5081
2024-12-09 04:10:44.817594: Pseudo dice [0.7198]
2024-12-09 04:10:44.818496: Epoch time: 86.0 s
2024-12-09 04:10:44.819257: Yayy! New best EMA pseudo Dice: 0.69
2024-12-09 04:10:46.288851: 
2024-12-09 04:10:46.290948: Epoch 29
2024-12-09 04:10:46.292348: Current learning rate: 0.00974
2024-12-09 04:12:12.390830: Validation loss did not improve from -0.50809. Patience: 1/50
2024-12-09 04:12:12.392125: train_loss -0.5848
2024-12-09 04:12:12.393746: val_loss -0.4194
2024-12-09 04:12:12.394636: Pseudo dice [0.6735]
2024-12-09 04:12:12.395442: Epoch time: 86.1 s
2024-12-09 04:12:14.210954: 
2024-12-09 04:12:14.213439: Epoch 30
2024-12-09 04:12:14.214478: Current learning rate: 0.00973
2024-12-09 04:13:40.194368: Validation loss did not improve from -0.50809. Patience: 2/50
2024-12-09 04:13:40.195619: train_loss -0.6001
2024-12-09 04:13:40.196734: val_loss -0.3873
2024-12-09 04:13:40.197789: Pseudo dice [0.6708]
2024-12-09 04:13:40.198685: Epoch time: 85.99 s
2024-12-09 04:13:41.383650: 
2024-12-09 04:13:41.385182: Epoch 31
2024-12-09 04:13:41.386133: Current learning rate: 0.00972
2024-12-09 04:15:07.398647: Validation loss did not improve from -0.50809. Patience: 3/50
2024-12-09 04:15:07.400127: train_loss -0.5947
2024-12-09 04:15:07.401472: val_loss -0.4647
2024-12-09 04:15:07.402630: Pseudo dice [0.6889]
2024-12-09 04:15:07.403535: Epoch time: 86.02 s
2024-12-09 04:15:08.553953: 
2024-12-09 04:15:08.556530: Epoch 32
2024-12-09 04:15:08.557837: Current learning rate: 0.00971
2024-12-09 04:16:34.660285: Validation loss did not improve from -0.50809. Patience: 4/50
2024-12-09 04:16:34.661483: train_loss -0.6044
2024-12-09 04:16:34.662679: val_loss -0.4581
2024-12-09 04:16:34.663487: Pseudo dice [0.7109]
2024-12-09 04:16:34.664539: Epoch time: 86.11 s
2024-12-09 04:16:35.842859: 
2024-12-09 04:16:35.845047: Epoch 33
2024-12-09 04:16:35.846261: Current learning rate: 0.0097
2024-12-09 04:18:01.810228: Validation loss did not improve from -0.50809. Patience: 5/50
2024-12-09 04:18:01.811109: train_loss -0.5943
2024-12-09 04:18:01.812414: val_loss -0.471
2024-12-09 04:18:01.813548: Pseudo dice [0.7036]
2024-12-09 04:18:01.814629: Epoch time: 85.97 s
2024-12-09 04:18:01.815499: Yayy! New best EMA pseudo Dice: 0.6907
2024-12-09 04:18:03.316004: 
2024-12-09 04:18:03.318107: Epoch 34
2024-12-09 04:18:03.319341: Current learning rate: 0.00969
2024-12-09 04:19:29.331708: Validation loss did not improve from -0.50809. Patience: 6/50
2024-12-09 04:19:29.332973: train_loss -0.5941
2024-12-09 04:19:29.334415: val_loss -0.4661
2024-12-09 04:19:29.335478: Pseudo dice [0.7048]
2024-12-09 04:19:29.336121: Epoch time: 86.02 s
2024-12-09 04:19:29.669461: Yayy! New best EMA pseudo Dice: 0.6921
2024-12-09 04:19:31.201225: 
2024-12-09 04:19:31.203299: Epoch 35
2024-12-09 04:19:31.204427: Current learning rate: 0.00968
2024-12-09 04:20:57.216919: Validation loss did not improve from -0.50809. Patience: 7/50
2024-12-09 04:20:57.218237: train_loss -0.5992
2024-12-09 04:20:57.219771: val_loss -0.4674
2024-12-09 04:20:57.220849: Pseudo dice [0.7015]
2024-12-09 04:20:57.221987: Epoch time: 86.02 s
2024-12-09 04:20:57.223376: Yayy! New best EMA pseudo Dice: 0.693
2024-12-09 04:20:58.723978: 
2024-12-09 04:20:58.726135: Epoch 36
2024-12-09 04:20:58.727281: Current learning rate: 0.00968
2024-12-09 04:22:24.747720: Validation loss did not improve from -0.50809. Patience: 8/50
2024-12-09 04:22:24.749479: train_loss -0.5993
2024-12-09 04:22:24.750884: val_loss -0.4535
2024-12-09 04:22:24.751906: Pseudo dice [0.7067]
2024-12-09 04:22:24.753222: Epoch time: 86.03 s
2024-12-09 04:22:24.754157: Yayy! New best EMA pseudo Dice: 0.6944
2024-12-09 04:22:26.256898: 
2024-12-09 04:22:26.258769: Epoch 37
2024-12-09 04:22:26.259921: Current learning rate: 0.00967
2024-12-09 04:23:52.241657: Validation loss did not improve from -0.50809. Patience: 9/50
2024-12-09 04:23:52.242668: train_loss -0.6063
2024-12-09 04:23:52.243893: val_loss -0.4677
2024-12-09 04:23:52.244732: Pseudo dice [0.7069]
2024-12-09 04:23:52.245708: Epoch time: 85.99 s
2024-12-09 04:23:52.246608: Yayy! New best EMA pseudo Dice: 0.6956
2024-12-09 04:23:53.766417: 
2024-12-09 04:23:53.768284: Epoch 38
2024-12-09 04:23:53.769207: Current learning rate: 0.00966
2024-12-09 04:25:19.761794: Validation loss did not improve from -0.50809. Patience: 10/50
2024-12-09 04:25:19.763298: train_loss -0.6202
2024-12-09 04:25:19.764774: val_loss -0.484
2024-12-09 04:25:19.765668: Pseudo dice [0.7124]
2024-12-09 04:25:19.766831: Epoch time: 86.0 s
2024-12-09 04:25:19.767940: Yayy! New best EMA pseudo Dice: 0.6973
2024-12-09 04:25:21.251263: 
2024-12-09 04:25:21.253036: Epoch 39
2024-12-09 04:25:21.254386: Current learning rate: 0.00965
2024-12-09 04:26:47.356609: Validation loss did not improve from -0.50809. Patience: 11/50
2024-12-09 04:26:47.357900: train_loss -0.6187
2024-12-09 04:26:47.358974: val_loss -0.4676
2024-12-09 04:26:47.360183: Pseudo dice [0.7121]
2024-12-09 04:26:47.361468: Epoch time: 86.11 s
2024-12-09 04:26:47.697145: Yayy! New best EMA pseudo Dice: 0.6988
2024-12-09 04:26:49.545610: 
2024-12-09 04:26:49.547648: Epoch 40
2024-12-09 04:26:49.548969: Current learning rate: 0.00964
2024-12-09 04:28:15.535072: Validation loss did not improve from -0.50809. Patience: 12/50
2024-12-09 04:28:15.536256: train_loss -0.6207
2024-12-09 04:28:15.537457: val_loss -0.4875
2024-12-09 04:28:15.538221: Pseudo dice [0.7232]
2024-12-09 04:28:15.539093: Epoch time: 85.99 s
2024-12-09 04:28:15.539889: Yayy! New best EMA pseudo Dice: 0.7012
2024-12-09 04:28:17.090468: 
2024-12-09 04:28:17.092604: Epoch 41
2024-12-09 04:28:17.093869: Current learning rate: 0.00963
2024-12-09 04:29:43.955591: Validation loss did not improve from -0.50809. Patience: 13/50
2024-12-09 04:29:43.956736: train_loss -0.6225
2024-12-09 04:29:43.958060: val_loss -0.456
2024-12-09 04:29:43.959639: Pseudo dice [0.6895]
2024-12-09 04:29:43.960695: Epoch time: 86.87 s
2024-12-09 04:29:45.103983: 
2024-12-09 04:29:45.105658: Epoch 42
2024-12-09 04:29:45.106994: Current learning rate: 0.00962
2024-12-09 04:31:11.173151: Validation loss did not improve from -0.50809. Patience: 14/50
2024-12-09 04:31:11.174703: train_loss -0.6159
2024-12-09 04:31:11.176040: val_loss -0.5022
2024-12-09 04:31:11.176849: Pseudo dice [0.7156]
2024-12-09 04:31:11.177696: Epoch time: 86.07 s
2024-12-09 04:31:11.178382: Yayy! New best EMA pseudo Dice: 0.7016
2024-12-09 04:31:12.654825: 
2024-12-09 04:31:12.657003: Epoch 43
2024-12-09 04:31:12.658022: Current learning rate: 0.00961
2024-12-09 04:32:38.679281: Validation loss improved from -0.50809 to -0.51853! Patience: 14/50
2024-12-09 04:32:38.680726: train_loss -0.6264
2024-12-09 04:32:38.682026: val_loss -0.5185
2024-12-09 04:32:38.683100: Pseudo dice [0.731]
2024-12-09 04:32:38.684126: Epoch time: 86.03 s
2024-12-09 04:32:38.685252: Yayy! New best EMA pseudo Dice: 0.7046
2024-12-09 04:32:40.225041: 
2024-12-09 04:32:40.249933: Epoch 44
2024-12-09 04:32:40.251406: Current learning rate: 0.0096
2024-12-09 04:34:06.377321: Validation loss did not improve from -0.51853. Patience: 1/50
2024-12-09 04:34:06.380843: train_loss -0.626
2024-12-09 04:34:06.385197: val_loss -0.5109
2024-12-09 04:34:06.386203: Pseudo dice [0.7229]
2024-12-09 04:34:06.387842: Epoch time: 86.16 s
2024-12-09 04:34:06.715505: Yayy! New best EMA pseudo Dice: 0.7064
2024-12-09 04:34:08.243808: 
2024-12-09 04:34:08.246419: Epoch 45
2024-12-09 04:34:08.247458: Current learning rate: 0.00959
2024-12-09 04:35:34.358209: Validation loss did not improve from -0.51853. Patience: 2/50
2024-12-09 04:35:34.359548: train_loss -0.6303
2024-12-09 04:35:34.360646: val_loss -0.4851
2024-12-09 04:35:34.361294: Pseudo dice [0.7194]
2024-12-09 04:35:34.361951: Epoch time: 86.12 s
2024-12-09 04:35:34.362800: Yayy! New best EMA pseudo Dice: 0.7077
2024-12-09 04:35:35.912768: 
2024-12-09 04:35:35.915484: Epoch 46
2024-12-09 04:35:35.917166: Current learning rate: 0.00959
2024-12-09 04:37:02.023442: Validation loss did not improve from -0.51853. Patience: 3/50
2024-12-09 04:37:02.026047: train_loss -0.6336
2024-12-09 04:37:02.027595: val_loss -0.4679
2024-12-09 04:37:02.028371: Pseudo dice [0.6939]
2024-12-09 04:37:02.029310: Epoch time: 86.11 s
2024-12-09 04:37:03.239576: 
2024-12-09 04:37:03.241391: Epoch 47
2024-12-09 04:37:03.242558: Current learning rate: 0.00958
2024-12-09 04:38:29.249768: Validation loss improved from -0.51853 to -0.52010! Patience: 3/50
2024-12-09 04:38:29.252651: train_loss -0.6312
2024-12-09 04:38:29.254407: val_loss -0.5201
2024-12-09 04:38:29.255647: Pseudo dice [0.7263]
2024-12-09 04:38:29.257288: Epoch time: 86.01 s
2024-12-09 04:38:29.258229: Yayy! New best EMA pseudo Dice: 0.7083
2024-12-09 04:38:30.707274: 
2024-12-09 04:38:30.710035: Epoch 48
2024-12-09 04:38:30.711086: Current learning rate: 0.00957
2024-12-09 04:39:56.736059: Validation loss did not improve from -0.52010. Patience: 1/50
2024-12-09 04:39:56.737399: train_loss -0.6468
2024-12-09 04:39:56.738552: val_loss -0.4802
2024-12-09 04:39:56.739486: Pseudo dice [0.7013]
2024-12-09 04:39:56.740538: Epoch time: 86.03 s
2024-12-09 04:39:57.910111: 
2024-12-09 04:39:57.912338: Epoch 49
2024-12-09 04:39:57.913415: Current learning rate: 0.00956
2024-12-09 04:41:24.098878: Validation loss did not improve from -0.52010. Patience: 2/50
2024-12-09 04:41:24.100248: train_loss -0.642
2024-12-09 04:41:24.101182: val_loss -0.4947
2024-12-09 04:41:24.102392: Pseudo dice [0.7207]
2024-12-09 04:41:24.103530: Epoch time: 86.19 s
2024-12-09 04:41:24.475934: Yayy! New best EMA pseudo Dice: 0.7089
2024-12-09 04:41:26.060054: 
2024-12-09 04:41:26.062457: Epoch 50
2024-12-09 04:41:26.063969: Current learning rate: 0.00955
2024-12-09 04:42:52.235769: Validation loss did not improve from -0.52010. Patience: 3/50
2024-12-09 04:42:52.237082: train_loss -0.6427
2024-12-09 04:42:52.238000: val_loss -0.4358
2024-12-09 04:42:52.239046: Pseudo dice [0.6966]
2024-12-09 04:42:52.239704: Epoch time: 86.18 s
2024-12-09 04:42:53.825581: 
2024-12-09 04:42:53.827840: Epoch 51
2024-12-09 04:42:53.828774: Current learning rate: 0.00954
2024-12-09 04:44:19.857849: Validation loss did not improve from -0.52010. Patience: 4/50
2024-12-09 04:44:19.859307: train_loss -0.6416
2024-12-09 04:44:19.860579: val_loss -0.4741
2024-12-09 04:44:19.861626: Pseudo dice [0.7187]
2024-12-09 04:44:19.862633: Epoch time: 86.03 s
2024-12-09 04:44:21.015318: 
2024-12-09 04:44:21.017281: Epoch 52
2024-12-09 04:44:21.018409: Current learning rate: 0.00953
2024-12-09 04:45:47.085052: Validation loss did not improve from -0.52010. Patience: 5/50
2024-12-09 04:45:47.086327: train_loss -0.6484
2024-12-09 04:45:47.087781: val_loss -0.5001
2024-12-09 04:45:47.088793: Pseudo dice [0.7117]
2024-12-09 04:45:47.089966: Epoch time: 86.07 s
2024-12-09 04:45:47.090948: Yayy! New best EMA pseudo Dice: 0.7091
2024-12-09 04:45:48.566302: 
2024-12-09 04:45:48.568226: Epoch 53
2024-12-09 04:45:48.569224: Current learning rate: 0.00952
2024-12-09 04:47:14.554840: Validation loss did not improve from -0.52010. Patience: 6/50
2024-12-09 04:47:14.555994: train_loss -0.6425
2024-12-09 04:47:14.557259: val_loss -0.4102
2024-12-09 04:47:14.558252: Pseudo dice [0.6908]
2024-12-09 04:47:14.559301: Epoch time: 85.99 s
2024-12-09 04:47:15.707856: 
2024-12-09 04:47:15.710057: Epoch 54
2024-12-09 04:47:15.710998: Current learning rate: 0.00951
2024-12-09 04:48:41.692616: Validation loss did not improve from -0.52010. Patience: 7/50
2024-12-09 04:48:41.693960: train_loss -0.6464
2024-12-09 04:48:41.695626: val_loss -0.4644
2024-12-09 04:48:41.696730: Pseudo dice [0.7003]
2024-12-09 04:48:41.697792: Epoch time: 85.99 s
2024-12-09 04:48:43.163971: 
2024-12-09 04:48:43.166153: Epoch 55
2024-12-09 04:48:43.167045: Current learning rate: 0.0095
2024-12-09 04:50:09.141981: Validation loss did not improve from -0.52010. Patience: 8/50
2024-12-09 04:50:09.142979: train_loss -0.6402
2024-12-09 04:50:09.144142: val_loss -0.5155
2024-12-09 04:50:09.145116: Pseudo dice [0.7184]
2024-12-09 04:50:09.146044: Epoch time: 85.98 s
2024-12-09 04:50:10.301455: 
2024-12-09 04:50:10.302858: Epoch 56
2024-12-09 04:50:10.303589: Current learning rate: 0.00949
2024-12-09 04:51:36.328063: Validation loss did not improve from -0.52010. Patience: 9/50
2024-12-09 04:51:36.329628: train_loss -0.6443
2024-12-09 04:51:36.331097: val_loss -0.4997
2024-12-09 04:51:36.332323: Pseudo dice [0.7215]
2024-12-09 04:51:36.333558: Epoch time: 86.03 s
2024-12-09 04:51:36.334413: Yayy! New best EMA pseudo Dice: 0.7091
2024-12-09 04:51:37.813308: 
2024-12-09 04:51:37.815452: Epoch 57
2024-12-09 04:51:37.816726: Current learning rate: 0.00949
2024-12-09 04:53:03.787917: Validation loss did not improve from -0.52010. Patience: 10/50
2024-12-09 04:53:03.788903: train_loss -0.6547
2024-12-09 04:53:03.790266: val_loss -0.4783
2024-12-09 04:53:03.791317: Pseudo dice [0.703]
2024-12-09 04:53:03.792449: Epoch time: 85.98 s
2024-12-09 04:53:04.943350: 
2024-12-09 04:53:04.945610: Epoch 58
2024-12-09 04:53:04.947038: Current learning rate: 0.00948
2024-12-09 04:54:30.940271: Validation loss did not improve from -0.52010. Patience: 11/50
2024-12-09 04:54:30.941713: train_loss -0.6505
2024-12-09 04:54:30.942945: val_loss -0.4821
2024-12-09 04:54:30.943949: Pseudo dice [0.7152]
2024-12-09 04:54:30.944980: Epoch time: 86.0 s
2024-12-09 04:54:30.945920: Yayy! New best EMA pseudo Dice: 0.7092
2024-12-09 04:54:32.457570: 
2024-12-09 04:54:32.459884: Epoch 59
2024-12-09 04:54:32.461038: Current learning rate: 0.00947
2024-12-09 04:55:58.552574: Validation loss improved from -0.52010 to -0.53046! Patience: 11/50
2024-12-09 04:55:58.554116: train_loss -0.6557
2024-12-09 04:55:58.555529: val_loss -0.5305
2024-12-09 04:55:58.556526: Pseudo dice [0.7339]
2024-12-09 04:55:58.557607: Epoch time: 86.1 s
2024-12-09 04:55:58.906667: Yayy! New best EMA pseudo Dice: 0.7117
2024-12-09 04:56:00.410589: 
2024-12-09 04:56:00.412924: Epoch 60
2024-12-09 04:56:00.414259: Current learning rate: 0.00946
2024-12-09 04:57:26.407274: Validation loss did not improve from -0.53046. Patience: 1/50
2024-12-09 04:57:26.408556: train_loss -0.6468
2024-12-09 04:57:26.409562: val_loss -0.4953
2024-12-09 04:57:26.410412: Pseudo dice [0.7237]
2024-12-09 04:57:26.411236: Epoch time: 86.0 s
2024-12-09 04:57:26.412083: Yayy! New best EMA pseudo Dice: 0.7129
2024-12-09 04:57:27.936927: 
2024-12-09 04:57:27.938811: Epoch 61
2024-12-09 04:57:27.939787: Current learning rate: 0.00945
2024-12-09 04:58:53.931884: Validation loss did not improve from -0.53046. Patience: 2/50
2024-12-09 04:58:53.933200: train_loss -0.6603
2024-12-09 04:58:53.934594: val_loss -0.5237
2024-12-09 04:58:53.935509: Pseudo dice [0.732]
2024-12-09 04:58:53.936248: Epoch time: 86.0 s
2024-12-09 04:58:53.936967: Yayy! New best EMA pseudo Dice: 0.7148
2024-12-09 04:58:55.747284: 
2024-12-09 04:58:55.749143: Epoch 62
2024-12-09 04:58:55.750245: Current learning rate: 0.00944
2024-12-09 05:00:21.802021: Validation loss did not improve from -0.53046. Patience: 3/50
2024-12-09 05:00:21.803190: train_loss -0.658
2024-12-09 05:00:21.804181: val_loss -0.5082
2024-12-09 05:00:21.804980: Pseudo dice [0.731]
2024-12-09 05:00:21.805902: Epoch time: 86.06 s
2024-12-09 05:00:21.806659: Yayy! New best EMA pseudo Dice: 0.7164
2024-12-09 05:00:23.324260: 
2024-12-09 05:00:23.326580: Epoch 63
2024-12-09 05:00:23.327585: Current learning rate: 0.00943
2024-12-09 05:01:49.315272: Validation loss did not improve from -0.53046. Patience: 4/50
2024-12-09 05:01:49.316819: train_loss -0.6674
2024-12-09 05:01:49.318264: val_loss -0.4705
2024-12-09 05:01:49.319031: Pseudo dice [0.7012]
2024-12-09 05:01:49.319864: Epoch time: 85.99 s
2024-12-09 05:01:50.484177: 
2024-12-09 05:01:50.485926: Epoch 64
2024-12-09 05:01:50.487003: Current learning rate: 0.00942
2024-12-09 05:03:16.440515: Validation loss did not improve from -0.53046. Patience: 5/50
2024-12-09 05:03:16.442214: train_loss -0.6684
2024-12-09 05:03:16.443843: val_loss -0.4786
2024-12-09 05:03:16.444886: Pseudo dice [0.7196]
2024-12-09 05:03:16.445713: Epoch time: 85.96 s
2024-12-09 05:03:17.982975: 
2024-12-09 05:03:17.985125: Epoch 65
2024-12-09 05:03:17.986079: Current learning rate: 0.00941
2024-12-09 05:04:44.034803: Validation loss improved from -0.53046 to -0.53281! Patience: 5/50
2024-12-09 05:04:44.036187: train_loss -0.6625
2024-12-09 05:04:44.037017: val_loss -0.5328
2024-12-09 05:04:44.037671: Pseudo dice [0.7349]
2024-12-09 05:04:44.038612: Epoch time: 86.05 s
2024-12-09 05:04:44.039185: Yayy! New best EMA pseudo Dice: 0.7173
2024-12-09 05:04:45.642700: 
2024-12-09 05:04:45.644165: Epoch 66
2024-12-09 05:04:45.645361: Current learning rate: 0.0094
2024-12-09 05:06:11.704661: Validation loss did not improve from -0.53281. Patience: 1/50
2024-12-09 05:06:11.707554: train_loss -0.6656
2024-12-09 05:06:11.709221: val_loss -0.5121
2024-12-09 05:06:11.710108: Pseudo dice [0.7347]
2024-12-09 05:06:11.711471: Epoch time: 86.07 s
2024-12-09 05:06:11.712326: Yayy! New best EMA pseudo Dice: 0.719
2024-12-09 05:06:13.308912: 
2024-12-09 05:06:13.311246: Epoch 67
2024-12-09 05:06:13.312421: Current learning rate: 0.00939
2024-12-09 05:07:39.331889: Validation loss did not improve from -0.53281. Patience: 2/50
2024-12-09 05:07:39.333220: train_loss -0.6734
2024-12-09 05:07:39.334306: val_loss -0.4846
2024-12-09 05:07:39.335286: Pseudo dice [0.7202]
2024-12-09 05:07:39.336380: Epoch time: 86.03 s
2024-12-09 05:07:39.337270: Yayy! New best EMA pseudo Dice: 0.7192
2024-12-09 05:07:40.931110: 
2024-12-09 05:07:40.933925: Epoch 68
2024-12-09 05:07:40.935549: Current learning rate: 0.00939
2024-12-09 05:09:06.943121: Validation loss did not improve from -0.53281. Patience: 3/50
2024-12-09 05:09:06.945056: train_loss -0.6798
2024-12-09 05:09:06.946643: val_loss -0.5097
2024-12-09 05:09:06.947527: Pseudo dice [0.7318]
2024-12-09 05:09:06.948673: Epoch time: 86.02 s
2024-12-09 05:09:06.949601: Yayy! New best EMA pseudo Dice: 0.7204
2024-12-09 05:09:08.540038: 
2024-12-09 05:09:08.541637: Epoch 69
2024-12-09 05:09:08.542972: Current learning rate: 0.00938
2024-12-09 05:10:34.671219: Validation loss improved from -0.53281 to -0.53876! Patience: 3/50
2024-12-09 05:10:34.672528: train_loss -0.6751
2024-12-09 05:10:34.673805: val_loss -0.5388
2024-12-09 05:10:34.675113: Pseudo dice [0.7441]
2024-12-09 05:10:34.676227: Epoch time: 86.13 s
2024-12-09 05:10:35.024314: Yayy! New best EMA pseudo Dice: 0.7228
2024-12-09 05:10:36.560048: 
2024-12-09 05:10:36.561926: Epoch 70
2024-12-09 05:10:36.563076: Current learning rate: 0.00937
2024-12-09 05:12:02.666195: Validation loss did not improve from -0.53876. Patience: 1/50
2024-12-09 05:12:02.667310: train_loss -0.6854
2024-12-09 05:12:02.668186: val_loss -0.4905
2024-12-09 05:12:02.668930: Pseudo dice [0.72]
2024-12-09 05:12:02.669783: Epoch time: 86.11 s
2024-12-09 05:12:03.860833: 
2024-12-09 05:12:03.862734: Epoch 71
2024-12-09 05:12:03.863971: Current learning rate: 0.00936
2024-12-09 05:13:29.912126: Validation loss did not improve from -0.53876. Patience: 2/50
2024-12-09 05:13:29.913413: train_loss -0.6695
2024-12-09 05:13:29.914884: val_loss -0.4677
2024-12-09 05:13:29.915992: Pseudo dice [0.7249]
2024-12-09 05:13:29.916953: Epoch time: 86.05 s
2024-12-09 05:13:31.448941: 
2024-12-09 05:13:31.451012: Epoch 72
2024-12-09 05:13:31.452478: Current learning rate: 0.00935
2024-12-09 05:14:57.514247: Validation loss did not improve from -0.53876. Patience: 3/50
2024-12-09 05:14:57.515898: train_loss -0.6759
2024-12-09 05:14:57.517496: val_loss -0.4626
2024-12-09 05:14:57.518595: Pseudo dice [0.7087]
2024-12-09 05:14:57.519898: Epoch time: 86.07 s
2024-12-09 05:14:58.792224: 
2024-12-09 05:14:58.794672: Epoch 73
2024-12-09 05:14:58.796118: Current learning rate: 0.00934
2024-12-09 05:16:24.780234: Validation loss did not improve from -0.53876. Patience: 4/50
2024-12-09 05:16:24.781545: train_loss -0.6762
2024-12-09 05:16:24.782813: val_loss -0.5156
2024-12-09 05:16:24.783761: Pseudo dice [0.7281]
2024-12-09 05:16:24.784658: Epoch time: 85.99 s
2024-12-09 05:16:25.996837: 
2024-12-09 05:16:25.998844: Epoch 74
2024-12-09 05:16:26.000011: Current learning rate: 0.00933
2024-12-09 05:17:51.987096: Validation loss did not improve from -0.53876. Patience: 5/50
2024-12-09 05:17:51.988822: train_loss -0.6727
2024-12-09 05:17:51.990444: val_loss -0.504
2024-12-09 05:17:51.991276: Pseudo dice [0.7212]
2024-12-09 05:17:51.992013: Epoch time: 85.99 s
2024-12-09 05:17:53.523034: 
2024-12-09 05:17:53.524409: Epoch 75
2024-12-09 05:17:53.525199: Current learning rate: 0.00932
2024-12-09 05:19:19.540512: Validation loss did not improve from -0.53876. Patience: 6/50
2024-12-09 05:19:19.541641: train_loss -0.6788
2024-12-09 05:19:19.542722: val_loss -0.5211
2024-12-09 05:19:19.543433: Pseudo dice [0.7301]
2024-12-09 05:19:19.544303: Epoch time: 86.02 s
2024-12-09 05:19:20.760813: 
2024-12-09 05:19:20.762733: Epoch 76
2024-12-09 05:19:20.763622: Current learning rate: 0.00931
2024-12-09 05:20:46.805716: Validation loss did not improve from -0.53876. Patience: 7/50
2024-12-09 05:20:46.807173: train_loss -0.6793
2024-12-09 05:20:46.808341: val_loss -0.49
2024-12-09 05:20:46.809543: Pseudo dice [0.716]
2024-12-09 05:20:46.810369: Epoch time: 86.05 s
2024-12-09 05:20:47.992145: 
2024-12-09 05:20:47.995087: Epoch 77
2024-12-09 05:20:47.996598: Current learning rate: 0.0093
2024-12-09 05:22:13.993449: Validation loss did not improve from -0.53876. Patience: 8/50
2024-12-09 05:22:13.994512: train_loss -0.6799
2024-12-09 05:22:13.995715: val_loss -0.4905
2024-12-09 05:22:13.996989: Pseudo dice [0.7247]
2024-12-09 05:22:13.997999: Epoch time: 86.0 s
2024-12-09 05:22:15.251808: 
2024-12-09 05:22:15.253796: Epoch 78
2024-12-09 05:22:15.255004: Current learning rate: 0.0093
2024-12-09 05:23:41.284642: Validation loss did not improve from -0.53876. Patience: 9/50
2024-12-09 05:23:41.285859: train_loss -0.6863
2024-12-09 05:23:41.287242: val_loss -0.5079
2024-12-09 05:23:41.288167: Pseudo dice [0.7252]
2024-12-09 05:23:41.289028: Epoch time: 86.04 s
2024-12-09 05:23:42.488799: 
2024-12-09 05:23:42.490849: Epoch 79
2024-12-09 05:23:42.491734: Current learning rate: 0.00929
2024-12-09 05:25:08.597971: Validation loss did not improve from -0.53876. Patience: 10/50
2024-12-09 05:25:08.598964: train_loss -0.687
2024-12-09 05:25:08.600203: val_loss -0.4729
2024-12-09 05:25:08.601179: Pseudo dice [0.7192]
2024-12-09 05:25:08.602066: Epoch time: 86.11 s
2024-12-09 05:25:10.182036: 
2024-12-09 05:25:10.184223: Epoch 80
2024-12-09 05:25:10.185659: Current learning rate: 0.00928
2024-12-09 05:26:36.179489: Validation loss did not improve from -0.53876. Patience: 11/50
2024-12-09 05:26:36.180826: train_loss -0.6804
2024-12-09 05:26:36.181836: val_loss -0.5224
2024-12-09 05:26:36.182730: Pseudo dice [0.7405]
2024-12-09 05:26:36.183586: Epoch time: 86.0 s
2024-12-09 05:26:36.184568: Yayy! New best EMA pseudo Dice: 0.7241
2024-12-09 05:26:37.777842: 
2024-12-09 05:26:37.780011: Epoch 81
2024-12-09 05:26:37.781159: Current learning rate: 0.00927
2024-12-09 05:28:03.791554: Validation loss did not improve from -0.53876. Patience: 12/50
2024-12-09 05:28:03.792815: train_loss -0.6809
2024-12-09 05:28:03.794240: val_loss -0.512
2024-12-09 05:28:03.795353: Pseudo dice [0.7332]
2024-12-09 05:28:03.796308: Epoch time: 86.02 s
2024-12-09 05:28:03.797331: Yayy! New best EMA pseudo Dice: 0.725
2024-12-09 05:28:05.391212: 
2024-12-09 05:28:05.393367: Epoch 82
2024-12-09 05:28:05.394486: Current learning rate: 0.00926
2024-12-09 05:29:31.428839: Validation loss did not improve from -0.53876. Patience: 13/50
2024-12-09 05:29:31.430036: train_loss -0.6903
2024-12-09 05:29:31.431176: val_loss -0.5022
2024-12-09 05:29:31.432107: Pseudo dice [0.7192]
2024-12-09 05:29:31.433015: Epoch time: 86.04 s
2024-12-09 05:29:32.869463: 
2024-12-09 05:29:32.871285: Epoch 83
2024-12-09 05:29:32.872621: Current learning rate: 0.00925
2024-12-09 05:30:58.868639: Validation loss did not improve from -0.53876. Patience: 14/50
2024-12-09 05:30:58.869961: train_loss -0.6909
2024-12-09 05:30:58.871195: val_loss -0.5054
2024-12-09 05:30:58.872196: Pseudo dice [0.7192]
2024-12-09 05:30:58.873149: Epoch time: 86.0 s
2024-12-09 05:30:59.976066: 
2024-12-09 05:30:59.978065: Epoch 84
2024-12-09 05:30:59.979121: Current learning rate: 0.00924
2024-12-09 05:32:25.951309: Validation loss did not improve from -0.53876. Patience: 15/50
2024-12-09 05:32:25.952443: train_loss -0.6938
2024-12-09 05:32:25.953317: val_loss -0.5109
2024-12-09 05:32:25.954175: Pseudo dice [0.7364]
2024-12-09 05:32:25.954931: Epoch time: 85.98 s
2024-12-09 05:32:26.300058: Yayy! New best EMA pseudo Dice: 0.7252
2024-12-09 05:32:27.776236: 
2024-12-09 05:32:27.778618: Epoch 85
2024-12-09 05:32:27.779826: Current learning rate: 0.00923
2024-12-09 05:33:53.794939: Validation loss did not improve from -0.53876. Patience: 16/50
2024-12-09 05:33:53.796534: train_loss -0.6798
2024-12-09 05:33:53.797966: val_loss -0.5325
2024-12-09 05:33:53.798695: Pseudo dice [0.7338]
2024-12-09 05:33:53.799554: Epoch time: 86.02 s
2024-12-09 05:33:53.800473: Yayy! New best EMA pseudo Dice: 0.726
2024-12-09 05:33:55.279131: 
2024-12-09 05:33:55.281094: Epoch 86
2024-12-09 05:33:55.282078: Current learning rate: 0.00922
2024-12-09 05:35:21.529387: Validation loss did not improve from -0.53876. Patience: 17/50
2024-12-09 05:35:21.535722: train_loss -0.6815
2024-12-09 05:35:21.537743: val_loss -0.5062
2024-12-09 05:35:21.538890: Pseudo dice [0.7309]
2024-12-09 05:35:21.540089: Epoch time: 86.26 s
2024-12-09 05:35:21.541686: Yayy! New best EMA pseudo Dice: 0.7265
2024-12-09 05:35:23.110954: 
2024-12-09 05:35:23.113122: Epoch 87
2024-12-09 05:35:23.113904: Current learning rate: 0.00921
2024-12-09 05:36:49.093277: Validation loss did not improve from -0.53876. Patience: 18/50
2024-12-09 05:36:49.095022: train_loss -0.698
2024-12-09 05:36:49.096282: val_loss -0.5165
2024-12-09 05:36:49.097574: Pseudo dice [0.7277]
2024-12-09 05:36:49.098438: Epoch time: 85.98 s
2024-12-09 05:36:49.099268: Yayy! New best EMA pseudo Dice: 0.7266
2024-12-09 05:36:50.573114: 
2024-12-09 05:36:50.575475: Epoch 88
2024-12-09 05:36:50.576213: Current learning rate: 0.0092
2024-12-09 05:38:16.681486: Validation loss did not improve from -0.53876. Patience: 19/50
2024-12-09 05:38:16.684469: train_loss -0.6842
2024-12-09 05:38:16.686302: val_loss -0.4722
2024-12-09 05:38:16.687549: Pseudo dice [0.7119]
2024-12-09 05:38:16.688266: Epoch time: 86.11 s
2024-12-09 05:38:17.867060: 
2024-12-09 05:38:17.869143: Epoch 89
2024-12-09 05:38:17.870277: Current learning rate: 0.0092
2024-12-09 05:39:44.010043: Validation loss did not improve from -0.53876. Patience: 20/50
2024-12-09 05:39:44.011174: train_loss -0.6895
2024-12-09 05:39:44.013908: val_loss -0.5104
2024-12-09 05:39:44.014892: Pseudo dice [0.7277]
2024-12-09 05:39:44.016099: Epoch time: 86.14 s
2024-12-09 05:39:45.510086: 
2024-12-09 05:39:45.511978: Epoch 90
2024-12-09 05:39:45.512948: Current learning rate: 0.00919
2024-12-09 05:41:11.486726: Validation loss did not improve from -0.53876. Patience: 21/50
2024-12-09 05:41:11.488192: train_loss -0.6898
2024-12-09 05:41:11.489503: val_loss -0.5228
2024-12-09 05:41:11.490860: Pseudo dice [0.7355]
2024-12-09 05:41:11.491983: Epoch time: 85.98 s
2024-12-09 05:41:12.650280: 
2024-12-09 05:41:12.652080: Epoch 91
2024-12-09 05:41:12.653198: Current learning rate: 0.00918
2024-12-09 05:42:38.663784: Validation loss did not improve from -0.53876. Patience: 22/50
2024-12-09 05:42:38.665030: train_loss -0.7034
2024-12-09 05:42:38.666262: val_loss -0.5031
2024-12-09 05:42:38.667383: Pseudo dice [0.7222]
2024-12-09 05:42:38.668257: Epoch time: 86.02 s
2024-12-09 05:42:39.816519: 
2024-12-09 05:42:39.818200: Epoch 92
2024-12-09 05:42:39.819622: Current learning rate: 0.00917
2024-12-09 05:44:05.820512: Validation loss did not improve from -0.53876. Patience: 23/50
2024-12-09 05:44:05.821631: train_loss -0.7013
2024-12-09 05:44:05.822412: val_loss -0.5162
2024-12-09 05:44:05.823005: Pseudo dice [0.7327]
2024-12-09 05:44:05.824077: Epoch time: 86.01 s
2024-12-09 05:44:05.824886: Yayy! New best EMA pseudo Dice: 0.7267
2024-12-09 05:44:07.309049: 
2024-12-09 05:44:07.310961: Epoch 93
2024-12-09 05:44:07.311761: Current learning rate: 0.00916
2024-12-09 05:45:33.311386: Validation loss did not improve from -0.53876. Patience: 24/50
2024-12-09 05:45:33.312545: train_loss -0.6944
2024-12-09 05:45:33.313937: val_loss -0.4983
2024-12-09 05:45:33.314990: Pseudo dice [0.727]
2024-12-09 05:45:33.315799: Epoch time: 86.0 s
2024-12-09 05:45:33.316855: Yayy! New best EMA pseudo Dice: 0.7267
2024-12-09 05:45:35.257926: 
2024-12-09 05:45:35.259757: Epoch 94
2024-12-09 05:45:35.261103: Current learning rate: 0.00915
2024-12-09 05:47:01.269259: Validation loss did not improve from -0.53876. Patience: 25/50
2024-12-09 05:47:01.270559: train_loss -0.7013
2024-12-09 05:47:01.271787: val_loss -0.4919
2024-12-09 05:47:01.272499: Pseudo dice [0.7287]
2024-12-09 05:47:01.273140: Epoch time: 86.01 s
2024-12-09 05:47:01.640183: Yayy! New best EMA pseudo Dice: 0.7269
2024-12-09 05:47:03.102447: 
2024-12-09 05:47:03.104745: Epoch 95
2024-12-09 05:47:03.105936: Current learning rate: 0.00914
2024-12-09 05:48:29.205731: Validation loss did not improve from -0.53876. Patience: 26/50
2024-12-09 05:48:29.209014: train_loss -0.705
2024-12-09 05:48:29.210259: val_loss -0.475
2024-12-09 05:48:29.211094: Pseudo dice [0.7177]
2024-12-09 05:48:29.211905: Epoch time: 86.11 s
2024-12-09 05:48:30.368734: 
2024-12-09 05:48:30.370884: Epoch 96
2024-12-09 05:48:30.372162: Current learning rate: 0.00913
2024-12-09 05:49:56.465667: Validation loss did not improve from -0.53876. Patience: 27/50
2024-12-09 05:49:56.467173: train_loss -0.7017
2024-12-09 05:49:56.468266: val_loss -0.4906
2024-12-09 05:49:56.469106: Pseudo dice [0.7158]
2024-12-09 05:49:56.469920: Epoch time: 86.1 s
2024-12-09 05:49:57.605189: 
2024-12-09 05:49:57.607319: Epoch 97
2024-12-09 05:49:57.608167: Current learning rate: 0.00912
2024-12-09 05:51:23.623491: Validation loss did not improve from -0.53876. Patience: 28/50
2024-12-09 05:51:23.624804: train_loss -0.7095
2024-12-09 05:51:23.626046: val_loss -0.4945
2024-12-09 05:51:23.626841: Pseudo dice [0.7192]
2024-12-09 05:51:23.627600: Epoch time: 86.02 s
2024-12-09 05:51:24.785074: 
2024-12-09 05:51:24.786952: Epoch 98
2024-12-09 05:51:24.788188: Current learning rate: 0.00911
2024-12-09 05:52:50.856153: Validation loss did not improve from -0.53876. Patience: 29/50
2024-12-09 05:52:50.857812: train_loss -0.7077
2024-12-09 05:52:50.858944: val_loss -0.5006
2024-12-09 05:52:50.859972: Pseudo dice [0.7123]
2024-12-09 05:52:50.861058: Epoch time: 86.07 s
2024-12-09 05:52:52.025148: 
2024-12-09 05:52:52.027129: Epoch 99
2024-12-09 05:52:52.028068: Current learning rate: 0.0091
2024-12-09 05:54:18.143235: Validation loss did not improve from -0.53876. Patience: 30/50
2024-12-09 05:54:18.144654: train_loss -0.7119
2024-12-09 05:54:18.146344: val_loss -0.5016
2024-12-09 05:54:18.147928: Pseudo dice [0.7255]
2024-12-09 05:54:18.149263: Epoch time: 86.12 s
2024-12-09 05:54:19.666847: 
2024-12-09 05:54:19.669344: Epoch 100
2024-12-09 05:54:19.670876: Current learning rate: 0.0091
2024-12-09 05:55:45.675281: Validation loss did not improve from -0.53876. Patience: 31/50
2024-12-09 05:55:45.676341: train_loss -0.7094
2024-12-09 05:55:45.677397: val_loss -0.4967
2024-12-09 05:55:45.678306: Pseudo dice [0.7288]
2024-12-09 05:55:45.679003: Epoch time: 86.01 s
2024-12-09 05:55:46.858253: 
2024-12-09 05:55:46.860471: Epoch 101
2024-12-09 05:55:46.861401: Current learning rate: 0.00909
2024-12-09 05:57:12.864243: Validation loss did not improve from -0.53876. Patience: 32/50
2024-12-09 05:57:12.865547: train_loss -0.7084
2024-12-09 05:57:12.866571: val_loss -0.5049
2024-12-09 05:57:12.867580: Pseudo dice [0.7328]
2024-12-09 05:57:12.868610: Epoch time: 86.01 s
2024-12-09 05:57:14.044402: 
2024-12-09 05:57:14.046194: Epoch 102
2024-12-09 05:57:14.047449: Current learning rate: 0.00908
2024-12-09 05:58:40.053289: Validation loss did not improve from -0.53876. Patience: 33/50
2024-12-09 05:58:40.054449: train_loss -0.7129
2024-12-09 05:58:40.055902: val_loss -0.4595
2024-12-09 05:58:40.056989: Pseudo dice [0.716]
2024-12-09 05:58:40.057980: Epoch time: 86.01 s
2024-12-09 05:58:41.232029: 
2024-12-09 05:58:41.234253: Epoch 103
2024-12-09 05:58:41.235811: Current learning rate: 0.00907
2024-12-09 06:00:07.284414: Validation loss did not improve from -0.53876. Patience: 34/50
2024-12-09 06:00:07.285568: train_loss -0.7123
2024-12-09 06:00:07.286378: val_loss -0.4933
2024-12-09 06:00:07.287317: Pseudo dice [0.7203]
2024-12-09 06:00:07.287962: Epoch time: 86.05 s
2024-12-09 06:00:08.803204: 
2024-12-09 06:00:08.805383: Epoch 104
2024-12-09 06:00:08.806229: Current learning rate: 0.00906
2024-12-09 06:01:34.837167: Validation loss did not improve from -0.53876. Patience: 35/50
2024-12-09 06:01:34.838458: train_loss -0.7124
2024-12-09 06:01:34.840237: val_loss -0.4689
2024-12-09 06:01:34.841020: Pseudo dice [0.7128]
2024-12-09 06:01:34.841771: Epoch time: 86.04 s
2024-12-09 06:01:36.331335: 
2024-12-09 06:01:36.333501: Epoch 105
2024-12-09 06:01:36.334927: Current learning rate: 0.00905
2024-12-09 06:03:02.359133: Validation loss did not improve from -0.53876. Patience: 36/50
2024-12-09 06:03:02.360070: train_loss -0.7131
2024-12-09 06:03:02.361411: val_loss -0.5081
2024-12-09 06:03:02.362420: Pseudo dice [0.7264]
2024-12-09 06:03:02.363423: Epoch time: 86.03 s
2024-12-09 06:03:03.485259: 
2024-12-09 06:03:03.487360: Epoch 106
2024-12-09 06:03:03.488765: Current learning rate: 0.00904
2024-12-09 06:04:29.573674: Validation loss did not improve from -0.53876. Patience: 37/50
2024-12-09 06:04:29.575299: train_loss -0.7168
2024-12-09 06:04:29.577203: val_loss -0.5246
2024-12-09 06:04:29.578422: Pseudo dice [0.7406]
2024-12-09 06:04:29.579092: Epoch time: 86.09 s
2024-12-09 06:04:30.741786: 
2024-12-09 06:04:30.743873: Epoch 107
2024-12-09 06:04:30.744916: Current learning rate: 0.00903
2024-12-09 06:05:56.759260: Validation loss did not improve from -0.53876. Patience: 38/50
2024-12-09 06:05:56.760249: train_loss -0.7193
2024-12-09 06:05:56.761307: val_loss -0.5227
2024-12-09 06:05:56.762390: Pseudo dice [0.7341]
2024-12-09 06:05:56.763370: Epoch time: 86.02 s
2024-12-09 06:05:57.959547: 
2024-12-09 06:05:57.961190: Epoch 108
2024-12-09 06:05:57.962602: Current learning rate: 0.00902
2024-12-09 06:07:23.996572: Validation loss did not improve from -0.53876. Patience: 39/50
2024-12-09 06:07:23.997885: train_loss -0.7224
2024-12-09 06:07:23.999118: val_loss -0.5306
2024-12-09 06:07:23.999940: Pseudo dice [0.7392]
2024-12-09 06:07:24.000782: Epoch time: 86.04 s
2024-12-09 06:07:24.001931: Yayy! New best EMA pseudo Dice: 0.727
2024-12-09 06:07:25.509031: 
2024-12-09 06:07:25.511019: Epoch 109
2024-12-09 06:07:25.511886: Current learning rate: 0.00901
2024-12-09 06:08:51.542022: Validation loss did not improve from -0.53876. Patience: 40/50
2024-12-09 06:08:51.543350: train_loss -0.7209
2024-12-09 06:08:51.544760: val_loss -0.5192
2024-12-09 06:08:51.545744: Pseudo dice [0.73]
2024-12-09 06:08:51.546492: Epoch time: 86.04 s
2024-12-09 06:08:51.902738: Yayy! New best EMA pseudo Dice: 0.7273
2024-12-09 06:08:53.426822: 
2024-12-09 06:08:53.428647: Epoch 110
2024-12-09 06:08:53.429597: Current learning rate: 0.009
2024-12-09 06:10:19.391727: Validation loss did not improve from -0.53876. Patience: 41/50
2024-12-09 06:10:19.393190: train_loss -0.7148
2024-12-09 06:10:19.394424: val_loss -0.4766
2024-12-09 06:10:19.395243: Pseudo dice [0.7123]
2024-12-09 06:10:19.396004: Epoch time: 85.97 s
2024-12-09 06:10:20.564623: 
2024-12-09 06:10:20.566260: Epoch 111
2024-12-09 06:10:20.567442: Current learning rate: 0.009
2024-12-09 06:11:46.575110: Validation loss did not improve from -0.53876. Patience: 42/50
2024-12-09 06:11:46.576063: train_loss -0.7165
2024-12-09 06:11:46.577065: val_loss -0.5119
2024-12-09 06:11:46.577972: Pseudo dice [0.7274]
2024-12-09 06:11:46.578904: Epoch time: 86.01 s
2024-12-09 06:11:47.731810: 
2024-12-09 06:11:47.733554: Epoch 112
2024-12-09 06:11:47.734292: Current learning rate: 0.00899
2024-12-09 06:13:13.742433: Validation loss did not improve from -0.53876. Patience: 43/50
2024-12-09 06:13:13.743721: train_loss -0.7177
2024-12-09 06:13:13.744614: val_loss -0.5338
2024-12-09 06:13:13.745682: Pseudo dice [0.7431]
2024-12-09 06:13:13.746772: Epoch time: 86.01 s
2024-12-09 06:13:13.747658: Yayy! New best EMA pseudo Dice: 0.7277
2024-12-09 06:13:15.237035: 
2024-12-09 06:13:15.238930: Epoch 113
2024-12-09 06:13:15.239933: Current learning rate: 0.00898
2024-12-09 06:14:41.315289: Validation loss did not improve from -0.53876. Patience: 44/50
2024-12-09 06:14:41.316849: train_loss -0.7211
2024-12-09 06:14:41.318518: val_loss -0.5167
2024-12-09 06:14:41.319759: Pseudo dice [0.7291]
2024-12-09 06:14:41.320820: Epoch time: 86.08 s
2024-12-09 06:14:41.321923: Yayy! New best EMA pseudo Dice: 0.7278
2024-12-09 06:14:42.825671: 
2024-12-09 06:14:42.827664: Epoch 114
2024-12-09 06:14:42.828786: Current learning rate: 0.00897
2024-12-09 06:16:08.846525: Validation loss did not improve from -0.53876. Patience: 45/50
2024-12-09 06:16:08.848202: train_loss -0.7262
2024-12-09 06:16:08.849316: val_loss -0.5283
2024-12-09 06:16:08.850451: Pseudo dice [0.7391]
2024-12-09 06:16:08.851553: Epoch time: 86.02 s
2024-12-09 06:16:09.201107: Yayy! New best EMA pseudo Dice: 0.7289
2024-12-09 06:16:11.028947: 
2024-12-09 06:16:11.031193: Epoch 115
2024-12-09 06:16:11.032326: Current learning rate: 0.00896
2024-12-09 06:17:37.046904: Validation loss did not improve from -0.53876. Patience: 46/50
2024-12-09 06:17:37.048286: train_loss -0.7258
2024-12-09 06:17:37.049348: val_loss -0.5056
2024-12-09 06:17:37.050285: Pseudo dice [0.7264]
2024-12-09 06:17:37.051337: Epoch time: 86.02 s
2024-12-09 06:17:38.230205: 
2024-12-09 06:17:38.232301: Epoch 116
2024-12-09 06:17:38.233254: Current learning rate: 0.00895
2024-12-09 06:19:04.381552: Validation loss did not improve from -0.53876. Patience: 47/50
2024-12-09 06:19:04.382915: train_loss -0.7189
2024-12-09 06:19:04.384454: val_loss -0.4736
2024-12-09 06:19:04.385604: Pseudo dice [0.7166]
2024-12-09 06:19:04.386691: Epoch time: 86.15 s
2024-12-09 06:19:05.577483: 
2024-12-09 06:19:05.579556: Epoch 117
2024-12-09 06:19:05.580620: Current learning rate: 0.00894
2024-12-09 06:20:31.664024: Validation loss did not improve from -0.53876. Patience: 48/50
2024-12-09 06:20:31.665456: train_loss -0.7205
2024-12-09 06:20:31.666388: val_loss -0.5163
2024-12-09 06:20:31.667413: Pseudo dice [0.7311]
2024-12-09 06:20:31.668257: Epoch time: 86.09 s
2024-12-09 06:20:32.854596: 
2024-12-09 06:20:32.856697: Epoch 118
2024-12-09 06:20:32.857653: Current learning rate: 0.00893
2024-12-09 06:21:58.879702: Validation loss did not improve from -0.53876. Patience: 49/50
2024-12-09 06:21:58.880938: train_loss -0.7294
2024-12-09 06:21:58.882163: val_loss -0.5082
2024-12-09 06:21:58.883365: Pseudo dice [0.7361]
2024-12-09 06:21:58.884466: Epoch time: 86.03 s
2024-12-09 06:22:00.064858: 
2024-12-09 06:22:00.067327: Epoch 119
2024-12-09 06:22:00.068467: Current learning rate: 0.00892
2024-12-09 06:23:26.125147: Validation loss did not improve from -0.53876. Patience: 50/50
2024-12-09 06:23:26.126398: train_loss -0.7291
2024-12-09 06:23:26.127578: val_loss -0.5163
2024-12-09 06:23:26.128421: Pseudo dice [0.7344]
2024-12-09 06:23:26.129188: Epoch time: 86.06 s
2024-12-09 06:23:26.482455: Yayy! New best EMA pseudo Dice: 0.7292
2024-12-09 06:23:28.024407: Patience reached. Stopping training.
2024-12-09 06:23:28.395114: Training done.
2024-12-09 06:23:28.534895: Using splits from existing split file: /home/gridsan/nchutisilp/datasets/nnUNet_Datasets/nnUNet_preprocessed/Dataset309_Sohee_Calcium_OCT_CrossValidation/splits_final.json
2024-12-09 06:23:28.537964: The split file contains 5 splits.
2024-12-09 06:23:28.539028: Desired fold for training: 4
2024-12-09 06:23:28.539684: This split has 7 training and 1 validation cases.
2024-12-09 06:23:28.540721: predicting 101-045
2024-12-09 06:23:28.554522: 101-045, shape torch.Size([1, 375, 498, 498]), rank 0
2024-12-09 06:25:32.340141: Validation complete
2024-12-09 06:25:32.340954: Mean Validation Dice:  0.733154002819443
