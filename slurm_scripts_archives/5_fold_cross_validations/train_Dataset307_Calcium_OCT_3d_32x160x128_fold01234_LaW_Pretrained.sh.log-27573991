
CommandNotFoundError: Your shell has not been properly configured to use 'conda activate'.
To initialize your shell, run

    $ conda init <SHELL_NAME>

Currently supported shells are:
  - bash
  - fish
  - tcsh
  - xonsh
  - zsh
  - powershell

See 'conda init --help' for more information and options.

IMPORTANT: You may need to close and restart your shell after running 'conda init'.


/home/gridsan/nchutisilp/.conda/envs/nnunet/bin/python
Starting training with CONFIG=3d_32x160x128_b10, DATASET_ID=308, TRAINER=nnUNetTrainer

############################
INFO: You are using the old nnU-Net default plans. We have updated our recommendations. Please consider using those instead! Read more here: https://github.com/MIC-DKFZ/nnUNet/blob/master/documentation/resenc_presets.md
############################

Using device: cuda:0

#######################################################################
Please cite the following paper when using nnU-Net:
Isensee, F., Jaeger, P. F., Kohl, S. A., Petersen, J., & Maier-Hein, K. H. (2021). nnU-Net: a self-configuring method for deep learning-based biomedical image segmentation. Nature methods, 18(2), 203-211.
#######################################################################

2024-12-08 16:21:51.114253: Using torch.compile...

############################
INFO: You are using the old nnU-Net default plans. We have updated our recommendations. Please consider using those instead! Read more here: https://github.com/MIC-DKFZ/nnUNet/blob/master/documentation/resenc_presets.md
############################

Using device: cuda:0

#######################################################################
Please cite the following paper when using nnU-Net:
Isensee, F., Jaeger, P. F., Kohl, S. A., Petersen, J., & Maier-Hein, K. H. (2021). nnU-Net: a self-configuring method for deep learning-based biomedical image segmentation. Nature methods, 18(2), 203-211.
#######################################################################

2024-12-08 16:21:51.114007: Using torch.compile...
/home/gridsan/nchutisilp/.local/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn("The verbose parameter is deprecated. Please use get_last_lr() "
/home/gridsan/nchutisilp/.local/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn("The verbose parameter is deprecated. Please use get_last_lr() "
################### Loading pretrained weights from file  /home/gridsan/nchutisilp/datasets/nnUNet_Datasets/nnUNet_results/Dataset300_Lumen_and_Wall_OCT/nnUNetTrainer__nnUNetPreprocessPlans__3d_fullres/fold_all/checkpoint_best.pth ###################
Below is the list of overlapping blocks in pretrained model and nnUNet architecture:
encoder.stages.0.0.convs.0.conv.weight shape torch.Size([32, 1, 3, 3, 3])
encoder.stages.0.0.convs.0.conv.bias shape torch.Size([32])
encoder.stages.0.0.convs.0.norm.weight shape torch.Size([32])
encoder.stages.0.0.convs.0.norm.bias shape torch.Size([32])
encoder.stages.0.0.convs.0.all_modules.0.weight shape torch.Size([32, 1, 3, 3, 3])
encoder.stages.0.0.convs.0.all_modules.0.bias shape torch.Size([32])
encoder.stages.0.0.convs.0.all_modules.1.weight shape torch.Size([32])
encoder.stages.0.0.convs.0.all_modules.1.bias shape torch.Size([32])
encoder.stages.0.0.convs.1.conv.weight shape torch.Size([32, 32, 3, 3, 3])
encoder.stages.0.0.convs.1.conv.bias shape torch.Size([32])
encoder.stages.0.0.convs.1.norm.weight shape torch.Size([32])
encoder.stages.0.0.convs.1.norm.bias shape torch.Size([32])
encoder.stages.0.0.convs.1.all_modules.0.weight shape torch.Size([32, 32, 3, 3, 3])
encoder.stages.0.0.convs.1.all_modules.0.bias shape torch.Size([32])
encoder.stages.0.0.convs.1.all_modules.1.weight shape torch.Size([32])
encoder.stages.0.0.convs.1.all_modules.1.bias shape torch.Size([32])
encoder.stages.1.0.convs.0.conv.weight shape torch.Size([64, 32, 3, 3, 3])
encoder.stages.1.0.convs.0.conv.bias shape torch.Size([64])
encoder.stages.1.0.convs.0.norm.weight shape torch.Size([64])
encoder.stages.1.0.convs.0.norm.bias shape torch.Size([64])
encoder.stages.1.0.convs.0.all_modules.0.weight shape torch.Size([64, 32, 3, 3, 3])
encoder.stages.1.0.convs.0.all_modules.0.bias shape torch.Size([64])
encoder.stages.1.0.convs.0.all_modules.1.weight shape torch.Size([64])
encoder.stages.1.0.convs.0.all_modules.1.bias shape torch.Size([64])
encoder.stages.1.0.convs.1.conv.weight shape torch.Size([64, 64, 3, 3, 3])
encoder.stages.1.0.convs.1.conv.bias shape torch.Size([64])
encoder.stages.1.0.convs.1.norm.weight shape torch.Size([64])
encoder.stages.1.0.convs.1.norm.bias shape torch.Size([64])
encoder.stages.1.0.convs.1.all_modules.0.weight shape torch.Size([64, 64, 3, 3, 3])
encoder.stages.1.0.convs.1.all_modules.0.bias shape torch.Size([64])
encoder.stages.1.0.convs.1.all_modules.1.weight shape torch.Size([64])
encoder.stages.1.0.convs.1.all_modules.1.bias shape torch.Size([64])
encoder.stages.2.0.convs.0.conv.weight shape torch.Size([128, 64, 3, 3, 3])
encoder.stages.2.0.convs.0.conv.bias shape torch.Size([128])
encoder.stages.2.0.convs.0.norm.weight shape torch.Size([128])
encoder.stages.2.0.convs.0.norm.bias shape torch.Size([128])
encoder.stages.2.0.convs.0.all_modules.0.weight shape torch.Size([128, 64, 3, 3, 3])
encoder.stages.2.0.convs.0.all_modules.0.bias shape torch.Size([128])
encoder.stages.2.0.convs.0.all_modules.1.weight shape torch.Size([128])
encoder.stages.2.0.convs.0.all_modules.1.bias shape torch.Size([128])
encoder.stages.2.0.convs.1.conv.weight shape torch.Size([128, 128, 3, 3, 3])
encoder.stages.2.0.convs.1.conv.bias shape torch.Size([128])
encoder.stages.2.0.convs.1.norm.weight shape torch.Size([128])
encoder.stages.2.0.convs.1.norm.bias shape torch.Size([128])
encoder.stages.2.0.convs.1.all_modules.0.weight shape torch.Size([128, 128, 3, 3, 3])
encoder.stages.2.0.convs.1.all_modules.0.bias shape torch.Size([128])
encoder.stages.2.0.convs.1.all_modules.1.weight shape torch.Size([128])
encoder.stages.2.0.convs.1.all_modules.1.bias shape torch.Size([128])
encoder.stages.3.0.convs.0.conv.weight shape torch.Size([256, 128, 3, 3, 3])
encoder.stages.3.0.convs.0.conv.bias shape torch.Size([256])
encoder.stages.3.0.convs.0.norm.weight shape torch.Size([256])
encoder.stages.3.0.convs.0.norm.bias shape torch.Size([256])
encoder.stages.3.0.convs.0.all_modules.0.weight shape torch.Size([256, 128, 3, 3, 3])
encoder.stages.3.0.convs.0.all_modules.0.bias shape torch.Size([256])
encoder.stages.3.0.convs.0.all_modules.1.weight shape torch.Size([256])
encoder.stages.3.0.convs.0.all_modules.1.bias shape torch.Size([256])
encoder.stages.3.0.convs.1.conv.weight shape torch.Size([256, 256, 3, 3, 3])
encoder.stages.3.0.convs.1.conv.bias shape torch.Size([256])
encoder.stages.3.0.convs.1.norm.weight shape torch.Size([256])
encoder.stages.3.0.convs.1.norm.bias shape torch.Size([256])
encoder.stages.3.0.convs.1.all_modules.0.weight shape torch.Size([256, 256, 3, 3, 3])
encoder.stages.3.0.convs.1.all_modules.0.bias shape torch.Size([256])
encoder.stages.3.0.convs.1.all_modules.1.weight shape torch.Size([256])
encoder.stages.3.0.convs.1.all_modules.1.bias shape torch.Size([256])
encoder.stages.4.0.convs.0.conv.weight shape torch.Size([320, 256, 3, 3, 3])
encoder.stages.4.0.convs.0.conv.bias shape torch.Size([320])
encoder.stages.4.0.convs.0.norm.weight shape torch.Size([320])
encoder.stages.4.0.convs.0.norm.bias shape torch.Size([320])
encoder.stages.4.0.convs.0.all_modules.0.weight shape torch.Size([320, 256, 3, 3, 3])
encoder.stages.4.0.convs.0.all_modules.0.bias shape torch.Size([320])
encoder.stages.4.0.convs.0.all_modules.1.weight shape torch.Size([320])
encoder.stages.4.0.convs.0.all_modules.1.bias shape torch.Size([320])
encoder.stages.4.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.4.0.convs.1.conv.bias shape torch.Size([320])
encoder.stages.4.0.convs.1.norm.weight shape torch.Size([320])
encoder.stages.4.0.convs.1.norm.bias shape torch.Size([320])
encoder.stages.4.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.4.0.convs.1.all_modules.0.bias shape torch.Size([320])
encoder.stages.4.0.convs.1.all_modules.1.weight shape torch.Size([320])
encoder.stages.4.0.convs.1.all_modules.1.bias shape torch.Size([320])
encoder.stages.5.0.convs.0.conv.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.5.0.convs.0.conv.bias shape torch.Size([320])
encoder.stages.5.0.convs.0.norm.weight shape torch.Size([320])
encoder.stages.5.0.convs.0.norm.bias shape torch.Size([320])
encoder.stages.5.0.convs.0.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.5.0.convs.0.all_modules.0.bias shape torch.Size([320])
encoder.stages.5.0.convs.0.all_modules.1.weight shape torch.Size([320])
encoder.stages.5.0.convs.0.all_modules.1.bias shape torch.Size([320])
encoder.stages.5.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.5.0.convs.1.conv.bias shape torch.Size([320])
encoder.stages.5.0.convs.1.norm.weight shape torch.Size([320])
encoder.stages.5.0.convs.1.norm.bias shape torch.Size([320])
encoder.stages.5.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.5.0.convs.1.all_modules.0.bias shape torch.Size([320])
encoder.stages.5.0.convs.1.all_modules.1.weight shape torch.Size([320])
encoder.stages.5.0.convs.1.all_modules.1.bias shape torch.Size([320])
decoder.encoder.stages.0.0.convs.0.conv.weight shape torch.Size([32, 1, 3, 3, 3])
decoder.encoder.stages.0.0.convs.0.conv.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.norm.weight shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.norm.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.all_modules.0.weight shape torch.Size([32, 1, 3, 3, 3])
decoder.encoder.stages.0.0.convs.0.all_modules.0.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.all_modules.1.weight shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.all_modules.1.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.conv.weight shape torch.Size([32, 32, 3, 3, 3])
decoder.encoder.stages.0.0.convs.1.conv.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.norm.weight shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.norm.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.all_modules.0.weight shape torch.Size([32, 32, 3, 3, 3])
decoder.encoder.stages.0.0.convs.1.all_modules.0.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.all_modules.1.weight shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.all_modules.1.bias shape torch.Size([32])
decoder.encoder.stages.1.0.convs.0.conv.weight shape torch.Size([64, 32, 3, 3, 3])
decoder.encoder.stages.1.0.convs.0.conv.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.norm.weight shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.norm.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.all_modules.0.weight shape torch.Size([64, 32, 3, 3, 3])
decoder.encoder.stages.1.0.convs.0.all_modules.0.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.all_modules.1.weight shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.all_modules.1.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.conv.weight shape torch.Size([64, 64, 3, 3, 3])
decoder.encoder.stages.1.0.convs.1.conv.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.norm.weight shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.norm.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.all_modules.0.weight shape torch.Size([64, 64, 3, 3, 3])
decoder.encoder.stages.1.0.convs.1.all_modules.0.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.all_modules.1.weight shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.all_modules.1.bias shape torch.Size([64])
decoder.encoder.stages.2.0.convs.0.conv.weight shape torch.Size([128, 64, 3, 3, 3])
decoder.encoder.stages.2.0.convs.0.conv.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.norm.weight shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.norm.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.all_modules.0.weight shape torch.Size([128, 64, 3, 3, 3])
decoder.encoder.stages.2.0.convs.0.all_modules.0.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.all_modules.1.weight shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.all_modules.1.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.conv.weight shape torch.Size([128, 128, 3, 3, 3])
decoder.encoder.stages.2.0.convs.1.conv.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.norm.weight shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.norm.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.all_modules.0.weight shape torch.Size([128, 128, 3, 3, 3])
decoder.encoder.stages.2.0.convs.1.all_modules.0.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.all_modules.1.weight shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.all_modules.1.bias shape torch.Size([128])
decoder.encoder.stages.3.0.convs.0.conv.weight shape torch.Size([256, 128, 3, 3, 3])
decoder.encoder.stages.3.0.convs.0.conv.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.norm.weight shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.norm.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.all_modules.0.weight shape torch.Size([256, 128, 3, 3, 3])
decoder.encoder.stages.3.0.convs.0.all_modules.0.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.all_modules.1.weight shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.all_modules.1.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.conv.weight shape torch.Size([256, 256, 3, 3, 3])
decoder.encoder.stages.3.0.convs.1.conv.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.norm.weight shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.norm.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.all_modules.0.weight shape torch.Size([256, 256, 3, 3, 3])
decoder.encoder.stages.3.0.convs.1.all_modules.0.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.all_modules.1.weight shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.all_modules.1.bias shape torch.Size([256])
decoder.encoder.stages.4.0.convs.0.conv.weight shape torch.Size([320, 256, 3, 3, 3])
decoder.encoder.stages.4.0.convs.0.conv.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.norm.weight shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.norm.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.all_modules.0.weight shape torch.Size([320, 256, 3, 3, 3])
decoder.encoder.stages.4.0.convs.0.all_modules.0.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.all_modules.1.weight shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.all_modules.1.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.4.0.convs.1.conv.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.norm.weight shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.norm.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.4.0.convs.1.all_modules.0.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.all_modules.1.weight shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.all_modules.1.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.conv.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.5.0.convs.0.conv.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.norm.weight shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.norm.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.5.0.convs.0.all_modules.0.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.all_modules.1.weight shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.all_modules.1.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.5.0.convs.1.conv.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.norm.weight shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.norm.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.5.0.convs.1.all_modules.0.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.all_modules.1.weight shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.all_modules.1.bias shape torch.Size([320])
decoder.stages.0.convs.0.conv.weight shape torch.Size([320, 640, 3, 3, 3])
decoder.stages.0.convs.0.conv.bias shape torch.Size([320])
decoder.stages.0.convs.0.norm.weight shape torch.Size([320])
decoder.stages.0.convs.0.norm.bias shape torch.Size([320])
decoder.stages.0.convs.0.all_modules.0.weight shape torch.Size([320, 640, 3, 3, 3])
decoder.stages.0.convs.0.all_modules.0.bias shape torch.Size([320])
decoder.stages.0.convs.0.all_modules.1.weight shape torch.Size([320])
decoder.stages.0.convs.0.all_modules.1.bias shape torch.Size([320])
decoder.stages.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.stages.0.convs.1.conv.bias shape torch.Size([320])
decoder.stages.0.convs.1.norm.weight shape torch.Size([320])
decoder.stages.0.convs.1.norm.bias shape torch.Size([320])
decoder.stages.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.stages.0.convs.1.all_modules.0.bias shape torch.Size([320])
decoder.stages.0.convs.1.all_modules.1.weight shape torch.Size([320])
decoder.stages.0.convs.1.all_modules.1.bias shape torch.Size([320])
decoder.stages.1.convs.0.conv.weight shape torch.Size([256, 512, 3, 3, 3])
decoder.stages.1.convs.0.conv.bias shape torch.Size([256])
decoder.stages.1.convs.0.norm.weight shape torch.Size([256])
decoder.stages.1.convs.0.norm.bias shape torch.Size([256])
decoder.stages.1.convs.0.all_modules.0.weight shape torch.Size([256, 512, 3, 3, 3])
decoder.stages.1.convs.0.all_modules.0.bias shape torch.Size([256])
decoder.stages.1.convs.0.all_modules.1.weight shape torch.Size([256])
decoder.stages.1.convs.0.all_modules.1.bias shape torch.Size([256])
decoder.stages.1.convs.1.conv.weight shape torch.Size([256, 256, 3, 3, 3])
decoder.stages.1.convs.1.conv.bias shape torch.Size([256])
decoder.stages.1.convs.1.norm.weight shape torch.Size([256])
decoder.stages.1.convs.1.norm.bias shape torch.Size([256])
decoder.stages.1.convs.1.all_modules.0.weight shape torch.Size([256, 256, 3, 3, 3])
decoder.stages.1.convs.1.all_modules.0.bias shape torch.Size([256])
decoder.stages.1.convs.1.all_modules.1.weight shape torch.Size([256])
decoder.stages.1.convs.1.all_modules.1.bias shape torch.Size([256])
decoder.stages.2.convs.0.conv.weight shape torch.Size([128, 256, 3, 3, 3])
decoder.stages.2.convs.0.conv.bias shape torch.Size([128])
decoder.stages.2.convs.0.norm.weight shape torch.Size([128])
decoder.stages.2.convs.0.norm.bias shape torch.Size([128])
decoder.stages.2.convs.0.all_modules.0.weight shape torch.Size([128, 256, 3, 3, 3])
decoder.stages.2.convs.0.all_modules.0.bias shape torch.Size([128])
decoder.stages.2.convs.0.all_modules.1.weight shape torch.Size([128])
decoder.stages.2.convs.0.all_modules.1.bias shape torch.Size([128])
decoder.stages.2.convs.1.conv.weight shape torch.Size([128, 128, 3, 3, 3])
decoder.stages.2.convs.1.conv.bias shape torch.Size([128])
decoder.stages.2.convs.1.norm.weight shape torch.Size([128])
decoder.stages.2.convs.1.norm.bias shape torch.Size([128])
decoder.stages.2.convs.1.all_modules.0.weight shape torch.Size([128, 128, 3, 3, 3])
decoder.stages.2.convs.1.all_modules.0.bias shape torch.Size([128])
decoder.stages.2.convs.1.all_modules.1.weight shape torch.Size([128])
decoder.stages.2.convs.1.all_modules.1.bias shape torch.Size([128])
decoder.stages.3.convs.0.conv.weight shape torch.Size([64, 128, 3, 3, 3])
decoder.stages.3.convs.0.conv.bias shape torch.Size([64])
decoder.stages.3.convs.0.norm.weight shape torch.Size([64])
decoder.stages.3.convs.0.norm.bias shape torch.Size([64])
decoder.stages.3.convs.0.all_modules.0.weight shape torch.Size([64, 128, 3, 3, 3])
decoder.stages.3.convs.0.all_modules.0.bias shape torch.Size([64])
decoder.stages.3.convs.0.all_modules.1.weight shape torch.Size([64])
decoder.stages.3.convs.0.all_modules.1.bias shape torch.Size([64])
decoder.stages.3.convs.1.conv.weight shape torch.Size([64, 64, 3, 3, 3])
decoder.stages.3.convs.1.conv.bias shape torch.Size([64])
decoder.stages.3.convs.1.norm.weight shape torch.Size([64])
decoder.stages.3.convs.1.norm.bias shape torch.Size([64])
decoder.stages.3.convs.1.all_modules.0.weight shape torch.Size([64, 64, 3, 3, 3])
decoder.stages.3.convs.1.all_modules.0.bias shape torch.Size([64])
decoder.stages.3.convs.1.all_modules.1.weight shape torch.Size([64])
decoder.stages.3.convs.1.all_modules.1.bias shape torch.Size([64])
decoder.stages.4.convs.0.conv.weight shape torch.Size([32, 64, 3, 3, 3])
decoder.stages.4.convs.0.conv.bias shape torch.Size([32])
decoder.stages.4.convs.0.norm.weight shape torch.Size([32])
decoder.stages.4.convs.0.norm.bias shape torch.Size([32])
decoder.stages.4.convs.0.all_modules.0.weight shape torch.Size([32, 64, 3, 3, 3])
decoder.stages.4.convs.0.all_modules.0.bias shape torch.Size([32])
decoder.stages.4.convs.0.all_modules.1.weight shape torch.Size([32])
decoder.stages.4.convs.0.all_modules.1.bias shape torch.Size([32])
decoder.stages.4.convs.1.conv.weight shape torch.Size([32, 32, 3, 3, 3])
decoder.stages.4.convs.1.conv.bias shape torch.Size([32])
decoder.stages.4.convs.1.norm.weight shape torch.Size([32])
decoder.stages.4.convs.1.norm.bias shape torch.Size([32])
decoder.stages.4.convs.1.all_modules.0.weight shape torch.Size([32, 32, 3, 3, 3])
decoder.stages.4.convs.1.all_modules.0.bias shape torch.Size([32])
decoder.stages.4.convs.1.all_modules.1.weight shape torch.Size([32])
decoder.stages.4.convs.1.all_modules.1.bias shape torch.Size([32])
decoder.transpconvs.0.weight shape torch.Size([320, 320, 1, 2, 2])
decoder.transpconvs.0.bias shape torch.Size([320])
decoder.transpconvs.1.weight shape torch.Size([320, 256, 2, 2, 2])
decoder.transpconvs.1.bias shape torch.Size([256])
decoder.transpconvs.2.weight shape torch.Size([256, 128, 2, 2, 2])
decoder.transpconvs.2.bias shape torch.Size([128])
decoder.transpconvs.3.weight shape torch.Size([128, 64, 2, 2, 2])
decoder.transpconvs.3.bias shape torch.Size([64])
decoder.transpconvs.4.weight shape torch.Size([64, 32, 2, 2, 2])
decoder.transpconvs.4.bias shape torch.Size([32])
################### Done ###################
2024-12-08 16:22:07.834821: do_dummy_2d_data_aug: True
2024-12-08 16:22:07.838530: Using splits from existing split file: /home/gridsan/nchutisilp/datasets/nnUNet_Datasets/nnUNet_preprocessed/Dataset308_Sohee_Calcium_OCT_CrossValidation/splits_final.json
2024-12-08 16:22:07.841116: The split file contains 5 splits.
2024-12-08 16:22:07.841997: Desired fold for training: 1
2024-12-08 16:22:07.842580: This split has 6 training and 2 validation cases.
################### Loading pretrained weights from file  /home/gridsan/nchutisilp/datasets/nnUNet_Datasets/nnUNet_results/Dataset300_Lumen_and_Wall_OCT/nnUNetTrainer__nnUNetPreprocessPlans__3d_fullres/fold_all/checkpoint_best.pth ###################
Below is the list of overlapping blocks in pretrained model and nnUNet architecture:
encoder.stages.0.0.convs.0.conv.weight shape torch.Size([32, 1, 3, 3, 3])
encoder.stages.0.0.convs.0.conv.bias shape torch.Size([32])
encoder.stages.0.0.convs.0.norm.weight shape torch.Size([32])
encoder.stages.0.0.convs.0.norm.bias shape torch.Size([32])
encoder.stages.0.0.convs.0.all_modules.0.weight shape torch.Size([32, 1, 3, 3, 3])
encoder.stages.0.0.convs.0.all_modules.0.bias shape torch.Size([32])
encoder.stages.0.0.convs.0.all_modules.1.weight shape torch.Size([32])
encoder.stages.0.0.convs.0.all_modules.1.bias shape torch.Size([32])
encoder.stages.0.0.convs.1.conv.weight shape torch.Size([32, 32, 3, 3, 3])
encoder.stages.0.0.convs.1.conv.bias shape torch.Size([32])
encoder.stages.0.0.convs.1.norm.weight shape torch.Size([32])
encoder.stages.0.0.convs.1.norm.bias shape torch.Size([32])
encoder.stages.0.0.convs.1.all_modules.0.weight shape torch.Size([32, 32, 3, 3, 3])
encoder.stages.0.0.convs.1.all_modules.0.bias shape torch.Size([32])
encoder.stages.0.0.convs.1.all_modules.1.weight shape torch.Size([32])
encoder.stages.0.0.convs.1.all_modules.1.bias shape torch.Size([32])
encoder.stages.1.0.convs.0.conv.weight shape torch.Size([64, 32, 3, 3, 3])
encoder.stages.1.0.convs.0.conv.bias shape torch.Size([64])
encoder.stages.1.0.convs.0.norm.weight shape torch.Size([64])
encoder.stages.1.0.convs.0.norm.bias shape torch.Size([64])
encoder.stages.1.0.convs.0.all_modules.0.weight shape torch.Size([64, 32, 3, 3, 3])
encoder.stages.1.0.convs.0.all_modules.0.bias shape torch.Size([64])
encoder.stages.1.0.convs.0.all_modules.1.weight shape torch.Size([64])
encoder.stages.1.0.convs.0.all_modules.1.bias shape torch.Size([64])
encoder.stages.1.0.convs.1.conv.weight shape torch.Size([64, 64, 3, 3, 3])
encoder.stages.1.0.convs.1.conv.bias shape torch.Size([64])
encoder.stages.1.0.convs.1.norm.weight shape torch.Size([64])
encoder.stages.1.0.convs.1.norm.bias shape torch.Size([64])
encoder.stages.1.0.convs.1.all_modules.0.weight shape torch.Size([64, 64, 3, 3, 3])
encoder.stages.1.0.convs.1.all_modules.0.bias shape torch.Size([64])
encoder.stages.1.0.convs.1.all_modules.1.weight shape torch.Size([64])
encoder.stages.1.0.convs.1.all_modules.1.bias shape torch.Size([64])
encoder.stages.2.0.convs.0.conv.weight shape torch.Size([128, 64, 3, 3, 3])
encoder.stages.2.0.convs.0.conv.bias shape torch.Size([128])
encoder.stages.2.0.convs.0.norm.weight shape torch.Size([128])
encoder.stages.2.0.convs.0.norm.bias shape torch.Size([128])
encoder.stages.2.0.convs.0.all_modules.0.weight shape torch.Size([128, 64, 3, 3, 3])
encoder.stages.2.0.convs.0.all_modules.0.bias shape torch.Size([128])
encoder.stages.2.0.convs.0.all_modules.1.weight shape torch.Size([128])
encoder.stages.2.0.convs.0.all_modules.1.bias shape torch.Size([128])
encoder.stages.2.0.convs.1.conv.weight shape torch.Size([128, 128, 3, 3, 3])
encoder.stages.2.0.convs.1.conv.bias shape torch.Size([128])
encoder.stages.2.0.convs.1.norm.weight shape torch.Size([128])
encoder.stages.2.0.convs.1.norm.bias shape torch.Size([128])
encoder.stages.2.0.convs.1.all_modules.0.weight shape torch.Size([128, 128, 3, 3, 3])
encoder.stages.2.0.convs.1.all_modules.0.bias shape torch.Size([128])
encoder.stages.2.0.convs.1.all_modules.1.weight shape torch.Size([128])
encoder.stages.2.0.convs.1.all_modules.1.bias shape torch.Size([128])
encoder.stages.3.0.convs.0.conv.weight shape torch.Size([256, 128, 3, 3, 3])
encoder.stages.3.0.convs.0.conv.bias shape torch.Size([256])
encoder.stages.3.0.convs.0.norm.weight shape torch.Size([256])
encoder.stages.3.0.convs.0.norm.bias shape torch.Size([256])
encoder.stages.3.0.convs.0.all_modules.0.weight shape torch.Size([256, 128, 3, 3, 3])
encoder.stages.3.0.convs.0.all_modules.0.bias shape torch.Size([256])
encoder.stages.3.0.convs.0.all_modules.1.weight shape torch.Size([256])
encoder.stages.3.0.convs.0.all_modules.1.bias shape torch.Size([256])
encoder.stages.3.0.convs.1.conv.weight shape torch.Size([256, 256, 3, 3, 3])
encoder.stages.3.0.convs.1.conv.bias shape torch.Size([256])
encoder.stages.3.0.convs.1.norm.weight shape torch.Size([256])
encoder.stages.3.0.convs.1.norm.bias shape torch.Size([256])
encoder.stages.3.0.convs.1.all_modules.0.weight shape torch.Size([256, 256, 3, 3, 3])
encoder.stages.3.0.convs.1.all_modules.0.bias shape torch.Size([256])
encoder.stages.3.0.convs.1.all_modules.1.weight shape torch.Size([256])
encoder.stages.3.0.convs.1.all_modules.1.bias shape torch.Size([256])
encoder.stages.4.0.convs.0.conv.weight shape torch.Size([320, 256, 3, 3, 3])
encoder.stages.4.0.convs.0.conv.bias shape torch.Size([320])
encoder.stages.4.0.convs.0.norm.weight shape torch.Size([320])
encoder.stages.4.0.convs.0.norm.bias shape torch.Size([320])
encoder.stages.4.0.convs.0.all_modules.0.weight shape torch.Size([320, 256, 3, 3, 3])
encoder.stages.4.0.convs.0.all_modules.0.bias shape torch.Size([320])
encoder.stages.4.0.convs.0.all_modules.1.weight shape torch.Size([320])
encoder.stages.4.0.convs.0.all_modules.1.bias shape torch.Size([320])
encoder.stages.4.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.4.0.convs.1.conv.bias shape torch.Size([320])
encoder.stages.4.0.convs.1.norm.weight shape torch.Size([320])
encoder.stages.4.0.convs.1.norm.bias shape torch.Size([320])
encoder.stages.4.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.4.0.convs.1.all_modules.0.bias shape torch.Size([320])
encoder.stages.4.0.convs.1.all_modules.1.weight shape torch.Size([320])
encoder.stages.4.0.convs.1.all_modules.1.bias shape torch.Size([320])
encoder.stages.5.0.convs.0.conv.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.5.0.convs.0.conv.bias shape torch.Size([320])
encoder.stages.5.0.convs.0.norm.weight shape torch.Size([320])
encoder.stages.5.0.convs.0.norm.bias shape torch.Size([320])
encoder.stages.5.0.convs.0.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.5.0.convs.0.all_modules.0.bias shape torch.Size([320])
encoder.stages.5.0.convs.0.all_modules.1.weight shape torch.Size([320])
encoder.stages.5.0.convs.0.all_modules.1.bias shape torch.Size([320])
encoder.stages.5.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.5.0.convs.1.conv.bias shape torch.Size([320])
encoder.stages.5.0.convs.1.norm.weight shape torch.Size([320])
encoder.stages.5.0.convs.1.norm.bias shape torch.Size([320])
encoder.stages.5.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.5.0.convs.1.all_modules.0.bias shape torch.Size([320])
encoder.stages.5.0.convs.1.all_modules.1.weight shape torch.Size([320])
encoder.stages.5.0.convs.1.all_modules.1.bias shape torch.Size([320])
decoder.encoder.stages.0.0.convs.0.conv.weight shape torch.Size([32, 1, 3, 3, 3])
decoder.encoder.stages.0.0.convs.0.conv.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.norm.weight shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.norm.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.all_modules.0.weight shape torch.Size([32, 1, 3, 3, 3])
decoder.encoder.stages.0.0.convs.0.all_modules.0.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.all_modules.1.weight shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.all_modules.1.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.conv.weight shape torch.Size([32, 32, 3, 3, 3])
decoder.encoder.stages.0.0.convs.1.conv.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.norm.weight shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.norm.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.all_modules.0.weight shape torch.Size([32, 32, 3, 3, 3])
decoder.encoder.stages.0.0.convs.1.all_modules.0.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.all_modules.1.weight shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.all_modules.1.bias shape torch.Size([32])
decoder.encoder.stages.1.0.convs.0.conv.weight shape torch.Size([64, 32, 3, 3, 3])
decoder.encoder.stages.1.0.convs.0.conv.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.norm.weight shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.norm.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.all_modules.0.weight shape torch.Size([64, 32, 3, 3, 3])
decoder.encoder.stages.1.0.convs.0.all_modules.0.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.all_modules.1.weight shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.all_modules.1.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.conv.weight shape torch.Size([64, 64, 3, 3, 3])
decoder.encoder.stages.1.0.convs.1.conv.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.norm.weight shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.norm.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.all_modules.0.weight shape torch.Size([64, 64, 3, 3, 3])
decoder.encoder.stages.1.0.convs.1.all_modules.0.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.all_modules.1.weight shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.all_modules.1.bias shape torch.Size([64])
decoder.encoder.stages.2.0.convs.0.conv.weight shape torch.Size([128, 64, 3, 3, 3])
decoder.encoder.stages.2.0.convs.0.conv.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.norm.weight shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.norm.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.all_modules.0.weight shape torch.Size([128, 64, 3, 3, 3])
decoder.encoder.stages.2.0.convs.0.all_modules.0.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.all_modules.1.weight shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.all_modules.1.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.conv.weight shape torch.Size([128, 128, 3, 3, 3])
decoder.encoder.stages.2.0.convs.1.conv.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.norm.weight shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.norm.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.all_modules.0.weight shape torch.Size([128, 128, 3, 3, 3])
decoder.encoder.stages.2.0.convs.1.all_modules.0.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.all_modules.1.weight shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.all_modules.1.bias shape torch.Size([128])
decoder.encoder.stages.3.0.convs.0.conv.weight shape torch.Size([256, 128, 3, 3, 3])
decoder.encoder.stages.3.0.convs.0.conv.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.norm.weight shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.norm.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.all_modules.0.weight shape torch.Size([256, 128, 3, 3, 3])
decoder.encoder.stages.3.0.convs.0.all_modules.0.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.all_modules.1.weight shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.all_modules.1.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.conv.weight shape torch.Size([256, 256, 3, 3, 3])
decoder.encoder.stages.3.0.convs.1.conv.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.norm.weight shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.norm.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.all_modules.0.weight shape torch.Size([256, 256, 3, 3, 3])
decoder.encoder.stages.3.0.convs.1.all_modules.0.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.all_modules.1.weight shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.all_modules.1.bias shape torch.Size([256])
decoder.encoder.stages.4.0.convs.0.conv.weight shape torch.Size([320, 256, 3, 3, 3])
decoder.encoder.stages.4.0.convs.0.conv.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.norm.weight shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.norm.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.all_modules.0.weight shape torch.Size([320, 256, 3, 3, 3])
decoder.encoder.stages.4.0.convs.0.all_modules.0.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.all_modules.1.weight shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.all_modules.1.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.4.0.convs.1.conv.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.norm.weight shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.norm.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.4.0.convs.1.all_modules.0.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.all_modules.1.weight shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.all_modules.1.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.conv.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.5.0.convs.0.conv.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.norm.weight shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.norm.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.5.0.convs.0.all_modules.0.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.all_modules.1.weight shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.all_modules.1.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.5.0.convs.1.conv.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.norm.weight shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.norm.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.5.0.convs.1.all_modules.0.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.all_modules.1.weight shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.all_modules.1.bias shape torch.Size([320])
decoder.stages.0.convs.0.conv.weight shape torch.Size([320, 640, 3, 3, 3])
decoder.stages.0.convs.0.conv.bias shape torch.Size([320])
decoder.stages.0.convs.0.norm.weight shape torch.Size([320])
decoder.stages.0.convs.0.norm.bias shape torch.Size([320])
decoder.stages.0.convs.0.all_modules.0.weight shape torch.Size([320, 640, 3, 3, 3])
decoder.stages.0.convs.0.all_modules.0.bias shape torch.Size([320])
decoder.stages.0.convs.0.all_modules.1.weight shape torch.Size([320])
decoder.stages.0.convs.0.all_modules.1.bias shape torch.Size([320])
decoder.stages.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.stages.0.convs.1.conv.bias shape torch.Size([320])
decoder.stages.0.convs.1.norm.weight shape torch.Size([320])
decoder.stages.0.convs.1.norm.bias shape torch.Size([320])
decoder.stages.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.stages.0.convs.1.all_modules.0.bias shape torch.Size([320])
decoder.stages.0.convs.1.all_modules.1.weight shape torch.Size([320])
decoder.stages.0.convs.1.all_modules.1.bias shape torch.Size([320])
decoder.stages.1.convs.0.conv.weight shape torch.Size([256, 512, 3, 3, 3])
decoder.stages.1.convs.0.conv.bias shape torch.Size([256])
decoder.stages.1.convs.0.norm.weight shape torch.Size([256])
decoder.stages.1.convs.0.norm.bias shape torch.Size([256])
decoder.stages.1.convs.0.all_modules.0.weight shape torch.Size([256, 512, 3, 3, 3])
decoder.stages.1.convs.0.all_modules.0.bias shape torch.Size([256])
decoder.stages.1.convs.0.all_modules.1.weight shape torch.Size([256])
decoder.stages.1.convs.0.all_modules.1.bias shape torch.Size([256])
decoder.stages.1.convs.1.conv.weight shape torch.Size([256, 256, 3, 3, 3])
decoder.stages.1.convs.1.conv.bias shape torch.Size([256])
decoder.stages.1.convs.1.norm.weight shape torch.Size([256])
decoder.stages.1.convs.1.norm.bias shape torch.Size([256])
decoder.stages.1.convs.1.all_modules.0.weight shape torch.Size([256, 256, 3, 3, 3])
decoder.stages.1.convs.1.all_modules.0.bias shape torch.Size([256])
decoder.stages.1.convs.1.all_modules.1.weight shape torch.Size([256])
decoder.stages.1.convs.1.all_modules.1.bias shape torch.Size([256])
decoder.stages.2.convs.0.conv.weight shape torch.Size([128, 256, 3, 3, 3])
decoder.stages.2.convs.0.conv.bias shape torch.Size([128])
decoder.stages.2.convs.0.norm.weight shape torch.Size([128])
decoder.stages.2.convs.0.norm.bias shape torch.Size([128])
decoder.stages.2.convs.0.all_modules.0.weight shape torch.Size([128, 256, 3, 3, 3])
decoder.stages.2.convs.0.all_modules.0.bias shape torch.Size([128])
decoder.stages.2.convs.0.all_modules.1.weight shape torch.Size([128])
decoder.stages.2.convs.0.all_modules.1.bias shape torch.Size([128])
decoder.stages.2.convs.1.conv.weight shape torch.Size([128, 128, 3, 3, 3])
decoder.stages.2.convs.1.conv.bias shape torch.Size([128])
decoder.stages.2.convs.1.norm.weight shape torch.Size([128])
decoder.stages.2.convs.1.norm.bias shape torch.Size([128])
decoder.stages.2.convs.1.all_modules.0.weight shape torch.Size([128, 128, 3, 3, 3])
decoder.stages.2.convs.1.all_modules.0.bias shape torch.Size([128])
decoder.stages.2.convs.1.all_modules.1.weight shape torch.Size([128])
decoder.stages.2.convs.1.all_modules.1.bias shape torch.Size([128])
decoder.stages.3.convs.0.conv.weight shape torch.Size([64, 128, 3, 3, 3])
decoder.stages.3.convs.0.conv.bias shape torch.Size([64])
decoder.stages.3.convs.0.norm.weight shape torch.Size([64])
decoder.stages.3.convs.0.norm.bias shape torch.Size([64])
decoder.stages.3.convs.0.all_modules.0.weight shape torch.Size([64, 128, 3, 3, 3])
decoder.stages.3.convs.0.all_modules.0.bias shape torch.Size([64])
decoder.stages.3.convs.0.all_modules.1.weight shape torch.Size([64])
decoder.stages.3.convs.0.all_modules.1.bias shape torch.Size([64])
decoder.stages.3.convs.1.conv.weight shape torch.Size([64, 64, 3, 3, 3])
decoder.stages.3.convs.1.conv.bias shape torch.Size([64])
decoder.stages.3.convs.1.norm.weight shape torch.Size([64])
decoder.stages.3.convs.1.norm.bias shape torch.Size([64])
decoder.stages.3.convs.1.all_modules.0.weight shape torch.Size([64, 64, 3, 3, 3])
decoder.stages.3.convs.1.all_modules.0.bias shape torch.Size([64])
decoder.stages.3.convs.1.all_modules.1.weight shape torch.Size([64])
decoder.stages.3.convs.1.all_modules.1.bias shape torch.Size([64])
decoder.stages.4.convs.0.conv.weight shape torch.Size([32, 64, 3, 3, 3])
decoder.stages.4.convs.0.conv.bias shape torch.Size([32])
decoder.stages.4.convs.0.norm.weight shape torch.Size([32])
decoder.stages.4.convs.0.norm.bias shape torch.Size([32])
decoder.stages.4.convs.0.all_modules.0.weight shape torch.Size([32, 64, 3, 3, 3])
decoder.stages.4.convs.0.all_modules.0.bias shape torch.Size([32])
decoder.stages.4.convs.0.all_modules.1.weight shape torch.Size([32])
decoder.stages.4.convs.0.all_modules.1.bias shape torch.Size([32])
decoder.stages.4.convs.1.conv.weight shape torch.Size([32, 32, 3, 3, 3])
decoder.stages.4.convs.1.conv.bias shape torch.Size([32])
decoder.stages.4.convs.1.norm.weight shape torch.Size([32])
decoder.stages.4.convs.1.norm.bias shape torch.Size([32])
decoder.stages.4.convs.1.all_modules.0.weight shape torch.Size([32, 32, 3, 3, 3])
decoder.stages.4.convs.1.all_modules.0.bias shape torch.Size([32])
decoder.stages.4.convs.1.all_modules.1.weight shape torch.Size([32])
decoder.stages.4.convs.1.all_modules.1.bias shape torch.Size([32])
decoder.transpconvs.0.weight shape torch.Size([320, 320, 1, 2, 2])
decoder.transpconvs.0.bias shape torch.Size([320])
decoder.transpconvs.1.weight shape torch.Size([320, 256, 2, 2, 2])
decoder.transpconvs.1.bias shape torch.Size([256])
decoder.transpconvs.2.weight shape torch.Size([256, 128, 2, 2, 2])
decoder.transpconvs.2.bias shape torch.Size([128])
decoder.transpconvs.3.weight shape torch.Size([128, 64, 2, 2, 2])
decoder.transpconvs.3.bias shape torch.Size([64])
decoder.transpconvs.4.weight shape torch.Size([64, 32, 2, 2, 2])
decoder.transpconvs.4.bias shape torch.Size([32])
################### Done ###################
2024-12-08 16:22:07.834806: do_dummy_2d_data_aug: True
2024-12-08 16:22:07.838502: Using splits from existing split file: /home/gridsan/nchutisilp/datasets/nnUNet_Datasets/nnUNet_preprocessed/Dataset308_Sohee_Calcium_OCT_CrossValidation/splits_final.json
2024-12-08 16:22:07.840991: The split file contains 5 splits.
2024-12-08 16:22:07.841895: Desired fold for training: 0
2024-12-08 16:22:07.842523: This split has 6 training and 2 validation cases.
using pin_memory on device 0
using pin_memory on device 0
using pin_memory on device 0

This is the configuration used by this training:
Configuration name: 3d_32x160x128_b10
 {'data_identifier': 'nnUNetPlans_3d_fullres', 'preprocessor_name': 'DefaultPreprocessor', 'batch_size': 10, 'patch_size': [32, 160, 128], 'median_image_size_in_voxels': [375.0, 498.0, 498.0], 'spacing': [1.0, 1.0, 1.0], 'normalization_schemes': ['ZScoreNormalization'], 'use_mask_for_norm': [False], 'resampling_fn_data': 'resample_data_or_seg_to_shape', 'resampling_fn_seg': 'resample_data_or_seg_to_shape', 'resampling_fn_data_kwargs': {'is_seg': False, 'order': 3, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_seg_kwargs': {'is_seg': True, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_probabilities': 'resample_data_or_seg_to_shape', 'resampling_fn_probabilities_kwargs': {'is_seg': False, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'architecture': {'network_class_name': 'dynamic_network_architectures.architectures.unet.PlainConvUNet', 'arch_kwargs': {'n_stages': 6, 'features_per_stage': [32, 64, 128, 256, 320, 320], 'conv_op': 'torch.nn.modules.conv.Conv3d', 'kernel_sizes': [[3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3]], 'strides': [[1, 1, 1], [2, 2, 2], [2, 2, 2], [2, 2, 2], [2, 2, 2], [1, 2, 2]], 'n_conv_per_stage': [2, 2, 2, 2, 2, 2], 'n_conv_per_stage_decoder': [2, 2, 2, 2, 2], 'conv_bias': True, 'norm_op': 'torch.nn.modules.instancenorm.InstanceNorm3d', 'norm_op_kwargs': {'eps': 1e-05, 'affine': True}, 'dropout_op': None, 'dropout_op_kwargs': None, 'nonlin': 'torch.nn.LeakyReLU', 'nonlin_kwargs': {'inplace': True}, 'deep_supervision': True}, '_kw_requires_import': ['conv_op', 'norm_op', 'dropout_op', 'nonlin']}, 'batch_dice': True, 'inherits_from': '3d_fullres'} 

These are the global plan.json settings:
 {'dataset_name': 'Dataset308_Sohee_Calcium_OCT_CrossValidation', 'plans_name': 'nnUNetPlans', 'original_median_spacing_after_transp': [1.0, 1.0, 1.0], 'original_median_shape_after_transp': [375, 498, 498], 'image_reader_writer': 'SimpleITKIO', 'transpose_forward': [0, 1, 2], 'transpose_backward': [0, 1, 2], 'experiment_planner_used': 'ExperimentPlanner', 'label_manager': 'LabelManager', 'foreground_intensity_properties_per_channel': {'0': {'max': 0.4977976381778717, 'mean': 0.13507525622844696, 'median': 0.09599608182907104, 'min': 0.0, 'percentile_00_5': 0.014754901640117168, 'percentile_99_5': 0.4977976381778717, 'std': 0.12025152146816254}}} 

2024-12-08 16:22:17.538250: unpacking dataset...
using pin_memory on device 0

This is the configuration used by this training:
Configuration name: 3d_32x160x128_b10
 {'data_identifier': 'nnUNetPlans_3d_fullres', 'preprocessor_name': 'DefaultPreprocessor', 'batch_size': 10, 'patch_size': [32, 160, 128], 'median_image_size_in_voxels': [375.0, 498.0, 498.0], 'spacing': [1.0, 1.0, 1.0], 'normalization_schemes': ['ZScoreNormalization'], 'use_mask_for_norm': [False], 'resampling_fn_data': 'resample_data_or_seg_to_shape', 'resampling_fn_seg': 'resample_data_or_seg_to_shape', 'resampling_fn_data_kwargs': {'is_seg': False, 'order': 3, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_seg_kwargs': {'is_seg': True, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_probabilities': 'resample_data_or_seg_to_shape', 'resampling_fn_probabilities_kwargs': {'is_seg': False, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'architecture': {'network_class_name': 'dynamic_network_architectures.architectures.unet.PlainConvUNet', 'arch_kwargs': {'n_stages': 6, 'features_per_stage': [32, 64, 128, 256, 320, 320], 'conv_op': 'torch.nn.modules.conv.Conv3d', 'kernel_sizes': [[3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3]], 'strides': [[1, 1, 1], [2, 2, 2], [2, 2, 2], [2, 2, 2], [2, 2, 2], [1, 2, 2]], 'n_conv_per_stage': [2, 2, 2, 2, 2, 2], 'n_conv_per_stage_decoder': [2, 2, 2, 2, 2], 'conv_bias': True, 'norm_op': 'torch.nn.modules.instancenorm.InstanceNorm3d', 'norm_op_kwargs': {'eps': 1e-05, 'affine': True}, 'dropout_op': None, 'dropout_op_kwargs': None, 'nonlin': 'torch.nn.LeakyReLU', 'nonlin_kwargs': {'inplace': True}, 'deep_supervision': True}, '_kw_requires_import': ['conv_op', 'norm_op', 'dropout_op', 'nonlin']}, 'batch_dice': True, 'inherits_from': '3d_fullres'} 

These are the global plan.json settings:
 {'dataset_name': 'Dataset308_Sohee_Calcium_OCT_CrossValidation', 'plans_name': 'nnUNetPlans', 'original_median_spacing_after_transp': [1.0, 1.0, 1.0], 'original_median_shape_after_transp': [375, 498, 498], 'image_reader_writer': 'SimpleITKIO', 'transpose_forward': [0, 1, 2], 'transpose_backward': [0, 1, 2], 'experiment_planner_used': 'ExperimentPlanner', 'label_manager': 'LabelManager', 'foreground_intensity_properties_per_channel': {'0': {'max': 0.4977976381778717, 'mean': 0.13507525622844696, 'median': 0.09599608182907104, 'min': 0.0, 'percentile_00_5': 0.014754901640117168, 'percentile_99_5': 0.4977976381778717, 'std': 0.12025152146816254}}} 

2024-12-08 16:22:18.022345: unpacking dataset...
2024-12-08 16:22:23.304635: unpacking done...
2024-12-08 16:22:23.376410: Unable to plot network architecture: nnUNet_compile is enabled!
2024-12-08 16:22:23.445325: 
2024-12-08 16:22:23.447389: Epoch 0
2024-12-08 16:22:23.448978: Current learning rate: 0.01
2024-12-08 16:25:51.803746: Validation loss improved from 1000.00000 to -0.36959! Patience: 0/50
2024-12-08 16:25:51.830388: train_loss -0.3268
2024-12-08 16:25:51.840648: val_loss -0.3696
2024-12-08 16:25:51.841609: Pseudo dice [0.6965]
2024-12-08 16:25:51.842467: Epoch time: 208.36 s
2024-12-08 16:25:51.843113: Yayy! New best EMA pseudo Dice: 0.6965
2024-12-08 16:25:54.416025: 
2024-12-08 16:25:54.417645: Epoch 1
2024-12-08 16:25:54.418819: Current learning rate: 0.00999
2024-12-08 16:27:23.389963: Validation loss improved from -0.36959 to -0.42616! Patience: 0/50
2024-12-08 16:27:23.390827: train_loss -0.4655
2024-12-08 16:27:23.391609: val_loss -0.4262
2024-12-08 16:27:23.392601: Pseudo dice [0.7169]
2024-12-08 16:27:23.393627: Epoch time: 88.98 s
2024-12-08 16:27:23.394711: Yayy! New best EMA pseudo Dice: 0.6985
2024-12-08 16:27:24.999383: 
2024-12-08 16:27:25.000798: Epoch 2
2024-12-08 16:27:25.001588: Current learning rate: 0.00998
2024-12-08 16:28:54.893523: Validation loss improved from -0.42616 to -0.43662! Patience: 0/50
2024-12-08 16:28:54.894626: train_loss -0.4981
2024-12-08 16:28:54.895489: val_loss -0.4366
2024-12-08 16:28:54.896266: Pseudo dice [0.688]
2024-12-08 16:28:54.896868: Epoch time: 89.9 s
2024-12-08 16:28:56.233013: 
2024-12-08 16:28:56.234134: Epoch 3
2024-12-08 16:28:56.234897: Current learning rate: 0.00997
2024-12-08 16:30:26.541626: Validation loss improved from -0.43662 to -0.48778! Patience: 0/50
2024-12-08 16:30:26.542663: train_loss -0.5234
2024-12-08 16:30:26.543655: val_loss -0.4878
2024-12-08 16:30:26.544413: Pseudo dice [0.7497]
2024-12-08 16:30:26.545223: Epoch time: 90.31 s
2024-12-08 16:30:26.545940: Yayy! New best EMA pseudo Dice: 0.7027
2024-12-08 16:30:28.188127: 
2024-12-08 16:30:28.190120: Epoch 4
2024-12-08 16:30:28.190807: Current learning rate: 0.00996
2024-12-08 16:31:58.470837: Validation loss did not improve from -0.48778. Patience: 1/50
2024-12-08 16:31:58.471537: train_loss -0.5458
2024-12-08 16:31:58.472595: val_loss -0.4611
2024-12-08 16:31:58.473506: Pseudo dice [0.7325]
2024-12-08 16:31:58.474190: Epoch time: 90.28 s
2024-12-08 16:31:58.780666: Yayy! New best EMA pseudo Dice: 0.7057
2024-12-08 16:32:00.418882: 
2024-12-08 16:32:00.420706: Epoch 5
2024-12-08 16:32:00.421491: Current learning rate: 0.00995
2024-12-08 16:33:29.225422: Validation loss improved from -0.48778 to -0.52635! Patience: 1/50
2024-12-08 16:33:29.226359: train_loss -0.5424
2024-12-08 16:33:29.227307: val_loss -0.5264
2024-12-08 16:33:29.228326: Pseudo dice [0.7589]
2024-12-08 16:33:29.229234: Epoch time: 88.81 s
2024-12-08 16:33:29.230370: Yayy! New best EMA pseudo Dice: 0.711
2024-12-08 16:33:30.758207: 
2024-12-08 16:33:30.759359: Epoch 6
2024-12-08 16:33:30.760429: Current learning rate: 0.00995
2024-12-08 16:34:59.513901: Validation loss did not improve from -0.52635. Patience: 1/50
2024-12-08 16:34:59.514630: train_loss -0.5709
2024-12-08 16:34:59.515401: val_loss -0.5062
2024-12-08 16:34:59.516085: Pseudo dice [0.7399]
2024-12-08 16:34:59.516657: Epoch time: 88.76 s
2024-12-08 16:34:59.517239: Yayy! New best EMA pseudo Dice: 0.7139
2024-12-08 16:35:01.080556: 
2024-12-08 16:35:01.082240: Epoch 7
2024-12-08 16:35:01.083454: Current learning rate: 0.00994
2024-12-08 16:36:29.896154: Validation loss did not improve from -0.52635. Patience: 2/50
2024-12-08 16:36:29.897692: train_loss -0.561
2024-12-08 16:36:29.899108: val_loss -0.5074
2024-12-08 16:36:29.899873: Pseudo dice [0.7554]
2024-12-08 16:36:29.900553: Epoch time: 88.82 s
2024-12-08 16:36:29.901355: Yayy! New best EMA pseudo Dice: 0.718
2024-12-08 16:36:31.998477: 
2024-12-08 16:36:32.000795: Epoch 8
2024-12-08 16:36:32.001542: Current learning rate: 0.00993
2024-12-08 16:38:01.060529: Validation loss improved from -0.52635 to -0.52871! Patience: 2/50
2024-12-08 16:38:01.061312: train_loss -0.5785
2024-12-08 16:38:01.062166: val_loss -0.5287
2024-12-08 16:38:01.062909: Pseudo dice [0.7644]
2024-12-08 16:38:01.063622: Epoch time: 89.06 s
2024-12-08 16:38:01.064264: Yayy! New best EMA pseudo Dice: 0.7227
2024-12-08 16:38:02.728191: 
2024-12-08 16:38:02.729673: Epoch 9
2024-12-08 16:38:02.730383: Current learning rate: 0.00992
2024-12-08 16:39:31.727090: Validation loss did not improve from -0.52871. Patience: 1/50
2024-12-08 16:39:31.728023: train_loss -0.5847
2024-12-08 16:39:31.728849: val_loss -0.5187
2024-12-08 16:39:31.729620: Pseudo dice [0.7646]
2024-12-08 16:39:31.730341: Epoch time: 89.0 s
2024-12-08 16:39:32.087557: Yayy! New best EMA pseudo Dice: 0.7269
2024-12-08 16:39:33.655781: 
2024-12-08 16:39:33.657803: Epoch 10
2024-12-08 16:39:33.658701: Current learning rate: 0.00991
2024-12-08 16:41:04.588430: Validation loss did not improve from -0.52871. Patience: 2/50
2024-12-08 16:41:04.589287: train_loss -0.5891
2024-12-08 16:41:04.590031: val_loss -0.5114
2024-12-08 16:41:04.590707: Pseudo dice [0.7627]
2024-12-08 16:41:04.591720: Epoch time: 90.94 s
2024-12-08 16:41:04.592730: Yayy! New best EMA pseudo Dice: 0.7304
2024-12-08 16:41:06.197117: 
2024-12-08 16:41:06.198837: Epoch 11
2024-12-08 16:41:06.199667: Current learning rate: 0.0099
2024-12-08 16:42:35.161544: Validation loss did not improve from -0.52871. Patience: 3/50
2024-12-08 16:42:35.162475: train_loss -0.5962
2024-12-08 16:42:35.163351: val_loss -0.4848
2024-12-08 16:42:35.164119: Pseudo dice [0.7336]
2024-12-08 16:42:35.164864: Epoch time: 88.97 s
2024-12-08 16:42:35.165729: Yayy! New best EMA pseudo Dice: 0.7308
2024-12-08 16:42:36.743897: 
2024-12-08 16:42:36.745263: Epoch 12
2024-12-08 16:42:36.746094: Current learning rate: 0.00989
2024-12-08 16:44:05.791716: Validation loss did not improve from -0.52871. Patience: 4/50
2024-12-08 16:44:05.792687: train_loss -0.6155
2024-12-08 16:44:05.793459: val_loss -0.5246
2024-12-08 16:44:05.794205: Pseudo dice [0.7655]
2024-12-08 16:44:05.794832: Epoch time: 89.05 s
2024-12-08 16:44:05.795685: Yayy! New best EMA pseudo Dice: 0.7342
2024-12-08 16:44:07.427440: 
2024-12-08 16:44:07.428974: Epoch 13
2024-12-08 16:44:07.429837: Current learning rate: 0.00988
2024-12-08 16:45:36.512991: Validation loss did not improve from -0.52871. Patience: 5/50
2024-12-08 16:45:36.513664: train_loss -0.6111
2024-12-08 16:45:36.514446: val_loss -0.5048
2024-12-08 16:45:36.515300: Pseudo dice [0.7477]
2024-12-08 16:45:36.516013: Epoch time: 89.09 s
2024-12-08 16:45:36.516717: Yayy! New best EMA pseudo Dice: 0.7356
2024-12-08 16:45:38.097595: 
2024-12-08 16:45:38.099340: Epoch 14
2024-12-08 16:45:38.100564: Current learning rate: 0.00987
2024-12-08 16:47:07.191109: Validation loss improved from -0.52871 to -0.52912! Patience: 5/50
2024-12-08 16:47:07.192170: train_loss -0.6149
2024-12-08 16:47:07.193321: val_loss -0.5291
2024-12-08 16:47:07.194283: Pseudo dice [0.7711]
2024-12-08 16:47:07.195177: Epoch time: 89.1 s
2024-12-08 16:47:07.538256: Yayy! New best EMA pseudo Dice: 0.7391
2024-12-08 16:47:09.194578: 
2024-12-08 16:47:09.196047: Epoch 15
2024-12-08 16:47:09.196872: Current learning rate: 0.00986
2024-12-08 16:48:38.192530: Validation loss did not improve from -0.52912. Patience: 1/50
2024-12-08 16:48:38.193237: train_loss -0.6225
2024-12-08 16:48:38.194100: val_loss -0.5108
2024-12-08 16:48:38.194972: Pseudo dice [0.7579]
2024-12-08 16:48:38.195602: Epoch time: 89.0 s
2024-12-08 16:48:38.196453: Yayy! New best EMA pseudo Dice: 0.741
2024-12-08 16:48:39.812181: 
2024-12-08 16:48:39.814034: Epoch 16
2024-12-08 16:48:39.815259: Current learning rate: 0.00986
2024-12-08 16:50:09.182331: Validation loss did not improve from -0.52912. Patience: 2/50
2024-12-08 16:50:09.183362: train_loss -0.6254
2024-12-08 16:50:09.184132: val_loss -0.5202
2024-12-08 16:50:09.184916: Pseudo dice [0.7649]
2024-12-08 16:50:09.185712: Epoch time: 89.37 s
2024-12-08 16:50:09.186358: Yayy! New best EMA pseudo Dice: 0.7434
2024-12-08 16:50:10.834457: 
2024-12-08 16:50:10.835574: Epoch 17
2024-12-08 16:50:10.836592: Current learning rate: 0.00985
2024-12-08 16:51:40.255154: Validation loss improved from -0.52912 to -0.54237! Patience: 2/50
2024-12-08 16:51:40.256512: train_loss -0.6304
2024-12-08 16:51:40.257859: val_loss -0.5424
2024-12-08 16:51:40.258622: Pseudo dice [0.771]
2024-12-08 16:51:40.259308: Epoch time: 89.42 s
2024-12-08 16:51:40.260160: Yayy! New best EMA pseudo Dice: 0.7462
2024-12-08 16:51:42.282614: 
2024-12-08 16:51:42.284156: Epoch 18
2024-12-08 16:51:42.284954: Current learning rate: 0.00984
2024-12-08 16:53:11.697231: Validation loss did not improve from -0.54237. Patience: 1/50
2024-12-08 16:53:11.698547: train_loss -0.6332
2024-12-08 16:53:11.699435: val_loss -0.5319
2024-12-08 16:53:11.700297: Pseudo dice [0.7679]
2024-12-08 16:53:11.700974: Epoch time: 89.42 s
2024-12-08 16:53:11.701595: Yayy! New best EMA pseudo Dice: 0.7483
2024-12-08 16:53:13.335078: 
2024-12-08 16:53:13.336156: Epoch 19
2024-12-08 16:53:13.337144: Current learning rate: 0.00983
2024-12-08 16:54:42.698415: Validation loss did not improve from -0.54237. Patience: 2/50
2024-12-08 16:54:42.699477: train_loss -0.6327
2024-12-08 16:54:42.700490: val_loss -0.5297
2024-12-08 16:54:42.701339: Pseudo dice [0.771]
2024-12-08 16:54:42.702132: Epoch time: 89.37 s
2024-12-08 16:54:43.047822: Yayy! New best EMA pseudo Dice: 0.7506
2024-12-08 16:54:44.678485: 
2024-12-08 16:54:44.680290: Epoch 20
2024-12-08 16:54:44.681078: Current learning rate: 0.00982
2024-12-08 16:56:15.670545: Validation loss improved from -0.54237 to -0.54557! Patience: 2/50
2024-12-08 16:56:15.671608: train_loss -0.6367
2024-12-08 16:56:15.672395: val_loss -0.5456
2024-12-08 16:56:15.673185: Pseudo dice [0.7705]
2024-12-08 16:56:15.673949: Epoch time: 90.99 s
2024-12-08 16:56:15.674765: Yayy! New best EMA pseudo Dice: 0.7526
2024-12-08 16:56:17.303782: 
2024-12-08 16:56:17.305519: Epoch 21
2024-12-08 16:56:17.306414: Current learning rate: 0.00981
2024-12-08 16:57:46.561934: Validation loss did not improve from -0.54557. Patience: 1/50
2024-12-08 16:57:46.563154: train_loss -0.6279
2024-12-08 16:57:46.563905: val_loss -0.5214
2024-12-08 16:57:46.564773: Pseudo dice [0.7601]
2024-12-08 16:57:46.565533: Epoch time: 89.26 s
2024-12-08 16:57:46.566202: Yayy! New best EMA pseudo Dice: 0.7533
2024-12-08 16:57:48.135076: 
2024-12-08 16:57:48.136913: Epoch 22
2024-12-08 16:57:48.137748: Current learning rate: 0.0098
2024-12-08 16:59:17.360088: Validation loss improved from -0.54557 to -0.54920! Patience: 1/50
2024-12-08 16:59:17.361008: train_loss -0.6434
2024-12-08 16:59:17.361730: val_loss -0.5492
2024-12-08 16:59:17.362341: Pseudo dice [0.7778]
2024-12-08 16:59:17.363097: Epoch time: 89.23 s
2024-12-08 16:59:17.363899: Yayy! New best EMA pseudo Dice: 0.7558
2024-12-08 16:59:18.920690: 
2024-12-08 16:59:18.922446: Epoch 23
2024-12-08 16:59:18.923289: Current learning rate: 0.00979
2024-12-08 17:00:48.220706: Validation loss improved from -0.54920 to -0.59367! Patience: 0/50
2024-12-08 17:00:48.222034: train_loss -0.6395
2024-12-08 17:00:48.222919: val_loss -0.5937
2024-12-08 17:00:48.223594: Pseudo dice [0.7922]
2024-12-08 17:00:48.224325: Epoch time: 89.3 s
2024-12-08 17:00:48.225067: Yayy! New best EMA pseudo Dice: 0.7594
2024-12-08 17:00:49.796961: 
2024-12-08 17:00:49.798716: Epoch 24
2024-12-08 17:00:49.799485: Current learning rate: 0.00978
2024-12-08 17:02:19.424542: Validation loss did not improve from -0.59367. Patience: 1/50
2024-12-08 17:02:19.425875: train_loss -0.6496
2024-12-08 17:02:19.427043: val_loss -0.5051
2024-12-08 17:02:19.428073: Pseudo dice [0.7567]
2024-12-08 17:02:19.429143: Epoch time: 89.63 s
2024-12-08 17:02:21.048009: 
2024-12-08 17:02:21.049742: Epoch 25
2024-12-08 17:02:21.050670: Current learning rate: 0.00977
2024-12-08 17:03:50.591063: Validation loss did not improve from -0.59367. Patience: 2/50
2024-12-08 17:03:50.592222: train_loss -0.6497
2024-12-08 17:03:50.593256: val_loss -0.5179
2024-12-08 17:03:50.594031: Pseudo dice [0.754]
2024-12-08 17:03:50.594820: Epoch time: 89.55 s
2024-12-08 17:03:51.810786: 
2024-12-08 17:03:51.812189: Epoch 26
2024-12-08 17:03:51.813010: Current learning rate: 0.00977
2024-12-08 17:05:21.320195: Validation loss did not improve from -0.59367. Patience: 3/50
2024-12-08 17:05:21.321137: train_loss -0.6526
2024-12-08 17:05:21.322044: val_loss -0.5363
2024-12-08 17:05:21.322702: Pseudo dice [0.7688]
2024-12-08 17:05:21.323428: Epoch time: 89.51 s
2024-12-08 17:05:21.324063: Yayy! New best EMA pseudo Dice: 0.7597
2024-12-08 17:05:22.884905: 
2024-12-08 17:05:22.886103: Epoch 27
2024-12-08 17:05:22.887021: Current learning rate: 0.00976
2024-12-08 17:06:52.449105: Validation loss did not improve from -0.59367. Patience: 4/50
2024-12-08 17:06:52.450315: train_loss -0.6565
2024-12-08 17:06:52.451202: val_loss -0.5573
2024-12-08 17:06:52.451896: Pseudo dice [0.7779]
2024-12-08 17:06:52.452624: Epoch time: 89.57 s
2024-12-08 17:06:52.453258: Yayy! New best EMA pseudo Dice: 0.7615
2024-12-08 17:06:54.028739: 
2024-12-08 17:06:54.030594: Epoch 28
2024-12-08 17:06:54.031261: Current learning rate: 0.00975
2024-12-08 17:08:23.689921: Validation loss did not improve from -0.59367. Patience: 5/50
2024-12-08 17:08:23.690943: train_loss -0.6635
2024-12-08 17:08:23.692003: val_loss -0.5325
2024-12-08 17:08:23.692966: Pseudo dice [0.7731]
2024-12-08 17:08:23.693697: Epoch time: 89.66 s
2024-12-08 17:08:23.694671: Yayy! New best EMA pseudo Dice: 0.7626
2024-12-08 17:08:25.674785: 
2024-12-08 17:08:25.676421: Epoch 29
2024-12-08 17:08:25.677384: Current learning rate: 0.00974
2024-12-08 17:09:55.397122: Validation loss did not improve from -0.59367. Patience: 6/50
2024-12-08 17:09:55.398227: train_loss -0.6602
2024-12-08 17:09:55.398997: val_loss -0.5315
2024-12-08 17:09:55.399622: Pseudo dice [0.7618]
2024-12-08 17:09:55.400292: Epoch time: 89.72 s
2024-12-08 17:09:57.001193: 
2024-12-08 17:09:57.002722: Epoch 30
2024-12-08 17:09:57.003926: Current learning rate: 0.00973
2024-12-08 17:11:26.628155: Validation loss did not improve from -0.59367. Patience: 7/50
2024-12-08 17:11:26.629379: train_loss -0.6586
2024-12-08 17:11:26.630110: val_loss -0.5476
2024-12-08 17:11:26.630786: Pseudo dice [0.7771]
2024-12-08 17:11:26.631486: Epoch time: 89.63 s
2024-12-08 17:11:26.632115: Yayy! New best EMA pseudo Dice: 0.764
2024-12-08 17:11:28.238220: 
2024-12-08 17:11:28.239707: Epoch 31
2024-12-08 17:11:28.240482: Current learning rate: 0.00972
2024-12-08 17:12:57.370380: Validation loss did not improve from -0.59367. Patience: 8/50
2024-12-08 17:12:57.371178: train_loss -0.6655
2024-12-08 17:12:57.372016: val_loss -0.5389
2024-12-08 17:12:57.372642: Pseudo dice [0.7752]
2024-12-08 17:12:57.373358: Epoch time: 89.13 s
2024-12-08 17:12:57.373996: Yayy! New best EMA pseudo Dice: 0.7651
2024-12-08 17:12:58.974584: 
2024-12-08 17:12:58.975697: Epoch 32
2024-12-08 17:12:58.976380: Current learning rate: 0.00971
2024-12-08 17:14:29.876936: Validation loss did not improve from -0.59367. Patience: 9/50
2024-12-08 17:14:29.877939: train_loss -0.6672
2024-12-08 17:14:29.878853: val_loss -0.5212
2024-12-08 17:14:29.879671: Pseudo dice [0.7648]
2024-12-08 17:14:29.880436: Epoch time: 90.9 s
2024-12-08 17:14:31.143496: 
2024-12-08 17:14:31.145595: Epoch 33
2024-12-08 17:14:31.146366: Current learning rate: 0.0097
2024-12-08 17:16:02.065628: Validation loss did not improve from -0.59367. Patience: 10/50
2024-12-08 17:16:02.066890: train_loss -0.6686
2024-12-08 17:16:02.067967: val_loss -0.533
2024-12-08 17:16:02.068785: Pseudo dice [0.7758]
2024-12-08 17:16:02.069595: Epoch time: 90.92 s
2024-12-08 17:16:02.070373: Yayy! New best EMA pseudo Dice: 0.7662
2024-12-08 17:16:03.716551: 
2024-12-08 17:16:03.718091: Epoch 34
2024-12-08 17:16:03.719343: Current learning rate: 0.00969
2024-12-08 17:17:34.454932: Validation loss did not improve from -0.59367. Patience: 11/50
2024-12-08 17:17:34.455904: train_loss -0.6755
2024-12-08 17:17:34.456815: val_loss -0.5605
2024-12-08 17:17:34.457502: Pseudo dice [0.7847]
2024-12-08 17:17:34.458244: Epoch time: 90.74 s
2024-12-08 17:17:34.805289: Yayy! New best EMA pseudo Dice: 0.768
2024-12-08 17:17:36.459275: 
2024-12-08 17:17:36.460557: Epoch 35
2024-12-08 17:17:36.461351: Current learning rate: 0.00968
2024-12-08 17:19:07.163055: Validation loss did not improve from -0.59367. Patience: 12/50
2024-12-08 17:19:07.164160: train_loss -0.6728
2024-12-08 17:19:07.165352: val_loss -0.5318
2024-12-08 17:19:07.166070: Pseudo dice [0.7668]
2024-12-08 17:19:07.166799: Epoch time: 90.71 s
2024-12-08 17:19:08.440787: 
2024-12-08 17:19:08.442387: Epoch 36
2024-12-08 17:19:08.443083: Current learning rate: 0.00968
2024-12-08 17:20:39.212805: Validation loss did not improve from -0.59367. Patience: 13/50
2024-12-08 17:20:39.214096: train_loss -0.6812
2024-12-08 17:20:39.215330: val_loss -0.55
2024-12-08 17:20:39.216155: Pseudo dice [0.7766]
2024-12-08 17:20:39.216834: Epoch time: 90.77 s
2024-12-08 17:20:39.217587: Yayy! New best EMA pseudo Dice: 0.7688
2024-12-08 17:20:40.843085: 
2024-12-08 17:20:40.844986: Epoch 37
2024-12-08 17:20:40.845940: Current learning rate: 0.00967
2024-12-08 17:22:11.508847: Validation loss did not improve from -0.59367. Patience: 14/50
2024-12-08 17:22:11.509895: train_loss -0.6843
2024-12-08 17:22:11.510706: val_loss -0.5214
2024-12-08 17:22:11.511340: Pseudo dice [0.759]
2024-12-08 17:22:11.512045: Epoch time: 90.67 s
2024-12-08 17:22:12.780792: 
2024-12-08 17:22:12.782235: Epoch 38
2024-12-08 17:22:12.783026: Current learning rate: 0.00966
2024-12-08 17:23:43.526665: Validation loss did not improve from -0.59367. Patience: 15/50
2024-12-08 17:23:43.527807: train_loss -0.6807
2024-12-08 17:23:43.528574: val_loss -0.5286
2024-12-08 17:23:43.529335: Pseudo dice [0.763]
2024-12-08 17:23:43.530035: Epoch time: 90.75 s
2024-12-08 17:23:45.239655: 
2024-12-08 17:23:45.241140: Epoch 39
2024-12-08 17:23:45.241820: Current learning rate: 0.00965
2024-12-08 17:25:16.150850: Validation loss did not improve from -0.59367. Patience: 16/50
2024-12-08 17:25:16.151953: train_loss -0.6863
2024-12-08 17:25:16.152856: val_loss -0.5494
2024-12-08 17:25:16.153471: Pseudo dice [0.7804]
2024-12-08 17:25:16.154177: Epoch time: 90.91 s
2024-12-08 17:25:17.832682: 
2024-12-08 17:25:17.834300: Epoch 40
2024-12-08 17:25:17.834909: Current learning rate: 0.00964
2024-12-08 17:26:48.776420: Validation loss did not improve from -0.59367. Patience: 17/50
2024-12-08 17:26:48.777526: train_loss -0.6872
2024-12-08 17:26:48.778814: val_loss -0.5211
2024-12-08 17:26:48.779623: Pseudo dice [0.7683]
2024-12-08 17:26:48.780555: Epoch time: 90.95 s
2024-12-08 17:26:50.127196: 
2024-12-08 17:26:50.128585: Epoch 41
2024-12-08 17:26:50.129602: Current learning rate: 0.00963
2024-12-08 17:28:21.113991: Validation loss did not improve from -0.59367. Patience: 18/50
2024-12-08 17:28:21.115197: train_loss -0.6856
2024-12-08 17:28:21.116146: val_loss -0.5
2024-12-08 17:28:21.116773: Pseudo dice [0.761]
2024-12-08 17:28:21.118038: Epoch time: 90.99 s
2024-12-08 17:28:22.384507: 
2024-12-08 17:28:22.386761: Epoch 42
2024-12-08 17:28:22.387515: Current learning rate: 0.00962
2024-12-08 17:29:53.346987: Validation loss did not improve from -0.59367. Patience: 19/50
2024-12-08 17:29:53.348069: train_loss -0.6845
2024-12-08 17:29:53.348910: val_loss -0.5552
2024-12-08 17:29:53.349589: Pseudo dice [0.7796]
2024-12-08 17:29:53.350421: Epoch time: 90.96 s
2024-12-08 17:29:53.351321: Yayy! New best EMA pseudo Dice: 0.769
2024-12-08 17:29:54.946943: 
2024-12-08 17:29:54.948559: Epoch 43
2024-12-08 17:29:54.949334: Current learning rate: 0.00961
2024-12-08 17:31:26.037426: Validation loss did not improve from -0.59367. Patience: 20/50
2024-12-08 17:31:26.040644: train_loss -0.6845
2024-12-08 17:31:26.042807: val_loss -0.5514
2024-12-08 17:31:26.043681: Pseudo dice [0.7756]
2024-12-08 17:31:26.044675: Epoch time: 91.09 s
2024-12-08 17:31:26.045456: Yayy! New best EMA pseudo Dice: 0.7697
2024-12-08 17:31:27.680558: 
2024-12-08 17:31:27.681917: Epoch 44
2024-12-08 17:31:27.682664: Current learning rate: 0.0096
2024-12-08 17:32:58.686987: Validation loss did not improve from -0.59367. Patience: 21/50
2024-12-08 17:32:58.688180: train_loss -0.6946
2024-12-08 17:32:58.689235: val_loss -0.5366
2024-12-08 17:32:58.689970: Pseudo dice [0.767]
2024-12-08 17:32:58.690712: Epoch time: 91.01 s
2024-12-08 17:33:00.355385: 
2024-12-08 17:33:00.357158: Epoch 45
2024-12-08 17:33:00.357940: Current learning rate: 0.00959
2024-12-08 17:34:31.355697: Validation loss did not improve from -0.59367. Patience: 22/50
2024-12-08 17:34:31.356961: train_loss -0.6875
2024-12-08 17:34:31.357760: val_loss -0.5715
2024-12-08 17:34:31.358534: Pseudo dice [0.7864]
2024-12-08 17:34:31.359215: Epoch time: 91.0 s
2024-12-08 17:34:31.359916: Yayy! New best EMA pseudo Dice: 0.7711
2024-12-08 17:34:32.961980: 
2024-12-08 17:34:32.963207: Epoch 46
2024-12-08 17:34:32.964014: Current learning rate: 0.00959
2024-12-08 17:36:03.998817: Validation loss did not improve from -0.59367. Patience: 23/50
2024-12-08 17:36:03.999882: train_loss -0.6887
2024-12-08 17:36:04.000873: val_loss -0.5577
2024-12-08 17:36:04.001670: Pseudo dice [0.7765]
2024-12-08 17:36:04.002438: Epoch time: 91.04 s
2024-12-08 17:36:04.003172: Yayy! New best EMA pseudo Dice: 0.7716
2024-12-08 17:36:05.585590: 
2024-12-08 17:36:05.586454: Epoch 47
2024-12-08 17:36:05.587095: Current learning rate: 0.00958
2024-12-08 17:37:36.696711: Validation loss did not improve from -0.59367. Patience: 24/50
2024-12-08 17:37:36.697726: train_loss -0.693
2024-12-08 17:37:36.698788: val_loss -0.5018
2024-12-08 17:37:36.699500: Pseudo dice [0.7565]
2024-12-08 17:37:36.700209: Epoch time: 91.11 s
2024-12-08 17:37:37.949888: 
2024-12-08 17:37:37.951131: Epoch 48
2024-12-08 17:37:37.951848: Current learning rate: 0.00957
2024-12-08 17:39:08.937279: Validation loss did not improve from -0.59367. Patience: 25/50
2024-12-08 17:39:08.938318: train_loss -0.6894
2024-12-08 17:39:08.939126: val_loss -0.5599
2024-12-08 17:39:08.939803: Pseudo dice [0.7797]
2024-12-08 17:39:08.940669: Epoch time: 90.99 s
2024-12-08 17:39:10.175059: 
2024-12-08 17:39:10.177011: Epoch 49
2024-12-08 17:39:10.178441: Current learning rate: 0.00956
2024-12-08 17:40:41.187759: Validation loss did not improve from -0.59367. Patience: 26/50
2024-12-08 17:40:41.188812: train_loss -0.6966
2024-12-08 17:40:41.189898: val_loss -0.5526
2024-12-08 17:40:41.190760: Pseudo dice [0.7872]
2024-12-08 17:40:41.191659: Epoch time: 91.02 s
2024-12-08 17:40:41.975366: Yayy! New best EMA pseudo Dice: 0.7727
2024-12-08 17:40:43.565966: 
2024-12-08 17:40:43.567616: Epoch 50
2024-12-08 17:40:43.568444: Current learning rate: 0.00955
2024-12-08 17:42:14.678002: Validation loss did not improve from -0.59367. Patience: 27/50
2024-12-08 17:42:14.678752: train_loss -0.6981
2024-12-08 17:42:14.679737: val_loss -0.554
2024-12-08 17:42:14.680478: Pseudo dice [0.7887]
2024-12-08 17:42:14.681182: Epoch time: 91.11 s
2024-12-08 17:42:14.681935: Yayy! New best EMA pseudo Dice: 0.7743
2024-12-08 17:42:16.250325: 
2024-12-08 17:42:16.251857: Epoch 51
2024-12-08 17:42:16.252851: Current learning rate: 0.00954
2024-12-08 17:43:47.252781: Validation loss did not improve from -0.59367. Patience: 28/50
2024-12-08 17:43:47.253998: train_loss -0.6998
2024-12-08 17:43:47.254947: val_loss -0.5299
2024-12-08 17:43:47.255654: Pseudo dice [0.7738]
2024-12-08 17:43:47.256464: Epoch time: 91.0 s
2024-12-08 17:43:48.503305: 
2024-12-08 17:43:48.505274: Epoch 52
2024-12-08 17:43:48.506172: Current learning rate: 0.00953
2024-12-08 17:45:19.219104: Validation loss did not improve from -0.59367. Patience: 29/50
2024-12-08 17:45:19.220007: train_loss -0.7035
2024-12-08 17:45:19.220684: val_loss -0.559
2024-12-08 17:45:19.221310: Pseudo dice [0.7804]
2024-12-08 17:45:19.221912: Epoch time: 90.72 s
2024-12-08 17:45:19.222574: Yayy! New best EMA pseudo Dice: 0.7749
2024-12-08 17:45:20.813441: 
2024-12-08 17:45:20.815288: Epoch 53
2024-12-08 17:45:20.816072: Current learning rate: 0.00952
2024-12-08 17:46:51.428852: Validation loss did not improve from -0.59367. Patience: 30/50
2024-12-08 17:46:51.430024: train_loss -0.7
2024-12-08 17:46:51.430961: val_loss -0.5336
2024-12-08 17:46:51.431742: Pseudo dice [0.7653]
2024-12-08 17:46:51.432581: Epoch time: 90.62 s
2024-12-08 17:46:52.682257: 
2024-12-08 17:46:52.683862: Epoch 54
2024-12-08 17:46:52.684941: Current learning rate: 0.00951
2024-12-08 17:48:23.395093: Validation loss did not improve from -0.59367. Patience: 31/50
2024-12-08 17:48:23.396406: train_loss -0.7037
2024-12-08 17:48:23.397552: val_loss -0.5361
2024-12-08 17:48:23.398294: Pseudo dice [0.7724]
2024-12-08 17:48:23.398971: Epoch time: 90.72 s
2024-12-08 17:48:25.005554: 
2024-12-08 17:48:25.007607: Epoch 55
2024-12-08 17:48:25.008488: Current learning rate: 0.0095
2024-12-08 17:49:55.589598: Validation loss did not improve from -0.59367. Patience: 32/50
2024-12-08 17:49:55.590636: train_loss -0.7114
2024-12-08 17:49:55.591667: val_loss -0.5372
2024-12-08 17:49:55.592614: Pseudo dice [0.7757]
2024-12-08 17:49:55.593724: Epoch time: 90.59 s
2024-12-08 17:49:56.857363: 
2024-12-08 17:49:56.858960: Epoch 56
2024-12-08 17:49:56.859858: Current learning rate: 0.00949
2024-12-08 17:51:27.404382: Validation loss did not improve from -0.59367. Patience: 33/50
2024-12-08 17:51:27.405809: train_loss -0.7053
2024-12-08 17:51:27.406598: val_loss -0.5839
2024-12-08 17:51:27.407322: Pseudo dice [0.7986]
2024-12-08 17:51:27.408000: Epoch time: 90.55 s
2024-12-08 17:51:27.408695: Yayy! New best EMA pseudo Dice: 0.7764
2024-12-08 17:51:29.055885: 
2024-12-08 17:51:29.057991: Epoch 57
2024-12-08 17:51:29.058825: Current learning rate: 0.00949
2024-12-08 17:52:59.688373: Validation loss did not improve from -0.59367. Patience: 34/50
2024-12-08 17:52:59.689108: train_loss -0.6966
2024-12-08 17:52:59.690389: val_loss -0.559
2024-12-08 17:52:59.691418: Pseudo dice [0.7831]
2024-12-08 17:52:59.692527: Epoch time: 90.63 s
2024-12-08 17:52:59.693491: Yayy! New best EMA pseudo Dice: 0.7771
2024-12-08 17:53:01.269079: 
2024-12-08 17:53:01.270727: Epoch 58
2024-12-08 17:53:01.271567: Current learning rate: 0.00948
2024-12-08 17:54:31.842801: Validation loss did not improve from -0.59367. Patience: 35/50
2024-12-08 17:54:31.844242: train_loss -0.7074
2024-12-08 17:54:31.845293: val_loss -0.5356
2024-12-08 17:54:31.846164: Pseudo dice [0.7743]
2024-12-08 17:54:31.846976: Epoch time: 90.58 s
2024-12-08 17:54:33.081866: 
2024-12-08 17:54:33.083902: Epoch 59
2024-12-08 17:54:33.085014: Current learning rate: 0.00947
2024-12-08 17:56:03.607144: Validation loss did not improve from -0.59367. Patience: 36/50
2024-12-08 17:56:03.608036: train_loss -0.7083
2024-12-08 17:56:03.609344: val_loss -0.542
2024-12-08 17:56:03.610416: Pseudo dice [0.7725]
2024-12-08 17:56:03.611857: Epoch time: 90.53 s
2024-12-08 17:56:05.551611: 
2024-12-08 17:56:05.553170: Epoch 60
2024-12-08 17:56:05.554064: Current learning rate: 0.00946
2024-12-08 17:57:36.486753: Validation loss did not improve from -0.59367. Patience: 37/50
2024-12-08 17:57:36.487452: train_loss -0.7121
2024-12-08 17:57:36.488387: val_loss -0.4255
2024-12-08 17:57:36.489046: Pseudo dice [0.7097]
2024-12-08 17:57:36.489660: Epoch time: 90.94 s
2024-12-08 17:57:37.800228: 
2024-12-08 17:57:37.801809: Epoch 61
2024-12-08 17:57:37.802829: Current learning rate: 0.00945
2024-12-08 17:59:08.728866: Validation loss did not improve from -0.59367. Patience: 38/50
2024-12-08 17:59:08.729659: train_loss -0.7127
2024-12-08 17:59:08.730475: val_loss -0.521
2024-12-08 17:59:08.731089: Pseudo dice [0.7603]
2024-12-08 17:59:08.731711: Epoch time: 90.93 s
2024-12-08 17:59:09.954051: 
2024-12-08 17:59:09.955832: Epoch 62
2024-12-08 17:59:09.956860: Current learning rate: 0.00944
2024-12-08 18:00:40.851102: Validation loss did not improve from -0.59367. Patience: 39/50
2024-12-08 18:00:40.852267: train_loss -0.7125
2024-12-08 18:00:40.853650: val_loss -0.538
2024-12-08 18:00:40.854440: Pseudo dice [0.7679]
2024-12-08 18:00:40.855535: Epoch time: 90.9 s
2024-12-08 18:00:42.121033: 
2024-12-08 18:00:42.122756: Epoch 63
2024-12-08 18:00:42.123868: Current learning rate: 0.00943
2024-12-08 18:02:13.005455: Validation loss did not improve from -0.59367. Patience: 40/50
2024-12-08 18:02:13.006797: train_loss -0.7233
2024-12-08 18:02:13.007631: val_loss -0.5126
2024-12-08 18:02:13.008253: Pseudo dice [0.7587]
2024-12-08 18:02:13.008979: Epoch time: 90.89 s
2024-12-08 18:02:14.258258: 
2024-12-08 18:02:14.259756: Epoch 64
2024-12-08 18:02:14.260701: Current learning rate: 0.00942
2024-12-08 18:03:45.216233: Validation loss did not improve from -0.59367. Patience: 41/50
2024-12-08 18:03:45.217515: train_loss -0.7215
2024-12-08 18:03:45.218442: val_loss -0.5139
2024-12-08 18:03:45.219199: Pseudo dice [0.7535]
2024-12-08 18:03:45.220032: Epoch time: 90.96 s
2024-12-08 18:03:46.826969: 
2024-12-08 18:03:46.828821: Epoch 65
2024-12-08 18:03:46.829971: Current learning rate: 0.00941
2024-12-08 18:05:17.877483: Validation loss did not improve from -0.59367. Patience: 42/50
2024-12-08 18:05:17.878182: train_loss -0.7159
2024-12-08 18:05:17.879054: val_loss -0.5208
2024-12-08 18:05:17.879985: Pseudo dice [0.7722]
2024-12-08 18:05:17.880626: Epoch time: 91.05 s
2024-12-08 18:05:19.163861: 
2024-12-08 18:05:19.165498: Epoch 66
2024-12-08 18:05:19.166385: Current learning rate: 0.0094
2024-12-08 18:06:50.195071: Validation loss did not improve from -0.59367. Patience: 43/50
2024-12-08 18:06:50.195999: train_loss -0.7162
2024-12-08 18:06:50.196937: val_loss -0.5058
2024-12-08 18:06:50.197883: Pseudo dice [0.7501]
2024-12-08 18:06:50.198916: Epoch time: 91.03 s
2024-12-08 18:06:51.445319: 
2024-12-08 18:06:51.447070: Epoch 67
2024-12-08 18:06:51.448042: Current learning rate: 0.00939
2024-12-08 18:08:22.620036: Validation loss did not improve from -0.59367. Patience: 44/50
2024-12-08 18:08:22.620926: train_loss -0.7112
2024-12-08 18:08:22.621861: val_loss -0.4758
2024-12-08 18:08:22.622701: Pseudo dice [0.744]
2024-12-08 18:08:22.623757: Epoch time: 91.18 s
2024-12-08 18:08:23.887501: 
2024-12-08 18:08:23.889107: Epoch 68
2024-12-08 18:08:23.890104: Current learning rate: 0.00939
2024-12-08 18:09:55.093477: Validation loss did not improve from -0.59367. Patience: 45/50
2024-12-08 18:09:55.094593: train_loss -0.718
2024-12-08 18:09:55.095780: val_loss -0.5386
2024-12-08 18:09:55.096990: Pseudo dice [0.7712]
2024-12-08 18:09:55.098017: Epoch time: 91.21 s
2024-12-08 18:09:56.361399: 
2024-12-08 18:09:56.363466: Epoch 69
2024-12-08 18:09:56.364345: Current learning rate: 0.00938
2024-12-08 18:11:27.628142: Validation loss did not improve from -0.59367. Patience: 46/50
2024-12-08 18:11:27.629217: train_loss -0.7228
2024-12-08 18:11:27.630165: val_loss -0.5087
2024-12-08 18:11:27.630975: Pseudo dice [0.754]
2024-12-08 18:11:27.631966: Epoch time: 91.27 s
2024-12-08 18:11:29.241734: 
2024-12-08 18:11:29.243260: Epoch 70
2024-12-08 18:11:29.244278: Current learning rate: 0.00937
2024-12-08 18:13:00.489401: Validation loss did not improve from -0.59367. Patience: 47/50
2024-12-08 18:13:00.490169: train_loss -0.7275
2024-12-08 18:13:00.491005: val_loss -0.5124
2024-12-08 18:13:00.491724: Pseudo dice [0.7594]
2024-12-08 18:13:00.492484: Epoch time: 91.25 s
2024-12-08 18:13:02.078149: 
2024-12-08 18:13:02.079916: Epoch 71
2024-12-08 18:13:02.080933: Current learning rate: 0.00936
2024-12-08 18:14:33.333127: Validation loss did not improve from -0.59367. Patience: 48/50
2024-12-08 18:14:33.334546: train_loss -0.726
2024-12-08 18:14:33.336316: val_loss -0.5084
2024-12-08 18:14:33.337702: Pseudo dice [0.7516]
2024-12-08 18:14:33.339155: Epoch time: 91.26 s
2024-12-08 18:14:34.625913: 
2024-12-08 18:14:34.627410: Epoch 72
2024-12-08 18:14:34.628250: Current learning rate: 0.00935
2024-12-08 18:16:05.833474: Validation loss did not improve from -0.59367. Patience: 49/50
2024-12-08 18:16:05.834264: train_loss -0.73
2024-12-08 18:16:05.835114: val_loss -0.5563
2024-12-08 18:16:05.835925: Pseudo dice [0.7755]
2024-12-08 18:16:05.836688: Epoch time: 91.21 s
2024-12-08 18:16:07.111104: 
2024-12-08 18:16:07.112688: Epoch 73
2024-12-08 18:16:07.113564: Current learning rate: 0.00934
2024-12-08 18:17:36.827137: Validation loss did not improve from -0.59367. Patience: 50/50
2024-12-08 18:17:36.828471: train_loss -0.7312
2024-12-08 18:17:36.829837: val_loss -0.4893
2024-12-08 18:17:36.830986: Pseudo dice [0.7581]
2024-12-08 18:17:36.832279: Epoch time: 89.72 s
2024-12-08 18:17:38.085828: Patience reached. Stopping training.
2024-12-08 18:17:38.478403: Training done.
2024-12-08 18:17:38.697144: Using splits from existing split file: /home/gridsan/nchutisilp/datasets/nnUNet_Datasets/nnUNet_preprocessed/Dataset308_Sohee_Calcium_OCT_CrossValidation/splits_final.json
2024-12-08 18:17:38.700518: The split file contains 5 splits.
2024-12-08 18:17:38.701805: Desired fold for training: 0
2024-12-08 18:17:38.702893: This split has 6 training and 2 validation cases.
2024-12-08 18:17:38.704150: predicting 106-002
2024-12-08 18:17:38.759640: 106-002, shape torch.Size([1, 539, 498, 498]), rank 0
2024-12-08 18:20:06.214494: predicting 706-005
2024-12-08 18:20:06.238850: 706-005, shape torch.Size([1, 375, 498, 498]), rank 0
2024-12-08 18:21:58.865236: Validation complete
2024-12-08 18:21:58.865745: Mean Validation Dice:  0.7650235989402012
2024-12-08 16:22:23.305054: unpacking done...
2024-12-08 16:22:23.378299: Unable to plot network architecture: nnUNet_compile is enabled!
2024-12-08 16:22:23.445607: 
2024-12-08 16:22:23.447298: Epoch 0
2024-12-08 16:22:23.448907: Current learning rate: 0.01
2024-12-08 16:25:51.792742: Validation loss improved from 1000.00000 to -0.38352! Patience: 0/50
2024-12-08 16:25:51.830646: train_loss -0.3267
2024-12-08 16:25:51.840634: val_loss -0.3835
2024-12-08 16:25:51.841776: Pseudo dice [0.6441]
2024-12-08 16:25:51.842646: Epoch time: 208.35 s
2024-12-08 16:25:51.843293: Yayy! New best EMA pseudo Dice: 0.6441
2024-12-08 16:25:54.438313: 
2024-12-08 16:25:54.439566: Epoch 1
2024-12-08 16:25:54.440451: Current learning rate: 0.00999
2024-12-08 16:27:22.981859: Validation loss improved from -0.38352 to -0.43136! Patience: 0/50
2024-12-08 16:27:22.984887: train_loss -0.4679
2024-12-08 16:27:22.985993: val_loss -0.4314
2024-12-08 16:27:22.986670: Pseudo dice [0.6677]
2024-12-08 16:27:22.987650: Epoch time: 88.55 s
2024-12-08 16:27:22.988390: Yayy! New best EMA pseudo Dice: 0.6465
2024-12-08 16:27:24.581862: 
2024-12-08 16:27:24.583678: Epoch 2
2024-12-08 16:27:24.584754: Current learning rate: 0.00998
2024-12-08 16:28:53.932803: Validation loss improved from -0.43136 to -0.47801! Patience: 0/50
2024-12-08 16:28:53.933697: train_loss -0.5025
2024-12-08 16:28:53.934433: val_loss -0.478
2024-12-08 16:28:53.935253: Pseudo dice [0.6991]
2024-12-08 16:28:53.935977: Epoch time: 89.35 s
2024-12-08 16:28:53.936686: Yayy! New best EMA pseudo Dice: 0.6517
2024-12-08 16:28:55.571728: 
2024-12-08 16:28:55.573370: Epoch 3
2024-12-08 16:28:55.574379: Current learning rate: 0.00997
2024-12-08 16:30:25.265691: Validation loss did not improve from -0.47801. Patience: 1/50
2024-12-08 16:30:25.266878: train_loss -0.5157
2024-12-08 16:30:25.268035: val_loss -0.4666
2024-12-08 16:30:25.268893: Pseudo dice [0.7004]
2024-12-08 16:30:25.269683: Epoch time: 89.7 s
2024-12-08 16:30:25.270513: Yayy! New best EMA pseudo Dice: 0.6566
2024-12-08 16:30:26.898887: 
2024-12-08 16:30:26.900724: Epoch 4
2024-12-08 16:30:26.901978: Current learning rate: 0.00996
2024-12-08 16:31:56.751769: Validation loss improved from -0.47801 to -0.49567! Patience: 1/50
2024-12-08 16:31:56.753918: train_loss -0.5389
2024-12-08 16:31:56.755071: val_loss -0.4957
2024-12-08 16:31:56.755767: Pseudo dice [0.7082]
2024-12-08 16:31:56.756467: Epoch time: 89.86 s
2024-12-08 16:31:57.101888: Yayy! New best EMA pseudo Dice: 0.6618
2024-12-08 16:31:58.790749: 
2024-12-08 16:31:58.792349: Epoch 5
2024-12-08 16:31:58.793127: Current learning rate: 0.00995
2024-12-08 16:33:28.644089: Validation loss did not improve from -0.49567. Patience: 1/50
2024-12-08 16:33:28.645324: train_loss -0.547
2024-12-08 16:33:28.646210: val_loss -0.4795
2024-12-08 16:33:28.647005: Pseudo dice [0.693]
2024-12-08 16:33:28.647749: Epoch time: 89.86 s
2024-12-08 16:33:28.648384: Yayy! New best EMA pseudo Dice: 0.6649
2024-12-08 16:33:30.279773: 
2024-12-08 16:33:30.281233: Epoch 6
2024-12-08 16:33:30.281962: Current learning rate: 0.00995
2024-12-08 16:35:00.197195: Validation loss improved from -0.49567 to -0.50803! Patience: 1/50
2024-12-08 16:35:00.197949: train_loss -0.5586
2024-12-08 16:35:00.198633: val_loss -0.508
2024-12-08 16:35:00.199291: Pseudo dice [0.7241]
2024-12-08 16:35:00.200164: Epoch time: 89.92 s
2024-12-08 16:35:00.201077: Yayy! New best EMA pseudo Dice: 0.6708
2024-12-08 16:35:01.774070: 
2024-12-08 16:35:01.775707: Epoch 7
2024-12-08 16:35:01.776438: Current learning rate: 0.00994
2024-12-08 16:36:31.600283: Validation loss did not improve from -0.50803. Patience: 1/50
2024-12-08 16:36:31.601701: train_loss -0.5743
2024-12-08 16:36:31.602607: val_loss -0.4408
2024-12-08 16:36:31.603370: Pseudo dice [0.6945]
2024-12-08 16:36:31.604033: Epoch time: 89.83 s
2024-12-08 16:36:31.604689: Yayy! New best EMA pseudo Dice: 0.6732
2024-12-08 16:36:33.792200: 
2024-12-08 16:36:33.794072: Epoch 8
2024-12-08 16:36:33.795077: Current learning rate: 0.00993
2024-12-08 16:38:03.903243: Validation loss did not improve from -0.50803. Patience: 2/50
2024-12-08 16:38:03.904459: train_loss -0.5702
2024-12-08 16:38:03.905336: val_loss -0.5014
2024-12-08 16:38:03.906104: Pseudo dice [0.7158]
2024-12-08 16:38:03.906951: Epoch time: 90.11 s
2024-12-08 16:38:03.907727: Yayy! New best EMA pseudo Dice: 0.6774
2024-12-08 16:38:05.568784: 
2024-12-08 16:38:05.570436: Epoch 9
2024-12-08 16:38:05.571316: Current learning rate: 0.00992
2024-12-08 16:39:35.458079: Validation loss did not improve from -0.50803. Patience: 3/50
2024-12-08 16:39:35.459146: train_loss -0.5805
2024-12-08 16:39:35.460073: val_loss -0.4896
2024-12-08 16:39:35.460840: Pseudo dice [0.7114]
2024-12-08 16:39:35.461562: Epoch time: 89.89 s
2024-12-08 16:39:35.809599: Yayy! New best EMA pseudo Dice: 0.6808
2024-12-08 16:39:37.351474: 
2024-12-08 16:39:37.353243: Epoch 10
2024-12-08 16:39:37.354406: Current learning rate: 0.00991
2024-12-08 16:41:07.353976: Validation loss did not improve from -0.50803. Patience: 4/50
2024-12-08 16:41:07.354887: train_loss -0.5866
2024-12-08 16:41:07.355845: val_loss -0.5039
2024-12-08 16:41:07.356594: Pseudo dice [0.7225]
2024-12-08 16:41:07.357396: Epoch time: 90.0 s
2024-12-08 16:41:07.358086: Yayy! New best EMA pseudo Dice: 0.685
2024-12-08 16:41:08.876226: 
2024-12-08 16:41:08.877828: Epoch 11
2024-12-08 16:41:08.878711: Current learning rate: 0.0099
2024-12-08 16:42:38.904137: Validation loss improved from -0.50803 to -0.51586! Patience: 4/50
2024-12-08 16:42:38.905015: train_loss -0.5919
2024-12-08 16:42:38.905760: val_loss -0.5159
2024-12-08 16:42:38.906462: Pseudo dice [0.724]
2024-12-08 16:42:38.907109: Epoch time: 90.03 s
2024-12-08 16:42:38.907705: Yayy! New best EMA pseudo Dice: 0.6889
2024-12-08 16:42:40.521001: 
2024-12-08 16:42:40.522501: Epoch 12
2024-12-08 16:42:40.523243: Current learning rate: 0.00989
2024-12-08 16:44:10.594845: Validation loss did not improve from -0.51586. Patience: 1/50
2024-12-08 16:44:10.595949: train_loss -0.6017
2024-12-08 16:44:10.596983: val_loss -0.4998
2024-12-08 16:44:10.597801: Pseudo dice [0.7142]
2024-12-08 16:44:10.598728: Epoch time: 90.08 s
2024-12-08 16:44:10.599462: Yayy! New best EMA pseudo Dice: 0.6914
2024-12-08 16:44:12.204225: 
2024-12-08 16:44:12.205810: Epoch 13
2024-12-08 16:44:12.206832: Current learning rate: 0.00988
2024-12-08 16:45:42.499850: Validation loss improved from -0.51586 to -0.52420! Patience: 1/50
2024-12-08 16:45:42.500761: train_loss -0.6091
2024-12-08 16:45:42.501626: val_loss -0.5242
2024-12-08 16:45:42.502375: Pseudo dice [0.7336]
2024-12-08 16:45:42.503165: Epoch time: 90.3 s
2024-12-08 16:45:42.503891: Yayy! New best EMA pseudo Dice: 0.6957
2024-12-08 16:45:44.054293: 
2024-12-08 16:45:44.056017: Epoch 14
2024-12-08 16:45:44.057193: Current learning rate: 0.00987
2024-12-08 16:47:14.320138: Validation loss did not improve from -0.52420. Patience: 1/50
2024-12-08 16:47:14.321016: train_loss -0.6149
2024-12-08 16:47:14.321910: val_loss -0.5198
2024-12-08 16:47:14.322694: Pseudo dice [0.7197]
2024-12-08 16:47:14.323489: Epoch time: 90.27 s
2024-12-08 16:47:14.663298: Yayy! New best EMA pseudo Dice: 0.6981
2024-12-08 16:47:16.277603: 
2024-12-08 16:47:16.278919: Epoch 15
2024-12-08 16:47:16.280198: Current learning rate: 0.00986
2024-12-08 16:48:46.345670: Validation loss did not improve from -0.52420. Patience: 2/50
2024-12-08 16:48:46.346809: train_loss -0.623
2024-12-08 16:48:46.347963: val_loss -0.4934
2024-12-08 16:48:46.348881: Pseudo dice [0.7077]
2024-12-08 16:48:46.349888: Epoch time: 90.07 s
2024-12-08 16:48:46.350812: Yayy! New best EMA pseudo Dice: 0.699
2024-12-08 16:48:47.946950: 
2024-12-08 16:48:47.948717: Epoch 16
2024-12-08 16:48:47.949953: Current learning rate: 0.00986
2024-12-08 16:50:18.558843: Validation loss did not improve from -0.52420. Patience: 3/50
2024-12-08 16:50:18.559906: train_loss -0.621
2024-12-08 16:50:18.560826: val_loss -0.5019
2024-12-08 16:50:18.561688: Pseudo dice [0.7146]
2024-12-08 16:50:18.562455: Epoch time: 90.61 s
2024-12-08 16:50:18.563186: Yayy! New best EMA pseudo Dice: 0.7006
2024-12-08 16:50:20.199401: 
2024-12-08 16:50:20.201012: Epoch 17
2024-12-08 16:50:20.202099: Current learning rate: 0.00985
2024-12-08 16:51:50.804805: Validation loss did not improve from -0.52420. Patience: 4/50
2024-12-08 16:51:50.805768: train_loss -0.6278
2024-12-08 16:51:50.806957: val_loss -0.5107
2024-12-08 16:51:50.808020: Pseudo dice [0.7253]
2024-12-08 16:51:50.809091: Epoch time: 90.61 s
2024-12-08 16:51:50.810181: Yayy! New best EMA pseudo Dice: 0.703
2024-12-08 16:51:52.770188: 
2024-12-08 16:51:52.771281: Epoch 18
2024-12-08 16:51:52.772120: Current learning rate: 0.00984
2024-12-08 16:53:23.384047: Validation loss improved from -0.52420 to -0.53800! Patience: 4/50
2024-12-08 16:53:23.385278: train_loss -0.6096
2024-12-08 16:53:23.386306: val_loss -0.538
2024-12-08 16:53:23.387089: Pseudo dice [0.7381]
2024-12-08 16:53:23.387988: Epoch time: 90.62 s
2024-12-08 16:53:23.388756: Yayy! New best EMA pseudo Dice: 0.7065
2024-12-08 16:53:25.022666: 
2024-12-08 16:53:25.024382: Epoch 19
2024-12-08 16:53:25.025309: Current learning rate: 0.00983
2024-12-08 16:54:55.519425: Validation loss did not improve from -0.53800. Patience: 1/50
2024-12-08 16:54:55.520898: train_loss -0.6295
2024-12-08 16:54:55.521780: val_loss -0.5345
2024-12-08 16:54:55.522597: Pseudo dice [0.7374]
2024-12-08 16:54:55.523260: Epoch time: 90.5 s
2024-12-08 16:54:55.865936: Yayy! New best EMA pseudo Dice: 0.7096
2024-12-08 16:54:57.462225: 
2024-12-08 16:54:57.464076: Epoch 20
2024-12-08 16:54:57.464906: Current learning rate: 0.00982
2024-12-08 16:56:27.829906: Validation loss did not improve from -0.53800. Patience: 2/50
2024-12-08 16:56:27.831000: train_loss -0.634
2024-12-08 16:56:27.831940: val_loss -0.5211
2024-12-08 16:56:27.832715: Pseudo dice [0.7325]
2024-12-08 16:56:27.833494: Epoch time: 90.37 s
2024-12-08 16:56:27.834175: Yayy! New best EMA pseudo Dice: 0.7119
2024-12-08 16:56:29.449784: 
2024-12-08 16:56:29.450724: Epoch 21
2024-12-08 16:56:29.451710: Current learning rate: 0.00981
2024-12-08 16:57:59.911265: Validation loss did not improve from -0.53800. Patience: 3/50
2024-12-08 16:57:59.912205: train_loss -0.6292
2024-12-08 16:57:59.912970: val_loss -0.5186
2024-12-08 16:57:59.913719: Pseudo dice [0.7253]
2024-12-08 16:57:59.914651: Epoch time: 90.46 s
2024-12-08 16:57:59.915349: Yayy! New best EMA pseudo Dice: 0.7133
2024-12-08 16:58:01.514513: 
2024-12-08 16:58:01.516636: Epoch 22
2024-12-08 16:58:01.517793: Current learning rate: 0.0098
2024-12-08 16:59:31.869612: Validation loss did not improve from -0.53800. Patience: 4/50
2024-12-08 16:59:31.870822: train_loss -0.6356
2024-12-08 16:59:31.871787: val_loss -0.5226
2024-12-08 16:59:31.872571: Pseudo dice [0.7312]
2024-12-08 16:59:31.873325: Epoch time: 90.36 s
2024-12-08 16:59:31.873995: Yayy! New best EMA pseudo Dice: 0.7151
2024-12-08 16:59:33.431863: 
2024-12-08 16:59:33.433437: Epoch 23
2024-12-08 16:59:33.434490: Current learning rate: 0.00979
2024-12-08 17:01:03.756928: Validation loss improved from -0.53800 to -0.56163! Patience: 4/50
2024-12-08 17:01:03.757809: train_loss -0.637
2024-12-08 17:01:03.758787: val_loss -0.5616
2024-12-08 17:01:03.759507: Pseudo dice [0.7448]
2024-12-08 17:01:03.760357: Epoch time: 90.33 s
2024-12-08 17:01:03.760997: Yayy! New best EMA pseudo Dice: 0.718
2024-12-08 17:01:05.284683: 
2024-12-08 17:01:05.286742: Epoch 24
2024-12-08 17:01:05.287683: Current learning rate: 0.00978
2024-12-08 17:02:36.013238: Validation loss did not improve from -0.56163. Patience: 1/50
2024-12-08 17:02:36.014449: train_loss -0.6386
2024-12-08 17:02:36.015623: val_loss -0.5108
2024-12-08 17:02:36.016770: Pseudo dice [0.7183]
2024-12-08 17:02:36.017693: Epoch time: 90.73 s
2024-12-08 17:02:36.356098: Yayy! New best EMA pseudo Dice: 0.7181
2024-12-08 17:02:37.869778: 
2024-12-08 17:02:37.871558: Epoch 25
2024-12-08 17:02:37.872431: Current learning rate: 0.00977
2024-12-08 17:04:08.585546: Validation loss did not improve from -0.56163. Patience: 2/50
2024-12-08 17:04:08.586525: train_loss -0.642
2024-12-08 17:04:08.587480: val_loss -0.5382
2024-12-08 17:04:08.588422: Pseudo dice [0.7424]
2024-12-08 17:04:08.589383: Epoch time: 90.72 s
2024-12-08 17:04:08.590243: Yayy! New best EMA pseudo Dice: 0.7205
2024-12-08 17:04:10.138988: 
2024-12-08 17:04:10.140369: Epoch 26
2024-12-08 17:04:10.141313: Current learning rate: 0.00977
2024-12-08 17:05:40.902926: Validation loss did not improve from -0.56163. Patience: 3/50
2024-12-08 17:05:40.904001: train_loss -0.6334
2024-12-08 17:05:40.904962: val_loss -0.5449
2024-12-08 17:05:40.905724: Pseudo dice [0.7407]
2024-12-08 17:05:40.906722: Epoch time: 90.77 s
2024-12-08 17:05:40.907503: Yayy! New best EMA pseudo Dice: 0.7225
2024-12-08 17:05:42.510353: 
2024-12-08 17:05:42.511483: Epoch 27
2024-12-08 17:05:42.512789: Current learning rate: 0.00976
2024-12-08 17:07:13.244761: Validation loss did not improve from -0.56163. Patience: 4/50
2024-12-08 17:07:13.245866: train_loss -0.6512
2024-12-08 17:07:13.246818: val_loss -0.5582
2024-12-08 17:07:13.247666: Pseudo dice [0.753]
2024-12-08 17:07:13.248400: Epoch time: 90.74 s
2024-12-08 17:07:13.249087: Yayy! New best EMA pseudo Dice: 0.7256
2024-12-08 17:07:14.778524: 
2024-12-08 17:07:14.780254: Epoch 28
2024-12-08 17:07:14.781126: Current learning rate: 0.00975
2024-12-08 17:08:45.646466: Validation loss did not improve from -0.56163. Patience: 5/50
2024-12-08 17:08:45.647228: train_loss -0.6509
2024-12-08 17:08:45.648049: val_loss -0.552
2024-12-08 17:08:45.648918: Pseudo dice [0.746]
2024-12-08 17:08:45.649814: Epoch time: 90.87 s
2024-12-08 17:08:45.650703: Yayy! New best EMA pseudo Dice: 0.7276
2024-12-08 17:08:47.499597: 
2024-12-08 17:08:47.501510: Epoch 29
2024-12-08 17:08:47.502254: Current learning rate: 0.00974
2024-12-08 17:10:18.417412: Validation loss did not improve from -0.56163. Patience: 6/50
2024-12-08 17:10:18.418956: train_loss -0.6581
2024-12-08 17:10:18.419940: val_loss -0.5414
2024-12-08 17:10:18.420782: Pseudo dice [0.7436]
2024-12-08 17:10:18.421605: Epoch time: 90.92 s
2024-12-08 17:10:18.764387: Yayy! New best EMA pseudo Dice: 0.7292
2024-12-08 17:10:20.308345: 
2024-12-08 17:10:20.309861: Epoch 30
2024-12-08 17:10:20.311003: Current learning rate: 0.00973
2024-12-08 17:11:50.962527: Validation loss did not improve from -0.56163. Patience: 7/50
2024-12-08 17:11:50.963915: train_loss -0.6633
2024-12-08 17:11:50.964871: val_loss -0.5616
2024-12-08 17:11:50.965614: Pseudo dice [0.7485]
2024-12-08 17:11:50.966489: Epoch time: 90.66 s
2024-12-08 17:11:50.967234: Yayy! New best EMA pseudo Dice: 0.7311
2024-12-08 17:11:52.547826: 
2024-12-08 17:11:52.549088: Epoch 31
2024-12-08 17:11:52.550027: Current learning rate: 0.00972
2024-12-08 17:13:23.089504: Validation loss did not improve from -0.56163. Patience: 8/50
2024-12-08 17:13:23.090628: train_loss -0.6586
2024-12-08 17:13:23.091657: val_loss -0.5514
2024-12-08 17:13:23.092336: Pseudo dice [0.7455]
2024-12-08 17:13:23.093250: Epoch time: 90.54 s
2024-12-08 17:13:23.093918: Yayy! New best EMA pseudo Dice: 0.7326
2024-12-08 17:13:24.667855: 
2024-12-08 17:13:24.669610: Epoch 32
2024-12-08 17:13:24.670490: Current learning rate: 0.00971
2024-12-08 17:14:55.414620: Validation loss did not improve from -0.56163. Patience: 9/50
2024-12-08 17:14:55.415912: train_loss -0.6627
2024-12-08 17:14:55.417091: val_loss -0.5538
2024-12-08 17:14:55.417930: Pseudo dice [0.7426]
2024-12-08 17:14:55.418786: Epoch time: 90.75 s
2024-12-08 17:14:55.419609: Yayy! New best EMA pseudo Dice: 0.7336
2024-12-08 17:14:57.034410: 
2024-12-08 17:14:57.035941: Epoch 33
2024-12-08 17:14:57.037303: Current learning rate: 0.0097
2024-12-08 17:16:27.762382: Validation loss did not improve from -0.56163. Patience: 10/50
2024-12-08 17:16:27.763526: train_loss -0.6653
2024-12-08 17:16:27.764511: val_loss -0.5533
2024-12-08 17:16:27.765370: Pseudo dice [0.7488]
2024-12-08 17:16:27.766135: Epoch time: 90.73 s
2024-12-08 17:16:27.766938: Yayy! New best EMA pseudo Dice: 0.7351
2024-12-08 17:16:29.415950: 
2024-12-08 17:16:29.417588: Epoch 34
2024-12-08 17:16:29.418746: Current learning rate: 0.00969
2024-12-08 17:17:59.723576: Validation loss improved from -0.56163 to -0.56603! Patience: 10/50
2024-12-08 17:17:59.724832: train_loss -0.6698
2024-12-08 17:17:59.726014: val_loss -0.566
2024-12-08 17:17:59.726933: Pseudo dice [0.7531]
2024-12-08 17:17:59.727914: Epoch time: 90.31 s
2024-12-08 17:18:00.078008: Yayy! New best EMA pseudo Dice: 0.7369
2024-12-08 17:18:01.706051: 
2024-12-08 17:18:01.707620: Epoch 35
2024-12-08 17:18:01.708666: Current learning rate: 0.00968
2024-12-08 17:19:31.894188: Validation loss did not improve from -0.56603. Patience: 1/50
2024-12-08 17:19:31.895114: train_loss -0.6776
2024-12-08 17:19:31.896079: val_loss -0.56
2024-12-08 17:19:31.896808: Pseudo dice [0.7535]
2024-12-08 17:19:31.897445: Epoch time: 90.19 s
2024-12-08 17:19:31.898192: Yayy! New best EMA pseudo Dice: 0.7386
2024-12-08 17:19:33.574710: 
2024-12-08 17:19:33.576489: Epoch 36
2024-12-08 17:19:33.577439: Current learning rate: 0.00968
2024-12-08 17:21:03.750364: Validation loss did not improve from -0.56603. Patience: 2/50
2024-12-08 17:21:03.751391: train_loss -0.6774
2024-12-08 17:21:03.752388: val_loss -0.5451
2024-12-08 17:21:03.753105: Pseudo dice [0.744]
2024-12-08 17:21:03.753779: Epoch time: 90.18 s
2024-12-08 17:21:03.754409: Yayy! New best EMA pseudo Dice: 0.7391
2024-12-08 17:21:05.372167: 
2024-12-08 17:21:05.374196: Epoch 37
2024-12-08 17:21:05.374982: Current learning rate: 0.00967
2024-12-08 17:22:35.567166: Validation loss did not improve from -0.56603. Patience: 3/50
2024-12-08 17:22:35.568259: train_loss -0.6703
2024-12-08 17:22:35.569314: val_loss -0.5452
2024-12-08 17:22:35.570436: Pseudo dice [0.7365]
2024-12-08 17:22:35.571446: Epoch time: 90.2 s
2024-12-08 17:22:36.862219: 
2024-12-08 17:22:36.863928: Epoch 38
2024-12-08 17:22:36.864887: Current learning rate: 0.00966
2024-12-08 17:24:07.056085: Validation loss did not improve from -0.56603. Patience: 4/50
2024-12-08 17:24:07.057084: train_loss -0.6756
2024-12-08 17:24:07.058146: val_loss -0.5547
2024-12-08 17:24:07.058847: Pseudo dice [0.7548]
2024-12-08 17:24:07.059711: Epoch time: 90.2 s
2024-12-08 17:24:07.060556: Yayy! New best EMA pseudo Dice: 0.7404
2024-12-08 17:24:08.981064: 
2024-12-08 17:24:08.982533: Epoch 39
2024-12-08 17:24:08.983317: Current learning rate: 0.00965
2024-12-08 17:25:39.162612: Validation loss did not improve from -0.56603. Patience: 5/50
2024-12-08 17:25:39.163690: train_loss -0.6791
2024-12-08 17:25:39.164609: val_loss -0.5645
2024-12-08 17:25:39.165385: Pseudo dice [0.7514]
2024-12-08 17:25:39.166073: Epoch time: 90.18 s
2024-12-08 17:25:39.537929: Yayy! New best EMA pseudo Dice: 0.7415
2024-12-08 17:25:41.220750: 
2024-12-08 17:25:41.221858: Epoch 40
2024-12-08 17:25:41.222621: Current learning rate: 0.00964
2024-12-08 17:27:11.350345: Validation loss did not improve from -0.56603. Patience: 6/50
2024-12-08 17:27:11.351358: train_loss -0.6898
2024-12-08 17:27:11.352261: val_loss -0.5473
2024-12-08 17:27:11.352952: Pseudo dice [0.7448]
2024-12-08 17:27:11.353718: Epoch time: 90.13 s
2024-12-08 17:27:11.354559: Yayy! New best EMA pseudo Dice: 0.7419
2024-12-08 17:27:12.985576: 
2024-12-08 17:27:12.987388: Epoch 41
2024-12-08 17:27:12.988252: Current learning rate: 0.00963
2024-12-08 17:28:43.114291: Validation loss improved from -0.56603 to -0.57584! Patience: 6/50
2024-12-08 17:28:43.115370: train_loss -0.6924
2024-12-08 17:28:43.116551: val_loss -0.5758
2024-12-08 17:28:43.117242: Pseudo dice [0.7676]
2024-12-08 17:28:43.118072: Epoch time: 90.13 s
2024-12-08 17:28:43.118807: Yayy! New best EMA pseudo Dice: 0.7444
2024-12-08 17:28:44.689389: 
2024-12-08 17:28:44.690678: Epoch 42
2024-12-08 17:28:44.691374: Current learning rate: 0.00962
2024-12-08 17:30:15.100670: Validation loss did not improve from -0.57584. Patience: 1/50
2024-12-08 17:30:15.101873: train_loss -0.6868
2024-12-08 17:30:15.102938: val_loss -0.5486
2024-12-08 17:30:15.103812: Pseudo dice [0.7412]
2024-12-08 17:30:15.104417: Epoch time: 90.41 s
2024-12-08 17:30:16.357195: 
2024-12-08 17:30:16.358934: Epoch 43
2024-12-08 17:30:16.359947: Current learning rate: 0.00961
2024-12-08 17:31:46.803102: Validation loss did not improve from -0.57584. Patience: 2/50
2024-12-08 17:31:46.804394: train_loss -0.6856
2024-12-08 17:31:46.805558: val_loss -0.5393
2024-12-08 17:31:46.806537: Pseudo dice [0.7294]
2024-12-08 17:31:46.807615: Epoch time: 90.45 s
2024-12-08 17:31:48.075742: 
2024-12-08 17:31:48.077100: Epoch 44
2024-12-08 17:31:48.077875: Current learning rate: 0.0096
2024-12-08 17:33:18.390724: Validation loss did not improve from -0.57584. Patience: 3/50
2024-12-08 17:33:18.391720: train_loss -0.6886
2024-12-08 17:33:18.392644: val_loss -0.5606
2024-12-08 17:33:18.393426: Pseudo dice [0.7491]
2024-12-08 17:33:18.394146: Epoch time: 90.32 s
2024-12-08 17:33:20.019152: 
2024-12-08 17:33:20.020936: Epoch 45
2024-12-08 17:33:20.021768: Current learning rate: 0.00959
2024-12-08 17:34:50.305968: Validation loss improved from -0.57584 to -0.57852! Patience: 3/50
2024-12-08 17:34:50.306839: train_loss -0.6925
2024-12-08 17:34:50.307913: val_loss -0.5785
2024-12-08 17:34:50.308679: Pseudo dice [0.7536]
2024-12-08 17:34:50.309391: Epoch time: 90.29 s
2024-12-08 17:34:51.523218: 
2024-12-08 17:34:51.524652: Epoch 46
2024-12-08 17:34:51.525652: Current learning rate: 0.00959
2024-12-08 17:36:21.658745: Validation loss did not improve from -0.57852. Patience: 1/50
2024-12-08 17:36:21.660039: train_loss -0.6902
2024-12-08 17:36:21.661143: val_loss -0.5419
2024-12-08 17:36:21.661930: Pseudo dice [0.7385]
2024-12-08 17:36:21.662694: Epoch time: 90.14 s
2024-12-08 17:36:22.917752: 
2024-12-08 17:36:22.919328: Epoch 47
2024-12-08 17:36:22.920151: Current learning rate: 0.00958
2024-12-08 17:37:52.808346: Validation loss did not improve from -0.57852. Patience: 2/50
2024-12-08 17:37:52.809333: train_loss -0.6874
2024-12-08 17:37:52.810310: val_loss -0.5343
2024-12-08 17:37:52.810976: Pseudo dice [0.7347]
2024-12-08 17:37:52.811636: Epoch time: 89.89 s
2024-12-08 17:37:54.086437: 
2024-12-08 17:37:54.088042: Epoch 48
2024-12-08 17:37:54.088827: Current learning rate: 0.00957
2024-12-08 17:39:24.027373: Validation loss did not improve from -0.57852. Patience: 3/50
2024-12-08 17:39:24.028125: train_loss -0.6927
2024-12-08 17:39:24.028835: val_loss -0.5534
2024-12-08 17:39:24.029635: Pseudo dice [0.7477]
2024-12-08 17:39:24.030375: Epoch time: 89.94 s
2024-12-08 17:39:25.242647: 
2024-12-08 17:39:25.244188: Epoch 49
2024-12-08 17:39:25.245204: Current learning rate: 0.00956
2024-12-08 17:40:55.063335: Validation loss did not improve from -0.57852. Patience: 4/50
2024-12-08 17:40:55.064519: train_loss -0.6999
2024-12-08 17:40:55.065386: val_loss -0.5593
2024-12-08 17:40:55.066194: Pseudo dice [0.7486]
2024-12-08 17:40:55.067137: Epoch time: 89.82 s
2024-12-08 17:40:56.994056: 
2024-12-08 17:40:56.996002: Epoch 50
2024-12-08 17:40:56.997101: Current learning rate: 0.00955
2024-12-08 17:42:26.866619: Validation loss did not improve from -0.57852. Patience: 5/50
2024-12-08 17:42:26.867782: train_loss -0.7004
2024-12-08 17:42:26.868673: val_loss -0.5244
2024-12-08 17:42:26.869389: Pseudo dice [0.7215]
2024-12-08 17:42:26.870077: Epoch time: 89.87 s
2024-12-08 17:42:28.113501: 
2024-12-08 17:42:28.115618: Epoch 51
2024-12-08 17:42:28.116775: Current learning rate: 0.00954
2024-12-08 17:43:58.034978: Validation loss did not improve from -0.57852. Patience: 6/50
2024-12-08 17:43:58.036208: train_loss -0.69
2024-12-08 17:43:58.037492: val_loss -0.5429
2024-12-08 17:43:58.038428: Pseudo dice [0.7411]
2024-12-08 17:43:58.039254: Epoch time: 89.92 s
2024-12-08 17:43:59.291842: 
2024-12-08 17:43:59.293373: Epoch 52
2024-12-08 17:43:59.294400: Current learning rate: 0.00953
2024-12-08 17:45:29.245885: Validation loss did not improve from -0.57852. Patience: 7/50
2024-12-08 17:45:29.246901: train_loss -0.6884
2024-12-08 17:45:29.247935: val_loss -0.5206
2024-12-08 17:45:29.248722: Pseudo dice [0.7344]
2024-12-08 17:45:29.249567: Epoch time: 89.96 s
2024-12-08 17:45:30.510586: 
2024-12-08 17:45:30.512367: Epoch 53
2024-12-08 17:45:30.513348: Current learning rate: 0.00952
2024-12-08 17:47:00.267648: Validation loss did not improve from -0.57852. Patience: 8/50
2024-12-08 17:47:00.268697: train_loss -0.7031
2024-12-08 17:47:00.269758: val_loss -0.5622
2024-12-08 17:47:00.270433: Pseudo dice [0.746]
2024-12-08 17:47:00.271102: Epoch time: 89.76 s
2024-12-08 17:47:01.553296: 
2024-12-08 17:47:01.554723: Epoch 54
2024-12-08 17:47:01.555651: Current learning rate: 0.00951
2024-12-08 17:48:31.571927: Validation loss did not improve from -0.57852. Patience: 9/50
2024-12-08 17:48:31.572871: train_loss -0.7043
2024-12-08 17:48:31.573829: val_loss -0.5722
2024-12-08 17:48:31.574479: Pseudo dice [0.7556]
2024-12-08 17:48:31.575282: Epoch time: 90.02 s
2024-12-08 17:48:33.136630: 
2024-12-08 17:48:33.138601: Epoch 55
2024-12-08 17:48:33.139561: Current learning rate: 0.0095
2024-12-08 17:50:03.221385: Validation loss did not improve from -0.57852. Patience: 10/50
2024-12-08 17:50:03.222582: train_loss -0.7022
2024-12-08 17:50:03.223811: val_loss -0.5646
2024-12-08 17:50:03.224764: Pseudo dice [0.7553]
2024-12-08 17:50:03.225758: Epoch time: 90.09 s
2024-12-08 17:50:04.462755: 
2024-12-08 17:50:04.464828: Epoch 56
2024-12-08 17:50:04.465867: Current learning rate: 0.00949
2024-12-08 17:51:34.507608: Validation loss did not improve from -0.57852. Patience: 11/50
2024-12-08 17:51:34.508425: train_loss -0.7083
2024-12-08 17:51:34.509220: val_loss -0.5165
2024-12-08 17:51:34.509902: Pseudo dice [0.7304]
2024-12-08 17:51:34.510755: Epoch time: 90.05 s
2024-12-08 17:51:35.790000: 
2024-12-08 17:51:35.791478: Epoch 57
2024-12-08 17:51:35.792268: Current learning rate: 0.00949
2024-12-08 17:53:05.895711: Validation loss did not improve from -0.57852. Patience: 12/50
2024-12-08 17:53:05.896878: train_loss -0.701
2024-12-08 17:53:05.897676: val_loss -0.5575
2024-12-08 17:53:05.898367: Pseudo dice [0.7501]
2024-12-08 17:53:05.899188: Epoch time: 90.11 s
2024-12-08 17:53:07.216633: 
2024-12-08 17:53:07.218163: Epoch 58
2024-12-08 17:53:07.218892: Current learning rate: 0.00948
2024-12-08 17:54:37.306000: Validation loss did not improve from -0.57852. Patience: 13/50
2024-12-08 17:54:37.307026: train_loss -0.7003
2024-12-08 17:54:37.308047: val_loss -0.5576
2024-12-08 17:54:37.308894: Pseudo dice [0.7484]
2024-12-08 17:54:37.309706: Epoch time: 90.09 s
2024-12-08 17:54:38.592322: 
2024-12-08 17:54:38.593819: Epoch 59
2024-12-08 17:54:38.594978: Current learning rate: 0.00947
2024-12-08 17:56:08.665077: Validation loss did not improve from -0.57852. Patience: 14/50
2024-12-08 17:56:08.666044: train_loss -0.7089
2024-12-08 17:56:08.666853: val_loss -0.5626
2024-12-08 17:56:08.667649: Pseudo dice [0.7504]
2024-12-08 17:56:08.668547: Epoch time: 90.07 s
2024-12-08 17:56:09.016566: Yayy! New best EMA pseudo Dice: 0.7446
2024-12-08 17:56:10.977775: 
2024-12-08 17:56:10.979419: Epoch 60
2024-12-08 17:56:10.980198: Current learning rate: 0.00946
2024-12-08 17:57:41.072447: Validation loss did not improve from -0.57852. Patience: 15/50
2024-12-08 17:57:41.073354: train_loss -0.7099
2024-12-08 17:57:41.074221: val_loss -0.5661
2024-12-08 17:57:41.074974: Pseudo dice [0.7537]
2024-12-08 17:57:41.075722: Epoch time: 90.1 s
2024-12-08 17:57:41.076466: Yayy! New best EMA pseudo Dice: 0.7455
2024-12-08 17:57:42.659556: 
2024-12-08 17:57:42.661436: Epoch 61
2024-12-08 17:57:42.662432: Current learning rate: 0.00945
2024-12-08 17:59:12.685803: Validation loss did not improve from -0.57852. Patience: 16/50
2024-12-08 17:59:12.686896: train_loss -0.71
2024-12-08 17:59:12.687866: val_loss -0.5462
2024-12-08 17:59:12.688725: Pseudo dice [0.7444]
2024-12-08 17:59:12.689453: Epoch time: 90.03 s
2024-12-08 17:59:13.965294: 
2024-12-08 17:59:13.967326: Epoch 62
2024-12-08 17:59:13.968385: Current learning rate: 0.00944
2024-12-08 18:00:44.124180: Validation loss did not improve from -0.57852. Patience: 17/50
2024-12-08 18:00:44.125352: train_loss -0.7071
2024-12-08 18:00:44.126251: val_loss -0.5542
2024-12-08 18:00:44.126913: Pseudo dice [0.7445]
2024-12-08 18:00:44.127842: Epoch time: 90.16 s
2024-12-08 18:00:45.432968: 
2024-12-08 18:00:45.434237: Epoch 63
2024-12-08 18:00:45.435022: Current learning rate: 0.00943
2024-12-08 18:02:15.855401: Validation loss did not improve from -0.57852. Patience: 18/50
2024-12-08 18:02:15.856870: train_loss -0.7068
2024-12-08 18:02:15.857960: val_loss -0.5767
2024-12-08 18:02:15.858629: Pseudo dice [0.764]
2024-12-08 18:02:15.859314: Epoch time: 90.42 s
2024-12-08 18:02:15.859968: Yayy! New best EMA pseudo Dice: 0.7472
2024-12-08 18:02:17.622591: 
2024-12-08 18:02:17.624529: Epoch 64
2024-12-08 18:02:17.625299: Current learning rate: 0.00942
2024-12-08 18:03:48.047397: Validation loss did not improve from -0.57852. Patience: 19/50
2024-12-08 18:03:48.048432: train_loss -0.7079
2024-12-08 18:03:48.049260: val_loss -0.5726
2024-12-08 18:03:48.050132: Pseudo dice [0.7604]
2024-12-08 18:03:48.050882: Epoch time: 90.43 s
2024-12-08 18:03:48.418660: Yayy! New best EMA pseudo Dice: 0.7485
2024-12-08 18:03:50.041202: 
2024-12-08 18:03:50.042359: Epoch 65
2024-12-08 18:03:50.043293: Current learning rate: 0.00941
2024-12-08 18:05:20.610281: Validation loss did not improve from -0.57852. Patience: 20/50
2024-12-08 18:05:20.611803: train_loss -0.7097
2024-12-08 18:05:20.612868: val_loss -0.5411
2024-12-08 18:05:20.613659: Pseudo dice [0.746]
2024-12-08 18:05:20.614478: Epoch time: 90.57 s
2024-12-08 18:05:21.877225: 
2024-12-08 18:05:21.879063: Epoch 66
2024-12-08 18:05:21.880255: Current learning rate: 0.0094
2024-12-08 18:06:52.546605: Validation loss did not improve from -0.57852. Patience: 21/50
2024-12-08 18:06:52.547341: train_loss -0.7122
2024-12-08 18:06:52.548360: val_loss -0.5304
2024-12-08 18:06:52.549371: Pseudo dice [0.7363]
2024-12-08 18:06:52.550286: Epoch time: 90.67 s
2024-12-08 18:06:53.834389: 
2024-12-08 18:06:53.836200: Epoch 67
2024-12-08 18:06:53.837450: Current learning rate: 0.00939
2024-12-08 18:08:24.315112: Validation loss did not improve from -0.57852. Patience: 22/50
2024-12-08 18:08:24.315813: train_loss -0.7105
2024-12-08 18:08:24.316527: val_loss -0.5476
2024-12-08 18:08:24.317276: Pseudo dice [0.7439]
2024-12-08 18:08:24.318097: Epoch time: 90.48 s
2024-12-08 18:08:25.598529: 
2024-12-08 18:08:25.599684: Epoch 68
2024-12-08 18:08:25.600803: Current learning rate: 0.00939
2024-12-08 18:09:56.182669: Validation loss did not improve from -0.57852. Patience: 23/50
2024-12-08 18:09:56.183928: train_loss -0.7133
2024-12-08 18:09:56.185294: val_loss -0.5653
2024-12-08 18:09:56.186362: Pseudo dice [0.7524]
2024-12-08 18:09:56.187423: Epoch time: 90.59 s
2024-12-08 18:09:57.504901: 
2024-12-08 18:09:57.506867: Epoch 69
2024-12-08 18:09:57.507829: Current learning rate: 0.00938
2024-12-08 18:11:28.038723: Validation loss did not improve from -0.57852. Patience: 24/50
2024-12-08 18:11:28.039857: train_loss -0.7175
2024-12-08 18:11:28.040721: val_loss -0.5519
2024-12-08 18:11:28.041502: Pseudo dice [0.7421]
2024-12-08 18:11:28.042242: Epoch time: 90.54 s
2024-12-08 18:11:29.650574: 
2024-12-08 18:11:29.652032: Epoch 70
2024-12-08 18:11:29.653057: Current learning rate: 0.00937
2024-12-08 18:13:00.167665: Validation loss did not improve from -0.57852. Patience: 25/50
2024-12-08 18:13:00.168834: train_loss -0.7168
2024-12-08 18:13:00.169784: val_loss -0.5584
2024-12-08 18:13:00.170656: Pseudo dice [0.7463]
2024-12-08 18:13:00.171463: Epoch time: 90.52 s
2024-12-08 18:13:01.856723: 
2024-12-08 18:13:01.857923: Epoch 71
2024-12-08 18:13:01.858596: Current learning rate: 0.00936
2024-12-08 18:14:32.542916: Validation loss did not improve from -0.57852. Patience: 26/50
2024-12-08 18:14:32.543799: train_loss -0.7099
2024-12-08 18:14:32.544762: val_loss -0.5593
2024-12-08 18:14:32.545463: Pseudo dice [0.748]
2024-12-08 18:14:32.546256: Epoch time: 90.69 s
2024-12-08 18:14:33.832581: 
2024-12-08 18:14:33.834552: Epoch 72
2024-12-08 18:14:33.835557: Current learning rate: 0.00935
2024-12-08 18:16:04.475914: Validation loss did not improve from -0.57852. Patience: 27/50
2024-12-08 18:16:04.477127: train_loss -0.7142
2024-12-08 18:16:04.477958: val_loss -0.5607
2024-12-08 18:16:04.478775: Pseudo dice [0.755]
2024-12-08 18:16:04.479474: Epoch time: 90.65 s
2024-12-08 18:16:05.832935: 
2024-12-08 18:16:05.834521: Epoch 73
2024-12-08 18:16:05.835783: Current learning rate: 0.00934
2024-12-08 18:17:36.454787: Validation loss did not improve from -0.57852. Patience: 28/50
2024-12-08 18:17:36.456046: train_loss -0.7223
2024-12-08 18:17:36.457224: val_loss -0.5199
2024-12-08 18:17:36.458122: Pseudo dice [0.7244]
2024-12-08 18:17:36.459062: Epoch time: 90.62 s
2024-12-08 18:17:37.736543: 
2024-12-08 18:17:37.738549: Epoch 74
2024-12-08 18:17:37.739983: Current learning rate: 0.00933
2024-12-08 18:19:08.156420: Validation loss did not improve from -0.57852. Patience: 29/50
2024-12-08 18:19:08.157402: train_loss -0.7195
2024-12-08 18:19:08.158317: val_loss -0.5571
2024-12-08 18:19:08.159036: Pseudo dice [0.7499]
2024-12-08 18:19:08.159817: Epoch time: 90.42 s
2024-12-08 18:19:09.839309: 
2024-12-08 18:19:09.840993: Epoch 75
2024-12-08 18:19:09.842250: Current learning rate: 0.00932
2024-12-08 18:20:40.097228: Validation loss did not improve from -0.57852. Patience: 30/50
2024-12-08 18:20:40.098553: train_loss -0.7164
2024-12-08 18:20:40.099803: val_loss -0.53
2024-12-08 18:20:40.100649: Pseudo dice [0.7343]
2024-12-08 18:20:40.101451: Epoch time: 90.26 s
2024-12-08 18:20:41.349480: 
2024-12-08 18:20:41.351038: Epoch 76
2024-12-08 18:20:41.352031: Current learning rate: 0.00931
2024-12-08 18:22:11.616889: Validation loss did not improve from -0.57852. Patience: 31/50
2024-12-08 18:22:11.618196: train_loss -0.7214
2024-12-08 18:22:11.619461: val_loss -0.5286
2024-12-08 18:22:11.620609: Pseudo dice [0.7372]
2024-12-08 18:22:11.621769: Epoch time: 90.27 s
2024-12-08 18:22:12.874714: 
2024-12-08 18:22:12.876861: Epoch 77
2024-12-08 18:22:12.878167: Current learning rate: 0.0093
2024-12-08 18:23:43.114072: Validation loss improved from -0.57852 to -0.57932! Patience: 31/50
2024-12-08 18:23:43.115237: train_loss -0.724
2024-12-08 18:23:43.116592: val_loss -0.5793
2024-12-08 18:23:43.117544: Pseudo dice [0.7662]
2024-12-08 18:23:43.118400: Epoch time: 90.24 s
2024-12-08 18:23:44.379300: 
2024-12-08 18:23:44.381293: Epoch 78
2024-12-08 18:23:44.382517: Current learning rate: 0.0093
2024-12-08 18:25:14.587338: Validation loss did not improve from -0.57932. Patience: 1/50
2024-12-08 18:25:14.588531: train_loss -0.7211
2024-12-08 18:25:14.589703: val_loss -0.5642
2024-12-08 18:25:14.590479: Pseudo dice [0.7524]
2024-12-08 18:25:14.591225: Epoch time: 90.21 s
2024-12-08 18:25:15.830348: 
2024-12-08 18:25:15.832026: Epoch 79
2024-12-08 18:25:15.833211: Current learning rate: 0.00929
2024-12-08 18:26:45.882500: Validation loss did not improve from -0.57932. Patience: 2/50
2024-12-08 18:26:45.883814: train_loss -0.7189
2024-12-08 18:26:45.885013: val_loss -0.5583
2024-12-08 18:26:45.885913: Pseudo dice [0.7517]
2024-12-08 18:26:45.886728: Epoch time: 90.05 s
2024-12-08 18:26:47.468244: 
2024-12-08 18:26:47.470752: Epoch 80
2024-12-08 18:26:47.471994: Current learning rate: 0.00928
2024-12-08 18:28:17.307007: Validation loss did not improve from -0.57932. Patience: 3/50
2024-12-08 18:28:17.308332: train_loss -0.7194
2024-12-08 18:28:17.309300: val_loss -0.546
2024-12-08 18:28:17.309979: Pseudo dice [0.7444]
2024-12-08 18:28:17.310758: Epoch time: 89.84 s
2024-12-08 18:28:18.880361: 
2024-12-08 18:28:18.882200: Epoch 81
2024-12-08 18:28:18.883272: Current learning rate: 0.00927
2024-12-08 18:29:48.668948: Validation loss did not improve from -0.57932. Patience: 4/50
2024-12-08 18:29:48.670340: train_loss -0.7234
2024-12-08 18:29:48.671716: val_loss -0.5342
2024-12-08 18:29:48.672623: Pseudo dice [0.7367]
2024-12-08 18:29:48.673487: Epoch time: 89.79 s
2024-12-08 18:29:49.914708: 
2024-12-08 18:29:49.917120: Epoch 82
2024-12-08 18:29:49.918400: Current learning rate: 0.00926
2024-12-08 18:31:19.820101: Validation loss did not improve from -0.57932. Patience: 5/50
2024-12-08 18:31:19.820993: train_loss -0.7321
2024-12-08 18:31:19.822040: val_loss -0.5691
2024-12-08 18:31:19.822742: Pseudo dice [0.7531]
2024-12-08 18:31:19.823479: Epoch time: 89.91 s
2024-12-08 18:31:21.000242: 
2024-12-08 18:31:21.002040: Epoch 83
2024-12-08 18:31:21.003287: Current learning rate: 0.00925
2024-12-08 18:32:51.128135: Validation loss did not improve from -0.57932. Patience: 6/50
2024-12-08 18:32:51.129606: train_loss -0.7235
2024-12-08 18:32:51.130645: val_loss -0.546
2024-12-08 18:32:51.131512: Pseudo dice [0.75]
2024-12-08 18:32:51.132385: Epoch time: 90.13 s
2024-12-08 18:32:52.325954: 
2024-12-08 18:32:52.328003: Epoch 84
2024-12-08 18:32:52.329262: Current learning rate: 0.00924
2024-12-08 18:34:22.402743: Validation loss did not improve from -0.57932. Patience: 7/50
2024-12-08 18:34:22.403807: train_loss -0.7299
2024-12-08 18:34:22.404993: val_loss -0.5637
2024-12-08 18:34:22.405896: Pseudo dice [0.7524]
2024-12-08 18:34:22.406815: Epoch time: 90.08 s
2024-12-08 18:34:23.962228: 
2024-12-08 18:34:23.964553: Epoch 85
2024-12-08 18:34:23.965730: Current learning rate: 0.00923
2024-12-08 18:35:54.018810: Validation loss did not improve from -0.57932. Patience: 8/50
2024-12-08 18:35:54.020101: train_loss -0.7331
2024-12-08 18:35:54.021013: val_loss -0.53
2024-12-08 18:35:54.021871: Pseudo dice [0.7366]
2024-12-08 18:35:54.022707: Epoch time: 90.06 s
2024-12-08 18:35:55.199732: 
2024-12-08 18:35:55.201630: Epoch 86
2024-12-08 18:35:55.203023: Current learning rate: 0.00922
2024-12-08 18:37:25.311582: Validation loss did not improve from -0.57932. Patience: 9/50
2024-12-08 18:37:25.312942: train_loss -0.7345
2024-12-08 18:37:25.314402: val_loss -0.5381
2024-12-08 18:37:25.315500: Pseudo dice [0.7411]
2024-12-08 18:37:25.316473: Epoch time: 90.11 s
2024-12-08 18:37:26.497573: 
2024-12-08 18:37:26.499994: Epoch 87
2024-12-08 18:37:26.501444: Current learning rate: 0.00921
2024-12-08 18:38:56.524506: Validation loss did not improve from -0.57932. Patience: 10/50
2024-12-08 18:38:56.525559: train_loss -0.73
2024-12-08 18:38:56.526760: val_loss -0.5687
2024-12-08 18:38:56.528096: Pseudo dice [0.7596]
2024-12-08 18:38:56.528988: Epoch time: 90.03 s
2024-12-08 18:38:57.712629: 
2024-12-08 18:38:57.715081: Epoch 88
2024-12-08 18:38:57.716301: Current learning rate: 0.0092
2024-12-08 18:40:27.652811: Validation loss did not improve from -0.57932. Patience: 11/50
2024-12-08 18:40:27.654160: train_loss -0.7306
2024-12-08 18:40:27.655448: val_loss -0.566
2024-12-08 18:40:27.656425: Pseudo dice [0.7628]
2024-12-08 18:40:27.657499: Epoch time: 89.94 s
2024-12-08 18:40:27.658449: Yayy! New best EMA pseudo Dice: 0.7488
2024-12-08 18:40:29.157324: 
2024-12-08 18:40:29.159251: Epoch 89
2024-12-08 18:40:29.160296: Current learning rate: 0.0092
2024-12-08 18:41:59.041120: Validation loss did not improve from -0.57932. Patience: 12/50
2024-12-08 18:41:59.042147: train_loss -0.731
2024-12-08 18:41:59.043152: val_loss -0.5689
2024-12-08 18:41:59.043991: Pseudo dice [0.7568]
2024-12-08 18:41:59.044798: Epoch time: 89.89 s
2024-12-08 18:41:59.398612: Yayy! New best EMA pseudo Dice: 0.7496
2024-12-08 18:42:00.934622: 
2024-12-08 18:42:00.937094: Epoch 90
2024-12-08 18:42:00.938500: Current learning rate: 0.00919
2024-12-08 18:43:30.781190: Validation loss did not improve from -0.57932. Patience: 13/50
2024-12-08 18:43:30.782438: train_loss -0.7325
2024-12-08 18:43:30.783730: val_loss -0.5531
2024-12-08 18:43:30.784811: Pseudo dice [0.7482]
2024-12-08 18:43:30.785816: Epoch time: 89.85 s
2024-12-08 18:43:31.980917: 
2024-12-08 18:43:31.982770: Epoch 91
2024-12-08 18:43:31.984368: Current learning rate: 0.00918
2024-12-08 18:45:01.867883: Validation loss did not improve from -0.57932. Patience: 14/50
2024-12-08 18:45:01.869275: train_loss -0.738
2024-12-08 18:45:01.870886: val_loss -0.5587
2024-12-08 18:45:01.871938: Pseudo dice [0.7519]
2024-12-08 18:45:01.872941: Epoch time: 89.89 s
2024-12-08 18:45:01.873679: Yayy! New best EMA pseudo Dice: 0.7497
2024-12-08 18:45:03.675961: 
2024-12-08 18:45:03.677917: Epoch 92
2024-12-08 18:45:03.678948: Current learning rate: 0.00917
2024-12-08 18:46:33.498068: Validation loss did not improve from -0.57932. Patience: 15/50
2024-12-08 18:46:33.499154: train_loss -0.7332
2024-12-08 18:46:33.500214: val_loss -0.5424
2024-12-08 18:46:33.501188: Pseudo dice [0.74]
2024-12-08 18:46:33.502176: Epoch time: 89.82 s
2024-12-08 18:46:34.657535: 
2024-12-08 18:46:34.659602: Epoch 93
2024-12-08 18:46:34.660820: Current learning rate: 0.00916
2024-12-08 18:48:04.608330: Validation loss did not improve from -0.57932. Patience: 16/50
2024-12-08 18:48:04.609371: train_loss -0.7323
2024-12-08 18:48:04.610465: val_loss -0.5513
2024-12-08 18:48:04.611156: Pseudo dice [0.7451]
2024-12-08 18:48:04.612066: Epoch time: 89.95 s
2024-12-08 18:48:05.753968: 
2024-12-08 18:48:05.755986: Epoch 94
2024-12-08 18:48:05.757397: Current learning rate: 0.00915
2024-12-08 18:49:35.701168: Validation loss did not improve from -0.57932. Patience: 17/50
2024-12-08 18:49:35.702515: train_loss -0.7325
2024-12-08 18:49:35.703740: val_loss -0.5778
2024-12-08 18:49:35.704577: Pseudo dice [0.7668]
2024-12-08 18:49:35.705223: Epoch time: 89.95 s
2024-12-08 18:49:36.061920: Yayy! New best EMA pseudo Dice: 0.7502
2024-12-08 18:49:37.515945: 
2024-12-08 18:49:37.517503: Epoch 95
2024-12-08 18:49:37.518207: Current learning rate: 0.00914
2024-12-08 18:51:07.615138: Validation loss did not improve from -0.57932. Patience: 18/50
2024-12-08 18:51:07.616249: train_loss -0.7293
2024-12-08 18:51:07.617641: val_loss -0.5457
2024-12-08 18:51:07.618386: Pseudo dice [0.7454]
2024-12-08 18:51:07.619206: Epoch time: 90.1 s
2024-12-08 18:51:08.782619: 
2024-12-08 18:51:08.785206: Epoch 96
2024-12-08 18:51:08.786247: Current learning rate: 0.00913
2024-12-08 18:52:39.221805: Validation loss did not improve from -0.57932. Patience: 19/50
2024-12-08 18:52:39.222927: train_loss -0.7346
2024-12-08 18:52:39.224251: val_loss -0.5706
2024-12-08 18:52:39.225524: Pseudo dice [0.7568]
2024-12-08 18:52:39.226521: Epoch time: 90.44 s
2024-12-08 18:52:39.227468: Yayy! New best EMA pseudo Dice: 0.7505
2024-12-08 18:52:40.768033: 
2024-12-08 18:52:40.770534: Epoch 97
2024-12-08 18:52:40.772061: Current learning rate: 0.00912
2024-12-08 18:54:11.418235: Validation loss did not improve from -0.57932. Patience: 20/50
2024-12-08 18:54:11.419510: train_loss -0.7349
2024-12-08 18:54:11.420862: val_loss -0.5518
2024-12-08 18:54:11.421837: Pseudo dice [0.7467]
2024-12-08 18:54:11.422709: Epoch time: 90.65 s
2024-12-08 18:54:12.580920: 
2024-12-08 18:54:12.582855: Epoch 98
2024-12-08 18:54:12.584131: Current learning rate: 0.00911
2024-12-08 18:55:43.276669: Validation loss did not improve from -0.57932. Patience: 21/50
2024-12-08 18:55:43.277820: train_loss -0.7341
2024-12-08 18:55:43.278941: val_loss -0.5519
2024-12-08 18:55:43.279848: Pseudo dice [0.7538]
2024-12-08 18:55:43.280883: Epoch time: 90.7 s
2024-12-08 18:55:44.444796: 
2024-12-08 18:55:44.446806: Epoch 99
2024-12-08 18:55:44.447895: Current learning rate: 0.0091
2024-12-08 18:57:15.228331: Validation loss did not improve from -0.57932. Patience: 22/50
2024-12-08 18:57:15.229568: train_loss -0.7332
2024-12-08 18:57:15.230980: val_loss -0.5667
2024-12-08 18:57:15.232023: Pseudo dice [0.7599]
2024-12-08 18:57:15.232677: Epoch time: 90.79 s
2024-12-08 18:57:15.582939: Yayy! New best EMA pseudo Dice: 0.7514
2024-12-08 18:57:17.086969: 
2024-12-08 18:57:17.089168: Epoch 100
2024-12-08 18:57:17.090310: Current learning rate: 0.0091
2024-12-08 18:58:47.773951: Validation loss did not improve from -0.57932. Patience: 23/50
2024-12-08 18:58:47.775039: train_loss -0.736
2024-12-08 18:58:47.776124: val_loss -0.5518
2024-12-08 18:58:47.776905: Pseudo dice [0.7481]
2024-12-08 18:58:47.777651: Epoch time: 90.69 s
2024-12-08 18:58:48.952403: 
2024-12-08 18:58:48.955115: Epoch 101
2024-12-08 18:58:48.956067: Current learning rate: 0.00909
2024-12-08 19:00:19.638235: Validation loss did not improve from -0.57932. Patience: 24/50
2024-12-08 19:00:19.639593: train_loss -0.7418
2024-12-08 19:00:19.640796: val_loss -0.5751
2024-12-08 19:00:19.641888: Pseudo dice [0.763]
2024-12-08 19:00:19.642817: Epoch time: 90.69 s
2024-12-08 19:00:19.643692: Yayy! New best EMA pseudo Dice: 0.7523
2024-12-08 19:00:21.133323: 
2024-12-08 19:00:21.135832: Epoch 102
2024-12-08 19:00:21.137434: Current learning rate: 0.00908
2024-12-08 19:01:51.803876: Validation loss did not improve from -0.57932. Patience: 25/50
2024-12-08 19:01:51.804717: train_loss -0.7325
2024-12-08 19:01:51.805636: val_loss -0.557
2024-12-08 19:01:51.806412: Pseudo dice [0.7491]
2024-12-08 19:01:51.807175: Epoch time: 90.67 s
2024-12-08 19:01:53.292674: 
2024-12-08 19:01:53.294759: Epoch 103
2024-12-08 19:01:53.295807: Current learning rate: 0.00907
2024-12-08 19:03:24.076791: Validation loss did not improve from -0.57932. Patience: 26/50
2024-12-08 19:03:24.078007: train_loss -0.7383
2024-12-08 19:03:24.079202: val_loss -0.5575
2024-12-08 19:03:24.080039: Pseudo dice [0.7462]
2024-12-08 19:03:24.080969: Epoch time: 90.79 s
2024-12-08 19:03:25.264079: 
2024-12-08 19:03:25.266283: Epoch 104
2024-12-08 19:03:25.267853: Current learning rate: 0.00906
2024-12-08 19:04:56.052417: Validation loss did not improve from -0.57932. Patience: 27/50
2024-12-08 19:04:56.053761: train_loss -0.7421
2024-12-08 19:04:56.054883: val_loss -0.5373
2024-12-08 19:04:56.055928: Pseudo dice [0.7342]
2024-12-08 19:04:56.056876: Epoch time: 90.79 s
2024-12-08 19:04:57.579089: 
2024-12-08 19:04:57.581164: Epoch 105
2024-12-08 19:04:57.582170: Current learning rate: 0.00905
2024-12-08 19:06:28.282800: Validation loss did not improve from -0.57932. Patience: 28/50
2024-12-08 19:06:28.283771: train_loss -0.7378
2024-12-08 19:06:28.284766: val_loss -0.5348
2024-12-08 19:06:28.285570: Pseudo dice [0.7386]
2024-12-08 19:06:28.286505: Epoch time: 90.71 s
2024-12-08 19:06:29.455055: 
2024-12-08 19:06:29.456880: Epoch 106
2024-12-08 19:06:29.457941: Current learning rate: 0.00904
2024-12-08 19:08:00.231903: Validation loss did not improve from -0.57932. Patience: 29/50
2024-12-08 19:08:00.232896: train_loss -0.7322
2024-12-08 19:08:00.234013: val_loss -0.5336
2024-12-08 19:08:00.234680: Pseudo dice [0.7384]
2024-12-08 19:08:00.235380: Epoch time: 90.78 s
2024-12-08 19:08:01.412515: 
2024-12-08 19:08:01.414519: Epoch 107
2024-12-08 19:08:01.415666: Current learning rate: 0.00903
2024-12-08 19:09:32.236512: Validation loss did not improve from -0.57932. Patience: 30/50
2024-12-08 19:09:32.237916: train_loss -0.7377
2024-12-08 19:09:32.238887: val_loss -0.5356
2024-12-08 19:09:32.239784: Pseudo dice [0.7362]
2024-12-08 19:09:32.240563: Epoch time: 90.83 s
2024-12-08 19:09:33.412257: 
2024-12-08 19:09:33.414181: Epoch 108
2024-12-08 19:09:33.415507: Current learning rate: 0.00902
2024-12-08 19:11:04.169713: Validation loss improved from -0.57932 to -0.59573! Patience: 30/50
2024-12-08 19:11:04.170602: train_loss -0.7457
2024-12-08 19:11:04.171538: val_loss -0.5957
2024-12-08 19:11:04.172411: Pseudo dice [0.7763]
2024-12-08 19:11:04.173198: Epoch time: 90.76 s
2024-12-08 19:11:05.358690: 
2024-12-08 19:11:05.360426: Epoch 109
2024-12-08 19:11:05.361555: Current learning rate: 0.00901
2024-12-08 19:12:36.123807: Validation loss did not improve from -0.59573. Patience: 1/50
2024-12-08 19:12:36.124967: train_loss -0.7469
2024-12-08 19:12:36.126427: val_loss -0.5794
2024-12-08 19:12:36.127350: Pseudo dice [0.7579]
2024-12-08 19:12:36.128368: Epoch time: 90.77 s
2024-12-08 19:12:37.671210: 
2024-12-08 19:12:37.673522: Epoch 110
2024-12-08 19:12:37.675144: Current learning rate: 0.009
2024-12-08 19:14:08.465810: Validation loss did not improve from -0.59573. Patience: 2/50
2024-12-08 19:14:08.467167: train_loss -0.7529
2024-12-08 19:14:08.468524: val_loss -0.5563
2024-12-08 19:14:08.469532: Pseudo dice [0.753]
2024-12-08 19:14:08.470415: Epoch time: 90.8 s
2024-12-08 19:14:09.638440: 
2024-12-08 19:14:09.640431: Epoch 111
2024-12-08 19:14:09.641830: Current learning rate: 0.009
2024-12-08 19:15:40.225266: Validation loss did not improve from -0.59573. Patience: 3/50
2024-12-08 19:15:40.226154: train_loss -0.7487
2024-12-08 19:15:40.227152: val_loss -0.5429
2024-12-08 19:15:40.228262: Pseudo dice [0.7479]
2024-12-08 19:15:40.229266: Epoch time: 90.59 s
2024-12-08 19:15:41.442580: 
2024-12-08 19:15:41.444468: Epoch 112
2024-12-08 19:15:41.445546: Current learning rate: 0.00899
2024-12-08 19:17:12.299072: Validation loss did not improve from -0.59573. Patience: 4/50
2024-12-08 19:17:12.300353: train_loss -0.7457
2024-12-08 19:17:12.301603: val_loss -0.5409
2024-12-08 19:17:12.302581: Pseudo dice [0.7409]
2024-12-08 19:17:12.303625: Epoch time: 90.86 s
2024-12-08 19:17:13.481222: 
2024-12-08 19:17:13.483603: Epoch 113
2024-12-08 19:17:13.484867: Current learning rate: 0.00898
2024-12-08 19:18:44.352437: Validation loss did not improve from -0.59573. Patience: 5/50
2024-12-08 19:18:44.353870: train_loss -0.7308
2024-12-08 19:18:44.354780: val_loss -0.5503
2024-12-08 19:18:44.355606: Pseudo dice [0.7477]
2024-12-08 19:18:44.356396: Epoch time: 90.87 s
2024-12-08 19:18:45.867166: 
2024-12-08 19:18:45.868832: Epoch 114
2024-12-08 19:18:45.870103: Current learning rate: 0.00897
2024-12-08 19:20:16.516334: Validation loss did not improve from -0.59573. Patience: 6/50
2024-12-08 19:20:16.517430: train_loss -0.7425
2024-12-08 19:20:16.518661: val_loss -0.5619
2024-12-08 19:20:16.519740: Pseudo dice [0.7562]
2024-12-08 19:20:16.520730: Epoch time: 90.65 s
2024-12-08 19:20:18.061371: 
2024-12-08 19:20:18.062929: Epoch 115
2024-12-08 19:20:18.064208: Current learning rate: 0.00896
2024-12-08 19:21:48.593348: Validation loss did not improve from -0.59573. Patience: 7/50
2024-12-08 19:21:48.594606: train_loss -0.7518
2024-12-08 19:21:48.595686: val_loss -0.5537
2024-12-08 19:21:48.596673: Pseudo dice [0.749]
2024-12-08 19:21:48.597550: Epoch time: 90.53 s
2024-12-08 19:21:49.800341: 
2024-12-08 19:21:49.802253: Epoch 116
2024-12-08 19:21:49.803508: Current learning rate: 0.00895
2024-12-08 19:23:20.359564: Validation loss did not improve from -0.59573. Patience: 8/50
2024-12-08 19:23:20.361029: train_loss -0.7471
2024-12-08 19:23:20.362072: val_loss -0.5574
2024-12-08 19:23:20.362858: Pseudo dice [0.753]
2024-12-08 19:23:20.363606: Epoch time: 90.56 s
2024-12-08 19:23:21.568839: 
2024-12-08 19:23:21.571085: Epoch 117
2024-12-08 19:23:21.572044: Current learning rate: 0.00894
2024-12-08 19:24:52.049935: Validation loss did not improve from -0.59573. Patience: 9/50
2024-12-08 19:24:52.051002: train_loss -0.7342
2024-12-08 19:24:52.052121: val_loss -0.5352
2024-12-08 19:24:52.052914: Pseudo dice [0.7326]
2024-12-08 19:24:52.053654: Epoch time: 90.48 s
2024-12-08 19:24:53.255496: 
2024-12-08 19:24:53.257571: Epoch 118
2024-12-08 19:24:53.258412: Current learning rate: 0.00893
2024-12-08 19:26:23.656292: Validation loss did not improve from -0.59573. Patience: 10/50
2024-12-08 19:26:23.657789: train_loss -0.7377
2024-12-08 19:26:23.659226: val_loss -0.5672
2024-12-08 19:26:23.660059: Pseudo dice [0.756]
2024-12-08 19:26:23.660978: Epoch time: 90.4 s
2024-12-08 19:26:24.858799: 
2024-12-08 19:26:24.861199: Epoch 119
2024-12-08 19:26:24.862123: Current learning rate: 0.00892
2024-12-08 19:27:55.444459: Validation loss did not improve from -0.59573. Patience: 11/50
2024-12-08 19:27:55.452569: train_loss -0.7452
2024-12-08 19:27:55.456792: val_loss -0.5547
2024-12-08 19:27:55.458084: Pseudo dice [0.7477]
2024-12-08 19:27:55.459807: Epoch time: 90.59 s
2024-12-08 19:27:57.126370: 
2024-12-08 19:27:57.128258: Epoch 120
2024-12-08 19:27:57.129447: Current learning rate: 0.00891
2024-12-08 19:29:27.171887: Validation loss did not improve from -0.59573. Patience: 12/50
2024-12-08 19:29:27.173075: train_loss -0.756
2024-12-08 19:29:27.173986: val_loss -0.5555
2024-12-08 19:29:27.174975: Pseudo dice [0.7523]
2024-12-08 19:29:27.175736: Epoch time: 90.05 s
2024-12-08 19:29:28.372947: 
2024-12-08 19:29:28.375201: Epoch 121
2024-12-08 19:29:28.376234: Current learning rate: 0.0089
2024-12-08 19:30:58.408127: Validation loss did not improve from -0.59573. Patience: 13/50
2024-12-08 19:30:58.409470: train_loss -0.7551
2024-12-08 19:30:58.410744: val_loss -0.5783
2024-12-08 19:30:58.411700: Pseudo dice [0.761]
2024-12-08 19:30:58.412427: Epoch time: 90.04 s
2024-12-08 19:30:59.640644: 
2024-12-08 19:30:59.643112: Epoch 122
2024-12-08 19:30:59.644491: Current learning rate: 0.00889
2024-12-08 19:32:29.611923: Validation loss did not improve from -0.59573. Patience: 14/50
2024-12-08 19:32:29.613218: train_loss -0.7537
2024-12-08 19:32:29.614376: val_loss -0.5056
2024-12-08 19:32:29.615233: Pseudo dice [0.7295]
2024-12-08 19:32:29.616154: Epoch time: 89.97 s
2024-12-08 19:32:30.834364: 
2024-12-08 19:32:30.836035: Epoch 123
2024-12-08 19:32:30.836978: Current learning rate: 0.00889
2024-12-08 19:34:00.786025: Validation loss did not improve from -0.59573. Patience: 15/50
2024-12-08 19:34:00.787145: train_loss -0.7488
2024-12-08 19:34:00.788615: val_loss -0.5673
2024-12-08 19:34:00.789337: Pseudo dice [0.7617]
2024-12-08 19:34:00.790327: Epoch time: 89.95 s
2024-12-08 19:34:01.978813: 
2024-12-08 19:34:01.981097: Epoch 124
2024-12-08 19:34:01.982051: Current learning rate: 0.00888
2024-12-08 19:35:31.986964: Validation loss did not improve from -0.59573. Patience: 16/50
2024-12-08 19:35:31.988117: train_loss -0.7433
2024-12-08 19:35:31.989125: val_loss -0.5547
2024-12-08 19:35:31.989989: Pseudo dice [0.7498]
2024-12-08 19:35:31.990953: Epoch time: 90.01 s
2024-12-08 19:35:34.184922: 
2024-12-08 19:35:34.186698: Epoch 125
2024-12-08 19:35:34.187941: Current learning rate: 0.00887
2024-12-08 19:37:04.074467: Validation loss did not improve from -0.59573. Patience: 17/50
2024-12-08 19:37:04.075896: train_loss -0.7488
2024-12-08 19:37:04.077098: val_loss -0.5233
2024-12-08 19:37:04.078269: Pseudo dice [0.7388]
2024-12-08 19:37:04.079470: Epoch time: 89.89 s
2024-12-08 19:37:05.281541: 
2024-12-08 19:37:05.283986: Epoch 126
2024-12-08 19:37:05.285541: Current learning rate: 0.00886
2024-12-08 19:38:35.184151: Validation loss did not improve from -0.59573. Patience: 18/50
2024-12-08 19:38:35.185247: train_loss -0.7497
2024-12-08 19:38:35.186330: val_loss -0.5416
2024-12-08 19:38:35.187271: Pseudo dice [0.7399]
2024-12-08 19:38:35.188066: Epoch time: 89.9 s
2024-12-08 19:38:36.380227: 
2024-12-08 19:38:36.382431: Epoch 127
2024-12-08 19:38:36.383629: Current learning rate: 0.00885
2024-12-08 19:40:06.547683: Validation loss did not improve from -0.59573. Patience: 19/50
2024-12-08 19:40:06.548667: train_loss -0.7433
2024-12-08 19:40:06.549670: val_loss -0.5545
2024-12-08 19:40:06.550514: Pseudo dice [0.7544]
2024-12-08 19:40:06.551483: Epoch time: 90.17 s
2024-12-08 19:40:07.822360: 
2024-12-08 19:40:07.824631: Epoch 128
2024-12-08 19:40:07.825555: Current learning rate: 0.00884
2024-12-08 19:41:38.524127: Validation loss did not improve from -0.59573. Patience: 20/50
2024-12-08 19:41:38.525207: train_loss -0.7475
2024-12-08 19:41:38.526612: val_loss -0.5496
2024-12-08 19:41:38.527831: Pseudo dice [0.7562]
2024-12-08 19:41:38.528896: Epoch time: 90.7 s
2024-12-08 19:41:39.761002: 
2024-12-08 19:41:39.762795: Epoch 129
2024-12-08 19:41:39.764053: Current learning rate: 0.00883
2024-12-08 19:43:10.566331: Validation loss did not improve from -0.59573. Patience: 21/50
2024-12-08 19:43:10.567463: train_loss -0.7438
2024-12-08 19:43:10.568970: val_loss -0.5606
2024-12-08 19:43:10.569762: Pseudo dice [0.7553]
2024-12-08 19:43:10.570530: Epoch time: 90.81 s
2024-12-08 19:43:12.142412: 
2024-12-08 19:43:12.144047: Epoch 130
2024-12-08 19:43:12.145428: Current learning rate: 0.00882
2024-12-08 19:44:42.970271: Validation loss did not improve from -0.59573. Patience: 22/50
2024-12-08 19:44:42.971470: train_loss -0.7533
2024-12-08 19:44:42.972565: val_loss -0.5605
2024-12-08 19:44:42.973471: Pseudo dice [0.7491]
2024-12-08 19:44:42.974638: Epoch time: 90.83 s
2024-12-08 19:44:44.176553: 
2024-12-08 19:44:44.178508: Epoch 131
2024-12-08 19:44:44.179744: Current learning rate: 0.00881
2024-12-08 19:46:14.686524: Validation loss did not improve from -0.59573. Patience: 23/50
2024-12-08 19:46:14.687876: train_loss -0.7518
2024-12-08 19:46:14.689142: val_loss -0.5308
2024-12-08 19:46:14.690240: Pseudo dice [0.7409]
2024-12-08 19:46:14.691100: Epoch time: 90.51 s
2024-12-08 19:46:15.945899: 
2024-12-08 19:46:15.947510: Epoch 132
2024-12-08 19:46:15.948822: Current learning rate: 0.0088
2024-12-08 19:47:46.219526: Validation loss did not improve from -0.59573. Patience: 24/50
2024-12-08 19:47:46.220812: train_loss -0.756
2024-12-08 19:47:46.222235: val_loss -0.5389
2024-12-08 19:47:46.223524: Pseudo dice [0.7481]
2024-12-08 19:47:46.224548: Epoch time: 90.28 s
2024-12-08 19:47:47.427304: 
2024-12-08 19:47:47.429483: Epoch 133
2024-12-08 19:47:47.430506: Current learning rate: 0.00879
2024-12-08 19:49:17.671821: Validation loss did not improve from -0.59573. Patience: 25/50
2024-12-08 19:49:17.673209: train_loss -0.7526
2024-12-08 19:49:17.674153: val_loss -0.5216
2024-12-08 19:49:17.674878: Pseudo dice [0.7293]
2024-12-08 19:49:17.675546: Epoch time: 90.25 s
2024-12-08 19:49:18.871451: 
2024-12-08 19:49:18.872831: Epoch 134
2024-12-08 19:49:18.874046: Current learning rate: 0.00879
2024-12-08 19:50:48.959792: Validation loss did not improve from -0.59573. Patience: 26/50
2024-12-08 19:50:48.960760: train_loss -0.7583
2024-12-08 19:50:48.961906: val_loss -0.5403
2024-12-08 19:50:48.963076: Pseudo dice [0.7433]
2024-12-08 19:50:48.964119: Epoch time: 90.09 s
2024-12-08 19:50:50.663753: 
2024-12-08 19:50:50.665862: Epoch 135
2024-12-08 19:50:50.667156: Current learning rate: 0.00878
2024-12-08 19:52:20.747623: Validation loss did not improve from -0.59573. Patience: 27/50
2024-12-08 19:52:20.748969: train_loss -0.758
2024-12-08 19:52:20.750143: val_loss -0.5768
2024-12-08 19:52:20.750984: Pseudo dice [0.7646]
2024-12-08 19:52:20.751740: Epoch time: 90.09 s
2024-12-08 19:52:22.301681: 
2024-12-08 19:52:22.303791: Epoch 136
2024-12-08 19:52:22.305001: Current learning rate: 0.00877
2024-12-08 19:53:52.425701: Validation loss did not improve from -0.59573. Patience: 28/50
2024-12-08 19:53:52.426650: train_loss -0.7564
2024-12-08 19:53:52.427979: val_loss -0.548
2024-12-08 19:53:52.429202: Pseudo dice [0.7509]
2024-12-08 19:53:52.430217: Epoch time: 90.13 s
2024-12-08 19:53:53.671231: 
2024-12-08 19:53:53.673449: Epoch 137
2024-12-08 19:53:53.674850: Current learning rate: 0.00876
2024-12-08 19:55:23.748041: Validation loss did not improve from -0.59573. Patience: 29/50
2024-12-08 19:55:23.749203: train_loss -0.7598
2024-12-08 19:55:23.750451: val_loss -0.5213
2024-12-08 19:55:23.751266: Pseudo dice [0.7323]
2024-12-08 19:55:23.752037: Epoch time: 90.08 s
2024-12-08 19:55:24.992549: 
2024-12-08 19:55:24.994409: Epoch 138
2024-12-08 19:55:24.995708: Current learning rate: 0.00875
2024-12-08 19:56:55.113774: Validation loss did not improve from -0.59573. Patience: 30/50
2024-12-08 19:56:55.115070: train_loss -0.7576
2024-12-08 19:56:55.116183: val_loss -0.5634
2024-12-08 19:56:55.117125: Pseudo dice [0.7558]
2024-12-08 19:56:55.118031: Epoch time: 90.12 s
2024-12-08 19:56:56.345797: 
2024-12-08 19:56:56.348228: Epoch 139
2024-12-08 19:56:56.349390: Current learning rate: 0.00874
2024-12-08 19:58:26.621895: Validation loss did not improve from -0.59573. Patience: 31/50
2024-12-08 19:58:26.622956: train_loss -0.7648
2024-12-08 19:58:26.624099: val_loss -0.5556
2024-12-08 19:58:26.624999: Pseudo dice [0.7581]
2024-12-08 19:58:26.625731: Epoch time: 90.28 s
2024-12-08 19:58:28.197324: 
2024-12-08 19:58:28.199176: Epoch 140
2024-12-08 19:58:28.200644: Current learning rate: 0.00873
2024-12-08 19:59:58.588050: Validation loss did not improve from -0.59573. Patience: 32/50
2024-12-08 19:59:58.589406: train_loss -0.7676
2024-12-08 19:59:58.590803: val_loss -0.5357
2024-12-08 19:59:58.591583: Pseudo dice [0.7449]
2024-12-08 19:59:58.592260: Epoch time: 90.39 s
2024-12-08 19:59:59.821314: 
2024-12-08 19:59:59.823053: Epoch 141
2024-12-08 19:59:59.823945: Current learning rate: 0.00872
2024-12-08 20:01:30.345814: Validation loss did not improve from -0.59573. Patience: 33/50
2024-12-08 20:01:30.346753: train_loss -0.7578
2024-12-08 20:01:30.347796: val_loss -0.5707
2024-12-08 20:01:30.348779: Pseudo dice [0.7524]
2024-12-08 20:01:30.349862: Epoch time: 90.53 s
2024-12-08 20:01:31.568719: 
2024-12-08 20:01:31.571128: Epoch 142
2024-12-08 20:01:31.572499: Current learning rate: 0.00871
2024-12-08 20:03:02.107667: Validation loss did not improve from -0.59573. Patience: 34/50
2024-12-08 20:03:02.108741: train_loss -0.7546
2024-12-08 20:03:02.109860: val_loss -0.5634
2024-12-08 20:03:02.110795: Pseudo dice [0.7538]
2024-12-08 20:03:02.111561: Epoch time: 90.54 s
2024-12-08 20:03:03.339956: 
2024-12-08 20:03:03.341723: Epoch 143
2024-12-08 20:03:03.342813: Current learning rate: 0.0087
2024-12-08 20:04:33.782345: Validation loss did not improve from -0.59573. Patience: 35/50
2024-12-08 20:04:33.783311: train_loss -0.7453
2024-12-08 20:04:33.784698: val_loss -0.5155
2024-12-08 20:04:33.785597: Pseudo dice [0.739]
2024-12-08 20:04:33.786393: Epoch time: 90.44 s
2024-12-08 20:04:35.012675: 
2024-12-08 20:04:35.014814: Epoch 144
2024-12-08 20:04:35.016059: Current learning rate: 0.00869
2024-12-08 20:06:05.453697: Validation loss did not improve from -0.59573. Patience: 36/50
2024-12-08 20:06:05.454464: train_loss -0.7251
2024-12-08 20:06:05.455625: val_loss -0.5807
2024-12-08 20:06:05.456690: Pseudo dice [0.757]
2024-12-08 20:06:05.457832: Epoch time: 90.44 s
2024-12-08 20:06:07.029912: 
2024-12-08 20:06:07.031728: Epoch 145
2024-12-08 20:06:07.032778: Current learning rate: 0.00868
2024-12-08 20:07:37.182317: Validation loss did not improve from -0.59573. Patience: 37/50
2024-12-08 20:07:37.183666: train_loss -0.7315
2024-12-08 20:07:37.184855: val_loss -0.5068
2024-12-08 20:07:37.185868: Pseudo dice [0.7341]
2024-12-08 20:07:37.186757: Epoch time: 90.15 s
2024-12-08 20:07:38.750269: 
2024-12-08 20:07:38.752148: Epoch 146
2024-12-08 20:07:38.753338: Current learning rate: 0.00868
2024-12-08 20:09:08.978275: Validation loss did not improve from -0.59573. Patience: 38/50
2024-12-08 20:09:08.979474: train_loss -0.7341
2024-12-08 20:09:08.980495: val_loss -0.5483
2024-12-08 20:09:08.981365: Pseudo dice [0.7495]
2024-12-08 20:09:08.982201: Epoch time: 90.23 s
2024-12-08 20:09:10.209688: 
2024-12-08 20:09:10.211711: Epoch 147
2024-12-08 20:09:10.212891: Current learning rate: 0.00867
2024-12-08 20:10:40.613958: Validation loss did not improve from -0.59573. Patience: 39/50
2024-12-08 20:10:40.615136: train_loss -0.7454
2024-12-08 20:10:40.616221: val_loss -0.5576
2024-12-08 20:10:40.617187: Pseudo dice [0.7579]
2024-12-08 20:10:40.617902: Epoch time: 90.41 s
2024-12-08 20:10:41.844306: 
2024-12-08 20:10:41.846673: Epoch 148
2024-12-08 20:10:41.847931: Current learning rate: 0.00866
2024-12-08 20:12:12.204463: Validation loss did not improve from -0.59573. Patience: 40/50
2024-12-08 20:12:12.205665: train_loss -0.7496
2024-12-08 20:12:12.206824: val_loss -0.5619
2024-12-08 20:12:12.207718: Pseudo dice [0.7528]
2024-12-08 20:12:12.208593: Epoch time: 90.36 s
2024-12-08 20:12:13.444246: 
2024-12-08 20:12:13.446160: Epoch 149
2024-12-08 20:12:13.447427: Current learning rate: 0.00865
2024-12-08 20:13:43.560712: Validation loss did not improve from -0.59573. Patience: 41/50
2024-12-08 20:13:43.562031: train_loss -0.7617
2024-12-08 20:13:43.563438: val_loss -0.553
2024-12-08 20:13:43.564837: Pseudo dice [0.7373]
2024-12-08 20:13:43.566043: Epoch time: 90.12 s
2024-12-08 20:13:45.152318: 
2024-12-08 20:13:45.154822: Epoch 150
2024-12-08 20:13:45.156152: Current learning rate: 0.00864
2024-12-08 20:15:15.164354: Validation loss did not improve from -0.59573. Patience: 42/50
2024-12-08 20:15:15.165632: train_loss -0.7634
2024-12-08 20:15:15.166812: val_loss -0.5501
2024-12-08 20:15:15.167657: Pseudo dice [0.7463]
2024-12-08 20:15:15.168573: Epoch time: 90.01 s
2024-12-08 20:15:16.394795: 
2024-12-08 20:15:16.396577: Epoch 151
2024-12-08 20:15:16.397833: Current learning rate: 0.00863
2024-12-08 20:16:46.744351: Validation loss did not improve from -0.59573. Patience: 43/50
2024-12-08 20:16:46.745289: train_loss -0.7645
2024-12-08 20:16:46.746342: val_loss -0.5388
2024-12-08 20:16:46.747040: Pseudo dice [0.7481]
2024-12-08 20:16:46.748058: Epoch time: 90.35 s
2024-12-08 20:16:48.026973: 
2024-12-08 20:16:48.028385: Epoch 152
2024-12-08 20:16:48.029883: Current learning rate: 0.00862
2024-12-08 20:18:18.601628: Validation loss did not improve from -0.59573. Patience: 44/50
2024-12-08 20:18:18.602503: train_loss -0.7617
2024-12-08 20:18:18.603709: val_loss -0.5655
2024-12-08 20:18:18.604799: Pseudo dice [0.7579]
2024-12-08 20:18:18.605786: Epoch time: 90.58 s
2024-12-08 20:18:19.832193: 
2024-12-08 20:18:19.834299: Epoch 153
2024-12-08 20:18:19.835910: Current learning rate: 0.00861
2024-12-08 20:19:50.547530: Validation loss did not improve from -0.59573. Patience: 45/50
2024-12-08 20:19:50.548712: train_loss -0.7634
2024-12-08 20:19:50.549581: val_loss -0.5634
2024-12-08 20:19:50.550563: Pseudo dice [0.7563]
2024-12-08 20:19:50.551268: Epoch time: 90.72 s
2024-12-08 20:19:51.792534: 
2024-12-08 20:19:51.793995: Epoch 154
2024-12-08 20:19:51.794684: Current learning rate: 0.0086
2024-12-08 20:21:22.665981: Validation loss did not improve from -0.59573. Patience: 46/50
2024-12-08 20:21:22.667224: train_loss -0.765
2024-12-08 20:21:22.668577: val_loss -0.5684
2024-12-08 20:21:22.669568: Pseudo dice [0.7565]
2024-12-08 20:21:22.670562: Epoch time: 90.88 s
2024-12-08 20:21:24.300514: 
2024-12-08 20:21:24.302066: Epoch 155
2024-12-08 20:21:24.303269: Current learning rate: 0.00859
2024-12-08 20:22:54.680634: Validation loss did not improve from -0.59573. Patience: 47/50
2024-12-08 20:22:54.681595: train_loss -0.7687
2024-12-08 20:22:54.682746: val_loss -0.5598
2024-12-08 20:22:54.683819: Pseudo dice [0.7552]
2024-12-08 20:22:54.684745: Epoch time: 90.38 s
2024-12-08 20:22:55.938276: 
2024-12-08 20:22:55.940171: Epoch 156
2024-12-08 20:22:55.941511: Current learning rate: 0.00858
2024-12-08 20:24:26.024435: Validation loss did not improve from -0.59573. Patience: 48/50
2024-12-08 20:24:26.025588: train_loss -0.7647
2024-12-08 20:24:26.026828: val_loss -0.5642
2024-12-08 20:24:26.027676: Pseudo dice [0.7582]
2024-12-08 20:24:26.028646: Epoch time: 90.09 s
2024-12-08 20:24:27.621686: 
2024-12-08 20:24:27.623918: Epoch 157
2024-12-08 20:24:27.625162: Current learning rate: 0.00858
2024-12-08 20:25:57.478143: Validation loss did not improve from -0.59573. Patience: 49/50
2024-12-08 20:25:57.478988: train_loss -0.7659
2024-12-08 20:25:57.480139: val_loss -0.5559
2024-12-08 20:25:57.480895: Pseudo dice [0.7567]
2024-12-08 20:25:57.481500: Epoch time: 89.86 s
2024-12-08 20:25:58.719587: 
2024-12-08 20:25:58.721432: Epoch 158
2024-12-08 20:25:58.722665: Current learning rate: 0.00857
2024-12-08 20:27:28.636889: Validation loss did not improve from -0.59573. Patience: 50/50
2024-12-08 20:27:28.638143: train_loss -0.7582
2024-12-08 20:27:28.639086: val_loss -0.5402
2024-12-08 20:27:28.639856: Pseudo dice [0.7526]
2024-12-08 20:27:28.640673: Epoch time: 89.92 s
2024-12-08 20:27:29.894842: Patience reached. Stopping training.
2024-12-08 20:27:30.273254: Training done.
2024-12-08 20:27:30.486425: Using splits from existing split file: /home/gridsan/nchutisilp/datasets/nnUNet_Datasets/nnUNet_preprocessed/Dataset308_Sohee_Calcium_OCT_CrossValidation/splits_final.json
2024-12-08 20:27:30.488828: The split file contains 5 splits.
2024-12-08 20:27:30.489638: Desired fold for training: 1
2024-12-08 20:27:30.490723: This split has 6 training and 2 validation cases.
2024-12-08 20:27:30.491984: predicting 101-019
2024-12-08 20:27:30.501194: 101-019, shape torch.Size([1, 375, 498, 498]), rank 0
2024-12-08 20:29:09.957291: predicting 401-004
2024-12-08 20:29:09.978484: 401-004, shape torch.Size([1, 375, 498, 498]), rank 0
2024-12-08 20:31:00.585182: Validation complete
2024-12-08 20:31:00.586100: Mean Validation Dice:  0.7550986519789262

############################
INFO: You are using the old nnU-Net default plans. We have updated our recommendations. Please consider using those instead! Read more here: https://github.com/MIC-DKFZ/nnUNet/blob/master/documentation/resenc_presets.md
############################

Using device: cuda:0

#######################################################################
Please cite the following paper when using nnU-Net:
Isensee, F., Jaeger, P. F., Kohl, S. A., Petersen, J., & Maier-Hein, K. H. (2021). nnU-Net: a self-configuring method for deep learning-based biomedical image segmentation. Nature methods, 18(2), 203-211.
#######################################################################

2024-12-08 20:31:13.270472: Using torch.compile...

############################
INFO: You are using the old nnU-Net default plans. We have updated our recommendations. Please consider using those instead! Read more here: https://github.com/MIC-DKFZ/nnUNet/blob/master/documentation/resenc_presets.md
############################

Using device: cuda:0

#######################################################################
Please cite the following paper when using nnU-Net:
Isensee, F., Jaeger, P. F., Kohl, S. A., Petersen, J., & Maier-Hein, K. H. (2021). nnU-Net: a self-configuring method for deep learning-based biomedical image segmentation. Nature methods, 18(2), 203-211.
#######################################################################

2024-12-08 20:31:13.272727: Using torch.compile...
/home/gridsan/nchutisilp/.local/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn("The verbose parameter is deprecated. Please use get_last_lr() "
/home/gridsan/nchutisilp/.local/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn("The verbose parameter is deprecated. Please use get_last_lr() "
################### Loading pretrained weights from file  /home/gridsan/nchutisilp/datasets/nnUNet_Datasets/nnUNet_results/Dataset300_Lumen_and_Wall_OCT/nnUNetTrainer__nnUNetPreprocessPlans__3d_fullres/fold_all/checkpoint_best.pth ###################
Below is the list of overlapping blocks in pretrained model and nnUNet architecture:
encoder.stages.0.0.convs.0.conv.weight shape torch.Size([32, 1, 3, 3, 3])
encoder.stages.0.0.convs.0.conv.bias shape torch.Size([32])
encoder.stages.0.0.convs.0.norm.weight shape torch.Size([32])
encoder.stages.0.0.convs.0.norm.bias shape torch.Size([32])
encoder.stages.0.0.convs.0.all_modules.0.weight shape torch.Size([32, 1, 3, 3, 3])
encoder.stages.0.0.convs.0.all_modules.0.bias shape torch.Size([32])
encoder.stages.0.0.convs.0.all_modules.1.weight shape torch.Size([32])
encoder.stages.0.0.convs.0.all_modules.1.bias shape torch.Size([32])
encoder.stages.0.0.convs.1.conv.weight shape torch.Size([32, 32, 3, 3, 3])
encoder.stages.0.0.convs.1.conv.bias shape torch.Size([32])
encoder.stages.0.0.convs.1.norm.weight shape torch.Size([32])
encoder.stages.0.0.convs.1.norm.bias shape torch.Size([32])
encoder.stages.0.0.convs.1.all_modules.0.weight shape torch.Size([32, 32, 3, 3, 3])
encoder.stages.0.0.convs.1.all_modules.0.bias shape torch.Size([32])
encoder.stages.0.0.convs.1.all_modules.1.weight shape torch.Size([32])
encoder.stages.0.0.convs.1.all_modules.1.bias shape torch.Size([32])
encoder.stages.1.0.convs.0.conv.weight shape torch.Size([64, 32, 3, 3, 3])
encoder.stages.1.0.convs.0.conv.bias shape torch.Size([64])
encoder.stages.1.0.convs.0.norm.weight shape torch.Size([64])
encoder.stages.1.0.convs.0.norm.bias shape torch.Size([64])
encoder.stages.1.0.convs.0.all_modules.0.weight shape torch.Size([64, 32, 3, 3, 3])
encoder.stages.1.0.convs.0.all_modules.0.bias shape torch.Size([64])
encoder.stages.1.0.convs.0.all_modules.1.weight shape torch.Size([64])
encoder.stages.1.0.convs.0.all_modules.1.bias shape torch.Size([64])
encoder.stages.1.0.convs.1.conv.weight shape torch.Size([64, 64, 3, 3, 3])
encoder.stages.1.0.convs.1.conv.bias shape torch.Size([64])
encoder.stages.1.0.convs.1.norm.weight shape torch.Size([64])
encoder.stages.1.0.convs.1.norm.bias shape torch.Size([64])
encoder.stages.1.0.convs.1.all_modules.0.weight shape torch.Size([64, 64, 3, 3, 3])
encoder.stages.1.0.convs.1.all_modules.0.bias shape torch.Size([64])
encoder.stages.1.0.convs.1.all_modules.1.weight shape torch.Size([64])
encoder.stages.1.0.convs.1.all_modules.1.bias shape torch.Size([64])
encoder.stages.2.0.convs.0.conv.weight shape torch.Size([128, 64, 3, 3, 3])
encoder.stages.2.0.convs.0.conv.bias shape torch.Size([128])
encoder.stages.2.0.convs.0.norm.weight shape torch.Size([128])
encoder.stages.2.0.convs.0.norm.bias shape torch.Size([128])
encoder.stages.2.0.convs.0.all_modules.0.weight shape torch.Size([128, 64, 3, 3, 3])
encoder.stages.2.0.convs.0.all_modules.0.bias shape torch.Size([128])
encoder.stages.2.0.convs.0.all_modules.1.weight shape torch.Size([128])
encoder.stages.2.0.convs.0.all_modules.1.bias shape torch.Size([128])
encoder.stages.2.0.convs.1.conv.weight shape torch.Size([128, 128, 3, 3, 3])
encoder.stages.2.0.convs.1.conv.bias shape torch.Size([128])
encoder.stages.2.0.convs.1.norm.weight shape torch.Size([128])
encoder.stages.2.0.convs.1.norm.bias shape torch.Size([128])
encoder.stages.2.0.convs.1.all_modules.0.weight shape torch.Size([128, 128, 3, 3, 3])
encoder.stages.2.0.convs.1.all_modules.0.bias shape torch.Size([128])
encoder.stages.2.0.convs.1.all_modules.1.weight shape torch.Size([128])
encoder.stages.2.0.convs.1.all_modules.1.bias shape torch.Size([128])
encoder.stages.3.0.convs.0.conv.weight shape torch.Size([256, 128, 3, 3, 3])
encoder.stages.3.0.convs.0.conv.bias shape torch.Size([256])
encoder.stages.3.0.convs.0.norm.weight shape torch.Size([256])
encoder.stages.3.0.convs.0.norm.bias shape torch.Size([256])
encoder.stages.3.0.convs.0.all_modules.0.weight shape torch.Size([256, 128, 3, 3, 3])
encoder.stages.3.0.convs.0.all_modules.0.bias shape torch.Size([256])
encoder.stages.3.0.convs.0.all_modules.1.weight shape torch.Size([256])
encoder.stages.3.0.convs.0.all_modules.1.bias shape torch.Size([256])
encoder.stages.3.0.convs.1.conv.weight shape torch.Size([256, 256, 3, 3, 3])
encoder.stages.3.0.convs.1.conv.bias shape torch.Size([256])
encoder.stages.3.0.convs.1.norm.weight shape torch.Size([256])
encoder.stages.3.0.convs.1.norm.bias shape torch.Size([256])
encoder.stages.3.0.convs.1.all_modules.0.weight shape torch.Size([256, 256, 3, 3, 3])
encoder.stages.3.0.convs.1.all_modules.0.bias shape torch.Size([256])
encoder.stages.3.0.convs.1.all_modules.1.weight shape torch.Size([256])
encoder.stages.3.0.convs.1.all_modules.1.bias shape torch.Size([256])
encoder.stages.4.0.convs.0.conv.weight shape torch.Size([320, 256, 3, 3, 3])
encoder.stages.4.0.convs.0.conv.bias shape torch.Size([320])
encoder.stages.4.0.convs.0.norm.weight shape torch.Size([320])
encoder.stages.4.0.convs.0.norm.bias shape torch.Size([320])
encoder.stages.4.0.convs.0.all_modules.0.weight shape torch.Size([320, 256, 3, 3, 3])
encoder.stages.4.0.convs.0.all_modules.0.bias shape torch.Size([320])
encoder.stages.4.0.convs.0.all_modules.1.weight shape torch.Size([320])
encoder.stages.4.0.convs.0.all_modules.1.bias shape torch.Size([320])
encoder.stages.4.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.4.0.convs.1.conv.bias shape torch.Size([320])
encoder.stages.4.0.convs.1.norm.weight shape torch.Size([320])
encoder.stages.4.0.convs.1.norm.bias shape torch.Size([320])
encoder.stages.4.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.4.0.convs.1.all_modules.0.bias shape torch.Size([320])
encoder.stages.4.0.convs.1.all_modules.1.weight shape torch.Size([320])
encoder.stages.4.0.convs.1.all_modules.1.bias shape torch.Size([320])
encoder.stages.5.0.convs.0.conv.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.5.0.convs.0.conv.bias shape torch.Size([320])
encoder.stages.5.0.convs.0.norm.weight shape torch.Size([320])
encoder.stages.5.0.convs.0.norm.bias shape torch.Size([320])
encoder.stages.5.0.convs.0.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.5.0.convs.0.all_modules.0.bias shape torch.Size([320])
encoder.stages.5.0.convs.0.all_modules.1.weight shape torch.Size([320])
encoder.stages.5.0.convs.0.all_modules.1.bias shape torch.Size([320])
encoder.stages.5.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.5.0.convs.1.conv.bias shape torch.Size([320])
encoder.stages.5.0.convs.1.norm.weight shape torch.Size([320])
encoder.stages.5.0.convs.1.norm.bias shape torch.Size([320])
encoder.stages.5.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.5.0.convs.1.all_modules.0.bias shape torch.Size([320])
encoder.stages.5.0.convs.1.all_modules.1.weight shape torch.Size([320])
encoder.stages.5.0.convs.1.all_modules.1.bias shape torch.Size([320])
decoder.encoder.stages.0.0.convs.0.conv.weight shape torch.Size([32, 1, 3, 3, 3])
decoder.encoder.stages.0.0.convs.0.conv.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.norm.weight shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.norm.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.all_modules.0.weight shape torch.Size([32, 1, 3, 3, 3])
decoder.encoder.stages.0.0.convs.0.all_modules.0.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.all_modules.1.weight shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.all_modules.1.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.conv.weight shape torch.Size([32, 32, 3, 3, 3])
decoder.encoder.stages.0.0.convs.1.conv.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.norm.weight shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.norm.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.all_modules.0.weight shape torch.Size([32, 32, 3, 3, 3])
decoder.encoder.stages.0.0.convs.1.all_modules.0.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.all_modules.1.weight shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.all_modules.1.bias shape torch.Size([32])
decoder.encoder.stages.1.0.convs.0.conv.weight shape torch.Size([64, 32, 3, 3, 3])
decoder.encoder.stages.1.0.convs.0.conv.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.norm.weight shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.norm.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.all_modules.0.weight shape torch.Size([64, 32, 3, 3, 3])
decoder.encoder.stages.1.0.convs.0.all_modules.0.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.all_modules.1.weight shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.all_modules.1.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.conv.weight shape torch.Size([64, 64, 3, 3, 3])
decoder.encoder.stages.1.0.convs.1.conv.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.norm.weight shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.norm.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.all_modules.0.weight shape torch.Size([64, 64, 3, 3, 3])
decoder.encoder.stages.1.0.convs.1.all_modules.0.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.all_modules.1.weight shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.all_modules.1.bias shape torch.Size([64])
decoder.encoder.stages.2.0.convs.0.conv.weight shape torch.Size([128, 64, 3, 3, 3])
decoder.encoder.stages.2.0.convs.0.conv.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.norm.weight shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.norm.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.all_modules.0.weight shape torch.Size([128, 64, 3, 3, 3])
decoder.encoder.stages.2.0.convs.0.all_modules.0.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.all_modules.1.weight shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.all_modules.1.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.conv.weight shape torch.Size([128, 128, 3, 3, 3])
decoder.encoder.stages.2.0.convs.1.conv.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.norm.weight shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.norm.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.all_modules.0.weight shape torch.Size([128, 128, 3, 3, 3])
decoder.encoder.stages.2.0.convs.1.all_modules.0.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.all_modules.1.weight shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.all_modules.1.bias shape torch.Size([128])
decoder.encoder.stages.3.0.convs.0.conv.weight shape torch.Size([256, 128, 3, 3, 3])
decoder.encoder.stages.3.0.convs.0.conv.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.norm.weight shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.norm.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.all_modules.0.weight shape torch.Size([256, 128, 3, 3, 3])
decoder.encoder.stages.3.0.convs.0.all_modules.0.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.all_modules.1.weight shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.all_modules.1.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.conv.weight shape torch.Size([256, 256, 3, 3, 3])
decoder.encoder.stages.3.0.convs.1.conv.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.norm.weight shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.norm.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.all_modules.0.weight shape torch.Size([256, 256, 3, 3, 3])
decoder.encoder.stages.3.0.convs.1.all_modules.0.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.all_modules.1.weight shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.all_modules.1.bias shape torch.Size([256])
decoder.encoder.stages.4.0.convs.0.conv.weight shape torch.Size([320, 256, 3, 3, 3])
decoder.encoder.stages.4.0.convs.0.conv.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.norm.weight shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.norm.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.all_modules.0.weight shape torch.Size([320, 256, 3, 3, 3])
decoder.encoder.stages.4.0.convs.0.all_modules.0.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.all_modules.1.weight shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.all_modules.1.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.4.0.convs.1.conv.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.norm.weight shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.norm.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.4.0.convs.1.all_modules.0.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.all_modules.1.weight shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.all_modules.1.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.conv.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.5.0.convs.0.conv.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.norm.weight shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.norm.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.5.0.convs.0.all_modules.0.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.all_modules.1.weight shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.all_modules.1.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.5.0.convs.1.conv.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.norm.weight shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.norm.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.5.0.convs.1.all_modules.0.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.all_modules.1.weight shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.all_modules.1.bias shape torch.Size([320])
decoder.stages.0.convs.0.conv.weight shape torch.Size([320, 640, 3, 3, 3])
decoder.stages.0.convs.0.conv.bias shape torch.Size([320])
decoder.stages.0.convs.0.norm.weight shape torch.Size([320])
decoder.stages.0.convs.0.norm.bias shape torch.Size([320])
decoder.stages.0.convs.0.all_modules.0.weight shape torch.Size([320, 640, 3, 3, 3])
decoder.stages.0.convs.0.all_modules.0.bias shape torch.Size([320])
decoder.stages.0.convs.0.all_modules.1.weight shape torch.Size([320])
decoder.stages.0.convs.0.all_modules.1.bias shape torch.Size([320])
decoder.stages.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.stages.0.convs.1.conv.bias shape torch.Size([320])
decoder.stages.0.convs.1.norm.weight shape torch.Size([320])
decoder.stages.0.convs.1.norm.bias shape torch.Size([320])
decoder.stages.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.stages.0.convs.1.all_modules.0.bias shape torch.Size([320])
decoder.stages.0.convs.1.all_modules.1.weight shape torch.Size([320])
decoder.stages.0.convs.1.all_modules.1.bias shape torch.Size([320])
decoder.stages.1.convs.0.conv.weight shape torch.Size([256, 512, 3, 3, 3])
decoder.stages.1.convs.0.conv.bias shape torch.Size([256])
decoder.stages.1.convs.0.norm.weight shape torch.Size([256])
decoder.stages.1.convs.0.norm.bias shape torch.Size([256])
decoder.stages.1.convs.0.all_modules.0.weight shape torch.Size([256, 512, 3, 3, 3])
decoder.stages.1.convs.0.all_modules.0.bias shape torch.Size([256])
decoder.stages.1.convs.0.all_modules.1.weight shape torch.Size([256])
decoder.stages.1.convs.0.all_modules.1.bias shape torch.Size([256])
decoder.stages.1.convs.1.conv.weight shape torch.Size([256, 256, 3, 3, 3])
decoder.stages.1.convs.1.conv.bias shape torch.Size([256])
decoder.stages.1.convs.1.norm.weight shape torch.Size([256])
decoder.stages.1.convs.1.norm.bias shape torch.Size([256])
decoder.stages.1.convs.1.all_modules.0.weight shape torch.Size([256, 256, 3, 3, 3])
decoder.stages.1.convs.1.all_modules.0.bias shape torch.Size([256])
decoder.stages.1.convs.1.all_modules.1.weight shape torch.Size([256])
decoder.stages.1.convs.1.all_modules.1.bias shape torch.Size([256])
decoder.stages.2.convs.0.conv.weight shape torch.Size([128, 256, 3, 3, 3])
decoder.stages.2.convs.0.conv.bias shape torch.Size([128])
decoder.stages.2.convs.0.norm.weight shape torch.Size([128])
decoder.stages.2.convs.0.norm.bias shape torch.Size([128])
decoder.stages.2.convs.0.all_modules.0.weight shape torch.Size([128, 256, 3, 3, 3])
decoder.stages.2.convs.0.all_modules.0.bias shape torch.Size([128])
decoder.stages.2.convs.0.all_modules.1.weight shape torch.Size([128])
decoder.stages.2.convs.0.all_modules.1.bias shape torch.Size([128])
decoder.stages.2.convs.1.conv.weight shape torch.Size([128, 128, 3, 3, 3])
decoder.stages.2.convs.1.conv.bias shape torch.Size([128])
decoder.stages.2.convs.1.norm.weight shape torch.Size([128])
decoder.stages.2.convs.1.norm.bias shape torch.Size([128])
decoder.stages.2.convs.1.all_modules.0.weight shape torch.Size([128, 128, 3, 3, 3])
decoder.stages.2.convs.1.all_modules.0.bias shape torch.Size([128])
decoder.stages.2.convs.1.all_modules.1.weight shape torch.Size([128])
decoder.stages.2.convs.1.all_modules.1.bias shape torch.Size([128])
decoder.stages.3.convs.0.conv.weight shape torch.Size([64, 128, 3, 3, 3])
decoder.stages.3.convs.0.conv.bias shape torch.Size([64])
decoder.stages.3.convs.0.norm.weight shape torch.Size([64])
decoder.stages.3.convs.0.norm.bias shape torch.Size([64])
decoder.stages.3.convs.0.all_modules.0.weight shape torch.Size([64, 128, 3, 3, 3])
decoder.stages.3.convs.0.all_modules.0.bias shape torch.Size([64])
decoder.stages.3.convs.0.all_modules.1.weight shape torch.Size([64])
decoder.stages.3.convs.0.all_modules.1.bias shape torch.Size([64])
decoder.stages.3.convs.1.conv.weight shape torch.Size([64, 64, 3, 3, 3])
decoder.stages.3.convs.1.conv.bias shape torch.Size([64])
decoder.stages.3.convs.1.norm.weight shape torch.Size([64])
decoder.stages.3.convs.1.norm.bias shape torch.Size([64])
decoder.stages.3.convs.1.all_modules.0.weight shape torch.Size([64, 64, 3, 3, 3])
decoder.stages.3.convs.1.all_modules.0.bias shape torch.Size([64])
decoder.stages.3.convs.1.all_modules.1.weight shape torch.Size([64])
decoder.stages.3.convs.1.all_modules.1.bias shape torch.Size([64])
decoder.stages.4.convs.0.conv.weight shape torch.Size([32, 64, 3, 3, 3])
decoder.stages.4.convs.0.conv.bias shape torch.Size([32])
decoder.stages.4.convs.0.norm.weight shape torch.Size([32])
decoder.stages.4.convs.0.norm.bias shape torch.Size([32])
decoder.stages.4.convs.0.all_modules.0.weight shape torch.Size([32, 64, 3, 3, 3])
decoder.stages.4.convs.0.all_modules.0.bias shape torch.Size([32])
decoder.stages.4.convs.0.all_modules.1.weight shape torch.Size([32])
decoder.stages.4.convs.0.all_modules.1.bias shape torch.Size([32])
decoder.stages.4.convs.1.conv.weight shape torch.Size([32, 32, 3, 3, 3])
decoder.stages.4.convs.1.conv.bias shape torch.Size([32])
decoder.stages.4.convs.1.norm.weight shape torch.Size([32])
decoder.stages.4.convs.1.norm.bias shape torch.Size([32])
decoder.stages.4.convs.1.all_modules.0.weight shape torch.Size([32, 32, 3, 3, 3])
decoder.stages.4.convs.1.all_modules.0.bias shape torch.Size([32])
decoder.stages.4.convs.1.all_modules.1.weight shape torch.Size([32])
decoder.stages.4.convs.1.all_modules.1.bias shape torch.Size([32])
decoder.transpconvs.0.weight shape torch.Size([320, 320, 1, 2, 2])
decoder.transpconvs.0.bias shape torch.Size([320])
decoder.transpconvs.1.weight shape torch.Size([320, 256, 2, 2, 2])
decoder.transpconvs.1.bias shape torch.Size([256])
decoder.transpconvs.2.weight shape torch.Size([256, 128, 2, 2, 2])
decoder.transpconvs.2.bias shape torch.Size([128])
decoder.transpconvs.3.weight shape torch.Size([128, 64, 2, 2, 2])
decoder.transpconvs.3.bias shape torch.Size([64])
decoder.transpconvs.4.weight shape torch.Size([64, 32, 2, 2, 2])
decoder.transpconvs.4.bias shape torch.Size([32])
################### Done ###################
2024-12-08 20:31:37.682358: do_dummy_2d_data_aug: True
2024-12-08 20:31:37.685299: Using splits from existing split file: /home/gridsan/nchutisilp/datasets/nnUNet_Datasets/nnUNet_preprocessed/Dataset308_Sohee_Calcium_OCT_CrossValidation/splits_final.json
2024-12-08 20:31:37.687361: The split file contains 5 splits.
2024-12-08 20:31:37.688909: Desired fold for training: 2
2024-12-08 20:31:37.690550: This split has 6 training and 2 validation cases.
################### Loading pretrained weights from file  /home/gridsan/nchutisilp/datasets/nnUNet_Datasets/nnUNet_results/Dataset300_Lumen_and_Wall_OCT/nnUNetTrainer__nnUNetPreprocessPlans__3d_fullres/fold_all/checkpoint_best.pth ###################
Below is the list of overlapping blocks in pretrained model and nnUNet architecture:
encoder.stages.0.0.convs.0.conv.weight shape torch.Size([32, 1, 3, 3, 3])
encoder.stages.0.0.convs.0.conv.bias shape torch.Size([32])
encoder.stages.0.0.convs.0.norm.weight shape torch.Size([32])
encoder.stages.0.0.convs.0.norm.bias shape torch.Size([32])
encoder.stages.0.0.convs.0.all_modules.0.weight shape torch.Size([32, 1, 3, 3, 3])
encoder.stages.0.0.convs.0.all_modules.0.bias shape torch.Size([32])
encoder.stages.0.0.convs.0.all_modules.1.weight shape torch.Size([32])
encoder.stages.0.0.convs.0.all_modules.1.bias shape torch.Size([32])
encoder.stages.0.0.convs.1.conv.weight shape torch.Size([32, 32, 3, 3, 3])
encoder.stages.0.0.convs.1.conv.bias shape torch.Size([32])
encoder.stages.0.0.convs.1.norm.weight shape torch.Size([32])
encoder.stages.0.0.convs.1.norm.bias shape torch.Size([32])
encoder.stages.0.0.convs.1.all_modules.0.weight shape torch.Size([32, 32, 3, 3, 3])
encoder.stages.0.0.convs.1.all_modules.0.bias shape torch.Size([32])
encoder.stages.0.0.convs.1.all_modules.1.weight shape torch.Size([32])
encoder.stages.0.0.convs.1.all_modules.1.bias shape torch.Size([32])
encoder.stages.1.0.convs.0.conv.weight shape torch.Size([64, 32, 3, 3, 3])
encoder.stages.1.0.convs.0.conv.bias shape torch.Size([64])
encoder.stages.1.0.convs.0.norm.weight shape torch.Size([64])
encoder.stages.1.0.convs.0.norm.bias shape torch.Size([64])
encoder.stages.1.0.convs.0.all_modules.0.weight shape torch.Size([64, 32, 3, 3, 3])
encoder.stages.1.0.convs.0.all_modules.0.bias shape torch.Size([64])
encoder.stages.1.0.convs.0.all_modules.1.weight shape torch.Size([64])
encoder.stages.1.0.convs.0.all_modules.1.bias shape torch.Size([64])
encoder.stages.1.0.convs.1.conv.weight shape torch.Size([64, 64, 3, 3, 3])
encoder.stages.1.0.convs.1.conv.bias shape torch.Size([64])
encoder.stages.1.0.convs.1.norm.weight shape torch.Size([64])
encoder.stages.1.0.convs.1.norm.bias shape torch.Size([64])
encoder.stages.1.0.convs.1.all_modules.0.weight shape torch.Size([64, 64, 3, 3, 3])
encoder.stages.1.0.convs.1.all_modules.0.bias shape torch.Size([64])
encoder.stages.1.0.convs.1.all_modules.1.weight shape torch.Size([64])
encoder.stages.1.0.convs.1.all_modules.1.bias shape torch.Size([64])
encoder.stages.2.0.convs.0.conv.weight shape torch.Size([128, 64, 3, 3, 3])
encoder.stages.2.0.convs.0.conv.bias shape torch.Size([128])
encoder.stages.2.0.convs.0.norm.weight shape torch.Size([128])
encoder.stages.2.0.convs.0.norm.bias shape torch.Size([128])
encoder.stages.2.0.convs.0.all_modules.0.weight shape torch.Size([128, 64, 3, 3, 3])
encoder.stages.2.0.convs.0.all_modules.0.bias shape torch.Size([128])
encoder.stages.2.0.convs.0.all_modules.1.weight shape torch.Size([128])
encoder.stages.2.0.convs.0.all_modules.1.bias shape torch.Size([128])
encoder.stages.2.0.convs.1.conv.weight shape torch.Size([128, 128, 3, 3, 3])
encoder.stages.2.0.convs.1.conv.bias shape torch.Size([128])
encoder.stages.2.0.convs.1.norm.weight shape torch.Size([128])
encoder.stages.2.0.convs.1.norm.bias shape torch.Size([128])
encoder.stages.2.0.convs.1.all_modules.0.weight shape torch.Size([128, 128, 3, 3, 3])
encoder.stages.2.0.convs.1.all_modules.0.bias shape torch.Size([128])
encoder.stages.2.0.convs.1.all_modules.1.weight shape torch.Size([128])
encoder.stages.2.0.convs.1.all_modules.1.bias shape torch.Size([128])
encoder.stages.3.0.convs.0.conv.weight shape torch.Size([256, 128, 3, 3, 3])
encoder.stages.3.0.convs.0.conv.bias shape torch.Size([256])
encoder.stages.3.0.convs.0.norm.weight shape torch.Size([256])
encoder.stages.3.0.convs.0.norm.bias shape torch.Size([256])
encoder.stages.3.0.convs.0.all_modules.0.weight shape torch.Size([256, 128, 3, 3, 3])
encoder.stages.3.0.convs.0.all_modules.0.bias shape torch.Size([256])
encoder.stages.3.0.convs.0.all_modules.1.weight shape torch.Size([256])
encoder.stages.3.0.convs.0.all_modules.1.bias shape torch.Size([256])
encoder.stages.3.0.convs.1.conv.weight shape torch.Size([256, 256, 3, 3, 3])
encoder.stages.3.0.convs.1.conv.bias shape torch.Size([256])
encoder.stages.3.0.convs.1.norm.weight shape torch.Size([256])
encoder.stages.3.0.convs.1.norm.bias shape torch.Size([256])
encoder.stages.3.0.convs.1.all_modules.0.weight shape torch.Size([256, 256, 3, 3, 3])
encoder.stages.3.0.convs.1.all_modules.0.bias shape torch.Size([256])
encoder.stages.3.0.convs.1.all_modules.1.weight shape torch.Size([256])
encoder.stages.3.0.convs.1.all_modules.1.bias shape torch.Size([256])
encoder.stages.4.0.convs.0.conv.weight shape torch.Size([320, 256, 3, 3, 3])
encoder.stages.4.0.convs.0.conv.bias shape torch.Size([320])
encoder.stages.4.0.convs.0.norm.weight shape torch.Size([320])
encoder.stages.4.0.convs.0.norm.bias shape torch.Size([320])
encoder.stages.4.0.convs.0.all_modules.0.weight shape torch.Size([320, 256, 3, 3, 3])
encoder.stages.4.0.convs.0.all_modules.0.bias shape torch.Size([320])
encoder.stages.4.0.convs.0.all_modules.1.weight shape torch.Size([320])
encoder.stages.4.0.convs.0.all_modules.1.bias shape torch.Size([320])
encoder.stages.4.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.4.0.convs.1.conv.bias shape torch.Size([320])
encoder.stages.4.0.convs.1.norm.weight shape torch.Size([320])
encoder.stages.4.0.convs.1.norm.bias shape torch.Size([320])
encoder.stages.4.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.4.0.convs.1.all_modules.0.bias shape torch.Size([320])
encoder.stages.4.0.convs.1.all_modules.1.weight shape torch.Size([320])
encoder.stages.4.0.convs.1.all_modules.1.bias shape torch.Size([320])
encoder.stages.5.0.convs.0.conv.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.5.0.convs.0.conv.bias shape torch.Size([320])
encoder.stages.5.0.convs.0.norm.weight shape torch.Size([320])
encoder.stages.5.0.convs.0.norm.bias shape torch.Size([320])
encoder.stages.5.0.convs.0.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.5.0.convs.0.all_modules.0.bias shape torch.Size([320])
encoder.stages.5.0.convs.0.all_modules.1.weight shape torch.Size([320])
encoder.stages.5.0.convs.0.all_modules.1.bias shape torch.Size([320])
encoder.stages.5.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.5.0.convs.1.conv.bias shape torch.Size([320])
encoder.stages.5.0.convs.1.norm.weight shape torch.Size([320])
encoder.stages.5.0.convs.1.norm.bias shape torch.Size([320])
encoder.stages.5.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.5.0.convs.1.all_modules.0.bias shape torch.Size([320])
encoder.stages.5.0.convs.1.all_modules.1.weight shape torch.Size([320])
encoder.stages.5.0.convs.1.all_modules.1.bias shape torch.Size([320])
decoder.encoder.stages.0.0.convs.0.conv.weight shape torch.Size([32, 1, 3, 3, 3])
decoder.encoder.stages.0.0.convs.0.conv.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.norm.weight shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.norm.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.all_modules.0.weight shape torch.Size([32, 1, 3, 3, 3])
decoder.encoder.stages.0.0.convs.0.all_modules.0.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.all_modules.1.weight shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.all_modules.1.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.conv.weight shape torch.Size([32, 32, 3, 3, 3])
decoder.encoder.stages.0.0.convs.1.conv.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.norm.weight shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.norm.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.all_modules.0.weight shape torch.Size([32, 32, 3, 3, 3])
decoder.encoder.stages.0.0.convs.1.all_modules.0.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.all_modules.1.weight shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.all_modules.1.bias shape torch.Size([32])
decoder.encoder.stages.1.0.convs.0.conv.weight shape torch.Size([64, 32, 3, 3, 3])
decoder.encoder.stages.1.0.convs.0.conv.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.norm.weight shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.norm.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.all_modules.0.weight shape torch.Size([64, 32, 3, 3, 3])
decoder.encoder.stages.1.0.convs.0.all_modules.0.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.all_modules.1.weight shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.all_modules.1.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.conv.weight shape torch.Size([64, 64, 3, 3, 3])
decoder.encoder.stages.1.0.convs.1.conv.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.norm.weight shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.norm.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.all_modules.0.weight shape torch.Size([64, 64, 3, 3, 3])
decoder.encoder.stages.1.0.convs.1.all_modules.0.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.all_modules.1.weight shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.all_modules.1.bias shape torch.Size([64])
decoder.encoder.stages.2.0.convs.0.conv.weight shape torch.Size([128, 64, 3, 3, 3])
decoder.encoder.stages.2.0.convs.0.conv.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.norm.weight shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.norm.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.all_modules.0.weight shape torch.Size([128, 64, 3, 3, 3])
decoder.encoder.stages.2.0.convs.0.all_modules.0.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.all_modules.1.weight shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.all_modules.1.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.conv.weight shape torch.Size([128, 128, 3, 3, 3])
decoder.encoder.stages.2.0.convs.1.conv.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.norm.weight shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.norm.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.all_modules.0.weight shape torch.Size([128, 128, 3, 3, 3])
decoder.encoder.stages.2.0.convs.1.all_modules.0.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.all_modules.1.weight shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.all_modules.1.bias shape torch.Size([128])
decoder.encoder.stages.3.0.convs.0.conv.weight shape torch.Size([256, 128, 3, 3, 3])
decoder.encoder.stages.3.0.convs.0.conv.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.norm.weight shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.norm.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.all_modules.0.weight shape torch.Size([256, 128, 3, 3, 3])
decoder.encoder.stages.3.0.convs.0.all_modules.0.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.all_modules.1.weight shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.all_modules.1.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.conv.weight shape torch.Size([256, 256, 3, 3, 3])
decoder.encoder.stages.3.0.convs.1.conv.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.norm.weight shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.norm.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.all_modules.0.weight shape torch.Size([256, 256, 3, 3, 3])
decoder.encoder.stages.3.0.convs.1.all_modules.0.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.all_modules.1.weight shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.all_modules.1.bias shape torch.Size([256])
decoder.encoder.stages.4.0.convs.0.conv.weight shape torch.Size([320, 256, 3, 3, 3])
decoder.encoder.stages.4.0.convs.0.conv.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.norm.weight shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.norm.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.all_modules.0.weight shape torch.Size([320, 256, 3, 3, 3])
decoder.encoder.stages.4.0.convs.0.all_modules.0.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.all_modules.1.weight shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.all_modules.1.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.4.0.convs.1.conv.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.norm.weight shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.norm.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.4.0.convs.1.all_modules.0.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.all_modules.1.weight shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.all_modules.1.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.conv.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.5.0.convs.0.conv.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.norm.weight shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.norm.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.5.0.convs.0.all_modules.0.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.all_modules.1.weight shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.all_modules.1.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.5.0.convs.1.conv.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.norm.weight shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.norm.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.5.0.convs.1.all_modules.0.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.all_modules.1.weight shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.all_modules.1.bias shape torch.Size([320])
decoder.stages.0.convs.0.conv.weight shape torch.Size([320, 640, 3, 3, 3])
decoder.stages.0.convs.0.conv.bias shape torch.Size([320])
decoder.stages.0.convs.0.norm.weight shape torch.Size([320])
decoder.stages.0.convs.0.norm.bias shape torch.Size([320])
decoder.stages.0.convs.0.all_modules.0.weight shape torch.Size([320, 640, 3, 3, 3])
decoder.stages.0.convs.0.all_modules.0.bias shape torch.Size([320])
decoder.stages.0.convs.0.all_modules.1.weight shape torch.Size([320])
decoder.stages.0.convs.0.all_modules.1.bias shape torch.Size([320])
decoder.stages.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.stages.0.convs.1.conv.bias shape torch.Size([320])
decoder.stages.0.convs.1.norm.weight shape torch.Size([320])
decoder.stages.0.convs.1.norm.bias shape torch.Size([320])
decoder.stages.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.stages.0.convs.1.all_modules.0.bias shape torch.Size([320])
decoder.stages.0.convs.1.all_modules.1.weight shape torch.Size([320])
decoder.stages.0.convs.1.all_modules.1.bias shape torch.Size([320])
decoder.stages.1.convs.0.conv.weight shape torch.Size([256, 512, 3, 3, 3])
decoder.stages.1.convs.0.conv.bias shape torch.Size([256])
decoder.stages.1.convs.0.norm.weight shape torch.Size([256])
decoder.stages.1.convs.0.norm.bias shape torch.Size([256])
decoder.stages.1.convs.0.all_modules.0.weight shape torch.Size([256, 512, 3, 3, 3])
decoder.stages.1.convs.0.all_modules.0.bias shape torch.Size([256])
decoder.stages.1.convs.0.all_modules.1.weight shape torch.Size([256])
decoder.stages.1.convs.0.all_modules.1.bias shape torch.Size([256])
decoder.stages.1.convs.1.conv.weight shape torch.Size([256, 256, 3, 3, 3])
decoder.stages.1.convs.1.conv.bias shape torch.Size([256])
decoder.stages.1.convs.1.norm.weight shape torch.Size([256])
decoder.stages.1.convs.1.norm.bias shape torch.Size([256])
decoder.stages.1.convs.1.all_modules.0.weight shape torch.Size([256, 256, 3, 3, 3])
decoder.stages.1.convs.1.all_modules.0.bias shape torch.Size([256])
decoder.stages.1.convs.1.all_modules.1.weight shape torch.Size([256])
decoder.stages.1.convs.1.all_modules.1.bias shape torch.Size([256])
decoder.stages.2.convs.0.conv.weight shape torch.Size([128, 256, 3, 3, 3])
decoder.stages.2.convs.0.conv.bias shape torch.Size([128])
decoder.stages.2.convs.0.norm.weight shape torch.Size([128])
decoder.stages.2.convs.0.norm.bias shape torch.Size([128])
decoder.stages.2.convs.0.all_modules.0.weight shape torch.Size([128, 256, 3, 3, 3])
decoder.stages.2.convs.0.all_modules.0.bias shape torch.Size([128])
decoder.stages.2.convs.0.all_modules.1.weight shape torch.Size([128])
decoder.stages.2.convs.0.all_modules.1.bias shape torch.Size([128])
decoder.stages.2.convs.1.conv.weight shape torch.Size([128, 128, 3, 3, 3])
decoder.stages.2.convs.1.conv.bias shape torch.Size([128])
decoder.stages.2.convs.1.norm.weight shape torch.Size([128])
decoder.stages.2.convs.1.norm.bias shape torch.Size([128])
decoder.stages.2.convs.1.all_modules.0.weight shape torch.Size([128, 128, 3, 3, 3])
decoder.stages.2.convs.1.all_modules.0.bias shape torch.Size([128])
decoder.stages.2.convs.1.all_modules.1.weight shape torch.Size([128])
decoder.stages.2.convs.1.all_modules.1.bias shape torch.Size([128])
decoder.stages.3.convs.0.conv.weight shape torch.Size([64, 128, 3, 3, 3])
decoder.stages.3.convs.0.conv.bias shape torch.Size([64])
decoder.stages.3.convs.0.norm.weight shape torch.Size([64])
decoder.stages.3.convs.0.norm.bias shape torch.Size([64])
decoder.stages.3.convs.0.all_modules.0.weight shape torch.Size([64, 128, 3, 3, 3])
decoder.stages.3.convs.0.all_modules.0.bias shape torch.Size([64])
decoder.stages.3.convs.0.all_modules.1.weight shape torch.Size([64])
decoder.stages.3.convs.0.all_modules.1.bias shape torch.Size([64])
decoder.stages.3.convs.1.conv.weight shape torch.Size([64, 64, 3, 3, 3])
decoder.stages.3.convs.1.conv.bias shape torch.Size([64])
decoder.stages.3.convs.1.norm.weight shape torch.Size([64])
decoder.stages.3.convs.1.norm.bias shape torch.Size([64])
decoder.stages.3.convs.1.all_modules.0.weight shape torch.Size([64, 64, 3, 3, 3])
decoder.stages.3.convs.1.all_modules.0.bias shape torch.Size([64])
decoder.stages.3.convs.1.all_modules.1.weight shape torch.Size([64])
decoder.stages.3.convs.1.all_modules.1.bias shape torch.Size([64])
decoder.stages.4.convs.0.conv.weight shape torch.Size([32, 64, 3, 3, 3])
decoder.stages.4.convs.0.conv.bias shape torch.Size([32])
decoder.stages.4.convs.0.norm.weight shape torch.Size([32])
decoder.stages.4.convs.0.norm.bias shape torch.Size([32])
decoder.stages.4.convs.0.all_modules.0.weight shape torch.Size([32, 64, 3, 3, 3])
decoder.stages.4.convs.0.all_modules.0.bias shape torch.Size([32])
decoder.stages.4.convs.0.all_modules.1.weight shape torch.Size([32])
decoder.stages.4.convs.0.all_modules.1.bias shape torch.Size([32])
decoder.stages.4.convs.1.conv.weight shape torch.Size([32, 32, 3, 3, 3])
decoder.stages.4.convs.1.conv.bias shape torch.Size([32])
decoder.stages.4.convs.1.norm.weight shape torch.Size([32])
decoder.stages.4.convs.1.norm.bias shape torch.Size([32])
decoder.stages.4.convs.1.all_modules.0.weight shape torch.Size([32, 32, 3, 3, 3])
decoder.stages.4.convs.1.all_modules.0.bias shape torch.Size([32])
decoder.stages.4.convs.1.all_modules.1.weight shape torch.Size([32])
decoder.stages.4.convs.1.all_modules.1.bias shape torch.Size([32])
decoder.transpconvs.0.weight shape torch.Size([320, 320, 1, 2, 2])
decoder.transpconvs.0.bias shape torch.Size([320])
decoder.transpconvs.1.weight shape torch.Size([320, 256, 2, 2, 2])
decoder.transpconvs.1.bias shape torch.Size([256])
decoder.transpconvs.2.weight shape torch.Size([256, 128, 2, 2, 2])
decoder.transpconvs.2.bias shape torch.Size([128])
decoder.transpconvs.3.weight shape torch.Size([128, 64, 2, 2, 2])
decoder.transpconvs.3.bias shape torch.Size([64])
decoder.transpconvs.4.weight shape torch.Size([64, 32, 2, 2, 2])
decoder.transpconvs.4.bias shape torch.Size([32])
################### Done ###################
2024-12-08 20:31:37.682390: do_dummy_2d_data_aug: True
2024-12-08 20:31:37.685455: Using splits from existing split file: /home/gridsan/nchutisilp/datasets/nnUNet_Datasets/nnUNet_preprocessed/Dataset308_Sohee_Calcium_OCT_CrossValidation/splits_final.json
2024-12-08 20:31:37.687593: The split file contains 5 splits.
2024-12-08 20:31:37.688918: Desired fold for training: 3
2024-12-08 20:31:37.690667: This split has 7 training and 1 validation cases.
using pin_memory on device 0
using pin_memory on device 0

This is the configuration used by this training:
Configuration name: 3d_32x160x128_b10
 {'data_identifier': 'nnUNetPlans_3d_fullres', 'preprocessor_name': 'DefaultPreprocessor', 'batch_size': 10, 'patch_size': [32, 160, 128], 'median_image_size_in_voxels': [375.0, 498.0, 498.0], 'spacing': [1.0, 1.0, 1.0], 'normalization_schemes': ['ZScoreNormalization'], 'use_mask_for_norm': [False], 'resampling_fn_data': 'resample_data_or_seg_to_shape', 'resampling_fn_seg': 'resample_data_or_seg_to_shape', 'resampling_fn_data_kwargs': {'is_seg': False, 'order': 3, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_seg_kwargs': {'is_seg': True, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_probabilities': 'resample_data_or_seg_to_shape', 'resampling_fn_probabilities_kwargs': {'is_seg': False, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'architecture': {'network_class_name': 'dynamic_network_architectures.architectures.unet.PlainConvUNet', 'arch_kwargs': {'n_stages': 6, 'features_per_stage': [32, 64, 128, 256, 320, 320], 'conv_op': 'torch.nn.modules.conv.Conv3d', 'kernel_sizes': [[3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3]], 'strides': [[1, 1, 1], [2, 2, 2], [2, 2, 2], [2, 2, 2], [2, 2, 2], [1, 2, 2]], 'n_conv_per_stage': [2, 2, 2, 2, 2, 2], 'n_conv_per_stage_decoder': [2, 2, 2, 2, 2], 'conv_bias': True, 'norm_op': 'torch.nn.modules.instancenorm.InstanceNorm3d', 'norm_op_kwargs': {'eps': 1e-05, 'affine': True}, 'dropout_op': None, 'dropout_op_kwargs': None, 'nonlin': 'torch.nn.LeakyReLU', 'nonlin_kwargs': {'inplace': True}, 'deep_supervision': True}, '_kw_requires_import': ['conv_op', 'norm_op', 'dropout_op', 'nonlin']}, 'batch_dice': True, 'inherits_from': '3d_fullres'} 

These are the global plan.json settings:
 {'dataset_name': 'Dataset308_Sohee_Calcium_OCT_CrossValidation', 'plans_name': 'nnUNetPlans', 'original_median_spacing_after_transp': [1.0, 1.0, 1.0], 'original_median_shape_after_transp': [375, 498, 498], 'image_reader_writer': 'SimpleITKIO', 'transpose_forward': [0, 1, 2], 'transpose_backward': [0, 1, 2], 'experiment_planner_used': 'ExperimentPlanner', 'label_manager': 'LabelManager', 'foreground_intensity_properties_per_channel': {'0': {'max': 0.4977976381778717, 'mean': 0.13507525622844696, 'median': 0.09599608182907104, 'min': 0.0, 'percentile_00_5': 0.014754901640117168, 'percentile_99_5': 0.4977976381778717, 'std': 0.12025152146816254}}} 

2024-12-08 20:31:39.034552: unpacking dataset...
using pin_memory on device 0
using pin_memory on device 0

This is the configuration used by this training:
Configuration name: 3d_32x160x128_b10
 {'data_identifier': 'nnUNetPlans_3d_fullres', 'preprocessor_name': 'DefaultPreprocessor', 'batch_size': 10, 'patch_size': [32, 160, 128], 'median_image_size_in_voxels': [375.0, 498.0, 498.0], 'spacing': [1.0, 1.0, 1.0], 'normalization_schemes': ['ZScoreNormalization'], 'use_mask_for_norm': [False], 'resampling_fn_data': 'resample_data_or_seg_to_shape', 'resampling_fn_seg': 'resample_data_or_seg_to_shape', 'resampling_fn_data_kwargs': {'is_seg': False, 'order': 3, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_seg_kwargs': {'is_seg': True, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_probabilities': 'resample_data_or_seg_to_shape', 'resampling_fn_probabilities_kwargs': {'is_seg': False, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'architecture': {'network_class_name': 'dynamic_network_architectures.architectures.unet.PlainConvUNet', 'arch_kwargs': {'n_stages': 6, 'features_per_stage': [32, 64, 128, 256, 320, 320], 'conv_op': 'torch.nn.modules.conv.Conv3d', 'kernel_sizes': [[3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3]], 'strides': [[1, 1, 1], [2, 2, 2], [2, 2, 2], [2, 2, 2], [2, 2, 2], [1, 2, 2]], 'n_conv_per_stage': [2, 2, 2, 2, 2, 2], 'n_conv_per_stage_decoder': [2, 2, 2, 2, 2], 'conv_bias': True, 'norm_op': 'torch.nn.modules.instancenorm.InstanceNorm3d', 'norm_op_kwargs': {'eps': 1e-05, 'affine': True}, 'dropout_op': None, 'dropout_op_kwargs': None, 'nonlin': 'torch.nn.LeakyReLU', 'nonlin_kwargs': {'inplace': True}, 'deep_supervision': True}, '_kw_requires_import': ['conv_op', 'norm_op', 'dropout_op', 'nonlin']}, 'batch_dice': True, 'inherits_from': '3d_fullres'} 

These are the global plan.json settings:
 {'dataset_name': 'Dataset308_Sohee_Calcium_OCT_CrossValidation', 'plans_name': 'nnUNetPlans', 'original_median_spacing_after_transp': [1.0, 1.0, 1.0], 'original_median_shape_after_transp': [375, 498, 498], 'image_reader_writer': 'SimpleITKIO', 'transpose_forward': [0, 1, 2], 'transpose_backward': [0, 1, 2], 'experiment_planner_used': 'ExperimentPlanner', 'label_manager': 'LabelManager', 'foreground_intensity_properties_per_channel': {'0': {'max': 0.4977976381778717, 'mean': 0.13507525622844696, 'median': 0.09599608182907104, 'min': 0.0, 'percentile_00_5': 0.014754901640117168, 'percentile_99_5': 0.4977976381778717, 'std': 0.12025152146816254}}} 

2024-12-08 20:31:40.604386: unpacking dataset...
2024-12-08 20:31:43.372405: unpacking done...
2024-12-08 20:31:43.783170: Unable to plot network architecture: nnUNet_compile is enabled!
2024-12-08 20:31:43.847684: 
2024-12-08 20:31:43.849443: Epoch 0
2024-12-08 20:31:43.850423: Current learning rate: 0.01
2024-12-08 20:34:34.219331: Validation loss improved from 1000.00000 to -0.37004! Patience: 0/50
2024-12-08 20:34:34.221259: train_loss -0.3321
2024-12-08 20:34:34.222737: val_loss -0.37
2024-12-08 20:34:34.223820: Pseudo dice [0.6682]
2024-12-08 20:34:34.224810: Epoch time: 170.38 s
2024-12-08 20:34:34.225773: Yayy! New best EMA pseudo Dice: 0.6682
2024-12-08 20:34:36.240381: 
2024-12-08 20:34:36.242011: Epoch 1
2024-12-08 20:34:36.242912: Current learning rate: 0.00999
2024-12-08 20:36:04.483747: Validation loss improved from -0.37004 to -0.39150! Patience: 0/50
2024-12-08 20:36:04.484854: train_loss -0.4823
2024-12-08 20:36:04.485974: val_loss -0.3915
2024-12-08 20:36:04.486783: Pseudo dice [0.6715]
2024-12-08 20:36:04.487528: Epoch time: 88.25 s
2024-12-08 20:36:04.488336: Yayy! New best EMA pseudo Dice: 0.6686
2024-12-08 20:36:06.134193: 
2024-12-08 20:36:06.135659: Epoch 2
2024-12-08 20:36:06.136365: Current learning rate: 0.00998
2024-12-08 20:37:34.935526: Validation loss improved from -0.39150 to -0.39265! Patience: 0/50
2024-12-08 20:37:34.936553: train_loss -0.5098
2024-12-08 20:37:34.937373: val_loss -0.3927
2024-12-08 20:37:34.938171: Pseudo dice [0.6657]
2024-12-08 20:37:34.938986: Epoch time: 88.8 s
2024-12-08 20:37:36.331967: 
2024-12-08 20:37:36.333198: Epoch 3
2024-12-08 20:37:36.333952: Current learning rate: 0.00997
2024-12-08 20:39:05.245676: Validation loss improved from -0.39265 to -0.39849! Patience: 0/50
2024-12-08 20:39:05.246899: train_loss -0.5349
2024-12-08 20:39:05.247802: val_loss -0.3985
2024-12-08 20:39:05.248682: Pseudo dice [0.6646]
2024-12-08 20:39:05.249414: Epoch time: 88.92 s
2024-12-08 20:39:06.548565: 
2024-12-08 20:39:06.550322: Epoch 4
2024-12-08 20:39:06.551742: Current learning rate: 0.00996
2024-12-08 20:40:35.590322: Validation loss improved from -0.39849 to -0.42932! Patience: 0/50
2024-12-08 20:40:35.591009: train_loss -0.5496
2024-12-08 20:40:35.591717: val_loss -0.4293
2024-12-08 20:40:35.592399: Pseudo dice [0.6937]
2024-12-08 20:40:35.593031: Epoch time: 89.04 s
2024-12-08 20:40:35.955041: Yayy! New best EMA pseudo Dice: 0.6705
2024-12-08 20:40:37.602376: 
2024-12-08 20:40:37.603947: Epoch 5
2024-12-08 20:40:37.604716: Current learning rate: 0.00995
2024-12-08 20:42:06.873877: Validation loss did not improve from -0.42932. Patience: 1/50
2024-12-08 20:42:06.875157: train_loss -0.556
2024-12-08 20:42:06.876300: val_loss -0.3982
2024-12-08 20:42:06.877028: Pseudo dice [0.6616]
2024-12-08 20:42:06.877738: Epoch time: 89.27 s
2024-12-08 20:42:08.171019: 
2024-12-08 20:42:08.172696: Epoch 6
2024-12-08 20:42:08.173655: Current learning rate: 0.00995
2024-12-08 20:43:37.559469: Validation loss did not improve from -0.42932. Patience: 2/50
2024-12-08 20:43:37.560718: train_loss -0.5752
2024-12-08 20:43:37.561735: val_loss -0.3889
2024-12-08 20:43:37.562376: Pseudo dice [0.6585]
2024-12-08 20:43:37.563048: Epoch time: 89.39 s
2024-12-08 20:43:38.819276: 
2024-12-08 20:43:38.820821: Epoch 7
2024-12-08 20:43:38.821696: Current learning rate: 0.00994
2024-12-08 20:45:08.060930: Validation loss did not improve from -0.42932. Patience: 3/50
2024-12-08 20:45:08.061714: train_loss -0.5893
2024-12-08 20:45:08.062519: val_loss -0.3941
2024-12-08 20:45:08.063305: Pseudo dice [0.6881]
2024-12-08 20:45:08.064180: Epoch time: 89.24 s
2024-12-08 20:45:10.041845: 
2024-12-08 20:45:10.043435: Epoch 8
2024-12-08 20:45:10.044284: Current learning rate: 0.00993
2024-12-08 20:46:39.528519: Validation loss improved from -0.42932 to -0.43608! Patience: 3/50
2024-12-08 20:46:39.529586: train_loss -0.5818
2024-12-08 20:46:39.530367: val_loss -0.4361
2024-12-08 20:46:39.531199: Pseudo dice [0.7003]
2024-12-08 20:46:39.531846: Epoch time: 89.49 s
2024-12-08 20:46:39.532639: Yayy! New best EMA pseudo Dice: 0.6734
2024-12-08 20:46:41.206024: 
2024-12-08 20:46:41.207640: Epoch 9
2024-12-08 20:46:41.208353: Current learning rate: 0.00992
2024-12-08 20:48:10.710040: Validation loss did not improve from -0.43608. Patience: 1/50
2024-12-08 20:48:10.711435: train_loss -0.5851
2024-12-08 20:48:10.713046: val_loss -0.4071
2024-12-08 20:48:10.713931: Pseudo dice [0.6684]
2024-12-08 20:48:10.714791: Epoch time: 89.51 s
2024-12-08 20:48:12.295630: 
2024-12-08 20:48:12.296949: Epoch 10
2024-12-08 20:48:12.297753: Current learning rate: 0.00991
2024-12-08 20:49:41.712707: Validation loss improved from -0.43608 to -0.43735! Patience: 1/50
2024-12-08 20:49:41.713855: train_loss -0.5946
2024-12-08 20:49:41.715060: val_loss -0.4374
2024-12-08 20:49:41.715952: Pseudo dice [0.6993]
2024-12-08 20:49:41.716897: Epoch time: 89.42 s
2024-12-08 20:49:41.717851: Yayy! New best EMA pseudo Dice: 0.6756
2024-12-08 20:49:43.361912: 
2024-12-08 20:49:43.363661: Epoch 11
2024-12-08 20:49:43.364585: Current learning rate: 0.0099
2024-12-08 20:51:12.726373: Validation loss did not improve from -0.43735. Patience: 1/50
2024-12-08 20:51:12.727444: train_loss -0.6058
2024-12-08 20:51:12.728341: val_loss -0.392
2024-12-08 20:51:12.729108: Pseudo dice [0.6613]
2024-12-08 20:51:12.729816: Epoch time: 89.37 s
2024-12-08 20:51:13.964311: 
2024-12-08 20:51:13.966195: Epoch 12
2024-12-08 20:51:13.966977: Current learning rate: 0.00989
2024-12-08 20:52:43.402724: Validation loss did not improve from -0.43735. Patience: 2/50
2024-12-08 20:52:43.403858: train_loss -0.6167
2024-12-08 20:52:43.404883: val_loss -0.3871
2024-12-08 20:52:43.405735: Pseudo dice [0.6603]
2024-12-08 20:52:43.406684: Epoch time: 89.44 s
2024-12-08 20:52:44.647951: 
2024-12-08 20:52:44.648768: Epoch 13
2024-12-08 20:52:44.649512: Current learning rate: 0.00988
2024-12-08 20:54:13.998121: Validation loss did not improve from -0.43735. Patience: 3/50
2024-12-08 20:54:13.998832: train_loss -0.6222
2024-12-08 20:54:13.999824: val_loss -0.3736
2024-12-08 20:54:14.000769: Pseudo dice [0.6614]
2024-12-08 20:54:14.001801: Epoch time: 89.35 s
2024-12-08 20:54:15.242333: 
2024-12-08 20:54:15.243775: Epoch 14
2024-12-08 20:54:15.244548: Current learning rate: 0.00987
2024-12-08 20:55:44.656200: Validation loss improved from -0.43735 to -0.45873! Patience: 3/50
2024-12-08 20:55:44.657192: train_loss -0.6238
2024-12-08 20:55:44.658280: val_loss -0.4587
2024-12-08 20:55:44.659003: Pseudo dice [0.7045]
2024-12-08 20:55:44.659610: Epoch time: 89.42 s
2024-12-08 20:55:46.378085: 
2024-12-08 20:55:46.379636: Epoch 15
2024-12-08 20:55:46.380498: Current learning rate: 0.00986
2024-12-08 20:57:15.766389: Validation loss did not improve from -0.45873. Patience: 1/50
2024-12-08 20:57:15.767440: train_loss -0.6197
2024-12-08 20:57:15.768199: val_loss -0.4245
2024-12-08 20:57:15.768887: Pseudo dice [0.6796]
2024-12-08 20:57:15.769497: Epoch time: 89.39 s
2024-12-08 20:57:17.104786: 
2024-12-08 20:57:17.106182: Epoch 16
2024-12-08 20:57:17.106861: Current learning rate: 0.00986
2024-12-08 20:58:46.889450: Validation loss did not improve from -0.45873. Patience: 2/50
2024-12-08 20:58:46.890760: train_loss -0.6152
2024-12-08 20:58:46.891611: val_loss -0.3966
2024-12-08 20:58:46.892291: Pseudo dice [0.6533]
2024-12-08 20:58:46.893074: Epoch time: 89.79 s
2024-12-08 20:58:48.198246: 
2024-12-08 20:58:48.199133: Epoch 17
2024-12-08 20:58:48.199849: Current learning rate: 0.00985
2024-12-08 21:00:17.909223: Validation loss did not improve from -0.45873. Patience: 3/50
2024-12-08 21:00:17.910504: train_loss -0.6276
2024-12-08 21:00:17.911281: val_loss -0.4212
2024-12-08 21:00:17.912156: Pseudo dice [0.6894]
2024-12-08 21:00:17.912853: Epoch time: 89.71 s
2024-12-08 21:00:19.229126: 
2024-12-08 21:00:19.230779: Epoch 18
2024-12-08 21:00:19.231565: Current learning rate: 0.00984
2024-12-08 21:01:48.960512: Validation loss did not improve from -0.45873. Patience: 4/50
2024-12-08 21:01:48.961299: train_loss -0.6379
2024-12-08 21:01:48.962317: val_loss -0.4065
2024-12-08 21:01:48.963133: Pseudo dice [0.6683]
2024-12-08 21:01:48.963845: Epoch time: 89.73 s
2024-12-08 21:01:50.730235: 
2024-12-08 21:01:50.732529: Epoch 19
2024-12-08 21:01:50.733687: Current learning rate: 0.00983
2024-12-08 21:03:20.520927: Validation loss did not improve from -0.45873. Patience: 5/50
2024-12-08 21:03:20.522068: train_loss -0.6373
2024-12-08 21:03:20.523151: val_loss -0.4158
2024-12-08 21:03:20.523922: Pseudo dice [0.6908]
2024-12-08 21:03:20.524682: Epoch time: 89.79 s
2024-12-08 21:03:20.885264: Yayy! New best EMA pseudo Dice: 0.6758
2024-12-08 21:03:22.551321: 
2024-12-08 21:03:22.552700: Epoch 20
2024-12-08 21:03:22.553489: Current learning rate: 0.00982
2024-12-08 21:04:52.314528: Validation loss did not improve from -0.45873. Patience: 6/50
2024-12-08 21:04:52.315671: train_loss -0.6371
2024-12-08 21:04:52.316510: val_loss -0.4523
2024-12-08 21:04:52.317261: Pseudo dice [0.7048]
2024-12-08 21:04:52.317981: Epoch time: 89.77 s
2024-12-08 21:04:52.318689: Yayy! New best EMA pseudo Dice: 0.6787
2024-12-08 21:04:54.007385: 
2024-12-08 21:04:54.008377: Epoch 21
2024-12-08 21:04:54.009258: Current learning rate: 0.00981
2024-12-08 21:06:23.803313: Validation loss did not improve from -0.45873. Patience: 7/50
2024-12-08 21:06:23.804476: train_loss -0.6323
2024-12-08 21:06:23.805259: val_loss -0.3945
2024-12-08 21:06:23.805872: Pseudo dice [0.6718]
2024-12-08 21:06:23.806505: Epoch time: 89.8 s
2024-12-08 21:06:24.992001: 
2024-12-08 21:06:24.993419: Epoch 22
2024-12-08 21:06:24.994311: Current learning rate: 0.0098
2024-12-08 21:07:54.807716: Validation loss did not improve from -0.45873. Patience: 8/50
2024-12-08 21:07:54.808793: train_loss -0.645
2024-12-08 21:07:54.809553: val_loss -0.4456
2024-12-08 21:07:54.810171: Pseudo dice [0.7077]
2024-12-08 21:07:54.810809: Epoch time: 89.82 s
2024-12-08 21:07:54.811579: Yayy! New best EMA pseudo Dice: 0.681
2024-12-08 21:07:56.440310: 
2024-12-08 21:07:56.441658: Epoch 23
2024-12-08 21:07:56.442600: Current learning rate: 0.00979
2024-12-08 21:09:26.204047: Validation loss improved from -0.45873 to -0.54469! Patience: 8/50
2024-12-08 21:09:26.205228: train_loss -0.6487
2024-12-08 21:09:26.206258: val_loss -0.5447
2024-12-08 21:09:26.207107: Pseudo dice [0.7635]
2024-12-08 21:09:26.207846: Epoch time: 89.77 s
2024-12-08 21:09:26.208661: Yayy! New best EMA pseudo Dice: 0.6892
2024-12-08 21:09:27.825017: 
2024-12-08 21:09:27.826558: Epoch 24
2024-12-08 21:09:27.827398: Current learning rate: 0.00978
2024-12-08 21:10:58.021540: Validation loss did not improve from -0.54469. Patience: 1/50
2024-12-08 21:10:58.022827: train_loss -0.6468
2024-12-08 21:10:58.023557: val_loss -0.454
2024-12-08 21:10:58.024294: Pseudo dice [0.6839]
2024-12-08 21:10:58.024903: Epoch time: 90.2 s
2024-12-08 21:10:59.708460: 
2024-12-08 21:10:59.709110: Epoch 25
2024-12-08 21:10:59.709807: Current learning rate: 0.00977
2024-12-08 21:12:29.754076: Validation loss did not improve from -0.54469. Patience: 2/50
2024-12-08 21:12:29.754742: train_loss -0.6461
2024-12-08 21:12:29.755639: val_loss -0.3374
2024-12-08 21:12:29.756533: Pseudo dice [0.64]
2024-12-08 21:12:29.757293: Epoch time: 90.05 s
2024-12-08 21:12:30.985944: 
2024-12-08 21:12:30.987248: Epoch 26
2024-12-08 21:12:30.988067: Current learning rate: 0.00977
2024-12-08 21:14:01.098298: Validation loss did not improve from -0.54469. Patience: 3/50
2024-12-08 21:14:01.099341: train_loss -0.6535
2024-12-08 21:14:01.100263: val_loss -0.4778
2024-12-08 21:14:01.101023: Pseudo dice [0.7208]
2024-12-08 21:14:01.101738: Epoch time: 90.11 s
2024-12-08 21:14:02.346976: 
2024-12-08 21:14:02.348508: Epoch 27
2024-12-08 21:14:02.349160: Current learning rate: 0.00976
2024-12-08 21:15:32.736149: Validation loss did not improve from -0.54469. Patience: 4/50
2024-12-08 21:15:32.737377: train_loss -0.6599
2024-12-08 21:15:32.738238: val_loss -0.4164
2024-12-08 21:15:32.738935: Pseudo dice [0.6834]
2024-12-08 21:15:32.739610: Epoch time: 90.39 s
2024-12-08 21:15:34.004509: 
2024-12-08 21:15:34.006083: Epoch 28
2024-12-08 21:15:34.006799: Current learning rate: 0.00975
2024-12-08 21:17:04.375422: Validation loss did not improve from -0.54469. Patience: 5/50
2024-12-08 21:17:04.376738: train_loss -0.6584
2024-12-08 21:17:04.378108: val_loss -0.3547
2024-12-08 21:17:04.378891: Pseudo dice [0.6648]
2024-12-08 21:17:04.379626: Epoch time: 90.37 s
2024-12-08 21:17:05.629329: 
2024-12-08 21:17:05.630938: Epoch 29
2024-12-08 21:17:05.631960: Current learning rate: 0.00974
2024-12-08 21:18:36.041708: Validation loss did not improve from -0.54469. Patience: 6/50
2024-12-08 21:18:36.042681: train_loss -0.6586
2024-12-08 21:18:36.043775: val_loss -0.4337
2024-12-08 21:18:36.044378: Pseudo dice [0.7014]
2024-12-08 21:18:36.045034: Epoch time: 90.41 s
2024-12-08 21:18:38.125852: 
2024-12-08 21:18:38.127369: Epoch 30
2024-12-08 21:18:38.128199: Current learning rate: 0.00973
2024-12-08 21:20:08.394343: Validation loss did not improve from -0.54469. Patience: 7/50
2024-12-08 21:20:08.395385: train_loss -0.6657
2024-12-08 21:20:08.396352: val_loss -0.4747
2024-12-08 21:20:08.397169: Pseudo dice [0.7131]
2024-12-08 21:20:08.397864: Epoch time: 90.27 s
2024-12-08 21:20:09.734585: 
2024-12-08 21:20:09.735873: Epoch 31
2024-12-08 21:20:09.736563: Current learning rate: 0.00972
2024-12-08 21:21:39.879257: Validation loss did not improve from -0.54469. Patience: 8/50
2024-12-08 21:21:39.880374: train_loss -0.6707
2024-12-08 21:21:39.881186: val_loss -0.5125
2024-12-08 21:21:39.881851: Pseudo dice [0.7419]
2024-12-08 21:21:39.882600: Epoch time: 90.15 s
2024-12-08 21:21:39.883570: Yayy! New best EMA pseudo Dice: 0.6945
2024-12-08 21:21:41.596097: 
2024-12-08 21:21:41.598080: Epoch 32
2024-12-08 21:21:41.598949: Current learning rate: 0.00971
2024-12-08 21:23:12.113821: Validation loss did not improve from -0.54469. Patience: 9/50
2024-12-08 21:23:12.114967: train_loss -0.6718
2024-12-08 21:23:12.115778: val_loss -0.4561
2024-12-08 21:23:12.116394: Pseudo dice [0.7187]
2024-12-08 21:23:12.117018: Epoch time: 90.52 s
2024-12-08 21:23:12.117744: Yayy! New best EMA pseudo Dice: 0.6969
2024-12-08 21:23:13.788254: 
2024-12-08 21:23:13.789385: Epoch 33
2024-12-08 21:23:13.790127: Current learning rate: 0.0097
2024-12-08 21:24:43.878374: Validation loss did not improve from -0.54469. Patience: 10/50
2024-12-08 21:24:43.879575: train_loss -0.6654
2024-12-08 21:24:43.880444: val_loss -0.36
2024-12-08 21:24:43.881069: Pseudo dice [0.6444]
2024-12-08 21:24:43.881684: Epoch time: 90.09 s
2024-12-08 21:24:45.203558: 
2024-12-08 21:24:45.204951: Epoch 34
2024-12-08 21:24:45.205706: Current learning rate: 0.00969
2024-12-08 21:26:15.389459: Validation loss did not improve from -0.54469. Patience: 11/50
2024-12-08 21:26:15.390429: train_loss -0.6712
2024-12-08 21:26:15.391396: val_loss -0.5195
2024-12-08 21:26:15.392060: Pseudo dice [0.7359]
2024-12-08 21:26:15.392721: Epoch time: 90.19 s
2024-12-08 21:26:17.085861: 
2024-12-08 21:26:17.087287: Epoch 35
2024-12-08 21:26:17.088137: Current learning rate: 0.00968
2024-12-08 21:27:47.144308: Validation loss did not improve from -0.54469. Patience: 12/50
2024-12-08 21:27:47.145415: train_loss -0.6743
2024-12-08 21:27:47.146477: val_loss -0.385
2024-12-08 21:27:47.147827: Pseudo dice [0.6419]
2024-12-08 21:27:47.148891: Epoch time: 90.06 s
2024-12-08 21:27:48.470184: 
2024-12-08 21:27:48.471340: Epoch 36
2024-12-08 21:27:48.472308: Current learning rate: 0.00968
2024-12-08 21:29:18.534485: Validation loss did not improve from -0.54469. Patience: 13/50
2024-12-08 21:29:18.535784: train_loss -0.6825
2024-12-08 21:29:18.537027: val_loss -0.1865
2024-12-08 21:29:18.538186: Pseudo dice [0.5422]
2024-12-08 21:29:18.539136: Epoch time: 90.07 s
2024-12-08 21:29:19.797407: 
2024-12-08 21:29:19.799205: Epoch 37
2024-12-08 21:29:19.800169: Current learning rate: 0.00967
2024-12-08 21:30:49.787367: Validation loss did not improve from -0.54469. Patience: 14/50
2024-12-08 21:30:49.788301: train_loss -0.6872
2024-12-08 21:30:49.789185: val_loss -0.2892
2024-12-08 21:30:49.789968: Pseudo dice [0.6028]
2024-12-08 21:30:49.790837: Epoch time: 89.99 s
2024-12-08 21:30:51.147602: 
2024-12-08 21:30:51.149130: Epoch 38
2024-12-08 21:30:51.150198: Current learning rate: 0.00966
2024-12-08 21:32:21.224691: Validation loss did not improve from -0.54469. Patience: 15/50
2024-12-08 21:32:21.225854: train_loss -0.676
2024-12-08 21:32:21.226707: val_loss -0.4403
2024-12-08 21:32:21.227465: Pseudo dice [0.7]
2024-12-08 21:32:21.228113: Epoch time: 90.08 s
2024-12-08 21:32:22.515734: 
2024-12-08 21:32:22.517221: Epoch 39
2024-12-08 21:32:22.518239: Current learning rate: 0.00965
2024-12-08 21:33:52.590693: Validation loss did not improve from -0.54469. Patience: 16/50
2024-12-08 21:33:52.591670: train_loss -0.6873
2024-12-08 21:33:52.592376: val_loss -0.3532
2024-12-08 21:33:52.593116: Pseudo dice [0.6637]
2024-12-08 21:33:52.593856: Epoch time: 90.08 s
2024-12-08 21:33:54.685318: 
2024-12-08 21:33:54.686907: Epoch 40
2024-12-08 21:33:54.687902: Current learning rate: 0.00964
2024-12-08 21:35:24.757227: Validation loss did not improve from -0.54469. Patience: 17/50
2024-12-08 21:35:24.758464: train_loss -0.68
2024-12-08 21:35:24.759405: val_loss -0.4245
2024-12-08 21:35:24.760221: Pseudo dice [0.6863]
2024-12-08 21:35:24.760892: Epoch time: 90.07 s
2024-12-08 21:35:26.124809: 
2024-12-08 21:35:26.126416: Epoch 41
2024-12-08 21:35:26.127196: Current learning rate: 0.00963
2024-12-08 21:36:56.555747: Validation loss did not improve from -0.54469. Patience: 18/50
2024-12-08 21:36:56.733214: train_loss -0.6881
2024-12-08 21:36:56.734174: val_loss -0.4693
2024-12-08 21:36:56.735112: Pseudo dice [0.7157]
2024-12-08 21:36:56.736025: Epoch time: 90.61 s
2024-12-08 21:36:58.033794: 
2024-12-08 21:36:58.035403: Epoch 42
2024-12-08 21:36:58.036316: Current learning rate: 0.00962
2024-12-08 21:38:28.586683: Validation loss did not improve from -0.54469. Patience: 19/50
2024-12-08 21:38:28.587601: train_loss -0.6915
2024-12-08 21:38:28.588503: val_loss -0.4776
2024-12-08 21:38:28.589142: Pseudo dice [0.7296]
2024-12-08 21:38:28.589998: Epoch time: 90.55 s
2024-12-08 21:38:29.856520: 
2024-12-08 21:38:29.857958: Epoch 43
2024-12-08 21:38:29.858691: Current learning rate: 0.00961
2024-12-08 21:40:00.541335: Validation loss did not improve from -0.54469. Patience: 20/50
2024-12-08 21:40:00.542356: train_loss -0.6912
2024-12-08 21:40:00.543195: val_loss -0.3541
2024-12-08 21:40:00.544017: Pseudo dice [0.6447]
2024-12-08 21:40:00.544677: Epoch time: 90.69 s
2024-12-08 21:40:01.789894: 
2024-12-08 21:40:01.791103: Epoch 44
2024-12-08 21:40:01.791820: Current learning rate: 0.0096
2024-12-08 21:41:32.458032: Validation loss did not improve from -0.54469. Patience: 21/50
2024-12-08 21:41:32.458951: train_loss -0.6916
2024-12-08 21:41:32.459675: val_loss -0.3418
2024-12-08 21:41:32.460417: Pseudo dice [0.6555]
2024-12-08 21:41:32.461073: Epoch time: 90.67 s
2024-12-08 21:41:34.053488: 
2024-12-08 21:41:34.054834: Epoch 45
2024-12-08 21:41:34.055578: Current learning rate: 0.00959
2024-12-08 21:43:04.515973: Validation loss did not improve from -0.54469. Patience: 22/50
2024-12-08 21:43:04.516895: train_loss -0.6997
2024-12-08 21:43:04.517966: val_loss -0.3574
2024-12-08 21:43:04.518940: Pseudo dice [0.658]
2024-12-08 21:43:04.519896: Epoch time: 90.46 s
2024-12-08 21:43:05.822335: 
2024-12-08 21:43:05.824172: Epoch 46
2024-12-08 21:43:05.825206: Current learning rate: 0.00959
2024-12-08 21:44:36.265223: Validation loss did not improve from -0.54469. Patience: 23/50
2024-12-08 21:44:36.266371: train_loss -0.6982
2024-12-08 21:44:36.267214: val_loss -0.4939
2024-12-08 21:44:36.267911: Pseudo dice [0.7313]
2024-12-08 21:44:36.268599: Epoch time: 90.44 s
2024-12-08 21:44:37.498868: 
2024-12-08 21:44:37.500446: Epoch 47
2024-12-08 21:44:37.501183: Current learning rate: 0.00958
2024-12-08 21:46:07.667746: Validation loss did not improve from -0.54469. Patience: 24/50
2024-12-08 21:46:07.668988: train_loss -0.6976
2024-12-08 21:46:07.670118: val_loss -0.4278
2024-12-08 21:46:07.670874: Pseudo dice [0.6911]
2024-12-08 21:46:07.671582: Epoch time: 90.17 s
2024-12-08 21:46:08.898318: 
2024-12-08 21:46:08.899686: Epoch 48
2024-12-08 21:46:08.900422: Current learning rate: 0.00957
2024-12-08 21:47:38.884787: Validation loss did not improve from -0.54469. Patience: 25/50
2024-12-08 21:47:38.885566: train_loss -0.6919
2024-12-08 21:47:38.886442: val_loss -0.3691
2024-12-08 21:47:38.887291: Pseudo dice [0.6637]
2024-12-08 21:47:38.887950: Epoch time: 89.99 s
2024-12-08 21:47:40.143334: 
2024-12-08 21:47:40.144869: Epoch 49
2024-12-08 21:47:40.145616: Current learning rate: 0.00956
2024-12-08 21:49:10.225524: Validation loss did not improve from -0.54469. Patience: 26/50
2024-12-08 21:49:10.226494: train_loss -0.6948
2024-12-08 21:49:10.227597: val_loss -0.4371
2024-12-08 21:49:10.228485: Pseudo dice [0.6996]
2024-12-08 21:49:10.229380: Epoch time: 90.08 s
2024-12-08 21:49:11.875547: 
2024-12-08 21:49:11.877316: Epoch 50
2024-12-08 21:49:11.879815: Current learning rate: 0.00955
2024-12-08 21:50:41.828754: Validation loss did not improve from -0.54469. Patience: 27/50
2024-12-08 21:50:41.829890: train_loss -0.6999
2024-12-08 21:50:41.830799: val_loss -0.3882
2024-12-08 21:50:41.831581: Pseudo dice [0.6651]
2024-12-08 21:50:41.832445: Epoch time: 89.96 s
2024-12-08 21:50:43.682799: 
2024-12-08 21:50:43.684245: Epoch 51
2024-12-08 21:50:43.685265: Current learning rate: 0.00954
2024-12-08 21:52:13.678221: Validation loss did not improve from -0.54469. Patience: 28/50
2024-12-08 21:52:13.679182: train_loss -0.7
2024-12-08 21:52:13.680040: val_loss -0.416
2024-12-08 21:52:13.680773: Pseudo dice [0.6922]
2024-12-08 21:52:13.681533: Epoch time: 90.0 s
2024-12-08 21:52:14.991891: 
2024-12-08 21:52:14.993287: Epoch 52
2024-12-08 21:52:14.994311: Current learning rate: 0.00953
2024-12-08 21:53:45.320988: Validation loss did not improve from -0.54469. Patience: 29/50
2024-12-08 21:53:45.321942: train_loss -0.6984
2024-12-08 21:53:45.322736: val_loss -0.4338
2024-12-08 21:53:45.323469: Pseudo dice [0.6957]
2024-12-08 21:53:45.324206: Epoch time: 90.33 s
2024-12-08 21:53:46.607337: 
2024-12-08 21:53:46.608768: Epoch 53
2024-12-08 21:53:46.609490: Current learning rate: 0.00952
2024-12-08 21:55:16.922675: Validation loss did not improve from -0.54469. Patience: 30/50
2024-12-08 21:55:16.923807: train_loss -0.6942
2024-12-08 21:55:16.924709: val_loss -0.4208
2024-12-08 21:55:16.925346: Pseudo dice [0.6829]
2024-12-08 21:55:16.926001: Epoch time: 90.32 s
2024-12-08 21:55:18.147134: 
2024-12-08 21:55:18.148746: Epoch 54
2024-12-08 21:55:18.149509: Current learning rate: 0.00951
2024-12-08 21:56:48.586206: Validation loss did not improve from -0.54469. Patience: 31/50
2024-12-08 21:56:48.586898: train_loss -0.7004
2024-12-08 21:56:48.587780: val_loss -0.471
2024-12-08 21:56:48.588697: Pseudo dice [0.7213]
2024-12-08 21:56:48.589497: Epoch time: 90.44 s
2024-12-08 21:56:50.216427: 
2024-12-08 21:56:50.217932: Epoch 55
2024-12-08 21:56:50.218754: Current learning rate: 0.0095
2024-12-08 21:58:20.612730: Validation loss did not improve from -0.54469. Patience: 32/50
2024-12-08 21:58:20.613931: train_loss -0.7036
2024-12-08 21:58:20.614869: val_loss -0.4571
2024-12-08 21:58:20.615568: Pseudo dice [0.7126]
2024-12-08 21:58:20.616309: Epoch time: 90.4 s
2024-12-08 21:58:21.872726: 
2024-12-08 21:58:21.873594: Epoch 56
2024-12-08 21:58:21.874397: Current learning rate: 0.00949
2024-12-08 21:59:52.020086: Validation loss did not improve from -0.54469. Patience: 33/50
2024-12-08 21:59:52.021104: train_loss -0.7068
2024-12-08 21:59:52.021958: val_loss -0.4829
2024-12-08 21:59:52.022720: Pseudo dice [0.7304]
2024-12-08 21:59:52.023593: Epoch time: 90.15 s
2024-12-08 21:59:53.266662: 
2024-12-08 21:59:53.267976: Epoch 57
2024-12-08 21:59:53.268723: Current learning rate: 0.00949
2024-12-08 22:01:23.411575: Validation loss did not improve from -0.54469. Patience: 34/50
2024-12-08 22:01:23.413182: train_loss -0.7071
2024-12-08 22:01:23.414674: val_loss -0.4963
2024-12-08 22:01:23.415368: Pseudo dice [0.7354]
2024-12-08 22:01:23.416024: Epoch time: 90.15 s
2024-12-08 22:01:23.416629: Yayy! New best EMA pseudo Dice: 0.6973
2024-12-08 22:01:24.954234: 
2024-12-08 22:01:24.955812: Epoch 58
2024-12-08 22:01:24.956645: Current learning rate: 0.00948
2024-12-08 22:02:54.771332: Validation loss did not improve from -0.54469. Patience: 35/50
2024-12-08 22:02:54.772598: train_loss -0.7049
2024-12-08 22:02:54.773517: val_loss -0.4294
2024-12-08 22:02:54.774269: Pseudo dice [0.6853]
2024-12-08 22:02:54.774883: Epoch time: 89.82 s
2024-12-08 22:02:56.009868: 
2024-12-08 22:02:56.011335: Epoch 59
2024-12-08 22:02:56.012667: Current learning rate: 0.00947
2024-12-08 22:04:25.705711: Validation loss did not improve from -0.54469. Patience: 36/50
2024-12-08 22:04:25.707050: train_loss -0.7028
2024-12-08 22:04:25.708033: val_loss -0.4093
2024-12-08 22:04:25.708816: Pseudo dice [0.681]
2024-12-08 22:04:25.709468: Epoch time: 89.7 s
2024-12-08 22:04:27.387845: 
2024-12-08 22:04:27.389981: Epoch 60
2024-12-08 22:04:27.391119: Current learning rate: 0.00946
2024-12-08 22:05:57.061330: Validation loss did not improve from -0.54469. Patience: 37/50
2024-12-08 22:05:57.062449: train_loss -0.7069
2024-12-08 22:05:57.063592: val_loss -0.413
2024-12-08 22:05:57.064515: Pseudo dice [0.6781]
2024-12-08 22:05:57.065377: Epoch time: 89.68 s
2024-12-08 22:05:58.353240: 
2024-12-08 22:05:58.354655: Epoch 61
2024-12-08 22:05:58.355350: Current learning rate: 0.00945
2024-12-08 22:07:28.036355: Validation loss did not improve from -0.54469. Patience: 38/50
2024-12-08 22:07:28.037529: train_loss -0.7067
2024-12-08 22:07:28.038275: val_loss -0.3712
2024-12-08 22:07:28.038989: Pseudo dice [0.6614]
2024-12-08 22:07:28.039795: Epoch time: 89.69 s
2024-12-08 22:07:29.844212: 
2024-12-08 22:07:29.845530: Epoch 62
2024-12-08 22:07:29.846507: Current learning rate: 0.00944
2024-12-08 22:08:59.380445: Validation loss did not improve from -0.54469. Patience: 39/50
2024-12-08 22:08:59.381520: train_loss -0.7038
2024-12-08 22:08:59.382333: val_loss -0.5145
2024-12-08 22:08:59.382971: Pseudo dice [0.7362]
2024-12-08 22:08:59.383647: Epoch time: 89.54 s
2024-12-08 22:09:00.634178: 
2024-12-08 22:09:00.635533: Epoch 63
2024-12-08 22:09:00.636291: Current learning rate: 0.00943
2024-12-08 22:10:30.349397: Validation loss did not improve from -0.54469. Patience: 40/50
2024-12-08 22:10:30.350580: train_loss -0.7078
2024-12-08 22:10:30.351606: val_loss -0.3489
2024-12-08 22:10:30.352458: Pseudo dice [0.646]
2024-12-08 22:10:30.353318: Epoch time: 89.72 s
2024-12-08 22:10:31.664866: 
2024-12-08 22:10:31.666486: Epoch 64
2024-12-08 22:10:31.667418: Current learning rate: 0.00942
2024-12-08 22:12:01.179865: Validation loss did not improve from -0.54469. Patience: 41/50
2024-12-08 22:12:01.180795: train_loss -0.7085
2024-12-08 22:12:01.181622: val_loss -0.4036
2024-12-08 22:12:01.182385: Pseudo dice [0.6836]
2024-12-08 22:12:01.183078: Epoch time: 89.52 s
2024-12-08 22:12:02.850698: 
2024-12-08 22:12:02.852390: Epoch 65
2024-12-08 22:12:02.853208: Current learning rate: 0.00941
2024-12-08 22:13:32.305958: Validation loss did not improve from -0.54469. Patience: 42/50
2024-12-08 22:13:32.307285: train_loss -0.7092
2024-12-08 22:13:32.308600: val_loss -0.405
2024-12-08 22:13:32.309671: Pseudo dice [0.681]
2024-12-08 22:13:32.310753: Epoch time: 89.46 s
2024-12-08 22:13:33.614521: 
2024-12-08 22:13:33.615935: Epoch 66
2024-12-08 22:13:33.617204: Current learning rate: 0.0094
2024-12-08 22:15:03.449545: Validation loss did not improve from -0.54469. Patience: 43/50
2024-12-08 22:15:03.450567: train_loss -0.7168
2024-12-08 22:15:03.451833: val_loss -0.4548
2024-12-08 22:15:03.452776: Pseudo dice [0.7151]
2024-12-08 22:15:03.453724: Epoch time: 89.84 s
2024-12-08 22:15:04.720726: 
2024-12-08 22:15:04.722238: Epoch 67
2024-12-08 22:15:04.723044: Current learning rate: 0.00939
2024-12-08 22:16:34.732334: Validation loss did not improve from -0.54469. Patience: 44/50
2024-12-08 22:16:34.733678: train_loss -0.7109
2024-12-08 22:16:34.734748: val_loss -0.415
2024-12-08 22:16:34.735727: Pseudo dice [0.692]
2024-12-08 22:16:34.736509: Epoch time: 90.01 s
2024-12-08 22:16:36.053318: 
2024-12-08 22:16:36.054541: Epoch 68
2024-12-08 22:16:36.055251: Current learning rate: 0.00939
2024-12-08 22:18:06.142727: Validation loss did not improve from -0.54469. Patience: 45/50
2024-12-08 22:18:06.143690: train_loss -0.7115
2024-12-08 22:18:06.144597: val_loss -0.3201
2024-12-08 22:18:06.145256: Pseudo dice [0.6449]
2024-12-08 22:18:06.145883: Epoch time: 90.09 s
2024-12-08 22:18:07.448118: 
2024-12-08 22:18:07.450109: Epoch 69
2024-12-08 22:18:07.450868: Current learning rate: 0.00938
2024-12-08 22:19:37.582972: Validation loss did not improve from -0.54469. Patience: 46/50
2024-12-08 22:19:37.583855: train_loss -0.7167
2024-12-08 22:19:37.584835: val_loss -0.3386
2024-12-08 22:19:37.585526: Pseudo dice [0.6434]
2024-12-08 22:19:37.586298: Epoch time: 90.14 s
2024-12-08 22:19:39.251625: 
2024-12-08 22:19:39.252773: Epoch 70
2024-12-08 22:19:39.253548: Current learning rate: 0.00937
2024-12-08 22:21:09.338151: Validation loss did not improve from -0.54469. Patience: 47/50
2024-12-08 22:21:09.339382: train_loss -0.7162
2024-12-08 22:21:09.340652: val_loss -0.4456
2024-12-08 22:21:09.341372: Pseudo dice [0.7068]
2024-12-08 22:21:09.341960: Epoch time: 90.09 s
2024-12-08 22:21:10.641517: 
2024-12-08 22:21:10.643121: Epoch 71
2024-12-08 22:21:10.643800: Current learning rate: 0.00936
2024-12-08 22:22:40.787563: Validation loss did not improve from -0.54469. Patience: 48/50
2024-12-08 22:22:40.788796: train_loss -0.7146
2024-12-08 22:22:40.789567: val_loss -0.4473
2024-12-08 22:22:40.790356: Pseudo dice [0.6933]
2024-12-08 22:22:40.791046: Epoch time: 90.15 s
2024-12-08 22:22:42.084442: 
2024-12-08 22:22:42.085780: Epoch 72
2024-12-08 22:22:42.086460: Current learning rate: 0.00935
2024-12-08 22:24:12.204829: Validation loss did not improve from -0.54469. Patience: 49/50
2024-12-08 22:24:12.205786: train_loss -0.7146
2024-12-08 22:24:12.206693: val_loss -0.3936
2024-12-08 22:24:12.207425: Pseudo dice [0.6721]
2024-12-08 22:24:12.208051: Epoch time: 90.12 s
2024-12-08 22:24:14.029150: 
2024-12-08 22:24:14.030143: Epoch 73
2024-12-08 22:24:14.030828: Current learning rate: 0.00934
2024-12-08 22:25:44.195443: Validation loss did not improve from -0.54469. Patience: 50/50
2024-12-08 22:25:44.196536: train_loss -0.7245
2024-12-08 22:25:44.197504: val_loss -0.4372
2024-12-08 22:25:44.198325: Pseudo dice [0.6939]
2024-12-08 22:25:44.199002: Epoch time: 90.17 s
2024-12-08 22:25:45.483534: Patience reached. Stopping training.
2024-12-08 22:25:45.918582: Training done.
2024-12-08 22:25:46.133756: Using splits from existing split file: /home/gridsan/nchutisilp/datasets/nnUNet_Datasets/nnUNet_preprocessed/Dataset308_Sohee_Calcium_OCT_CrossValidation/splits_final.json
2024-12-08 22:25:46.136768: The split file contains 5 splits.
2024-12-08 22:25:46.137647: Desired fold for training: 3
2024-12-08 22:25:46.138294: This split has 7 training and 1 validation cases.
2024-12-08 22:25:46.139059: predicting 701-013
2024-12-08 22:25:46.150335: 701-013, shape torch.Size([1, 375, 498, 498]), rank 0
2024-12-08 22:27:49.917984: Validation complete
2024-12-08 22:27:49.919241: Mean Validation Dice:  0.7212616379631795
2024-12-08 20:31:44.893220: unpacking done...
2024-12-08 20:31:44.905158: Unable to plot network architecture: nnUNet_compile is enabled!
2024-12-08 20:31:44.958676: 
2024-12-08 20:31:44.960800: Epoch 0
2024-12-08 20:31:44.961958: Current learning rate: 0.01
2024-12-08 20:34:35.263936: Validation loss improved from 1000.00000 to -0.43920! Patience: 0/50
2024-12-08 20:34:35.265850: train_loss -0.3089
2024-12-08 20:34:35.267475: val_loss -0.4392
2024-12-08 20:34:35.268626: Pseudo dice [0.6796]
2024-12-08 20:34:35.269593: Epoch time: 170.31 s
2024-12-08 20:34:35.270512: Yayy! New best EMA pseudo Dice: 0.6796
2024-12-08 20:34:36.684956: 
2024-12-08 20:34:36.686245: Epoch 1
2024-12-08 20:34:36.687160: Current learning rate: 0.00999
2024-12-08 20:36:06.694974: Validation loss did not improve from -0.43920. Patience: 1/50
2024-12-08 20:36:06.696079: train_loss -0.4427
2024-12-08 20:36:06.697304: val_loss -0.42
2024-12-08 20:36:06.698139: Pseudo dice [0.6686]
2024-12-08 20:36:06.699068: Epoch time: 90.01 s
2024-12-08 20:36:07.920949: 
2024-12-08 20:36:07.922524: Epoch 2
2024-12-08 20:36:07.923338: Current learning rate: 0.00998
2024-12-08 20:37:38.749233: Validation loss improved from -0.43920 to -0.49512! Patience: 1/50
2024-12-08 20:37:38.750370: train_loss -0.4865
2024-12-08 20:37:38.751494: val_loss -0.4951
2024-12-08 20:37:38.752484: Pseudo dice [0.714]
2024-12-08 20:37:38.753361: Epoch time: 90.83 s
2024-12-08 20:37:38.754248: Yayy! New best EMA pseudo Dice: 0.6821
2024-12-08 20:37:40.364742: 
2024-12-08 20:37:40.366303: Epoch 3
2024-12-08 20:37:40.367505: Current learning rate: 0.00997
2024-12-08 20:39:11.351835: Validation loss improved from -0.49512 to -0.52537! Patience: 0/50
2024-12-08 20:39:11.352623: train_loss -0.5101
2024-12-08 20:39:11.353854: val_loss -0.5254
2024-12-08 20:39:11.354628: Pseudo dice [0.7254]
2024-12-08 20:39:11.355784: Epoch time: 90.99 s
2024-12-08 20:39:11.356595: Yayy! New best EMA pseudo Dice: 0.6864
2024-12-08 20:39:12.976890: 
2024-12-08 20:39:12.978827: Epoch 4
2024-12-08 20:39:12.980119: Current learning rate: 0.00996
2024-12-08 20:40:44.056370: Validation loss did not improve from -0.52537. Patience: 1/50
2024-12-08 20:40:44.057524: train_loss -0.529
2024-12-08 20:40:44.058458: val_loss -0.493
2024-12-08 20:40:44.059372: Pseudo dice [0.7066]
2024-12-08 20:40:44.060170: Epoch time: 91.08 s
2024-12-08 20:40:44.450794: Yayy! New best EMA pseudo Dice: 0.6884
2024-12-08 20:40:46.084326: 
2024-12-08 20:40:46.085919: Epoch 5
2024-12-08 20:40:46.086606: Current learning rate: 0.00995
2024-12-08 20:42:17.263601: Validation loss did not improve from -0.52537. Patience: 2/50
2024-12-08 20:42:17.264888: train_loss -0.5458
2024-12-08 20:42:17.266254: val_loss -0.5144
2024-12-08 20:42:17.267199: Pseudo dice [0.7203]
2024-12-08 20:42:17.268115: Epoch time: 91.18 s
2024-12-08 20:42:17.268970: Yayy! New best EMA pseudo Dice: 0.6916
2024-12-08 20:42:18.886219: 
2024-12-08 20:42:18.887974: Epoch 6
2024-12-08 20:42:18.888854: Current learning rate: 0.00995
2024-12-08 20:43:50.052717: Validation loss did not improve from -0.52537. Patience: 3/50
2024-12-08 20:43:50.054023: train_loss -0.5605
2024-12-08 20:43:50.055062: val_loss -0.521
2024-12-08 20:43:50.055985: Pseudo dice [0.7221]
2024-12-08 20:43:50.056775: Epoch time: 91.17 s
2024-12-08 20:43:50.057631: Yayy! New best EMA pseudo Dice: 0.6947
2024-12-08 20:43:51.743327: 
2024-12-08 20:43:51.744916: Epoch 7
2024-12-08 20:43:51.745738: Current learning rate: 0.00994
2024-12-08 20:45:22.937623: Validation loss improved from -0.52537 to -0.55809! Patience: 3/50
2024-12-08 20:45:22.939070: train_loss -0.5623
2024-12-08 20:45:22.940183: val_loss -0.5581
2024-12-08 20:45:22.941009: Pseudo dice [0.7464]
2024-12-08 20:45:22.941838: Epoch time: 91.2 s
2024-12-08 20:45:22.942845: Yayy! New best EMA pseudo Dice: 0.6998
2024-12-08 20:45:25.106832: 
2024-12-08 20:45:25.108448: Epoch 8
2024-12-08 20:45:25.109636: Current learning rate: 0.00993
2024-12-08 20:46:56.307692: Validation loss did not improve from -0.55809. Patience: 1/50
2024-12-08 20:46:56.308820: train_loss -0.5838
2024-12-08 20:46:56.309654: val_loss -0.5496
2024-12-08 20:46:56.310506: Pseudo dice [0.7421]
2024-12-08 20:46:56.311150: Epoch time: 91.2 s
2024-12-08 20:46:56.311835: Yayy! New best EMA pseudo Dice: 0.7041
2024-12-08 20:46:58.000830: 
2024-12-08 20:46:58.002731: Epoch 9
2024-12-08 20:46:58.003686: Current learning rate: 0.00992
2024-12-08 20:48:29.231444: Validation loss improved from -0.55809 to -0.56595! Patience: 1/50
2024-12-08 20:48:29.232223: train_loss -0.5902
2024-12-08 20:48:29.232988: val_loss -0.5659
2024-12-08 20:48:29.233686: Pseudo dice [0.7552]
2024-12-08 20:48:29.234457: Epoch time: 91.23 s
2024-12-08 20:48:29.602245: Yayy! New best EMA pseudo Dice: 0.7092
2024-12-08 20:48:31.205635: 
2024-12-08 20:48:31.207480: Epoch 10
2024-12-08 20:48:31.208214: Current learning rate: 0.00991
2024-12-08 20:50:02.441465: Validation loss did not improve from -0.56595. Patience: 1/50
2024-12-08 20:50:02.442752: train_loss -0.5866
2024-12-08 20:50:02.443677: val_loss -0.5248
2024-12-08 20:50:02.444485: Pseudo dice [0.7211]
2024-12-08 20:50:02.445433: Epoch time: 91.24 s
2024-12-08 20:50:02.446318: Yayy! New best EMA pseudo Dice: 0.7104
2024-12-08 20:50:04.043863: 
2024-12-08 20:50:04.045353: Epoch 11
2024-12-08 20:50:04.046084: Current learning rate: 0.0099
2024-12-08 20:51:35.237399: Validation loss improved from -0.56595 to -0.57200! Patience: 1/50
2024-12-08 20:51:35.238376: train_loss -0.5872
2024-12-08 20:51:35.239269: val_loss -0.572
2024-12-08 20:51:35.240018: Pseudo dice [0.7498]
2024-12-08 20:51:35.240833: Epoch time: 91.2 s
2024-12-08 20:51:35.241907: Yayy! New best EMA pseudo Dice: 0.7143
2024-12-08 20:51:36.879749: 
2024-12-08 20:51:36.882355: Epoch 12
2024-12-08 20:51:36.884165: Current learning rate: 0.00989
2024-12-08 20:53:08.191350: Validation loss did not improve from -0.57200. Patience: 1/50
2024-12-08 20:53:08.192495: train_loss -0.6034
2024-12-08 20:53:08.193474: val_loss -0.5435
2024-12-08 20:53:08.194193: Pseudo dice [0.7381]
2024-12-08 20:53:08.195123: Epoch time: 91.32 s
2024-12-08 20:53:08.195848: Yayy! New best EMA pseudo Dice: 0.7167
2024-12-08 20:53:09.836103: 
2024-12-08 20:53:09.837683: Epoch 13
2024-12-08 20:53:09.838493: Current learning rate: 0.00988
2024-12-08 20:54:41.021022: Validation loss did not improve from -0.57200. Patience: 2/50
2024-12-08 20:54:41.021974: train_loss -0.6108
2024-12-08 20:54:41.022890: val_loss -0.5489
2024-12-08 20:54:41.023581: Pseudo dice [0.7395]
2024-12-08 20:54:41.024322: Epoch time: 91.19 s
2024-12-08 20:54:41.024929: Yayy! New best EMA pseudo Dice: 0.719
2024-12-08 20:54:42.661482: 
2024-12-08 20:54:42.663185: Epoch 14
2024-12-08 20:54:42.664208: Current learning rate: 0.00987
2024-12-08 20:56:13.782195: Validation loss did not improve from -0.57200. Patience: 3/50
2024-12-08 20:56:13.783040: train_loss -0.6181
2024-12-08 20:56:13.783850: val_loss -0.568
2024-12-08 20:56:13.784647: Pseudo dice [0.7492]
2024-12-08 20:56:13.785450: Epoch time: 91.12 s
2024-12-08 20:56:14.184340: Yayy! New best EMA pseudo Dice: 0.722
2024-12-08 20:56:16.141852: 
2024-12-08 20:56:16.144207: Epoch 15
2024-12-08 20:56:16.145155: Current learning rate: 0.00986
2024-12-08 20:57:47.353771: Validation loss did not improve from -0.57200. Patience: 4/50
2024-12-08 20:57:47.355240: train_loss -0.6175
2024-12-08 20:57:47.356476: val_loss -0.5632
2024-12-08 20:57:47.357291: Pseudo dice [0.7487]
2024-12-08 20:57:47.358117: Epoch time: 91.21 s
2024-12-08 20:57:47.358766: Yayy! New best EMA pseudo Dice: 0.7247
2024-12-08 20:57:49.064185: 
2024-12-08 20:57:49.065662: Epoch 16
2024-12-08 20:57:49.066335: Current learning rate: 0.00986
2024-12-08 20:59:20.288896: Validation loss did not improve from -0.57200. Patience: 5/50
2024-12-08 20:59:20.290070: train_loss -0.6298
2024-12-08 20:59:20.291357: val_loss -0.5622
2024-12-08 20:59:20.292296: Pseudo dice [0.746]
2024-12-08 20:59:20.293146: Epoch time: 91.23 s
2024-12-08 20:59:20.294053: Yayy! New best EMA pseudo Dice: 0.7268
2024-12-08 20:59:22.027119: 
2024-12-08 20:59:22.029073: Epoch 17
2024-12-08 20:59:22.030051: Current learning rate: 0.00985
2024-12-08 21:00:53.233460: Validation loss did not improve from -0.57200. Patience: 6/50
2024-12-08 21:00:53.234524: train_loss -0.628
2024-12-08 21:00:53.235449: val_loss -0.5602
2024-12-08 21:00:53.236345: Pseudo dice [0.7506]
2024-12-08 21:00:53.237036: Epoch time: 91.21 s
2024-12-08 21:00:53.237660: Yayy! New best EMA pseudo Dice: 0.7292
2024-12-08 21:00:54.873355: 
2024-12-08 21:00:54.875276: Epoch 18
2024-12-08 21:00:54.875984: Current learning rate: 0.00984
2024-12-08 21:02:26.262059: Validation loss did not improve from -0.57200. Patience: 7/50
2024-12-08 21:02:26.264462: train_loss -0.6233
2024-12-08 21:02:26.265676: val_loss -0.5075
2024-12-08 21:02:26.266474: Pseudo dice [0.7119]
2024-12-08 21:02:26.267298: Epoch time: 91.39 s
2024-12-08 21:02:28.054670: 
2024-12-08 21:02:28.056494: Epoch 19
2024-12-08 21:02:28.057518: Current learning rate: 0.00983
2024-12-08 21:03:59.309416: Validation loss did not improve from -0.57200. Patience: 8/50
2024-12-08 21:03:59.310825: train_loss -0.6354
2024-12-08 21:03:59.311795: val_loss -0.5617
2024-12-08 21:03:59.312594: Pseudo dice [0.7516]
2024-12-08 21:03:59.313412: Epoch time: 91.26 s
2024-12-08 21:03:59.700305: Yayy! New best EMA pseudo Dice: 0.7299
2024-12-08 21:04:01.364498: 
2024-12-08 21:04:01.365932: Epoch 20
2024-12-08 21:04:01.366752: Current learning rate: 0.00982
2024-12-08 21:05:32.626899: Validation loss improved from -0.57200 to -0.58463! Patience: 8/50
2024-12-08 21:05:32.629258: train_loss -0.6357
2024-12-08 21:05:32.630491: val_loss -0.5846
2024-12-08 21:05:32.631194: Pseudo dice [0.763]
2024-12-08 21:05:32.632076: Epoch time: 91.27 s
2024-12-08 21:05:32.632871: Yayy! New best EMA pseudo Dice: 0.7332
2024-12-08 21:05:34.400325: 
2024-12-08 21:05:34.402052: Epoch 21
2024-12-08 21:05:34.403011: Current learning rate: 0.00981
2024-12-08 21:07:05.721855: Validation loss did not improve from -0.58463. Patience: 1/50
2024-12-08 21:07:05.723325: train_loss -0.6357
2024-12-08 21:07:05.724375: val_loss -0.5684
2024-12-08 21:07:05.725119: Pseudo dice [0.749]
2024-12-08 21:07:05.725837: Epoch time: 91.32 s
2024-12-08 21:07:05.726487: Yayy! New best EMA pseudo Dice: 0.7348
2024-12-08 21:07:07.383051: 
2024-12-08 21:07:07.384622: Epoch 22
2024-12-08 21:07:07.385468: Current learning rate: 0.0098
2024-12-08 21:08:38.652793: Validation loss did not improve from -0.58463. Patience: 2/50
2024-12-08 21:08:38.653612: train_loss -0.6337
2024-12-08 21:08:38.654590: val_loss -0.5635
2024-12-08 21:08:38.655436: Pseudo dice [0.7418]
2024-12-08 21:08:38.656257: Epoch time: 91.27 s
2024-12-08 21:08:38.657209: Yayy! New best EMA pseudo Dice: 0.7355
2024-12-08 21:08:40.328293: 
2024-12-08 21:08:40.329934: Epoch 23
2024-12-08 21:08:40.330805: Current learning rate: 0.00979
2024-12-08 21:10:11.585025: Validation loss improved from -0.58463 to -0.60137! Patience: 2/50
2024-12-08 21:10:11.586162: train_loss -0.6452
2024-12-08 21:10:11.587107: val_loss -0.6014
2024-12-08 21:10:11.588106: Pseudo dice [0.7702]
2024-12-08 21:10:11.588990: Epoch time: 91.26 s
2024-12-08 21:10:11.589810: Yayy! New best EMA pseudo Dice: 0.7389
2024-12-08 21:10:13.232995: 
2024-12-08 21:10:13.234924: Epoch 24
2024-12-08 21:10:13.235875: Current learning rate: 0.00978
2024-12-08 21:11:44.640410: Validation loss did not improve from -0.60137. Patience: 1/50
2024-12-08 21:11:44.641293: train_loss -0.6463
2024-12-08 21:11:44.642373: val_loss -0.5681
2024-12-08 21:11:44.643263: Pseudo dice [0.7565]
2024-12-08 21:11:44.644067: Epoch time: 91.41 s
2024-12-08 21:11:45.083261: Yayy! New best EMA pseudo Dice: 0.7407
2024-12-08 21:11:46.761387: 
2024-12-08 21:11:46.763012: Epoch 25
2024-12-08 21:11:46.763920: Current learning rate: 0.00977
2024-12-08 21:13:18.158053: Validation loss did not improve from -0.60137. Patience: 2/50
2024-12-08 21:13:18.159314: train_loss -0.6496
2024-12-08 21:13:18.160236: val_loss -0.5649
2024-12-08 21:13:18.160956: Pseudo dice [0.7468]
2024-12-08 21:13:18.161666: Epoch time: 91.4 s
2024-12-08 21:13:18.162312: Yayy! New best EMA pseudo Dice: 0.7413
2024-12-08 21:13:19.811481: 
2024-12-08 21:13:19.813015: Epoch 26
2024-12-08 21:13:19.813682: Current learning rate: 0.00977
2024-12-08 21:14:51.395278: Validation loss did not improve from -0.60137. Patience: 3/50
2024-12-08 21:14:51.396376: train_loss -0.6499
2024-12-08 21:14:51.397717: val_loss -0.5767
2024-12-08 21:14:51.398696: Pseudo dice [0.7574]
2024-12-08 21:14:51.399621: Epoch time: 91.59 s
2024-12-08 21:14:51.400753: Yayy! New best EMA pseudo Dice: 0.7429
2024-12-08 21:14:53.039552: 
2024-12-08 21:14:53.040762: Epoch 27
2024-12-08 21:14:53.041899: Current learning rate: 0.00976
2024-12-08 21:16:24.521833: Validation loss did not improve from -0.60137. Patience: 4/50
2024-12-08 21:16:24.522972: train_loss -0.6512
2024-12-08 21:16:24.524226: val_loss -0.5413
2024-12-08 21:16:24.524918: Pseudo dice [0.7354]
2024-12-08 21:16:24.525720: Epoch time: 91.48 s
2024-12-08 21:16:25.821089: 
2024-12-08 21:16:25.822448: Epoch 28
2024-12-08 21:16:25.823407: Current learning rate: 0.00975
2024-12-08 21:17:57.290840: Validation loss did not improve from -0.60137. Patience: 5/50
2024-12-08 21:17:57.291848: train_loss -0.6506
2024-12-08 21:17:57.292892: val_loss -0.5741
2024-12-08 21:17:57.293670: Pseudo dice [0.7551]
2024-12-08 21:17:57.294455: Epoch time: 91.47 s
2024-12-08 21:17:57.295216: Yayy! New best EMA pseudo Dice: 0.7435
2024-12-08 21:17:58.926875: 
2024-12-08 21:17:58.928746: Epoch 29
2024-12-08 21:17:58.929786: Current learning rate: 0.00974
2024-12-08 21:19:30.443266: Validation loss improved from -0.60137 to -0.60230! Patience: 5/50
2024-12-08 21:19:30.444374: train_loss -0.6546
2024-12-08 21:19:30.445306: val_loss -0.6023
2024-12-08 21:19:30.446179: Pseudo dice [0.7756]
2024-12-08 21:19:30.446974: Epoch time: 91.52 s
2024-12-08 21:19:30.822061: Yayy! New best EMA pseudo Dice: 0.7467
2024-12-08 21:19:32.907560: 
2024-12-08 21:19:32.909191: Epoch 30
2024-12-08 21:19:32.910134: Current learning rate: 0.00973
2024-12-08 21:21:04.285731: Validation loss did not improve from -0.60230. Patience: 1/50
2024-12-08 21:21:04.286941: train_loss -0.6483
2024-12-08 21:21:04.287939: val_loss -0.5356
2024-12-08 21:21:04.288724: Pseudo dice [0.7366]
2024-12-08 21:21:04.289593: Epoch time: 91.38 s
2024-12-08 21:21:05.574633: 
2024-12-08 21:21:05.576020: Epoch 31
2024-12-08 21:21:05.576891: Current learning rate: 0.00972
2024-12-08 21:22:37.010857: Validation loss did not improve from -0.60230. Patience: 2/50
2024-12-08 21:22:37.011650: train_loss -0.661
2024-12-08 21:22:37.012488: val_loss -0.5527
2024-12-08 21:22:37.013169: Pseudo dice [0.7441]
2024-12-08 21:22:37.013829: Epoch time: 91.44 s
2024-12-08 21:22:38.277888: 
2024-12-08 21:22:38.279593: Epoch 32
2024-12-08 21:22:38.280374: Current learning rate: 0.00971
2024-12-08 21:24:09.749031: Validation loss did not improve from -0.60230. Patience: 3/50
2024-12-08 21:24:09.750249: train_loss -0.6566
2024-12-08 21:24:09.751175: val_loss -0.5817
2024-12-08 21:24:09.752007: Pseudo dice [0.7497]
2024-12-08 21:24:09.752751: Epoch time: 91.47 s
2024-12-08 21:24:11.061066: 
2024-12-08 21:24:11.062579: Epoch 33
2024-12-08 21:24:11.063430: Current learning rate: 0.0097
2024-12-08 21:25:42.666010: Validation loss did not improve from -0.60230. Patience: 4/50
2024-12-08 21:25:42.666842: train_loss -0.6691
2024-12-08 21:25:42.667708: val_loss -0.5251
2024-12-08 21:25:42.668366: Pseudo dice [0.7305]
2024-12-08 21:25:42.668996: Epoch time: 91.61 s
2024-12-08 21:25:43.932200: 
2024-12-08 21:25:43.933835: Epoch 34
2024-12-08 21:25:43.934561: Current learning rate: 0.00969
2024-12-08 21:27:15.426865: Validation loss did not improve from -0.60230. Patience: 5/50
2024-12-08 21:27:15.427801: train_loss -0.6628
2024-12-08 21:27:15.428979: val_loss -0.5895
2024-12-08 21:27:15.429717: Pseudo dice [0.772]
2024-12-08 21:27:15.430608: Epoch time: 91.5 s
2024-12-08 21:27:15.839439: Yayy! New best EMA pseudo Dice: 0.7471
2024-12-08 21:27:17.473465: 
2024-12-08 21:27:17.474997: Epoch 35
2024-12-08 21:27:17.475742: Current learning rate: 0.00968
2024-12-08 21:28:48.849909: Validation loss did not improve from -0.60230. Patience: 6/50
2024-12-08 21:28:48.850865: train_loss -0.6652
2024-12-08 21:28:48.851696: val_loss -0.5806
2024-12-08 21:28:48.852592: Pseudo dice [0.7614]
2024-12-08 21:28:48.853377: Epoch time: 91.38 s
2024-12-08 21:28:48.854067: Yayy! New best EMA pseudo Dice: 0.7486
2024-12-08 21:28:50.517733: 
2024-12-08 21:28:50.519580: Epoch 36
2024-12-08 21:28:50.520400: Current learning rate: 0.00968
2024-12-08 21:30:21.879790: Validation loss improved from -0.60230 to -0.60458! Patience: 6/50
2024-12-08 21:30:21.880672: train_loss -0.6749
2024-12-08 21:30:21.881453: val_loss -0.6046
2024-12-08 21:30:21.882398: Pseudo dice [0.7763]
2024-12-08 21:30:21.883306: Epoch time: 91.36 s
2024-12-08 21:30:21.884140: Yayy! New best EMA pseudo Dice: 0.7513
2024-12-08 21:30:23.592905: 
2024-12-08 21:30:23.594462: Epoch 37
2024-12-08 21:30:23.595234: Current learning rate: 0.00967
2024-12-08 21:31:55.011602: Validation loss did not improve from -0.60458. Patience: 1/50
2024-12-08 21:31:55.012360: train_loss -0.672
2024-12-08 21:31:55.013248: val_loss -0.5681
2024-12-08 21:31:55.014038: Pseudo dice [0.7522]
2024-12-08 21:31:55.014800: Epoch time: 91.42 s
2024-12-08 21:31:55.015620: Yayy! New best EMA pseudo Dice: 0.7514
2024-12-08 21:31:56.695823: 
2024-12-08 21:31:56.697579: Epoch 38
2024-12-08 21:31:56.698591: Current learning rate: 0.00966
2024-12-08 21:33:28.061627: Validation loss did not improve from -0.60458. Patience: 2/50
2024-12-08 21:33:28.062814: train_loss -0.674
2024-12-08 21:33:28.063763: val_loss -0.5633
2024-12-08 21:33:28.064464: Pseudo dice [0.7577]
2024-12-08 21:33:28.065269: Epoch time: 91.37 s
2024-12-08 21:33:28.066095: Yayy! New best EMA pseudo Dice: 0.7521
2024-12-08 21:33:29.729219: 
2024-12-08 21:33:29.730175: Epoch 39
2024-12-08 21:33:29.731024: Current learning rate: 0.00965
2024-12-08 21:35:01.152953: Validation loss did not improve from -0.60458. Patience: 3/50
2024-12-08 21:35:01.154279: train_loss -0.6782
2024-12-08 21:35:01.155228: val_loss -0.5778
2024-12-08 21:35:01.156012: Pseudo dice [0.7635]
2024-12-08 21:35:01.156779: Epoch time: 91.43 s
2024-12-08 21:35:01.521403: Yayy! New best EMA pseudo Dice: 0.7532
2024-12-08 21:35:03.531693: 
2024-12-08 21:35:03.533097: Epoch 40
2024-12-08 21:35:03.533811: Current learning rate: 0.00964
2024-12-08 21:36:34.862954: Validation loss did not improve from -0.60458. Patience: 4/50
2024-12-08 21:36:34.864030: train_loss -0.6773
2024-12-08 21:36:34.864880: val_loss -0.5973
2024-12-08 21:36:34.865704: Pseudo dice [0.7669]
2024-12-08 21:36:34.866524: Epoch time: 91.33 s
2024-12-08 21:36:34.867199: Yayy! New best EMA pseudo Dice: 0.7546
2024-12-08 21:36:36.655033: 
2024-12-08 21:36:36.656293: Epoch 41
2024-12-08 21:36:36.657106: Current learning rate: 0.00963
2024-12-08 21:38:08.167717: Validation loss did not improve from -0.60458. Patience: 5/50
2024-12-08 21:38:08.169474: train_loss -0.6753
2024-12-08 21:38:08.171107: val_loss -0.5715
2024-12-08 21:38:08.171870: Pseudo dice [0.7598]
2024-12-08 21:38:08.172806: Epoch time: 91.51 s
2024-12-08 21:38:08.173511: Yayy! New best EMA pseudo Dice: 0.7551
2024-12-08 21:38:09.853551: 
2024-12-08 21:38:09.855406: Epoch 42
2024-12-08 21:38:09.856147: Current learning rate: 0.00962
2024-12-08 21:39:41.333659: Validation loss did not improve from -0.60458. Patience: 6/50
2024-12-08 21:39:41.334728: train_loss -0.6758
2024-12-08 21:39:41.335519: val_loss -0.551
2024-12-08 21:39:41.336115: Pseudo dice [0.739]
2024-12-08 21:39:41.336739: Epoch time: 91.48 s
2024-12-08 21:39:42.571165: 
2024-12-08 21:39:42.572626: Epoch 43
2024-12-08 21:39:42.573455: Current learning rate: 0.00961
2024-12-08 21:41:14.406120: Validation loss did not improve from -0.60458. Patience: 7/50
2024-12-08 21:41:14.408116: train_loss -0.6801
2024-12-08 21:41:14.409631: val_loss -0.5961
2024-12-08 21:41:14.410375: Pseudo dice [0.7666]
2024-12-08 21:41:14.411125: Epoch time: 91.84 s
2024-12-08 21:41:15.674101: 
2024-12-08 21:41:15.676713: Epoch 44
2024-12-08 21:41:15.678354: Current learning rate: 0.0096
2024-12-08 21:42:46.756305: Validation loss did not improve from -0.60458. Patience: 8/50
2024-12-08 21:42:46.757576: train_loss -0.6754
2024-12-08 21:42:46.758687: val_loss -0.5774
2024-12-08 21:42:46.759601: Pseudo dice [0.7575]
2024-12-08 21:42:46.760541: Epoch time: 91.09 s
2024-12-08 21:42:48.491493: 
2024-12-08 21:42:48.493902: Epoch 45
2024-12-08 21:42:48.494854: Current learning rate: 0.00959
2024-12-08 21:44:19.977744: Validation loss did not improve from -0.60458. Patience: 9/50
2024-12-08 21:44:19.979893: train_loss -0.6919
2024-12-08 21:44:19.981219: val_loss -0.5898
2024-12-08 21:44:19.982152: Pseudo dice [0.7686]
2024-12-08 21:44:19.983077: Epoch time: 91.49 s
2024-12-08 21:44:19.983795: Yayy! New best EMA pseudo Dice: 0.7564
2024-12-08 21:44:21.745652: 
2024-12-08 21:44:21.747176: Epoch 46
2024-12-08 21:44:21.747946: Current learning rate: 0.00959
2024-12-08 21:45:53.268965: Validation loss did not improve from -0.60458. Patience: 10/50
2024-12-08 21:45:53.270109: train_loss -0.6806
2024-12-08 21:45:53.271309: val_loss -0.5944
2024-12-08 21:45:53.272242: Pseudo dice [0.7741]
2024-12-08 21:45:53.273153: Epoch time: 91.53 s
2024-12-08 21:45:53.273983: Yayy! New best EMA pseudo Dice: 0.7582
2024-12-08 21:45:54.919607: 
2024-12-08 21:45:54.921116: Epoch 47
2024-12-08 21:45:54.922055: Current learning rate: 0.00958
2024-12-08 21:47:26.373030: Validation loss did not improve from -0.60458. Patience: 11/50
2024-12-08 21:47:26.374177: train_loss -0.6882
2024-12-08 21:47:26.375038: val_loss -0.6024
2024-12-08 21:47:26.375633: Pseudo dice [0.7739]
2024-12-08 21:47:26.376292: Epoch time: 91.46 s
2024-12-08 21:47:26.377039: Yayy! New best EMA pseudo Dice: 0.7597
2024-12-08 21:47:28.018224: 
2024-12-08 21:47:28.019724: Epoch 48
2024-12-08 21:47:28.020455: Current learning rate: 0.00957
2024-12-08 21:48:59.340405: Validation loss did not improve from -0.60458. Patience: 12/50
2024-12-08 21:48:59.341705: train_loss -0.6928
2024-12-08 21:48:59.342845: val_loss -0.5778
2024-12-08 21:48:59.343659: Pseudo dice [0.7592]
2024-12-08 21:48:59.344366: Epoch time: 91.32 s
2024-12-08 21:49:00.666006: 
2024-12-08 21:49:00.667590: Epoch 49
2024-12-08 21:49:00.668409: Current learning rate: 0.00956
2024-12-08 21:50:31.975226: Validation loss did not improve from -0.60458. Patience: 13/50
2024-12-08 21:50:31.976022: train_loss -0.6996
2024-12-08 21:50:31.976762: val_loss -0.5722
2024-12-08 21:50:31.977552: Pseudo dice [0.7551]
2024-12-08 21:50:31.978339: Epoch time: 91.31 s
2024-12-08 21:50:33.666353: 
2024-12-08 21:50:33.667946: Epoch 50
2024-12-08 21:50:33.668662: Current learning rate: 0.00955
2024-12-08 21:52:04.982117: Validation loss improved from -0.60458 to -0.61549! Patience: 13/50
2024-12-08 21:52:04.982863: train_loss -0.6952
2024-12-08 21:52:04.983743: val_loss -0.6155
2024-12-08 21:52:04.984440: Pseudo dice [0.7828]
2024-12-08 21:52:04.985214: Epoch time: 91.32 s
2024-12-08 21:52:04.985982: Yayy! New best EMA pseudo Dice: 0.7616
2024-12-08 21:52:07.044470: 
2024-12-08 21:52:07.046002: Epoch 51
2024-12-08 21:52:07.046700: Current learning rate: 0.00954
2024-12-08 21:53:38.274981: Validation loss did not improve from -0.61549. Patience: 1/50
2024-12-08 21:53:38.276293: train_loss -0.6951
2024-12-08 21:53:38.277601: val_loss -0.5934
2024-12-08 21:53:38.278646: Pseudo dice [0.7657]
2024-12-08 21:53:38.279560: Epoch time: 91.23 s
2024-12-08 21:53:38.280463: Yayy! New best EMA pseudo Dice: 0.762
2024-12-08 21:53:39.939825: 
2024-12-08 21:53:39.941729: Epoch 52
2024-12-08 21:53:39.942888: Current learning rate: 0.00953
2024-12-08 21:55:11.234447: Validation loss did not improve from -0.61549. Patience: 2/50
2024-12-08 21:55:11.235845: train_loss -0.7042
2024-12-08 21:55:11.237165: val_loss -0.5671
2024-12-08 21:55:11.237876: Pseudo dice [0.7583]
2024-12-08 21:55:11.238642: Epoch time: 91.3 s
2024-12-08 21:55:12.525283: 
2024-12-08 21:55:12.526899: Epoch 53
2024-12-08 21:55:12.527836: Current learning rate: 0.00952
2024-12-08 21:56:43.717212: Validation loss did not improve from -0.61549. Patience: 3/50
2024-12-08 21:56:43.718140: train_loss -0.6997
2024-12-08 21:56:43.719372: val_loss -0.6154
2024-12-08 21:56:43.720461: Pseudo dice [0.7782]
2024-12-08 21:56:43.721336: Epoch time: 91.19 s
2024-12-08 21:56:43.722147: Yayy! New best EMA pseudo Dice: 0.7633
2024-12-08 21:56:45.404354: 
2024-12-08 21:56:45.406003: Epoch 54
2024-12-08 21:56:45.406868: Current learning rate: 0.00951
2024-12-08 21:58:16.587229: Validation loss improved from -0.61549 to -0.62206! Patience: 3/50
2024-12-08 21:58:16.588532: train_loss -0.6978
2024-12-08 21:58:16.589447: val_loss -0.6221
2024-12-08 21:58:16.590334: Pseudo dice [0.7897]
2024-12-08 21:58:16.591354: Epoch time: 91.19 s
2024-12-08 21:58:16.983253: Yayy! New best EMA pseudo Dice: 0.7659
2024-12-08 21:58:18.657687: 
2024-12-08 21:58:18.659161: Epoch 55
2024-12-08 21:58:18.659901: Current learning rate: 0.0095
2024-12-08 21:59:49.859711: Validation loss did not improve from -0.62206. Patience: 1/50
2024-12-08 21:59:49.860864: train_loss -0.7008
2024-12-08 21:59:49.861671: val_loss -0.5792
2024-12-08 21:59:49.862438: Pseudo dice [0.7621]
2024-12-08 21:59:49.863132: Epoch time: 91.2 s
2024-12-08 21:59:51.166778: 
2024-12-08 21:59:51.168267: Epoch 56
2024-12-08 21:59:51.169182: Current learning rate: 0.00949
2024-12-08 22:01:22.353426: Validation loss did not improve from -0.62206. Patience: 2/50
2024-12-08 22:01:22.354364: train_loss -0.706
2024-12-08 22:01:22.355292: val_loss -0.5939
2024-12-08 22:01:22.356108: Pseudo dice [0.7646]
2024-12-08 22:01:22.356809: Epoch time: 91.19 s
2024-12-08 22:01:23.617501: 
2024-12-08 22:01:23.619384: Epoch 57
2024-12-08 22:01:23.620313: Current learning rate: 0.00949
2024-12-08 22:02:54.813520: Validation loss did not improve from -0.62206. Patience: 3/50
2024-12-08 22:02:54.814516: train_loss -0.6944
2024-12-08 22:02:54.815391: val_loss -0.5824
2024-12-08 22:02:54.816179: Pseudo dice [0.7627]
2024-12-08 22:02:54.816962: Epoch time: 91.2 s
2024-12-08 22:02:56.042504: 
2024-12-08 22:02:56.044138: Epoch 58
2024-12-08 22:02:56.045067: Current learning rate: 0.00948
2024-12-08 22:04:27.170114: Validation loss did not improve from -0.62206. Patience: 4/50
2024-12-08 22:04:27.171017: train_loss -0.694
2024-12-08 22:04:27.172204: val_loss -0.6
2024-12-08 22:04:27.173233: Pseudo dice [0.7754]
2024-12-08 22:04:27.174150: Epoch time: 91.13 s
2024-12-08 22:04:27.175248: Yayy! New best EMA pseudo Dice: 0.7662
2024-12-08 22:04:28.872951: 
2024-12-08 22:04:28.874844: Epoch 59
2024-12-08 22:04:28.875653: Current learning rate: 0.00947
2024-12-08 22:06:00.003819: Validation loss did not improve from -0.62206. Patience: 5/50
2024-12-08 22:06:00.004589: train_loss -0.705
2024-12-08 22:06:00.005309: val_loss -0.6004
2024-12-08 22:06:00.005864: Pseudo dice [0.7648]
2024-12-08 22:06:00.006484: Epoch time: 91.13 s
2024-12-08 22:06:01.639950: 
2024-12-08 22:06:01.641675: Epoch 60
2024-12-08 22:06:01.642465: Current learning rate: 0.00946
2024-12-08 22:07:32.866351: Validation loss did not improve from -0.62206. Patience: 6/50
2024-12-08 22:07:32.867163: train_loss -0.7003
2024-12-08 22:07:32.868080: val_loss -0.6131
2024-12-08 22:07:32.868946: Pseudo dice [0.7688]
2024-12-08 22:07:32.869652: Epoch time: 91.23 s
2024-12-08 22:07:32.870325: Yayy! New best EMA pseudo Dice: 0.7663
2024-12-08 22:07:34.858503: 
2024-12-08 22:07:34.860174: Epoch 61
2024-12-08 22:07:34.861258: Current learning rate: 0.00945
2024-12-08 22:09:06.118055: Validation loss did not improve from -0.62206. Patience: 7/50
2024-12-08 22:09:06.119100: train_loss -0.7034
2024-12-08 22:09:06.120233: val_loss -0.5718
2024-12-08 22:09:06.121050: Pseudo dice [0.7567]
2024-12-08 22:09:06.121761: Epoch time: 91.26 s
2024-12-08 22:09:07.410447: 
2024-12-08 22:09:07.411718: Epoch 62
2024-12-08 22:09:07.412395: Current learning rate: 0.00944
2024-12-08 22:10:38.698219: Validation loss did not improve from -0.62206. Patience: 8/50
2024-12-08 22:10:38.699482: train_loss -0.6956
2024-12-08 22:10:38.700415: val_loss -0.5916
2024-12-08 22:10:38.701161: Pseudo dice [0.7642]
2024-12-08 22:10:38.701857: Epoch time: 91.29 s
2024-12-08 22:10:40.066603: 
2024-12-08 22:10:40.068069: Epoch 63
2024-12-08 22:10:40.068966: Current learning rate: 0.00943
2024-12-08 22:12:11.347710: Validation loss did not improve from -0.62206. Patience: 9/50
2024-12-08 22:12:11.348885: train_loss -0.6981
2024-12-08 22:12:11.349805: val_loss -0.5755
2024-12-08 22:12:11.350679: Pseudo dice [0.7679]
2024-12-08 22:12:11.351455: Epoch time: 91.28 s
2024-12-08 22:12:12.612284: 
2024-12-08 22:12:12.613626: Epoch 64
2024-12-08 22:12:12.614421: Current learning rate: 0.00942
2024-12-08 22:13:43.858066: Validation loss did not improve from -0.62206. Patience: 10/50
2024-12-08 22:13:43.859114: train_loss -0.7057
2024-12-08 22:13:43.860102: val_loss -0.5746
2024-12-08 22:13:43.861002: Pseudo dice [0.7605]
2024-12-08 22:13:43.862004: Epoch time: 91.25 s
2024-12-08 22:13:45.523818: 
2024-12-08 22:13:45.525483: Epoch 65
2024-12-08 22:13:45.526603: Current learning rate: 0.00941
2024-12-08 22:15:16.801665: Validation loss did not improve from -0.62206. Patience: 11/50
2024-12-08 22:15:16.802832: train_loss -0.698
2024-12-08 22:15:16.803649: val_loss -0.5411
2024-12-08 22:15:16.804387: Pseudo dice [0.7451]
2024-12-08 22:15:16.805189: Epoch time: 91.28 s
2024-12-08 22:15:18.081939: 
2024-12-08 22:15:18.083303: Epoch 66
2024-12-08 22:15:18.084205: Current learning rate: 0.0094
2024-12-08 22:16:49.314302: Validation loss did not improve from -0.62206. Patience: 12/50
2024-12-08 22:16:49.315476: train_loss -0.7038
2024-12-08 22:16:49.316336: val_loss -0.5841
2024-12-08 22:16:49.317011: Pseudo dice [0.7632]
2024-12-08 22:16:49.317747: Epoch time: 91.23 s
2024-12-08 22:16:50.642032: 
2024-12-08 22:16:50.642976: Epoch 67
2024-12-08 22:16:50.643866: Current learning rate: 0.00939
2024-12-08 22:18:21.868663: Validation loss did not improve from -0.62206. Patience: 13/50
2024-12-08 22:18:21.869886: train_loss -0.7151
2024-12-08 22:18:21.870846: val_loss -0.622
2024-12-08 22:18:21.871596: Pseudo dice [0.7795]
2024-12-08 22:18:21.872390: Epoch time: 91.23 s
2024-12-08 22:18:23.202566: 
2024-12-08 22:18:23.203872: Epoch 68
2024-12-08 22:18:23.204619: Current learning rate: 0.00939
2024-12-08 22:19:54.541386: Validation loss did not improve from -0.62206. Patience: 14/50
2024-12-08 22:19:54.542662: train_loss -0.7109
2024-12-08 22:19:54.543901: val_loss -0.5921
2024-12-08 22:19:54.544740: Pseudo dice [0.764]
2024-12-08 22:19:54.545766: Epoch time: 91.34 s
2024-12-08 22:19:55.842492: 
2024-12-08 22:19:55.843953: Epoch 69
2024-12-08 22:19:55.844680: Current learning rate: 0.00938
2024-12-08 22:21:27.232109: Validation loss did not improve from -0.62206. Patience: 15/50
2024-12-08 22:21:27.232984: train_loss -0.7133
2024-12-08 22:21:27.233901: val_loss -0.6069
2024-12-08 22:21:27.234636: Pseudo dice [0.776]
2024-12-08 22:21:27.235548: Epoch time: 91.39 s
2024-12-08 22:21:28.979227: 
2024-12-08 22:21:28.980362: Epoch 70
2024-12-08 22:21:28.981079: Current learning rate: 0.00937
2024-12-08 22:23:00.341144: Validation loss did not improve from -0.62206. Patience: 16/50
2024-12-08 22:23:00.342286: train_loss -0.7112
2024-12-08 22:23:00.343090: val_loss -0.6214
2024-12-08 22:23:00.343780: Pseudo dice [0.7829]
2024-12-08 22:23:00.344471: Epoch time: 91.36 s
2024-12-08 22:23:00.345149: Yayy! New best EMA pseudo Dice: 0.7675
2024-12-08 22:23:02.458007: 
2024-12-08 22:23:02.459637: Epoch 71
2024-12-08 22:23:02.460502: Current learning rate: 0.00936
2024-12-08 22:24:33.896274: Validation loss did not improve from -0.62206. Patience: 17/50
2024-12-08 22:24:33.897701: train_loss -0.7188
2024-12-08 22:24:33.898623: val_loss -0.5972
2024-12-08 22:24:33.899434: Pseudo dice [0.774]
2024-12-08 22:24:33.900279: Epoch time: 91.44 s
2024-12-08 22:24:33.900939: Yayy! New best EMA pseudo Dice: 0.7681
2024-12-08 22:24:35.634843: 
2024-12-08 22:24:35.636278: Epoch 72
2024-12-08 22:24:35.637146: Current learning rate: 0.00935
2024-12-08 22:26:07.228126: Validation loss did not improve from -0.62206. Patience: 18/50
2024-12-08 22:26:07.229460: train_loss -0.7175
2024-12-08 22:26:07.230693: val_loss -0.5982
2024-12-08 22:26:07.231423: Pseudo dice [0.7646]
2024-12-08 22:26:07.232177: Epoch time: 91.6 s
2024-12-08 22:26:08.485435: 
2024-12-08 22:26:08.487137: Epoch 73
2024-12-08 22:26:08.488074: Current learning rate: 0.00934
2024-12-08 22:27:39.840643: Validation loss did not improve from -0.62206. Patience: 19/50
2024-12-08 22:27:39.841525: train_loss -0.7229
2024-12-08 22:27:39.842423: val_loss -0.6052
2024-12-08 22:27:39.843089: Pseudo dice [0.7705]
2024-12-08 22:27:39.843748: Epoch time: 91.36 s
2024-12-08 22:27:41.133259: 
2024-12-08 22:27:41.135310: Epoch 74
2024-12-08 22:27:41.136228: Current learning rate: 0.00933
2024-12-08 22:29:12.334455: Validation loss did not improve from -0.62206. Patience: 20/50
2024-12-08 22:29:12.335554: train_loss -0.7236
2024-12-08 22:29:12.336585: val_loss -0.6161
2024-12-08 22:29:12.337538: Pseudo dice [0.7793]
2024-12-08 22:29:12.338453: Epoch time: 91.2 s
2024-12-08 22:29:12.700489: Yayy! New best EMA pseudo Dice: 0.7692
2024-12-08 22:29:14.192321: 
2024-12-08 22:29:14.194428: Epoch 75
2024-12-08 22:29:14.195600: Current learning rate: 0.00932
2024-12-08 22:30:45.212457: Validation loss did not improve from -0.62206. Patience: 21/50
2024-12-08 22:30:45.213687: train_loss -0.7227
2024-12-08 22:30:45.214654: val_loss -0.5984
2024-12-08 22:30:45.215608: Pseudo dice [0.7704]
2024-12-08 22:30:45.216277: Epoch time: 91.02 s
2024-12-08 22:30:45.216955: Yayy! New best EMA pseudo Dice: 0.7693
2024-12-08 22:30:46.774798: 
2024-12-08 22:30:46.776889: Epoch 76
2024-12-08 22:30:46.778287: Current learning rate: 0.00931
2024-12-08 22:32:17.594619: Validation loss did not improve from -0.62206. Patience: 22/50
2024-12-08 22:32:17.595607: train_loss -0.7214
2024-12-08 22:32:17.596891: val_loss -0.5763
2024-12-08 22:32:17.597797: Pseudo dice [0.7548]
2024-12-08 22:32:17.598632: Epoch time: 90.82 s
2024-12-08 22:32:18.806199: 
2024-12-08 22:32:18.808156: Epoch 77
2024-12-08 22:32:18.809385: Current learning rate: 0.0093
2024-12-08 22:33:49.491770: Validation loss did not improve from -0.62206. Patience: 23/50
2024-12-08 22:33:49.492723: train_loss -0.7176
2024-12-08 22:33:49.493651: val_loss -0.5827
2024-12-08 22:33:49.494482: Pseudo dice [0.7598]
2024-12-08 22:33:49.495309: Epoch time: 90.69 s
2024-12-08 22:33:50.711201: 
2024-12-08 22:33:50.713496: Epoch 78
2024-12-08 22:33:50.714686: Current learning rate: 0.0093
2024-12-08 22:35:20.966446: Validation loss did not improve from -0.62206. Patience: 24/50
2024-12-08 22:35:20.967674: train_loss -0.7206
2024-12-08 22:35:20.968847: val_loss -0.5605
2024-12-08 22:35:20.970067: Pseudo dice [0.7467]
2024-12-08 22:35:20.971151: Epoch time: 90.26 s
2024-12-08 22:35:22.131060: 
2024-12-08 22:35:22.133111: Epoch 79
2024-12-08 22:35:22.134460: Current learning rate: 0.00929
2024-12-08 22:36:52.202436: Validation loss did not improve from -0.62206. Patience: 25/50
2024-12-08 22:36:52.203309: train_loss -0.7208
2024-12-08 22:36:52.204217: val_loss -0.6042
2024-12-08 22:36:52.205212: Pseudo dice [0.7792]
2024-12-08 22:36:52.206072: Epoch time: 90.07 s
2024-12-08 22:36:53.744942: 
2024-12-08 22:36:53.746483: Epoch 80
2024-12-08 22:36:53.747353: Current learning rate: 0.00928
2024-12-08 22:38:23.807904: Validation loss did not improve from -0.62206. Patience: 26/50
2024-12-08 22:38:23.808735: train_loss -0.7184
2024-12-08 22:38:23.809532: val_loss -0.5506
2024-12-08 22:38:23.810202: Pseudo dice [0.7418]
2024-12-08 22:38:23.810972: Epoch time: 90.06 s
2024-12-08 22:38:25.045103: 
2024-12-08 22:38:25.047004: Epoch 81
2024-12-08 22:38:25.048399: Current learning rate: 0.00927
2024-12-08 22:39:55.104051: Validation loss did not improve from -0.62206. Patience: 27/50
2024-12-08 22:39:55.105273: train_loss -0.7223
2024-12-08 22:39:55.106389: val_loss -0.5719
2024-12-08 22:39:55.107181: Pseudo dice [0.7568]
2024-12-08 22:39:55.107854: Epoch time: 90.06 s
2024-12-08 22:39:56.657192: 
2024-12-08 22:39:56.659448: Epoch 82
2024-12-08 22:39:56.660264: Current learning rate: 0.00926
2024-12-08 22:41:26.690830: Validation loss did not improve from -0.62206. Patience: 28/50
2024-12-08 22:41:26.691884: train_loss -0.7213
2024-12-08 22:41:26.693002: val_loss -0.5768
2024-12-08 22:41:26.693945: Pseudo dice [0.76]
2024-12-08 22:41:26.694855: Epoch time: 90.04 s
2024-12-08 22:41:27.857052: 
2024-12-08 22:41:27.859284: Epoch 83
2024-12-08 22:41:27.860011: Current learning rate: 0.00925
2024-12-08 22:42:58.016925: Validation loss did not improve from -0.62206. Patience: 29/50
2024-12-08 22:42:58.018427: train_loss -0.719
2024-12-08 22:42:58.019562: val_loss -0.5769
2024-12-08 22:42:58.020531: Pseudo dice [0.7512]
2024-12-08 22:42:58.021247: Epoch time: 90.16 s
2024-12-08 22:42:59.156161: 
2024-12-08 22:42:59.158597: Epoch 84
2024-12-08 22:42:59.159709: Current learning rate: 0.00924
2024-12-08 22:44:29.270298: Validation loss did not improve from -0.62206. Patience: 30/50
2024-12-08 22:44:29.271199: train_loss -0.7299
2024-12-08 22:44:29.272699: val_loss -0.5811
2024-12-08 22:44:29.273602: Pseudo dice [0.7656]
2024-12-08 22:44:29.274440: Epoch time: 90.12 s
2024-12-08 22:44:30.743593: 
2024-12-08 22:44:30.745153: Epoch 85
2024-12-08 22:44:30.746162: Current learning rate: 0.00923
2024-12-08 22:46:00.924590: Validation loss did not improve from -0.62206. Patience: 31/50
2024-12-08 22:46:00.925658: train_loss -0.7259
2024-12-08 22:46:00.926909: val_loss -0.5976
2024-12-08 22:46:00.927943: Pseudo dice [0.7682]
2024-12-08 22:46:00.928974: Epoch time: 90.18 s
2024-12-08 22:46:02.059270: 
2024-12-08 22:46:02.061424: Epoch 86
2024-12-08 22:46:02.062766: Current learning rate: 0.00922
2024-12-08 22:47:32.060119: Validation loss did not improve from -0.62206. Patience: 32/50
2024-12-08 22:47:32.061438: train_loss -0.726
2024-12-08 22:47:32.062345: val_loss -0.5754
2024-12-08 22:47:32.063121: Pseudo dice [0.7588]
2024-12-08 22:47:32.064059: Epoch time: 90.0 s
2024-12-08 22:47:33.204716: 
2024-12-08 22:47:33.206854: Epoch 87
2024-12-08 22:47:33.207941: Current learning rate: 0.00921
2024-12-08 22:49:03.219908: Validation loss did not improve from -0.62206. Patience: 33/50
2024-12-08 22:49:03.220984: train_loss -0.7194
2024-12-08 22:49:03.222224: val_loss -0.6153
2024-12-08 22:49:03.223013: Pseudo dice [0.7787]
2024-12-08 22:49:03.224024: Epoch time: 90.02 s
2024-12-08 22:49:04.353669: 
2024-12-08 22:49:04.355775: Epoch 88
2024-12-08 22:49:04.356791: Current learning rate: 0.0092
2024-12-08 22:50:34.608251: Validation loss did not improve from -0.62206. Patience: 34/50
2024-12-08 22:50:34.609446: train_loss -0.7302
2024-12-08 22:50:34.610826: val_loss -0.5954
2024-12-08 22:50:34.611589: Pseudo dice [0.7731]
2024-12-08 22:50:34.612592: Epoch time: 90.26 s
2024-12-08 22:50:35.771381: 
2024-12-08 22:50:35.773029: Epoch 89
2024-12-08 22:50:35.774303: Current learning rate: 0.0092
2024-12-08 22:52:06.232301: Validation loss did not improve from -0.62206. Patience: 35/50
2024-12-08 22:52:06.233809: train_loss -0.7309
2024-12-08 22:52:06.235432: val_loss -0.5963
2024-12-08 22:52:06.236624: Pseudo dice [0.7625]
2024-12-08 22:52:06.237693: Epoch time: 90.46 s
2024-12-08 22:52:07.722631: 
2024-12-08 22:52:07.724861: Epoch 90
2024-12-08 22:52:07.726254: Current learning rate: 0.00919
2024-12-08 22:53:38.228868: Validation loss did not improve from -0.62206. Patience: 36/50
2024-12-08 22:53:38.230144: train_loss -0.7316
2024-12-08 22:53:38.230998: val_loss -0.5709
2024-12-08 22:53:38.231925: Pseudo dice [0.7551]
2024-12-08 22:53:38.232857: Epoch time: 90.51 s
2024-12-08 22:53:39.377927: 
2024-12-08 22:53:39.379770: Epoch 91
2024-12-08 22:53:39.381194: Current learning rate: 0.00918
2024-12-08 22:55:09.997401: Validation loss did not improve from -0.62206. Patience: 37/50
2024-12-08 22:55:09.998767: train_loss -0.7315
2024-12-08 22:55:09.999872: val_loss -0.5972
2024-12-08 22:55:10.000829: Pseudo dice [0.7737]
2024-12-08 22:55:10.001886: Epoch time: 90.62 s
2024-12-08 22:55:11.118217: 
2024-12-08 22:55:11.120348: Epoch 92
2024-12-08 22:55:11.121682: Current learning rate: 0.00917
2024-12-08 22:56:41.679890: Validation loss did not improve from -0.62206. Patience: 38/50
2024-12-08 22:56:41.681052: train_loss -0.7305
2024-12-08 22:56:41.681851: val_loss -0.5777
2024-12-08 22:56:41.682663: Pseudo dice [0.7555]
2024-12-08 22:56:41.683595: Epoch time: 90.56 s
2024-12-08 22:56:43.129851: 
2024-12-08 22:56:43.131922: Epoch 93
2024-12-08 22:56:43.133089: Current learning rate: 0.00916
2024-12-08 22:58:13.554507: Validation loss did not improve from -0.62206. Patience: 39/50
2024-12-08 22:58:13.555468: train_loss -0.7257
2024-12-08 22:58:13.556402: val_loss -0.5926
2024-12-08 22:58:13.557107: Pseudo dice [0.7739]
2024-12-08 22:58:13.557922: Epoch time: 90.43 s
2024-12-08 22:58:14.706875: 
2024-12-08 22:58:14.708957: Epoch 94
2024-12-08 22:58:14.709828: Current learning rate: 0.00915
2024-12-08 22:59:45.013445: Validation loss did not improve from -0.62206. Patience: 40/50
2024-12-08 22:59:45.015079: train_loss -0.7329
2024-12-08 22:59:45.015936: val_loss -0.594
2024-12-08 22:59:45.016724: Pseudo dice [0.774]
2024-12-08 22:59:45.017432: Epoch time: 90.31 s
2024-12-08 22:59:46.482475: 
2024-12-08 22:59:46.484869: Epoch 95
2024-12-08 22:59:46.485895: Current learning rate: 0.00914
2024-12-08 23:01:16.848022: Validation loss did not improve from -0.62206. Patience: 41/50
2024-12-08 23:01:16.849239: train_loss -0.7351
2024-12-08 23:01:16.850386: val_loss -0.5728
2024-12-08 23:01:16.851371: Pseudo dice [0.7572]
2024-12-08 23:01:16.852184: Epoch time: 90.37 s
2024-12-08 23:01:17.984563: 
2024-12-08 23:01:17.986486: Epoch 96
2024-12-08 23:01:17.987696: Current learning rate: 0.00913
2024-12-08 23:02:48.294193: Validation loss did not improve from -0.62206. Patience: 42/50
2024-12-08 23:02:48.295161: train_loss -0.7329
2024-12-08 23:02:48.296364: val_loss -0.6139
2024-12-08 23:02:48.297192: Pseudo dice [0.7736]
2024-12-08 23:02:48.298050: Epoch time: 90.31 s
2024-12-08 23:02:49.469647: 
2024-12-08 23:02:49.471749: Epoch 97
2024-12-08 23:02:49.473190: Current learning rate: 0.00912
2024-12-08 23:04:19.666663: Validation loss did not improve from -0.62206. Patience: 43/50
2024-12-08 23:04:19.667619: train_loss -0.7299
2024-12-08 23:04:19.669007: val_loss -0.5916
2024-12-08 23:04:19.670355: Pseudo dice [0.7724]
2024-12-08 23:04:19.671561: Epoch time: 90.2 s
2024-12-08 23:04:20.823316: 
2024-12-08 23:04:20.825152: Epoch 98
2024-12-08 23:04:20.826285: Current learning rate: 0.00911
2024-12-08 23:05:50.891028: Validation loss did not improve from -0.62206. Patience: 44/50
2024-12-08 23:05:50.892478: train_loss -0.7303
2024-12-08 23:05:50.893850: val_loss -0.5943
2024-12-08 23:05:50.895015: Pseudo dice [0.7728]
2024-12-08 23:05:50.896236: Epoch time: 90.07 s
2024-12-08 23:05:52.059153: 
2024-12-08 23:05:52.061418: Epoch 99
2024-12-08 23:05:52.062988: Current learning rate: 0.0091
2024-12-08 23:07:21.946716: Validation loss did not improve from -0.62206. Patience: 45/50
2024-12-08 23:07:21.947925: train_loss -0.7334
2024-12-08 23:07:21.948914: val_loss -0.5925
2024-12-08 23:07:21.949652: Pseudo dice [0.7659]
2024-12-08 23:07:21.950480: Epoch time: 89.89 s
2024-12-08 23:07:23.479538: 
2024-12-08 23:07:23.481528: Epoch 100
2024-12-08 23:07:23.482827: Current learning rate: 0.0091
2024-12-08 23:08:53.399747: Validation loss did not improve from -0.62206. Patience: 46/50
2024-12-08 23:08:53.400692: train_loss -0.7292
2024-12-08 23:08:53.401643: val_loss -0.5792
2024-12-08 23:08:53.402618: Pseudo dice [0.7619]
2024-12-08 23:08:53.403688: Epoch time: 89.92 s
2024-12-08 23:08:54.564013: 
2024-12-08 23:08:54.566258: Epoch 101
2024-12-08 23:08:54.567580: Current learning rate: 0.00909
2024-12-08 23:10:24.650497: Validation loss did not improve from -0.62206. Patience: 47/50
2024-12-08 23:10:24.651204: train_loss -0.7295
2024-12-08 23:10:24.652377: val_loss -0.5997
2024-12-08 23:10:24.653674: Pseudo dice [0.7732]
2024-12-08 23:10:24.654966: Epoch time: 90.09 s
2024-12-08 23:10:25.815203: 
2024-12-08 23:10:25.817513: Epoch 102
2024-12-08 23:10:25.818799: Current learning rate: 0.00908
2024-12-08 23:11:56.357691: Validation loss did not improve from -0.62206. Patience: 48/50
2024-12-08 23:11:56.359040: train_loss -0.7311
2024-12-08 23:11:56.360348: val_loss -0.6195
2024-12-08 23:11:56.361499: Pseudo dice [0.7819]
2024-12-08 23:11:56.362381: Epoch time: 90.54 s
2024-12-08 23:11:57.507082: 
2024-12-08 23:11:57.508754: Epoch 103
2024-12-08 23:11:57.509570: Current learning rate: 0.00907
2024-12-08 23:13:27.772142: Validation loss did not improve from -0.62206. Patience: 49/50
2024-12-08 23:13:27.773487: train_loss -0.7317
2024-12-08 23:13:27.775007: val_loss -0.6132
2024-12-08 23:13:27.776036: Pseudo dice [0.7829]
2024-12-08 23:13:27.776981: Epoch time: 90.27 s
2024-12-08 23:13:27.777840: Yayy! New best EMA pseudo Dice: 0.77
2024-12-08 23:13:29.332131: 
2024-12-08 23:13:29.334385: Epoch 104
2024-12-08 23:13:29.335392: Current learning rate: 0.00906
2024-12-08 23:14:59.734457: Validation loss improved from -0.62206 to -0.63026! Patience: 49/50
2024-12-08 23:14:59.735948: train_loss -0.7409
2024-12-08 23:14:59.737311: val_loss -0.6303
2024-12-08 23:14:59.738243: Pseudo dice [0.7897]
2024-12-08 23:14:59.739070: Epoch time: 90.4 s
2024-12-08 23:15:00.490966: Yayy! New best EMA pseudo Dice: 0.772
2024-12-08 23:15:01.989776: 
2024-12-08 23:15:01.991092: Epoch 105
2024-12-08 23:15:01.992011: Current learning rate: 0.00905
2024-12-08 23:16:31.745120: Validation loss did not improve from -0.63026. Patience: 1/50
2024-12-08 23:16:31.746527: train_loss -0.74
2024-12-08 23:16:31.748128: val_loss -0.5922
2024-12-08 23:16:31.749146: Pseudo dice [0.7716]
2024-12-08 23:16:31.749997: Epoch time: 89.76 s
2024-12-08 23:16:32.916697: 
2024-12-08 23:16:32.919219: Epoch 106
2024-12-08 23:16:32.920345: Current learning rate: 0.00904
2024-12-08 23:18:02.524113: Validation loss did not improve from -0.63026. Patience: 2/50
2024-12-08 23:18:02.525042: train_loss -0.7395
2024-12-08 23:18:02.525892: val_loss -0.5893
2024-12-08 23:18:02.526921: Pseudo dice [0.7726]
2024-12-08 23:18:02.527891: Epoch time: 89.61 s
2024-12-08 23:18:02.528691: Yayy! New best EMA pseudo Dice: 0.772
2024-12-08 23:18:03.994668: 
2024-12-08 23:18:03.996671: Epoch 107
2024-12-08 23:18:03.997819: Current learning rate: 0.00903
2024-12-08 23:19:33.381062: Validation loss did not improve from -0.63026. Patience: 3/50
2024-12-08 23:19:33.382252: train_loss -0.7455
2024-12-08 23:19:33.383400: val_loss -0.5918
2024-12-08 23:19:33.384039: Pseudo dice [0.7698]
2024-12-08 23:19:33.384803: Epoch time: 89.39 s
2024-12-08 23:19:34.548058: 
2024-12-08 23:19:34.550364: Epoch 108
2024-12-08 23:19:34.551673: Current learning rate: 0.00902
2024-12-08 23:21:03.990719: Validation loss did not improve from -0.63026. Patience: 4/50
2024-12-08 23:21:03.991788: train_loss -0.7419
2024-12-08 23:21:03.992820: val_loss -0.6197
2024-12-08 23:21:03.993549: Pseudo dice [0.7879]
2024-12-08 23:21:03.994224: Epoch time: 89.44 s
2024-12-08 23:21:03.995072: Yayy! New best EMA pseudo Dice: 0.7734
2024-12-08 23:21:05.518193: 
2024-12-08 23:21:05.520226: Epoch 109
2024-12-08 23:21:05.521106: Current learning rate: 0.00901
2024-12-08 23:22:34.964839: Validation loss did not improve from -0.63026. Patience: 5/50
2024-12-08 23:22:34.966355: train_loss -0.7461
2024-12-08 23:22:34.967213: val_loss -0.5682
2024-12-08 23:22:34.968157: Pseudo dice [0.7572]
2024-12-08 23:22:34.968956: Epoch time: 89.45 s
2024-12-08 23:22:36.455368: 
2024-12-08 23:22:36.458038: Epoch 110
2024-12-08 23:22:36.459209: Current learning rate: 0.009
2024-12-08 23:24:06.018121: Validation loss did not improve from -0.63026. Patience: 6/50
2024-12-08 23:24:06.019242: train_loss -0.7507
2024-12-08 23:24:06.020169: val_loss -0.6099
2024-12-08 23:24:06.020954: Pseudo dice [0.7742]
2024-12-08 23:24:06.021725: Epoch time: 89.57 s
2024-12-08 23:24:07.187048: 
2024-12-08 23:24:07.189252: Epoch 111
2024-12-08 23:24:07.190598: Current learning rate: 0.009
2024-12-08 23:25:37.083811: Validation loss did not improve from -0.63026. Patience: 7/50
2024-12-08 23:25:37.085053: train_loss -0.7469
2024-12-08 23:25:37.086222: val_loss -0.5752
2024-12-08 23:25:37.087164: Pseudo dice [0.7587]
2024-12-08 23:25:37.088112: Epoch time: 89.9 s
2024-12-08 23:25:38.260642: 
2024-12-08 23:25:38.262408: Epoch 112
2024-12-08 23:25:38.263353: Current learning rate: 0.00899
2024-12-08 23:27:08.439695: Validation loss did not improve from -0.63026. Patience: 8/50
2024-12-08 23:27:08.441322: train_loss -0.7447
2024-12-08 23:27:08.442993: val_loss -0.5954
2024-12-08 23:27:08.444260: Pseudo dice [0.7574]
2024-12-08 23:27:08.445238: Epoch time: 90.18 s
2024-12-08 23:27:09.614048: 
2024-12-08 23:27:09.615975: Epoch 113
2024-12-08 23:27:09.617089: Current learning rate: 0.00898
2024-12-08 23:28:39.931780: Validation loss did not improve from -0.63026. Patience: 9/50
2024-12-08 23:28:39.932899: train_loss -0.7412
2024-12-08 23:28:39.934027: val_loss -0.5718
2024-12-08 23:28:39.935309: Pseudo dice [0.7609]
2024-12-08 23:28:39.936331: Epoch time: 90.32 s
2024-12-08 23:28:41.087067: 
2024-12-08 23:28:41.088644: Epoch 114
2024-12-08 23:28:41.089439: Current learning rate: 0.00897
2024-12-08 23:30:11.616120: Validation loss did not improve from -0.63026. Patience: 10/50
2024-12-08 23:30:11.617433: train_loss -0.7441
2024-12-08 23:30:11.618920: val_loss -0.6135
2024-12-08 23:30:11.620095: Pseudo dice [0.7841]
2024-12-08 23:30:11.621089: Epoch time: 90.53 s
2024-12-08 23:30:13.401988: 
2024-12-08 23:30:13.404124: Epoch 115
2024-12-08 23:30:13.405268: Current learning rate: 0.00896
2024-12-08 23:31:44.417639: Validation loss did not improve from -0.63026. Patience: 11/50
2024-12-08 23:31:44.418942: train_loss -0.7449
2024-12-08 23:31:44.420644: val_loss -0.6049
2024-12-08 23:31:44.421606: Pseudo dice [0.7683]
2024-12-08 23:31:44.422576: Epoch time: 91.02 s
2024-12-08 23:31:45.627191: 
2024-12-08 23:31:45.628962: Epoch 116
2024-12-08 23:31:45.630251: Current learning rate: 0.00895
2024-12-08 23:33:19.055142: Validation loss did not improve from -0.63026. Patience: 12/50
2024-12-08 23:33:19.085046: train_loss -0.7479
2024-12-08 23:33:19.119457: val_loss -0.6127
2024-12-08 23:33:19.120701: Pseudo dice [0.7821]
2024-12-08 23:33:19.122643: Epoch time: 93.45 s
2024-12-08 23:33:20.815521: 
2024-12-08 23:33:20.817736: Epoch 117
2024-12-08 23:33:20.818819: Current learning rate: 0.00894
2024-12-08 23:34:51.266133: Validation loss did not improve from -0.63026. Patience: 13/50
2024-12-08 23:34:51.267734: train_loss -0.7486
2024-12-08 23:34:51.269076: val_loss -0.5932
2024-12-08 23:34:51.270200: Pseudo dice [0.7703]
2024-12-08 23:34:51.271293: Epoch time: 90.45 s
2024-12-08 23:34:52.443159: 
2024-12-08 23:34:52.445155: Epoch 118
2024-12-08 23:34:52.446177: Current learning rate: 0.00893
2024-12-08 23:36:23.001410: Validation loss did not improve from -0.63026. Patience: 14/50
2024-12-08 23:36:23.002614: train_loss -0.748
2024-12-08 23:36:23.003846: val_loss -0.6259
2024-12-08 23:36:23.004726: Pseudo dice [0.7889]
2024-12-08 23:36:23.005486: Epoch time: 90.56 s
2024-12-08 23:36:24.175321: 
2024-12-08 23:36:24.177013: Epoch 119
2024-12-08 23:36:24.178240: Current learning rate: 0.00892
2024-12-08 23:37:54.811638: Validation loss did not improve from -0.63026. Patience: 15/50
2024-12-08 23:37:54.812734: train_loss -0.7475
2024-12-08 23:37:54.814247: val_loss -0.5912
2024-12-08 23:37:54.815064: Pseudo dice [0.7658]
2024-12-08 23:37:54.815677: Epoch time: 90.64 s
2024-12-08 23:37:56.422084: 
2024-12-08 23:37:56.424127: Epoch 120
2024-12-08 23:37:56.425394: Current learning rate: 0.00891
2024-12-08 23:39:27.305867: Validation loss did not improve from -0.63026. Patience: 16/50
2024-12-08 23:39:27.307155: train_loss -0.7409
2024-12-08 23:39:27.308578: val_loss -0.6232
2024-12-08 23:39:27.309515: Pseudo dice [0.7894]
2024-12-08 23:39:27.310399: Epoch time: 90.89 s
2024-12-08 23:39:27.311355: Yayy! New best EMA pseudo Dice: 0.7738
2024-12-08 23:39:28.804727: 
2024-12-08 23:39:28.806706: Epoch 121
2024-12-08 23:39:28.808176: Current learning rate: 0.0089
2024-12-08 23:40:59.507373: Validation loss did not improve from -0.63026. Patience: 17/50
2024-12-08 23:40:59.508386: train_loss -0.7489
2024-12-08 23:40:59.509331: val_loss -0.5936
2024-12-08 23:40:59.510280: Pseudo dice [0.768]
2024-12-08 23:40:59.511128: Epoch time: 90.7 s
2024-12-08 23:41:00.686554: 
2024-12-08 23:41:00.688488: Epoch 122
2024-12-08 23:41:00.689588: Current learning rate: 0.00889
2024-12-08 23:42:31.277704: Validation loss did not improve from -0.63026. Patience: 18/50
2024-12-08 23:42:31.279462: train_loss -0.7552
2024-12-08 23:42:31.280879: val_loss -0.6033
2024-12-08 23:42:31.281620: Pseudo dice [0.7806]
2024-12-08 23:42:31.282422: Epoch time: 90.59 s
2024-12-08 23:42:31.283066: Yayy! New best EMA pseudo Dice: 0.774
2024-12-08 23:42:32.794739: 
2024-12-08 23:42:32.796684: Epoch 123
2024-12-08 23:42:32.797871: Current learning rate: 0.00889
2024-12-08 23:44:03.295982: Validation loss did not improve from -0.63026. Patience: 19/50
2024-12-08 23:44:03.297494: train_loss -0.7582
2024-12-08 23:44:03.298677: val_loss -0.608
2024-12-08 23:44:03.299602: Pseudo dice [0.7795]
2024-12-08 23:44:03.300433: Epoch time: 90.5 s
2024-12-08 23:44:03.301178: Yayy! New best EMA pseudo Dice: 0.7745
2024-12-08 23:44:04.814427: 
2024-12-08 23:44:04.816566: Epoch 124
2024-12-08 23:44:04.817885: Current learning rate: 0.00888
2024-12-08 23:45:35.367148: Validation loss did not improve from -0.63026. Patience: 20/50
2024-12-08 23:45:35.368313: train_loss -0.7544
2024-12-08 23:45:35.369611: val_loss -0.579
2024-12-08 23:45:35.370932: Pseudo dice [0.7706]
2024-12-08 23:45:35.371981: Epoch time: 90.55 s
2024-12-08 23:45:36.922154: 
2024-12-08 23:45:36.923959: Epoch 125
2024-12-08 23:45:36.924993: Current learning rate: 0.00887
2024-12-08 23:47:07.498787: Validation loss did not improve from -0.63026. Patience: 21/50
2024-12-08 23:47:07.500512: train_loss -0.7588
2024-12-08 23:47:07.501806: val_loss -0.5881
2024-12-08 23:47:07.502809: Pseudo dice [0.7703]
2024-12-08 23:47:07.504106: Epoch time: 90.58 s
2024-12-08 23:47:09.564460: 
2024-12-08 23:47:09.566480: Epoch 126
2024-12-08 23:47:09.567382: Current learning rate: 0.00886
2024-12-08 23:48:40.008671: Validation loss did not improve from -0.63026. Patience: 22/50
2024-12-08 23:48:40.009903: train_loss -0.7544
2024-12-08 23:48:40.011201: val_loss -0.5834
2024-12-08 23:48:40.012367: Pseudo dice [0.7658]
2024-12-08 23:48:40.013332: Epoch time: 90.45 s
2024-12-08 23:48:41.209098: 
2024-12-08 23:48:41.211354: Epoch 127
2024-12-08 23:48:41.212686: Current learning rate: 0.00885
2024-12-08 23:50:11.768046: Validation loss did not improve from -0.63026. Patience: 23/50
2024-12-08 23:50:11.769027: train_loss -0.754
2024-12-08 23:50:11.769801: val_loss -0.5941
2024-12-08 23:50:11.770735: Pseudo dice [0.7718]
2024-12-08 23:50:11.771585: Epoch time: 90.56 s
2024-12-08 23:50:12.964936: 
2024-12-08 23:50:12.967215: Epoch 128
2024-12-08 23:50:12.968408: Current learning rate: 0.00884
2024-12-08 23:51:43.589389: Validation loss did not improve from -0.63026. Patience: 24/50
2024-12-08 23:51:43.591038: train_loss -0.7441
2024-12-08 23:51:43.592237: val_loss -0.5902
2024-12-08 23:51:43.593275: Pseudo dice [0.7575]
2024-12-08 23:51:43.594156: Epoch time: 90.63 s
2024-12-08 23:51:44.785509: 
2024-12-08 23:51:44.788412: Epoch 129
2024-12-08 23:51:44.790208: Current learning rate: 0.00883
2024-12-08 23:53:15.583133: Validation loss did not improve from -0.63026. Patience: 25/50
2024-12-08 23:53:15.584673: train_loss -0.7494
2024-12-08 23:53:15.585880: val_loss -0.5893
2024-12-08 23:53:15.586566: Pseudo dice [0.7676]
2024-12-08 23:53:15.587271: Epoch time: 90.8 s
2024-12-08 23:53:17.127114: 
2024-12-08 23:53:17.129158: Epoch 130
2024-12-08 23:53:17.130600: Current learning rate: 0.00882
2024-12-08 23:54:48.075773: Validation loss did not improve from -0.63026. Patience: 26/50
2024-12-08 23:54:48.076926: train_loss -0.7487
2024-12-08 23:54:48.078051: val_loss -0.5635
2024-12-08 23:54:48.078972: Pseudo dice [0.7467]
2024-12-08 23:54:48.079950: Epoch time: 90.95 s
2024-12-08 23:54:49.283049: 
2024-12-08 23:54:49.285056: Epoch 131
2024-12-08 23:54:49.286338: Current learning rate: 0.00881
2024-12-08 23:56:20.157998: Validation loss did not improve from -0.63026. Patience: 27/50
2024-12-08 23:56:20.158879: train_loss -0.7493
2024-12-08 23:56:20.159969: val_loss -0.6096
2024-12-08 23:56:20.160813: Pseudo dice [0.7799]
2024-12-08 23:56:20.161641: Epoch time: 90.88 s
2024-12-08 23:56:21.369782: 
2024-12-08 23:56:21.371955: Epoch 132
2024-12-08 23:56:21.373322: Current learning rate: 0.0088
2024-12-08 23:57:51.892037: Validation loss did not improve from -0.63026. Patience: 28/50
2024-12-08 23:57:51.893225: train_loss -0.7515
2024-12-08 23:57:51.894349: val_loss -0.6036
2024-12-08 23:57:51.895215: Pseudo dice [0.7749]
2024-12-08 23:57:51.896331: Epoch time: 90.52 s
2024-12-08 23:57:53.076969: 
2024-12-08 23:57:53.078840: Epoch 133
2024-12-08 23:57:53.079937: Current learning rate: 0.00879
2024-12-08 23:59:22.832633: Validation loss did not improve from -0.63026. Patience: 29/50
2024-12-08 23:59:22.834127: train_loss -0.7586
2024-12-08 23:59:22.835491: val_loss -0.566
2024-12-08 23:59:22.836499: Pseudo dice [0.7584]
2024-12-08 23:59:22.837218: Epoch time: 89.76 s
2024-12-08 23:59:24.013160: 
2024-12-08 23:59:24.015070: Epoch 134
2024-12-08 23:59:24.016191: Current learning rate: 0.00879
2024-12-09 00:00:53.345324: Validation loss did not improve from -0.63026. Patience: 30/50
2024-12-09 00:00:53.346262: train_loss -0.7548
2024-12-09 00:00:53.347709: val_loss -0.5812
2024-12-09 00:00:53.348526: Pseudo dice [0.7547]
2024-12-09 00:00:53.349327: Epoch time: 89.33 s
2024-12-09 00:00:54.876009: 
2024-12-09 00:00:54.878171: Epoch 135
2024-12-09 00:00:54.879541: Current learning rate: 0.00878
2024-12-09 00:02:24.159422: Validation loss did not improve from -0.63026. Patience: 31/50
2024-12-09 00:02:24.160367: train_loss -0.7492
2024-12-09 00:02:24.161332: val_loss -0.5998
2024-12-09 00:02:24.162451: Pseudo dice [0.7798]
2024-12-09 00:02:24.163373: Epoch time: 89.29 s
2024-12-09 00:02:25.375102: 
2024-12-09 00:02:25.377244: Epoch 136
2024-12-09 00:02:25.378410: Current learning rate: 0.00877
2024-12-09 00:03:54.682193: Validation loss did not improve from -0.63026. Patience: 32/50
2024-12-09 00:03:54.683555: train_loss -0.7505
2024-12-09 00:03:54.684959: val_loss -0.5826
2024-12-09 00:03:54.685853: Pseudo dice [0.7559]
2024-12-09 00:03:54.687072: Epoch time: 89.31 s
2024-12-09 00:03:56.249748: 
2024-12-09 00:03:56.251723: Epoch 137
2024-12-09 00:03:56.253209: Current learning rate: 0.00876
2024-12-09 00:05:25.536104: Validation loss did not improve from -0.63026. Patience: 33/50
2024-12-09 00:05:25.536929: train_loss -0.7544
2024-12-09 00:05:25.537893: val_loss -0.5752
2024-12-09 00:05:25.538698: Pseudo dice [0.7591]
2024-12-09 00:05:25.539748: Epoch time: 89.29 s
2024-12-09 00:05:26.744390: 
2024-12-09 00:05:26.746601: Epoch 138
2024-12-09 00:05:26.747969: Current learning rate: 0.00875
2024-12-09 00:06:56.315740: Validation loss did not improve from -0.63026. Patience: 34/50
2024-12-09 00:06:56.316805: train_loss -0.7497
2024-12-09 00:06:56.317577: val_loss -0.5861
2024-12-09 00:06:56.318305: Pseudo dice [0.7703]
2024-12-09 00:06:56.318949: Epoch time: 89.57 s
2024-12-09 00:06:57.505745: 
2024-12-09 00:06:57.507251: Epoch 139
2024-12-09 00:06:57.508176: Current learning rate: 0.00874
2024-12-09 00:08:27.316013: Validation loss did not improve from -0.63026. Patience: 35/50
2024-12-09 00:08:27.317111: train_loss -0.7529
2024-12-09 00:08:27.318223: val_loss -0.5956
2024-12-09 00:08:27.319158: Pseudo dice [0.7702]
2024-12-09 00:08:27.320009: Epoch time: 89.81 s
2024-12-09 00:08:28.835160: 
2024-12-09 00:08:28.837268: Epoch 140
2024-12-09 00:08:28.838588: Current learning rate: 0.00873
2024-12-09 00:09:58.622289: Validation loss did not improve from -0.63026. Patience: 36/50
2024-12-09 00:09:58.623526: train_loss -0.7518
2024-12-09 00:09:58.624538: val_loss -0.5829
2024-12-09 00:09:58.625217: Pseudo dice [0.762]
2024-12-09 00:09:58.625891: Epoch time: 89.79 s
2024-12-09 00:09:59.796313: 
2024-12-09 00:09:59.798325: Epoch 141
2024-12-09 00:09:59.799525: Current learning rate: 0.00872
2024-12-09 00:11:29.668155: Validation loss did not improve from -0.63026. Patience: 37/50
2024-12-09 00:11:29.669265: train_loss -0.7514
2024-12-09 00:11:29.670519: val_loss -0.6071
2024-12-09 00:11:29.671522: Pseudo dice [0.7762]
2024-12-09 00:11:29.672339: Epoch time: 89.87 s
2024-12-09 00:11:30.876790: 
2024-12-09 00:11:30.878700: Epoch 142
2024-12-09 00:11:30.879714: Current learning rate: 0.00871
2024-12-09 00:13:00.796139: Validation loss did not improve from -0.63026. Patience: 38/50
2024-12-09 00:13:00.797342: train_loss -0.7425
2024-12-09 00:13:00.798539: val_loss -0.599
2024-12-09 00:13:00.799541: Pseudo dice [0.7776]
2024-12-09 00:13:00.800560: Epoch time: 89.92 s
2024-12-09 00:13:02.019751: 
2024-12-09 00:13:02.020957: Epoch 143
2024-12-09 00:13:02.022416: Current learning rate: 0.0087
2024-12-09 00:14:31.901877: Validation loss did not improve from -0.63026. Patience: 39/50
2024-12-09 00:14:31.903234: train_loss -0.746
2024-12-09 00:14:31.904563: val_loss -0.594
2024-12-09 00:14:31.905452: Pseudo dice [0.7713]
2024-12-09 00:14:31.906586: Epoch time: 89.88 s
2024-12-09 00:14:33.126435: 
2024-12-09 00:14:33.128474: Epoch 144
2024-12-09 00:14:33.129699: Current learning rate: 0.00869
2024-12-09 00:16:03.178487: Validation loss improved from -0.63026 to -0.63519! Patience: 39/50
2024-12-09 00:16:03.179646: train_loss -0.7544
2024-12-09 00:16:03.180605: val_loss -0.6352
2024-12-09 00:16:03.181401: Pseudo dice [0.7925]
2024-12-09 00:16:03.182122: Epoch time: 90.05 s
2024-12-09 00:16:04.738144: 
2024-12-09 00:16:04.740264: Epoch 145
2024-12-09 00:16:04.741476: Current learning rate: 0.00868
2024-12-09 00:17:34.914291: Validation loss did not improve from -0.63519. Patience: 1/50
2024-12-09 00:17:34.915345: train_loss -0.7567
2024-12-09 00:17:34.916643: val_loss -0.5885
2024-12-09 00:17:34.917674: Pseudo dice [0.7731]
2024-12-09 00:17:34.918901: Epoch time: 90.18 s
2024-12-09 00:17:36.130920: 
2024-12-09 00:17:36.132499: Epoch 146
2024-12-09 00:17:36.133467: Current learning rate: 0.00868
2024-12-09 00:19:06.256289: Validation loss did not improve from -0.63519. Patience: 2/50
2024-12-09 00:19:06.257141: train_loss -0.7555
2024-12-09 00:19:06.258406: val_loss -0.629
2024-12-09 00:19:06.259437: Pseudo dice [0.7834]
2024-12-09 00:19:06.260593: Epoch time: 90.13 s
2024-12-09 00:19:07.482340: 
2024-12-09 00:19:07.484360: Epoch 147
2024-12-09 00:19:07.485876: Current learning rate: 0.00867
2024-12-09 00:20:37.657087: Validation loss did not improve from -0.63519. Patience: 3/50
2024-12-09 00:20:37.658148: train_loss -0.7526
2024-12-09 00:20:37.658905: val_loss -0.5982
2024-12-09 00:20:37.659600: Pseudo dice [0.7768]
2024-12-09 00:20:37.660694: Epoch time: 90.18 s
2024-12-09 00:20:39.231411: 
2024-12-09 00:20:39.233035: Epoch 148
2024-12-09 00:20:39.234288: Current learning rate: 0.00866
2024-12-09 00:22:09.151020: Validation loss did not improve from -0.63519. Patience: 4/50
2024-12-09 00:22:09.151856: train_loss -0.7617
2024-12-09 00:22:09.152819: val_loss -0.5923
2024-12-09 00:22:09.154139: Pseudo dice [0.7721]
2024-12-09 00:22:09.155117: Epoch time: 89.92 s
2024-12-09 00:22:10.371689: 
2024-12-09 00:22:10.373870: Epoch 149
2024-12-09 00:22:10.375131: Current learning rate: 0.00865
2024-12-09 00:23:40.023748: Validation loss did not improve from -0.63519. Patience: 5/50
2024-12-09 00:23:40.024849: train_loss -0.7674
2024-12-09 00:23:40.025987: val_loss -0.6099
2024-12-09 00:23:40.026773: Pseudo dice [0.7774]
2024-12-09 00:23:40.027469: Epoch time: 89.65 s
2024-12-09 00:23:41.567915: 
2024-12-09 00:23:41.570224: Epoch 150
2024-12-09 00:23:41.571386: Current learning rate: 0.00864
2024-12-09 00:25:11.182800: Validation loss did not improve from -0.63519. Patience: 6/50
2024-12-09 00:25:11.184132: train_loss -0.7664
2024-12-09 00:25:11.185554: val_loss -0.6225
2024-12-09 00:25:11.186683: Pseudo dice [0.787]
2024-12-09 00:25:11.187823: Epoch time: 89.62 s
2024-12-09 00:25:11.188802: Yayy! New best EMA pseudo Dice: 0.7748
2024-12-09 00:25:12.758756: 
2024-12-09 00:25:12.760507: Epoch 151
2024-12-09 00:25:12.761938: Current learning rate: 0.00863
2024-12-09 00:26:42.354722: Validation loss did not improve from -0.63519. Patience: 7/50
2024-12-09 00:26:42.356205: train_loss -0.7629
2024-12-09 00:26:42.357541: val_loss -0.6169
2024-12-09 00:26:42.358410: Pseudo dice [0.7825]
2024-12-09 00:26:42.359312: Epoch time: 89.6 s
2024-12-09 00:26:42.360248: Yayy! New best EMA pseudo Dice: 0.7756
2024-12-09 00:26:43.937373: 
2024-12-09 00:26:43.939087: Epoch 152
2024-12-09 00:26:43.940192: Current learning rate: 0.00862
2024-12-09 00:28:13.574908: Validation loss did not improve from -0.63519. Patience: 8/50
2024-12-09 00:28:13.576241: train_loss -0.7648
2024-12-09 00:28:13.577742: val_loss -0.5724
2024-12-09 00:28:13.578910: Pseudo dice [0.7633]
2024-12-09 00:28:13.579790: Epoch time: 89.64 s
2024-12-09 00:28:14.799019: 
2024-12-09 00:28:14.801362: Epoch 153
2024-12-09 00:28:14.802781: Current learning rate: 0.00861
2024-12-09 00:29:44.670629: Validation loss did not improve from -0.63519. Patience: 9/50
2024-12-09 00:29:44.671554: train_loss -0.764
2024-12-09 00:29:44.672647: val_loss -0.6013
2024-12-09 00:29:44.673628: Pseudo dice [0.778]
2024-12-09 00:29:44.674444: Epoch time: 89.87 s
2024-12-09 00:29:45.882897: 
2024-12-09 00:29:45.884952: Epoch 154
2024-12-09 00:29:45.886055: Current learning rate: 0.0086
2024-12-09 00:31:15.637236: Validation loss did not improve from -0.63519. Patience: 10/50
2024-12-09 00:31:15.638638: train_loss -0.7617
2024-12-09 00:31:15.639819: val_loss -0.5999
2024-12-09 00:31:15.640597: Pseudo dice [0.7841]
2024-12-09 00:31:15.641495: Epoch time: 89.76 s
2024-12-09 00:31:15.991521: Yayy! New best EMA pseudo Dice: 0.7757
2024-12-09 00:31:17.550601: 
2024-12-09 00:31:17.552774: Epoch 155
2024-12-09 00:31:17.553850: Current learning rate: 0.00859
2024-12-09 00:32:47.291464: Validation loss did not improve from -0.63519. Patience: 11/50
2024-12-09 00:32:47.292615: train_loss -0.7631
2024-12-09 00:32:47.293852: val_loss -0.6269
2024-12-09 00:32:47.295005: Pseudo dice [0.7918]
2024-12-09 00:32:47.295891: Epoch time: 89.74 s
2024-12-09 00:32:47.296837: Yayy! New best EMA pseudo Dice: 0.7773
2024-12-09 00:32:48.861665: 
2024-12-09 00:32:48.863892: Epoch 156
2024-12-09 00:32:48.865450: Current learning rate: 0.00858
2024-12-09 00:34:18.714266: Validation loss did not improve from -0.63519. Patience: 12/50
2024-12-09 00:34:18.715655: train_loss -0.7553
2024-12-09 00:34:18.716594: val_loss -0.5865
2024-12-09 00:34:18.717280: Pseudo dice [0.7572]
2024-12-09 00:34:18.718174: Epoch time: 89.86 s
2024-12-09 00:34:19.951413: 
2024-12-09 00:34:19.953439: Epoch 157
2024-12-09 00:34:19.954629: Current learning rate: 0.00858
2024-12-09 00:35:49.701925: Validation loss did not improve from -0.63519. Patience: 13/50
2024-12-09 00:35:49.703142: train_loss -0.7592
2024-12-09 00:35:49.704211: val_loss -0.6053
2024-12-09 00:35:49.704934: Pseudo dice [0.7789]
2024-12-09 00:35:49.705730: Epoch time: 89.75 s
2024-12-09 00:35:50.953298: 
2024-12-09 00:35:50.955307: Epoch 158
2024-12-09 00:35:50.956564: Current learning rate: 0.00857
2024-12-09 00:37:21.502318: Validation loss did not improve from -0.63519. Patience: 14/50
2024-12-09 00:37:21.505984: train_loss -0.7529
2024-12-09 00:37:21.509195: val_loss -0.5707
2024-12-09 00:37:21.510357: Pseudo dice [0.7594]
2024-12-09 00:37:21.511739: Epoch time: 90.55 s
2024-12-09 00:37:22.746866: 
2024-12-09 00:37:22.748862: Epoch 159
2024-12-09 00:37:22.750079: Current learning rate: 0.00856
2024-12-09 00:38:52.641309: Validation loss did not improve from -0.63519. Patience: 15/50
2024-12-09 00:38:52.643983: train_loss -0.7592
2024-12-09 00:38:52.645162: val_loss -0.5853
2024-12-09 00:38:52.645954: Pseudo dice [0.7639]
2024-12-09 00:38:52.646857: Epoch time: 89.9 s
2024-12-09 00:38:54.242440: 
2024-12-09 00:38:54.244048: Epoch 160
2024-12-09 00:38:54.245237: Current learning rate: 0.00855
2024-12-09 00:40:24.013289: Validation loss did not improve from -0.63519. Patience: 16/50
2024-12-09 00:40:24.014302: train_loss -0.7549
2024-12-09 00:40:24.015319: val_loss -0.6008
2024-12-09 00:40:24.016004: Pseudo dice [0.7686]
2024-12-09 00:40:24.016797: Epoch time: 89.77 s
2024-12-09 00:40:25.267485: 
2024-12-09 00:40:25.269440: Epoch 161
2024-12-09 00:40:25.270425: Current learning rate: 0.00854
2024-12-09 00:41:55.170057: Validation loss did not improve from -0.63519. Patience: 17/50
2024-12-09 00:41:55.171292: train_loss -0.7605
2024-12-09 00:41:55.172329: val_loss -0.6027
2024-12-09 00:41:55.173168: Pseudo dice [0.7834]
2024-12-09 00:41:55.173861: Epoch time: 89.9 s
2024-12-09 00:41:56.400428: 
2024-12-09 00:41:56.402897: Epoch 162
2024-12-09 00:41:56.404007: Current learning rate: 0.00853
2024-12-09 00:43:26.511210: Validation loss did not improve from -0.63519. Patience: 18/50
2024-12-09 00:43:26.512080: train_loss -0.7626
2024-12-09 00:43:26.513064: val_loss -0.616
2024-12-09 00:43:26.514017: Pseudo dice [0.7871]
2024-12-09 00:43:26.515028: Epoch time: 90.11 s
2024-12-09 00:43:27.754546: 
2024-12-09 00:43:27.756418: Epoch 163
2024-12-09 00:43:27.757538: Current learning rate: 0.00852
2024-12-09 00:44:57.719101: Validation loss did not improve from -0.63519. Patience: 19/50
2024-12-09 00:44:57.720105: train_loss -0.7603
2024-12-09 00:44:57.721044: val_loss -0.5561
2024-12-09 00:44:57.722021: Pseudo dice [0.7614]
2024-12-09 00:44:57.722981: Epoch time: 89.97 s
2024-12-09 00:44:58.931493: 
2024-12-09 00:44:58.933465: Epoch 164
2024-12-09 00:44:58.934680: Current learning rate: 0.00851
2024-12-09 00:46:28.849095: Validation loss did not improve from -0.63519. Patience: 20/50
2024-12-09 00:46:28.850058: train_loss -0.7485
2024-12-09 00:46:28.850954: val_loss -0.57
2024-12-09 00:46:28.851696: Pseudo dice [0.7583]
2024-12-09 00:46:28.852370: Epoch time: 89.92 s
2024-12-09 00:46:30.375594: 
2024-12-09 00:46:30.377080: Epoch 165
2024-12-09 00:46:30.378201: Current learning rate: 0.0085
2024-12-09 00:47:59.999811: Validation loss did not improve from -0.63519. Patience: 21/50
2024-12-09 00:48:00.000898: train_loss -0.7578
2024-12-09 00:48:00.001817: val_loss -0.579
2024-12-09 00:48:00.002588: Pseudo dice [0.7667]
2024-12-09 00:48:00.003319: Epoch time: 89.63 s
2024-12-09 00:48:01.182772: 
2024-12-09 00:48:01.184337: Epoch 166
2024-12-09 00:48:01.185126: Current learning rate: 0.00849
2024-12-09 00:49:30.604543: Validation loss did not improve from -0.63519. Patience: 22/50
2024-12-09 00:49:30.605482: train_loss -0.7614
2024-12-09 00:49:30.606797: val_loss -0.588
2024-12-09 00:49:30.607690: Pseudo dice [0.764]
2024-12-09 00:49:30.608345: Epoch time: 89.42 s
2024-12-09 00:49:31.818856: 
2024-12-09 00:49:31.820993: Epoch 167
2024-12-09 00:49:31.821910: Current learning rate: 0.00848
2024-12-09 00:51:01.382218: Validation loss did not improve from -0.63519. Patience: 23/50
2024-12-09 00:51:01.383495: train_loss -0.7669
2024-12-09 00:51:01.384489: val_loss -0.6207
2024-12-09 00:51:01.385283: Pseudo dice [0.7795]
2024-12-09 00:51:01.386149: Epoch time: 89.57 s
2024-12-09 00:51:02.620349: 
2024-12-09 00:51:02.622214: Epoch 168
2024-12-09 00:51:02.623229: Current learning rate: 0.00847
2024-12-09 00:52:33.045428: Validation loss did not improve from -0.63519. Patience: 24/50
2024-12-09 00:52:33.048210: train_loss -0.7701
2024-12-09 00:52:33.049513: val_loss -0.6311
2024-12-09 00:52:33.050430: Pseudo dice [0.788]
2024-12-09 00:52:33.051580: Epoch time: 90.43 s
2024-12-09 00:52:34.732857: 
2024-12-09 00:52:34.735077: Epoch 169
2024-12-09 00:52:34.736188: Current learning rate: 0.00847
2024-12-09 00:54:04.053994: Validation loss did not improve from -0.63519. Patience: 25/50
2024-12-09 00:54:04.055054: train_loss -0.7666
2024-12-09 00:54:04.056216: val_loss -0.6142
2024-12-09 00:54:04.056934: Pseudo dice [0.7814]
2024-12-09 00:54:04.057768: Epoch time: 89.32 s
2024-12-09 00:54:05.621280: 
2024-12-09 00:54:05.623303: Epoch 170
2024-12-09 00:54:05.624287: Current learning rate: 0.00846
2024-12-09 00:55:35.094464: Validation loss did not improve from -0.63519. Patience: 26/50
2024-12-09 00:55:35.095563: train_loss -0.7595
2024-12-09 00:55:35.096605: val_loss -0.5983
2024-12-09 00:55:35.097501: Pseudo dice [0.768]
2024-12-09 00:55:35.098659: Epoch time: 89.48 s
2024-12-09 00:55:36.304599: 
2024-12-09 00:55:36.306566: Epoch 171
2024-12-09 00:55:36.307977: Current learning rate: 0.00845
2024-12-09 00:57:05.780387: Validation loss did not improve from -0.63519. Patience: 27/50
2024-12-09 00:57:05.781172: train_loss -0.767
2024-12-09 00:57:05.782452: val_loss -0.6162
2024-12-09 00:57:05.783888: Pseudo dice [0.7839]
2024-12-09 00:57:05.784952: Epoch time: 89.48 s
2024-12-09 00:57:06.954990: 
2024-12-09 00:57:06.956983: Epoch 172
2024-12-09 00:57:06.958162: Current learning rate: 0.00844
2024-12-09 00:58:36.584088: Validation loss did not improve from -0.63519. Patience: 28/50
2024-12-09 00:58:36.585245: train_loss -0.7716
2024-12-09 00:58:36.586164: val_loss -0.6182
2024-12-09 00:58:36.586994: Pseudo dice [0.7879]
2024-12-09 00:58:36.587660: Epoch time: 89.63 s
2024-12-09 00:58:37.811998: 
2024-12-09 00:58:37.814112: Epoch 173
2024-12-09 00:58:37.815107: Current learning rate: 0.00843
2024-12-09 01:00:07.419621: Validation loss did not improve from -0.63519. Patience: 29/50
2024-12-09 01:00:07.420644: train_loss -0.7742
2024-12-09 01:00:07.422072: val_loss -0.5834
2024-12-09 01:00:07.423183: Pseudo dice [0.7625]
2024-12-09 01:00:07.424120: Epoch time: 89.61 s
2024-12-09 01:00:08.637501: 
2024-12-09 01:00:08.639827: Epoch 174
2024-12-09 01:00:08.641294: Current learning rate: 0.00842
2024-12-09 01:01:38.260959: Validation loss did not improve from -0.63519. Patience: 30/50
2024-12-09 01:01:38.262064: train_loss -0.766
2024-12-09 01:01:38.263215: val_loss -0.5812
2024-12-09 01:01:38.264073: Pseudo dice [0.7626]
2024-12-09 01:01:38.264950: Epoch time: 89.63 s
2024-12-09 01:01:39.831715: 
2024-12-09 01:01:39.833591: Epoch 175
2024-12-09 01:01:39.834767: Current learning rate: 0.00841
2024-12-09 01:03:09.617932: Validation loss did not improve from -0.63519. Patience: 31/50
2024-12-09 01:03:09.619216: train_loss -0.7681
2024-12-09 01:03:09.620199: val_loss -0.6136
2024-12-09 01:03:09.620951: Pseudo dice [0.783]
2024-12-09 01:03:09.621830: Epoch time: 89.79 s
2024-12-09 01:03:10.837502: 
2024-12-09 01:03:10.839576: Epoch 176
2024-12-09 01:03:10.840698: Current learning rate: 0.0084
2024-12-09 01:04:40.602953: Validation loss did not improve from -0.63519. Patience: 32/50
2024-12-09 01:04:40.604180: train_loss -0.7681
2024-12-09 01:04:40.605421: val_loss -0.5948
2024-12-09 01:04:40.606227: Pseudo dice [0.7763]
2024-12-09 01:04:40.606963: Epoch time: 89.77 s
2024-12-09 01:04:41.820832: 
2024-12-09 01:04:41.822940: Epoch 177
2024-12-09 01:04:41.824123: Current learning rate: 0.00839
2024-12-09 01:06:11.530501: Validation loss did not improve from -0.63519. Patience: 33/50
2024-12-09 01:06:11.531720: train_loss -0.7683
2024-12-09 01:06:11.532670: val_loss -0.5723
2024-12-09 01:06:11.533263: Pseudo dice [0.7555]
2024-12-09 01:06:11.533901: Epoch time: 89.71 s
2024-12-09 01:06:12.740733: 
2024-12-09 01:06:12.742583: Epoch 178
2024-12-09 01:06:12.743762: Current learning rate: 0.00838
2024-12-09 01:07:42.505128: Validation loss did not improve from -0.63519. Patience: 34/50
2024-12-09 01:07:42.506239: train_loss -0.7705
2024-12-09 01:07:42.507531: val_loss -0.598
2024-12-09 01:07:42.508286: Pseudo dice [0.7707]
2024-12-09 01:07:42.509046: Epoch time: 89.77 s
2024-12-09 01:07:44.043240: 
2024-12-09 01:07:44.045390: Epoch 179
2024-12-09 01:07:44.046394: Current learning rate: 0.00837
2024-12-09 01:09:13.656642: Validation loss did not improve from -0.63519. Patience: 35/50
2024-12-09 01:09:13.658133: train_loss -0.7734
2024-12-09 01:09:13.659540: val_loss -0.6158
2024-12-09 01:09:13.660736: Pseudo dice [0.7858]
2024-12-09 01:09:13.661885: Epoch time: 89.62 s
2024-12-09 01:09:15.249169: 
2024-12-09 01:09:15.251779: Epoch 180
2024-12-09 01:09:15.253220: Current learning rate: 0.00836
2024-12-09 01:10:45.090993: Validation loss did not improve from -0.63519. Patience: 36/50
2024-12-09 01:10:45.092124: train_loss -0.7746
2024-12-09 01:10:45.093062: val_loss -0.6208
2024-12-09 01:10:45.093808: Pseudo dice [0.7843]
2024-12-09 01:10:45.094801: Epoch time: 89.84 s
2024-12-09 01:10:46.296325: 
2024-12-09 01:10:46.298167: Epoch 181
2024-12-09 01:10:46.299359: Current learning rate: 0.00836
2024-12-09 01:12:16.473898: Validation loss did not improve from -0.63519. Patience: 37/50
2024-12-09 01:12:16.474715: train_loss -0.7735
2024-12-09 01:12:16.475712: val_loss -0.6087
2024-12-09 01:12:16.476604: Pseudo dice [0.7759]
2024-12-09 01:12:16.477420: Epoch time: 90.18 s
2024-12-09 01:12:17.709271: 
2024-12-09 01:12:17.711080: Epoch 182
2024-12-09 01:12:17.712361: Current learning rate: 0.00835
2024-12-09 01:13:47.744242: Validation loss did not improve from -0.63519. Patience: 38/50
2024-12-09 01:13:47.745500: train_loss -0.7709
2024-12-09 01:13:47.746911: val_loss -0.6087
2024-12-09 01:13:47.747926: Pseudo dice [0.7824]
2024-12-09 01:13:47.749036: Epoch time: 90.04 s
2024-12-09 01:13:48.956051: 
2024-12-09 01:13:48.958098: Epoch 183
2024-12-09 01:13:48.959479: Current learning rate: 0.00834
2024-12-09 01:15:18.927450: Validation loss did not improve from -0.63519. Patience: 39/50
2024-12-09 01:15:18.928760: train_loss -0.7732
2024-12-09 01:15:18.930190: val_loss -0.5858
2024-12-09 01:15:18.930943: Pseudo dice [0.7692]
2024-12-09 01:15:18.931599: Epoch time: 89.97 s
2024-12-09 01:15:20.119658: 
2024-12-09 01:15:20.121841: Epoch 184
2024-12-09 01:15:20.122947: Current learning rate: 0.00833
2024-12-09 01:16:50.445493: Validation loss did not improve from -0.63519. Patience: 40/50
2024-12-09 01:16:50.446572: train_loss -0.771
2024-12-09 01:16:50.447566: val_loss -0.6082
2024-12-09 01:16:50.448455: Pseudo dice [0.78]
2024-12-09 01:16:50.449204: Epoch time: 90.33 s
2024-12-09 01:16:51.999612: 
2024-12-09 01:16:52.001996: Epoch 185
2024-12-09 01:16:52.002928: Current learning rate: 0.00832
2024-12-09 01:18:22.400026: Validation loss did not improve from -0.63519. Patience: 41/50
2024-12-09 01:18:22.401240: train_loss -0.7716
2024-12-09 01:18:22.402113: val_loss -0.542
2024-12-09 01:18:22.402807: Pseudo dice [0.7334]
2024-12-09 01:18:22.403514: Epoch time: 90.4 s
2024-12-09 01:18:23.620462: 
2024-12-09 01:18:23.622390: Epoch 186
2024-12-09 01:18:23.623543: Current learning rate: 0.00831
2024-12-09 01:19:53.832968: Validation loss did not improve from -0.63519. Patience: 42/50
2024-12-09 01:19:53.834495: train_loss -0.7739
2024-12-09 01:19:53.835763: val_loss -0.6183
2024-12-09 01:19:53.836905: Pseudo dice [0.7854]
2024-12-09 01:19:53.838177: Epoch time: 90.21 s
2024-12-09 01:19:55.067179: 
2024-12-09 01:19:55.069403: Epoch 187
2024-12-09 01:19:55.070499: Current learning rate: 0.0083
2024-12-09 01:21:24.994198: Validation loss did not improve from -0.63519. Patience: 43/50
2024-12-09 01:21:24.995460: train_loss -0.7732
2024-12-09 01:21:24.996689: val_loss -0.5765
2024-12-09 01:21:24.997659: Pseudo dice [0.7671]
2024-12-09 01:21:24.998515: Epoch time: 89.93 s
2024-12-09 01:21:26.176010: 
2024-12-09 01:21:26.178219: Epoch 188
2024-12-09 01:21:26.179719: Current learning rate: 0.00829
2024-12-09 01:22:55.979719: Validation loss did not improve from -0.63519. Patience: 44/50
2024-12-09 01:22:55.981178: train_loss -0.7724
2024-12-09 01:22:55.982770: val_loss -0.5913
2024-12-09 01:22:55.983888: Pseudo dice [0.7743]
2024-12-09 01:22:55.984731: Epoch time: 89.81 s
2024-12-09 01:22:57.210705: 
2024-12-09 01:22:57.212701: Epoch 189
2024-12-09 01:22:57.213965: Current learning rate: 0.00828
2024-12-09 01:24:27.268566: Validation loss did not improve from -0.63519. Patience: 45/50
2024-12-09 01:24:27.270036: train_loss -0.771
2024-12-09 01:24:27.271374: val_loss -0.6013
2024-12-09 01:24:27.272046: Pseudo dice [0.7762]
2024-12-09 01:24:27.272686: Epoch time: 90.06 s
2024-12-09 01:24:29.140117: 
2024-12-09 01:24:29.141962: Epoch 190
2024-12-09 01:24:29.143205: Current learning rate: 0.00827
2024-12-09 01:25:59.382569: Validation loss did not improve from -0.63519. Patience: 46/50
2024-12-09 01:25:59.383753: train_loss -0.7737
2024-12-09 01:25:59.385138: val_loss -0.5949
2024-12-09 01:25:59.385986: Pseudo dice [0.7755]
2024-12-09 01:25:59.387149: Epoch time: 90.24 s
2024-12-09 01:26:00.603298: 
2024-12-09 01:26:00.605226: Epoch 191
2024-12-09 01:26:00.606450: Current learning rate: 0.00826
2024-12-09 01:27:30.825405: Validation loss did not improve from -0.63519. Patience: 47/50
2024-12-09 01:27:30.826405: train_loss -0.7773
2024-12-09 01:27:30.827482: val_loss -0.6346
2024-12-09 01:27:30.828640: Pseudo dice [0.7908]
2024-12-09 01:27:30.830024: Epoch time: 90.22 s
2024-12-09 01:27:32.058340: 
2024-12-09 01:27:32.060285: Epoch 192
2024-12-09 01:27:32.061455: Current learning rate: 0.00825
2024-12-09 01:29:02.203934: Validation loss did not improve from -0.63519. Patience: 48/50
2024-12-09 01:29:02.204796: train_loss -0.7771
2024-12-09 01:29:02.206020: val_loss -0.5821
2024-12-09 01:29:02.207068: Pseudo dice [0.7622]
2024-12-09 01:29:02.208128: Epoch time: 90.15 s
2024-12-09 01:29:03.440923: 
2024-12-09 01:29:03.443515: Epoch 193
2024-12-09 01:29:03.445073: Current learning rate: 0.00824
2024-12-09 01:30:33.354131: Validation loss did not improve from -0.63519. Patience: 49/50
2024-12-09 01:30:33.355681: train_loss -0.7803
2024-12-09 01:30:33.357016: val_loss -0.6144
2024-12-09 01:30:33.358212: Pseudo dice [0.7759]
2024-12-09 01:30:33.359232: Epoch time: 89.92 s
2024-12-09 01:30:34.597713: 
2024-12-09 01:30:34.599952: Epoch 194
2024-12-09 01:30:34.601486: Current learning rate: 0.00824
2024-12-09 01:32:04.608837: Validation loss did not improve from -0.63519. Patience: 50/50
2024-12-09 01:32:04.609990: train_loss -0.7789
2024-12-09 01:32:04.611343: val_loss -0.6014
2024-12-09 01:32:04.612369: Pseudo dice [0.7754]
2024-12-09 01:32:04.613676: Epoch time: 90.01 s
2024-12-09 01:32:06.201025: Patience reached. Stopping training.
2024-12-09 01:32:06.531685: Training done.
2024-12-09 01:32:06.690313: Using splits from existing split file: /home/gridsan/nchutisilp/datasets/nnUNet_Datasets/nnUNet_preprocessed/Dataset308_Sohee_Calcium_OCT_CrossValidation/splits_final.json
2024-12-09 01:32:06.726912: The split file contains 5 splits.
2024-12-09 01:32:06.727708: Desired fold for training: 2
2024-12-09 01:32:06.728581: This split has 6 training and 2 validation cases.
2024-12-09 01:32:06.729313: predicting 101-044
2024-12-09 01:32:06.768474: 101-044, shape torch.Size([1, 404, 498, 498]), rank 0
2024-12-09 01:33:53.079808: predicting 704-003
2024-12-09 01:33:53.108633: 704-003, shape torch.Size([1, 375, 498, 498]), rank 0
2024-12-09 01:35:42.961637: Validation complete
2024-12-09 01:35:42.962704: Mean Validation Dice:  0.7557721284593206

############################
INFO: You are using the old nnU-Net default plans. We have updated our recommendations. Please consider using those instead! Read more here: https://github.com/MIC-DKFZ/nnUNet/blob/master/documentation/resenc_presets.md
############################

Using device: cuda:0

#######################################################################
Please cite the following paper when using nnU-Net:
Isensee, F., Jaeger, P. F., Kohl, S. A., Petersen, J., & Maier-Hein, K. H. (2021). nnU-Net: a self-configuring method for deep learning-based biomedical image segmentation. Nature methods, 18(2), 203-211.
#######################################################################

2024-12-09 01:35:51.587275: Using torch.compile...
/home/gridsan/nchutisilp/.local/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn("The verbose parameter is deprecated. Please use get_last_lr() "
################### Loading pretrained weights from file  /home/gridsan/nchutisilp/datasets/nnUNet_Datasets/nnUNet_results/Dataset300_Lumen_and_Wall_OCT/nnUNetTrainer__nnUNetPreprocessPlans__3d_fullres/fold_all/checkpoint_best.pth ###################
Below is the list of overlapping blocks in pretrained model and nnUNet architecture:
encoder.stages.0.0.convs.0.conv.weight shape torch.Size([32, 1, 3, 3, 3])
encoder.stages.0.0.convs.0.conv.bias shape torch.Size([32])
encoder.stages.0.0.convs.0.norm.weight shape torch.Size([32])
encoder.stages.0.0.convs.0.norm.bias shape torch.Size([32])
encoder.stages.0.0.convs.0.all_modules.0.weight shape torch.Size([32, 1, 3, 3, 3])
encoder.stages.0.0.convs.0.all_modules.0.bias shape torch.Size([32])
encoder.stages.0.0.convs.0.all_modules.1.weight shape torch.Size([32])
encoder.stages.0.0.convs.0.all_modules.1.bias shape torch.Size([32])
encoder.stages.0.0.convs.1.conv.weight shape torch.Size([32, 32, 3, 3, 3])
encoder.stages.0.0.convs.1.conv.bias shape torch.Size([32])
encoder.stages.0.0.convs.1.norm.weight shape torch.Size([32])
encoder.stages.0.0.convs.1.norm.bias shape torch.Size([32])
encoder.stages.0.0.convs.1.all_modules.0.weight shape torch.Size([32, 32, 3, 3, 3])
encoder.stages.0.0.convs.1.all_modules.0.bias shape torch.Size([32])
encoder.stages.0.0.convs.1.all_modules.1.weight shape torch.Size([32])
encoder.stages.0.0.convs.1.all_modules.1.bias shape torch.Size([32])
encoder.stages.1.0.convs.0.conv.weight shape torch.Size([64, 32, 3, 3, 3])
encoder.stages.1.0.convs.0.conv.bias shape torch.Size([64])
encoder.stages.1.0.convs.0.norm.weight shape torch.Size([64])
encoder.stages.1.0.convs.0.norm.bias shape torch.Size([64])
encoder.stages.1.0.convs.0.all_modules.0.weight shape torch.Size([64, 32, 3, 3, 3])
encoder.stages.1.0.convs.0.all_modules.0.bias shape torch.Size([64])
encoder.stages.1.0.convs.0.all_modules.1.weight shape torch.Size([64])
encoder.stages.1.0.convs.0.all_modules.1.bias shape torch.Size([64])
encoder.stages.1.0.convs.1.conv.weight shape torch.Size([64, 64, 3, 3, 3])
encoder.stages.1.0.convs.1.conv.bias shape torch.Size([64])
encoder.stages.1.0.convs.1.norm.weight shape torch.Size([64])
encoder.stages.1.0.convs.1.norm.bias shape torch.Size([64])
encoder.stages.1.0.convs.1.all_modules.0.weight shape torch.Size([64, 64, 3, 3, 3])
encoder.stages.1.0.convs.1.all_modules.0.bias shape torch.Size([64])
encoder.stages.1.0.convs.1.all_modules.1.weight shape torch.Size([64])
encoder.stages.1.0.convs.1.all_modules.1.bias shape torch.Size([64])
encoder.stages.2.0.convs.0.conv.weight shape torch.Size([128, 64, 3, 3, 3])
encoder.stages.2.0.convs.0.conv.bias shape torch.Size([128])
encoder.stages.2.0.convs.0.norm.weight shape torch.Size([128])
encoder.stages.2.0.convs.0.norm.bias shape torch.Size([128])
encoder.stages.2.0.convs.0.all_modules.0.weight shape torch.Size([128, 64, 3, 3, 3])
encoder.stages.2.0.convs.0.all_modules.0.bias shape torch.Size([128])
encoder.stages.2.0.convs.0.all_modules.1.weight shape torch.Size([128])
encoder.stages.2.0.convs.0.all_modules.1.bias shape torch.Size([128])
encoder.stages.2.0.convs.1.conv.weight shape torch.Size([128, 128, 3, 3, 3])
encoder.stages.2.0.convs.1.conv.bias shape torch.Size([128])
encoder.stages.2.0.convs.1.norm.weight shape torch.Size([128])
encoder.stages.2.0.convs.1.norm.bias shape torch.Size([128])
encoder.stages.2.0.convs.1.all_modules.0.weight shape torch.Size([128, 128, 3, 3, 3])
encoder.stages.2.0.convs.1.all_modules.0.bias shape torch.Size([128])
encoder.stages.2.0.convs.1.all_modules.1.weight shape torch.Size([128])
encoder.stages.2.0.convs.1.all_modules.1.bias shape torch.Size([128])
encoder.stages.3.0.convs.0.conv.weight shape torch.Size([256, 128, 3, 3, 3])
encoder.stages.3.0.convs.0.conv.bias shape torch.Size([256])
encoder.stages.3.0.convs.0.norm.weight shape torch.Size([256])
encoder.stages.3.0.convs.0.norm.bias shape torch.Size([256])
encoder.stages.3.0.convs.0.all_modules.0.weight shape torch.Size([256, 128, 3, 3, 3])
encoder.stages.3.0.convs.0.all_modules.0.bias shape torch.Size([256])
encoder.stages.3.0.convs.0.all_modules.1.weight shape torch.Size([256])
encoder.stages.3.0.convs.0.all_modules.1.bias shape torch.Size([256])
encoder.stages.3.0.convs.1.conv.weight shape torch.Size([256, 256, 3, 3, 3])
encoder.stages.3.0.convs.1.conv.bias shape torch.Size([256])
encoder.stages.3.0.convs.1.norm.weight shape torch.Size([256])
encoder.stages.3.0.convs.1.norm.bias shape torch.Size([256])
encoder.stages.3.0.convs.1.all_modules.0.weight shape torch.Size([256, 256, 3, 3, 3])
encoder.stages.3.0.convs.1.all_modules.0.bias shape torch.Size([256])
encoder.stages.3.0.convs.1.all_modules.1.weight shape torch.Size([256])
encoder.stages.3.0.convs.1.all_modules.1.bias shape torch.Size([256])
encoder.stages.4.0.convs.0.conv.weight shape torch.Size([320, 256, 3, 3, 3])
encoder.stages.4.0.convs.0.conv.bias shape torch.Size([320])
encoder.stages.4.0.convs.0.norm.weight shape torch.Size([320])
encoder.stages.4.0.convs.0.norm.bias shape torch.Size([320])
encoder.stages.4.0.convs.0.all_modules.0.weight shape torch.Size([320, 256, 3, 3, 3])
encoder.stages.4.0.convs.0.all_modules.0.bias shape torch.Size([320])
encoder.stages.4.0.convs.0.all_modules.1.weight shape torch.Size([320])
encoder.stages.4.0.convs.0.all_modules.1.bias shape torch.Size([320])
encoder.stages.4.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.4.0.convs.1.conv.bias shape torch.Size([320])
encoder.stages.4.0.convs.1.norm.weight shape torch.Size([320])
encoder.stages.4.0.convs.1.norm.bias shape torch.Size([320])
encoder.stages.4.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.4.0.convs.1.all_modules.0.bias shape torch.Size([320])
encoder.stages.4.0.convs.1.all_modules.1.weight shape torch.Size([320])
encoder.stages.4.0.convs.1.all_modules.1.bias shape torch.Size([320])
encoder.stages.5.0.convs.0.conv.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.5.0.convs.0.conv.bias shape torch.Size([320])
encoder.stages.5.0.convs.0.norm.weight shape torch.Size([320])
encoder.stages.5.0.convs.0.norm.bias shape torch.Size([320])
encoder.stages.5.0.convs.0.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.5.0.convs.0.all_modules.0.bias shape torch.Size([320])
encoder.stages.5.0.convs.0.all_modules.1.weight shape torch.Size([320])
encoder.stages.5.0.convs.0.all_modules.1.bias shape torch.Size([320])
encoder.stages.5.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.5.0.convs.1.conv.bias shape torch.Size([320])
encoder.stages.5.0.convs.1.norm.weight shape torch.Size([320])
encoder.stages.5.0.convs.1.norm.bias shape torch.Size([320])
encoder.stages.5.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.5.0.convs.1.all_modules.0.bias shape torch.Size([320])
encoder.stages.5.0.convs.1.all_modules.1.weight shape torch.Size([320])
encoder.stages.5.0.convs.1.all_modules.1.bias shape torch.Size([320])
decoder.encoder.stages.0.0.convs.0.conv.weight shape torch.Size([32, 1, 3, 3, 3])
decoder.encoder.stages.0.0.convs.0.conv.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.norm.weight shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.norm.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.all_modules.0.weight shape torch.Size([32, 1, 3, 3, 3])
decoder.encoder.stages.0.0.convs.0.all_modules.0.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.all_modules.1.weight shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.all_modules.1.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.conv.weight shape torch.Size([32, 32, 3, 3, 3])
decoder.encoder.stages.0.0.convs.1.conv.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.norm.weight shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.norm.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.all_modules.0.weight shape torch.Size([32, 32, 3, 3, 3])
decoder.encoder.stages.0.0.convs.1.all_modules.0.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.all_modules.1.weight shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.all_modules.1.bias shape torch.Size([32])
decoder.encoder.stages.1.0.convs.0.conv.weight shape torch.Size([64, 32, 3, 3, 3])
decoder.encoder.stages.1.0.convs.0.conv.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.norm.weight shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.norm.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.all_modules.0.weight shape torch.Size([64, 32, 3, 3, 3])
decoder.encoder.stages.1.0.convs.0.all_modules.0.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.all_modules.1.weight shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.all_modules.1.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.conv.weight shape torch.Size([64, 64, 3, 3, 3])
decoder.encoder.stages.1.0.convs.1.conv.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.norm.weight shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.norm.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.all_modules.0.weight shape torch.Size([64, 64, 3, 3, 3])
decoder.encoder.stages.1.0.convs.1.all_modules.0.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.all_modules.1.weight shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.all_modules.1.bias shape torch.Size([64])
decoder.encoder.stages.2.0.convs.0.conv.weight shape torch.Size([128, 64, 3, 3, 3])
decoder.encoder.stages.2.0.convs.0.conv.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.norm.weight shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.norm.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.all_modules.0.weight shape torch.Size([128, 64, 3, 3, 3])
decoder.encoder.stages.2.0.convs.0.all_modules.0.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.all_modules.1.weight shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.all_modules.1.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.conv.weight shape torch.Size([128, 128, 3, 3, 3])
decoder.encoder.stages.2.0.convs.1.conv.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.norm.weight shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.norm.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.all_modules.0.weight shape torch.Size([128, 128, 3, 3, 3])
decoder.encoder.stages.2.0.convs.1.all_modules.0.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.all_modules.1.weight shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.all_modules.1.bias shape torch.Size([128])
decoder.encoder.stages.3.0.convs.0.conv.weight shape torch.Size([256, 128, 3, 3, 3])
decoder.encoder.stages.3.0.convs.0.conv.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.norm.weight shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.norm.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.all_modules.0.weight shape torch.Size([256, 128, 3, 3, 3])
decoder.encoder.stages.3.0.convs.0.all_modules.0.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.all_modules.1.weight shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.all_modules.1.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.conv.weight shape torch.Size([256, 256, 3, 3, 3])
decoder.encoder.stages.3.0.convs.1.conv.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.norm.weight shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.norm.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.all_modules.0.weight shape torch.Size([256, 256, 3, 3, 3])
decoder.encoder.stages.3.0.convs.1.all_modules.0.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.all_modules.1.weight shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.all_modules.1.bias shape torch.Size([256])
decoder.encoder.stages.4.0.convs.0.conv.weight shape torch.Size([320, 256, 3, 3, 3])
decoder.encoder.stages.4.0.convs.0.conv.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.norm.weight shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.norm.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.all_modules.0.weight shape torch.Size([320, 256, 3, 3, 3])
decoder.encoder.stages.4.0.convs.0.all_modules.0.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.all_modules.1.weight shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.all_modules.1.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.4.0.convs.1.conv.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.norm.weight shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.norm.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.4.0.convs.1.all_modules.0.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.all_modules.1.weight shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.all_modules.1.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.conv.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.5.0.convs.0.conv.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.norm.weight shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.norm.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.5.0.convs.0.all_modules.0.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.all_modules.1.weight shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.all_modules.1.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.5.0.convs.1.conv.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.norm.weight shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.norm.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.5.0.convs.1.all_modules.0.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.all_modules.1.weight shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.all_modules.1.bias shape torch.Size([320])
decoder.stages.0.convs.0.conv.weight shape torch.Size([320, 640, 3, 3, 3])
decoder.stages.0.convs.0.conv.bias shape torch.Size([320])
decoder.stages.0.convs.0.norm.weight shape torch.Size([320])
decoder.stages.0.convs.0.norm.bias shape torch.Size([320])
decoder.stages.0.convs.0.all_modules.0.weight shape torch.Size([320, 640, 3, 3, 3])
decoder.stages.0.convs.0.all_modules.0.bias shape torch.Size([320])
decoder.stages.0.convs.0.all_modules.1.weight shape torch.Size([320])
decoder.stages.0.convs.0.all_modules.1.bias shape torch.Size([320])
decoder.stages.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.stages.0.convs.1.conv.bias shape torch.Size([320])
decoder.stages.0.convs.1.norm.weight shape torch.Size([320])
decoder.stages.0.convs.1.norm.bias shape torch.Size([320])
decoder.stages.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.stages.0.convs.1.all_modules.0.bias shape torch.Size([320])
decoder.stages.0.convs.1.all_modules.1.weight shape torch.Size([320])
decoder.stages.0.convs.1.all_modules.1.bias shape torch.Size([320])
decoder.stages.1.convs.0.conv.weight shape torch.Size([256, 512, 3, 3, 3])
decoder.stages.1.convs.0.conv.bias shape torch.Size([256])
decoder.stages.1.convs.0.norm.weight shape torch.Size([256])
decoder.stages.1.convs.0.norm.bias shape torch.Size([256])
decoder.stages.1.convs.0.all_modules.0.weight shape torch.Size([256, 512, 3, 3, 3])
decoder.stages.1.convs.0.all_modules.0.bias shape torch.Size([256])
decoder.stages.1.convs.0.all_modules.1.weight shape torch.Size([256])
decoder.stages.1.convs.0.all_modules.1.bias shape torch.Size([256])
decoder.stages.1.convs.1.conv.weight shape torch.Size([256, 256, 3, 3, 3])
decoder.stages.1.convs.1.conv.bias shape torch.Size([256])
decoder.stages.1.convs.1.norm.weight shape torch.Size([256])
decoder.stages.1.convs.1.norm.bias shape torch.Size([256])
decoder.stages.1.convs.1.all_modules.0.weight shape torch.Size([256, 256, 3, 3, 3])
decoder.stages.1.convs.1.all_modules.0.bias shape torch.Size([256])
decoder.stages.1.convs.1.all_modules.1.weight shape torch.Size([256])
decoder.stages.1.convs.1.all_modules.1.bias shape torch.Size([256])
decoder.stages.2.convs.0.conv.weight shape torch.Size([128, 256, 3, 3, 3])
decoder.stages.2.convs.0.conv.bias shape torch.Size([128])
decoder.stages.2.convs.0.norm.weight shape torch.Size([128])
decoder.stages.2.convs.0.norm.bias shape torch.Size([128])
decoder.stages.2.convs.0.all_modules.0.weight shape torch.Size([128, 256, 3, 3, 3])
decoder.stages.2.convs.0.all_modules.0.bias shape torch.Size([128])
decoder.stages.2.convs.0.all_modules.1.weight shape torch.Size([128])
decoder.stages.2.convs.0.all_modules.1.bias shape torch.Size([128])
decoder.stages.2.convs.1.conv.weight shape torch.Size([128, 128, 3, 3, 3])
decoder.stages.2.convs.1.conv.bias shape torch.Size([128])
decoder.stages.2.convs.1.norm.weight shape torch.Size([128])
decoder.stages.2.convs.1.norm.bias shape torch.Size([128])
decoder.stages.2.convs.1.all_modules.0.weight shape torch.Size([128, 128, 3, 3, 3])
decoder.stages.2.convs.1.all_modules.0.bias shape torch.Size([128])
decoder.stages.2.convs.1.all_modules.1.weight shape torch.Size([128])
decoder.stages.2.convs.1.all_modules.1.bias shape torch.Size([128])
decoder.stages.3.convs.0.conv.weight shape torch.Size([64, 128, 3, 3, 3])
decoder.stages.3.convs.0.conv.bias shape torch.Size([64])
decoder.stages.3.convs.0.norm.weight shape torch.Size([64])
decoder.stages.3.convs.0.norm.bias shape torch.Size([64])
decoder.stages.3.convs.0.all_modules.0.weight shape torch.Size([64, 128, 3, 3, 3])
decoder.stages.3.convs.0.all_modules.0.bias shape torch.Size([64])
decoder.stages.3.convs.0.all_modules.1.weight shape torch.Size([64])
decoder.stages.3.convs.0.all_modules.1.bias shape torch.Size([64])
decoder.stages.3.convs.1.conv.weight shape torch.Size([64, 64, 3, 3, 3])
decoder.stages.3.convs.1.conv.bias shape torch.Size([64])
decoder.stages.3.convs.1.norm.weight shape torch.Size([64])
decoder.stages.3.convs.1.norm.bias shape torch.Size([64])
decoder.stages.3.convs.1.all_modules.0.weight shape torch.Size([64, 64, 3, 3, 3])
decoder.stages.3.convs.1.all_modules.0.bias shape torch.Size([64])
decoder.stages.3.convs.1.all_modules.1.weight shape torch.Size([64])
decoder.stages.3.convs.1.all_modules.1.bias shape torch.Size([64])
decoder.stages.4.convs.0.conv.weight shape torch.Size([32, 64, 3, 3, 3])
decoder.stages.4.convs.0.conv.bias shape torch.Size([32])
decoder.stages.4.convs.0.norm.weight shape torch.Size([32])
decoder.stages.4.convs.0.norm.bias shape torch.Size([32])
decoder.stages.4.convs.0.all_modules.0.weight shape torch.Size([32, 64, 3, 3, 3])
decoder.stages.4.convs.0.all_modules.0.bias shape torch.Size([32])
decoder.stages.4.convs.0.all_modules.1.weight shape torch.Size([32])
decoder.stages.4.convs.0.all_modules.1.bias shape torch.Size([32])
decoder.stages.4.convs.1.conv.weight shape torch.Size([32, 32, 3, 3, 3])
decoder.stages.4.convs.1.conv.bias shape torch.Size([32])
decoder.stages.4.convs.1.norm.weight shape torch.Size([32])
decoder.stages.4.convs.1.norm.bias shape torch.Size([32])
decoder.stages.4.convs.1.all_modules.0.weight shape torch.Size([32, 32, 3, 3, 3])
decoder.stages.4.convs.1.all_modules.0.bias shape torch.Size([32])
decoder.stages.4.convs.1.all_modules.1.weight shape torch.Size([32])
decoder.stages.4.convs.1.all_modules.1.bias shape torch.Size([32])
decoder.transpconvs.0.weight shape torch.Size([320, 320, 1, 2, 2])
decoder.transpconvs.0.bias shape torch.Size([320])
decoder.transpconvs.1.weight shape torch.Size([320, 256, 2, 2, 2])
decoder.transpconvs.1.bias shape torch.Size([256])
decoder.transpconvs.2.weight shape torch.Size([256, 128, 2, 2, 2])
decoder.transpconvs.2.bias shape torch.Size([128])
decoder.transpconvs.3.weight shape torch.Size([128, 64, 2, 2, 2])
decoder.transpconvs.3.bias shape torch.Size([64])
decoder.transpconvs.4.weight shape torch.Size([64, 32, 2, 2, 2])
decoder.transpconvs.4.bias shape torch.Size([32])
################### Done ###################
2024-12-09 01:36:10.274352: do_dummy_2d_data_aug: True
2024-12-09 01:36:10.277733: Using splits from existing split file: /home/gridsan/nchutisilp/datasets/nnUNet_Datasets/nnUNet_preprocessed/Dataset308_Sohee_Calcium_OCT_CrossValidation/splits_final.json
2024-12-09 01:36:10.280506: The split file contains 5 splits.
2024-12-09 01:36:10.281901: Desired fold for training: 4
2024-12-09 01:36:10.283193: This split has 7 training and 1 validation cases.
using pin_memory on device 0
using pin_memory on device 0

This is the configuration used by this training:
Configuration name: 3d_32x160x128_b10
 {'data_identifier': 'nnUNetPlans_3d_fullres', 'preprocessor_name': 'DefaultPreprocessor', 'batch_size': 10, 'patch_size': [32, 160, 128], 'median_image_size_in_voxels': [375.0, 498.0, 498.0], 'spacing': [1.0, 1.0, 1.0], 'normalization_schemes': ['ZScoreNormalization'], 'use_mask_for_norm': [False], 'resampling_fn_data': 'resample_data_or_seg_to_shape', 'resampling_fn_seg': 'resample_data_or_seg_to_shape', 'resampling_fn_data_kwargs': {'is_seg': False, 'order': 3, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_seg_kwargs': {'is_seg': True, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_probabilities': 'resample_data_or_seg_to_shape', 'resampling_fn_probabilities_kwargs': {'is_seg': False, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'architecture': {'network_class_name': 'dynamic_network_architectures.architectures.unet.PlainConvUNet', 'arch_kwargs': {'n_stages': 6, 'features_per_stage': [32, 64, 128, 256, 320, 320], 'conv_op': 'torch.nn.modules.conv.Conv3d', 'kernel_sizes': [[3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3]], 'strides': [[1, 1, 1], [2, 2, 2], [2, 2, 2], [2, 2, 2], [2, 2, 2], [1, 2, 2]], 'n_conv_per_stage': [2, 2, 2, 2, 2, 2], 'n_conv_per_stage_decoder': [2, 2, 2, 2, 2], 'conv_bias': True, 'norm_op': 'torch.nn.modules.instancenorm.InstanceNorm3d', 'norm_op_kwargs': {'eps': 1e-05, 'affine': True}, 'dropout_op': None, 'dropout_op_kwargs': None, 'nonlin': 'torch.nn.LeakyReLU', 'nonlin_kwargs': {'inplace': True}, 'deep_supervision': True}, '_kw_requires_import': ['conv_op', 'norm_op', 'dropout_op', 'nonlin']}, 'batch_dice': True, 'inherits_from': '3d_fullres'} 

These are the global plan.json settings:
 {'dataset_name': 'Dataset308_Sohee_Calcium_OCT_CrossValidation', 'plans_name': 'nnUNetPlans', 'original_median_spacing_after_transp': [1.0, 1.0, 1.0], 'original_median_shape_after_transp': [375, 498, 498], 'image_reader_writer': 'SimpleITKIO', 'transpose_forward': [0, 1, 2], 'transpose_backward': [0, 1, 2], 'experiment_planner_used': 'ExperimentPlanner', 'label_manager': 'LabelManager', 'foreground_intensity_properties_per_channel': {'0': {'max': 0.4977976381778717, 'mean': 0.13507525622844696, 'median': 0.09599608182907104, 'min': 0.0, 'percentile_00_5': 0.014754901640117168, 'percentile_99_5': 0.4977976381778717, 'std': 0.12025152146816254}}} 

2024-12-09 01:36:13.036567: unpacking dataset...
2024-12-09 01:36:17.231385: unpacking done...
2024-12-09 01:36:17.533541: Unable to plot network architecture: nnUNet_compile is enabled!
2024-12-09 01:36:17.580072: 
2024-12-09 01:36:17.581373: Epoch 0
2024-12-09 01:36:17.583030: Current learning rate: 0.01
2024-12-09 01:39:00.239429: Validation loss improved from 1000.00000 to -0.41901! Patience: 0/50
2024-12-09 01:39:00.241129: train_loss -0.334
2024-12-09 01:39:00.242201: val_loss -0.419
2024-12-09 01:39:00.242958: Pseudo dice [0.6651]
2024-12-09 01:39:00.243855: Epoch time: 162.66 s
2024-12-09 01:39:00.244776: Yayy! New best EMA pseudo Dice: 0.6651
2024-12-09 01:39:02.085072: 
2024-12-09 01:39:02.087216: Epoch 1
2024-12-09 01:39:02.088509: Current learning rate: 0.00999
2024-12-09 01:40:28.879111: Validation loss did not improve from -0.41901. Patience: 1/50
2024-12-09 01:40:28.880457: train_loss -0.4758
2024-12-09 01:40:28.881516: val_loss -0.4118
2024-12-09 01:40:28.882276: Pseudo dice [0.671]
2024-12-09 01:40:28.883020: Epoch time: 86.8 s
2024-12-09 01:40:28.883701: Yayy! New best EMA pseudo Dice: 0.6657
2024-12-09 01:40:30.354868: 
2024-12-09 01:40:30.357114: Epoch 2
2024-12-09 01:40:30.358038: Current learning rate: 0.00998
2024-12-09 01:41:57.856358: Validation loss improved from -0.41901 to -0.42522! Patience: 1/50
2024-12-09 01:41:57.857765: train_loss -0.5217
2024-12-09 01:41:57.859086: val_loss -0.4252
2024-12-09 01:41:57.860263: Pseudo dice [0.6804]
2024-12-09 01:41:57.861275: Epoch time: 87.5 s
2024-12-09 01:41:57.862105: Yayy! New best EMA pseudo Dice: 0.6672
2024-12-09 01:41:59.414614: 
2024-12-09 01:41:59.416508: Epoch 3
2024-12-09 01:41:59.417618: Current learning rate: 0.00997
2024-12-09 01:43:26.996920: Validation loss improved from -0.42522 to -0.44767! Patience: 0/50
2024-12-09 01:43:26.998066: train_loss -0.5254
2024-12-09 01:43:26.999460: val_loss -0.4477
2024-12-09 01:43:27.000579: Pseudo dice [0.69]
2024-12-09 01:43:27.001908: Epoch time: 87.58 s
2024-12-09 01:43:27.002620: Yayy! New best EMA pseudo Dice: 0.6695
2024-12-09 01:43:28.507702: 
2024-12-09 01:43:28.509542: Epoch 4
2024-12-09 01:43:28.510858: Current learning rate: 0.00996
2024-12-09 01:44:56.075130: Validation loss improved from -0.44767 to -0.45268! Patience: 0/50
2024-12-09 01:44:56.076329: train_loss -0.5451
2024-12-09 01:44:56.077525: val_loss -0.4527
2024-12-09 01:44:56.078459: Pseudo dice [0.693]
2024-12-09 01:44:56.079332: Epoch time: 87.57 s
2024-12-09 01:44:56.365755: Yayy! New best EMA pseudo Dice: 0.6718
2024-12-09 01:44:57.923639: 
2024-12-09 01:44:57.925942: Epoch 5
2024-12-09 01:44:57.927135: Current learning rate: 0.00995
2024-12-09 01:46:25.410525: Validation loss did not improve from -0.45268. Patience: 1/50
2024-12-09 01:46:25.411544: train_loss -0.5568
2024-12-09 01:46:25.412622: val_loss -0.4035
2024-12-09 01:46:25.413815: Pseudo dice [0.6706]
2024-12-09 01:46:25.414794: Epoch time: 87.49 s
2024-12-09 01:46:26.590112: 
2024-12-09 01:46:26.591969: Epoch 6
2024-12-09 01:46:26.593233: Current learning rate: 0.00995
2024-12-09 01:47:54.140628: Validation loss did not improve from -0.45268. Patience: 2/50
2024-12-09 01:47:54.141859: train_loss -0.5642
2024-12-09 01:47:54.143318: val_loss -0.4453
2024-12-09 01:47:54.144126: Pseudo dice [0.7019]
2024-12-09 01:47:54.145094: Epoch time: 87.55 s
2024-12-09 01:47:54.145761: Yayy! New best EMA pseudo Dice: 0.6747
2024-12-09 01:47:55.640949: 
2024-12-09 01:47:55.643062: Epoch 7
2024-12-09 01:47:55.644119: Current learning rate: 0.00994
2024-12-09 01:49:23.259073: Validation loss improved from -0.45268 to -0.46457! Patience: 2/50
2024-12-09 01:49:23.260302: train_loss -0.5779
2024-12-09 01:49:23.261833: val_loss -0.4646
2024-12-09 01:49:23.263004: Pseudo dice [0.7058]
2024-12-09 01:49:23.263930: Epoch time: 87.62 s
2024-12-09 01:49:23.264947: Yayy! New best EMA pseudo Dice: 0.6778
2024-12-09 01:49:25.189888: 
2024-12-09 01:49:25.192150: Epoch 8
2024-12-09 01:49:25.193295: Current learning rate: 0.00993
2024-12-09 01:50:53.028207: Validation loss improved from -0.46457 to -0.48014! Patience: 0/50
2024-12-09 01:50:53.029342: train_loss -0.5941
2024-12-09 01:50:53.030527: val_loss -0.4801
2024-12-09 01:50:53.031462: Pseudo dice [0.7149]
2024-12-09 01:50:53.032425: Epoch time: 87.84 s
2024-12-09 01:50:53.033204: Yayy! New best EMA pseudo Dice: 0.6815
2024-12-09 01:50:54.555475: 
2024-12-09 01:50:54.557391: Epoch 9
2024-12-09 01:50:54.558804: Current learning rate: 0.00992
2024-12-09 01:52:22.459888: Validation loss did not improve from -0.48014. Patience: 1/50
2024-12-09 01:52:22.460940: train_loss -0.5879
2024-12-09 01:52:22.461912: val_loss -0.4607
2024-12-09 01:52:22.462876: Pseudo dice [0.7038]
2024-12-09 01:52:22.463589: Epoch time: 87.91 s
2024-12-09 01:52:22.804079: Yayy! New best EMA pseudo Dice: 0.6837
2024-12-09 01:52:24.278236: 
2024-12-09 01:52:24.280109: Epoch 10
2024-12-09 01:52:24.281156: Current learning rate: 0.00991
2024-12-09 01:53:52.234666: Validation loss did not improve from -0.48014. Patience: 2/50
2024-12-09 01:53:52.235982: train_loss -0.5998
2024-12-09 01:53:52.236965: val_loss -0.449
2024-12-09 01:53:52.237776: Pseudo dice [0.7055]
2024-12-09 01:53:52.238615: Epoch time: 87.96 s
2024-12-09 01:53:52.239645: Yayy! New best EMA pseudo Dice: 0.6859
2024-12-09 01:53:53.742175: 
2024-12-09 01:53:53.744297: Epoch 11
2024-12-09 01:53:53.745746: Current learning rate: 0.0099
2024-12-09 01:55:21.715492: Validation loss did not improve from -0.48014. Patience: 3/50
2024-12-09 01:55:21.716575: train_loss -0.612
2024-12-09 01:55:21.717720: val_loss -0.478
2024-12-09 01:55:21.718462: Pseudo dice [0.7124]
2024-12-09 01:55:21.719534: Epoch time: 87.98 s
2024-12-09 01:55:21.720544: Yayy! New best EMA pseudo Dice: 0.6886
2024-12-09 01:55:23.209477: 
2024-12-09 01:55:23.211300: Epoch 12
2024-12-09 01:55:23.212344: Current learning rate: 0.00989
2024-12-09 01:56:51.223453: Validation loss did not improve from -0.48014. Patience: 4/50
2024-12-09 01:56:51.224635: train_loss -0.5994
2024-12-09 01:56:51.226044: val_loss -0.4443
2024-12-09 01:56:51.227090: Pseudo dice [0.6949]
2024-12-09 01:56:51.228028: Epoch time: 88.02 s
2024-12-09 01:56:51.228889: Yayy! New best EMA pseudo Dice: 0.6892
2024-12-09 01:56:52.750340: 
2024-12-09 01:56:52.752195: Epoch 13
2024-12-09 01:56:52.753246: Current learning rate: 0.00988
2024-12-09 01:58:20.712729: Validation loss improved from -0.48014 to -0.48962! Patience: 4/50
2024-12-09 01:58:20.714386: train_loss -0.6149
2024-12-09 01:58:20.716033: val_loss -0.4896
2024-12-09 01:58:20.717080: Pseudo dice [0.7175]
2024-12-09 01:58:20.718088: Epoch time: 87.97 s
2024-12-09 01:58:20.719139: Yayy! New best EMA pseudo Dice: 0.692
2024-12-09 01:58:22.231017: 
2024-12-09 01:58:22.233399: Epoch 14
2024-12-09 01:58:22.234828: Current learning rate: 0.00987
2024-12-09 01:59:50.099694: Validation loss did not improve from -0.48962. Patience: 1/50
2024-12-09 01:59:50.102161: train_loss -0.6201
2024-12-09 01:59:50.103666: val_loss -0.4418
2024-12-09 01:59:50.104612: Pseudo dice [0.6847]
2024-12-09 01:59:50.105468: Epoch time: 87.87 s
2024-12-09 01:59:51.610326: 
2024-12-09 01:59:51.611782: Epoch 15
2024-12-09 01:59:51.612950: Current learning rate: 0.00986
2024-12-09 02:01:21.608378: Validation loss did not improve from -0.48962. Patience: 2/50
2024-12-09 02:01:21.610225: train_loss -0.6263
2024-12-09 02:01:21.612329: val_loss -0.4563
2024-12-09 02:01:21.613589: Pseudo dice [0.6951]
2024-12-09 02:01:21.614856: Epoch time: 90.0 s
2024-12-09 02:01:22.816873: 
2024-12-09 02:01:22.819235: Epoch 16
2024-12-09 02:01:22.820723: Current learning rate: 0.00986
2024-12-09 02:02:51.009025: Validation loss improved from -0.48962 to -0.49329! Patience: 2/50
2024-12-09 02:02:51.010138: train_loss -0.6209
2024-12-09 02:02:51.011151: val_loss -0.4933
2024-12-09 02:02:51.012170: Pseudo dice [0.7232]
2024-12-09 02:02:51.013082: Epoch time: 88.19 s
2024-12-09 02:02:51.013880: Yayy! New best EMA pseudo Dice: 0.6948
2024-12-09 02:02:52.545267: 
2024-12-09 02:02:52.547623: Epoch 17
2024-12-09 02:02:52.548718: Current learning rate: 0.00985
2024-12-09 02:04:20.715879: Validation loss improved from -0.49329 to -0.49541! Patience: 0/50
2024-12-09 02:04:20.717186: train_loss -0.6293
2024-12-09 02:04:20.718716: val_loss -0.4954
2024-12-09 02:04:20.720226: Pseudo dice [0.7169]
2024-12-09 02:04:20.721400: Epoch time: 88.17 s
2024-12-09 02:04:20.722552: Yayy! New best EMA pseudo Dice: 0.697
2024-12-09 02:04:22.591954: 
2024-12-09 02:04:22.594160: Epoch 18
2024-12-09 02:04:22.595287: Current learning rate: 0.00984
2024-12-09 02:05:50.796138: Validation loss did not improve from -0.49541. Patience: 1/50
2024-12-09 02:05:50.797577: train_loss -0.6437
2024-12-09 02:05:50.798595: val_loss -0.4689
2024-12-09 02:05:50.799491: Pseudo dice [0.7046]
2024-12-09 02:05:50.800291: Epoch time: 88.21 s
2024-12-09 02:05:50.801135: Yayy! New best EMA pseudo Dice: 0.6978
2024-12-09 02:05:52.350542: 
2024-12-09 02:05:52.352376: Epoch 19
2024-12-09 02:05:52.353562: Current learning rate: 0.00983
2024-12-09 02:07:20.632896: Validation loss did not improve from -0.49541. Patience: 2/50
2024-12-09 02:07:20.634195: train_loss -0.6382
2024-12-09 02:07:20.635621: val_loss -0.4379
2024-12-09 02:07:20.636824: Pseudo dice [0.6962]
2024-12-09 02:07:20.637995: Epoch time: 88.28 s
2024-12-09 02:07:22.222014: 
2024-12-09 02:07:22.224311: Epoch 20
2024-12-09 02:07:22.225747: Current learning rate: 0.00982
2024-12-09 02:08:50.464938: Validation loss did not improve from -0.49541. Patience: 3/50
2024-12-09 02:08:50.466146: train_loss -0.6345
2024-12-09 02:08:50.467351: val_loss -0.461
2024-12-09 02:08:50.468280: Pseudo dice [0.6988]
2024-12-09 02:08:50.468999: Epoch time: 88.25 s
2024-12-09 02:08:51.746738: 
2024-12-09 02:08:51.748645: Epoch 21
2024-12-09 02:08:51.749909: Current learning rate: 0.00981
2024-12-09 02:10:19.850696: Validation loss did not improve from -0.49541. Patience: 4/50
2024-12-09 02:10:19.851754: train_loss -0.654
2024-12-09 02:10:19.852835: val_loss -0.4872
2024-12-09 02:10:19.853625: Pseudo dice [0.7272]
2024-12-09 02:10:19.854658: Epoch time: 88.11 s
2024-12-09 02:10:19.855656: Yayy! New best EMA pseudo Dice: 0.7007
2024-12-09 02:10:21.423068: 
2024-12-09 02:10:21.425052: Epoch 22
2024-12-09 02:10:21.426204: Current learning rate: 0.0098
2024-12-09 02:11:49.786729: Validation loss did not improve from -0.49541. Patience: 5/50
2024-12-09 02:11:49.788156: train_loss -0.6436
2024-12-09 02:11:49.789506: val_loss -0.4553
2024-12-09 02:11:49.790747: Pseudo dice [0.7058]
2024-12-09 02:11:49.791858: Epoch time: 88.37 s
2024-12-09 02:11:49.792954: Yayy! New best EMA pseudo Dice: 0.7012
2024-12-09 02:11:51.291853: 
2024-12-09 02:11:51.293533: Epoch 23
2024-12-09 02:11:51.294551: Current learning rate: 0.00979
2024-12-09 02:13:19.567345: Validation loss did not improve from -0.49541. Patience: 6/50
2024-12-09 02:13:19.568390: train_loss -0.6457
2024-12-09 02:13:19.569286: val_loss -0.4914
2024-12-09 02:13:19.570324: Pseudo dice [0.7181]
2024-12-09 02:13:19.571356: Epoch time: 88.28 s
2024-12-09 02:13:19.572299: Yayy! New best EMA pseudo Dice: 0.7029
2024-12-09 02:13:21.085575: 
2024-12-09 02:13:21.087869: Epoch 24
2024-12-09 02:13:21.088942: Current learning rate: 0.00978
2024-12-09 02:14:49.397752: Validation loss did not improve from -0.49541. Patience: 7/50
2024-12-09 02:14:49.399078: train_loss -0.6522
2024-12-09 02:14:49.399921: val_loss -0.4914
2024-12-09 02:14:49.400893: Pseudo dice [0.7205]
2024-12-09 02:14:49.401821: Epoch time: 88.31 s
2024-12-09 02:14:49.750401: Yayy! New best EMA pseudo Dice: 0.7047
2024-12-09 02:14:51.274081: 
2024-12-09 02:14:51.276090: Epoch 25
2024-12-09 02:14:51.277302: Current learning rate: 0.00977
2024-12-09 02:16:19.486916: Validation loss improved from -0.49541 to -0.50605! Patience: 7/50
2024-12-09 02:16:19.488134: train_loss -0.6578
2024-12-09 02:16:19.489104: val_loss -0.506
2024-12-09 02:16:19.490157: Pseudo dice [0.7231]
2024-12-09 02:16:19.490989: Epoch time: 88.22 s
2024-12-09 02:16:19.491895: Yayy! New best EMA pseudo Dice: 0.7065
2024-12-09 02:16:21.024487: 
2024-12-09 02:16:21.027067: Epoch 26
2024-12-09 02:16:21.028027: Current learning rate: 0.00977
2024-12-09 02:17:49.167334: Validation loss did not improve from -0.50605. Patience: 1/50
2024-12-09 02:17:49.168423: train_loss -0.6427
2024-12-09 02:17:49.169940: val_loss -0.4913
2024-12-09 02:17:49.171081: Pseudo dice [0.7184]
2024-12-09 02:17:49.171994: Epoch time: 88.14 s
2024-12-09 02:17:49.172870: Yayy! New best EMA pseudo Dice: 0.7077
2024-12-09 02:17:50.688814: 
2024-12-09 02:17:50.690533: Epoch 27
2024-12-09 02:17:50.691857: Current learning rate: 0.00976
2024-12-09 02:19:18.822711: Validation loss did not improve from -0.50605. Patience: 2/50
2024-12-09 02:19:18.824100: train_loss -0.6624
2024-12-09 02:19:18.825796: val_loss -0.4697
2024-12-09 02:19:18.826875: Pseudo dice [0.7089]
2024-12-09 02:19:18.827750: Epoch time: 88.14 s
2024-12-09 02:19:18.828731: Yayy! New best EMA pseudo Dice: 0.7078
2024-12-09 02:19:20.364094: 
2024-12-09 02:19:20.365880: Epoch 28
2024-12-09 02:19:20.366974: Current learning rate: 0.00975
2024-12-09 02:20:48.524018: Validation loss did not improve from -0.50605. Patience: 3/50
2024-12-09 02:20:48.525458: train_loss -0.6616
2024-12-09 02:20:48.527074: val_loss -0.4923
2024-12-09 02:20:48.528430: Pseudo dice [0.7202]
2024-12-09 02:20:48.529468: Epoch time: 88.16 s
2024-12-09 02:20:48.530709: Yayy! New best EMA pseudo Dice: 0.709
2024-12-09 02:20:50.645795: 
2024-12-09 02:20:50.648358: Epoch 29
2024-12-09 02:20:50.649714: Current learning rate: 0.00974
2024-12-09 02:22:18.867575: Validation loss did not improve from -0.50605. Patience: 4/50
2024-12-09 02:22:18.868705: train_loss -0.6567
2024-12-09 02:22:18.869961: val_loss -0.484
2024-12-09 02:22:18.870985: Pseudo dice [0.721]
2024-12-09 02:22:18.872017: Epoch time: 88.22 s
2024-12-09 02:22:19.219934: Yayy! New best EMA pseudo Dice: 0.7102
2024-12-09 02:22:20.780477: 
2024-12-09 02:22:20.782329: Epoch 30
2024-12-09 02:22:20.783711: Current learning rate: 0.00973
2024-12-09 02:23:48.936442: Validation loss did not improve from -0.50605. Patience: 5/50
2024-12-09 02:23:48.939141: train_loss -0.6585
2024-12-09 02:23:48.940822: val_loss -0.4814
2024-12-09 02:23:48.941708: Pseudo dice [0.7066]
2024-12-09 02:23:48.942664: Epoch time: 88.16 s
2024-12-09 02:23:50.182233: 
2024-12-09 02:23:50.184349: Epoch 31
2024-12-09 02:23:50.185386: Current learning rate: 0.00972
2024-12-09 02:25:18.384139: Validation loss did not improve from -0.50605. Patience: 6/50
2024-12-09 02:25:18.385258: train_loss -0.6706
2024-12-09 02:25:18.386772: val_loss -0.4708
2024-12-09 02:25:18.387726: Pseudo dice [0.7118]
2024-12-09 02:25:18.388773: Epoch time: 88.2 s
2024-12-09 02:25:19.609424: 
2024-12-09 02:25:19.611767: Epoch 32
2024-12-09 02:25:19.613017: Current learning rate: 0.00971
2024-12-09 02:26:47.947082: Validation loss did not improve from -0.50605. Patience: 7/50
2024-12-09 02:26:47.948245: train_loss -0.6807
2024-12-09 02:26:47.949455: val_loss -0.4998
2024-12-09 02:26:47.950917: Pseudo dice [0.7268]
2024-12-09 02:26:47.952217: Epoch time: 88.34 s
2024-12-09 02:26:47.953160: Yayy! New best EMA pseudo Dice: 0.7117
2024-12-09 02:26:49.508298: 
2024-12-09 02:26:49.510324: Epoch 33
2024-12-09 02:26:49.511731: Current learning rate: 0.0097
2024-12-09 02:28:17.771658: Validation loss did not improve from -0.50605. Patience: 8/50
2024-12-09 02:28:17.772852: train_loss -0.6788
2024-12-09 02:28:17.773876: val_loss -0.4839
2024-12-09 02:28:17.774837: Pseudo dice [0.7098]
2024-12-09 02:28:17.775784: Epoch time: 88.27 s
2024-12-09 02:28:18.999400: 
2024-12-09 02:28:19.001654: Epoch 34
2024-12-09 02:28:19.002594: Current learning rate: 0.00969
2024-12-09 02:29:47.323991: Validation loss did not improve from -0.50605. Patience: 9/50
2024-12-09 02:29:47.325296: train_loss -0.6722
2024-12-09 02:29:47.326715: val_loss -0.4505
2024-12-09 02:29:47.327582: Pseudo dice [0.7037]
2024-12-09 02:29:47.328391: Epoch time: 88.33 s
2024-12-09 02:29:48.944753: 
2024-12-09 02:29:48.947353: Epoch 35
2024-12-09 02:29:48.948354: Current learning rate: 0.00968
2024-12-09 02:31:17.256130: Validation loss did not improve from -0.50605. Patience: 10/50
2024-12-09 02:31:17.257275: train_loss -0.6743
2024-12-09 02:31:17.258257: val_loss -0.4622
2024-12-09 02:31:17.259033: Pseudo dice [0.7044]
2024-12-09 02:31:17.260163: Epoch time: 88.31 s
2024-12-09 02:31:18.500065: 
2024-12-09 02:31:18.501596: Epoch 36
2024-12-09 02:31:18.502549: Current learning rate: 0.00968
2024-12-09 02:32:46.847818: Validation loss improved from -0.50605 to -0.51102! Patience: 10/50
2024-12-09 02:32:46.849097: train_loss -0.6716
2024-12-09 02:32:46.850466: val_loss -0.511
2024-12-09 02:32:46.851489: Pseudo dice [0.7326]
2024-12-09 02:32:46.852458: Epoch time: 88.35 s
2024-12-09 02:32:46.853348: Yayy! New best EMA pseudo Dice: 0.7124
2024-12-09 02:32:48.429013: 
2024-12-09 02:32:48.430637: Epoch 37
2024-12-09 02:32:48.431524: Current learning rate: 0.00967
2024-12-09 02:34:16.926666: Validation loss did not improve from -0.51102. Patience: 1/50
2024-12-09 02:34:16.928033: train_loss -0.6754
2024-12-09 02:34:16.929456: val_loss -0.445
2024-12-09 02:34:16.930457: Pseudo dice [0.6979]
2024-12-09 02:34:16.931318: Epoch time: 88.5 s
2024-12-09 02:34:18.176182: 
2024-12-09 02:34:18.177948: Epoch 38
2024-12-09 02:34:18.178903: Current learning rate: 0.00966
2024-12-09 02:35:46.619233: Validation loss did not improve from -0.51102. Patience: 2/50
2024-12-09 02:35:46.620726: train_loss -0.687
2024-12-09 02:35:46.621631: val_loss -0.4953
2024-12-09 02:35:46.622565: Pseudo dice [0.7313]
2024-12-09 02:35:46.623545: Epoch time: 88.45 s
2024-12-09 02:35:46.624670: Yayy! New best EMA pseudo Dice: 0.713
2024-12-09 02:35:48.569877: 
2024-12-09 02:35:48.572029: Epoch 39
2024-12-09 02:35:48.573113: Current learning rate: 0.00965
2024-12-09 02:37:17.011712: Validation loss did not improve from -0.51102. Patience: 3/50
2024-12-09 02:37:17.012927: train_loss -0.6795
2024-12-09 02:37:17.013815: val_loss -0.4911
2024-12-09 02:37:17.014728: Pseudo dice [0.7196]
2024-12-09 02:37:17.015683: Epoch time: 88.44 s
2024-12-09 02:37:17.361426: Yayy! New best EMA pseudo Dice: 0.7136
2024-12-09 02:37:18.968911: 
2024-12-09 02:37:18.971764: Epoch 40
2024-12-09 02:37:18.972930: Current learning rate: 0.00964
2024-12-09 02:38:47.284713: Validation loss improved from -0.51102 to -0.52144! Patience: 3/50
2024-12-09 02:38:47.286075: train_loss -0.6835
2024-12-09 02:38:47.287411: val_loss -0.5214
2024-12-09 02:38:47.288211: Pseudo dice [0.735]
2024-12-09 02:38:47.289104: Epoch time: 88.32 s
2024-12-09 02:38:47.289705: Yayy! New best EMA pseudo Dice: 0.7158
2024-12-09 02:38:48.956041: 
2024-12-09 02:38:48.958675: Epoch 41
2024-12-09 02:38:48.960458: Current learning rate: 0.00963
2024-12-09 02:40:17.269984: Validation loss improved from -0.52144 to -0.52678! Patience: 0/50
2024-12-09 02:40:17.272629: train_loss -0.6942
2024-12-09 02:40:17.274109: val_loss -0.5268
2024-12-09 02:40:17.275270: Pseudo dice [0.7298]
2024-12-09 02:40:17.276258: Epoch time: 88.32 s
2024-12-09 02:40:17.277241: Yayy! New best EMA pseudo Dice: 0.7172
2024-12-09 02:40:18.895279: 
2024-12-09 02:40:18.898752: Epoch 42
2024-12-09 02:40:18.900531: Current learning rate: 0.00962
2024-12-09 02:41:47.461482: Validation loss did not improve from -0.52678. Patience: 1/50
2024-12-09 02:41:47.465149: train_loss -0.682
2024-12-09 02:41:47.466875: val_loss -0.4781
2024-12-09 02:41:47.467673: Pseudo dice [0.7129]
2024-12-09 02:41:47.468484: Epoch time: 88.57 s
2024-12-09 02:41:48.666280: 
2024-12-09 02:41:48.668663: Epoch 43
2024-12-09 02:41:48.669788: Current learning rate: 0.00961
2024-12-09 02:43:17.240879: Validation loss did not improve from -0.52678. Patience: 2/50
2024-12-09 02:43:17.243289: train_loss -0.6804
2024-12-09 02:43:17.246394: val_loss -0.4491
2024-12-09 02:43:17.247406: Pseudo dice [0.7057]
2024-12-09 02:43:17.248840: Epoch time: 88.58 s
2024-12-09 02:43:18.518959: 
2024-12-09 02:43:18.521128: Epoch 44
2024-12-09 02:43:18.522147: Current learning rate: 0.0096
2024-12-09 02:44:47.025200: Validation loss did not improve from -0.52678. Patience: 3/50
2024-12-09 02:44:47.026322: train_loss -0.6865
2024-12-09 02:44:47.027502: val_loss -0.4763
2024-12-09 02:44:47.028812: Pseudo dice [0.7193]
2024-12-09 02:44:47.029934: Epoch time: 88.51 s
2024-12-09 02:44:48.607484: 
2024-12-09 02:44:48.609750: Epoch 45
2024-12-09 02:44:48.611043: Current learning rate: 0.00959
2024-12-09 02:46:17.113506: Validation loss did not improve from -0.52678. Patience: 4/50
2024-12-09 02:46:17.114962: train_loss -0.69
2024-12-09 02:46:17.116552: val_loss -0.4675
2024-12-09 02:46:17.117423: Pseudo dice [0.6984]
2024-12-09 02:46:17.118227: Epoch time: 88.51 s
2024-12-09 02:46:18.302290: 
2024-12-09 02:46:18.304341: Epoch 46
2024-12-09 02:46:18.305660: Current learning rate: 0.00959
2024-12-09 02:47:46.774741: Validation loss did not improve from -0.52678. Patience: 5/50
2024-12-09 02:47:46.776258: train_loss -0.6864
2024-12-09 02:47:46.777527: val_loss -0.4765
2024-12-09 02:47:46.778582: Pseudo dice [0.7068]
2024-12-09 02:47:46.779585: Epoch time: 88.48 s
2024-12-09 02:47:47.943721: 
2024-12-09 02:47:47.946033: Epoch 47
2024-12-09 02:47:47.947123: Current learning rate: 0.00958
2024-12-09 02:49:16.445895: Validation loss did not improve from -0.52678. Patience: 6/50
2024-12-09 02:49:16.447409: train_loss -0.6836
2024-12-09 02:49:16.448752: val_loss -0.4838
2024-12-09 02:49:16.449443: Pseudo dice [0.7165]
2024-12-09 02:49:16.450392: Epoch time: 88.51 s
2024-12-09 02:49:17.618554: 
2024-12-09 02:49:17.620488: Epoch 48
2024-12-09 02:49:17.621727: Current learning rate: 0.00957
2024-12-09 02:50:46.080291: Validation loss did not improve from -0.52678. Patience: 7/50
2024-12-09 02:50:46.081296: train_loss -0.6916
2024-12-09 02:50:46.082146: val_loss -0.4875
2024-12-09 02:50:46.082944: Pseudo dice [0.7258]
2024-12-09 02:50:46.083760: Epoch time: 88.46 s
2024-12-09 02:50:47.259687: 
2024-12-09 02:50:47.261596: Epoch 49
2024-12-09 02:50:47.262801: Current learning rate: 0.00956
2024-12-09 02:52:15.909490: Validation loss did not improve from -0.52678. Patience: 8/50
2024-12-09 02:52:15.910515: train_loss -0.6847
2024-12-09 02:52:15.911829: val_loss -0.493
2024-12-09 02:52:15.912651: Pseudo dice [0.7138]
2024-12-09 02:52:15.913533: Epoch time: 88.65 s
2024-12-09 02:52:18.380745: 
2024-12-09 02:52:18.382350: Epoch 50
2024-12-09 02:52:18.383345: Current learning rate: 0.00955
2024-12-09 02:53:47.089450: Validation loss did not improve from -0.52678. Patience: 9/50
2024-12-09 02:53:47.090777: train_loss -0.6929
2024-12-09 02:53:47.092006: val_loss -0.4992
2024-12-09 02:53:47.093039: Pseudo dice [0.7274]
2024-12-09 02:53:47.093789: Epoch time: 88.71 s
2024-12-09 02:53:48.280000: 
2024-12-09 02:53:48.281462: Epoch 51
2024-12-09 02:53:48.282692: Current learning rate: 0.00954
2024-12-09 02:55:16.987763: Validation loss improved from -0.52678 to -0.53544! Patience: 9/50
2024-12-09 02:55:16.988815: train_loss -0.6999
2024-12-09 02:55:16.989998: val_loss -0.5354
2024-12-09 02:55:16.990960: Pseudo dice [0.7473]
2024-12-09 02:55:16.991735: Epoch time: 88.71 s
2024-12-09 02:55:16.992535: Yayy! New best EMA pseudo Dice: 0.7192
2024-12-09 02:55:18.528942: 
2024-12-09 02:55:18.530745: Epoch 52
2024-12-09 02:55:18.531919: Current learning rate: 0.00953
2024-12-09 02:56:47.321565: Validation loss did not improve from -0.53544. Patience: 1/50
2024-12-09 02:56:47.322722: train_loss -0.6977
2024-12-09 02:56:47.324075: val_loss -0.4788
2024-12-09 02:56:47.324936: Pseudo dice [0.7086]
2024-12-09 02:56:47.325754: Epoch time: 88.79 s
2024-12-09 02:56:48.505477: 
2024-12-09 02:56:48.507713: Epoch 53
2024-12-09 02:56:48.509056: Current learning rate: 0.00952
2024-12-09 02:58:17.248258: Validation loss did not improve from -0.53544. Patience: 2/50
2024-12-09 02:58:17.249543: train_loss -0.7066
2024-12-09 02:58:17.250524: val_loss -0.4912
2024-12-09 02:58:17.251212: Pseudo dice [0.7099]
2024-12-09 02:58:17.251866: Epoch time: 88.75 s
2024-12-09 02:58:18.447853: 
2024-12-09 02:58:18.449778: Epoch 54
2024-12-09 02:58:18.450595: Current learning rate: 0.00951
2024-12-09 02:59:47.112435: Validation loss did not improve from -0.53544. Patience: 3/50
2024-12-09 02:59:47.113758: train_loss -0.7039
2024-12-09 02:59:47.114832: val_loss -0.4759
2024-12-09 02:59:47.115520: Pseudo dice [0.7102]
2024-12-09 02:59:47.116251: Epoch time: 88.67 s
2024-12-09 02:59:48.661744: 
2024-12-09 02:59:48.663748: Epoch 55
2024-12-09 02:59:48.664831: Current learning rate: 0.0095
2024-12-09 03:01:17.147022: Validation loss did not improve from -0.53544. Patience: 4/50
2024-12-09 03:01:17.148268: train_loss -0.6985
2024-12-09 03:01:17.149624: val_loss -0.4837
2024-12-09 03:01:17.150605: Pseudo dice [0.7055]
2024-12-09 03:01:17.151327: Epoch time: 88.49 s
2024-12-09 03:01:18.338873: 
2024-12-09 03:01:18.340551: Epoch 56
2024-12-09 03:01:18.341829: Current learning rate: 0.00949
2024-12-09 03:02:46.804157: Validation loss did not improve from -0.53544. Patience: 5/50
2024-12-09 03:02:46.805258: train_loss -0.6989
2024-12-09 03:02:46.806475: val_loss -0.4582
2024-12-09 03:02:46.807434: Pseudo dice [0.7108]
2024-12-09 03:02:46.808306: Epoch time: 88.47 s
2024-12-09 03:02:48.003473: 
2024-12-09 03:02:48.005475: Epoch 57
2024-12-09 03:02:48.006819: Current learning rate: 0.00949
2024-12-09 03:04:16.546907: Validation loss did not improve from -0.53544. Patience: 6/50
2024-12-09 03:04:16.548373: train_loss -0.7058
2024-12-09 03:04:16.549409: val_loss -0.5299
2024-12-09 03:04:16.550552: Pseudo dice [0.7377]
2024-12-09 03:04:16.551589: Epoch time: 88.55 s
2024-12-09 03:04:17.741773: 
2024-12-09 03:04:17.743908: Epoch 58
2024-12-09 03:04:17.745196: Current learning rate: 0.00948
2024-12-09 03:05:46.253562: Validation loss did not improve from -0.53544. Patience: 7/50
2024-12-09 03:05:46.254906: train_loss -0.7044
2024-12-09 03:05:46.256515: val_loss -0.5074
2024-12-09 03:05:46.257205: Pseudo dice [0.7226]
2024-12-09 03:05:46.258078: Epoch time: 88.51 s
2024-12-09 03:05:47.493655: 
2024-12-09 03:05:47.495841: Epoch 59
2024-12-09 03:05:47.497261: Current learning rate: 0.00947
2024-12-09 03:07:16.015165: Validation loss did not improve from -0.53544. Patience: 8/50
2024-12-09 03:07:16.016368: train_loss -0.6989
2024-12-09 03:07:16.017458: val_loss -0.5284
2024-12-09 03:07:16.018358: Pseudo dice [0.7248]
2024-12-09 03:07:16.019339: Epoch time: 88.52 s
2024-12-09 03:07:17.577900: 
2024-12-09 03:07:17.580121: Epoch 60
2024-12-09 03:07:17.581310: Current learning rate: 0.00946
2024-12-09 03:08:46.108682: Validation loss did not improve from -0.53544. Patience: 9/50
2024-12-09 03:08:46.109754: train_loss -0.6997
2024-12-09 03:08:46.110775: val_loss -0.5011
2024-12-09 03:08:46.111621: Pseudo dice [0.725]
2024-12-09 03:08:46.112598: Epoch time: 88.53 s
2024-12-09 03:08:47.685656: 
2024-12-09 03:08:47.687740: Epoch 61
2024-12-09 03:08:47.688565: Current learning rate: 0.00945
2024-12-09 03:10:16.125672: Validation loss did not improve from -0.53544. Patience: 10/50
2024-12-09 03:10:16.127353: train_loss -0.7007
2024-12-09 03:10:16.128626: val_loss -0.4903
2024-12-09 03:10:16.129482: Pseudo dice [0.7179]
2024-12-09 03:10:16.130341: Epoch time: 88.44 s
2024-12-09 03:10:17.343598: 
2024-12-09 03:10:17.346205: Epoch 62
2024-12-09 03:10:17.347854: Current learning rate: 0.00944
2024-12-09 03:11:45.831184: Validation loss did not improve from -0.53544. Patience: 11/50
2024-12-09 03:11:45.832485: train_loss -0.7032
2024-12-09 03:11:45.834060: val_loss -0.48
2024-12-09 03:11:45.834747: Pseudo dice [0.7131]
2024-12-09 03:11:45.835730: Epoch time: 88.49 s
2024-12-09 03:11:47.042640: 
2024-12-09 03:11:47.044616: Epoch 63
2024-12-09 03:11:47.045540: Current learning rate: 0.00943
2024-12-09 03:13:15.479186: Validation loss did not improve from -0.53544. Patience: 12/50
2024-12-09 03:13:15.480007: train_loss -0.7088
2024-12-09 03:13:15.481015: val_loss -0.5079
2024-12-09 03:13:15.481848: Pseudo dice [0.7238]
2024-12-09 03:13:15.482628: Epoch time: 88.44 s
2024-12-09 03:13:16.719964: 
2024-12-09 03:13:16.722233: Epoch 64
2024-12-09 03:13:16.723625: Current learning rate: 0.00942
2024-12-09 03:14:45.184292: Validation loss did not improve from -0.53544. Patience: 13/50
2024-12-09 03:14:45.185822: train_loss -0.7079
2024-12-09 03:14:45.186986: val_loss -0.4912
2024-12-09 03:14:45.188034: Pseudo dice [0.7074]
2024-12-09 03:14:45.189334: Epoch time: 88.47 s
2024-12-09 03:14:46.772973: 
2024-12-09 03:14:46.775386: Epoch 65
2024-12-09 03:14:46.776510: Current learning rate: 0.00941
2024-12-09 03:16:15.179292: Validation loss did not improve from -0.53544. Patience: 14/50
2024-12-09 03:16:15.180481: train_loss -0.7119
2024-12-09 03:16:15.181663: val_loss -0.5052
2024-12-09 03:16:15.182646: Pseudo dice [0.7279]
2024-12-09 03:16:15.183281: Epoch time: 88.41 s
2024-12-09 03:16:16.400840: 
2024-12-09 03:16:16.402977: Epoch 66
2024-12-09 03:16:16.404227: Current learning rate: 0.0094
2024-12-09 03:17:44.869093: Validation loss did not improve from -0.53544. Patience: 15/50
2024-12-09 03:17:44.870152: train_loss -0.7134
2024-12-09 03:17:44.871068: val_loss -0.5003
2024-12-09 03:17:44.872027: Pseudo dice [0.7166]
2024-12-09 03:17:44.872866: Epoch time: 88.47 s
2024-12-09 03:17:46.096961: 
2024-12-09 03:17:46.098404: Epoch 67
2024-12-09 03:17:46.099686: Current learning rate: 0.00939
2024-12-09 03:19:14.471974: Validation loss did not improve from -0.53544. Patience: 16/50
2024-12-09 03:19:14.472960: train_loss -0.7186
2024-12-09 03:19:14.473855: val_loss -0.509
2024-12-09 03:19:14.474727: Pseudo dice [0.7199]
2024-12-09 03:19:14.475458: Epoch time: 88.38 s
2024-12-09 03:19:15.723175: 
2024-12-09 03:19:15.725142: Epoch 68
2024-12-09 03:19:15.726242: Current learning rate: 0.00939
2024-12-09 03:20:44.260607: Validation loss did not improve from -0.53544. Patience: 17/50
2024-12-09 03:20:44.262065: train_loss -0.7198
2024-12-09 03:20:44.263033: val_loss -0.4987
2024-12-09 03:20:44.264263: Pseudo dice [0.7322]
2024-12-09 03:20:44.265494: Epoch time: 88.54 s
2024-12-09 03:20:44.266380: Yayy! New best EMA pseudo Dice: 0.7201
2024-12-09 03:20:45.857539: 
2024-12-09 03:20:45.859617: Epoch 69
2024-12-09 03:20:45.860888: Current learning rate: 0.00938
2024-12-09 03:22:13.985774: Validation loss did not improve from -0.53544. Patience: 18/50
2024-12-09 03:22:13.987462: train_loss -0.7234
2024-12-09 03:22:13.988492: val_loss -0.5067
2024-12-09 03:22:13.989335: Pseudo dice [0.7235]
2024-12-09 03:22:13.990426: Epoch time: 88.13 s
2024-12-09 03:22:14.339723: Yayy! New best EMA pseudo Dice: 0.7204
2024-12-09 03:22:15.909602: 
2024-12-09 03:22:15.911685: Epoch 70
2024-12-09 03:22:15.913090: Current learning rate: 0.00937
2024-12-09 03:23:44.016180: Validation loss did not improve from -0.53544. Patience: 19/50
2024-12-09 03:23:44.017232: train_loss -0.7243
2024-12-09 03:23:44.018592: val_loss -0.5036
2024-12-09 03:23:44.019405: Pseudo dice [0.7205]
2024-12-09 03:23:44.020135: Epoch time: 88.11 s
2024-12-09 03:23:44.020942: Yayy! New best EMA pseudo Dice: 0.7204
2024-12-09 03:23:45.944472: 
2024-12-09 03:23:45.946573: Epoch 71
2024-12-09 03:23:45.947619: Current learning rate: 0.00936
2024-12-09 03:25:13.949312: Validation loss did not improve from -0.53544. Patience: 20/50
2024-12-09 03:25:13.950464: train_loss -0.722
2024-12-09 03:25:13.951527: val_loss -0.5297
2024-12-09 03:25:13.952224: Pseudo dice [0.7393]
2024-12-09 03:25:13.952972: Epoch time: 88.01 s
2024-12-09 03:25:13.953802: Yayy! New best EMA pseudo Dice: 0.7223
2024-12-09 03:25:15.537453: 
2024-12-09 03:25:15.539404: Epoch 72
2024-12-09 03:25:15.540752: Current learning rate: 0.00935
2024-12-09 03:26:43.747471: Validation loss did not improve from -0.53544. Patience: 21/50
2024-12-09 03:26:43.748603: train_loss -0.7163
2024-12-09 03:26:43.749796: val_loss -0.4779
2024-12-09 03:26:43.750718: Pseudo dice [0.711]
2024-12-09 03:26:43.751550: Epoch time: 88.21 s
2024-12-09 03:26:45.008036: 
2024-12-09 03:26:45.009973: Epoch 73
2024-12-09 03:26:45.011126: Current learning rate: 0.00934
2024-12-09 03:28:13.184907: Validation loss did not improve from -0.53544. Patience: 22/50
2024-12-09 03:28:13.186173: train_loss -0.7198
2024-12-09 03:28:13.187260: val_loss -0.5048
2024-12-09 03:28:13.188154: Pseudo dice [0.7226]
2024-12-09 03:28:13.189274: Epoch time: 88.18 s
2024-12-09 03:28:14.412827: 
2024-12-09 03:28:14.414600: Epoch 74
2024-12-09 03:28:14.415800: Current learning rate: 0.00933
2024-12-09 03:29:42.490789: Validation loss did not improve from -0.53544. Patience: 23/50
2024-12-09 03:29:42.491977: train_loss -0.7258
2024-12-09 03:29:42.493247: val_loss -0.5124
2024-12-09 03:29:42.494241: Pseudo dice [0.7321]
2024-12-09 03:29:42.495170: Epoch time: 88.08 s
2024-12-09 03:29:42.847817: Yayy! New best EMA pseudo Dice: 0.7224
2024-12-09 03:29:44.441706: 
2024-12-09 03:29:44.443921: Epoch 75
2024-12-09 03:29:44.445155: Current learning rate: 0.00932
2024-12-09 03:31:12.566528: Validation loss did not improve from -0.53544. Patience: 24/50
2024-12-09 03:31:12.567895: train_loss -0.7127
2024-12-09 03:31:12.569215: val_loss -0.5088
2024-12-09 03:31:12.570203: Pseudo dice [0.7286]
2024-12-09 03:31:12.571337: Epoch time: 88.13 s
2024-12-09 03:31:12.572074: Yayy! New best EMA pseudo Dice: 0.723
2024-12-09 03:31:14.142809: 
2024-12-09 03:31:14.144733: Epoch 76
2024-12-09 03:31:14.145638: Current learning rate: 0.00931
2024-12-09 03:32:42.273217: Validation loss did not improve from -0.53544. Patience: 25/50
2024-12-09 03:32:42.274430: train_loss -0.7163
2024-12-09 03:32:42.275435: val_loss -0.5073
2024-12-09 03:32:42.276301: Pseudo dice [0.7227]
2024-12-09 03:32:42.277071: Epoch time: 88.13 s
2024-12-09 03:32:43.516031: 
2024-12-09 03:32:43.518476: Epoch 77
2024-12-09 03:32:43.520163: Current learning rate: 0.0093
2024-12-09 03:34:11.838315: Validation loss did not improve from -0.53544. Patience: 26/50
2024-12-09 03:34:11.839549: train_loss -0.7224
2024-12-09 03:34:11.840347: val_loss -0.4907
2024-12-09 03:34:11.840985: Pseudo dice [0.7217]
2024-12-09 03:34:11.841633: Epoch time: 88.32 s
2024-12-09 03:34:13.075117: 
2024-12-09 03:34:13.110296: Epoch 78
2024-12-09 03:34:13.111889: Current learning rate: 0.0093
2024-12-09 03:35:41.602759: Validation loss improved from -0.53544 to -0.53679! Patience: 26/50
2024-12-09 03:35:41.603730: train_loss -0.7309
2024-12-09 03:35:41.604675: val_loss -0.5368
2024-12-09 03:35:41.605389: Pseudo dice [0.7397]
2024-12-09 03:35:41.606113: Epoch time: 88.53 s
2024-12-09 03:35:41.606850: Yayy! New best EMA pseudo Dice: 0.7245
2024-12-09 03:35:43.208783: 
2024-12-09 03:35:43.210574: Epoch 79
2024-12-09 03:35:43.211804: Current learning rate: 0.00929
2024-12-09 03:37:11.831514: Validation loss did not improve from -0.53679. Patience: 1/50
2024-12-09 03:37:11.832628: train_loss -0.7271
2024-12-09 03:37:11.833817: val_loss -0.4954
2024-12-09 03:37:11.834771: Pseudo dice [0.7193]
2024-12-09 03:37:11.835763: Epoch time: 88.62 s
2024-12-09 03:37:13.544170: 
2024-12-09 03:37:13.546306: Epoch 80
2024-12-09 03:37:13.547832: Current learning rate: 0.00928
2024-12-09 03:38:42.094714: Validation loss did not improve from -0.53679. Patience: 2/50
2024-12-09 03:38:42.095940: train_loss -0.7224
2024-12-09 03:38:42.097373: val_loss -0.4927
2024-12-09 03:38:42.098474: Pseudo dice [0.7129]
2024-12-09 03:38:42.099616: Epoch time: 88.55 s
2024-12-09 03:38:43.701529: 
2024-12-09 03:38:43.703588: Epoch 81
2024-12-09 03:38:43.704614: Current learning rate: 0.00927
2024-12-09 03:40:12.274113: Validation loss did not improve from -0.53679. Patience: 3/50
2024-12-09 03:40:12.275825: train_loss -0.7148
2024-12-09 03:40:12.277094: val_loss -0.4953
2024-12-09 03:40:12.277782: Pseudo dice [0.7316]
2024-12-09 03:40:12.278403: Epoch time: 88.58 s
2024-12-09 03:40:13.544503: 
2024-12-09 03:40:13.546566: Epoch 82
2024-12-09 03:40:13.547986: Current learning rate: 0.00926
2024-12-09 03:41:42.023151: Validation loss did not improve from -0.53679. Patience: 4/50
2024-12-09 03:41:42.024243: train_loss -0.7158
2024-12-09 03:41:42.025242: val_loss -0.5037
2024-12-09 03:41:42.026208: Pseudo dice [0.7212]
2024-12-09 03:41:42.027203: Epoch time: 88.48 s
2024-12-09 03:41:43.207983: 
2024-12-09 03:41:43.210142: Epoch 83
2024-12-09 03:41:43.211744: Current learning rate: 0.00925
2024-12-09 03:43:12.752230: Validation loss did not improve from -0.53679. Patience: 5/50
2024-12-09 03:43:12.755503: train_loss -0.7223
2024-12-09 03:43:12.758558: val_loss -0.5195
2024-12-09 03:43:12.759506: Pseudo dice [0.7273]
2024-12-09 03:43:12.761285: Epoch time: 89.55 s
2024-12-09 03:43:13.986918: 
2024-12-09 03:43:13.988710: Epoch 84
2024-12-09 03:43:13.989726: Current learning rate: 0.00924
2024-12-09 03:44:42.301029: Validation loss did not improve from -0.53679. Patience: 6/50
2024-12-09 03:44:42.302241: train_loss -0.7267
2024-12-09 03:44:42.303576: val_loss -0.5155
2024-12-09 03:44:42.304612: Pseudo dice [0.7327]
2024-12-09 03:44:42.305454: Epoch time: 88.32 s
2024-12-09 03:44:42.660550: Yayy! New best EMA pseudo Dice: 0.7248
2024-12-09 03:44:44.146286: 
2024-12-09 03:44:44.148221: Epoch 85
2024-12-09 03:44:44.149431: Current learning rate: 0.00923
2024-12-09 03:46:12.564789: Validation loss did not improve from -0.53679. Patience: 7/50
2024-12-09 03:46:12.565989: train_loss -0.7129
2024-12-09 03:46:12.567093: val_loss -0.5124
2024-12-09 03:46:12.567822: Pseudo dice [0.7304]
2024-12-09 03:46:12.568539: Epoch time: 88.42 s
2024-12-09 03:46:12.569195: Yayy! New best EMA pseudo Dice: 0.7253
2024-12-09 03:46:14.080354: 
2024-12-09 03:46:14.111500: Epoch 86
2024-12-09 03:46:14.112624: Current learning rate: 0.00922
2024-12-09 03:47:42.576214: Validation loss did not improve from -0.53679. Patience: 8/50
2024-12-09 03:47:42.579567: train_loss -0.7214
2024-12-09 03:47:42.580585: val_loss -0.4964
2024-12-09 03:47:42.581362: Pseudo dice [0.7265]
2024-12-09 03:47:42.582062: Epoch time: 88.5 s
2024-12-09 03:47:42.582661: Yayy! New best EMA pseudo Dice: 0.7255
2024-12-09 03:47:44.113382: 
2024-12-09 03:47:44.115554: Epoch 87
2024-12-09 03:47:44.116622: Current learning rate: 0.00921
2024-12-09 03:49:12.468608: Validation loss did not improve from -0.53679. Patience: 9/50
2024-12-09 03:49:12.469642: train_loss -0.7258
2024-12-09 03:49:12.470927: val_loss -0.4218
2024-12-09 03:49:12.472109: Pseudo dice [0.6751]
2024-12-09 03:49:12.473100: Epoch time: 88.36 s
2024-12-09 03:49:13.632732: 
2024-12-09 03:49:13.634809: Epoch 88
2024-12-09 03:49:13.636038: Current learning rate: 0.0092
2024-12-09 03:50:42.094044: Validation loss did not improve from -0.53679. Patience: 10/50
2024-12-09 03:50:42.095600: train_loss -0.7076
2024-12-09 03:50:42.097086: val_loss -0.4708
2024-12-09 03:50:42.098057: Pseudo dice [0.7072]
2024-12-09 03:50:42.099405: Epoch time: 88.46 s
2024-12-09 03:50:43.307875: 
2024-12-09 03:50:43.309963: Epoch 89
2024-12-09 03:50:43.310956: Current learning rate: 0.0092
2024-12-09 03:52:11.819332: Validation loss did not improve from -0.53679. Patience: 11/50
2024-12-09 03:52:11.820970: train_loss -0.7152
2024-12-09 03:52:11.822090: val_loss -0.5201
2024-12-09 03:52:11.823040: Pseudo dice [0.7295]
2024-12-09 03:52:11.823925: Epoch time: 88.51 s
2024-12-09 03:52:13.304180: 
2024-12-09 03:52:13.306089: Epoch 90
2024-12-09 03:52:13.307160: Current learning rate: 0.00919
2024-12-09 03:53:41.974773: Validation loss did not improve from -0.53679. Patience: 12/50
2024-12-09 03:53:41.976092: train_loss -0.7292
2024-12-09 03:53:41.977145: val_loss -0.5145
2024-12-09 03:53:41.978032: Pseudo dice [0.7233]
2024-12-09 03:53:41.978811: Epoch time: 88.67 s
2024-12-09 03:53:43.184822: 
2024-12-09 03:53:43.187032: Epoch 91
2024-12-09 03:53:43.188033: Current learning rate: 0.00918
2024-12-09 03:55:11.801317: Validation loss did not improve from -0.53679. Patience: 13/50
2024-12-09 03:55:11.802516: train_loss -0.7331
2024-12-09 03:55:11.803719: val_loss -0.4988
2024-12-09 03:55:11.804877: Pseudo dice [0.7243]
2024-12-09 03:55:11.805802: Epoch time: 88.62 s
2024-12-09 03:55:12.971295: 
2024-12-09 03:55:12.973205: Epoch 92
2024-12-09 03:55:12.974379: Current learning rate: 0.00917
2024-12-09 03:56:41.560552: Validation loss did not improve from -0.53679. Patience: 14/50
2024-12-09 03:56:41.561416: train_loss -0.7268
2024-12-09 03:56:41.562704: val_loss -0.4944
2024-12-09 03:56:41.563368: Pseudo dice [0.7222]
2024-12-09 03:56:41.564098: Epoch time: 88.59 s
2024-12-09 03:56:43.255954: 
2024-12-09 03:56:43.258338: Epoch 93
2024-12-09 03:56:43.259662: Current learning rate: 0.00916
2024-12-09 03:58:11.818405: Validation loss did not improve from -0.53679. Patience: 15/50
2024-12-09 03:58:11.820014: train_loss -0.7328
2024-12-09 03:58:11.821369: val_loss -0.5043
2024-12-09 03:58:11.822541: Pseudo dice [0.7182]
2024-12-09 03:58:11.823279: Epoch time: 88.56 s
2024-12-09 03:58:12.988087: 
2024-12-09 03:58:12.990140: Epoch 94
2024-12-09 03:58:12.990905: Current learning rate: 0.00915
2024-12-09 03:59:41.368057: Validation loss did not improve from -0.53679. Patience: 16/50
2024-12-09 03:59:41.369346: train_loss -0.727
2024-12-09 03:59:41.370739: val_loss -0.4607
2024-12-09 03:59:41.371674: Pseudo dice [0.6965]
2024-12-09 03:59:41.372616: Epoch time: 88.38 s
2024-12-09 03:59:42.866558: 
2024-12-09 03:59:42.868237: Epoch 95
2024-12-09 03:59:42.869406: Current learning rate: 0.00914
2024-12-09 04:01:11.123102: Validation loss did not improve from -0.53679. Patience: 17/50
2024-12-09 04:01:11.124324: train_loss -0.7348
2024-12-09 04:01:11.125562: val_loss -0.4955
2024-12-09 04:01:11.126625: Pseudo dice [0.7239]
2024-12-09 04:01:11.127527: Epoch time: 88.26 s
2024-12-09 04:01:12.310341: 
2024-12-09 04:01:12.312156: Epoch 96
2024-12-09 04:01:12.313460: Current learning rate: 0.00913
2024-12-09 04:02:40.717461: Validation loss did not improve from -0.53679. Patience: 18/50
2024-12-09 04:02:40.718589: train_loss -0.7343
2024-12-09 04:02:40.719659: val_loss -0.5131
2024-12-09 04:02:40.720442: Pseudo dice [0.7369]
2024-12-09 04:02:40.721231: Epoch time: 88.41 s
2024-12-09 04:02:41.908002: 
2024-12-09 04:02:41.910188: Epoch 97
2024-12-09 04:02:41.911420: Current learning rate: 0.00912
2024-12-09 04:04:10.261787: Validation loss did not improve from -0.53679. Patience: 19/50
2024-12-09 04:04:10.263109: train_loss -0.7422
2024-12-09 04:04:10.264309: val_loss -0.5103
2024-12-09 04:04:10.265353: Pseudo dice [0.7341]
2024-12-09 04:04:10.266412: Epoch time: 88.36 s
2024-12-09 04:04:11.452053: 
2024-12-09 04:04:11.453739: Epoch 98
2024-12-09 04:04:11.454930: Current learning rate: 0.00911
2024-12-09 04:05:39.767721: Validation loss did not improve from -0.53679. Patience: 20/50
2024-12-09 04:05:39.768965: train_loss -0.737
2024-12-09 04:05:39.769782: val_loss -0.5036
2024-12-09 04:05:39.770533: Pseudo dice [0.7201]
2024-12-09 04:05:39.771460: Epoch time: 88.32 s
2024-12-09 04:05:41.010306: 
2024-12-09 04:05:41.011631: Epoch 99
2024-12-09 04:05:41.012758: Current learning rate: 0.0091
2024-12-09 04:07:09.346079: Validation loss did not improve from -0.53679. Patience: 21/50
2024-12-09 04:07:09.347584: train_loss -0.7368
2024-12-09 04:07:09.348666: val_loss -0.4942
2024-12-09 04:07:09.349697: Pseudo dice [0.718]
2024-12-09 04:07:09.350855: Epoch time: 88.34 s
2024-12-09 04:07:10.888991: 
2024-12-09 04:07:10.891231: Epoch 100
2024-12-09 04:07:10.892357: Current learning rate: 0.0091
2024-12-09 04:08:39.191081: Validation loss did not improve from -0.53679. Patience: 22/50
2024-12-09 04:08:39.192213: train_loss -0.7348
2024-12-09 04:08:39.193519: val_loss -0.5026
2024-12-09 04:08:39.194683: Pseudo dice [0.7235]
2024-12-09 04:08:39.195540: Epoch time: 88.3 s
2024-12-09 04:08:40.388994: 
2024-12-09 04:08:40.390917: Epoch 101
2024-12-09 04:08:40.391782: Current learning rate: 0.00909
2024-12-09 04:10:08.734012: Validation loss did not improve from -0.53679. Patience: 23/50
2024-12-09 04:10:08.735440: train_loss -0.7348
2024-12-09 04:10:08.736359: val_loss -0.511
2024-12-09 04:10:08.737275: Pseudo dice [0.726]
2024-12-09 04:10:08.738185: Epoch time: 88.35 s
2024-12-09 04:10:09.928382: 
2024-12-09 04:10:09.930682: Epoch 102
2024-12-09 04:10:09.931839: Current learning rate: 0.00908
2024-12-09 04:11:38.308234: Validation loss did not improve from -0.53679. Patience: 24/50
2024-12-09 04:11:38.309930: train_loss -0.7367
2024-12-09 04:11:38.311471: val_loss -0.4947
2024-12-09 04:11:38.312362: Pseudo dice [0.7211]
2024-12-09 04:11:38.313294: Epoch time: 88.38 s
2024-12-09 04:11:39.502124: 
2024-12-09 04:11:39.504380: Epoch 103
2024-12-09 04:11:39.505657: Current learning rate: 0.00907
2024-12-09 04:13:07.933563: Validation loss did not improve from -0.53679. Patience: 25/50
2024-12-09 04:13:07.934700: train_loss -0.7375
2024-12-09 04:13:07.935588: val_loss -0.4931
2024-12-09 04:13:07.936297: Pseudo dice [0.7131]
2024-12-09 04:13:07.936921: Epoch time: 88.43 s
2024-12-09 04:13:09.520573: 
2024-12-09 04:13:09.522314: Epoch 104
2024-12-09 04:13:09.523307: Current learning rate: 0.00906
2024-12-09 04:14:37.850507: Validation loss did not improve from -0.53679. Patience: 26/50
2024-12-09 04:14:37.851370: train_loss -0.7459
2024-12-09 04:14:37.852310: val_loss -0.5093
2024-12-09 04:14:37.853104: Pseudo dice [0.7346]
2024-12-09 04:14:37.853769: Epoch time: 88.33 s
2024-12-09 04:14:39.425012: 
2024-12-09 04:14:39.427270: Epoch 105
2024-12-09 04:14:39.428420: Current learning rate: 0.00905
2024-12-09 04:16:07.808053: Validation loss did not improve from -0.53679. Patience: 27/50
2024-12-09 04:16:07.808784: train_loss -0.7419
2024-12-09 04:16:07.810053: val_loss -0.5068
2024-12-09 04:16:07.811220: Pseudo dice [0.7286]
2024-12-09 04:16:07.812413: Epoch time: 88.38 s
2024-12-09 04:16:08.971880: 
2024-12-09 04:16:08.974030: Epoch 106
2024-12-09 04:16:08.975250: Current learning rate: 0.00904
2024-12-09 04:17:37.443908: Validation loss did not improve from -0.53679. Patience: 28/50
2024-12-09 04:17:37.444961: train_loss -0.7209
2024-12-09 04:17:37.446181: val_loss -0.44
2024-12-09 04:17:37.446985: Pseudo dice [0.6969]
2024-12-09 04:17:37.447722: Epoch time: 88.47 s
2024-12-09 04:17:38.644412: 
2024-12-09 04:17:38.646142: Epoch 107
2024-12-09 04:17:38.647357: Current learning rate: 0.00903
2024-12-09 04:19:07.246873: Validation loss did not improve from -0.53679. Patience: 29/50
2024-12-09 04:19:07.248190: train_loss -0.6997
2024-12-09 04:19:07.249567: val_loss -0.4865
2024-12-09 04:19:07.250269: Pseudo dice [0.7213]
2024-12-09 04:19:07.250934: Epoch time: 88.6 s
2024-12-09 04:19:08.423396: 
2024-12-09 04:19:08.425616: Epoch 108
2024-12-09 04:19:08.426757: Current learning rate: 0.00902
2024-12-09 04:20:36.856145: Validation loss did not improve from -0.53679. Patience: 30/50
2024-12-09 04:20:36.857377: train_loss -0.7219
2024-12-09 04:20:36.858248: val_loss -0.4902
2024-12-09 04:20:36.859000: Pseudo dice [0.721]
2024-12-09 04:20:36.859776: Epoch time: 88.44 s
2024-12-09 04:20:37.999617: 
2024-12-09 04:20:38.001651: Epoch 109
2024-12-09 04:20:38.002646: Current learning rate: 0.00901
2024-12-09 04:22:06.472599: Validation loss did not improve from -0.53679. Patience: 31/50
2024-12-09 04:22:06.473644: train_loss -0.7238
2024-12-09 04:22:06.474710: val_loss -0.5066
2024-12-09 04:22:06.475539: Pseudo dice [0.725]
2024-12-09 04:22:06.476190: Epoch time: 88.48 s
2024-12-09 04:22:07.980440: 
2024-12-09 04:22:07.982413: Epoch 110
2024-12-09 04:22:07.983499: Current learning rate: 0.009
2024-12-09 04:23:36.607059: Validation loss did not improve from -0.53679. Patience: 32/50
2024-12-09 04:23:36.608587: train_loss -0.7312
2024-12-09 04:23:36.609622: val_loss -0.4981
2024-12-09 04:23:36.610495: Pseudo dice [0.7287]
2024-12-09 04:23:36.611591: Epoch time: 88.63 s
2024-12-09 04:23:37.776883: 
2024-12-09 04:23:37.779202: Epoch 111
2024-12-09 04:23:37.780111: Current learning rate: 0.009
2024-12-09 04:25:06.520068: Validation loss did not improve from -0.53679. Patience: 33/50
2024-12-09 04:25:06.521104: train_loss -0.7369
2024-12-09 04:25:06.522051: val_loss -0.4539
2024-12-09 04:25:06.522834: Pseudo dice [0.7041]
2024-12-09 04:25:06.523623: Epoch time: 88.75 s
2024-12-09 04:25:07.693487: 
2024-12-09 04:25:07.695720: Epoch 112
2024-12-09 04:25:07.696796: Current learning rate: 0.00899
2024-12-09 04:26:36.462244: Validation loss did not improve from -0.53679. Patience: 34/50
2024-12-09 04:26:36.463343: train_loss -0.7423
2024-12-09 04:26:36.464832: val_loss -0.4843
2024-12-09 04:26:36.465922: Pseudo dice [0.7166]
2024-12-09 04:26:36.466983: Epoch time: 88.77 s
2024-12-09 04:26:37.642883: 
2024-12-09 04:26:37.644850: Epoch 113
2024-12-09 04:26:37.646247: Current learning rate: 0.00898
2024-12-09 04:28:06.399209: Validation loss did not improve from -0.53679. Patience: 35/50
2024-12-09 04:28:06.400625: train_loss -0.7432
2024-12-09 04:28:06.402256: val_loss -0.5008
2024-12-09 04:28:06.403321: Pseudo dice [0.7273]
2024-12-09 04:28:06.404603: Epoch time: 88.76 s
2024-12-09 04:28:07.582603: 
2024-12-09 04:28:07.585016: Epoch 114
2024-12-09 04:28:07.586824: Current learning rate: 0.00897
2024-12-09 04:29:35.939416: Validation loss did not improve from -0.53679. Patience: 36/50
2024-12-09 04:29:35.940604: train_loss -0.7366
2024-12-09 04:29:35.941636: val_loss -0.5202
2024-12-09 04:29:35.942560: Pseudo dice [0.7273]
2024-12-09 04:29:35.943366: Epoch time: 88.36 s
2024-12-09 04:29:37.437278: 
2024-12-09 04:29:37.439206: Epoch 115
2024-12-09 04:29:37.440298: Current learning rate: 0.00896
2024-12-09 04:31:05.799273: Validation loss did not improve from -0.53679. Patience: 37/50
2024-12-09 04:31:05.800902: train_loss -0.741
2024-12-09 04:31:05.802154: val_loss -0.5211
2024-12-09 04:31:05.803258: Pseudo dice [0.7342]
2024-12-09 04:31:05.804227: Epoch time: 88.36 s
2024-12-09 04:31:07.331023: 
2024-12-09 04:31:07.333154: Epoch 116
2024-12-09 04:31:07.334554: Current learning rate: 0.00895
2024-12-09 04:32:35.734670: Validation loss did not improve from -0.53679. Patience: 38/50
2024-12-09 04:32:35.735895: train_loss -0.7393
2024-12-09 04:32:35.736954: val_loss -0.5029
2024-12-09 04:32:35.737868: Pseudo dice [0.7183]
2024-12-09 04:32:35.738878: Epoch time: 88.41 s
2024-12-09 04:32:36.924468: 
2024-12-09 04:32:36.926314: Epoch 117
2024-12-09 04:32:36.927148: Current learning rate: 0.00894
2024-12-09 04:34:05.259191: Validation loss did not improve from -0.53679. Patience: 39/50
2024-12-09 04:34:05.260447: train_loss -0.7439
2024-12-09 04:34:05.261504: val_loss -0.5069
2024-12-09 04:34:05.262316: Pseudo dice [0.7207]
2024-12-09 04:34:05.262994: Epoch time: 88.34 s
2024-12-09 04:34:06.447854: 
2024-12-09 04:34:06.450037: Epoch 118
2024-12-09 04:34:06.451147: Current learning rate: 0.00893
2024-12-09 04:35:34.814402: Validation loss did not improve from -0.53679. Patience: 40/50
2024-12-09 04:35:34.815527: train_loss -0.7443
2024-12-09 04:35:34.816513: val_loss -0.5294
2024-12-09 04:35:34.817323: Pseudo dice [0.7418]
2024-12-09 04:35:34.818129: Epoch time: 88.37 s
2024-12-09 04:35:36.015598: 
2024-12-09 04:35:36.017839: Epoch 119
2024-12-09 04:35:36.019056: Current learning rate: 0.00892
2024-12-09 04:37:04.391591: Validation loss did not improve from -0.53679. Patience: 41/50
2024-12-09 04:37:04.392929: train_loss -0.7413
2024-12-09 04:37:04.394258: val_loss -0.476
2024-12-09 04:37:04.395328: Pseudo dice [0.7204]
2024-12-09 04:37:04.396067: Epoch time: 88.38 s
2024-12-09 04:37:05.916794: 
2024-12-09 04:37:05.919084: Epoch 120
2024-12-09 04:37:05.920578: Current learning rate: 0.00891
2024-12-09 04:38:34.225655: Validation loss did not improve from -0.53679. Patience: 42/50
2024-12-09 04:38:34.226768: train_loss -0.7476
2024-12-09 04:38:34.227684: val_loss -0.4704
2024-12-09 04:38:34.228747: Pseudo dice [0.7171]
2024-12-09 04:38:34.229857: Epoch time: 88.31 s
2024-12-09 04:38:35.435329: 
2024-12-09 04:38:35.437337: Epoch 121
2024-12-09 04:38:35.438580: Current learning rate: 0.0089
2024-12-09 04:40:03.781490: Validation loss did not improve from -0.53679. Patience: 43/50
2024-12-09 04:40:03.782606: train_loss -0.7437
2024-12-09 04:40:03.783870: val_loss -0.5129
2024-12-09 04:40:03.784671: Pseudo dice [0.7308]
2024-12-09 04:40:03.785503: Epoch time: 88.35 s
2024-12-09 04:40:04.954175: 
2024-12-09 04:40:04.956280: Epoch 122
2024-12-09 04:40:04.957336: Current learning rate: 0.00889
2024-12-09 04:41:33.314149: Validation loss did not improve from -0.53679. Patience: 44/50
2024-12-09 04:41:33.315515: train_loss -0.7417
2024-12-09 04:41:33.317064: val_loss -0.5125
2024-12-09 04:41:33.318133: Pseudo dice [0.7382]
2024-12-09 04:41:33.319408: Epoch time: 88.36 s
2024-12-09 04:41:34.523182: 
2024-12-09 04:41:34.525135: Epoch 123
2024-12-09 04:41:34.526303: Current learning rate: 0.00889
2024-12-09 04:43:02.843731: Validation loss did not improve from -0.53679. Patience: 45/50
2024-12-09 04:43:02.844713: train_loss -0.7467
2024-12-09 04:43:02.845773: val_loss -0.526
2024-12-09 04:43:02.846784: Pseudo dice [0.7291]
2024-12-09 04:43:02.847812: Epoch time: 88.32 s
2024-12-09 04:43:02.848846: Yayy! New best EMA pseudo Dice: 0.7255
2024-12-09 04:43:04.428416: 
2024-12-09 04:43:04.430398: Epoch 124
2024-12-09 04:43:04.431724: Current learning rate: 0.00888
2024-12-09 04:44:32.779020: Validation loss did not improve from -0.53679. Patience: 46/50
2024-12-09 04:44:32.780073: train_loss -0.7538
2024-12-09 04:44:32.781154: val_loss -0.4956
2024-12-09 04:44:32.782160: Pseudo dice [0.7295]
2024-12-09 04:44:32.783131: Epoch time: 88.35 s
2024-12-09 04:44:33.128826: Yayy! New best EMA pseudo Dice: 0.7259
2024-12-09 04:44:34.650509: 
2024-12-09 04:44:34.652496: Epoch 125
2024-12-09 04:44:34.653947: Current learning rate: 0.00887
2024-12-09 04:46:02.998163: Validation loss did not improve from -0.53679. Patience: 47/50
2024-12-09 04:46:02.999341: train_loss -0.7542
2024-12-09 04:46:03.000555: val_loss -0.5038
2024-12-09 04:46:03.001437: Pseudo dice [0.7332]
2024-12-09 04:46:03.002322: Epoch time: 88.35 s
2024-12-09 04:46:03.003128: Yayy! New best EMA pseudo Dice: 0.7266
2024-12-09 04:46:04.499664: 
2024-12-09 04:46:04.502054: Epoch 126
2024-12-09 04:46:04.502957: Current learning rate: 0.00886
2024-12-09 04:47:32.801955: Validation loss did not improve from -0.53679. Patience: 48/50
2024-12-09 04:47:32.804308: train_loss -0.7525
2024-12-09 04:47:32.805676: val_loss -0.5126
2024-12-09 04:47:32.806629: Pseudo dice [0.7349]
2024-12-09 04:47:32.807616: Epoch time: 88.31 s
2024-12-09 04:47:32.808667: Yayy! New best EMA pseudo Dice: 0.7275
2024-12-09 04:47:34.934639: 
2024-12-09 04:47:34.936767: Epoch 127
2024-12-09 04:47:34.937930: Current learning rate: 0.00885
2024-12-09 04:49:03.006123: Validation loss did not improve from -0.53679. Patience: 49/50
2024-12-09 04:49:03.009314: train_loss -0.7525
2024-12-09 04:49:03.012667: val_loss -0.5261
2024-12-09 04:49:03.014255: Pseudo dice [0.7361]
2024-12-09 04:49:03.016604: Epoch time: 88.07 s
2024-12-09 04:49:03.018739: Yayy! New best EMA pseudo Dice: 0.7283
2024-12-09 04:49:04.719534: 
2024-12-09 04:49:04.721871: Epoch 128
2024-12-09 04:49:04.723218: Current learning rate: 0.00884
2024-12-09 04:50:32.801210: Validation loss did not improve from -0.53679. Patience: 50/50
2024-12-09 04:50:32.802332: train_loss -0.7491
2024-12-09 04:50:32.803481: val_loss -0.4869
2024-12-09 04:50:32.804282: Pseudo dice [0.7282]
2024-12-09 04:50:32.805141: Epoch time: 88.08 s
2024-12-09 04:50:34.025695: Patience reached. Stopping training.
2024-12-09 04:50:34.382961: Training done.
2024-12-09 04:50:34.568259: Using splits from existing split file: /home/gridsan/nchutisilp/datasets/nnUNet_Datasets/nnUNet_preprocessed/Dataset308_Sohee_Calcium_OCT_CrossValidation/splits_final.json
2024-12-09 04:50:34.591934: The split file contains 5 splits.
2024-12-09 04:50:34.593131: Desired fold for training: 4
2024-12-09 04:50:34.593898: This split has 7 training and 1 validation cases.
2024-12-09 04:50:34.594944: predicting 101-045
2024-12-09 04:50:34.608064: 101-045, shape torch.Size([1, 375, 498, 498]), rank 0
2024-12-09 04:52:37.130512: Validation complete
2024-12-09 04:52:37.131716: Mean Validation Dice:  0.7196133623763336
