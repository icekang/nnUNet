/nfs/erelab001/shared/Computational_Group/Naravich/nnUNet/.venv/bin/python
Starting training with CONFIG=3d_32x160x128_b10, DATASET_ID=310, TRAINER=nnUNetTrainerScaleAnalysis60
/nfs/erelab001/shared/Computational_Group/Naravich/nnUNet/nnunetv2/training/nnUNetTrainer/nnUNetTrainer.py:169: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  self.grad_scaler = GradScaler() if self.device.type == 'cuda' else None

############################
INFO: You are using the old nnU-Net default plans. We have updated our recommendations. Please consider using those instead! Read more here: https://github.com/MIC-DKFZ/nnUNet/blob/master/documentation/resenc_presets.md
############################

Using device: cuda:0

#######################################################################
Please cite the following paper when using nnU-Net:
Isensee, F., Jaeger, P. F., Kohl, S. A., Petersen, J., & Maier-Hein, K. H. (2021). nnU-Net: a self-configuring method for deep learning-based biomedical image segmentation. Nature methods, 18(2), 203-211.
#######################################################################

2025-10-05 10:46:28.199014: do_dummy_2d_data_aug: True
2025-10-05 10:46:28.199504: Using splits from existing split file: /nfs/erelab001/shared/Computational_Group/Naravich/datasets/nnUNet_Datasets/nnUNet_preprocessed/Dataset310_nnInteractive_Calcium_OCT_CrossValidation/splits_final_60.json
2025-10-05 10:46:28.200064: The split file contains 5 splits.
2025-10-05 10:46:28.200247: Desired fold for training: 0
2025-10-05 10:46:28.200431: This split has 4 training and 4 validation cases.
/nfs/erelab001/shared/Computational_Group/Naravich/nnUNet/.venv/lib/python3.12/site-packages/fft_conv_pytorch/fft_conv.py:139: UserWarning: Using a non-tuple sequence for multidimensional indexing is deprecated and will be changed in pytorch 2.9; use x[tuple(seq)] instead of x[seq]. In pytorch 2.9 this will be interpreted as tensor index, x[torch.tensor(seq)], which will result either in an error or a different result (Triggered internally at /pytorch/torch/csrc/autograd/python_variable_indexing.cpp:306.)
  output = output[crop_slices].contiguous()
/nfs/erelab001/shared/Computational_Group/Naravich/nnUNet/.venv/lib/python3.12/site-packages/fft_conv_pytorch/fft_conv.py:139: UserWarning: Using a non-tuple sequence for multidimensional indexing is deprecated and will be changed in pytorch 2.9; use x[tuple(seq)] instead of x[seq]. In pytorch 2.9 this will be interpreted as tensor index, x[torch.tensor(seq)], which will result either in an error or a different result (Triggered internally at /pytorch/torch/csrc/autograd/python_variable_indexing.cpp:306.)
  output = output[crop_slices].contiguous()
/nfs/erelab001/shared/Computational_Group/Naravich/nnUNet/.venv/lib/python3.12/site-packages/fft_conv_pytorch/fft_conv.py:139: UserWarning: Using a non-tuple sequence for multidimensional indexing is deprecated and will be changed in pytorch 2.9; use x[tuple(seq)] instead of x[seq]. In pytorch 2.9 this will be interpreted as tensor index, x[torch.tensor(seq)], which will result either in an error or a different result (Triggered internally at /pytorch/torch/csrc/autograd/python_variable_indexing.cpp:306.)
  output = output[crop_slices].contiguous()
/nfs/erelab001/shared/Computational_Group/Naravich/nnUNet/.venv/lib/python3.12/site-packages/fft_conv_pytorch/fft_conv.py:139: UserWarning: Using a non-tuple sequence for multidimensional indexing is deprecated and will be changed in pytorch 2.9; use x[tuple(seq)] instead of x[seq]. In pytorch 2.9 this will be interpreted as tensor index, x[torch.tensor(seq)], which will result either in an error or a different result (Triggered internally at /pytorch/torch/csrc/autograd/python_variable_indexing.cpp:306.)
  output = output[crop_slices].contiguous()
/nfs/erelab001/shared/Computational_Group/Naravich/nnUNet/.venv/lib/python3.12/site-packages/fft_conv_pytorch/fft_conv.py:139: UserWarning: Using a non-tuple sequence for multidimensional indexing is deprecated and will be changed in pytorch 2.9; use x[tuple(seq)] instead of x[seq]. In pytorch 2.9 this will be interpreted as tensor index, x[torch.tensor(seq)], which will result either in an error or a different result (Triggered internally at /pytorch/torch/csrc/autograd/python_variable_indexing.cpp:306.)
  output = output[crop_slices].contiguous()
/nfs/erelab001/shared/Computational_Group/Naravich/nnUNet/.venv/lib/python3.12/site-packages/fft_conv_pytorch/fft_conv.py:139: UserWarning: Using a non-tuple sequence for multidimensional indexing is deprecated and will be changed in pytorch 2.9; use x[tuple(seq)] instead of x[seq]. In pytorch 2.9 this will be interpreted as tensor index, x[torch.tensor(seq)], which will result either in an error or a different result (Triggered internally at /pytorch/torch/csrc/autograd/python_variable_indexing.cpp:306.)
  output = output[crop_slices].contiguous()
/nfs/erelab001/shared/Computational_Group/Naravich/nnUNet/.venv/lib/python3.12/site-packages/fft_conv_pytorch/fft_conv.py:139: UserWarning: Using a non-tuple sequence for multidimensional indexing is deprecated and will be changed in pytorch 2.9; use x[tuple(seq)] instead of x[seq]. In pytorch 2.9 this will be interpreted as tensor index, x[torch.tensor(seq)], which will result either in an error or a different result (Triggered internally at /pytorch/torch/csrc/autograd/python_variable_indexing.cpp:306.)
  output = output[crop_slices].contiguous()
/nfs/erelab001/shared/Computational_Group/Naravich/nnUNet/.venv/lib/python3.12/site-packages/fft_conv_pytorch/fft_conv.py:139: UserWarning: Using a non-tuple sequence for multidimensional indexing is deprecated and will be changed in pytorch 2.9; use x[tuple(seq)] instead of x[seq]. In pytorch 2.9 this will be interpreted as tensor index, x[torch.tensor(seq)], which will result either in an error or a different result (Triggered internally at /pytorch/torch/csrc/autograd/python_variable_indexing.cpp:306.)
  output = output[crop_slices].contiguous()
/nfs/erelab001/shared/Computational_Group/Naravich/nnUNet/.venv/lib/python3.12/site-packages/fft_conv_pytorch/fft_conv.py:139: UserWarning: Using a non-tuple sequence for multidimensional indexing is deprecated and will be changed in pytorch 2.9; use x[tuple(seq)] instead of x[seq]. In pytorch 2.9 this will be interpreted as tensor index, x[torch.tensor(seq)], which will result either in an error or a different result (Triggered internally at /pytorch/torch/csrc/autograd/python_variable_indexing.cpp:306.)
  output = output[crop_slices].contiguous()
/nfs/erelab001/shared/Computational_Group/Naravich/nnUNet/.venv/lib/python3.12/site-packages/fft_conv_pytorch/fft_conv.py:139: UserWarning: Using a non-tuple sequence for multidimensional indexing is deprecated and will be changed in pytorch 2.9; use x[tuple(seq)] instead of x[seq]. In pytorch 2.9 this will be interpreted as tensor index, x[torch.tensor(seq)], which will result either in an error or a different result (Triggered internally at /pytorch/torch/csrc/autograd/python_variable_indexing.cpp:306.)
  output = output[crop_slices].contiguous()
/nfs/erelab001/shared/Computational_Group/Naravich/nnUNet/.venv/lib/python3.12/site-packages/fft_conv_pytorch/fft_conv.py:139: UserWarning: Using a non-tuple sequence for multidimensional indexing is deprecated and will be changed in pytorch 2.9; use x[tuple(seq)] instead of x[seq]. In pytorch 2.9 this will be interpreted as tensor index, x[torch.tensor(seq)], which will result either in an error or a different result (Triggered internally at /pytorch/torch/csrc/autograd/python_variable_indexing.cpp:306.)
  output = output[crop_slices].contiguous()
/nfs/erelab001/shared/Computational_Group/Naravich/nnUNet/.venv/lib/python3.12/site-packages/fft_conv_pytorch/fft_conv.py:139: UserWarning: Using a non-tuple sequence for multidimensional indexing is deprecated and will be changed in pytorch 2.9; use x[tuple(seq)] instead of x[seq]. In pytorch 2.9 this will be interpreted as tensor index, x[torch.tensor(seq)], which will result either in an error or a different result (Triggered internally at /pytorch/torch/csrc/autograd/python_variable_indexing.cpp:306.)
  output = output[crop_slices].contiguous()
using pin_memory on device 0
using pin_memory on device 0
2025-10-05 10:46:34.209773: Using torch.compile...

This is the configuration used by this training:
Configuration name: 3d_32x160x128_b10
 {'data_identifier': 'nnUNetPlans_3d_fullres', 'preprocessor_name': 'DefaultPreprocessor', 'batch_size': 10, 'patch_size': [32, 160, 128], 'median_image_size_in_voxels': [375.0, 498.0, 498.0], 'spacing': [1.0, 1.0, 1.0], 'normalization_schemes': ['ZScoreNormalization'], 'use_mask_for_norm': [False], 'resampling_fn_data': 'resample_data_or_seg_to_shape', 'resampling_fn_seg': 'resample_data_or_seg_to_shape', 'resampling_fn_data_kwargs': {'is_seg': False, 'order': 3, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_seg_kwargs': {'is_seg': True, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_probabilities': 'resample_data_or_seg_to_shape', 'resampling_fn_probabilities_kwargs': {'is_seg': False, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'architecture': {'network_class_name': 'dynamic_network_architectures.architectures.unet.PlainConvUNet', 'arch_kwargs': {'n_stages': 6, 'features_per_stage': [32, 64, 128, 256, 320, 320], 'conv_op': 'torch.nn.modules.conv.Conv3d', 'kernel_sizes': [[3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3]], 'strides': [[1, 1, 1], [2, 2, 2], [2, 2, 2], [2, 2, 2], [2, 2, 2], [1, 2, 2]], 'n_conv_per_stage': [2, 2, 2, 2, 2, 2], 'n_conv_per_stage_decoder': [2, 2, 2, 2, 2], 'conv_bias': True, 'norm_op': 'torch.nn.modules.instancenorm.InstanceNorm3d', 'norm_op_kwargs': {'eps': 1e-05, 'affine': True}, 'dropout_op': None, 'dropout_op_kwargs': None, 'nonlin': 'torch.nn.LeakyReLU', 'nonlin_kwargs': {'inplace': True}, 'deep_supervision': True}, '_kw_requires_import': ['conv_op', 'norm_op', 'dropout_op', 'nonlin']}, 'batch_dice': True, 'inherits_from': '3d_fullres'} 

These are the global plan.json settings:
 {'dataset_name': 'Dataset310_nnInteractive_Calcium_OCT_CrossValidation', 'plans_name': 'nnUNetPlans', 'original_median_spacing_after_transp': [1.0, 1.0, 1.0], 'original_median_shape_after_transp': [375, 498, 498], 'image_reader_writer': 'SimpleITKIO', 'transpose_forward': [0, 1, 2], 'transpose_backward': [0, 1, 2], 'experiment_planner_used': 'ExperimentPlanner', 'label_manager': 'LabelManager', 'foreground_intensity_properties_per_channel': {'0': {'max': 0.4977976381778717, 'mean': 0.1394134759902954, 'median': 0.09849607944488525, 'min': 0.0, 'percentile_00_5': 0.015305490233004093, 'percentile_99_5': 0.4977976381778717, 'std': 0.121165432035923}}} 

2025-10-05 10:46:35.934826: unpacking dataset...
/nfs/erelab001/shared/Computational_Group/Naravich/nnUNet/.venv/lib/python3.12/site-packages/torch/_inductor/lowering.py:7095: UserWarning: 
Online softmax is disabled on the fly since Inductor decides to
split the reduction. Cut an issue to PyTorch if this is an
important use case and you want to speed it up with online
softmax.

  warnings.warn(
/nfs/erelab001/shared/Computational_Group/Naravich/nnUNet/.venv/lib/python3.12/site-packages/torch/_inductor/lowering.py:7095: UserWarning: 
Online softmax is disabled on the fly since Inductor decides to
split the reduction. Cut an issue to PyTorch if this is an
important use case and you want to speed it up with online
softmax.

  warnings.warn(
/nfs/erelab001/shared/Computational_Group/Naravich/nnUNet/.venv/lib/python3.12/site-packages/torch/_inductor/lowering.py:7095: UserWarning: 
Online softmax is disabled on the fly since Inductor decides to
split the reduction. Cut an issue to PyTorch if this is an
important use case and you want to speed it up with online
softmax.

  warnings.warn(
/nfs/erelab001/shared/Computational_Group/Naravich/nnUNet/.venv/lib/python3.12/site-packages/torch/_inductor/lowering.py:7095: UserWarning: 
Online softmax is disabled on the fly since Inductor decides to
split the reduction. Cut an issue to PyTorch if this is an
important use case and you want to speed it up with online
softmax.

  warnings.warn(
2025-10-05 10:46:40.215514: unpacking done...
2025-10-05 10:46:40.228786: Unable to plot network architecture: nnUNet_compile is enabled!
2025-10-05 10:46:40.236008: 
2025-10-05 10:46:40.236168: Epoch 0
2025-10-05 10:46:40.236329: Current learning rate: 0.01
2025-10-05 10:48:00.431145: Validation loss improved from 1000.00000 to -0.21013! Patience: 0/50
2025-10-05 10:48:00.431829: train_loss -0.1268
2025-10-05 10:48:00.431987: val_loss -0.2101
2025-10-05 10:48:00.432176: Pseudo dice [np.float32(0.5616)]
2025-10-05 10:48:00.432323: Epoch time: 80.2 s
2025-10-05 10:48:00.432454: Yayy! New best EMA pseudo Dice: 0.5616000294685364
2025-10-05 10:48:01.352090: 
2025-10-05 10:48:01.352399: Epoch 1
2025-10-05 10:48:01.352593: Current learning rate: 0.00994
2025-10-05 10:48:47.117026: Validation loss improved from -0.21013 to -0.21595! Patience: 0/50
2025-10-05 10:48:47.117539: train_loss -0.2485
2025-10-05 10:48:47.117711: val_loss -0.216
2025-10-05 10:48:47.117851: Pseudo dice [np.float32(0.564)]
2025-10-05 10:48:47.117994: Epoch time: 45.77 s
2025-10-05 10:48:47.118185: Yayy! New best EMA pseudo Dice: 0.5619000196456909
2025-10-05 10:48:48.152140: 
2025-10-05 10:48:48.152450: Epoch 2
2025-10-05 10:48:48.152639: Current learning rate: 0.00988
2025-10-05 10:49:34.045995: Validation loss did not improve from -0.21595. Patience: 1/50
2025-10-05 10:49:34.046401: train_loss -0.3065
2025-10-05 10:49:34.046526: val_loss -0.1505
2025-10-05 10:49:34.046636: Pseudo dice [np.float32(0.4871)]
2025-10-05 10:49:34.046761: Epoch time: 45.89 s
2025-10-05 10:49:34.706858: 
2025-10-05 10:49:34.707108: Epoch 3
2025-10-05 10:49:34.707293: Current learning rate: 0.00982
2025-10-05 10:50:20.546377: Validation loss improved from -0.21595 to -0.33176! Patience: 1/50
2025-10-05 10:50:20.546884: train_loss -0.369
2025-10-05 10:50:20.547016: val_loss -0.3318
2025-10-05 10:50:20.547145: Pseudo dice [np.float32(0.6335)]
2025-10-05 10:50:20.547299: Epoch time: 45.84 s
2025-10-05 10:50:20.547406: Yayy! New best EMA pseudo Dice: 0.5623000264167786
2025-10-05 10:50:21.590269: 
2025-10-05 10:50:21.590548: Epoch 4
2025-10-05 10:50:21.590728: Current learning rate: 0.00976
2025-10-05 10:51:07.454499: Validation loss did not improve from -0.33176. Patience: 1/50
2025-10-05 10:51:07.455058: train_loss -0.4132
2025-10-05 10:51:07.455293: val_loss -0.3254
2025-10-05 10:51:07.455526: Pseudo dice [np.float32(0.617)]
2025-10-05 10:51:07.455758: Epoch time: 45.87 s
2025-10-05 10:51:07.850460: Yayy! New best EMA pseudo Dice: 0.567799985408783
2025-10-05 10:51:09.037388: 
2025-10-05 10:51:09.037715: Epoch 5
2025-10-05 10:51:09.037966: Current learning rate: 0.0097
2025-10-05 10:51:54.912998: Validation loss improved from -0.33176 to -0.33868! Patience: 1/50
2025-10-05 10:51:54.913578: train_loss -0.4353
2025-10-05 10:51:54.913751: val_loss -0.3387
2025-10-05 10:51:54.913922: Pseudo dice [np.float32(0.6356)]
2025-10-05 10:51:54.914065: Epoch time: 45.88 s
2025-10-05 10:51:54.914209: Yayy! New best EMA pseudo Dice: 0.5745000243186951
2025-10-05 10:51:55.955592: 
2025-10-05 10:51:55.955944: Epoch 6
2025-10-05 10:51:55.956166: Current learning rate: 0.00964
2025-10-05 10:52:41.880338: Validation loss did not improve from -0.33868. Patience: 1/50
2025-10-05 10:52:41.881040: train_loss -0.4636
2025-10-05 10:52:41.881290: val_loss -0.3132
2025-10-05 10:52:41.881504: Pseudo dice [np.float32(0.6155)]
2025-10-05 10:52:41.881720: Epoch time: 45.93 s
2025-10-05 10:52:41.881953: Yayy! New best EMA pseudo Dice: 0.5785999894142151
2025-10-05 10:52:42.932873: 
2025-10-05 10:52:42.933259: Epoch 7
2025-10-05 10:52:42.933476: Current learning rate: 0.00958
2025-10-05 10:53:28.824239: Validation loss improved from -0.33868 to -0.33868! Patience: 1/50
2025-10-05 10:53:28.824718: train_loss -0.4755
2025-10-05 10:53:28.824858: val_loss -0.3387
2025-10-05 10:53:28.824974: Pseudo dice [np.float32(0.6298)]
2025-10-05 10:53:28.825115: Epoch time: 45.89 s
2025-10-05 10:53:28.825304: Yayy! New best EMA pseudo Dice: 0.5838000178337097
2025-10-05 10:53:29.878020: 
2025-10-05 10:53:29.878302: Epoch 8
2025-10-05 10:53:29.878620: Current learning rate: 0.00952
2025-10-05 10:54:15.689376: Validation loss improved from -0.33868 to -0.36062! Patience: 0/50
2025-10-05 10:54:15.690121: train_loss -0.4938
2025-10-05 10:54:15.690362: val_loss -0.3606
2025-10-05 10:54:15.690589: Pseudo dice [np.float32(0.6294)]
2025-10-05 10:54:15.690887: Epoch time: 45.81 s
2025-10-05 10:54:15.691124: Yayy! New best EMA pseudo Dice: 0.5882999897003174
2025-10-05 10:54:16.745955: 
2025-10-05 10:54:16.746337: Epoch 9
2025-10-05 10:54:16.746629: Current learning rate: 0.00946
2025-10-05 10:55:02.583850: Validation loss improved from -0.36062 to -0.36219! Patience: 0/50
2025-10-05 10:55:02.584545: train_loss -0.5034
2025-10-05 10:55:02.584796: val_loss -0.3622
2025-10-05 10:55:02.585034: Pseudo dice [np.float32(0.6472)]
2025-10-05 10:55:02.585285: Epoch time: 45.84 s
2025-10-05 10:55:03.014723: Yayy! New best EMA pseudo Dice: 0.5942000150680542
2025-10-05 10:55:04.037768: 
2025-10-05 10:55:04.038000: Epoch 10
2025-10-05 10:55:04.038267: Current learning rate: 0.0094
2025-10-05 10:55:49.820486: Validation loss improved from -0.36219 to -0.36335! Patience: 0/50
2025-10-05 10:55:49.821089: train_loss -0.5259
2025-10-05 10:55:49.821339: val_loss -0.3634
2025-10-05 10:55:49.821562: Pseudo dice [np.float32(0.658)]
2025-10-05 10:55:49.821811: Epoch time: 45.78 s
2025-10-05 10:55:49.821949: Yayy! New best EMA pseudo Dice: 0.600600004196167
2025-10-05 10:55:50.875221: 
2025-10-05 10:55:50.875602: Epoch 11
2025-10-05 10:55:50.875849: Current learning rate: 0.00934
2025-10-05 10:56:36.675256: Validation loss did not improve from -0.36335. Patience: 1/50
2025-10-05 10:56:36.675841: train_loss -0.524
2025-10-05 10:56:36.675997: val_loss -0.3579
2025-10-05 10:56:36.676178: Pseudo dice [np.float32(0.6454)]
2025-10-05 10:56:36.676392: Epoch time: 45.8 s
2025-10-05 10:56:36.676513: Yayy! New best EMA pseudo Dice: 0.6050999760627747
2025-10-05 10:56:38.081106: 
2025-10-05 10:56:38.081443: Epoch 12
2025-10-05 10:56:38.081614: Current learning rate: 0.00928
2025-10-05 10:57:23.846184: Validation loss improved from -0.36335 to -0.36739! Patience: 1/50
2025-10-05 10:57:23.846822: train_loss -0.5537
2025-10-05 10:57:23.847072: val_loss -0.3674
2025-10-05 10:57:23.847252: Pseudo dice [np.float32(0.6483)]
2025-10-05 10:57:23.847445: Epoch time: 45.77 s
2025-10-05 10:57:23.847623: Yayy! New best EMA pseudo Dice: 0.6093999743461609
2025-10-05 10:57:24.887247: 
2025-10-05 10:57:24.887512: Epoch 13
2025-10-05 10:57:24.887680: Current learning rate: 0.00922
2025-10-05 10:58:10.621510: Validation loss improved from -0.36739 to -0.38722! Patience: 0/50
2025-10-05 10:58:10.622109: train_loss -0.5456
2025-10-05 10:58:10.622320: val_loss -0.3872
2025-10-05 10:58:10.622466: Pseudo dice [np.float32(0.6628)]
2025-10-05 10:58:10.622603: Epoch time: 45.74 s
2025-10-05 10:58:10.622727: Yayy! New best EMA pseudo Dice: 0.6147000193595886
2025-10-05 10:58:11.675429: 
2025-10-05 10:58:11.675872: Epoch 14
2025-10-05 10:58:11.676201: Current learning rate: 0.00916
2025-10-05 10:58:57.455670: Validation loss improved from -0.38722 to -0.39551! Patience: 0/50
2025-10-05 10:58:57.456400: train_loss -0.5606
2025-10-05 10:58:57.456705: val_loss -0.3955
2025-10-05 10:58:57.456951: Pseudo dice [np.float32(0.6759)]
2025-10-05 10:58:57.457253: Epoch time: 45.78 s
2025-10-05 10:58:57.953743: Yayy! New best EMA pseudo Dice: 0.6208000183105469
2025-10-05 10:58:59.074493: 
2025-10-05 10:58:59.074852: Epoch 15
2025-10-05 10:58:59.075082: Current learning rate: 0.0091
2025-10-05 10:59:44.869325: Validation loss did not improve from -0.39551. Patience: 1/50
2025-10-05 10:59:44.870262: train_loss -0.5692
2025-10-05 10:59:44.870701: val_loss -0.3618
2025-10-05 10:59:44.871093: Pseudo dice [np.float32(0.6555)]
2025-10-05 10:59:44.871569: Epoch time: 45.8 s
2025-10-05 10:59:44.871895: Yayy! New best EMA pseudo Dice: 0.6243000030517578
2025-10-05 10:59:45.996948: 
2025-10-05 10:59:45.997458: Epoch 16
2025-10-05 10:59:45.997858: Current learning rate: 0.00903
2025-10-05 11:00:31.807168: Validation loss did not improve from -0.39551. Patience: 2/50
2025-10-05 11:00:31.808171: train_loss -0.5844
2025-10-05 11:00:31.808496: val_loss -0.3826
2025-10-05 11:00:31.808783: Pseudo dice [np.float32(0.6567)]
2025-10-05 11:00:31.808926: Epoch time: 45.81 s
2025-10-05 11:00:31.809077: Yayy! New best EMA pseudo Dice: 0.6276000142097473
2025-10-05 11:00:32.894278: 
2025-10-05 11:00:32.894695: Epoch 17
2025-10-05 11:00:32.894962: Current learning rate: 0.00897
2025-10-05 11:01:18.708173: Validation loss did not improve from -0.39551. Patience: 3/50
2025-10-05 11:01:18.709040: train_loss -0.5832
2025-10-05 11:01:18.709304: val_loss -0.3701
2025-10-05 11:01:18.709568: Pseudo dice [np.float32(0.6475)]
2025-10-05 11:01:18.709817: Epoch time: 45.82 s
2025-10-05 11:01:18.710033: Yayy! New best EMA pseudo Dice: 0.6295999884605408
2025-10-05 11:01:19.790931: 
2025-10-05 11:01:19.791285: Epoch 18
2025-10-05 11:01:19.791538: Current learning rate: 0.00891
2025-10-05 11:02:05.604410: Validation loss did not improve from -0.39551. Patience: 4/50
2025-10-05 11:02:05.605063: train_loss -0.6083
2025-10-05 11:02:05.605309: val_loss -0.3036
2025-10-05 11:02:05.605474: Pseudo dice [np.float32(0.6072)]
2025-10-05 11:02:05.605629: Epoch time: 45.81 s
2025-10-05 11:02:06.251829: 
2025-10-05 11:02:06.252185: Epoch 19
2025-10-05 11:02:06.252377: Current learning rate: 0.00885
2025-10-05 11:02:52.073834: Validation loss did not improve from -0.39551. Patience: 5/50
2025-10-05 11:02:52.074462: train_loss -0.5869
2025-10-05 11:02:52.074655: val_loss -0.3932
2025-10-05 11:02:52.074820: Pseudo dice [np.float32(0.6503)]
2025-10-05 11:02:52.074993: Epoch time: 45.82 s
2025-10-05 11:02:52.517600: Yayy! New best EMA pseudo Dice: 0.6295999884605408
2025-10-05 11:02:53.567310: 
2025-10-05 11:02:53.567581: Epoch 20
2025-10-05 11:02:53.567753: Current learning rate: 0.00879
2025-10-05 11:03:39.394549: Validation loss did not improve from -0.39551. Patience: 6/50
2025-10-05 11:03:39.395300: train_loss -0.6048
2025-10-05 11:03:39.395655: val_loss -0.3763
2025-10-05 11:03:39.395881: Pseudo dice [np.float32(0.6425)]
2025-10-05 11:03:39.396100: Epoch time: 45.83 s
2025-10-05 11:03:39.396261: Yayy! New best EMA pseudo Dice: 0.6309000253677368
2025-10-05 11:03:40.461274: 
2025-10-05 11:03:40.461620: Epoch 21
2025-10-05 11:03:40.461837: Current learning rate: 0.00873
2025-10-05 11:04:26.300998: Validation loss did not improve from -0.39551. Patience: 7/50
2025-10-05 11:04:26.301823: train_loss -0.6015
2025-10-05 11:04:26.302229: val_loss -0.3863
2025-10-05 11:04:26.302605: Pseudo dice [np.float32(0.6735)]
2025-10-05 11:04:26.302901: Epoch time: 45.84 s
2025-10-05 11:04:26.303190: Yayy! New best EMA pseudo Dice: 0.635200023651123
2025-10-05 11:04:27.472712: 
2025-10-05 11:04:27.473088: Epoch 22
2025-10-05 11:04:27.473404: Current learning rate: 0.00867
2025-10-05 11:05:13.342571: Validation loss did not improve from -0.39551. Patience: 8/50
2025-10-05 11:05:13.343214: train_loss -0.6067
2025-10-05 11:05:13.343401: val_loss -0.3558
2025-10-05 11:05:13.343631: Pseudo dice [np.float32(0.6361)]
2025-10-05 11:05:13.343816: Epoch time: 45.87 s
2025-10-05 11:05:13.344009: Yayy! New best EMA pseudo Dice: 0.6352999806404114
2025-10-05 11:05:14.403122: 
2025-10-05 11:05:14.403379: Epoch 23
2025-10-05 11:05:14.403639: Current learning rate: 0.00861
2025-10-05 11:06:00.339103: Validation loss improved from -0.39551 to -0.40931! Patience: 8/50
2025-10-05 11:06:00.339685: train_loss -0.62
2025-10-05 11:06:00.339868: val_loss -0.4093
2025-10-05 11:06:00.339988: Pseudo dice [np.float32(0.683)]
2025-10-05 11:06:00.340144: Epoch time: 45.94 s
2025-10-05 11:06:00.340260: Yayy! New best EMA pseudo Dice: 0.6399999856948853
2025-10-05 11:06:01.387073: 
2025-10-05 11:06:01.387419: Epoch 24
2025-10-05 11:06:01.387611: Current learning rate: 0.00855
2025-10-05 11:06:47.343567: Validation loss did not improve from -0.40931. Patience: 1/50
2025-10-05 11:06:47.344180: train_loss -0.6144
2025-10-05 11:06:47.344366: val_loss -0.3912
2025-10-05 11:06:47.344581: Pseudo dice [np.float32(0.6587)]
2025-10-05 11:06:47.344733: Epoch time: 45.96 s
2025-10-05 11:06:47.763055: Yayy! New best EMA pseudo Dice: 0.6419000029563904
2025-10-05 11:06:48.805204: 
2025-10-05 11:06:48.805505: Epoch 25
2025-10-05 11:06:48.805681: Current learning rate: 0.00849
2025-10-05 11:07:34.861263: Validation loss did not improve from -0.40931. Patience: 2/50
2025-10-05 11:07:34.861953: train_loss -0.6297
2025-10-05 11:07:34.862187: val_loss -0.4087
2025-10-05 11:07:34.862413: Pseudo dice [np.float32(0.6707)]
2025-10-05 11:07:34.862695: Epoch time: 46.06 s
2025-10-05 11:07:34.862824: Yayy! New best EMA pseudo Dice: 0.6448000073432922
2025-10-05 11:07:35.950391: 
2025-10-05 11:07:35.950746: Epoch 26
2025-10-05 11:07:35.950960: Current learning rate: 0.00843
2025-10-05 11:08:21.959574: Validation loss did not improve from -0.40931. Patience: 3/50
2025-10-05 11:08:21.960176: train_loss -0.6455
2025-10-05 11:08:21.960342: val_loss -0.3437
2025-10-05 11:08:21.960478: Pseudo dice [np.float32(0.6405)]
2025-10-05 11:08:21.960644: Epoch time: 46.01 s
2025-10-05 11:08:22.597476: 
2025-10-05 11:08:22.597854: Epoch 27
2025-10-05 11:08:22.598197: Current learning rate: 0.00836
2025-10-05 11:09:09.044176: Validation loss improved from -0.40931 to -0.42002! Patience: 3/50
2025-10-05 11:09:09.044759: train_loss -0.6396
2025-10-05 11:09:09.044905: val_loss -0.42
2025-10-05 11:09:09.045028: Pseudo dice [np.float32(0.6926)]
2025-10-05 11:09:09.045144: Epoch time: 46.45 s
2025-10-05 11:09:09.045251: Yayy! New best EMA pseudo Dice: 0.6492000222206116
2025-10-05 11:09:10.127708: 
2025-10-05 11:09:10.128007: Epoch 28
2025-10-05 11:09:10.128187: Current learning rate: 0.0083
2025-10-05 11:09:56.188004: Validation loss did not improve from -0.42002. Patience: 1/50
2025-10-05 11:09:56.188736: train_loss -0.6387
2025-10-05 11:09:56.188936: val_loss -0.3256
2025-10-05 11:09:56.189083: Pseudo dice [np.float32(0.6211)]
2025-10-05 11:09:56.189210: Epoch time: 46.06 s
2025-10-05 11:09:56.822150: 
2025-10-05 11:09:56.822545: Epoch 29
2025-10-05 11:09:56.822749: Current learning rate: 0.00824
2025-10-05 11:10:42.791136: Validation loss did not improve from -0.42002. Patience: 2/50
2025-10-05 11:10:42.791694: train_loss -0.6575
2025-10-05 11:10:42.791855: val_loss -0.4075
2025-10-05 11:10:42.791993: Pseudo dice [np.float32(0.6705)]
2025-10-05 11:10:42.792210: Epoch time: 45.97 s
2025-10-05 11:10:43.873443: 
2025-10-05 11:10:43.873700: Epoch 30
2025-10-05 11:10:43.873892: Current learning rate: 0.00818
2025-10-05 11:11:29.889560: Validation loss did not improve from -0.42002. Patience: 3/50
2025-10-05 11:11:29.890080: train_loss -0.6555
2025-10-05 11:11:29.890266: val_loss -0.3843
2025-10-05 11:11:29.890421: Pseudo dice [np.float32(0.657)]
2025-10-05 11:11:29.890554: Epoch time: 46.02 s
2025-10-05 11:11:29.890670: Yayy! New best EMA pseudo Dice: 0.6496000289916992
2025-10-05 11:11:30.958036: 
2025-10-05 11:11:30.958498: Epoch 31
2025-10-05 11:11:30.958795: Current learning rate: 0.00812
2025-10-05 11:12:16.921959: Validation loss did not improve from -0.42002. Patience: 4/50
2025-10-05 11:12:16.922508: train_loss -0.6587
2025-10-05 11:12:16.922694: val_loss -0.385
2025-10-05 11:12:16.922834: Pseudo dice [np.float32(0.6562)]
2025-10-05 11:12:16.922984: Epoch time: 45.97 s
2025-10-05 11:12:16.923103: Yayy! New best EMA pseudo Dice: 0.6503000259399414
2025-10-05 11:12:17.985531: 
2025-10-05 11:12:17.985886: Epoch 32
2025-10-05 11:12:17.986117: Current learning rate: 0.00806
2025-10-05 11:13:03.947500: Validation loss did not improve from -0.42002. Patience: 5/50
2025-10-05 11:13:03.948161: train_loss -0.6598
2025-10-05 11:13:03.948446: val_loss -0.3968
2025-10-05 11:13:03.948661: Pseudo dice [np.float32(0.6803)]
2025-10-05 11:13:03.948879: Epoch time: 45.96 s
2025-10-05 11:13:03.949075: Yayy! New best EMA pseudo Dice: 0.6532999873161316
2025-10-05 11:13:05.016359: 
2025-10-05 11:13:05.016649: Epoch 33
2025-10-05 11:13:05.016848: Current learning rate: 0.008
2025-10-05 11:13:50.965581: Validation loss did not improve from -0.42002. Patience: 6/50
2025-10-05 11:13:50.966356: train_loss -0.6697
2025-10-05 11:13:50.966702: val_loss -0.2799
2025-10-05 11:13:50.967052: Pseudo dice [np.float32(0.5906)]
2025-10-05 11:13:50.967705: Epoch time: 45.95 s
2025-10-05 11:13:51.613920: 
2025-10-05 11:13:51.614287: Epoch 34
2025-10-05 11:13:51.614571: Current learning rate: 0.00793
2025-10-05 11:14:37.588059: Validation loss did not improve from -0.42002. Patience: 7/50
2025-10-05 11:14:37.588848: train_loss -0.6698
2025-10-05 11:14:37.589169: val_loss -0.3717
2025-10-05 11:14:37.589436: Pseudo dice [np.float32(0.6584)]
2025-10-05 11:14:37.589651: Epoch time: 45.98 s
2025-10-05 11:14:38.770544: 
2025-10-05 11:14:38.770960: Epoch 35
2025-10-05 11:14:38.771229: Current learning rate: 0.00787
2025-10-05 11:15:24.721535: Validation loss did not improve from -0.42002. Patience: 8/50
2025-10-05 11:15:24.722158: train_loss -0.678
2025-10-05 11:15:24.722294: val_loss -0.409
2025-10-05 11:15:24.722492: Pseudo dice [np.float32(0.6836)]
2025-10-05 11:15:24.722708: Epoch time: 45.95 s
2025-10-05 11:15:25.364604: 
2025-10-05 11:15:25.364945: Epoch 36
2025-10-05 11:15:25.365163: Current learning rate: 0.00781
2025-10-05 11:16:11.422182: Validation loss did not improve from -0.42002. Patience: 9/50
2025-10-05 11:16:11.422846: train_loss -0.6884
2025-10-05 11:16:11.423039: val_loss -0.398
2025-10-05 11:16:11.423192: Pseudo dice [np.float32(0.6816)]
2025-10-05 11:16:11.423398: Epoch time: 46.06 s
2025-10-05 11:16:11.423600: Yayy! New best EMA pseudo Dice: 0.654699981212616
2025-10-05 11:16:12.494834: 
2025-10-05 11:16:12.495106: Epoch 37
2025-10-05 11:16:12.495310: Current learning rate: 0.00775
2025-10-05 11:16:58.536033: Validation loss did not improve from -0.42002. Patience: 10/50
2025-10-05 11:16:58.536633: train_loss -0.6801
2025-10-05 11:16:58.536796: val_loss -0.383
2025-10-05 11:16:58.536951: Pseudo dice [np.float32(0.6726)]
2025-10-05 11:16:58.537110: Epoch time: 46.04 s
2025-10-05 11:16:58.537290: Yayy! New best EMA pseudo Dice: 0.656499981880188
2025-10-05 11:16:59.622751: 
2025-10-05 11:16:59.623024: Epoch 38
2025-10-05 11:16:59.623238: Current learning rate: 0.00769
2025-10-05 11:17:45.627059: Validation loss did not improve from -0.42002. Patience: 11/50
2025-10-05 11:17:45.627768: train_loss -0.6781
2025-10-05 11:17:45.628012: val_loss -0.2948
2025-10-05 11:17:45.628239: Pseudo dice [np.float32(0.6129)]
2025-10-05 11:17:45.628466: Epoch time: 46.01 s
2025-10-05 11:17:46.272586: 
2025-10-05 11:17:46.272918: Epoch 39
2025-10-05 11:17:46.273177: Current learning rate: 0.00763
2025-10-05 11:18:32.272558: Validation loss did not improve from -0.42002. Patience: 12/50
2025-10-05 11:18:32.273125: train_loss -0.6973
2025-10-05 11:18:32.273290: val_loss -0.3866
2025-10-05 11:18:32.273440: Pseudo dice [np.float32(0.672)]
2025-10-05 11:18:32.273616: Epoch time: 46.0 s
2025-10-05 11:18:33.374416: 
2025-10-05 11:18:33.374856: Epoch 40
2025-10-05 11:18:33.375113: Current learning rate: 0.00756
2025-10-05 11:19:19.358896: Validation loss did not improve from -0.42002. Patience: 13/50
2025-10-05 11:19:19.359442: train_loss -0.7039
2025-10-05 11:19:19.359610: val_loss -0.3761
2025-10-05 11:19:19.359752: Pseudo dice [np.float32(0.6588)]
2025-10-05 11:19:19.359901: Epoch time: 45.99 s
2025-10-05 11:19:20.014472: 
2025-10-05 11:19:20.014808: Epoch 41
2025-10-05 11:19:20.014983: Current learning rate: 0.0075
2025-10-05 11:20:06.034852: Validation loss did not improve from -0.42002. Patience: 14/50
2025-10-05 11:20:06.035660: train_loss -0.7007
2025-10-05 11:20:06.035848: val_loss -0.381
2025-10-05 11:20:06.035990: Pseudo dice [np.float32(0.6671)]
2025-10-05 11:20:06.036186: Epoch time: 46.02 s
2025-10-05 11:20:06.665333: 
2025-10-05 11:20:06.665704: Epoch 42
2025-10-05 11:20:06.665946: Current learning rate: 0.00744
2025-10-05 11:20:52.668340: Validation loss did not improve from -0.42002. Patience: 15/50
2025-10-05 11:20:52.668946: train_loss -0.7021
2025-10-05 11:20:52.669100: val_loss -0.3637
2025-10-05 11:20:52.669236: Pseudo dice [np.float32(0.6723)]
2025-10-05 11:20:52.669384: Epoch time: 46.0 s
2025-10-05 11:20:52.669523: Yayy! New best EMA pseudo Dice: 0.6575000286102295
2025-10-05 11:20:54.230314: 
2025-10-05 11:20:54.230709: Epoch 43
2025-10-05 11:20:54.230962: Current learning rate: 0.00738
2025-10-05 11:21:40.251306: Validation loss did not improve from -0.42002. Patience: 16/50
2025-10-05 11:21:40.251907: train_loss -0.7127
2025-10-05 11:21:40.252115: val_loss -0.3278
2025-10-05 11:21:40.252259: Pseudo dice [np.float32(0.6494)]
2025-10-05 11:21:40.252443: Epoch time: 46.02 s
2025-10-05 11:21:40.882855: 
2025-10-05 11:21:40.883160: Epoch 44
2025-10-05 11:21:40.883370: Current learning rate: 0.00732
2025-10-05 11:22:26.898505: Validation loss did not improve from -0.42002. Patience: 17/50
2025-10-05 11:22:26.899204: train_loss -0.7136
2025-10-05 11:22:26.899486: val_loss -0.2759
2025-10-05 11:22:26.899786: Pseudo dice [np.float32(0.6173)]
2025-10-05 11:22:26.900042: Epoch time: 46.02 s
2025-10-05 11:22:27.953584: 
2025-10-05 11:22:27.953869: Epoch 45
2025-10-05 11:22:27.954054: Current learning rate: 0.00725
2025-10-05 11:23:13.977917: Validation loss did not improve from -0.42002. Patience: 18/50
2025-10-05 11:23:13.978598: train_loss -0.7146
2025-10-05 11:23:13.978818: val_loss -0.319
2025-10-05 11:23:13.979012: Pseudo dice [np.float32(0.647)]
2025-10-05 11:23:13.979209: Epoch time: 46.03 s
2025-10-05 11:23:14.604644: 
2025-10-05 11:23:14.605019: Epoch 46
2025-10-05 11:23:14.605245: Current learning rate: 0.00719
2025-10-05 11:24:00.534643: Validation loss did not improve from -0.42002. Patience: 19/50
2025-10-05 11:24:00.535424: train_loss -0.7217
2025-10-05 11:24:00.535722: val_loss -0.3218
2025-10-05 11:24:00.535938: Pseudo dice [np.float32(0.6272)]
2025-10-05 11:24:00.536237: Epoch time: 45.93 s
2025-10-05 11:24:01.168500: 
2025-10-05 11:24:01.168828: Epoch 47
2025-10-05 11:24:01.169079: Current learning rate: 0.00713
2025-10-05 11:24:47.144972: Validation loss did not improve from -0.42002. Patience: 20/50
2025-10-05 11:24:47.145489: train_loss -0.7176
2025-10-05 11:24:47.145667: val_loss -0.323
2025-10-05 11:24:47.145794: Pseudo dice [np.float32(0.6375)]
2025-10-05 11:24:47.145966: Epoch time: 45.98 s
2025-10-05 11:24:47.777576: 
2025-10-05 11:24:47.777949: Epoch 48
2025-10-05 11:24:47.778187: Current learning rate: 0.00707
2025-10-05 11:25:33.791190: Validation loss did not improve from -0.42002. Patience: 21/50
2025-10-05 11:25:33.791887: train_loss -0.7275
2025-10-05 11:25:33.792034: val_loss -0.3624
2025-10-05 11:25:33.792150: Pseudo dice [np.float32(0.6616)]
2025-10-05 11:25:33.792280: Epoch time: 46.02 s
2025-10-05 11:25:34.428081: 
2025-10-05 11:25:34.428412: Epoch 49
2025-10-05 11:25:34.428605: Current learning rate: 0.007
2025-10-05 11:26:20.510593: Validation loss did not improve from -0.42002. Patience: 22/50
2025-10-05 11:26:20.511438: train_loss -0.729
2025-10-05 11:26:20.511893: val_loss -0.3106
2025-10-05 11:26:20.512213: Pseudo dice [np.float32(0.6339)]
2025-10-05 11:26:20.512568: Epoch time: 46.08 s
2025-10-05 11:26:21.685379: 
2025-10-05 11:26:21.685777: Epoch 50
2025-10-05 11:26:21.686064: Current learning rate: 0.00694
2025-10-05 11:27:07.826528: Validation loss did not improve from -0.42002. Patience: 23/50
2025-10-05 11:27:07.827410: train_loss -0.7408
2025-10-05 11:27:07.827866: val_loss -0.3915
2025-10-05 11:27:07.828122: Pseudo dice [np.float32(0.6933)]
2025-10-05 11:27:07.828390: Epoch time: 46.14 s
2025-10-05 11:27:08.478554: 
2025-10-05 11:27:08.478903: Epoch 51
2025-10-05 11:27:08.479107: Current learning rate: 0.00688
2025-10-05 11:27:54.633507: Validation loss did not improve from -0.42002. Patience: 24/50
2025-10-05 11:27:54.634118: train_loss -0.7353
2025-10-05 11:27:54.634408: val_loss -0.2916
2025-10-05 11:27:54.634643: Pseudo dice [np.float32(0.6139)]
2025-10-05 11:27:54.634827: Epoch time: 46.16 s
2025-10-05 11:27:55.275497: 
2025-10-05 11:27:55.275770: Epoch 52
2025-10-05 11:27:55.275962: Current learning rate: 0.00682
2025-10-05 11:28:41.387975: Validation loss did not improve from -0.42002. Patience: 25/50
2025-10-05 11:28:41.388504: train_loss -0.7396
2025-10-05 11:28:41.388677: val_loss -0.3857
2025-10-05 11:28:41.388824: Pseudo dice [np.float32(0.6705)]
2025-10-05 11:28:41.388976: Epoch time: 46.11 s
2025-10-05 11:28:42.026138: 
2025-10-05 11:28:42.026386: Epoch 53
2025-10-05 11:28:42.026616: Current learning rate: 0.00675
2025-10-05 11:29:28.056779: Validation loss did not improve from -0.42002. Patience: 26/50
2025-10-05 11:29:28.057241: train_loss -0.7483
2025-10-05 11:29:28.057371: val_loss -0.321
2025-10-05 11:29:28.057477: Pseudo dice [np.float32(0.6317)]
2025-10-05 11:29:28.057656: Epoch time: 46.03 s
2025-10-05 11:29:28.689169: 
2025-10-05 11:29:28.689466: Epoch 54
2025-10-05 11:29:28.689635: Current learning rate: 0.00669
2025-10-05 11:30:14.690394: Validation loss improved from -0.42002 to -0.42810! Patience: 26/50
2025-10-05 11:30:14.691062: train_loss -0.7375
2025-10-05 11:30:14.691279: val_loss -0.4281
2025-10-05 11:30:14.691457: Pseudo dice [np.float32(0.6904)]
2025-10-05 11:30:14.691660: Epoch time: 46.0 s
2025-10-05 11:30:15.881407: 
2025-10-05 11:30:15.881725: Epoch 55
2025-10-05 11:30:15.881996: Current learning rate: 0.00663
2025-10-05 11:31:01.902894: Validation loss did not improve from -0.42810. Patience: 1/50
2025-10-05 11:31:01.903604: train_loss -0.7477
2025-10-05 11:31:01.903835: val_loss -0.3305
2025-10-05 11:31:01.904014: Pseudo dice [np.float32(0.6387)]
2025-10-05 11:31:01.904178: Epoch time: 46.02 s
2025-10-05 11:31:02.553828: 
2025-10-05 11:31:02.554230: Epoch 56
2025-10-05 11:31:02.554465: Current learning rate: 0.00657
2025-10-05 11:31:48.543365: Validation loss did not improve from -0.42810. Patience: 2/50
2025-10-05 11:31:48.544112: train_loss -0.7516
2025-10-05 11:31:48.544296: val_loss -0.3303
2025-10-05 11:31:48.544530: Pseudo dice [np.float32(0.6569)]
2025-10-05 11:31:48.544704: Epoch time: 45.99 s
2025-10-05 11:31:49.185561: 
2025-10-05 11:31:49.185945: Epoch 57
2025-10-05 11:31:49.186143: Current learning rate: 0.0065
2025-10-05 11:32:35.253656: Validation loss did not improve from -0.42810. Patience: 3/50
2025-10-05 11:32:35.254151: train_loss -0.751
2025-10-05 11:32:35.254342: val_loss -0.3746
2025-10-05 11:32:35.254538: Pseudo dice [np.float32(0.6714)]
2025-10-05 11:32:35.254700: Epoch time: 46.07 s
2025-10-05 11:32:35.897345: 
2025-10-05 11:32:35.897681: Epoch 58
2025-10-05 11:32:35.897902: Current learning rate: 0.00644
2025-10-05 11:33:21.964834: Validation loss did not improve from -0.42810. Patience: 4/50
2025-10-05 11:33:21.965456: train_loss -0.7494
2025-10-05 11:33:21.965639: val_loss -0.2944
2025-10-05 11:33:21.965796: Pseudo dice [np.float32(0.6349)]
2025-10-05 11:33:21.965949: Epoch time: 46.07 s
2025-10-05 11:33:23.139503: 
2025-10-05 11:33:23.139877: Epoch 59
2025-10-05 11:33:23.140139: Current learning rate: 0.00638
2025-10-05 11:34:09.196475: Validation loss did not improve from -0.42810. Patience: 5/50
2025-10-05 11:34:09.197269: train_loss -0.7549
2025-10-05 11:34:09.197542: val_loss -0.3539
2025-10-05 11:34:09.197733: Pseudo dice [np.float32(0.6636)]
2025-10-05 11:34:09.197970: Epoch time: 46.06 s
2025-10-05 11:34:10.397424: 
2025-10-05 11:34:10.397746: Epoch 60
2025-10-05 11:34:10.398048: Current learning rate: 0.00631
2025-10-05 11:34:56.462357: Validation loss did not improve from -0.42810. Patience: 6/50
2025-10-05 11:34:56.462910: train_loss -0.7642
2025-10-05 11:34:56.463075: val_loss -0.3464
2025-10-05 11:34:56.463200: Pseudo dice [np.float32(0.6465)]
2025-10-05 11:34:56.463364: Epoch time: 46.07 s
2025-10-05 11:34:57.108392: 
2025-10-05 11:34:57.108837: Epoch 61
2025-10-05 11:34:57.109046: Current learning rate: 0.00625
2025-10-05 11:35:43.316266: Validation loss did not improve from -0.42810. Patience: 7/50
2025-10-05 11:35:43.316841: train_loss -0.7663
2025-10-05 11:35:43.317005: val_loss -0.2602
2025-10-05 11:35:43.317182: Pseudo dice [np.float32(0.6293)]
2025-10-05 11:35:43.317347: Epoch time: 46.21 s
2025-10-05 11:35:43.964609: 
2025-10-05 11:35:43.964883: Epoch 62
2025-10-05 11:35:43.965079: Current learning rate: 0.00619
2025-10-05 11:36:30.064154: Validation loss did not improve from -0.42810. Patience: 8/50
2025-10-05 11:36:30.064879: train_loss -0.7743
2025-10-05 11:36:30.065140: val_loss -0.2941
2025-10-05 11:36:30.065375: Pseudo dice [np.float32(0.6309)]
2025-10-05 11:36:30.065616: Epoch time: 46.1 s
2025-10-05 11:36:30.721988: 
2025-10-05 11:36:30.722320: Epoch 63
2025-10-05 11:36:30.722511: Current learning rate: 0.00612
2025-10-05 11:37:16.837249: Validation loss did not improve from -0.42810. Patience: 9/50
2025-10-05 11:37:16.837801: train_loss -0.7799
2025-10-05 11:37:16.837966: val_loss -0.3166
2025-10-05 11:37:16.838114: Pseudo dice [np.float32(0.6486)]
2025-10-05 11:37:16.838269: Epoch time: 46.12 s
2025-10-05 11:37:17.499751: 
2025-10-05 11:37:17.500081: Epoch 64
2025-10-05 11:37:17.500289: Current learning rate: 0.00606
2025-10-05 11:38:03.568670: Validation loss did not improve from -0.42810. Patience: 10/50
2025-10-05 11:38:03.569249: train_loss -0.7723
2025-10-05 11:38:03.569386: val_loss -0.3195
2025-10-05 11:38:03.569502: Pseudo dice [np.float32(0.6469)]
2025-10-05 11:38:03.569629: Epoch time: 46.07 s
2025-10-05 11:38:04.707166: 
2025-10-05 11:38:04.707427: Epoch 65
2025-10-05 11:38:04.707649: Current learning rate: 0.006
2025-10-05 11:38:50.812310: Validation loss did not improve from -0.42810. Patience: 11/50
2025-10-05 11:38:50.812894: train_loss -0.7692
2025-10-05 11:38:50.813109: val_loss -0.3403
2025-10-05 11:38:50.813282: Pseudo dice [np.float32(0.6508)]
2025-10-05 11:38:50.813481: Epoch time: 46.11 s
2025-10-05 11:38:51.463228: 
2025-10-05 11:38:51.463507: Epoch 66
2025-10-05 11:38:51.463708: Current learning rate: 0.00593
2025-10-05 11:39:37.555087: Validation loss did not improve from -0.42810. Patience: 12/50
2025-10-05 11:39:37.555664: train_loss -0.7714
2025-10-05 11:39:37.555835: val_loss -0.34
2025-10-05 11:39:37.556003: Pseudo dice [np.float32(0.6602)]
2025-10-05 11:39:37.556147: Epoch time: 46.09 s
2025-10-05 11:39:38.205898: 
2025-10-05 11:39:38.206237: Epoch 67
2025-10-05 11:39:38.206405: Current learning rate: 0.00587
2025-10-05 11:40:24.314738: Validation loss did not improve from -0.42810. Patience: 13/50
2025-10-05 11:40:24.315326: train_loss -0.782
2025-10-05 11:40:24.315534: val_loss -0.339
2025-10-05 11:40:24.315686: Pseudo dice [np.float32(0.659)]
2025-10-05 11:40:24.315899: Epoch time: 46.11 s
2025-10-05 11:40:24.958175: 
2025-10-05 11:40:24.958605: Epoch 68
2025-10-05 11:40:24.958885: Current learning rate: 0.00581
2025-10-05 11:41:10.959266: Validation loss did not improve from -0.42810. Patience: 14/50
2025-10-05 11:41:10.959969: train_loss -0.7778
2025-10-05 11:41:10.960285: val_loss -0.3648
2025-10-05 11:41:10.960469: Pseudo dice [np.float32(0.6815)]
2025-10-05 11:41:10.960657: Epoch time: 46.0 s
2025-10-05 11:41:11.618461: 
2025-10-05 11:41:11.618804: Epoch 69
2025-10-05 11:41:11.618999: Current learning rate: 0.00574
2025-10-05 11:41:57.625557: Validation loss did not improve from -0.42810. Patience: 15/50
2025-10-05 11:41:57.626288: train_loss -0.7787
2025-10-05 11:41:57.626526: val_loss -0.3023
2025-10-05 11:41:57.626719: Pseudo dice [np.float32(0.6331)]
2025-10-05 11:41:57.626939: Epoch time: 46.01 s
2025-10-05 11:41:58.729051: 
2025-10-05 11:41:58.729348: Epoch 70
2025-10-05 11:41:58.729524: Current learning rate: 0.00568
2025-10-05 11:42:44.711352: Validation loss did not improve from -0.42810. Patience: 16/50
2025-10-05 11:42:44.711953: train_loss -0.7854
2025-10-05 11:42:44.712137: val_loss -0.2456
2025-10-05 11:42:44.712301: Pseudo dice [np.float32(0.6019)]
2025-10-05 11:42:44.712487: Epoch time: 45.98 s
2025-10-05 11:42:45.361316: 
2025-10-05 11:42:45.361712: Epoch 71
2025-10-05 11:42:45.361992: Current learning rate: 0.00562
2025-10-05 11:43:31.349442: Validation loss did not improve from -0.42810. Patience: 17/50
2025-10-05 11:43:31.349990: train_loss -0.7867
2025-10-05 11:43:31.350159: val_loss -0.2979
2025-10-05 11:43:31.350324: Pseudo dice [np.float32(0.6432)]
2025-10-05 11:43:31.350496: Epoch time: 45.99 s
2025-10-05 11:43:32.003384: 
2025-10-05 11:43:32.003769: Epoch 72
2025-10-05 11:43:32.004237: Current learning rate: 0.00555
2025-10-05 11:44:17.984096: Validation loss did not improve from -0.42810. Patience: 18/50
2025-10-05 11:44:17.984557: train_loss -0.7899
2025-10-05 11:44:17.984704: val_loss -0.2692
2025-10-05 11:44:17.984946: Pseudo dice [np.float32(0.6226)]
2025-10-05 11:44:17.985075: Epoch time: 45.98 s
2025-10-05 11:44:18.626791: 
2025-10-05 11:44:18.627040: Epoch 73
2025-10-05 11:44:18.627211: Current learning rate: 0.00549
2025-10-05 11:45:04.592186: Validation loss did not improve from -0.42810. Patience: 19/50
2025-10-05 11:45:04.592913: train_loss -0.788
2025-10-05 11:45:04.593220: val_loss -0.3601
2025-10-05 11:45:04.593519: Pseudo dice [np.float32(0.6712)]
2025-10-05 11:45:04.593729: Epoch time: 45.97 s
2025-10-05 11:45:05.239722: 
2025-10-05 11:45:05.240020: Epoch 74
2025-10-05 11:45:05.240296: Current learning rate: 0.00542
2025-10-05 11:45:51.275726: Validation loss did not improve from -0.42810. Patience: 20/50
2025-10-05 11:45:51.276236: train_loss -0.7881
2025-10-05 11:45:51.276371: val_loss -0.3219
2025-10-05 11:45:51.276484: Pseudo dice [np.float32(0.6513)]
2025-10-05 11:45:51.276602: Epoch time: 46.04 s
2025-10-05 11:45:52.846493: 
2025-10-05 11:45:52.846884: Epoch 75
2025-10-05 11:45:52.847146: Current learning rate: 0.00536
2025-10-05 11:46:38.861289: Validation loss did not improve from -0.42810. Patience: 21/50
2025-10-05 11:46:38.861817: train_loss -0.7959
2025-10-05 11:46:38.861975: val_loss -0.3196
2025-10-05 11:46:38.862125: Pseudo dice [np.float32(0.639)]
2025-10-05 11:46:38.862317: Epoch time: 46.02 s
2025-10-05 11:46:39.503761: 
2025-10-05 11:46:39.504114: Epoch 76
2025-10-05 11:46:39.504306: Current learning rate: 0.00529
2025-10-05 11:47:25.566670: Validation loss did not improve from -0.42810. Patience: 22/50
2025-10-05 11:47:25.567199: train_loss -0.8018
2025-10-05 11:47:25.567341: val_loss -0.2742
2025-10-05 11:47:25.567478: Pseudo dice [np.float32(0.6241)]
2025-10-05 11:47:25.567607: Epoch time: 46.06 s
2025-10-05 11:47:26.214584: 
2025-10-05 11:47:26.214924: Epoch 77
2025-10-05 11:47:26.215167: Current learning rate: 0.00523
2025-10-05 11:48:12.244066: Validation loss did not improve from -0.42810. Patience: 23/50
2025-10-05 11:48:12.244660: train_loss -0.806
2025-10-05 11:48:12.244871: val_loss -0.2986
2025-10-05 11:48:12.245027: Pseudo dice [np.float32(0.6501)]
2025-10-05 11:48:12.245193: Epoch time: 46.03 s
2025-10-05 11:48:12.901107: 
2025-10-05 11:48:12.901443: Epoch 78
2025-10-05 11:48:12.901612: Current learning rate: 0.00517
2025-10-05 11:48:59.068724: Validation loss did not improve from -0.42810. Patience: 24/50
2025-10-05 11:48:59.069320: train_loss -0.8031
2025-10-05 11:48:59.069469: val_loss -0.3261
2025-10-05 11:48:59.069595: Pseudo dice [np.float32(0.6706)]
2025-10-05 11:48:59.069739: Epoch time: 46.17 s
2025-10-05 11:48:59.731065: 
2025-10-05 11:48:59.731391: Epoch 79
2025-10-05 11:48:59.731597: Current learning rate: 0.0051
2025-10-05 11:49:45.857859: Validation loss did not improve from -0.42810. Patience: 25/50
2025-10-05 11:49:45.858691: train_loss -0.8073
2025-10-05 11:49:45.858906: val_loss -0.2689
2025-10-05 11:49:45.859186: Pseudo dice [np.float32(0.6248)]
2025-10-05 11:49:45.859440: Epoch time: 46.13 s
2025-10-05 11:49:47.084972: 
2025-10-05 11:49:47.085286: Epoch 80
2025-10-05 11:49:47.085572: Current learning rate: 0.00504
2025-10-05 11:50:33.182044: Validation loss did not improve from -0.42810. Patience: 26/50
2025-10-05 11:50:33.182554: train_loss -0.8061
2025-10-05 11:50:33.182702: val_loss -0.306
2025-10-05 11:50:33.182828: Pseudo dice [np.float32(0.6339)]
2025-10-05 11:50:33.182974: Epoch time: 46.1 s
2025-10-05 11:50:33.844088: 
2025-10-05 11:50:33.844341: Epoch 81
2025-10-05 11:50:33.844513: Current learning rate: 0.00497
2025-10-05 11:51:19.906028: Validation loss did not improve from -0.42810. Patience: 27/50
2025-10-05 11:51:19.906607: train_loss -0.8097
2025-10-05 11:51:19.906916: val_loss -0.2446
2025-10-05 11:51:19.907118: Pseudo dice [np.float32(0.6152)]
2025-10-05 11:51:19.907296: Epoch time: 46.06 s
2025-10-05 11:51:20.565205: 
2025-10-05 11:51:20.565551: Epoch 82
2025-10-05 11:51:20.565812: Current learning rate: 0.00491
2025-10-05 11:52:06.706220: Validation loss did not improve from -0.42810. Patience: 28/50
2025-10-05 11:52:06.706822: train_loss -0.8075
2025-10-05 11:52:06.706964: val_loss -0.2431
2025-10-05 11:52:06.707099: Pseudo dice [np.float32(0.6196)]
2025-10-05 11:52:06.707308: Epoch time: 46.14 s
2025-10-05 11:52:07.341060: 
2025-10-05 11:52:07.341432: Epoch 83
2025-10-05 11:52:07.341641: Current learning rate: 0.00484
2025-10-05 11:52:53.426575: Validation loss did not improve from -0.42810. Patience: 29/50
2025-10-05 11:52:53.427221: train_loss -0.8154
2025-10-05 11:52:53.427374: val_loss -0.2404
2025-10-05 11:52:53.427572: Pseudo dice [np.float32(0.6251)]
2025-10-05 11:52:53.427883: Epoch time: 46.09 s
2025-10-05 11:52:54.068242: 
2025-10-05 11:52:54.068593: Epoch 84
2025-10-05 11:52:54.068824: Current learning rate: 0.00478
2025-10-05 11:53:40.154031: Validation loss did not improve from -0.42810. Patience: 30/50
2025-10-05 11:53:40.154648: train_loss -0.8131
2025-10-05 11:53:40.154960: val_loss -0.2991
2025-10-05 11:53:40.155155: Pseudo dice [np.float32(0.6407)]
2025-10-05 11:53:40.155456: Epoch time: 46.09 s
2025-10-05 11:53:41.231560: 
2025-10-05 11:53:41.231811: Epoch 85
2025-10-05 11:53:41.231981: Current learning rate: 0.00471
2025-10-05 11:54:27.372154: Validation loss did not improve from -0.42810. Patience: 31/50
2025-10-05 11:54:27.372685: train_loss -0.8157
2025-10-05 11:54:27.372855: val_loss -0.2841
2025-10-05 11:54:27.372992: Pseudo dice [np.float32(0.6437)]
2025-10-05 11:54:27.373147: Epoch time: 46.14 s
2025-10-05 11:54:28.009217: 
2025-10-05 11:54:28.009487: Epoch 86
2025-10-05 11:54:28.009652: Current learning rate: 0.00465
2025-10-05 11:55:14.141634: Validation loss did not improve from -0.42810. Patience: 32/50
2025-10-05 11:55:14.142207: train_loss -0.8129
2025-10-05 11:55:14.142375: val_loss -0.2788
2025-10-05 11:55:14.142515: Pseudo dice [np.float32(0.6422)]
2025-10-05 11:55:14.142663: Epoch time: 46.13 s
2025-10-05 11:55:14.772372: 
2025-10-05 11:55:14.772673: Epoch 87
2025-10-05 11:55:14.772861: Current learning rate: 0.00458
2025-10-05 11:56:01.010944: Validation loss did not improve from -0.42810. Patience: 33/50
2025-10-05 11:56:01.011817: train_loss -0.8138
2025-10-05 11:56:01.012247: val_loss -0.2586
2025-10-05 11:56:01.012510: Pseudo dice [np.float32(0.6311)]
2025-10-05 11:56:01.012778: Epoch time: 46.24 s
2025-10-05 11:56:01.658134: 
2025-10-05 11:56:01.658403: Epoch 88
2025-10-05 11:56:01.658661: Current learning rate: 0.00452
2025-10-05 11:56:47.806664: Validation loss did not improve from -0.42810. Patience: 34/50
2025-10-05 11:56:47.807266: train_loss -0.8228
2025-10-05 11:56:47.807430: val_loss -0.2828
2025-10-05 11:56:47.807546: Pseudo dice [np.float32(0.6366)]
2025-10-05 11:56:47.807673: Epoch time: 46.15 s
2025-10-05 11:56:48.440952: 
2025-10-05 11:56:48.441328: Epoch 89
2025-10-05 11:56:48.441642: Current learning rate: 0.00445
2025-10-05 11:57:34.676096: Validation loss did not improve from -0.42810. Patience: 35/50
2025-10-05 11:57:34.676817: train_loss -0.8236
2025-10-05 11:57:34.677169: val_loss -0.2582
2025-10-05 11:57:34.677473: Pseudo dice [np.float32(0.632)]
2025-10-05 11:57:34.677709: Epoch time: 46.24 s
2025-10-05 11:57:36.287613: 
2025-10-05 11:57:36.288002: Epoch 90
2025-10-05 11:57:36.288268: Current learning rate: 0.00438
2025-10-05 11:58:22.346838: Validation loss did not improve from -0.42810. Patience: 36/50
2025-10-05 11:58:22.347600: train_loss -0.8167
2025-10-05 11:58:22.347876: val_loss -0.2775
2025-10-05 11:58:22.348117: Pseudo dice [np.float32(0.63)]
2025-10-05 11:58:22.348347: Epoch time: 46.06 s
2025-10-05 11:58:22.983223: 
2025-10-05 11:58:22.983594: Epoch 91
2025-10-05 11:58:22.983866: Current learning rate: 0.00432
2025-10-05 11:59:09.180161: Validation loss did not improve from -0.42810. Patience: 37/50
2025-10-05 11:59:09.180656: train_loss -0.8232
2025-10-05 11:59:09.180799: val_loss -0.3133
2025-10-05 11:59:09.180932: Pseudo dice [np.float32(0.654)]
2025-10-05 11:59:09.181093: Epoch time: 46.2 s
2025-10-05 11:59:09.814124: 
2025-10-05 11:59:09.814442: Epoch 92
2025-10-05 11:59:09.814605: Current learning rate: 0.00425
2025-10-05 11:59:56.044435: Validation loss did not improve from -0.42810. Patience: 38/50
2025-10-05 11:59:56.045328: train_loss -0.8249
2025-10-05 11:59:56.045592: val_loss -0.3168
2025-10-05 11:59:56.045818: Pseudo dice [np.float32(0.6508)]
2025-10-05 11:59:56.046030: Epoch time: 46.23 s
2025-10-05 11:59:56.686684: 
2025-10-05 11:59:56.687077: Epoch 93
2025-10-05 11:59:56.687279: Current learning rate: 0.00419
2025-10-05 12:00:42.947829: Validation loss did not improve from -0.42810. Patience: 39/50
2025-10-05 12:00:42.948341: train_loss -0.826
2025-10-05 12:00:42.948494: val_loss -0.3317
2025-10-05 12:00:42.948620: Pseudo dice [np.float32(0.679)]
2025-10-05 12:00:42.948769: Epoch time: 46.26 s
2025-10-05 12:00:43.583537: 
2025-10-05 12:00:43.583865: Epoch 94
2025-10-05 12:00:43.584120: Current learning rate: 0.00412
2025-10-05 12:01:29.753789: Validation loss did not improve from -0.42810. Patience: 40/50
2025-10-05 12:01:29.754342: train_loss -0.8314
2025-10-05 12:01:29.754489: val_loss -0.2582
2025-10-05 12:01:29.754627: Pseudo dice [np.float32(0.6241)]
2025-10-05 12:01:29.754781: Epoch time: 46.17 s
2025-10-05 12:01:30.834162: 
2025-10-05 12:01:30.834450: Epoch 95
2025-10-05 12:01:30.834625: Current learning rate: 0.00405
2025-10-05 12:02:16.971190: Validation loss did not improve from -0.42810. Patience: 41/50
2025-10-05 12:02:16.971780: train_loss -0.8339
2025-10-05 12:02:16.971970: val_loss -0.2245
2025-10-05 12:02:16.972111: Pseudo dice [np.float32(0.6173)]
2025-10-05 12:02:16.972256: Epoch time: 46.14 s
2025-10-05 12:02:17.608830: 
2025-10-05 12:02:17.609140: Epoch 96
2025-10-05 12:02:17.609340: Current learning rate: 0.00399
2025-10-05 12:03:03.734033: Validation loss did not improve from -0.42810. Patience: 42/50
2025-10-05 12:03:03.734590: train_loss -0.8314
2025-10-05 12:03:03.734735: val_loss -0.3092
2025-10-05 12:03:03.734919: Pseudo dice [np.float32(0.6636)]
2025-10-05 12:03:03.735108: Epoch time: 46.13 s
2025-10-05 12:03:04.371833: 
2025-10-05 12:03:04.372177: Epoch 97
2025-10-05 12:03:04.372370: Current learning rate: 0.00392
2025-10-05 12:03:50.432141: Validation loss did not improve from -0.42810. Patience: 43/50
2025-10-05 12:03:50.432658: train_loss -0.832
2025-10-05 12:03:50.432827: val_loss -0.268
2025-10-05 12:03:50.433027: Pseudo dice [np.float32(0.6435)]
2025-10-05 12:03:50.433214: Epoch time: 46.06 s
2025-10-05 12:03:51.072208: 
2025-10-05 12:03:51.072543: Epoch 98
2025-10-05 12:03:51.072768: Current learning rate: 0.00385
2025-10-05 12:04:37.160710: Validation loss did not improve from -0.42810. Patience: 44/50
2025-10-05 12:04:37.161216: train_loss -0.835
2025-10-05 12:04:37.161366: val_loss -0.2543
2025-10-05 12:04:37.161492: Pseudo dice [np.float32(0.6324)]
2025-10-05 12:04:37.161662: Epoch time: 46.09 s
2025-10-05 12:04:37.805804: 
2025-10-05 12:04:37.806098: Epoch 99
2025-10-05 12:04:37.806263: Current learning rate: 0.00379
2025-10-05 12:05:23.944146: Validation loss did not improve from -0.42810. Patience: 45/50
2025-10-05 12:05:23.944734: train_loss -0.8381
2025-10-05 12:05:23.944875: val_loss -0.2649
2025-10-05 12:05:23.945005: Pseudo dice [np.float32(0.6373)]
2025-10-05 12:05:23.945132: Epoch time: 46.14 s
2025-10-05 12:05:25.029732: 
2025-10-05 12:05:25.030082: Epoch 100
2025-10-05 12:05:25.030314: Current learning rate: 0.00372
2025-10-05 12:06:11.160104: Validation loss did not improve from -0.42810. Patience: 46/50
2025-10-05 12:06:11.160662: train_loss -0.8325
2025-10-05 12:06:11.160839: val_loss -0.3046
2025-10-05 12:06:11.160980: Pseudo dice [np.float32(0.6538)]
2025-10-05 12:06:11.161113: Epoch time: 46.13 s
2025-10-05 12:06:11.809968: 
2025-10-05 12:06:11.810225: Epoch 101
2025-10-05 12:06:11.810410: Current learning rate: 0.00365
2025-10-05 12:06:57.968136: Validation loss did not improve from -0.42810. Patience: 47/50
2025-10-05 12:06:57.968843: train_loss -0.8405
2025-10-05 12:06:57.969072: val_loss -0.2758
2025-10-05 12:06:57.969229: Pseudo dice [np.float32(0.6462)]
2025-10-05 12:06:57.969364: Epoch time: 46.16 s
2025-10-05 12:06:58.619897: 
2025-10-05 12:06:58.620182: Epoch 102
2025-10-05 12:06:58.620357: Current learning rate: 0.00359
2025-10-05 12:07:44.731219: Validation loss did not improve from -0.42810. Patience: 48/50
2025-10-05 12:07:44.731702: train_loss -0.8372
2025-10-05 12:07:44.731909: val_loss -0.3223
2025-10-05 12:07:44.732032: Pseudo dice [np.float32(0.6551)]
2025-10-05 12:07:44.732157: Epoch time: 46.11 s
2025-10-05 12:07:45.380346: 
2025-10-05 12:07:45.380635: Epoch 103
2025-10-05 12:07:45.380808: Current learning rate: 0.00352
2025-10-05 12:08:31.458684: Validation loss did not improve from -0.42810. Patience: 49/50
2025-10-05 12:08:31.459367: train_loss -0.8452
2025-10-05 12:08:31.459545: val_loss -0.2665
2025-10-05 12:08:31.459681: Pseudo dice [np.float32(0.627)]
2025-10-05 12:08:31.459821: Epoch time: 46.08 s
2025-10-05 12:08:32.103236: 
2025-10-05 12:08:32.103474: Epoch 104
2025-10-05 12:08:32.103653: Current learning rate: 0.00345
2025-10-05 12:09:18.207068: Validation loss did not improve from -0.42810. Patience: 50/50
2025-10-05 12:09:18.207576: train_loss -0.8414
2025-10-05 12:09:18.207751: val_loss -0.262
2025-10-05 12:09:18.207882: Pseudo dice [np.float32(0.6279)]
2025-10-05 12:09:18.208025: Epoch time: 46.11 s
2025-10-05 12:09:19.346560: 
2025-10-05 12:09:19.346981: Epoch 105
2025-10-05 12:09:19.347182: Current learning rate: 0.00338
2025-10-05 12:10:05.474626: Validation loss did not improve from -0.42810. Patience: 51/50
2025-10-05 12:10:05.475197: train_loss -0.8424
2025-10-05 12:10:05.475336: val_loss -0.2908
2025-10-05 12:10:05.475489: Pseudo dice [np.float32(0.6513)]
2025-10-05 12:10:05.475668: Epoch time: 46.13 s
2025-10-05 12:10:06.658650: 
2025-10-05 12:10:06.658949: Epoch 106
2025-10-05 12:10:06.659135: Current learning rate: 0.00332
2025-10-05 12:10:52.788288: Validation loss did not improve from -0.42810. Patience: 52/50
2025-10-05 12:10:52.788913: train_loss -0.8462
2025-10-05 12:10:52.789084: val_loss -0.265
2025-10-05 12:10:52.789265: Pseudo dice [np.float32(0.6493)]
2025-10-05 12:10:52.789416: Epoch time: 46.13 s
2025-10-05 12:10:53.439054: 
2025-10-05 12:10:53.439373: Epoch 107
2025-10-05 12:10:53.439579: Current learning rate: 0.00325
2025-10-05 12:11:39.548422: Validation loss did not improve from -0.42810. Patience: 53/50
2025-10-05 12:11:39.549114: train_loss -0.8446
2025-10-05 12:11:39.549342: val_loss -0.2349
2025-10-05 12:11:39.549538: Pseudo dice [np.float32(0.627)]
2025-10-05 12:11:39.549762: Epoch time: 46.11 s
2025-10-05 12:11:40.202167: 
2025-10-05 12:11:40.202581: Epoch 108
2025-10-05 12:11:40.202844: Current learning rate: 0.00318
2025-10-05 12:12:26.392416: Validation loss did not improve from -0.42810. Patience: 54/50
2025-10-05 12:12:26.393029: train_loss -0.8461
2025-10-05 12:12:26.393257: val_loss -0.2919
2025-10-05 12:12:26.393461: Pseudo dice [np.float32(0.6419)]
2025-10-05 12:12:26.393665: Epoch time: 46.19 s
2025-10-05 12:12:27.035809: 
2025-10-05 12:12:27.036190: Epoch 109
2025-10-05 12:12:27.036445: Current learning rate: 0.00311
2025-10-05 12:13:13.144431: Validation loss did not improve from -0.42810. Patience: 55/50
2025-10-05 12:13:13.145094: train_loss -0.8478
2025-10-05 12:13:13.145329: val_loss -0.2718
2025-10-05 12:13:13.145552: Pseudo dice [np.float32(0.6441)]
2025-10-05 12:13:13.145758: Epoch time: 46.11 s
2025-10-05 12:13:14.372586: 
2025-10-05 12:13:14.373002: Epoch 110
2025-10-05 12:13:14.373336: Current learning rate: 0.00304
2025-10-05 12:14:00.496747: Validation loss did not improve from -0.42810. Patience: 56/50
2025-10-05 12:14:00.497355: train_loss -0.8514
2025-10-05 12:14:00.497557: val_loss -0.2893
2025-10-05 12:14:00.497774: Pseudo dice [np.float32(0.65)]
2025-10-05 12:14:00.497960: Epoch time: 46.13 s
2025-10-05 12:14:01.145549: 
2025-10-05 12:14:01.145922: Epoch 111
2025-10-05 12:14:01.146166: Current learning rate: 0.00297
2025-10-05 12:14:47.264783: Validation loss did not improve from -0.42810. Patience: 57/50
2025-10-05 12:14:47.265416: train_loss -0.8556
2025-10-05 12:14:47.265629: val_loss -0.2948
2025-10-05 12:14:47.265770: Pseudo dice [np.float32(0.6586)]
2025-10-05 12:14:47.265909: Epoch time: 46.12 s
2025-10-05 12:14:47.913034: 
2025-10-05 12:14:47.913571: Epoch 112
2025-10-05 12:14:47.913778: Current learning rate: 0.00291
2025-10-05 12:15:34.084846: Validation loss did not improve from -0.42810. Patience: 58/50
2025-10-05 12:15:34.085567: train_loss -0.8513
2025-10-05 12:15:34.085813: val_loss -0.2942
2025-10-05 12:15:34.086345: Pseudo dice [np.float32(0.6276)]
2025-10-05 12:15:34.086552: Epoch time: 46.17 s
2025-10-05 12:15:34.737432: 
2025-10-05 12:15:34.737734: Epoch 113
2025-10-05 12:15:34.737955: Current learning rate: 0.00284
2025-10-05 12:16:20.801087: Validation loss did not improve from -0.42810. Patience: 59/50
2025-10-05 12:16:20.801672: train_loss -0.8531
2025-10-05 12:16:20.801854: val_loss -0.2801
2025-10-05 12:16:20.801979: Pseudo dice [np.float32(0.6473)]
2025-10-05 12:16:20.802121: Epoch time: 46.06 s
2025-10-05 12:16:21.452678: 
2025-10-05 12:16:21.453064: Epoch 114
2025-10-05 12:16:21.453286: Current learning rate: 0.00277
2025-10-05 12:17:07.544894: Validation loss did not improve from -0.42810. Patience: 60/50
2025-10-05 12:17:07.545471: train_loss -0.8512
2025-10-05 12:17:07.545675: val_loss -0.2867
2025-10-05 12:17:07.545915: Pseudo dice [np.float32(0.6516)]
2025-10-05 12:17:07.546162: Epoch time: 46.09 s
2025-10-05 12:17:08.652898: 
2025-10-05 12:17:08.653194: Epoch 115
2025-10-05 12:17:08.653444: Current learning rate: 0.0027
2025-10-05 12:17:54.683861: Validation loss did not improve from -0.42810. Patience: 61/50
2025-10-05 12:17:54.684428: train_loss -0.8532
2025-10-05 12:17:54.684599: val_loss -0.2997
2025-10-05 12:17:54.684742: Pseudo dice [np.float32(0.6544)]
2025-10-05 12:17:54.684917: Epoch time: 46.03 s
2025-10-05 12:17:55.331805: 
2025-10-05 12:17:55.332067: Epoch 116
2025-10-05 12:17:55.332263: Current learning rate: 0.00263
2025-10-05 12:18:41.305069: Validation loss did not improve from -0.42810. Patience: 62/50
2025-10-05 12:18:41.305791: train_loss -0.8508
2025-10-05 12:18:41.306004: val_loss -0.25
2025-10-05 12:18:41.306221: Pseudo dice [np.float32(0.6413)]
2025-10-05 12:18:41.306429: Epoch time: 45.97 s
2025-10-05 12:18:41.948999: 
2025-10-05 12:18:41.949330: Epoch 117
2025-10-05 12:18:41.949583: Current learning rate: 0.00256
2025-10-05 12:19:27.991122: Validation loss did not improve from -0.42810. Patience: 63/50
2025-10-05 12:19:27.991608: train_loss -0.8527
2025-10-05 12:19:27.991807: val_loss -0.2815
2025-10-05 12:19:27.991949: Pseudo dice [np.float32(0.6594)]
2025-10-05 12:19:27.992097: Epoch time: 46.04 s
2025-10-05 12:19:28.636126: 
2025-10-05 12:19:28.636421: Epoch 118
2025-10-05 12:19:28.636619: Current learning rate: 0.00249
2025-10-05 12:20:14.791989: Validation loss did not improve from -0.42810. Patience: 64/50
2025-10-05 12:20:14.792542: train_loss -0.8551
2025-10-05 12:20:14.792790: val_loss -0.276
2025-10-05 12:20:14.792919: Pseudo dice [np.float32(0.6411)]
2025-10-05 12:20:14.793070: Epoch time: 46.16 s
2025-10-05 12:20:15.441413: 
2025-10-05 12:20:15.441701: Epoch 119
2025-10-05 12:20:15.441947: Current learning rate: 0.00242
2025-10-05 12:21:01.523245: Validation loss did not improve from -0.42810. Patience: 65/50
2025-10-05 12:21:01.523798: train_loss -0.8584
2025-10-05 12:21:01.523975: val_loss -0.2281
2025-10-05 12:21:01.524117: Pseudo dice [np.float32(0.6208)]
2025-10-05 12:21:01.524271: Epoch time: 46.08 s
2025-10-05 12:21:02.631042: 
2025-10-05 12:21:02.631321: Epoch 120
2025-10-05 12:21:02.631518: Current learning rate: 0.00235
2025-10-05 12:21:48.761919: Validation loss did not improve from -0.42810. Patience: 66/50
2025-10-05 12:21:48.762495: train_loss -0.8572
2025-10-05 12:21:48.762769: val_loss -0.3088
2025-10-05 12:21:48.762968: Pseudo dice [np.float32(0.6629)]
2025-10-05 12:21:48.763193: Epoch time: 46.13 s
2025-10-05 12:21:49.418409: 
2025-10-05 12:21:49.418720: Epoch 121
2025-10-05 12:21:49.418996: Current learning rate: 0.00228
2025-10-05 12:22:35.483207: Validation loss did not improve from -0.42810. Patience: 67/50
2025-10-05 12:22:35.483865: train_loss -0.8602
2025-10-05 12:22:35.484031: val_loss -0.2837
2025-10-05 12:22:35.484168: Pseudo dice [np.float32(0.6596)]
2025-10-05 12:22:35.484378: Epoch time: 46.07 s
2025-10-05 12:22:36.670842: 
2025-10-05 12:22:36.671164: Epoch 122
2025-10-05 12:22:36.671426: Current learning rate: 0.00221
2025-10-05 12:23:22.820760: Validation loss did not improve from -0.42810. Patience: 68/50
2025-10-05 12:23:22.821236: train_loss -0.8596
2025-10-05 12:23:22.821482: val_loss -0.2738
2025-10-05 12:23:22.821619: Pseudo dice [np.float32(0.6417)]
2025-10-05 12:23:22.821794: Epoch time: 46.15 s
2025-10-05 12:23:23.482291: 
2025-10-05 12:23:23.482578: Epoch 123
2025-10-05 12:23:23.482748: Current learning rate: 0.00214
2025-10-05 12:24:09.694209: Validation loss did not improve from -0.42810. Patience: 69/50
2025-10-05 12:24:09.694810: train_loss -0.8582
2025-10-05 12:24:09.694979: val_loss -0.264
2025-10-05 12:24:09.695108: Pseudo dice [np.float32(0.6445)]
2025-10-05 12:24:09.695231: Epoch time: 46.21 s
2025-10-05 12:24:10.353007: 
2025-10-05 12:24:10.353334: Epoch 124
2025-10-05 12:24:10.353516: Current learning rate: 0.00207
2025-10-05 12:24:56.504819: Validation loss did not improve from -0.42810. Patience: 70/50
2025-10-05 12:24:56.505443: train_loss -0.8662
2025-10-05 12:24:56.505589: val_loss -0.3168
2025-10-05 12:24:56.505703: Pseudo dice [np.float32(0.6724)]
2025-10-05 12:24:56.505841: Epoch time: 46.15 s
2025-10-05 12:24:57.590456: 
2025-10-05 12:24:57.590806: Epoch 125
2025-10-05 12:24:57.591003: Current learning rate: 0.00199
2025-10-05 12:25:43.774628: Validation loss did not improve from -0.42810. Patience: 71/50
2025-10-05 12:25:43.775337: train_loss -0.8627
2025-10-05 12:25:43.775584: val_loss -0.2503
2025-10-05 12:25:43.775839: Pseudo dice [np.float32(0.6327)]
2025-10-05 12:25:43.776055: Epoch time: 46.19 s
2025-10-05 12:25:44.440626: 
2025-10-05 12:25:44.441000: Epoch 126
2025-10-05 12:25:44.441295: Current learning rate: 0.00192
2025-10-05 12:26:30.628202: Validation loss did not improve from -0.42810. Patience: 72/50
2025-10-05 12:26:30.628861: train_loss -0.8637
2025-10-05 12:26:30.629101: val_loss -0.3042
2025-10-05 12:26:30.629265: Pseudo dice [np.float32(0.6712)]
2025-10-05 12:26:30.629455: Epoch time: 46.19 s
2025-10-05 12:26:31.295969: 
2025-10-05 12:26:31.296311: Epoch 127
2025-10-05 12:26:31.296486: Current learning rate: 0.00185
2025-10-05 12:27:17.523082: Validation loss did not improve from -0.42810. Patience: 73/50
2025-10-05 12:27:17.523558: train_loss -0.8625
2025-10-05 12:27:17.523766: val_loss -0.2641
2025-10-05 12:27:17.523922: Pseudo dice [np.float32(0.638)]
2025-10-05 12:27:17.524091: Epoch time: 46.23 s
2025-10-05 12:27:18.188635: 
2025-10-05 12:27:18.188970: Epoch 128
2025-10-05 12:27:18.189160: Current learning rate: 0.00178
2025-10-05 12:28:04.441289: Validation loss did not improve from -0.42810. Patience: 74/50
2025-10-05 12:28:04.441884: train_loss -0.867
2025-10-05 12:28:04.442040: val_loss -0.3163
2025-10-05 12:28:04.442169: Pseudo dice [np.float32(0.6627)]
2025-10-05 12:28:04.442333: Epoch time: 46.25 s
2025-10-05 12:28:05.095911: 
2025-10-05 12:28:05.096179: Epoch 129
2025-10-05 12:28:05.096380: Current learning rate: 0.0017
2025-10-05 12:28:51.251266: Validation loss did not improve from -0.42810. Patience: 75/50
2025-10-05 12:28:51.252047: train_loss -0.8669
2025-10-05 12:28:51.252380: val_loss -0.2842
2025-10-05 12:28:51.252625: Pseudo dice [np.float32(0.6579)]
2025-10-05 12:28:51.252855: Epoch time: 46.16 s
2025-10-05 12:28:52.334987: 
2025-10-05 12:28:52.335322: Epoch 130
2025-10-05 12:28:52.335539: Current learning rate: 0.00163
2025-10-05 12:29:38.513865: Validation loss did not improve from -0.42810. Patience: 76/50
2025-10-05 12:29:38.514466: train_loss -0.8694
2025-10-05 12:29:38.514616: val_loss -0.2716
2025-10-05 12:29:38.514773: Pseudo dice [np.float32(0.659)]
2025-10-05 12:29:38.514909: Epoch time: 46.18 s
2025-10-05 12:29:39.166794: 
2025-10-05 12:29:39.166997: Epoch 131
2025-10-05 12:29:39.167169: Current learning rate: 0.00156
2025-10-05 12:30:25.389738: Validation loss did not improve from -0.42810. Patience: 77/50
2025-10-05 12:30:25.390413: train_loss -0.8687
2025-10-05 12:30:25.390625: val_loss -0.2814
2025-10-05 12:30:25.390987: Pseudo dice [np.float32(0.6576)]
2025-10-05 12:30:25.391150: Epoch time: 46.22 s
2025-10-05 12:30:26.045148: 
2025-10-05 12:30:26.045361: Epoch 132
2025-10-05 12:30:26.045531: Current learning rate: 0.00148
2025-10-05 12:31:12.270583: Validation loss did not improve from -0.42810. Patience: 78/50
2025-10-05 12:31:12.271158: train_loss -0.868
2025-10-05 12:31:12.271322: val_loss -0.2691
2025-10-05 12:31:12.271461: Pseudo dice [np.float32(0.6581)]
2025-10-05 12:31:12.271597: Epoch time: 46.23 s
2025-10-05 12:31:12.921334: 
2025-10-05 12:31:12.921686: Epoch 133
2025-10-05 12:31:12.921878: Current learning rate: 0.00141
2025-10-05 12:31:59.103970: Validation loss did not improve from -0.42810. Patience: 79/50
2025-10-05 12:31:59.104417: train_loss -0.8707
2025-10-05 12:31:59.104554: val_loss -0.3248
2025-10-05 12:31:59.104685: Pseudo dice [np.float32(0.6727)]
2025-10-05 12:31:59.104840: Epoch time: 46.18 s
2025-10-05 12:31:59.752245: 
2025-10-05 12:31:59.752585: Epoch 134
2025-10-05 12:31:59.752791: Current learning rate: 0.00133
2025-10-05 12:32:45.924759: Validation loss did not improve from -0.42810. Patience: 80/50
2025-10-05 12:32:45.925527: train_loss -0.8729
2025-10-05 12:32:45.925802: val_loss -0.2055
2025-10-05 12:32:45.926023: Pseudo dice [np.float32(0.6201)]
2025-10-05 12:32:45.926223: Epoch time: 46.17 s
2025-10-05 12:32:47.020073: 
2025-10-05 12:32:47.020447: Epoch 135
2025-10-05 12:32:47.020656: Current learning rate: 0.00126
2025-10-05 12:33:33.179620: Validation loss did not improve from -0.42810. Patience: 81/50
2025-10-05 12:33:33.180304: train_loss -0.8716
2025-10-05 12:33:33.180467: val_loss -0.2744
2025-10-05 12:33:33.180613: Pseudo dice [np.float32(0.6458)]
2025-10-05 12:33:33.180743: Epoch time: 46.16 s
2025-10-05 12:33:33.841424: 
2025-10-05 12:33:33.841811: Epoch 136
2025-10-05 12:33:33.842026: Current learning rate: 0.00118
2025-10-05 12:34:19.819999: Validation loss did not improve from -0.42810. Patience: 82/50
2025-10-05 12:34:19.820486: train_loss -0.8723
2025-10-05 12:34:19.820633: val_loss -0.26
2025-10-05 12:34:19.820762: Pseudo dice [np.float32(0.6427)]
2025-10-05 12:34:19.820906: Epoch time: 45.98 s
2025-10-05 12:34:20.987014: 
2025-10-05 12:34:20.987368: Epoch 137
2025-10-05 12:34:20.987664: Current learning rate: 0.00111
2025-10-05 12:35:07.015280: Validation loss did not improve from -0.42810. Patience: 83/50
2025-10-05 12:35:07.016027: train_loss -0.87
2025-10-05 12:35:07.016257: val_loss -0.2546
2025-10-05 12:35:07.016481: Pseudo dice [np.float32(0.6407)]
2025-10-05 12:35:07.016752: Epoch time: 46.03 s
2025-10-05 12:35:07.667109: 
2025-10-05 12:35:07.667580: Epoch 138
2025-10-05 12:35:07.667961: Current learning rate: 0.00103
2025-10-05 12:35:53.745656: Validation loss did not improve from -0.42810. Patience: 84/50
2025-10-05 12:35:53.746231: train_loss -0.8729
2025-10-05 12:35:53.746379: val_loss -0.3176
2025-10-05 12:35:53.746493: Pseudo dice [np.float32(0.6788)]
2025-10-05 12:35:53.746647: Epoch time: 46.08 s
2025-10-05 12:35:54.396187: 
2025-10-05 12:35:54.396534: Epoch 139
2025-10-05 12:35:54.396735: Current learning rate: 0.00095
2025-10-05 12:36:40.388322: Validation loss did not improve from -0.42810. Patience: 85/50
2025-10-05 12:36:40.388977: train_loss -0.8742
2025-10-05 12:36:40.389218: val_loss -0.2647
2025-10-05 12:36:40.389455: Pseudo dice [np.float32(0.6596)]
2025-10-05 12:36:40.389685: Epoch time: 45.99 s
2025-10-05 12:36:41.488893: 
2025-10-05 12:36:41.489187: Epoch 140
2025-10-05 12:36:41.489364: Current learning rate: 0.00087
2025-10-05 12:37:27.483151: Validation loss did not improve from -0.42810. Patience: 86/50
2025-10-05 12:37:27.483808: train_loss -0.8765
2025-10-05 12:37:27.483949: val_loss -0.3065
2025-10-05 12:37:27.484126: Pseudo dice [np.float32(0.6642)]
2025-10-05 12:37:27.484315: Epoch time: 46.0 s
2025-10-05 12:37:28.131864: 
2025-10-05 12:37:28.132106: Epoch 141
2025-10-05 12:37:28.132360: Current learning rate: 0.00079
2025-10-05 12:38:14.099838: Validation loss did not improve from -0.42810. Patience: 87/50
2025-10-05 12:38:14.100393: train_loss -0.8781
2025-10-05 12:38:14.100525: val_loss -0.2363
2025-10-05 12:38:14.100647: Pseudo dice [np.float32(0.6476)]
2025-10-05 12:38:14.100795: Epoch time: 45.97 s
2025-10-05 12:38:14.753676: 
2025-10-05 12:38:14.753980: Epoch 142
2025-10-05 12:38:14.754216: Current learning rate: 0.00071
2025-10-05 12:39:00.656267: Validation loss did not improve from -0.42810. Patience: 88/50
2025-10-05 12:39:00.656818: train_loss -0.8762
2025-10-05 12:39:00.656967: val_loss -0.2771
2025-10-05 12:39:00.657109: Pseudo dice [np.float32(0.6578)]
2025-10-05 12:39:00.657291: Epoch time: 45.9 s
2025-10-05 12:39:01.302253: 
2025-10-05 12:39:01.302594: Epoch 143
2025-10-05 12:39:01.302817: Current learning rate: 0.00063
2025-10-05 12:39:47.292683: Validation loss did not improve from -0.42810. Patience: 89/50
2025-10-05 12:39:47.293358: train_loss -0.8754
2025-10-05 12:39:47.293494: val_loss -0.2442
2025-10-05 12:39:47.293641: Pseudo dice [np.float32(0.6482)]
2025-10-05 12:39:47.293848: Epoch time: 45.99 s
2025-10-05 12:39:47.948234: 
2025-10-05 12:39:47.948638: Epoch 144
2025-10-05 12:39:47.948957: Current learning rate: 0.00055
2025-10-05 12:40:34.047081: Validation loss did not improve from -0.42810. Patience: 90/50
2025-10-05 12:40:34.047576: train_loss -0.8797
2025-10-05 12:40:34.047725: val_loss -0.2605
2025-10-05 12:40:34.047851: Pseudo dice [np.float32(0.6546)]
2025-10-05 12:40:34.047989: Epoch time: 46.1 s
2025-10-05 12:40:35.134326: 
2025-10-05 12:40:35.134633: Epoch 145
2025-10-05 12:40:35.134901: Current learning rate: 0.00047
2025-10-05 12:41:21.024809: Validation loss did not improve from -0.42810. Patience: 91/50
2025-10-05 12:41:21.025402: train_loss -0.8773
2025-10-05 12:41:21.025611: val_loss -0.2753
2025-10-05 12:41:21.025747: Pseudo dice [np.float32(0.657)]
2025-10-05 12:41:21.025895: Epoch time: 45.89 s
2025-10-05 12:41:21.687155: 
2025-10-05 12:41:21.687483: Epoch 146
2025-10-05 12:41:21.687653: Current learning rate: 0.00038
2025-10-05 12:42:07.642780: Validation loss did not improve from -0.42810. Patience: 92/50
2025-10-05 12:42:07.643354: train_loss -0.8745
2025-10-05 12:42:07.643550: val_loss -0.2932
2025-10-05 12:42:07.643724: Pseudo dice [np.float32(0.6579)]
2025-10-05 12:42:07.643919: Epoch time: 45.96 s
2025-10-05 12:42:08.300083: 
2025-10-05 12:42:08.300353: Epoch 147
2025-10-05 12:42:08.300575: Current learning rate: 0.0003
2025-10-05 12:42:54.252932: Validation loss did not improve from -0.42810. Patience: 93/50
2025-10-05 12:42:54.253490: train_loss -0.8761
2025-10-05 12:42:54.253756: val_loss -0.2378
2025-10-05 12:42:54.253918: Pseudo dice [np.float32(0.6451)]
2025-10-05 12:42:54.254091: Epoch time: 45.95 s
2025-10-05 12:42:54.910400: 
2025-10-05 12:42:54.910746: Epoch 148
2025-10-05 12:42:54.910923: Current learning rate: 0.00021
2025-10-05 12:43:40.873952: Validation loss did not improve from -0.42810. Patience: 94/50
2025-10-05 12:43:40.874888: train_loss -0.8786
2025-10-05 12:43:40.875384: val_loss -0.2377
2025-10-05 12:43:40.875669: Pseudo dice [np.float32(0.6345)]
2025-10-05 12:43:40.875983: Epoch time: 45.97 s
2025-10-05 12:43:41.538393: 
2025-10-05 12:43:41.538805: Epoch 149
2025-10-05 12:43:41.539084: Current learning rate: 0.00011
2025-10-05 12:44:27.548965: Validation loss did not improve from -0.42810. Patience: 95/50
2025-10-05 12:44:27.549744: train_loss -0.8794
2025-10-05 12:44:27.550045: val_loss -0.311
2025-10-05 12:44:27.550311: Pseudo dice [np.float32(0.6693)]
2025-10-05 12:44:27.550497: Epoch time: 46.01 s
2025-10-05 12:44:28.684743: Training done.
2025-10-05 12:44:28.697211: Using splits from existing split file: /nfs/erelab001/shared/Computational_Group/Naravich/datasets/nnUNet_Datasets/nnUNet_preprocessed/Dataset310_nnInteractive_Calcium_OCT_CrossValidation/splits_final_60.json
2025-10-05 12:44:28.697545: The split file contains 5 splits.
2025-10-05 12:44:28.697699: Desired fold for training: 0
2025-10-05 12:44:28.697820: This split has 4 training and 4 validation cases.
2025-10-05 12:44:28.698031: predicting 101-045
2025-10-05 12:44:28.700513: 101-045, shape torch.Size([1, 375, 498, 498]), rank 0
2025-10-05 12:45:16.204073: predicting 701-013
2025-10-05 12:45:16.216309: 701-013, shape torch.Size([1, 375, 498, 498]), rank 0
2025-10-05 12:45:50.359234: predicting 704-003
2025-10-05 12:45:50.374101: 704-003, shape torch.Size([1, 375, 498, 498]), rank 0
2025-10-05 12:46:24.367253: predicting 706-005
2025-10-05 12:46:24.378805: 706-005, shape torch.Size([1, 375, 498, 498]), rank 0
2025-10-05 12:47:11.295547: Validation complete
2025-10-05 12:47:11.295788: Mean Validation Dice:  0.6459232396586967
Finished training fold 0 saving to /nfs/erelab001/shared/Computational_Group/Naravich/datasets/nnUNet_Datasets/nnUNet_results/Dataset310_nnInteractive_Calcium_OCT_CrossValidation/nnUNetTrainerScaleAnalysis60__nnUNetPlans__3d_32x160x128_b10/fold_0_No_Pretrained
