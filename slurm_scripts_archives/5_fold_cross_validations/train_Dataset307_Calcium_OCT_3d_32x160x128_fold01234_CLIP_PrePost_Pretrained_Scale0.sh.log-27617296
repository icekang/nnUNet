/home/gridsan/nchutisilp/.conda/envs/nnunet/bin/python
Starting training with CONFIG=3d_32x160x128_b10, DATASET_ID=307, TRAINER=nnUNetTrainerScaleAnalysis20

############################
INFO: You are using the old nnU-Net default plans. We have updated our recommendations. Please consider using those instead! Read more here: https://github.com/MIC-DKFZ/nnUNet/blob/master/documentation/resenc_presets.md
############################

Using device: cuda:0

#######################################################################
Please cite the following paper when using nnU-Net:
Isensee, F., Jaeger, P. F., Kohl, S. A., Petersen, J., & Maier-Hein, K. H. (2021). nnU-Net: a self-configuring method for deep learning-based biomedical image segmentation. Nature methods, 18(2), 203-211.
#######################################################################

2024-12-19 04:43:25.568179: Using torch.compile...

############################
INFO: You are using the old nnU-Net default plans. We have updated our recommendations. Please consider using those instead! Read more here: https://github.com/MIC-DKFZ/nnUNet/blob/master/documentation/resenc_presets.md
############################

Using device: cuda:0

#######################################################################
Please cite the following paper when using nnU-Net:
Isensee, F., Jaeger, P. F., Kohl, S. A., Petersen, J., & Maier-Hein, K. H. (2021). nnU-Net: a self-configuring method for deep learning-based biomedical image segmentation. Nature methods, 18(2), 203-211.
#######################################################################

2024-12-19 04:43:25.568822: Using torch.compile...
/home/gridsan/nchutisilp/.local/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn("The verbose parameter is deprecated. Please use get_last_lr() "
/home/gridsan/nchutisilp/.local/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn("The verbose parameter is deprecated. Please use get_last_lr() "
################### Loading pretrained weights from file  /home/gridsan/nchutisilp/projects/OpenAI-CLIP/clip_preivl_poststent_pretrained_nnUNet.pt ###################
Below is the list of overlapping blocks in pretrained model and nnUNet architecture:
encoder.stages.0.0.convs.0.conv.weight shape torch.Size([32, 1, 3, 3, 3])
encoder.stages.0.0.convs.0.conv.bias shape torch.Size([32])
encoder.stages.0.0.convs.0.norm.weight shape torch.Size([32])
encoder.stages.0.0.convs.0.norm.bias shape torch.Size([32])
encoder.stages.0.0.convs.0.all_modules.0.weight shape torch.Size([32, 1, 3, 3, 3])
encoder.stages.0.0.convs.0.all_modules.0.bias shape torch.Size([32])
encoder.stages.0.0.convs.0.all_modules.1.weight shape torch.Size([32])
encoder.stages.0.0.convs.0.all_modules.1.bias shape torch.Size([32])
encoder.stages.0.0.convs.1.conv.weight shape torch.Size([32, 32, 3, 3, 3])
encoder.stages.0.0.convs.1.conv.bias shape torch.Size([32])
encoder.stages.0.0.convs.1.norm.weight shape torch.Size([32])
encoder.stages.0.0.convs.1.norm.bias shape torch.Size([32])
encoder.stages.0.0.convs.1.all_modules.0.weight shape torch.Size([32, 32, 3, 3, 3])
encoder.stages.0.0.convs.1.all_modules.0.bias shape torch.Size([32])
encoder.stages.0.0.convs.1.all_modules.1.weight shape torch.Size([32])
encoder.stages.0.0.convs.1.all_modules.1.bias shape torch.Size([32])
encoder.stages.1.0.convs.0.conv.weight shape torch.Size([64, 32, 3, 3, 3])
encoder.stages.1.0.convs.0.conv.bias shape torch.Size([64])
encoder.stages.1.0.convs.0.norm.weight shape torch.Size([64])
encoder.stages.1.0.convs.0.norm.bias shape torch.Size([64])
encoder.stages.1.0.convs.0.all_modules.0.weight shape torch.Size([64, 32, 3, 3, 3])
encoder.stages.1.0.convs.0.all_modules.0.bias shape torch.Size([64])
encoder.stages.1.0.convs.0.all_modules.1.weight shape torch.Size([64])
encoder.stages.1.0.convs.0.all_modules.1.bias shape torch.Size([64])
encoder.stages.1.0.convs.1.conv.weight shape torch.Size([64, 64, 3, 3, 3])
encoder.stages.1.0.convs.1.conv.bias shape torch.Size([64])
encoder.stages.1.0.convs.1.norm.weight shape torch.Size([64])
encoder.stages.1.0.convs.1.norm.bias shape torch.Size([64])
encoder.stages.1.0.convs.1.all_modules.0.weight shape torch.Size([64, 64, 3, 3, 3])
encoder.stages.1.0.convs.1.all_modules.0.bias shape torch.Size([64])
encoder.stages.1.0.convs.1.all_modules.1.weight shape torch.Size([64])
encoder.stages.1.0.convs.1.all_modules.1.bias shape torch.Size([64])
encoder.stages.2.0.convs.0.conv.weight shape torch.Size([128, 64, 3, 3, 3])
encoder.stages.2.0.convs.0.conv.bias shape torch.Size([128])
encoder.stages.2.0.convs.0.norm.weight shape torch.Size([128])
encoder.stages.2.0.convs.0.norm.bias shape torch.Size([128])
encoder.stages.2.0.convs.0.all_modules.0.weight shape torch.Size([128, 64, 3, 3, 3])
encoder.stages.2.0.convs.0.all_modules.0.bias shape torch.Size([128])
encoder.stages.2.0.convs.0.all_modules.1.weight shape torch.Size([128])
encoder.stages.2.0.convs.0.all_modules.1.bias shape torch.Size([128])
encoder.stages.2.0.convs.1.conv.weight shape torch.Size([128, 128, 3, 3, 3])
encoder.stages.2.0.convs.1.conv.bias shape torch.Size([128])
encoder.stages.2.0.convs.1.norm.weight shape torch.Size([128])
encoder.stages.2.0.convs.1.norm.bias shape torch.Size([128])
encoder.stages.2.0.convs.1.all_modules.0.weight shape torch.Size([128, 128, 3, 3, 3])
encoder.stages.2.0.convs.1.all_modules.0.bias shape torch.Size([128])
encoder.stages.2.0.convs.1.all_modules.1.weight shape torch.Size([128])
encoder.stages.2.0.convs.1.all_modules.1.bias shape torch.Size([128])
encoder.stages.3.0.convs.0.conv.weight shape torch.Size([256, 128, 3, 3, 3])
encoder.stages.3.0.convs.0.conv.bias shape torch.Size([256])
encoder.stages.3.0.convs.0.norm.weight shape torch.Size([256])
encoder.stages.3.0.convs.0.norm.bias shape torch.Size([256])
encoder.stages.3.0.convs.0.all_modules.0.weight shape torch.Size([256, 128, 3, 3, 3])
encoder.stages.3.0.convs.0.all_modules.0.bias shape torch.Size([256])
encoder.stages.3.0.convs.0.all_modules.1.weight shape torch.Size([256])
encoder.stages.3.0.convs.0.all_modules.1.bias shape torch.Size([256])
encoder.stages.3.0.convs.1.conv.weight shape torch.Size([256, 256, 3, 3, 3])
encoder.stages.3.0.convs.1.conv.bias shape torch.Size([256])
encoder.stages.3.0.convs.1.norm.weight shape torch.Size([256])
encoder.stages.3.0.convs.1.norm.bias shape torch.Size([256])
encoder.stages.3.0.convs.1.all_modules.0.weight shape torch.Size([256, 256, 3, 3, 3])
encoder.stages.3.0.convs.1.all_modules.0.bias shape torch.Size([256])
encoder.stages.3.0.convs.1.all_modules.1.weight shape torch.Size([256])
encoder.stages.3.0.convs.1.all_modules.1.bias shape torch.Size([256])
encoder.stages.4.0.convs.0.conv.weight shape torch.Size([320, 256, 3, 3, 3])
encoder.stages.4.0.convs.0.conv.bias shape torch.Size([320])
encoder.stages.4.0.convs.0.norm.weight shape torch.Size([320])
encoder.stages.4.0.convs.0.norm.bias shape torch.Size([320])
encoder.stages.4.0.convs.0.all_modules.0.weight shape torch.Size([320, 256, 3, 3, 3])
encoder.stages.4.0.convs.0.all_modules.0.bias shape torch.Size([320])
encoder.stages.4.0.convs.0.all_modules.1.weight shape torch.Size([320])
encoder.stages.4.0.convs.0.all_modules.1.bias shape torch.Size([320])
encoder.stages.4.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.4.0.convs.1.conv.bias shape torch.Size([320])
encoder.stages.4.0.convs.1.norm.weight shape torch.Size([320])
encoder.stages.4.0.convs.1.norm.bias shape torch.Size([320])
encoder.stages.4.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.4.0.convs.1.all_modules.0.bias shape torch.Size([320])
encoder.stages.4.0.convs.1.all_modules.1.weight shape torch.Size([320])
encoder.stages.4.0.convs.1.all_modules.1.bias shape torch.Size([320])
encoder.stages.5.0.convs.0.conv.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.5.0.convs.0.conv.bias shape torch.Size([320])
encoder.stages.5.0.convs.0.norm.weight shape torch.Size([320])
encoder.stages.5.0.convs.0.norm.bias shape torch.Size([320])
encoder.stages.5.0.convs.0.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.5.0.convs.0.all_modules.0.bias shape torch.Size([320])
encoder.stages.5.0.convs.0.all_modules.1.weight shape torch.Size([320])
encoder.stages.5.0.convs.0.all_modules.1.bias shape torch.Size([320])
encoder.stages.5.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.5.0.convs.1.conv.bias shape torch.Size([320])
encoder.stages.5.0.convs.1.norm.weight shape torch.Size([320])
encoder.stages.5.0.convs.1.norm.bias shape torch.Size([320])
encoder.stages.5.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.5.0.convs.1.all_modules.0.bias shape torch.Size([320])
encoder.stages.5.0.convs.1.all_modules.1.weight shape torch.Size([320])
encoder.stages.5.0.convs.1.all_modules.1.bias shape torch.Size([320])
decoder.encoder.stages.0.0.convs.0.conv.weight shape torch.Size([32, 1, 3, 3, 3])
decoder.encoder.stages.0.0.convs.0.conv.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.norm.weight shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.norm.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.all_modules.0.weight shape torch.Size([32, 1, 3, 3, 3])
decoder.encoder.stages.0.0.convs.0.all_modules.0.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.all_modules.1.weight shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.all_modules.1.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.conv.weight shape torch.Size([32, 32, 3, 3, 3])
decoder.encoder.stages.0.0.convs.1.conv.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.norm.weight shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.norm.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.all_modules.0.weight shape torch.Size([32, 32, 3, 3, 3])
decoder.encoder.stages.0.0.convs.1.all_modules.0.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.all_modules.1.weight shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.all_modules.1.bias shape torch.Size([32])
decoder.encoder.stages.1.0.convs.0.conv.weight shape torch.Size([64, 32, 3, 3, 3])
decoder.encoder.stages.1.0.convs.0.conv.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.norm.weight shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.norm.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.all_modules.0.weight shape torch.Size([64, 32, 3, 3, 3])
decoder.encoder.stages.1.0.convs.0.all_modules.0.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.all_modules.1.weight shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.all_modules.1.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.conv.weight shape torch.Size([64, 64, 3, 3, 3])
decoder.encoder.stages.1.0.convs.1.conv.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.norm.weight shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.norm.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.all_modules.0.weight shape torch.Size([64, 64, 3, 3, 3])
decoder.encoder.stages.1.0.convs.1.all_modules.0.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.all_modules.1.weight shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.all_modules.1.bias shape torch.Size([64])
decoder.encoder.stages.2.0.convs.0.conv.weight shape torch.Size([128, 64, 3, 3, 3])
decoder.encoder.stages.2.0.convs.0.conv.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.norm.weight shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.norm.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.all_modules.0.weight shape torch.Size([128, 64, 3, 3, 3])
decoder.encoder.stages.2.0.convs.0.all_modules.0.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.all_modules.1.weight shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.all_modules.1.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.conv.weight shape torch.Size([128, 128, 3, 3, 3])
decoder.encoder.stages.2.0.convs.1.conv.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.norm.weight shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.norm.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.all_modules.0.weight shape torch.Size([128, 128, 3, 3, 3])
decoder.encoder.stages.2.0.convs.1.all_modules.0.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.all_modules.1.weight shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.all_modules.1.bias shape torch.Size([128])
decoder.encoder.stages.3.0.convs.0.conv.weight shape torch.Size([256, 128, 3, 3, 3])
decoder.encoder.stages.3.0.convs.0.conv.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.norm.weight shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.norm.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.all_modules.0.weight shape torch.Size([256, 128, 3, 3, 3])
decoder.encoder.stages.3.0.convs.0.all_modules.0.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.all_modules.1.weight shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.all_modules.1.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.conv.weight shape torch.Size([256, 256, 3, 3, 3])
decoder.encoder.stages.3.0.convs.1.conv.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.norm.weight shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.norm.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.all_modules.0.weight shape torch.Size([256, 256, 3, 3, 3])
decoder.encoder.stages.3.0.convs.1.all_modules.0.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.all_modules.1.weight shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.all_modules.1.bias shape torch.Size([256])
decoder.encoder.stages.4.0.convs.0.conv.weight shape torch.Size([320, 256, 3, 3, 3])
decoder.encoder.stages.4.0.convs.0.conv.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.norm.weight shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.norm.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.all_modules.0.weight shape torch.Size([320, 256, 3, 3, 3])
decoder.encoder.stages.4.0.convs.0.all_modules.0.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.all_modules.1.weight shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.all_modules.1.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.4.0.convs.1.conv.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.norm.weight shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.norm.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.4.0.convs.1.all_modules.0.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.all_modules.1.weight shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.all_modules.1.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.conv.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.5.0.convs.0.conv.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.norm.weight shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.norm.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.5.0.convs.0.all_modules.0.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.all_modules.1.weight shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.all_modules.1.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.5.0.convs.1.conv.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.norm.weight shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.norm.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.5.0.convs.1.all_modules.0.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.all_modules.1.weight shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.all_modules.1.bias shape torch.Size([320])
decoder.stages.0.convs.0.conv.weight shape torch.Size([320, 640, 3, 3, 3])
decoder.stages.0.convs.0.conv.bias shape torch.Size([320])
decoder.stages.0.convs.0.norm.weight shape torch.Size([320])
decoder.stages.0.convs.0.norm.bias shape torch.Size([320])
decoder.stages.0.convs.0.all_modules.0.weight shape torch.Size([320, 640, 3, 3, 3])
decoder.stages.0.convs.0.all_modules.0.bias shape torch.Size([320])
decoder.stages.0.convs.0.all_modules.1.weight shape torch.Size([320])
decoder.stages.0.convs.0.all_modules.1.bias shape torch.Size([320])
decoder.stages.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.stages.0.convs.1.conv.bias shape torch.Size([320])
decoder.stages.0.convs.1.norm.weight shape torch.Size([320])
decoder.stages.0.convs.1.norm.bias shape torch.Size([320])
decoder.stages.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.stages.0.convs.1.all_modules.0.bias shape torch.Size([320])
decoder.stages.0.convs.1.all_modules.1.weight shape torch.Size([320])
decoder.stages.0.convs.1.all_modules.1.bias shape torch.Size([320])
decoder.stages.1.convs.0.conv.weight shape torch.Size([256, 512, 3, 3, 3])
decoder.stages.1.convs.0.conv.bias shape torch.Size([256])
decoder.stages.1.convs.0.norm.weight shape torch.Size([256])
decoder.stages.1.convs.0.norm.bias shape torch.Size([256])
decoder.stages.1.convs.0.all_modules.0.weight shape torch.Size([256, 512, 3, 3, 3])
decoder.stages.1.convs.0.all_modules.0.bias shape torch.Size([256])
decoder.stages.1.convs.0.all_modules.1.weight shape torch.Size([256])
decoder.stages.1.convs.0.all_modules.1.bias shape torch.Size([256])
decoder.stages.1.convs.1.conv.weight shape torch.Size([256, 256, 3, 3, 3])
decoder.stages.1.convs.1.conv.bias shape torch.Size([256])
decoder.stages.1.convs.1.norm.weight shape torch.Size([256])
decoder.stages.1.convs.1.norm.bias shape torch.Size([256])
decoder.stages.1.convs.1.all_modules.0.weight shape torch.Size([256, 256, 3, 3, 3])
decoder.stages.1.convs.1.all_modules.0.bias shape torch.Size([256])
decoder.stages.1.convs.1.all_modules.1.weight shape torch.Size([256])
decoder.stages.1.convs.1.all_modules.1.bias shape torch.Size([256])
decoder.stages.2.convs.0.conv.weight shape torch.Size([128, 256, 3, 3, 3])
decoder.stages.2.convs.0.conv.bias shape torch.Size([128])
decoder.stages.2.convs.0.norm.weight shape torch.Size([128])
decoder.stages.2.convs.0.norm.bias shape torch.Size([128])
decoder.stages.2.convs.0.all_modules.0.weight shape torch.Size([128, 256, 3, 3, 3])
decoder.stages.2.convs.0.all_modules.0.bias shape torch.Size([128])
decoder.stages.2.convs.0.all_modules.1.weight shape torch.Size([128])
decoder.stages.2.convs.0.all_modules.1.bias shape torch.Size([128])
decoder.stages.2.convs.1.conv.weight shape torch.Size([128, 128, 3, 3, 3])
decoder.stages.2.convs.1.conv.bias shape torch.Size([128])
decoder.stages.2.convs.1.norm.weight shape torch.Size([128])
decoder.stages.2.convs.1.norm.bias shape torch.Size([128])
decoder.stages.2.convs.1.all_modules.0.weight shape torch.Size([128, 128, 3, 3, 3])
decoder.stages.2.convs.1.all_modules.0.bias shape torch.Size([128])
decoder.stages.2.convs.1.all_modules.1.weight shape torch.Size([128])
decoder.stages.2.convs.1.all_modules.1.bias shape torch.Size([128])
decoder.stages.3.convs.0.conv.weight shape torch.Size([64, 128, 3, 3, 3])
decoder.stages.3.convs.0.conv.bias shape torch.Size([64])
decoder.stages.3.convs.0.norm.weight shape torch.Size([64])
decoder.stages.3.convs.0.norm.bias shape torch.Size([64])
decoder.stages.3.convs.0.all_modules.0.weight shape torch.Size([64, 128, 3, 3, 3])
decoder.stages.3.convs.0.all_modules.0.bias shape torch.Size([64])
decoder.stages.3.convs.0.all_modules.1.weight shape torch.Size([64])
decoder.stages.3.convs.0.all_modules.1.bias shape torch.Size([64])
decoder.stages.3.convs.1.conv.weight shape torch.Size([64, 64, 3, 3, 3])
decoder.stages.3.convs.1.conv.bias shape torch.Size([64])
decoder.stages.3.convs.1.norm.weight shape torch.Size([64])
decoder.stages.3.convs.1.norm.bias shape torch.Size([64])
decoder.stages.3.convs.1.all_modules.0.weight shape torch.Size([64, 64, 3, 3, 3])
decoder.stages.3.convs.1.all_modules.0.bias shape torch.Size([64])
decoder.stages.3.convs.1.all_modules.1.weight shape torch.Size([64])
decoder.stages.3.convs.1.all_modules.1.bias shape torch.Size([64])
decoder.stages.4.convs.0.conv.weight shape torch.Size([32, 64, 3, 3, 3])
decoder.stages.4.convs.0.conv.bias shape torch.Size([32])
decoder.stages.4.convs.0.norm.weight shape torch.Size([32])
decoder.stages.4.convs.0.norm.bias shape torch.Size([32])
decoder.stages.4.convs.0.all_modules.0.weight shape torch.Size([32, 64, 3, 3, 3])
decoder.stages.4.convs.0.all_modules.0.bias shape torch.Size([32])
decoder.stages.4.convs.0.all_modules.1.weight shape torch.Size([32])
decoder.stages.4.convs.0.all_modules.1.bias shape torch.Size([32])
decoder.stages.4.convs.1.conv.weight shape torch.Size([32, 32, 3, 3, 3])
decoder.stages.4.convs.1.conv.bias shape torch.Size([32])
decoder.stages.4.convs.1.norm.weight shape torch.Size([32])
decoder.stages.4.convs.1.norm.bias shape torch.Size([32])
decoder.stages.4.convs.1.all_modules.0.weight shape torch.Size([32, 32, 3, 3, 3])
decoder.stages.4.convs.1.all_modules.0.bias shape torch.Size([32])
decoder.stages.4.convs.1.all_modules.1.weight shape torch.Size([32])
decoder.stages.4.convs.1.all_modules.1.bias shape torch.Size([32])
decoder.transpconvs.0.weight shape torch.Size([320, 320, 1, 2, 2])
decoder.transpconvs.0.bias shape torch.Size([320])
decoder.transpconvs.1.weight shape torch.Size([320, 256, 2, 2, 2])
decoder.transpconvs.1.bias shape torch.Size([256])
decoder.transpconvs.2.weight shape torch.Size([256, 128, 2, 2, 2])
decoder.transpconvs.2.bias shape torch.Size([128])
decoder.transpconvs.3.weight shape torch.Size([128, 64, 2, 2, 2])
decoder.transpconvs.3.bias shape torch.Size([64])
decoder.transpconvs.4.weight shape torch.Size([64, 32, 2, 2, 2])
decoder.transpconvs.4.bias shape torch.Size([32])
################### Done ###################
2024-12-19 04:43:55.842264: do_dummy_2d_data_aug: True
2024-12-19 04:43:55.867085: Using splits from existing split file: /home/gridsan/nchutisilp/datasets/nnUNet_Datasets/nnUNet_preprocessed/Dataset307_Sohee_Calcium_OCT_CrossValidation/splits_final_20.json
2024-12-19 04:43:55.881509: The split file contains 5 splits.
2024-12-19 04:43:55.883602: Desired fold for training: 2
2024-12-19 04:43:55.884457: This split has 1 training and 7 validation cases.
################### Loading pretrained weights from file  /home/gridsan/nchutisilp/projects/OpenAI-CLIP/clip_preivl_poststent_pretrained_nnUNet.pt ###################
Below is the list of overlapping blocks in pretrained model and nnUNet architecture:
encoder.stages.0.0.convs.0.conv.weight shape torch.Size([32, 1, 3, 3, 3])
encoder.stages.0.0.convs.0.conv.bias shape torch.Size([32])
encoder.stages.0.0.convs.0.norm.weight shape torch.Size([32])
encoder.stages.0.0.convs.0.norm.bias shape torch.Size([32])
encoder.stages.0.0.convs.0.all_modules.0.weight shape torch.Size([32, 1, 3, 3, 3])
encoder.stages.0.0.convs.0.all_modules.0.bias shape torch.Size([32])
encoder.stages.0.0.convs.0.all_modules.1.weight shape torch.Size([32])
encoder.stages.0.0.convs.0.all_modules.1.bias shape torch.Size([32])
encoder.stages.0.0.convs.1.conv.weight shape torch.Size([32, 32, 3, 3, 3])
encoder.stages.0.0.convs.1.conv.bias shape torch.Size([32])
encoder.stages.0.0.convs.1.norm.weight shape torch.Size([32])
encoder.stages.0.0.convs.1.norm.bias shape torch.Size([32])
encoder.stages.0.0.convs.1.all_modules.0.weight shape torch.Size([32, 32, 3, 3, 3])
encoder.stages.0.0.convs.1.all_modules.0.bias shape torch.Size([32])
encoder.stages.0.0.convs.1.all_modules.1.weight shape torch.Size([32])
encoder.stages.0.0.convs.1.all_modules.1.bias shape torch.Size([32])
encoder.stages.1.0.convs.0.conv.weight shape torch.Size([64, 32, 3, 3, 3])
encoder.stages.1.0.convs.0.conv.bias shape torch.Size([64])
encoder.stages.1.0.convs.0.norm.weight shape torch.Size([64])
encoder.stages.1.0.convs.0.norm.bias shape torch.Size([64])
encoder.stages.1.0.convs.0.all_modules.0.weight shape torch.Size([64, 32, 3, 3, 3])
encoder.stages.1.0.convs.0.all_modules.0.bias shape torch.Size([64])
encoder.stages.1.0.convs.0.all_modules.1.weight shape torch.Size([64])
encoder.stages.1.0.convs.0.all_modules.1.bias shape torch.Size([64])
encoder.stages.1.0.convs.1.conv.weight shape torch.Size([64, 64, 3, 3, 3])
encoder.stages.1.0.convs.1.conv.bias shape torch.Size([64])
encoder.stages.1.0.convs.1.norm.weight shape torch.Size([64])
encoder.stages.1.0.convs.1.norm.bias shape torch.Size([64])
encoder.stages.1.0.convs.1.all_modules.0.weight shape torch.Size([64, 64, 3, 3, 3])
encoder.stages.1.0.convs.1.all_modules.0.bias shape torch.Size([64])
encoder.stages.1.0.convs.1.all_modules.1.weight shape torch.Size([64])
encoder.stages.1.0.convs.1.all_modules.1.bias shape torch.Size([64])
encoder.stages.2.0.convs.0.conv.weight shape torch.Size([128, 64, 3, 3, 3])
encoder.stages.2.0.convs.0.conv.bias shape torch.Size([128])
encoder.stages.2.0.convs.0.norm.weight shape torch.Size([128])
encoder.stages.2.0.convs.0.norm.bias shape torch.Size([128])
encoder.stages.2.0.convs.0.all_modules.0.weight shape torch.Size([128, 64, 3, 3, 3])
encoder.stages.2.0.convs.0.all_modules.0.bias shape torch.Size([128])
encoder.stages.2.0.convs.0.all_modules.1.weight shape torch.Size([128])
encoder.stages.2.0.convs.0.all_modules.1.bias shape torch.Size([128])
encoder.stages.2.0.convs.1.conv.weight shape torch.Size([128, 128, 3, 3, 3])
encoder.stages.2.0.convs.1.conv.bias shape torch.Size([128])
encoder.stages.2.0.convs.1.norm.weight shape torch.Size([128])
encoder.stages.2.0.convs.1.norm.bias shape torch.Size([128])
encoder.stages.2.0.convs.1.all_modules.0.weight shape torch.Size([128, 128, 3, 3, 3])
encoder.stages.2.0.convs.1.all_modules.0.bias shape torch.Size([128])
encoder.stages.2.0.convs.1.all_modules.1.weight shape torch.Size([128])
encoder.stages.2.0.convs.1.all_modules.1.bias shape torch.Size([128])
encoder.stages.3.0.convs.0.conv.weight shape torch.Size([256, 128, 3, 3, 3])
encoder.stages.3.0.convs.0.conv.bias shape torch.Size([256])
encoder.stages.3.0.convs.0.norm.weight shape torch.Size([256])
encoder.stages.3.0.convs.0.norm.bias shape torch.Size([256])
encoder.stages.3.0.convs.0.all_modules.0.weight shape torch.Size([256, 128, 3, 3, 3])
encoder.stages.3.0.convs.0.all_modules.0.bias shape torch.Size([256])
encoder.stages.3.0.convs.0.all_modules.1.weight shape torch.Size([256])
encoder.stages.3.0.convs.0.all_modules.1.bias shape torch.Size([256])
encoder.stages.3.0.convs.1.conv.weight shape torch.Size([256, 256, 3, 3, 3])
encoder.stages.3.0.convs.1.conv.bias shape torch.Size([256])
encoder.stages.3.0.convs.1.norm.weight shape torch.Size([256])
encoder.stages.3.0.convs.1.norm.bias shape torch.Size([256])
encoder.stages.3.0.convs.1.all_modules.0.weight shape torch.Size([256, 256, 3, 3, 3])
encoder.stages.3.0.convs.1.all_modules.0.bias shape torch.Size([256])
encoder.stages.3.0.convs.1.all_modules.1.weight shape torch.Size([256])
encoder.stages.3.0.convs.1.all_modules.1.bias shape torch.Size([256])
encoder.stages.4.0.convs.0.conv.weight shape torch.Size([320, 256, 3, 3, 3])
encoder.stages.4.0.convs.0.conv.bias shape torch.Size([320])
encoder.stages.4.0.convs.0.norm.weight shape torch.Size([320])
encoder.stages.4.0.convs.0.norm.bias shape torch.Size([320])
encoder.stages.4.0.convs.0.all_modules.0.weight shape torch.Size([320, 256, 3, 3, 3])
encoder.stages.4.0.convs.0.all_modules.0.bias shape torch.Size([320])
encoder.stages.4.0.convs.0.all_modules.1.weight shape torch.Size([320])
encoder.stages.4.0.convs.0.all_modules.1.bias shape torch.Size([320])
encoder.stages.4.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.4.0.convs.1.conv.bias shape torch.Size([320])
encoder.stages.4.0.convs.1.norm.weight shape torch.Size([320])
encoder.stages.4.0.convs.1.norm.bias shape torch.Size([320])
encoder.stages.4.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.4.0.convs.1.all_modules.0.bias shape torch.Size([320])
encoder.stages.4.0.convs.1.all_modules.1.weight shape torch.Size([320])
encoder.stages.4.0.convs.1.all_modules.1.bias shape torch.Size([320])
encoder.stages.5.0.convs.0.conv.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.5.0.convs.0.conv.bias shape torch.Size([320])
encoder.stages.5.0.convs.0.norm.weight shape torch.Size([320])
encoder.stages.5.0.convs.0.norm.bias shape torch.Size([320])
encoder.stages.5.0.convs.0.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.5.0.convs.0.all_modules.0.bias shape torch.Size([320])
encoder.stages.5.0.convs.0.all_modules.1.weight shape torch.Size([320])
encoder.stages.5.0.convs.0.all_modules.1.bias shape torch.Size([320])
encoder.stages.5.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.5.0.convs.1.conv.bias shape torch.Size([320])
encoder.stages.5.0.convs.1.norm.weight shape torch.Size([320])
encoder.stages.5.0.convs.1.norm.bias shape torch.Size([320])
encoder.stages.5.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.5.0.convs.1.all_modules.0.bias shape torch.Size([320])
encoder.stages.5.0.convs.1.all_modules.1.weight shape torch.Size([320])
encoder.stages.5.0.convs.1.all_modules.1.bias shape torch.Size([320])
decoder.encoder.stages.0.0.convs.0.conv.weight shape torch.Size([32, 1, 3, 3, 3])
decoder.encoder.stages.0.0.convs.0.conv.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.norm.weight shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.norm.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.all_modules.0.weight shape torch.Size([32, 1, 3, 3, 3])
decoder.encoder.stages.0.0.convs.0.all_modules.0.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.all_modules.1.weight shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.all_modules.1.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.conv.weight shape torch.Size([32, 32, 3, 3, 3])
decoder.encoder.stages.0.0.convs.1.conv.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.norm.weight shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.norm.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.all_modules.0.weight shape torch.Size([32, 32, 3, 3, 3])
decoder.encoder.stages.0.0.convs.1.all_modules.0.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.all_modules.1.weight shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.all_modules.1.bias shape torch.Size([32])
decoder.encoder.stages.1.0.convs.0.conv.weight shape torch.Size([64, 32, 3, 3, 3])
decoder.encoder.stages.1.0.convs.0.conv.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.norm.weight shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.norm.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.all_modules.0.weight shape torch.Size([64, 32, 3, 3, 3])
decoder.encoder.stages.1.0.convs.0.all_modules.0.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.all_modules.1.weight shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.all_modules.1.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.conv.weight shape torch.Size([64, 64, 3, 3, 3])
decoder.encoder.stages.1.0.convs.1.conv.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.norm.weight shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.norm.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.all_modules.0.weight shape torch.Size([64, 64, 3, 3, 3])
decoder.encoder.stages.1.0.convs.1.all_modules.0.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.all_modules.1.weight shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.all_modules.1.bias shape torch.Size([64])
decoder.encoder.stages.2.0.convs.0.conv.weight shape torch.Size([128, 64, 3, 3, 3])
decoder.encoder.stages.2.0.convs.0.conv.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.norm.weight shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.norm.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.all_modules.0.weight shape torch.Size([128, 64, 3, 3, 3])
decoder.encoder.stages.2.0.convs.0.all_modules.0.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.all_modules.1.weight shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.all_modules.1.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.conv.weight shape torch.Size([128, 128, 3, 3, 3])
decoder.encoder.stages.2.0.convs.1.conv.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.norm.weight shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.norm.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.all_modules.0.weight shape torch.Size([128, 128, 3, 3, 3])
decoder.encoder.stages.2.0.convs.1.all_modules.0.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.all_modules.1.weight shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.all_modules.1.bias shape torch.Size([128])
decoder.encoder.stages.3.0.convs.0.conv.weight shape torch.Size([256, 128, 3, 3, 3])
decoder.encoder.stages.3.0.convs.0.conv.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.norm.weight shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.norm.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.all_modules.0.weight shape torch.Size([256, 128, 3, 3, 3])
decoder.encoder.stages.3.0.convs.0.all_modules.0.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.all_modules.1.weight shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.all_modules.1.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.conv.weight shape torch.Size([256, 256, 3, 3, 3])
decoder.encoder.stages.3.0.convs.1.conv.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.norm.weight shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.norm.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.all_modules.0.weight shape torch.Size([256, 256, 3, 3, 3])
decoder.encoder.stages.3.0.convs.1.all_modules.0.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.all_modules.1.weight shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.all_modules.1.bias shape torch.Size([256])
decoder.encoder.stages.4.0.convs.0.conv.weight shape torch.Size([320, 256, 3, 3, 3])
decoder.encoder.stages.4.0.convs.0.conv.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.norm.weight shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.norm.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.all_modules.0.weight shape torch.Size([320, 256, 3, 3, 3])
decoder.encoder.stages.4.0.convs.0.all_modules.0.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.all_modules.1.weight shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.all_modules.1.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.4.0.convs.1.conv.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.norm.weight shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.norm.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.4.0.convs.1.all_modules.0.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.all_modules.1.weight shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.all_modules.1.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.conv.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.5.0.convs.0.conv.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.norm.weight shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.norm.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.5.0.convs.0.all_modules.0.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.all_modules.1.weight shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.all_modules.1.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.5.0.convs.1.conv.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.norm.weight shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.norm.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.5.0.convs.1.all_modules.0.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.all_modules.1.weight shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.all_modules.1.bias shape torch.Size([320])
decoder.stages.0.convs.0.conv.weight shape torch.Size([320, 640, 3, 3, 3])
decoder.stages.0.convs.0.conv.bias shape torch.Size([320])
decoder.stages.0.convs.0.norm.weight shape torch.Size([320])
decoder.stages.0.convs.0.norm.bias shape torch.Size([320])
decoder.stages.0.convs.0.all_modules.0.weight shape torch.Size([320, 640, 3, 3, 3])
decoder.stages.0.convs.0.all_modules.0.bias shape torch.Size([320])
decoder.stages.0.convs.0.all_modules.1.weight shape torch.Size([320])
decoder.stages.0.convs.0.all_modules.1.bias shape torch.Size([320])
decoder.stages.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.stages.0.convs.1.conv.bias shape torch.Size([320])
decoder.stages.0.convs.1.norm.weight shape torch.Size([320])
decoder.stages.0.convs.1.norm.bias shape torch.Size([320])
decoder.stages.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.stages.0.convs.1.all_modules.0.bias shape torch.Size([320])
decoder.stages.0.convs.1.all_modules.1.weight shape torch.Size([320])
decoder.stages.0.convs.1.all_modules.1.bias shape torch.Size([320])
decoder.stages.1.convs.0.conv.weight shape torch.Size([256, 512, 3, 3, 3])
decoder.stages.1.convs.0.conv.bias shape torch.Size([256])
decoder.stages.1.convs.0.norm.weight shape torch.Size([256])
decoder.stages.1.convs.0.norm.bias shape torch.Size([256])
decoder.stages.1.convs.0.all_modules.0.weight shape torch.Size([256, 512, 3, 3, 3])
decoder.stages.1.convs.0.all_modules.0.bias shape torch.Size([256])
decoder.stages.1.convs.0.all_modules.1.weight shape torch.Size([256])
decoder.stages.1.convs.0.all_modules.1.bias shape torch.Size([256])
decoder.stages.1.convs.1.conv.weight shape torch.Size([256, 256, 3, 3, 3])
decoder.stages.1.convs.1.conv.bias shape torch.Size([256])
decoder.stages.1.convs.1.norm.weight shape torch.Size([256])
decoder.stages.1.convs.1.norm.bias shape torch.Size([256])
decoder.stages.1.convs.1.all_modules.0.weight shape torch.Size([256, 256, 3, 3, 3])
decoder.stages.1.convs.1.all_modules.0.bias shape torch.Size([256])
decoder.stages.1.convs.1.all_modules.1.weight shape torch.Size([256])
decoder.stages.1.convs.1.all_modules.1.bias shape torch.Size([256])
decoder.stages.2.convs.0.conv.weight shape torch.Size([128, 256, 3, 3, 3])
decoder.stages.2.convs.0.conv.bias shape torch.Size([128])
decoder.stages.2.convs.0.norm.weight shape torch.Size([128])
decoder.stages.2.convs.0.norm.bias shape torch.Size([128])
decoder.stages.2.convs.0.all_modules.0.weight shape torch.Size([128, 256, 3, 3, 3])
decoder.stages.2.convs.0.all_modules.0.bias shape torch.Size([128])
decoder.stages.2.convs.0.all_modules.1.weight shape torch.Size([128])
decoder.stages.2.convs.0.all_modules.1.bias shape torch.Size([128])
decoder.stages.2.convs.1.conv.weight shape torch.Size([128, 128, 3, 3, 3])
decoder.stages.2.convs.1.conv.bias shape torch.Size([128])
decoder.stages.2.convs.1.norm.weight shape torch.Size([128])
decoder.stages.2.convs.1.norm.bias shape torch.Size([128])
decoder.stages.2.convs.1.all_modules.0.weight shape torch.Size([128, 128, 3, 3, 3])
decoder.stages.2.convs.1.all_modules.0.bias shape torch.Size([128])
decoder.stages.2.convs.1.all_modules.1.weight shape torch.Size([128])
decoder.stages.2.convs.1.all_modules.1.bias shape torch.Size([128])
decoder.stages.3.convs.0.conv.weight shape torch.Size([64, 128, 3, 3, 3])
decoder.stages.3.convs.0.conv.bias shape torch.Size([64])
decoder.stages.3.convs.0.norm.weight shape torch.Size([64])
decoder.stages.3.convs.0.norm.bias shape torch.Size([64])
decoder.stages.3.convs.0.all_modules.0.weight shape torch.Size([64, 128, 3, 3, 3])
decoder.stages.3.convs.0.all_modules.0.bias shape torch.Size([64])
decoder.stages.3.convs.0.all_modules.1.weight shape torch.Size([64])
decoder.stages.3.convs.0.all_modules.1.bias shape torch.Size([64])
decoder.stages.3.convs.1.conv.weight shape torch.Size([64, 64, 3, 3, 3])
decoder.stages.3.convs.1.conv.bias shape torch.Size([64])
decoder.stages.3.convs.1.norm.weight shape torch.Size([64])
decoder.stages.3.convs.1.norm.bias shape torch.Size([64])
decoder.stages.3.convs.1.all_modules.0.weight shape torch.Size([64, 64, 3, 3, 3])
decoder.stages.3.convs.1.all_modules.0.bias shape torch.Size([64])
decoder.stages.3.convs.1.all_modules.1.weight shape torch.Size([64])
decoder.stages.3.convs.1.all_modules.1.bias shape torch.Size([64])
decoder.stages.4.convs.0.conv.weight shape torch.Size([32, 64, 3, 3, 3])
decoder.stages.4.convs.0.conv.bias shape torch.Size([32])
decoder.stages.4.convs.0.norm.weight shape torch.Size([32])
decoder.stages.4.convs.0.norm.bias shape torch.Size([32])
decoder.stages.4.convs.0.all_modules.0.weight shape torch.Size([32, 64, 3, 3, 3])
decoder.stages.4.convs.0.all_modules.0.bias shape torch.Size([32])
decoder.stages.4.convs.0.all_modules.1.weight shape torch.Size([32])
decoder.stages.4.convs.0.all_modules.1.bias shape torch.Size([32])
decoder.stages.4.convs.1.conv.weight shape torch.Size([32, 32, 3, 3, 3])
decoder.stages.4.convs.1.conv.bias shape torch.Size([32])
decoder.stages.4.convs.1.norm.weight shape torch.Size([32])
decoder.stages.4.convs.1.norm.bias shape torch.Size([32])
decoder.stages.4.convs.1.all_modules.0.weight shape torch.Size([32, 32, 3, 3, 3])
decoder.stages.4.convs.1.all_modules.0.bias shape torch.Size([32])
decoder.stages.4.convs.1.all_modules.1.weight shape torch.Size([32])
decoder.stages.4.convs.1.all_modules.1.bias shape torch.Size([32])
decoder.transpconvs.0.weight shape torch.Size([320, 320, 1, 2, 2])
decoder.transpconvs.0.bias shape torch.Size([320])
decoder.transpconvs.1.weight shape torch.Size([320, 256, 2, 2, 2])
decoder.transpconvs.1.bias shape torch.Size([256])
decoder.transpconvs.2.weight shape torch.Size([256, 128, 2, 2, 2])
decoder.transpconvs.2.bias shape torch.Size([128])
decoder.transpconvs.3.weight shape torch.Size([128, 64, 2, 2, 2])
decoder.transpconvs.3.bias shape torch.Size([64])
decoder.transpconvs.4.weight shape torch.Size([64, 32, 2, 2, 2])
decoder.transpconvs.4.bias shape torch.Size([32])
################### Done ###################
2024-12-19 04:43:55.842277: do_dummy_2d_data_aug: True
2024-12-19 04:43:55.867105: Using splits from existing split file: /home/gridsan/nchutisilp/datasets/nnUNet_Datasets/nnUNet_preprocessed/Dataset307_Sohee_Calcium_OCT_CrossValidation/splits_final_20.json
2024-12-19 04:43:55.882200: The split file contains 5 splits.
2024-12-19 04:43:55.883775: Desired fold for training: 3
2024-12-19 04:43:55.884623: This split has 1 training and 7 validation cases.
using pin_memory on device 0
using pin_memory on device 0
using pin_memory on device 0

This is the configuration used by this training:
Configuration name: 3d_32x160x128_b10
 {'data_identifier': 'nnUNetPlans_3d_fullres', 'preprocessor_name': 'DefaultPreprocessor', 'batch_size': 10, 'patch_size': [32, 160, 128], 'median_image_size_in_voxels': [375.0, 498.0, 498.0], 'spacing': [1.0, 1.0, 1.0], 'normalization_schemes': ['ZScoreNormalization'], 'use_mask_for_norm': [False], 'resampling_fn_data': 'resample_data_or_seg_to_shape', 'resampling_fn_seg': 'resample_data_or_seg_to_shape', 'resampling_fn_data_kwargs': {'is_seg': False, 'order': 3, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_seg_kwargs': {'is_seg': True, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_probabilities': 'resample_data_or_seg_to_shape', 'resampling_fn_probabilities_kwargs': {'is_seg': False, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'architecture': {'network_class_name': 'dynamic_network_architectures.architectures.unet.PlainConvUNet', 'arch_kwargs': {'n_stages': 6, 'features_per_stage': [32, 64, 128, 256, 320, 320], 'conv_op': 'torch.nn.modules.conv.Conv3d', 'kernel_sizes': [[3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3]], 'strides': [[1, 1, 1], [2, 2, 2], [2, 2, 2], [2, 2, 2], [2, 2, 2], [1, 2, 2]], 'n_conv_per_stage': [2, 2, 2, 2, 2, 2], 'n_conv_per_stage_decoder': [2, 2, 2, 2, 2], 'conv_bias': True, 'norm_op': 'torch.nn.modules.instancenorm.InstanceNorm3d', 'norm_op_kwargs': {'eps': 1e-05, 'affine': True}, 'dropout_op': None, 'dropout_op_kwargs': None, 'nonlin': 'torch.nn.LeakyReLU', 'nonlin_kwargs': {'inplace': True}, 'deep_supervision': True}, '_kw_requires_import': ['conv_op', 'norm_op', 'dropout_op', 'nonlin']}, 'batch_dice': True, 'inherits_from': '3d_fullres'} 

These are the global plan.json settings:
 {'dataset_name': 'Dataset307_Sohee_Calcium_OCT_CrossValidation', 'plans_name': 'nnUNetPlans', 'original_median_spacing_after_transp': [1.0, 1.0, 1.0], 'original_median_shape_after_transp': [375, 498, 498], 'image_reader_writer': 'SimpleITKIO', 'transpose_forward': [0, 1, 2], 'transpose_backward': [0, 1, 2], 'experiment_planner_used': 'ExperimentPlanner', 'label_manager': 'LabelManager', 'foreground_intensity_properties_per_channel': {'0': {'max': 0.4977976381778717, 'mean': 0.13507525622844696, 'median': 0.09599608182907104, 'min': 0.0, 'percentile_00_5': 0.014754901640117168, 'percentile_99_5': 0.4977976381778717, 'std': 0.12025152146816254}}} 

2024-12-19 04:44:10.576611: unpacking dataset...
using pin_memory on device 0

This is the configuration used by this training:
Configuration name: 3d_32x160x128_b10
 {'data_identifier': 'nnUNetPlans_3d_fullres', 'preprocessor_name': 'DefaultPreprocessor', 'batch_size': 10, 'patch_size': [32, 160, 128], 'median_image_size_in_voxels': [375.0, 498.0, 498.0], 'spacing': [1.0, 1.0, 1.0], 'normalization_schemes': ['ZScoreNormalization'], 'use_mask_for_norm': [False], 'resampling_fn_data': 'resample_data_or_seg_to_shape', 'resampling_fn_seg': 'resample_data_or_seg_to_shape', 'resampling_fn_data_kwargs': {'is_seg': False, 'order': 3, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_seg_kwargs': {'is_seg': True, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_probabilities': 'resample_data_or_seg_to_shape', 'resampling_fn_probabilities_kwargs': {'is_seg': False, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'architecture': {'network_class_name': 'dynamic_network_architectures.architectures.unet.PlainConvUNet', 'arch_kwargs': {'n_stages': 6, 'features_per_stage': [32, 64, 128, 256, 320, 320], 'conv_op': 'torch.nn.modules.conv.Conv3d', 'kernel_sizes': [[3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3]], 'strides': [[1, 1, 1], [2, 2, 2], [2, 2, 2], [2, 2, 2], [2, 2, 2], [1, 2, 2]], 'n_conv_per_stage': [2, 2, 2, 2, 2, 2], 'n_conv_per_stage_decoder': [2, 2, 2, 2, 2], 'conv_bias': True, 'norm_op': 'torch.nn.modules.instancenorm.InstanceNorm3d', 'norm_op_kwargs': {'eps': 1e-05, 'affine': True}, 'dropout_op': None, 'dropout_op_kwargs': None, 'nonlin': 'torch.nn.LeakyReLU', 'nonlin_kwargs': {'inplace': True}, 'deep_supervision': True}, '_kw_requires_import': ['conv_op', 'norm_op', 'dropout_op', 'nonlin']}, 'batch_dice': True, 'inherits_from': '3d_fullres'} 

These are the global plan.json settings:
 {'dataset_name': 'Dataset307_Sohee_Calcium_OCT_CrossValidation', 'plans_name': 'nnUNetPlans', 'original_median_spacing_after_transp': [1.0, 1.0, 1.0], 'original_median_shape_after_transp': [375, 498, 498], 'image_reader_writer': 'SimpleITKIO', 'transpose_forward': [0, 1, 2], 'transpose_backward': [0, 1, 2], 'experiment_planner_used': 'ExperimentPlanner', 'label_manager': 'LabelManager', 'foreground_intensity_properties_per_channel': {'0': {'max': 0.4977976381778717, 'mean': 0.13507525622844696, 'median': 0.09599608182907104, 'min': 0.0, 'percentile_00_5': 0.014754901640117168, 'percentile_99_5': 0.4977976381778717, 'std': 0.12025152146816254}}} 

2024-12-19 04:44:10.817889: unpacking dataset...
2024-12-19 04:44:14.782434: unpacking done...
2024-12-19 04:44:15.282661: Unable to plot network architecture: nnUNet_compile is enabled!
2024-12-19 04:44:15.559043: 
2024-12-19 04:44:15.560270: Epoch 0
2024-12-19 04:44:15.561531: Current learning rate: 0.01
2024-12-19 04:47:45.413247: Validation loss improved from 1000.00000 to -0.16199! Patience: 0/50
2024-12-19 04:47:45.414728: train_loss -0.1435
2024-12-19 04:47:45.416066: val_loss -0.162
2024-12-19 04:47:45.417056: Pseudo dice [0.5177]
2024-12-19 04:47:45.418040: Epoch time: 209.86 s
2024-12-19 04:47:45.418995: Yayy! New best EMA pseudo Dice: 0.5177
2024-12-19 04:47:47.658094: 
2024-12-19 04:47:47.659917: Epoch 1
2024-12-19 04:47:47.660990: Current learning rate: 0.00994
2024-12-19 04:49:16.151836: Validation loss improved from -0.16199 to -0.21044! Patience: 0/50
2024-12-19 04:49:16.152972: train_loss -0.3238
2024-12-19 04:49:16.153745: val_loss -0.2104
2024-12-19 04:49:16.154545: Pseudo dice [0.5572]
2024-12-19 04:49:16.155307: Epoch time: 88.5 s
2024-12-19 04:49:16.156090: Yayy! New best EMA pseudo Dice: 0.5216
2024-12-19 04:49:17.782911: 
2024-12-19 04:49:17.784975: Epoch 2
2024-12-19 04:49:17.786082: Current learning rate: 0.00988
2024-12-19 04:50:47.162333: Validation loss did not improve from -0.21044. Patience: 1/50
2024-12-19 04:50:47.163361: train_loss -0.3876
2024-12-19 04:50:47.164319: val_loss -0.203
2024-12-19 04:50:47.165254: Pseudo dice [0.5661]
2024-12-19 04:50:47.166210: Epoch time: 89.38 s
2024-12-19 04:50:47.167104: Yayy! New best EMA pseudo Dice: 0.526
2024-12-19 04:50:48.837677: 
2024-12-19 04:50:48.839491: Epoch 3
2024-12-19 04:50:48.840484: Current learning rate: 0.00982
2024-12-19 04:52:18.324916: Validation loss did not improve from -0.21044. Patience: 2/50
2024-12-19 04:52:18.325770: train_loss -0.4096
2024-12-19 04:52:18.326593: val_loss -0.1927
2024-12-19 04:52:18.327383: Pseudo dice [0.5317]
2024-12-19 04:52:18.328200: Epoch time: 89.49 s
2024-12-19 04:52:18.328846: Yayy! New best EMA pseudo Dice: 0.5266
2024-12-19 04:52:19.935863: 
2024-12-19 04:52:19.938059: Epoch 4
2024-12-19 04:52:19.939219: Current learning rate: 0.00976
2024-12-19 04:53:49.458714: Validation loss improved from -0.21044 to -0.22253! Patience: 2/50
2024-12-19 04:53:49.459687: train_loss -0.4613
2024-12-19 04:53:49.460751: val_loss -0.2225
2024-12-19 04:53:49.461454: Pseudo dice [0.5796]
2024-12-19 04:53:49.462186: Epoch time: 89.52 s
2024-12-19 04:53:49.813695: Yayy! New best EMA pseudo Dice: 0.5319
2024-12-19 04:53:51.467591: 
2024-12-19 04:53:51.469510: Epoch 5
2024-12-19 04:53:51.470747: Current learning rate: 0.0097
2024-12-19 04:55:20.994274: Validation loss improved from -0.22253 to -0.23656! Patience: 0/50
2024-12-19 04:55:20.995403: train_loss -0.496
2024-12-19 04:55:20.996417: val_loss -0.2366
2024-12-19 04:55:20.997101: Pseudo dice [0.598]
2024-12-19 04:55:20.997728: Epoch time: 89.53 s
2024-12-19 04:55:20.998330: Yayy! New best EMA pseudo Dice: 0.5385
2024-12-19 04:55:22.615767: 
2024-12-19 04:55:22.617360: Epoch 6
2024-12-19 04:55:22.618145: Current learning rate: 0.00964
2024-12-19 04:56:52.167858: Validation loss did not improve from -0.23656. Patience: 1/50
2024-12-19 04:56:52.169173: train_loss -0.514
2024-12-19 04:56:52.170192: val_loss -0.2046
2024-12-19 04:56:52.171139: Pseudo dice [0.5887]
2024-12-19 04:56:52.172317: Epoch time: 89.55 s
2024-12-19 04:56:52.173242: Yayy! New best EMA pseudo Dice: 0.5435
2024-12-19 04:56:53.833333: 
2024-12-19 04:56:53.835228: Epoch 7
2024-12-19 04:56:53.836364: Current learning rate: 0.00958
2024-12-19 04:58:23.481262: Validation loss improved from -0.23656 to -0.26825! Patience: 1/50
2024-12-19 04:58:23.482115: train_loss -0.5287
2024-12-19 04:58:23.482967: val_loss -0.2682
2024-12-19 04:58:23.483687: Pseudo dice [0.6135]
2024-12-19 04:58:23.484690: Epoch time: 89.65 s
2024-12-19 04:58:23.485504: Yayy! New best EMA pseudo Dice: 0.5505
2024-12-19 04:58:25.492520: 
2024-12-19 04:58:25.494208: Epoch 8
2024-12-19 04:58:25.495071: Current learning rate: 0.00952
2024-12-19 04:59:55.467936: Validation loss did not improve from -0.26825. Patience: 1/50
2024-12-19 04:59:55.469263: train_loss -0.5436
2024-12-19 04:59:55.470002: val_loss -0.2588
2024-12-19 04:59:55.470650: Pseudo dice [0.6105]
2024-12-19 04:59:55.471316: Epoch time: 89.98 s
2024-12-19 04:59:55.471999: Yayy! New best EMA pseudo Dice: 0.5565
2024-12-19 04:59:57.074384: 
2024-12-19 04:59:57.076047: Epoch 9
2024-12-19 04:59:57.076847: Current learning rate: 0.00946
2024-12-19 05:01:26.968920: Validation loss did not improve from -0.26825. Patience: 2/50
2024-12-19 05:01:26.969609: train_loss -0.5645
2024-12-19 05:01:26.970274: val_loss -0.2169
2024-12-19 05:01:26.971102: Pseudo dice [0.5897]
2024-12-19 05:01:26.971831: Epoch time: 89.9 s
2024-12-19 05:01:27.354137: Yayy! New best EMA pseudo Dice: 0.5598
2024-12-19 05:01:28.874827: 
2024-12-19 05:01:28.876786: Epoch 10
2024-12-19 05:01:28.877638: Current learning rate: 0.0094
2024-12-19 05:02:58.680220: Validation loss did not improve from -0.26825. Patience: 3/50
2024-12-19 05:02:58.681077: train_loss -0.5855
2024-12-19 05:02:58.681904: val_loss -0.2468
2024-12-19 05:02:58.682820: Pseudo dice [0.6059]
2024-12-19 05:02:58.683784: Epoch time: 89.81 s
2024-12-19 05:02:58.684788: Yayy! New best EMA pseudo Dice: 0.5645
2024-12-19 05:03:00.251780: 
2024-12-19 05:03:00.253736: Epoch 11
2024-12-19 05:03:00.254755: Current learning rate: 0.00934
2024-12-19 05:04:29.926421: Validation loss did not improve from -0.26825. Patience: 4/50
2024-12-19 05:04:29.927418: train_loss -0.596
2024-12-19 05:04:29.928228: val_loss -0.2436
2024-12-19 05:04:29.929025: Pseudo dice [0.6068]
2024-12-19 05:04:29.929716: Epoch time: 89.68 s
2024-12-19 05:04:29.930588: Yayy! New best EMA pseudo Dice: 0.5687
2024-12-19 05:04:31.489845: 
2024-12-19 05:04:31.491636: Epoch 12
2024-12-19 05:04:31.492435: Current learning rate: 0.00928
2024-12-19 05:06:01.092959: Validation loss did not improve from -0.26825. Patience: 5/50
2024-12-19 05:06:01.093961: train_loss -0.6049
2024-12-19 05:06:01.095260: val_loss -0.2156
2024-12-19 05:06:01.096210: Pseudo dice [0.602]
2024-12-19 05:06:01.097387: Epoch time: 89.61 s
2024-12-19 05:06:01.098621: Yayy! New best EMA pseudo Dice: 0.572
2024-12-19 05:06:02.717294: 
2024-12-19 05:06:02.718833: Epoch 13
2024-12-19 05:06:02.719855: Current learning rate: 0.00922
2024-12-19 05:07:32.302531: Validation loss did not improve from -0.26825. Patience: 6/50
2024-12-19 05:07:32.304053: train_loss -0.622
2024-12-19 05:07:32.305585: val_loss -0.2038
2024-12-19 05:07:32.306407: Pseudo dice [0.584]
2024-12-19 05:07:32.307513: Epoch time: 89.59 s
2024-12-19 05:07:32.308301: Yayy! New best EMA pseudo Dice: 0.5732
2024-12-19 05:07:33.967996: 
2024-12-19 05:07:33.969956: Epoch 14
2024-12-19 05:07:33.970795: Current learning rate: 0.00916
2024-12-19 05:09:03.536712: Validation loss improved from -0.26825 to -0.28436! Patience: 6/50
2024-12-19 05:09:03.537580: train_loss -0.6316
2024-12-19 05:09:03.538513: val_loss -0.2844
2024-12-19 05:09:03.539441: Pseudo dice [0.6364]
2024-12-19 05:09:03.540188: Epoch time: 89.57 s
2024-12-19 05:09:03.923796: Yayy! New best EMA pseudo Dice: 0.5795
2024-12-19 05:09:05.516358: 
2024-12-19 05:09:05.518187: Epoch 15
2024-12-19 05:09:05.519099: Current learning rate: 0.0091
2024-12-19 05:10:35.015365: Validation loss did not improve from -0.28436. Patience: 1/50
2024-12-19 05:10:35.016491: train_loss -0.6504
2024-12-19 05:10:35.017507: val_loss -0.2099
2024-12-19 05:10:35.018384: Pseudo dice [0.6008]
2024-12-19 05:10:35.019163: Epoch time: 89.5 s
2024-12-19 05:10:35.019809: Yayy! New best EMA pseudo Dice: 0.5817
2024-12-19 05:10:36.610716: 
2024-12-19 05:10:36.612192: Epoch 16
2024-12-19 05:10:36.612956: Current learning rate: 0.00903
2024-12-19 05:12:06.175149: Validation loss improved from -0.28436 to -0.31352! Patience: 1/50
2024-12-19 05:12:06.176155: train_loss -0.6447
2024-12-19 05:12:06.177126: val_loss -0.3135
2024-12-19 05:12:06.177996: Pseudo dice [0.6364]
2024-12-19 05:12:06.178821: Epoch time: 89.57 s
2024-12-19 05:12:06.179799: Yayy! New best EMA pseudo Dice: 0.5871
2024-12-19 05:12:07.814452: 
2024-12-19 05:12:07.816432: Epoch 17
2024-12-19 05:12:07.817490: Current learning rate: 0.00897
2024-12-19 05:13:37.221467: Validation loss did not improve from -0.31352. Patience: 1/50
2024-12-19 05:13:37.222409: train_loss -0.6613
2024-12-19 05:13:37.223199: val_loss -0.2345
2024-12-19 05:13:37.224016: Pseudo dice [0.6065]
2024-12-19 05:13:37.224707: Epoch time: 89.41 s
2024-12-19 05:13:37.225459: Yayy! New best EMA pseudo Dice: 0.5891
2024-12-19 05:13:39.166625: 
2024-12-19 05:13:39.168526: Epoch 18
2024-12-19 05:13:39.169311: Current learning rate: 0.00891
2024-12-19 05:15:08.702866: Validation loss did not improve from -0.31352. Patience: 2/50
2024-12-19 05:15:08.703727: train_loss -0.6517
2024-12-19 05:15:08.704920: val_loss -0.2161
2024-12-19 05:15:08.705657: Pseudo dice [0.5858]
2024-12-19 05:15:08.706447: Epoch time: 89.54 s
2024-12-19 05:15:10.018783: 
2024-12-19 05:15:10.020518: Epoch 19
2024-12-19 05:15:10.021397: Current learning rate: 0.00885
2024-12-19 05:16:39.848829: Validation loss did not improve from -0.31352. Patience: 3/50
2024-12-19 05:16:39.849451: train_loss -0.6701
2024-12-19 05:16:39.850293: val_loss -0.2392
2024-12-19 05:16:39.850944: Pseudo dice [0.6105]
2024-12-19 05:16:39.851749: Epoch time: 89.83 s
2024-12-19 05:16:40.268575: Yayy! New best EMA pseudo Dice: 0.5909
2024-12-19 05:16:41.885962: 
2024-12-19 05:16:41.887541: Epoch 20
2024-12-19 05:16:41.888632: Current learning rate: 0.00879
2024-12-19 05:18:11.648789: Validation loss did not improve from -0.31352. Patience: 4/50
2024-12-19 05:18:11.649754: train_loss -0.6821
2024-12-19 05:18:11.650553: val_loss -0.2609
2024-12-19 05:18:11.651564: Pseudo dice [0.6207]
2024-12-19 05:18:11.652328: Epoch time: 89.76 s
2024-12-19 05:18:11.653031: Yayy! New best EMA pseudo Dice: 0.5939
2024-12-19 05:18:13.356784: 
2024-12-19 05:18:13.358611: Epoch 21
2024-12-19 05:18:13.359635: Current learning rate: 0.00873
2024-12-19 05:19:43.129549: Validation loss did not improve from -0.31352. Patience: 5/50
2024-12-19 05:19:43.130553: train_loss -0.6869
2024-12-19 05:19:43.131473: val_loss -0.2589
2024-12-19 05:19:43.132361: Pseudo dice [0.6128]
2024-12-19 05:19:43.133128: Epoch time: 89.77 s
2024-12-19 05:19:43.134023: Yayy! New best EMA pseudo Dice: 0.5958
2024-12-19 05:19:44.786063: 
2024-12-19 05:19:44.787778: Epoch 22
2024-12-19 05:19:44.788582: Current learning rate: 0.00867
2024-12-19 05:21:14.561318: Validation loss did not improve from -0.31352. Patience: 6/50
2024-12-19 05:21:14.562658: train_loss -0.6978
2024-12-19 05:21:14.563625: val_loss -0.2554
2024-12-19 05:21:14.564333: Pseudo dice [0.6306]
2024-12-19 05:21:14.565067: Epoch time: 89.78 s
2024-12-19 05:21:14.565785: Yayy! New best EMA pseudo Dice: 0.5993
2024-12-19 05:21:16.162747: 
2024-12-19 05:21:16.164427: Epoch 23
2024-12-19 05:21:16.165553: Current learning rate: 0.00861
2024-12-19 05:22:45.934778: Validation loss did not improve from -0.31352. Patience: 7/50
2024-12-19 05:22:45.935926: train_loss -0.7029
2024-12-19 05:22:45.936969: val_loss -0.2344
2024-12-19 05:22:45.937824: Pseudo dice [0.6181]
2024-12-19 05:22:45.938684: Epoch time: 89.77 s
2024-12-19 05:22:45.939401: Yayy! New best EMA pseudo Dice: 0.6011
2024-12-19 05:22:47.505278: 
2024-12-19 05:22:47.507250: Epoch 24
2024-12-19 05:22:47.508456: Current learning rate: 0.00855
2024-12-19 05:24:17.429415: Validation loss did not improve from -0.31352. Patience: 8/50
2024-12-19 05:24:17.430527: train_loss -0.7069
2024-12-19 05:24:17.431495: val_loss -0.1742
2024-12-19 05:24:17.432270: Pseudo dice [0.5887]
2024-12-19 05:24:17.433098: Epoch time: 89.93 s
2024-12-19 05:24:19.092708: 
2024-12-19 05:24:19.094413: Epoch 25
2024-12-19 05:24:19.095538: Current learning rate: 0.00849
2024-12-19 05:25:48.930693: Validation loss did not improve from -0.31352. Patience: 9/50
2024-12-19 05:25:48.931795: train_loss -0.7191
2024-12-19 05:25:48.932676: val_loss -0.2168
2024-12-19 05:25:48.933643: Pseudo dice [0.6121]
2024-12-19 05:25:48.934435: Epoch time: 89.84 s
2024-12-19 05:25:50.206448: 
2024-12-19 05:25:50.208499: Epoch 26
2024-12-19 05:25:50.209698: Current learning rate: 0.00843
2024-12-19 05:27:20.108234: Validation loss did not improve from -0.31352. Patience: 10/50
2024-12-19 05:27:20.109232: train_loss -0.7239
2024-12-19 05:27:20.110221: val_loss -0.261
2024-12-19 05:27:20.111032: Pseudo dice [0.623]
2024-12-19 05:27:20.112040: Epoch time: 89.9 s
2024-12-19 05:27:20.112788: Yayy! New best EMA pseudo Dice: 0.6033
2024-12-19 05:27:21.690303: 
2024-12-19 05:27:21.692120: Epoch 27
2024-12-19 05:27:21.693251: Current learning rate: 0.00836
2024-12-19 05:28:51.792428: Validation loss did not improve from -0.31352. Patience: 11/50
2024-12-19 05:28:51.793095: train_loss -0.7316
2024-12-19 05:28:51.794027: val_loss -0.1954
2024-12-19 05:28:51.794650: Pseudo dice [0.6044]
2024-12-19 05:28:51.795318: Epoch time: 90.1 s
2024-12-19 05:28:51.796066: Yayy! New best EMA pseudo Dice: 0.6034
2024-12-19 05:28:53.415082: 
2024-12-19 05:28:53.417164: Epoch 28
2024-12-19 05:28:53.418143: Current learning rate: 0.0083
2024-12-19 05:30:23.249327: Validation loss did not improve from -0.31352. Patience: 12/50
2024-12-19 05:30:23.250189: train_loss -0.7313
2024-12-19 05:30:23.250989: val_loss -0.2072
2024-12-19 05:30:23.251773: Pseudo dice [0.6028]
2024-12-19 05:30:23.252571: Epoch time: 89.84 s
2024-12-19 05:30:24.883900: 
2024-12-19 05:30:24.885956: Epoch 29
2024-12-19 05:30:24.886647: Current learning rate: 0.00824
2024-12-19 05:31:54.688559: Validation loss did not improve from -0.31352. Patience: 13/50
2024-12-19 05:31:54.689798: train_loss -0.7335
2024-12-19 05:31:54.690680: val_loss -0.2361
2024-12-19 05:31:54.691449: Pseudo dice [0.6369]
2024-12-19 05:31:54.692124: Epoch time: 89.81 s
2024-12-19 05:31:55.056647: Yayy! New best EMA pseudo Dice: 0.6067
2024-12-19 05:31:56.726221: 
2024-12-19 05:31:56.728153: Epoch 30
2024-12-19 05:31:56.729127: Current learning rate: 0.00818
2024-12-19 05:33:26.576224: Validation loss did not improve from -0.31352. Patience: 14/50
2024-12-19 05:33:26.577319: train_loss -0.7347
2024-12-19 05:33:26.578241: val_loss -0.2289
2024-12-19 05:33:26.579211: Pseudo dice [0.6155]
2024-12-19 05:33:26.580099: Epoch time: 89.85 s
2024-12-19 05:33:26.581085: Yayy! New best EMA pseudo Dice: 0.6076
2024-12-19 05:33:28.222242: 
2024-12-19 05:33:28.224146: Epoch 31
2024-12-19 05:33:28.224858: Current learning rate: 0.00812
2024-12-19 05:34:58.063645: Validation loss did not improve from -0.31352. Patience: 15/50
2024-12-19 05:34:58.064646: train_loss -0.7433
2024-12-19 05:34:58.065498: val_loss -0.236
2024-12-19 05:34:58.066317: Pseudo dice [0.6406]
2024-12-19 05:34:58.067023: Epoch time: 89.84 s
2024-12-19 05:34:58.067755: Yayy! New best EMA pseudo Dice: 0.6109
2024-12-19 05:34:59.718480: 
2024-12-19 05:34:59.720176: Epoch 32
2024-12-19 05:34:59.720976: Current learning rate: 0.00806
2024-12-19 05:36:29.643064: Validation loss did not improve from -0.31352. Patience: 16/50
2024-12-19 05:36:29.644479: train_loss -0.744
2024-12-19 05:36:29.645528: val_loss -0.2149
2024-12-19 05:36:29.646441: Pseudo dice [0.606]
2024-12-19 05:36:29.647786: Epoch time: 89.93 s
2024-12-19 05:36:30.963367: 
2024-12-19 05:36:30.965188: Epoch 33
2024-12-19 05:36:30.966440: Current learning rate: 0.008
2024-12-19 05:38:00.763193: Validation loss did not improve from -0.31352. Patience: 17/50
2024-12-19 05:38:00.764137: train_loss -0.7492
2024-12-19 05:38:00.765279: val_loss -0.2471
2024-12-19 05:38:00.766351: Pseudo dice [0.6276]
2024-12-19 05:38:00.767283: Epoch time: 89.8 s
2024-12-19 05:38:00.768076: Yayy! New best EMA pseudo Dice: 0.6121
2024-12-19 05:38:02.440276: 
2024-12-19 05:38:02.441514: Epoch 34
2024-12-19 05:38:02.442334: Current learning rate: 0.00793
2024-12-19 05:39:32.144033: Validation loss did not improve from -0.31352. Patience: 18/50
2024-12-19 05:39:32.145168: train_loss -0.7495
2024-12-19 05:39:32.146213: val_loss -0.2489
2024-12-19 05:39:32.147122: Pseudo dice [0.6417]
2024-12-19 05:39:32.147927: Epoch time: 89.71 s
2024-12-19 05:39:32.525689: Yayy! New best EMA pseudo Dice: 0.6151
2024-12-19 05:39:34.144382: 
2024-12-19 05:39:34.146108: Epoch 35
2024-12-19 05:39:34.147266: Current learning rate: 0.00787
2024-12-19 05:41:03.883259: Validation loss did not improve from -0.31352. Patience: 19/50
2024-12-19 05:41:03.885297: train_loss -0.7509
2024-12-19 05:41:03.886269: val_loss -0.1778
2024-12-19 05:41:03.887036: Pseudo dice [0.6184]
2024-12-19 05:41:03.887787: Epoch time: 89.74 s
2024-12-19 05:41:03.888615: Yayy! New best EMA pseudo Dice: 0.6154
2024-12-19 05:41:05.546800: 
2024-12-19 05:41:05.548717: Epoch 36
2024-12-19 05:41:05.549655: Current learning rate: 0.00781
2024-12-19 05:42:35.554886: Validation loss did not improve from -0.31352. Patience: 20/50
2024-12-19 05:42:35.555704: train_loss -0.7593
2024-12-19 05:42:35.556515: val_loss -0.2211
2024-12-19 05:42:35.557212: Pseudo dice [0.6262]
2024-12-19 05:42:35.557844: Epoch time: 90.01 s
2024-12-19 05:42:35.558570: Yayy! New best EMA pseudo Dice: 0.6165
2024-12-19 05:42:37.269455: 
2024-12-19 05:42:37.270749: Epoch 37
2024-12-19 05:42:37.271724: Current learning rate: 0.00775
2024-12-19 05:44:07.281433: Validation loss did not improve from -0.31352. Patience: 21/50
2024-12-19 05:44:07.282614: train_loss -0.7567
2024-12-19 05:44:07.283648: val_loss -0.1992
2024-12-19 05:44:07.284679: Pseudo dice [0.61]
2024-12-19 05:44:07.285589: Epoch time: 90.01 s
2024-12-19 05:44:08.594975: 
2024-12-19 05:44:08.596676: Epoch 38
2024-12-19 05:44:08.597457: Current learning rate: 0.00769
2024-12-19 05:45:38.674308: Validation loss did not improve from -0.31352. Patience: 22/50
2024-12-19 05:45:38.675034: train_loss -0.7652
2024-12-19 05:45:38.675957: val_loss -0.2644
2024-12-19 05:45:38.676790: Pseudo dice [0.6321]
2024-12-19 05:45:38.677471: Epoch time: 90.08 s
2024-12-19 05:45:38.678216: Yayy! New best EMA pseudo Dice: 0.6175
2024-12-19 05:45:40.702754: 
2024-12-19 05:45:40.704190: Epoch 39
2024-12-19 05:45:40.704875: Current learning rate: 0.00763
2024-12-19 05:47:10.444104: Validation loss did not improve from -0.31352. Patience: 23/50
2024-12-19 05:47:10.445090: train_loss -0.7674
2024-12-19 05:47:10.446098: val_loss -0.1742
2024-12-19 05:47:10.447118: Pseudo dice [0.6141]
2024-12-19 05:47:10.448108: Epoch time: 89.74 s
2024-12-19 05:47:12.143876: 
2024-12-19 05:47:12.145480: Epoch 40
2024-12-19 05:47:12.146211: Current learning rate: 0.00756
2024-12-19 05:48:41.881490: Validation loss did not improve from -0.31352. Patience: 24/50
2024-12-19 05:48:41.882595: train_loss -0.768
2024-12-19 05:48:41.883468: val_loss -0.2498
2024-12-19 05:48:41.884123: Pseudo dice [0.6439]
2024-12-19 05:48:41.884942: Epoch time: 89.74 s
2024-12-19 05:48:41.885643: Yayy! New best EMA pseudo Dice: 0.6198
2024-12-19 05:48:43.588239: 
2024-12-19 05:48:43.589770: Epoch 41
2024-12-19 05:48:43.590718: Current learning rate: 0.0075
2024-12-19 05:50:13.450842: Validation loss did not improve from -0.31352. Patience: 25/50
2024-12-19 05:50:13.468159: train_loss -0.7705
2024-12-19 05:50:13.469173: val_loss -0.2248
2024-12-19 05:50:13.469935: Pseudo dice [0.636]
2024-12-19 05:50:13.470764: Epoch time: 89.88 s
2024-12-19 05:50:13.471566: Yayy! New best EMA pseudo Dice: 0.6214
2024-12-19 05:50:15.152552: 
2024-12-19 05:50:15.153759: Epoch 42
2024-12-19 05:50:15.154574: Current learning rate: 0.00744
2024-12-19 05:51:44.944489: Validation loss did not improve from -0.31352. Patience: 26/50
2024-12-19 05:51:44.947383: train_loss -0.7743
2024-12-19 05:51:44.949133: val_loss -0.2426
2024-12-19 05:51:44.950172: Pseudo dice [0.6259]
2024-12-19 05:51:44.951174: Epoch time: 89.79 s
2024-12-19 05:51:44.952033: Yayy! New best EMA pseudo Dice: 0.6219
2024-12-19 05:51:46.567535: 
2024-12-19 05:51:46.569172: Epoch 43
2024-12-19 05:51:46.570114: Current learning rate: 0.00738
2024-12-19 05:53:16.267008: Validation loss did not improve from -0.31352. Patience: 27/50
2024-12-19 05:53:16.267725: train_loss -0.7798
2024-12-19 05:53:16.269974: val_loss -0.1785
2024-12-19 05:53:16.271223: Pseudo dice [0.6222]
2024-12-19 05:53:16.272676: Epoch time: 89.7 s
2024-12-19 05:53:16.273736: Yayy! New best EMA pseudo Dice: 0.6219
2024-12-19 05:53:17.962883: 
2024-12-19 05:53:17.964411: Epoch 44
2024-12-19 05:53:17.965059: Current learning rate: 0.00732
2024-12-19 05:54:47.761923: Validation loss did not improve from -0.31352. Patience: 28/50
2024-12-19 05:54:47.763145: train_loss -0.7827
2024-12-19 05:54:47.764080: val_loss -0.1913
2024-12-19 05:54:47.764785: Pseudo dice [0.6194]
2024-12-19 05:54:47.765489: Epoch time: 89.8 s
2024-12-19 05:54:49.381245: 
2024-12-19 05:54:49.382838: Epoch 45
2024-12-19 05:54:49.383586: Current learning rate: 0.00725
2024-12-19 05:56:19.189490: Validation loss did not improve from -0.31352. Patience: 29/50
2024-12-19 05:56:19.190338: train_loss -0.7812
2024-12-19 05:56:19.191405: val_loss -0.1852
2024-12-19 05:56:19.192353: Pseudo dice [0.6128]
2024-12-19 05:56:19.193250: Epoch time: 89.81 s
2024-12-19 05:56:20.466236: 
2024-12-19 05:56:20.467766: Epoch 46
2024-12-19 05:56:20.468536: Current learning rate: 0.00719
2024-12-19 05:57:50.239339: Validation loss did not improve from -0.31352. Patience: 30/50
2024-12-19 05:57:50.240236: train_loss -0.7796
2024-12-19 05:57:50.241244: val_loss -0.2326
2024-12-19 05:57:50.241978: Pseudo dice [0.6462]
2024-12-19 05:57:50.242781: Epoch time: 89.78 s
2024-12-19 05:57:50.243567: Yayy! New best EMA pseudo Dice: 0.6233
2024-12-19 05:57:51.880429: 
2024-12-19 05:57:51.882149: Epoch 47
2024-12-19 05:57:51.883043: Current learning rate: 0.00713
2024-12-19 05:59:21.688360: Validation loss did not improve from -0.31352. Patience: 31/50
2024-12-19 05:59:21.689596: train_loss -0.7833
2024-12-19 05:59:21.690464: val_loss -0.1939
2024-12-19 05:59:21.691256: Pseudo dice [0.6378]
2024-12-19 05:59:21.691959: Epoch time: 89.81 s
2024-12-19 05:59:21.692574: Yayy! New best EMA pseudo Dice: 0.6248
2024-12-19 05:59:23.382926: 
2024-12-19 05:59:23.384814: Epoch 48
2024-12-19 05:59:23.385789: Current learning rate: 0.00707
2024-12-19 06:00:53.562475: Validation loss did not improve from -0.31352. Patience: 32/50
2024-12-19 06:00:53.563402: train_loss -0.7864
2024-12-19 06:00:53.564363: val_loss -0.2164
2024-12-19 06:00:53.565171: Pseudo dice [0.6238]
2024-12-19 06:00:53.566056: Epoch time: 90.19 s
2024-12-19 06:00:54.851662: 
2024-12-19 06:00:54.853254: Epoch 49
2024-12-19 06:00:54.854268: Current learning rate: 0.007
2024-12-19 06:02:24.905278: Validation loss did not improve from -0.31352. Patience: 33/50
2024-12-19 06:02:24.906598: train_loss -0.7898
2024-12-19 06:02:24.907555: val_loss -0.2256
2024-12-19 06:02:24.908524: Pseudo dice [0.6301]
2024-12-19 06:02:24.909359: Epoch time: 90.06 s
2024-12-19 06:02:25.341501: Yayy! New best EMA pseudo Dice: 0.6252
2024-12-19 06:02:27.544611: 
2024-12-19 06:02:27.546622: Epoch 50
2024-12-19 06:02:27.547716: Current learning rate: 0.00694
2024-12-19 06:03:57.555840: Validation loss did not improve from -0.31352. Patience: 34/50
2024-12-19 06:03:57.558046: train_loss -0.7906
2024-12-19 06:03:57.559141: val_loss -0.2279
2024-12-19 06:03:57.559991: Pseudo dice [0.6239]
2024-12-19 06:03:57.560836: Epoch time: 90.01 s
2024-12-19 06:03:58.910938: 
2024-12-19 06:03:58.911982: Epoch 51
2024-12-19 06:03:58.912839: Current learning rate: 0.00688
2024-12-19 06:05:28.938455: Validation loss did not improve from -0.31352. Patience: 35/50
2024-12-19 06:05:28.939647: train_loss -0.7888
2024-12-19 06:05:28.940617: val_loss -0.2217
2024-12-19 06:05:28.941522: Pseudo dice [0.6373]
2024-12-19 06:05:28.942327: Epoch time: 90.03 s
2024-12-19 06:05:28.943096: Yayy! New best EMA pseudo Dice: 0.6263
2024-12-19 06:05:30.583005: 
2024-12-19 06:05:30.584820: Epoch 52
2024-12-19 06:05:30.585779: Current learning rate: 0.00682
2024-12-19 06:07:00.624014: Validation loss did not improve from -0.31352. Patience: 36/50
2024-12-19 06:07:00.624978: train_loss -0.7936
2024-12-19 06:07:00.625855: val_loss -0.1717
2024-12-19 06:07:00.626741: Pseudo dice [0.6282]
2024-12-19 06:07:00.627488: Epoch time: 90.04 s
2024-12-19 06:07:00.628198: Yayy! New best EMA pseudo Dice: 0.6265
2024-12-19 06:07:02.224652: 
2024-12-19 06:07:02.226501: Epoch 53
2024-12-19 06:07:02.227431: Current learning rate: 0.00675
2024-12-19 06:08:32.223436: Validation loss did not improve from -0.31352. Patience: 37/50
2024-12-19 06:08:32.224618: train_loss -0.7963
2024-12-19 06:08:32.225708: val_loss -0.2061
2024-12-19 06:08:32.226545: Pseudo dice [0.6289]
2024-12-19 06:08:32.227341: Epoch time: 90.0 s
2024-12-19 06:08:32.228050: Yayy! New best EMA pseudo Dice: 0.6267
2024-12-19 06:08:33.846967: 
2024-12-19 06:08:33.848329: Epoch 54
2024-12-19 06:08:33.849280: Current learning rate: 0.00669
2024-12-19 06:10:03.680567: Validation loss did not improve from -0.31352. Patience: 38/50
2024-12-19 06:10:03.681499: train_loss -0.7998
2024-12-19 06:10:03.682457: val_loss -0.1881
2024-12-19 06:10:03.683208: Pseudo dice [0.6261]
2024-12-19 06:10:03.683940: Epoch time: 89.84 s
2024-12-19 06:10:05.303537: 
2024-12-19 06:10:05.304836: Epoch 55
2024-12-19 06:10:05.305606: Current learning rate: 0.00663
2024-12-19 06:11:35.563570: Validation loss did not improve from -0.31352. Patience: 39/50
2024-12-19 06:11:35.564523: train_loss -0.7979
2024-12-19 06:11:35.565222: val_loss -0.2177
2024-12-19 06:11:35.565879: Pseudo dice [0.6438]
2024-12-19 06:11:35.566509: Epoch time: 90.26 s
2024-12-19 06:11:35.567305: Yayy! New best EMA pseudo Dice: 0.6284
2024-12-19 06:11:37.288176: 
2024-12-19 06:11:37.289631: Epoch 56
2024-12-19 06:11:37.290377: Current learning rate: 0.00657
2024-12-19 06:13:07.553473: Validation loss did not improve from -0.31352. Patience: 40/50
2024-12-19 06:13:07.554477: train_loss -0.8029
2024-12-19 06:13:07.555486: val_loss -0.2084
2024-12-19 06:13:07.556525: Pseudo dice [0.6174]
2024-12-19 06:13:07.557610: Epoch time: 90.27 s
2024-12-19 06:13:08.844744: 
2024-12-19 06:13:08.845592: Epoch 57
2024-12-19 06:13:08.846339: Current learning rate: 0.0065
2024-12-19 06:14:39.017233: Validation loss did not improve from -0.31352. Patience: 41/50
2024-12-19 06:14:39.018372: train_loss -0.8007
2024-12-19 06:14:39.019610: val_loss -0.2296
2024-12-19 06:14:39.020387: Pseudo dice [0.6311]
2024-12-19 06:14:39.021235: Epoch time: 90.17 s
2024-12-19 06:14:40.353309: 
2024-12-19 06:14:40.354799: Epoch 58
2024-12-19 06:14:40.355507: Current learning rate: 0.00644
2024-12-19 06:16:10.677313: Validation loss did not improve from -0.31352. Patience: 42/50
2024-12-19 06:16:10.678207: train_loss -0.8045
2024-12-19 06:16:10.679152: val_loss -0.181
2024-12-19 06:16:10.679929: Pseudo dice [0.6293]
2024-12-19 06:16:10.680618: Epoch time: 90.33 s
2024-12-19 06:16:11.966891: 
2024-12-19 06:16:11.968727: Epoch 59
2024-12-19 06:16:11.969934: Current learning rate: 0.00638
2024-12-19 06:17:42.251286: Validation loss did not improve from -0.31352. Patience: 43/50
2024-12-19 06:17:42.252561: train_loss -0.804
2024-12-19 06:17:42.253868: val_loss -0.19
2024-12-19 06:17:42.254713: Pseudo dice [0.6218]
2024-12-19 06:17:42.255651: Epoch time: 90.29 s
2024-12-19 06:17:43.971029: 
2024-12-19 06:17:43.972542: Epoch 60
2024-12-19 06:17:43.973272: Current learning rate: 0.00631
2024-12-19 06:19:14.650801: Validation loss did not improve from -0.31352. Patience: 44/50
2024-12-19 06:19:14.651996: train_loss -0.8081
2024-12-19 06:19:14.652767: val_loss -0.1811
2024-12-19 06:19:14.653572: Pseudo dice [0.6219]
2024-12-19 06:19:14.654254: Epoch time: 90.68 s
2024-12-19 06:19:15.937739: 
2024-12-19 06:19:15.939383: Epoch 61
2024-12-19 06:19:15.940089: Current learning rate: 0.00625
2024-12-19 06:20:46.043771: Validation loss did not improve from -0.31352. Patience: 45/50
2024-12-19 06:20:46.044800: train_loss -0.8094
2024-12-19 06:20:46.045976: val_loss -0.2134
2024-12-19 06:20:46.047034: Pseudo dice [0.6223]
2024-12-19 06:20:46.048053: Epoch time: 90.11 s
2024-12-19 06:20:47.453413: 
2024-12-19 06:20:47.454950: Epoch 62
2024-12-19 06:20:47.455834: Current learning rate: 0.00619
2024-12-19 06:22:17.572057: Validation loss did not improve from -0.31352. Patience: 46/50
2024-12-19 06:22:17.573154: train_loss -0.8097
2024-12-19 06:22:17.574338: val_loss -0.2201
2024-12-19 06:22:17.575220: Pseudo dice [0.6302]
2024-12-19 06:22:17.576211: Epoch time: 90.12 s
2024-12-19 06:22:18.881338: 
2024-12-19 06:22:18.882704: Epoch 63
2024-12-19 06:22:18.883484: Current learning rate: 0.00612
2024-12-19 06:23:48.937549: Validation loss did not improve from -0.31352. Patience: 47/50
2024-12-19 06:23:48.938549: train_loss -0.8139
2024-12-19 06:23:48.939400: val_loss -0.2077
2024-12-19 06:23:48.940243: Pseudo dice [0.6199]
2024-12-19 06:23:48.941046: Epoch time: 90.06 s
2024-12-19 06:23:50.240860: 
2024-12-19 06:23:50.241809: Epoch 64
2024-12-19 06:23:50.242660: Current learning rate: 0.00606
2024-12-19 06:25:20.289717: Validation loss did not improve from -0.31352. Patience: 48/50
2024-12-19 06:25:20.290465: train_loss -0.8108
2024-12-19 06:25:20.291267: val_loss -0.247
2024-12-19 06:25:20.291995: Pseudo dice [0.6402]
2024-12-19 06:25:20.292692: Epoch time: 90.05 s
2024-12-19 06:25:22.190025: 
2024-12-19 06:25:22.191907: Epoch 65
2024-12-19 06:25:22.192872: Current learning rate: 0.006
2024-12-19 06:26:52.192823: Validation loss did not improve from -0.31352. Patience: 49/50
2024-12-19 06:26:52.193949: train_loss -0.811
2024-12-19 06:26:52.195204: val_loss -0.1491
2024-12-19 06:26:52.196252: Pseudo dice [0.6225]
2024-12-19 06:26:52.197197: Epoch time: 90.0 s
2024-12-19 06:26:53.533385: 
2024-12-19 06:26:53.534914: Epoch 66
2024-12-19 06:26:53.535844: Current learning rate: 0.00593
2024-12-19 06:28:23.529152: Validation loss did not improve from -0.31352. Patience: 50/50
2024-12-19 06:28:23.529813: train_loss -0.8169
2024-12-19 06:28:23.530605: val_loss -0.2018
2024-12-19 06:28:23.531291: Pseudo dice [0.6414]
2024-12-19 06:28:23.532201: Epoch time: 90.0 s
2024-12-19 06:28:24.899282: 
2024-12-19 06:28:24.900195: Epoch 67
2024-12-19 06:28:24.900904: Current learning rate: 0.00587
2024-12-19 06:29:54.833802: Validation loss did not improve from -0.31352. Patience: 51/50
2024-12-19 06:29:54.835092: train_loss -0.8183
2024-12-19 06:29:54.836048: val_loss -0.2221
2024-12-19 06:29:54.836725: Pseudo dice [0.6338]
2024-12-19 06:29:54.837571: Epoch time: 89.94 s
2024-12-19 06:29:54.838268: Yayy! New best EMA pseudo Dice: 0.6289
2024-12-19 06:29:56.531588: 
2024-12-19 06:29:56.532924: Epoch 68
2024-12-19 06:29:56.533643: Current learning rate: 0.00581
2024-12-19 06:31:26.440878: Validation loss did not improve from -0.31352. Patience: 52/50
2024-12-19 06:31:26.441810: train_loss -0.8197
2024-12-19 06:31:26.442801: val_loss -0.2104
2024-12-19 06:31:26.443712: Pseudo dice [0.6427]
2024-12-19 06:31:26.444408: Epoch time: 89.91 s
2024-12-19 06:31:26.445159: Yayy! New best EMA pseudo Dice: 0.6303
2024-12-19 06:31:28.144805: 
2024-12-19 06:31:28.146424: Epoch 69
2024-12-19 06:31:28.147361: Current learning rate: 0.00574
2024-12-19 06:32:57.864858: Validation loss did not improve from -0.31352. Patience: 53/50
2024-12-19 06:32:57.866046: train_loss -0.8203
2024-12-19 06:32:57.866955: val_loss -0.1703
2024-12-19 06:32:57.867743: Pseudo dice [0.6291]
2024-12-19 06:32:57.868508: Epoch time: 89.72 s
2024-12-19 06:32:59.529651: 
2024-12-19 06:32:59.531129: Epoch 70
2024-12-19 06:32:59.531828: Current learning rate: 0.00568
2024-12-19 06:34:29.529042: Validation loss did not improve from -0.31352. Patience: 54/50
2024-12-19 06:34:29.529907: train_loss -0.8171
2024-12-19 06:34:29.530872: val_loss -0.1734
2024-12-19 06:34:29.531625: Pseudo dice [0.6283]
2024-12-19 06:34:29.532632: Epoch time: 90.0 s
2024-12-19 06:34:31.279626: 
2024-12-19 06:34:31.280922: Epoch 71
2024-12-19 06:34:31.281600: Current learning rate: 0.00562
2024-12-19 06:36:01.491235: Validation loss did not improve from -0.31352. Patience: 55/50
2024-12-19 06:36:01.492345: train_loss -0.8192
2024-12-19 06:36:01.493322: val_loss -0.2017
2024-12-19 06:36:01.494138: Pseudo dice [0.631]
2024-12-19 06:36:01.495176: Epoch time: 90.21 s
2024-12-19 06:36:02.829149: 
2024-12-19 06:36:02.830623: Epoch 72
2024-12-19 06:36:02.831857: Current learning rate: 0.00555
2024-12-19 06:37:33.136873: Validation loss did not improve from -0.31352. Patience: 56/50
2024-12-19 06:37:33.137966: train_loss -0.8206
2024-12-19 06:37:33.139040: val_loss -0.1949
2024-12-19 06:37:33.139749: Pseudo dice [0.6295]
2024-12-19 06:37:33.140702: Epoch time: 90.31 s
2024-12-19 06:37:34.452021: 
2024-12-19 06:37:34.453246: Epoch 73
2024-12-19 06:37:34.454033: Current learning rate: 0.00549
2024-12-19 06:39:04.737496: Validation loss did not improve from -0.31352. Patience: 57/50
2024-12-19 06:39:04.739495: train_loss -0.8237
2024-12-19 06:39:04.741063: val_loss -0.1669
2024-12-19 06:39:04.742392: Pseudo dice [0.6206]
2024-12-19 06:39:04.743160: Epoch time: 90.29 s
2024-12-19 06:39:06.056073: 
2024-12-19 06:39:06.057494: Epoch 74
2024-12-19 06:39:06.058685: Current learning rate: 0.00542
2024-12-19 06:40:36.429147: Validation loss did not improve from -0.31352. Patience: 58/50
2024-12-19 06:40:36.430105: train_loss -0.8244
2024-12-19 06:40:36.431113: val_loss -0.2167
2024-12-19 06:40:36.431993: Pseudo dice [0.6418]
2024-12-19 06:40:36.432742: Epoch time: 90.37 s
2024-12-19 06:40:36.830533: Yayy! New best EMA pseudo Dice: 0.6303
2024-12-19 06:40:38.522285: 
2024-12-19 06:40:38.524117: Epoch 75
2024-12-19 06:40:38.524849: Current learning rate: 0.00536
2024-12-19 06:42:08.802118: Validation loss did not improve from -0.31352. Patience: 59/50
2024-12-19 06:42:08.803648: train_loss -0.8247
2024-12-19 06:42:08.804827: val_loss -0.2159
2024-12-19 06:42:08.805620: Pseudo dice [0.6389]
2024-12-19 06:42:08.806949: Epoch time: 90.28 s
2024-12-19 06:42:08.808221: Yayy! New best EMA pseudo Dice: 0.6312
2024-12-19 06:42:10.551621: 
2024-12-19 06:42:10.553255: Epoch 76
2024-12-19 06:42:10.554451: Current learning rate: 0.00529
2024-12-19 06:43:40.749057: Validation loss did not improve from -0.31352. Patience: 60/50
2024-12-19 06:43:40.750026: train_loss -0.8238
2024-12-19 06:43:40.750798: val_loss -0.1825
2024-12-19 06:43:40.751593: Pseudo dice [0.6303]
2024-12-19 06:43:40.752283: Epoch time: 90.2 s
2024-12-19 06:43:42.089953: 
2024-12-19 06:43:42.091631: Epoch 77
2024-12-19 06:43:42.092369: Current learning rate: 0.00523
2024-12-19 06:45:12.236566: Validation loss did not improve from -0.31352. Patience: 61/50
2024-12-19 06:45:12.237395: train_loss -0.8278
2024-12-19 06:45:12.238327: val_loss -0.2086
2024-12-19 06:45:12.239250: Pseudo dice [0.6349]
2024-12-19 06:45:12.239900: Epoch time: 90.15 s
2024-12-19 06:45:12.240697: Yayy! New best EMA pseudo Dice: 0.6315
2024-12-19 06:45:13.974359: 
2024-12-19 06:45:13.975427: Epoch 78
2024-12-19 06:45:13.976296: Current learning rate: 0.00517
2024-12-19 06:46:44.016909: Validation loss did not improve from -0.31352. Patience: 62/50
2024-12-19 06:46:44.017804: train_loss -0.8267
2024-12-19 06:46:44.018493: val_loss -0.2091
2024-12-19 06:46:44.019154: Pseudo dice [0.6451]
2024-12-19 06:46:44.019825: Epoch time: 90.04 s
2024-12-19 06:46:44.020434: Yayy! New best EMA pseudo Dice: 0.6329
2024-12-19 06:46:45.782499: 
2024-12-19 06:46:45.783707: Epoch 79
2024-12-19 06:46:45.784797: Current learning rate: 0.0051
2024-12-19 06:48:16.553365: Validation loss did not improve from -0.31352. Patience: 63/50
2024-12-19 06:48:16.555050: train_loss -0.8273
2024-12-19 06:48:16.556210: val_loss -0.2368
2024-12-19 06:48:16.557202: Pseudo dice [0.6577]
2024-12-19 06:48:16.558032: Epoch time: 90.77 s
2024-12-19 06:48:16.936887: Yayy! New best EMA pseudo Dice: 0.6353
2024-12-19 06:48:18.658427: 
2024-12-19 06:48:18.660171: Epoch 80
2024-12-19 06:48:18.661331: Current learning rate: 0.00504
2024-12-19 06:49:48.687675: Validation loss did not improve from -0.31352. Patience: 64/50
2024-12-19 06:49:48.688725: train_loss -0.8293
2024-12-19 06:49:48.689516: val_loss -0.1981
2024-12-19 06:49:48.690191: Pseudo dice [0.6394]
2024-12-19 06:49:48.690949: Epoch time: 90.03 s
2024-12-19 06:49:48.691588: Yayy! New best EMA pseudo Dice: 0.6357
2024-12-19 06:49:50.811236: 
2024-12-19 06:49:50.812751: Epoch 81
2024-12-19 06:49:50.813705: Current learning rate: 0.00497
2024-12-19 06:51:20.903812: Validation loss did not improve from -0.31352. Patience: 65/50
2024-12-19 06:51:20.904623: train_loss -0.8289
2024-12-19 06:51:20.905663: val_loss -0.1796
2024-12-19 06:51:20.906521: Pseudo dice [0.6382]
2024-12-19 06:51:20.907356: Epoch time: 90.09 s
2024-12-19 06:51:20.908069: Yayy! New best EMA pseudo Dice: 0.636
2024-12-19 06:51:22.608185: 
2024-12-19 06:51:22.609642: Epoch 82
2024-12-19 06:51:22.610327: Current learning rate: 0.00491
2024-12-19 06:52:52.586354: Validation loss did not improve from -0.31352. Patience: 66/50
2024-12-19 06:52:52.587391: train_loss -0.8318
2024-12-19 06:52:52.588332: val_loss -0.1584
2024-12-19 06:52:52.589000: Pseudo dice [0.6203]
2024-12-19 06:52:52.589731: Epoch time: 89.98 s
2024-12-19 06:52:53.859504: 
2024-12-19 06:52:53.860742: Epoch 83
2024-12-19 06:52:53.861573: Current learning rate: 0.00484
2024-12-19 06:54:23.959389: Validation loss did not improve from -0.31352. Patience: 67/50
2024-12-19 06:54:23.962824: train_loss -0.8321
2024-12-19 06:54:23.964123: val_loss -0.2068
2024-12-19 06:54:23.964837: Pseudo dice [0.6361]
2024-12-19 06:54:23.965594: Epoch time: 90.1 s
2024-12-19 06:54:25.253567: 
2024-12-19 06:54:25.254500: Epoch 84
2024-12-19 06:54:25.255438: Current learning rate: 0.00478
2024-12-19 06:55:55.542914: Validation loss did not improve from -0.31352. Patience: 68/50
2024-12-19 06:55:55.544617: train_loss -0.833
2024-12-19 06:55:55.547135: val_loss -0.2009
2024-12-19 06:55:55.548098: Pseudo dice [0.6438]
2024-12-19 06:55:55.549170: Epoch time: 90.29 s
2024-12-19 06:55:57.241816: 
2024-12-19 06:55:57.243274: Epoch 85
2024-12-19 06:55:57.243978: Current learning rate: 0.00471
2024-12-19 06:57:27.593281: Validation loss did not improve from -0.31352. Patience: 69/50
2024-12-19 06:57:27.594376: train_loss -0.8355
2024-12-19 06:57:27.595826: val_loss -0.2441
2024-12-19 06:57:27.596586: Pseudo dice [0.6444]
2024-12-19 06:57:27.597322: Epoch time: 90.35 s
2024-12-19 06:57:27.598308: Yayy! New best EMA pseudo Dice: 0.6364
2024-12-19 06:57:29.215023: 
2024-12-19 06:57:29.216897: Epoch 86
2024-12-19 06:57:29.217937: Current learning rate: 0.00465
2024-12-19 06:59:00.532352: Validation loss did not improve from -0.31352. Patience: 70/50
2024-12-19 06:59:00.533531: train_loss -0.8368
2024-12-19 06:59:00.534462: val_loss -0.1303
2024-12-19 06:59:00.535309: Pseudo dice [0.6327]
2024-12-19 06:59:00.536246: Epoch time: 91.32 s
2024-12-19 06:59:01.845852: 
2024-12-19 06:59:01.847062: Epoch 87
2024-12-19 06:59:01.847714: Current learning rate: 0.00458
2024-12-19 07:00:32.181049: Validation loss did not improve from -0.31352. Patience: 71/50
2024-12-19 07:00:32.182149: train_loss -0.8365
2024-12-19 07:00:32.183221: val_loss -0.1608
2024-12-19 07:00:32.184041: Pseudo dice [0.6375]
2024-12-19 07:00:32.184725: Epoch time: 90.34 s
2024-12-19 07:00:33.454168: 
2024-12-19 07:00:33.455736: Epoch 88
2024-12-19 07:00:33.456684: Current learning rate: 0.00452
2024-12-19 07:02:03.763453: Validation loss did not improve from -0.31352. Patience: 72/50
2024-12-19 07:02:03.764652: train_loss -0.8371
2024-12-19 07:02:03.765809: val_loss -0.204
2024-12-19 07:02:03.766743: Pseudo dice [0.6364]
2024-12-19 07:02:03.767641: Epoch time: 90.31 s
2024-12-19 07:02:05.042713: 
2024-12-19 07:02:05.044498: Epoch 89
2024-12-19 07:02:05.045519: Current learning rate: 0.00445
2024-12-19 07:03:35.430317: Validation loss did not improve from -0.31352. Patience: 73/50
2024-12-19 07:03:35.431365: train_loss -0.8377
2024-12-19 07:03:35.432020: val_loss -0.194
2024-12-19 07:03:35.432811: Pseudo dice [0.6301]
2024-12-19 07:03:35.433673: Epoch time: 90.39 s
2024-12-19 07:03:37.097784: 
2024-12-19 07:03:37.098987: Epoch 90
2024-12-19 07:03:37.099745: Current learning rate: 0.00438
2024-12-19 07:05:07.470037: Validation loss did not improve from -0.31352. Patience: 74/50
2024-12-19 07:05:07.470950: train_loss -0.8384
2024-12-19 07:05:07.471979: val_loss -0.1501
2024-12-19 07:05:07.472918: Pseudo dice [0.6363]
2024-12-19 07:05:07.473926: Epoch time: 90.37 s
2024-12-19 07:05:08.757150: 
2024-12-19 07:05:08.758769: Epoch 91
2024-12-19 07:05:08.759793: Current learning rate: 0.00432
2024-12-19 07:06:39.146554: Validation loss did not improve from -0.31352. Patience: 75/50
2024-12-19 07:06:39.147362: train_loss -0.8409
2024-12-19 07:06:39.148266: val_loss -0.1543
2024-12-19 07:06:39.149050: Pseudo dice [0.6286]
2024-12-19 07:06:39.149913: Epoch time: 90.39 s
2024-12-19 07:06:41.085232: 
2024-12-19 07:06:41.086605: Epoch 92
2024-12-19 07:06:41.087420: Current learning rate: 0.00425
2024-12-19 07:08:11.459572: Validation loss did not improve from -0.31352. Patience: 76/50
2024-12-19 07:08:11.460590: train_loss -0.8398
2024-12-19 07:08:11.461390: val_loss -0.1583
2024-12-19 07:08:11.462023: Pseudo dice [0.6394]
2024-12-19 07:08:11.462677: Epoch time: 90.38 s
2024-12-19 07:08:12.718633: 
2024-12-19 07:08:12.720197: Epoch 93
2024-12-19 07:08:12.721182: Current learning rate: 0.00419
2024-12-19 07:09:43.208409: Validation loss did not improve from -0.31352. Patience: 77/50
2024-12-19 07:09:43.209596: train_loss -0.841
2024-12-19 07:09:43.210596: val_loss -0.1821
2024-12-19 07:09:43.211566: Pseudo dice [0.6443]
2024-12-19 07:09:43.212406: Epoch time: 90.49 s
2024-12-19 07:09:44.490328: 
2024-12-19 07:09:44.491673: Epoch 94
2024-12-19 07:09:44.492628: Current learning rate: 0.00412
2024-12-19 07:11:15.030184: Validation loss did not improve from -0.31352. Patience: 78/50
2024-12-19 07:11:15.031088: train_loss -0.8455
2024-12-19 07:11:15.032063: val_loss -0.1494
2024-12-19 07:11:15.032760: Pseudo dice [0.6249]
2024-12-19 07:11:15.033535: Epoch time: 90.54 s
2024-12-19 07:11:16.654845: 
2024-12-19 07:11:16.656404: Epoch 95
2024-12-19 07:11:16.657104: Current learning rate: 0.00405
2024-12-19 07:12:46.939691: Validation loss did not improve from -0.31352. Patience: 79/50
2024-12-19 07:12:46.940598: train_loss -0.8437
2024-12-19 07:12:46.941652: val_loss -0.1132
2024-12-19 07:12:46.942491: Pseudo dice [0.6163]
2024-12-19 07:12:46.943303: Epoch time: 90.29 s
2024-12-19 07:12:48.189750: 
2024-12-19 07:12:48.191025: Epoch 96
2024-12-19 07:12:48.191744: Current learning rate: 0.00399
2024-12-19 07:14:18.459248: Validation loss did not improve from -0.31352. Patience: 80/50
2024-12-19 07:14:18.460033: train_loss -0.8454
2024-12-19 07:14:18.460826: val_loss -0.1701
2024-12-19 07:14:18.461539: Pseudo dice [0.6422]
2024-12-19 07:14:18.462286: Epoch time: 90.27 s
2024-12-19 07:14:19.781974: 
2024-12-19 07:14:19.783438: Epoch 97
2024-12-19 07:14:19.784441: Current learning rate: 0.00392
2024-12-19 07:15:50.035507: Validation loss did not improve from -0.31352. Patience: 81/50
2024-12-19 07:15:50.036355: train_loss -0.8456
2024-12-19 07:15:50.037313: val_loss -0.2249
2024-12-19 07:15:50.038156: Pseudo dice [0.6532]
2024-12-19 07:15:50.038923: Epoch time: 90.26 s
2024-12-19 07:15:51.345512: 
2024-12-19 07:15:51.347429: Epoch 98
2024-12-19 07:15:51.348432: Current learning rate: 0.00385
2024-12-19 07:17:21.557301: Validation loss did not improve from -0.31352. Patience: 82/50
2024-12-19 07:17:21.558374: train_loss -0.8474
2024-12-19 07:17:21.559161: val_loss -0.2049
2024-12-19 07:17:21.559847: Pseudo dice [0.6495]
2024-12-19 07:17:21.560557: Epoch time: 90.21 s
2024-12-19 07:17:21.561238: Yayy! New best EMA pseudo Dice: 0.6374
2024-12-19 07:17:23.385453: 
2024-12-19 07:17:23.386975: Epoch 99
2024-12-19 07:17:23.388247: Current learning rate: 0.00379
2024-12-19 07:18:53.595684: Validation loss did not improve from -0.31352. Patience: 83/50
2024-12-19 07:18:53.596867: train_loss -0.8476
2024-12-19 07:18:53.597675: val_loss -0.1603
2024-12-19 07:18:53.598369: Pseudo dice [0.6293]
2024-12-19 07:18:53.599076: Epoch time: 90.21 s
2024-12-19 07:18:55.271907: 
2024-12-19 07:18:55.273805: Epoch 100
2024-12-19 07:18:55.274816: Current learning rate: 0.00372
2024-12-19 07:20:25.534414: Validation loss did not improve from -0.31352. Patience: 84/50
2024-12-19 07:20:25.535078: train_loss -0.8474
2024-12-19 07:20:25.536032: val_loss -0.2017
2024-12-19 07:20:25.537258: Pseudo dice [0.6448]
2024-12-19 07:20:25.538242: Epoch time: 90.26 s
2024-12-19 07:20:25.539156: Yayy! New best EMA pseudo Dice: 0.6374
2024-12-19 07:20:27.201398: 
2024-12-19 07:20:27.202763: Epoch 101
2024-12-19 07:20:27.203855: Current learning rate: 0.00365
2024-12-19 07:21:57.659469: Validation loss did not improve from -0.31352. Patience: 85/50
2024-12-19 07:21:57.660382: train_loss -0.8482
2024-12-19 07:21:57.661146: val_loss -0.1717
2024-12-19 07:21:57.661949: Pseudo dice [0.6338]
2024-12-19 07:21:57.662714: Epoch time: 90.46 s
2024-12-19 07:21:59.584987: 
2024-12-19 07:21:59.586470: Epoch 102
2024-12-19 07:21:59.587188: Current learning rate: 0.00359
2024-12-19 07:23:29.887479: Validation loss did not improve from -0.31352. Patience: 86/50
2024-12-19 07:23:29.888737: train_loss -0.849
2024-12-19 07:23:29.889603: val_loss -0.1406
2024-12-19 07:23:29.890364: Pseudo dice [0.6228]
2024-12-19 07:23:29.890997: Epoch time: 90.3 s
2024-12-19 07:23:31.188209: 
2024-12-19 07:23:31.189448: Epoch 103
2024-12-19 07:23:31.190273: Current learning rate: 0.00352
2024-12-19 07:25:01.654528: Validation loss did not improve from -0.31352. Patience: 87/50
2024-12-19 07:25:01.655353: train_loss -0.8471
2024-12-19 07:25:01.656306: val_loss -0.1703
2024-12-19 07:25:01.657025: Pseudo dice [0.6364]
2024-12-19 07:25:01.657896: Epoch time: 90.47 s
2024-12-19 07:25:02.873542: 
2024-12-19 07:25:02.874862: Epoch 104
2024-12-19 07:25:02.876067: Current learning rate: 0.00345
2024-12-19 07:26:33.105650: Validation loss did not improve from -0.31352. Patience: 88/50
2024-12-19 07:26:33.106651: train_loss -0.8494
2024-12-19 07:26:33.107865: val_loss -0.1378
2024-12-19 07:26:33.108826: Pseudo dice [0.6076]
2024-12-19 07:26:33.109735: Epoch time: 90.23 s
2024-12-19 07:26:34.790203: 
2024-12-19 07:26:34.791738: Epoch 105
2024-12-19 07:26:34.792747: Current learning rate: 0.00338
2024-12-19 07:28:05.089317: Validation loss did not improve from -0.31352. Patience: 89/50
2024-12-19 07:28:05.090535: train_loss -0.8516
2024-12-19 07:28:05.091571: val_loss -0.1721
2024-12-19 07:28:05.092427: Pseudo dice [0.6288]
2024-12-19 07:28:05.093296: Epoch time: 90.3 s
2024-12-19 07:28:06.383004: 
2024-12-19 07:28:06.384704: Epoch 106
2024-12-19 07:28:06.385609: Current learning rate: 0.00332
2024-12-19 07:29:36.737357: Validation loss did not improve from -0.31352. Patience: 90/50
2024-12-19 07:29:36.738659: train_loss -0.8532
2024-12-19 07:29:36.739670: val_loss -0.1684
2024-12-19 07:29:36.740479: Pseudo dice [0.6255]
2024-12-19 07:29:36.741294: Epoch time: 90.36 s
2024-12-19 07:29:38.021278: 
2024-12-19 07:29:38.023054: Epoch 107
2024-12-19 07:29:38.024597: Current learning rate: 0.00325
2024-12-19 07:31:08.368503: Validation loss did not improve from -0.31352. Patience: 91/50
2024-12-19 07:31:08.369262: train_loss -0.8523
2024-12-19 07:31:08.370052: val_loss -0.1776
2024-12-19 07:31:08.370805: Pseudo dice [0.652]
2024-12-19 07:31:08.371558: Epoch time: 90.35 s
2024-12-19 07:31:09.638872: 
2024-12-19 07:31:09.640324: Epoch 108
2024-12-19 07:31:09.641101: Current learning rate: 0.00318
2024-12-19 07:32:39.985907: Validation loss did not improve from -0.31352. Patience: 92/50
2024-12-19 07:32:39.986718: train_loss -0.8544
2024-12-19 07:32:39.987638: val_loss -0.1321
2024-12-19 07:32:39.988400: Pseudo dice [0.6206]
2024-12-19 07:32:39.989115: Epoch time: 90.35 s
2024-12-19 07:32:41.294027: 
2024-12-19 07:32:41.295486: Epoch 109
2024-12-19 07:32:41.296401: Current learning rate: 0.00311
2024-12-19 07:34:11.577828: Validation loss did not improve from -0.31352. Patience: 93/50
2024-12-19 07:34:11.579005: train_loss -0.8549
2024-12-19 07:34:11.579892: val_loss -0.1407
2024-12-19 07:34:11.580652: Pseudo dice [0.6355]
2024-12-19 07:34:11.581399: Epoch time: 90.29 s
2024-12-19 07:34:13.386564: 
2024-12-19 07:34:13.387508: Epoch 110
2024-12-19 07:34:13.388334: Current learning rate: 0.00304
2024-12-19 07:35:43.774564: Validation loss did not improve from -0.31352. Patience: 94/50
2024-12-19 07:35:43.775285: train_loss -0.8558
2024-12-19 07:35:43.776139: val_loss -0.1347
2024-12-19 07:35:43.776913: Pseudo dice [0.6346]
2024-12-19 07:35:43.777604: Epoch time: 90.39 s
2024-12-19 07:35:45.107706: 
2024-12-19 07:35:45.109356: Epoch 111
2024-12-19 07:35:45.110069: Current learning rate: 0.00297
2024-12-19 07:37:15.672232: Validation loss did not improve from -0.31352. Patience: 95/50
2024-12-19 07:37:15.673138: train_loss -0.8569
2024-12-19 07:37:15.674449: val_loss -0.1778
2024-12-19 07:37:15.675391: Pseudo dice [0.6312]
2024-12-19 07:37:15.676213: Epoch time: 90.57 s
2024-12-19 07:37:16.979699: 
2024-12-19 07:37:16.980858: Epoch 112
2024-12-19 07:37:16.981729: Current learning rate: 0.00291
2024-12-19 07:38:47.292594: Validation loss did not improve from -0.31352. Patience: 96/50
2024-12-19 07:38:47.293566: train_loss -0.8571
2024-12-19 07:38:47.294422: val_loss -0.1288
2024-12-19 07:38:47.295314: Pseudo dice [0.628]
2024-12-19 07:38:47.296199: Epoch time: 90.31 s
2024-12-19 07:38:49.057461: 
2024-12-19 07:38:49.059349: Epoch 113
2024-12-19 07:38:49.060230: Current learning rate: 0.00284
2024-12-19 07:40:19.339765: Validation loss did not improve from -0.31352. Patience: 97/50
2024-12-19 07:40:19.341023: train_loss -0.8573
2024-12-19 07:40:19.342033: val_loss -0.1613
2024-12-19 07:40:19.342752: Pseudo dice [0.6471]
2024-12-19 07:40:19.343500: Epoch time: 90.28 s
2024-12-19 07:40:20.652791: 
2024-12-19 07:40:20.654370: Epoch 114
2024-12-19 07:40:20.655136: Current learning rate: 0.00277
2024-12-19 07:41:50.893234: Validation loss did not improve from -0.31352. Patience: 98/50
2024-12-19 07:41:50.893870: train_loss -0.8603
2024-12-19 07:41:50.894654: val_loss -0.1571
2024-12-19 07:41:50.895266: Pseudo dice [0.6399]
2024-12-19 07:41:50.896183: Epoch time: 90.24 s
2024-12-19 07:41:52.764123: 
2024-12-19 07:41:52.765692: Epoch 115
2024-12-19 07:41:52.766640: Current learning rate: 0.0027
2024-12-19 07:43:22.973098: Validation loss did not improve from -0.31352. Patience: 99/50
2024-12-19 07:43:22.974132: train_loss -0.858
2024-12-19 07:43:22.974942: val_loss -0.1306
2024-12-19 07:43:22.975601: Pseudo dice [0.6444]
2024-12-19 07:43:22.976317: Epoch time: 90.21 s
2024-12-19 07:43:24.318265: 
2024-12-19 07:43:24.319531: Epoch 116
2024-12-19 07:43:24.320231: Current learning rate: 0.00263
2024-12-19 07:44:54.634912: Validation loss did not improve from -0.31352. Patience: 100/50
2024-12-19 07:44:54.635756: train_loss -0.8579
2024-12-19 07:44:54.636654: val_loss -0.1351
2024-12-19 07:44:54.637319: Pseudo dice [0.6332]
2024-12-19 07:44:54.637999: Epoch time: 90.32 s
2024-12-19 07:44:56.017131: 
2024-12-19 07:44:56.018845: Epoch 117
2024-12-19 07:44:56.019765: Current learning rate: 0.00256
2024-12-19 07:46:26.363058: Validation loss did not improve from -0.31352. Patience: 101/50
2024-12-19 07:46:26.364394: train_loss -0.8597
2024-12-19 07:46:26.365547: val_loss -0.1401
2024-12-19 07:46:26.366430: Pseudo dice [0.6409]
2024-12-19 07:46:26.367801: Epoch time: 90.35 s
2024-12-19 07:46:27.695674: 
2024-12-19 07:46:27.697336: Epoch 118
2024-12-19 07:46:27.698292: Current learning rate: 0.00249
2024-12-19 07:47:58.008997: Validation loss did not improve from -0.31352. Patience: 102/50
2024-12-19 07:47:58.009804: train_loss -0.8594
2024-12-19 07:47:58.010598: val_loss -0.1742
2024-12-19 07:47:58.011301: Pseudo dice [0.6327]
2024-12-19 07:47:58.012151: Epoch time: 90.32 s
2024-12-19 07:47:59.321074: 
2024-12-19 07:47:59.322827: Epoch 119
2024-12-19 07:47:59.323554: Current learning rate: 0.00242
2024-12-19 07:49:29.607230: Validation loss did not improve from -0.31352. Patience: 103/50
2024-12-19 07:49:29.608210: train_loss -0.8621
2024-12-19 07:49:29.609021: val_loss -0.1531
2024-12-19 07:49:29.609791: Pseudo dice [0.639]
2024-12-19 07:49:29.610613: Epoch time: 90.29 s
2024-12-19 07:49:31.312382: 
2024-12-19 07:49:31.313732: Epoch 120
2024-12-19 07:49:31.314441: Current learning rate: 0.00235
2024-12-19 07:51:01.563439: Validation loss did not improve from -0.31352. Patience: 104/50
2024-12-19 07:51:01.564214: train_loss -0.8613
2024-12-19 07:51:01.565179: val_loss -0.1553
2024-12-19 07:51:01.566069: Pseudo dice [0.6446]
2024-12-19 07:51:01.567182: Epoch time: 90.25 s
2024-12-19 07:51:02.896906: 
2024-12-19 07:51:02.898474: Epoch 121
2024-12-19 07:51:02.899266: Current learning rate: 0.00228
2024-12-19 07:52:34.818449: Validation loss did not improve from -0.31352. Patience: 105/50
2024-12-19 07:52:34.820165: train_loss -0.8624
2024-12-19 07:52:34.821113: val_loss -0.196
2024-12-19 07:52:34.821953: Pseudo dice [0.6561]
2024-12-19 07:52:34.822708: Epoch time: 91.92 s
2024-12-19 07:52:34.823313: Yayy! New best EMA pseudo Dice: 0.6386
2024-12-19 07:52:36.600387: 
2024-12-19 07:52:36.601543: Epoch 122
2024-12-19 07:52:36.602657: Current learning rate: 0.00221
2024-12-19 07:54:06.956749: Validation loss did not improve from -0.31352. Patience: 106/50
2024-12-19 07:54:06.957840: train_loss -0.8629
2024-12-19 07:54:06.958720: val_loss -0.1464
2024-12-19 07:54:06.959405: Pseudo dice [0.6432]
2024-12-19 07:54:06.960077: Epoch time: 90.36 s
2024-12-19 07:54:06.960788: Yayy! New best EMA pseudo Dice: 0.6391
2024-12-19 07:54:08.708897: 
2024-12-19 07:54:08.710479: Epoch 123
2024-12-19 07:54:08.711272: Current learning rate: 0.00214
2024-12-19 07:55:39.036672: Validation loss did not improve from -0.31352. Patience: 107/50
2024-12-19 07:55:39.037809: train_loss -0.863
2024-12-19 07:55:39.038856: val_loss -0.1395
2024-12-19 07:55:39.039766: Pseudo dice [0.6403]
2024-12-19 07:55:39.040539: Epoch time: 90.33 s
2024-12-19 07:55:39.041320: Yayy! New best EMA pseudo Dice: 0.6392
2024-12-19 07:55:41.176021: 
2024-12-19 07:55:41.177834: Epoch 124
2024-12-19 07:55:41.178891: Current learning rate: 0.00207
2024-12-19 07:57:11.679204: Validation loss did not improve from -0.31352. Patience: 108/50
2024-12-19 07:57:11.680070: train_loss -0.8633
2024-12-19 07:57:11.681048: val_loss -0.1679
2024-12-19 07:57:11.682165: Pseudo dice [0.6397]
2024-12-19 07:57:11.683016: Epoch time: 90.5 s
2024-12-19 07:57:12.065156: Yayy! New best EMA pseudo Dice: 0.6393
2024-12-19 07:57:13.732347: 
2024-12-19 07:57:13.734314: Epoch 125
2024-12-19 07:57:13.735325: Current learning rate: 0.00199
2024-12-19 07:58:44.126948: Validation loss did not improve from -0.31352. Patience: 109/50
2024-12-19 07:58:44.128814: train_loss -0.8631
2024-12-19 07:58:44.130083: val_loss -0.1598
2024-12-19 07:58:44.130936: Pseudo dice [0.644]
2024-12-19 07:58:44.131739: Epoch time: 90.4 s
2024-12-19 07:58:44.132364: Yayy! New best EMA pseudo Dice: 0.6397
2024-12-19 07:58:45.765158: 
2024-12-19 07:58:45.766742: Epoch 126
2024-12-19 07:58:45.767389: Current learning rate: 0.00192
2024-12-19 08:00:16.119775: Validation loss did not improve from -0.31352. Patience: 110/50
2024-12-19 08:00:16.121474: train_loss -0.8643
2024-12-19 08:00:16.122778: val_loss -0.1427
2024-12-19 08:00:16.123797: Pseudo dice [0.6439]
2024-12-19 08:00:16.124738: Epoch time: 90.36 s
2024-12-19 08:00:16.125762: Yayy! New best EMA pseudo Dice: 0.6401
2024-12-19 08:00:17.737936: 
2024-12-19 08:00:17.739460: Epoch 127
2024-12-19 08:00:17.740227: Current learning rate: 0.00185
2024-12-19 08:01:48.385250: Validation loss did not improve from -0.31352. Patience: 111/50
2024-12-19 08:01:48.386521: train_loss -0.8663
2024-12-19 08:01:48.387691: val_loss -0.1674
2024-12-19 08:01:48.388652: Pseudo dice [0.6392]
2024-12-19 08:01:48.389413: Epoch time: 90.65 s
2024-12-19 08:01:49.647522: 
2024-12-19 08:01:49.649069: Epoch 128
2024-12-19 08:01:49.649888: Current learning rate: 0.00178
2024-12-19 08:03:20.218908: Validation loss did not improve from -0.31352. Patience: 112/50
2024-12-19 08:03:20.219720: train_loss -0.8647
2024-12-19 08:03:20.221004: val_loss -0.1364
2024-12-19 08:03:20.222086: Pseudo dice [0.6346]
2024-12-19 08:03:20.223231: Epoch time: 90.57 s
2024-12-19 08:03:21.508869: 
2024-12-19 08:03:21.510500: Epoch 129
2024-12-19 08:03:21.511421: Current learning rate: 0.0017
2024-12-19 08:04:52.074662: Validation loss did not improve from -0.31352. Patience: 113/50
2024-12-19 08:04:52.075608: train_loss -0.8669
2024-12-19 08:04:52.076773: val_loss -0.1591
2024-12-19 08:04:52.077823: Pseudo dice [0.6498]
2024-12-19 08:04:52.078674: Epoch time: 90.57 s
2024-12-19 08:04:52.430896: Yayy! New best EMA pseudo Dice: 0.6405
2024-12-19 08:04:54.046343: 
2024-12-19 08:04:54.048200: Epoch 130
2024-12-19 08:04:54.049086: Current learning rate: 0.00163
2024-12-19 08:06:24.531040: Validation loss did not improve from -0.31352. Patience: 114/50
2024-12-19 08:06:24.532140: train_loss -0.8674
2024-12-19 08:06:24.533158: val_loss -0.177
2024-12-19 08:06:24.534367: Pseudo dice [0.6581]
2024-12-19 08:06:24.535363: Epoch time: 90.49 s
2024-12-19 08:06:24.536195: Yayy! New best EMA pseudo Dice: 0.6423
2024-12-19 08:06:26.203132: 
2024-12-19 08:06:26.204906: Epoch 131
2024-12-19 08:06:26.205992: Current learning rate: 0.00156
2024-12-19 08:07:56.722095: Validation loss did not improve from -0.31352. Patience: 115/50
2024-12-19 08:07:56.723331: train_loss -0.8678
2024-12-19 08:07:56.724723: val_loss -0.1636
2024-12-19 08:07:56.725504: Pseudo dice [0.6482]
2024-12-19 08:07:56.726454: Epoch time: 90.52 s
2024-12-19 08:07:56.727248: Yayy! New best EMA pseudo Dice: 0.6429
2024-12-19 08:07:58.333117: 
2024-12-19 08:07:58.334800: Epoch 132
2024-12-19 08:07:58.335725: Current learning rate: 0.00148
2024-12-19 08:09:28.821530: Validation loss did not improve from -0.31352. Patience: 116/50
2024-12-19 08:09:28.822914: train_loss -0.8671
2024-12-19 08:09:28.824247: val_loss -0.1795
2024-12-19 08:09:28.825437: Pseudo dice [0.6427]
2024-12-19 08:09:28.826569: Epoch time: 90.49 s
2024-12-19 08:09:30.159405: 
2024-12-19 08:09:30.161181: Epoch 133
2024-12-19 08:09:30.162373: Current learning rate: 0.00141
2024-12-19 08:11:00.290593: Validation loss did not improve from -0.31352. Patience: 117/50
2024-12-19 08:11:00.291782: train_loss -0.8669
2024-12-19 08:11:00.292728: val_loss -0.1319
2024-12-19 08:11:00.293554: Pseudo dice [0.6343]
2024-12-19 08:11:00.294414: Epoch time: 90.13 s
2024-12-19 08:11:02.211379: 
2024-12-19 08:11:02.213582: Epoch 134
2024-12-19 08:11:02.214561: Current learning rate: 0.00133
2024-12-19 08:12:32.166808: Validation loss did not improve from -0.31352. Patience: 118/50
2024-12-19 08:12:32.168003: train_loss -0.8693
2024-12-19 08:12:32.169002: val_loss -0.1572
2024-12-19 08:12:32.169757: Pseudo dice [0.6484]
2024-12-19 08:12:32.170513: Epoch time: 89.96 s
2024-12-19 08:12:33.854711: 
2024-12-19 08:12:33.856600: Epoch 135
2024-12-19 08:12:33.857462: Current learning rate: 0.00126
2024-12-19 08:14:03.859197: Validation loss did not improve from -0.31352. Patience: 119/50
2024-12-19 08:14:03.860142: train_loss -0.8678
2024-12-19 08:14:03.860926: val_loss -0.1226
2024-12-19 08:14:03.861778: Pseudo dice [0.6396]
2024-12-19 08:14:03.862459: Epoch time: 90.01 s
2024-12-19 08:14:05.198771: 
2024-12-19 08:14:05.200371: Epoch 136
2024-12-19 08:14:05.201247: Current learning rate: 0.00118
2024-12-19 08:15:35.268129: Validation loss did not improve from -0.31352. Patience: 120/50
2024-12-19 08:15:35.269588: train_loss -0.868
2024-12-19 08:15:35.270875: val_loss -0.1266
2024-12-19 08:15:35.271960: Pseudo dice [0.6298]
2024-12-19 08:15:35.273079: Epoch time: 90.07 s
2024-12-19 08:15:36.558185: 
2024-12-19 08:15:36.560150: Epoch 137
2024-12-19 08:15:36.561418: Current learning rate: 0.00111
2024-12-19 08:17:06.543431: Validation loss did not improve from -0.31352. Patience: 121/50
2024-12-19 08:17:06.544449: train_loss -0.8712
2024-12-19 08:17:06.545238: val_loss -0.141
2024-12-19 08:17:06.546202: Pseudo dice [0.6468]
2024-12-19 08:17:06.546827: Epoch time: 89.99 s
2024-12-19 08:17:07.906947: 
2024-12-19 08:17:07.908602: Epoch 138
2024-12-19 08:17:07.909426: Current learning rate: 0.00103
2024-12-19 08:18:38.028796: Validation loss did not improve from -0.31352. Patience: 122/50
2024-12-19 08:18:38.029598: train_loss -0.8703
2024-12-19 08:18:38.030488: val_loss -0.1648
2024-12-19 08:18:38.031264: Pseudo dice [0.6344]
2024-12-19 08:18:38.032009: Epoch time: 90.12 s
2024-12-19 08:18:39.428594: 
2024-12-19 08:18:39.430433: Epoch 139
2024-12-19 08:18:39.431395: Current learning rate: 0.00095
2024-12-19 08:20:09.468983: Validation loss did not improve from -0.31352. Patience: 123/50
2024-12-19 08:20:09.469882: train_loss -0.8719
2024-12-19 08:20:09.471019: val_loss -0.1965
2024-12-19 08:20:09.471911: Pseudo dice [0.6589]
2024-12-19 08:20:09.472758: Epoch time: 90.04 s
2024-12-19 08:20:11.211472: 
2024-12-19 08:20:11.212649: Epoch 140
2024-12-19 08:20:11.213634: Current learning rate: 0.00087
2024-12-19 08:21:41.132071: Validation loss did not improve from -0.31352. Patience: 124/50
2024-12-19 08:21:41.133296: train_loss -0.8715
2024-12-19 08:21:41.134280: val_loss -0.1409
2024-12-19 08:21:41.135000: Pseudo dice [0.6426]
2024-12-19 08:21:41.135663: Epoch time: 89.92 s
2024-12-19 08:21:42.517639: 
2024-12-19 08:21:42.519097: Epoch 141
2024-12-19 08:21:42.519933: Current learning rate: 0.00079
2024-12-19 08:23:12.721811: Validation loss did not improve from -0.31352. Patience: 125/50
2024-12-19 08:23:12.723263: train_loss -0.8718
2024-12-19 08:23:12.724945: val_loss -0.1125
2024-12-19 08:23:12.725903: Pseudo dice [0.6343]
2024-12-19 08:23:12.726784: Epoch time: 90.21 s
2024-12-19 08:23:14.104254: 
2024-12-19 08:23:14.105757: Epoch 142
2024-12-19 08:23:14.106930: Current learning rate: 0.00071
2024-12-19 08:24:44.408221: Validation loss did not improve from -0.31352. Patience: 126/50
2024-12-19 08:24:44.409503: train_loss -0.8713
2024-12-19 08:24:44.410460: val_loss -0.1618
2024-12-19 08:24:44.411143: Pseudo dice [0.642]
2024-12-19 08:24:44.411933: Epoch time: 90.31 s
2024-12-19 08:24:45.745213: 
2024-12-19 08:24:45.746344: Epoch 143
2024-12-19 08:24:45.747212: Current learning rate: 0.00063
2024-12-19 08:26:16.052536: Validation loss did not improve from -0.31352. Patience: 127/50
2024-12-19 08:26:16.053750: train_loss -0.8712
2024-12-19 08:26:16.054657: val_loss -0.0952
2024-12-19 08:26:16.055527: Pseudo dice [0.6263]
2024-12-19 08:26:16.056424: Epoch time: 90.31 s
2024-12-19 08:26:17.806387: 
2024-12-19 08:26:17.808074: Epoch 144
2024-12-19 08:26:17.808842: Current learning rate: 0.00055
2024-12-19 08:27:48.140863: Validation loss did not improve from -0.31352. Patience: 128/50
2024-12-19 08:27:48.141853: train_loss -0.8725
2024-12-19 08:27:48.142740: val_loss -0.1034
2024-12-19 08:27:48.143578: Pseudo dice [0.6365]
2024-12-19 08:27:48.144323: Epoch time: 90.34 s
2024-12-19 08:27:49.999156: 
2024-12-19 08:27:50.000616: Epoch 145
2024-12-19 08:27:50.001609: Current learning rate: 0.00047
2024-12-19 08:29:20.219397: Validation loss did not improve from -0.31352. Patience: 129/50
2024-12-19 08:29:20.220203: train_loss -0.8711
2024-12-19 08:29:20.221202: val_loss -0.1454
2024-12-19 08:29:20.221963: Pseudo dice [0.6412]
2024-12-19 08:29:20.222704: Epoch time: 90.22 s
2024-12-19 08:29:21.553470: 
2024-12-19 08:29:21.555462: Epoch 146
2024-12-19 08:29:21.556315: Current learning rate: 0.00038
2024-12-19 08:30:51.806648: Validation loss did not improve from -0.31352. Patience: 130/50
2024-12-19 08:30:51.807403: train_loss -0.8716
2024-12-19 08:30:51.808296: val_loss -0.1273
2024-12-19 08:30:51.809029: Pseudo dice [0.643]
2024-12-19 08:30:51.809861: Epoch time: 90.25 s
2024-12-19 08:30:53.168254: 
2024-12-19 08:30:53.169809: Epoch 147
2024-12-19 08:30:53.170810: Current learning rate: 0.0003
2024-12-19 08:32:23.446739: Validation loss did not improve from -0.31352. Patience: 131/50
2024-12-19 08:32:23.447733: train_loss -0.8727
2024-12-19 08:32:23.448735: val_loss -0.0995
2024-12-19 08:32:23.449502: Pseudo dice [0.6271]
2024-12-19 08:32:23.450267: Epoch time: 90.28 s
2024-12-19 08:32:24.816597: 
2024-12-19 08:32:24.818320: Epoch 148
2024-12-19 08:32:24.819239: Current learning rate: 0.00021
2024-12-19 08:33:55.027795: Validation loss did not improve from -0.31352. Patience: 132/50
2024-12-19 08:33:55.028950: train_loss -0.8742
2024-12-19 08:33:55.029909: val_loss -0.1445
2024-12-19 08:33:55.030606: Pseudo dice [0.6393]
2024-12-19 08:33:55.031411: Epoch time: 90.21 s
2024-12-19 08:33:56.379640: 
2024-12-19 08:33:56.381828: Epoch 149
2024-12-19 08:33:56.382974: Current learning rate: 0.00011
2024-12-19 08:35:26.697033: Validation loss did not improve from -0.31352. Patience: 133/50
2024-12-19 08:35:26.698258: train_loss -0.8727
2024-12-19 08:35:26.699270: val_loss -0.1385
2024-12-19 08:35:26.700053: Pseudo dice [0.6401]
2024-12-19 08:35:26.700860: Epoch time: 90.32 s
2024-12-19 08:35:28.518312: Training done.
2024-12-19 04:44:14.844635: unpacking done...
2024-12-19 04:44:15.283057: Unable to plot network architecture: nnUNet_compile is enabled!
2024-12-19 04:44:15.620304: 
2024-12-19 04:44:15.621704: Epoch 0
2024-12-19 04:44:15.622558: Current learning rate: 0.01
2024-12-19 04:47:45.164160: Validation loss improved from 1000.00000 to -0.10286! Patience: 0/50
2024-12-19 04:47:45.165058: train_loss -0.0886
2024-12-19 04:47:45.166555: val_loss -0.1029
2024-12-19 04:47:45.167440: Pseudo dice [0.4764]
2024-12-19 04:47:45.168278: Epoch time: 209.55 s
2024-12-19 04:47:45.169188: Yayy! New best EMA pseudo Dice: 0.4764
2024-12-19 04:47:47.687485: 
2024-12-19 04:47:47.688966: Epoch 1
2024-12-19 04:47:47.689964: Current learning rate: 0.00994
2024-12-19 04:49:16.484123: Validation loss did not improve from -0.10286. Patience: 1/50
2024-12-19 04:49:16.484894: train_loss -0.3109
2024-12-19 04:49:16.485811: val_loss -0.1
2024-12-19 04:49:16.486600: Pseudo dice [0.5207]
2024-12-19 04:49:16.487828: Epoch time: 88.8 s
2024-12-19 04:49:16.489807: Yayy! New best EMA pseudo Dice: 0.4809
2024-12-19 04:49:18.075629: 
2024-12-19 04:49:18.077147: Epoch 2
2024-12-19 04:49:18.077832: Current learning rate: 0.00988
2024-12-19 04:50:47.415321: Validation loss improved from -0.10286 to -0.18615! Patience: 1/50
2024-12-19 04:50:47.416333: train_loss -0.4191
2024-12-19 04:50:47.417454: val_loss -0.1861
2024-12-19 04:50:47.420221: Pseudo dice [0.5431]
2024-12-19 04:50:47.421619: Epoch time: 89.34 s
2024-12-19 04:50:47.422602: Yayy! New best EMA pseudo Dice: 0.4871
2024-12-19 04:50:49.065107: 
2024-12-19 04:50:49.066847: Epoch 3
2024-12-19 04:50:49.067737: Current learning rate: 0.00982
2024-12-19 04:52:18.482387: Validation loss did not improve from -0.18615. Patience: 1/50
2024-12-19 04:52:18.483528: train_loss -0.4979
2024-12-19 04:52:18.484809: val_loss -0.1349
2024-12-19 04:52:18.486107: Pseudo dice [0.5486]
2024-12-19 04:52:18.487083: Epoch time: 89.42 s
2024-12-19 04:52:18.487920: Yayy! New best EMA pseudo Dice: 0.4932
2024-12-19 04:52:20.120541: 
2024-12-19 04:52:20.122262: Epoch 4
2024-12-19 04:52:20.123255: Current learning rate: 0.00976
2024-12-19 04:53:49.479407: Validation loss improved from -0.18615 to -0.20194! Patience: 1/50
2024-12-19 04:53:49.480366: train_loss -0.537
2024-12-19 04:53:49.481242: val_loss -0.2019
2024-12-19 04:53:49.482034: Pseudo dice [0.5745]
2024-12-19 04:53:49.482843: Epoch time: 89.36 s
2024-12-19 04:53:49.848557: Yayy! New best EMA pseudo Dice: 0.5014
2024-12-19 04:53:51.497264: 
2024-12-19 04:53:51.498629: Epoch 5
2024-12-19 04:53:51.499438: Current learning rate: 0.0097
2024-12-19 04:55:21.014984: Validation loss improved from -0.20194 to -0.22567! Patience: 0/50
2024-12-19 04:55:21.015793: train_loss -0.5614
2024-12-19 04:55:21.016899: val_loss -0.2257
2024-12-19 04:55:21.017666: Pseudo dice [0.5827]
2024-12-19 04:55:21.018656: Epoch time: 89.52 s
2024-12-19 04:55:21.019383: Yayy! New best EMA pseudo Dice: 0.5095
2024-12-19 04:55:22.608651: 
2024-12-19 04:55:22.610788: Epoch 6
2024-12-19 04:55:22.611516: Current learning rate: 0.00964
2024-12-19 04:56:51.933796: Validation loss did not improve from -0.22567. Patience: 1/50
2024-12-19 04:56:51.934697: train_loss -0.5935
2024-12-19 04:56:51.935759: val_loss -0.1308
2024-12-19 04:56:51.936715: Pseudo dice [0.5353]
2024-12-19 04:56:51.937641: Epoch time: 89.33 s
2024-12-19 04:56:51.938405: Yayy! New best EMA pseudo Dice: 0.5121
2024-12-19 04:56:53.546477: 
2024-12-19 04:56:53.548096: Epoch 7
2024-12-19 04:56:53.549306: Current learning rate: 0.00958
2024-12-19 04:58:22.865282: Validation loss did not improve from -0.22567. Patience: 2/50
2024-12-19 04:58:22.866453: train_loss -0.6131
2024-12-19 04:58:22.867512: val_loss -0.1972
2024-12-19 04:58:22.868190: Pseudo dice [0.587]
2024-12-19 04:58:22.868782: Epoch time: 89.32 s
2024-12-19 04:58:22.869421: Yayy! New best EMA pseudo Dice: 0.5196
2024-12-19 04:58:24.507237: 
2024-12-19 04:58:24.509285: Epoch 8
2024-12-19 04:58:24.510708: Current learning rate: 0.00952
2024-12-19 04:59:54.325822: Validation loss did not improve from -0.22567. Patience: 3/50
2024-12-19 04:59:54.326602: train_loss -0.636
2024-12-19 04:59:54.327344: val_loss -0.2077
2024-12-19 04:59:54.328078: Pseudo dice [0.5927]
2024-12-19 04:59:54.328836: Epoch time: 89.82 s
2024-12-19 04:59:54.329432: Yayy! New best EMA pseudo Dice: 0.5269
2024-12-19 04:59:56.531617: 
2024-12-19 04:59:56.533742: Epoch 9
2024-12-19 04:59:56.535038: Current learning rate: 0.00946
2024-12-19 05:01:26.292343: Validation loss did not improve from -0.22567. Patience: 4/50
2024-12-19 05:01:26.293450: train_loss -0.6485
2024-12-19 05:01:26.294127: val_loss -0.1983
2024-12-19 05:01:26.294960: Pseudo dice [0.5811]
2024-12-19 05:01:26.295657: Epoch time: 89.76 s
2024-12-19 05:01:26.684810: Yayy! New best EMA pseudo Dice: 0.5323
2024-12-19 05:01:28.225694: 
2024-12-19 05:01:28.227280: Epoch 10
2024-12-19 05:01:28.228024: Current learning rate: 0.0094
2024-12-19 05:02:58.065428: Validation loss did not improve from -0.22567. Patience: 5/50
2024-12-19 05:02:58.066331: train_loss -0.6647
2024-12-19 05:02:58.067148: val_loss -0.1188
2024-12-19 05:02:58.068001: Pseudo dice [0.5309]
2024-12-19 05:02:58.068658: Epoch time: 89.84 s
2024-12-19 05:02:59.324269: 
2024-12-19 05:02:59.326145: Epoch 11
2024-12-19 05:02:59.327292: Current learning rate: 0.00934
2024-12-19 05:04:29.127030: Validation loss did not improve from -0.22567. Patience: 6/50
2024-12-19 05:04:29.127993: train_loss -0.6737
2024-12-19 05:04:29.128691: val_loss -0.0618
2024-12-19 05:04:29.129511: Pseudo dice [0.527]
2024-12-19 05:04:29.130203: Epoch time: 89.8 s
2024-12-19 05:04:30.406971: 
2024-12-19 05:04:30.408728: Epoch 12
2024-12-19 05:04:30.409690: Current learning rate: 0.00928
2024-12-19 05:06:00.052314: Validation loss did not improve from -0.22567. Patience: 7/50
2024-12-19 05:06:00.053708: train_loss -0.6844
2024-12-19 05:06:00.054794: val_loss -0.1149
2024-12-19 05:06:00.055566: Pseudo dice [0.5061]
2024-12-19 05:06:00.056284: Epoch time: 89.65 s
2024-12-19 05:06:01.393976: 
2024-12-19 05:06:01.396003: Epoch 13
2024-12-19 05:06:01.396742: Current learning rate: 0.00922
2024-12-19 05:07:31.132731: Validation loss did not improve from -0.22567. Patience: 8/50
2024-12-19 05:07:31.134072: train_loss -0.688
2024-12-19 05:07:31.135231: val_loss -0.2255
2024-12-19 05:07:31.135890: Pseudo dice [0.5973]
2024-12-19 05:07:31.136713: Epoch time: 89.74 s
2024-12-19 05:07:31.137373: Yayy! New best EMA pseudo Dice: 0.5359
2024-12-19 05:07:32.892538: 
2024-12-19 05:07:32.894675: Epoch 14
2024-12-19 05:07:32.895432: Current learning rate: 0.00916
2024-12-19 05:09:02.503901: Validation loss improved from -0.22567 to -0.23053! Patience: 8/50
2024-12-19 05:09:02.504707: train_loss -0.7038
2024-12-19 05:09:02.505663: val_loss -0.2305
2024-12-19 05:09:02.506384: Pseudo dice [0.612]
2024-12-19 05:09:02.507050: Epoch time: 89.61 s
2024-12-19 05:09:02.904710: Yayy! New best EMA pseudo Dice: 0.5435
2024-12-19 05:09:04.574085: 
2024-12-19 05:09:04.576331: Epoch 15
2024-12-19 05:09:04.577371: Current learning rate: 0.0091
2024-12-19 05:10:34.117300: Validation loss improved from -0.23053 to -0.29869! Patience: 0/50
2024-12-19 05:10:34.118243: train_loss -0.7128
2024-12-19 05:10:34.119191: val_loss -0.2987
2024-12-19 05:10:34.119848: Pseudo dice [0.6275]
2024-12-19 05:10:34.120993: Epoch time: 89.55 s
2024-12-19 05:10:34.121903: Yayy! New best EMA pseudo Dice: 0.5519
2024-12-19 05:10:35.790008: 
2024-12-19 05:10:35.791689: Epoch 16
2024-12-19 05:10:35.792581: Current learning rate: 0.00903
2024-12-19 05:12:05.668036: Validation loss did not improve from -0.29869. Patience: 1/50
2024-12-19 05:12:05.669180: train_loss -0.7175
2024-12-19 05:12:05.670375: val_loss -0.2008
2024-12-19 05:12:05.671499: Pseudo dice [0.5833]
2024-12-19 05:12:05.672312: Epoch time: 89.88 s
2024-12-19 05:12:05.673023: Yayy! New best EMA pseudo Dice: 0.5551
2024-12-19 05:12:07.318973: 
2024-12-19 05:12:07.320931: Epoch 17
2024-12-19 05:12:07.321667: Current learning rate: 0.00897
2024-12-19 05:13:37.219019: Validation loss did not improve from -0.29869. Patience: 2/50
2024-12-19 05:13:37.219906: train_loss -0.7258
2024-12-19 05:13:37.221368: val_loss -0.2337
2024-12-19 05:13:37.222709: Pseudo dice [0.6006]
2024-12-19 05:13:37.223406: Epoch time: 89.9 s
2024-12-19 05:13:37.224064: Yayy! New best EMA pseudo Dice: 0.5596
2024-12-19 05:13:38.879174: 
2024-12-19 05:13:38.880891: Epoch 18
2024-12-19 05:13:38.881663: Current learning rate: 0.00891
2024-12-19 05:15:08.750033: Validation loss did not improve from -0.29869. Patience: 3/50
2024-12-19 05:15:08.751348: train_loss -0.7301
2024-12-19 05:15:08.752266: val_loss -0.1983
2024-12-19 05:15:08.752991: Pseudo dice [0.5867]
2024-12-19 05:15:08.753790: Epoch time: 89.87 s
2024-12-19 05:15:08.754480: Yayy! New best EMA pseudo Dice: 0.5623
2024-12-19 05:15:10.725521: 
2024-12-19 05:15:10.727170: Epoch 19
2024-12-19 05:15:10.728086: Current learning rate: 0.00885
2024-12-19 05:16:40.625609: Validation loss did not improve from -0.29869. Patience: 4/50
2024-12-19 05:16:40.626320: train_loss -0.7371
2024-12-19 05:16:40.627188: val_loss -0.252
2024-12-19 05:16:40.627796: Pseudo dice [0.6328]
2024-12-19 05:16:40.628698: Epoch time: 89.9 s
2024-12-19 05:16:40.972678: Yayy! New best EMA pseudo Dice: 0.5694
2024-12-19 05:16:42.668760: 
2024-12-19 05:16:42.670027: Epoch 20
2024-12-19 05:16:42.670629: Current learning rate: 0.00879
2024-12-19 05:18:12.445992: Validation loss did not improve from -0.29869. Patience: 5/50
2024-12-19 05:18:12.446836: train_loss -0.7414
2024-12-19 05:18:12.447663: val_loss -0.111
2024-12-19 05:18:12.448346: Pseudo dice [0.5605]
2024-12-19 05:18:12.449088: Epoch time: 89.78 s
2024-12-19 05:18:13.751472: 
2024-12-19 05:18:13.753179: Epoch 21
2024-12-19 05:18:13.754005: Current learning rate: 0.00873
2024-12-19 05:19:43.585799: Validation loss did not improve from -0.29869. Patience: 6/50
2024-12-19 05:19:43.586884: train_loss -0.7495
2024-12-19 05:19:43.588026: val_loss -0.1628
2024-12-19 05:19:43.588805: Pseudo dice [0.5805]
2024-12-19 05:19:43.589716: Epoch time: 89.84 s
2024-12-19 05:19:43.590392: Yayy! New best EMA pseudo Dice: 0.5697
2024-12-19 05:19:45.198921: 
2024-12-19 05:19:45.200390: Epoch 22
2024-12-19 05:19:45.201277: Current learning rate: 0.00867
2024-12-19 05:21:15.015841: Validation loss did not improve from -0.29869. Patience: 7/50
2024-12-19 05:21:15.016890: train_loss -0.749
2024-12-19 05:21:15.017676: val_loss -0.2773
2024-12-19 05:21:15.018350: Pseudo dice [0.6321]
2024-12-19 05:21:15.019134: Epoch time: 89.82 s
2024-12-19 05:21:15.019827: Yayy! New best EMA pseudo Dice: 0.5759
2024-12-19 05:21:16.636812: 
2024-12-19 05:21:16.638091: Epoch 23
2024-12-19 05:21:16.638898: Current learning rate: 0.00861
2024-12-19 05:22:46.437511: Validation loss did not improve from -0.29869. Patience: 8/50
2024-12-19 05:22:46.438476: train_loss -0.751
2024-12-19 05:22:46.439600: val_loss -0.2205
2024-12-19 05:22:46.440409: Pseudo dice [0.5999]
2024-12-19 05:22:46.441163: Epoch time: 89.8 s
2024-12-19 05:22:46.441849: Yayy! New best EMA pseudo Dice: 0.5783
2024-12-19 05:22:47.991702: 
2024-12-19 05:22:47.993216: Epoch 24
2024-12-19 05:22:47.994127: Current learning rate: 0.00855
2024-12-19 05:24:18.075868: Validation loss did not improve from -0.29869. Patience: 9/50
2024-12-19 05:24:18.076875: train_loss -0.7562
2024-12-19 05:24:18.077983: val_loss -0.0283
2024-12-19 05:24:18.078767: Pseudo dice [0.517]
2024-12-19 05:24:18.079604: Epoch time: 90.09 s
2024-12-19 05:24:19.652283: 
2024-12-19 05:24:19.654167: Epoch 25
2024-12-19 05:24:19.654926: Current learning rate: 0.00849
2024-12-19 05:25:49.748559: Validation loss did not improve from -0.29869. Patience: 10/50
2024-12-19 05:25:49.749548: train_loss -0.7614
2024-12-19 05:25:49.750537: val_loss -0.1727
2024-12-19 05:25:49.751471: Pseudo dice [0.5882]
2024-12-19 05:25:49.752535: Epoch time: 90.1 s
2024-12-19 05:25:51.009541: 
2024-12-19 05:25:51.011108: Epoch 26
2024-12-19 05:25:51.011979: Current learning rate: 0.00843
2024-12-19 05:27:21.035403: Validation loss did not improve from -0.29869. Patience: 11/50
2024-12-19 05:27:21.036329: train_loss -0.7657
2024-12-19 05:27:21.037563: val_loss -0.1364
2024-12-19 05:27:21.038529: Pseudo dice [0.5854]
2024-12-19 05:27:21.039256: Epoch time: 90.03 s
2024-12-19 05:27:22.340653: 
2024-12-19 05:27:22.342654: Epoch 27
2024-12-19 05:27:22.343572: Current learning rate: 0.00836
2024-12-19 05:28:52.354945: Validation loss did not improve from -0.29869. Patience: 12/50
2024-12-19 05:28:52.356101: train_loss -0.77
2024-12-19 05:28:52.357004: val_loss -0.2187
2024-12-19 05:28:52.357646: Pseudo dice [0.6089]
2024-12-19 05:28:52.358389: Epoch time: 90.02 s
2024-12-19 05:28:52.359175: Yayy! New best EMA pseudo Dice: 0.5783
2024-12-19 05:28:53.945720: 
2024-12-19 05:28:53.947802: Epoch 28
2024-12-19 05:28:53.948594: Current learning rate: 0.0083
2024-12-19 05:30:23.957593: Validation loss did not improve from -0.29869. Patience: 13/50
2024-12-19 05:30:23.958658: train_loss -0.7716
2024-12-19 05:30:23.959499: val_loss -0.1945
2024-12-19 05:30:23.960184: Pseudo dice [0.6164]
2024-12-19 05:30:23.960837: Epoch time: 90.01 s
2024-12-19 05:30:23.961483: Yayy! New best EMA pseudo Dice: 0.5822
2024-12-19 05:30:25.930415: 
2024-12-19 05:30:25.932335: Epoch 29
2024-12-19 05:30:25.933019: Current learning rate: 0.00824
2024-12-19 05:31:55.944218: Validation loss did not improve from -0.29869. Patience: 14/50
2024-12-19 05:31:55.945465: train_loss -0.7736
2024-12-19 05:31:55.946498: val_loss -0.0328
2024-12-19 05:31:55.947645: Pseudo dice [0.5192]
2024-12-19 05:31:55.948420: Epoch time: 90.02 s
2024-12-19 05:31:57.587321: 
2024-12-19 05:31:57.588762: Epoch 30
2024-12-19 05:31:57.589456: Current learning rate: 0.00818
2024-12-19 05:33:27.773779: Validation loss did not improve from -0.29869. Patience: 15/50
2024-12-19 05:33:27.775052: train_loss -0.7719
2024-12-19 05:33:27.775818: val_loss -0.1257
2024-12-19 05:33:27.776524: Pseudo dice [0.5733]
2024-12-19 05:33:27.777251: Epoch time: 90.19 s
2024-12-19 05:33:29.065869: 
2024-12-19 05:33:29.067629: Epoch 31
2024-12-19 05:33:29.068476: Current learning rate: 0.00812
2024-12-19 05:34:59.217123: Validation loss did not improve from -0.29869. Patience: 16/50
2024-12-19 05:34:59.218168: train_loss -0.7795
2024-12-19 05:34:59.218940: val_loss -0.1681
2024-12-19 05:34:59.219555: Pseudo dice [0.5925]
2024-12-19 05:34:59.220243: Epoch time: 90.15 s
2024-12-19 05:35:00.561174: 
2024-12-19 05:35:00.562726: Epoch 32
2024-12-19 05:35:00.563704: Current learning rate: 0.00806
2024-12-19 05:36:30.905166: Validation loss did not improve from -0.29869. Patience: 17/50
2024-12-19 05:36:30.905978: train_loss -0.7827
2024-12-19 05:36:30.906903: val_loss -0.2332
2024-12-19 05:36:30.907676: Pseudo dice [0.6425]
2024-12-19 05:36:30.908355: Epoch time: 90.35 s
2024-12-19 05:36:30.908998: Yayy! New best EMA pseudo Dice: 0.5838
2024-12-19 05:36:32.560083: 
2024-12-19 05:36:32.562066: Epoch 33
2024-12-19 05:36:32.562783: Current learning rate: 0.008
2024-12-19 05:38:02.735198: Validation loss did not improve from -0.29869. Patience: 18/50
2024-12-19 05:38:02.736090: train_loss -0.7836
2024-12-19 05:38:02.736858: val_loss -0.0193
2024-12-19 05:38:02.737463: Pseudo dice [0.5514]
2024-12-19 05:38:02.738174: Epoch time: 90.18 s
2024-12-19 05:38:04.066457: 
2024-12-19 05:38:04.067971: Epoch 34
2024-12-19 05:38:04.068882: Current learning rate: 0.00793
2024-12-19 05:39:34.050165: Validation loss did not improve from -0.29869. Patience: 19/50
2024-12-19 05:39:34.051259: train_loss -0.7849
2024-12-19 05:39:34.052087: val_loss -0.1259
2024-12-19 05:39:34.052725: Pseudo dice [0.5814]
2024-12-19 05:39:34.053421: Epoch time: 89.99 s
2024-12-19 05:39:35.755113: 
2024-12-19 05:39:35.756826: Epoch 35
2024-12-19 05:39:35.757644: Current learning rate: 0.00787
2024-12-19 05:41:05.726838: Validation loss did not improve from -0.29869. Patience: 20/50
2024-12-19 05:41:05.728106: train_loss -0.7879
2024-12-19 05:41:05.729017: val_loss -0.2081
2024-12-19 05:41:05.729899: Pseudo dice [0.6227]
2024-12-19 05:41:05.730693: Epoch time: 89.97 s
2024-12-19 05:41:05.731510: Yayy! New best EMA pseudo Dice: 0.5849
2024-12-19 05:41:07.429210: 
2024-12-19 05:41:07.430767: Epoch 36
2024-12-19 05:41:07.431863: Current learning rate: 0.00781
2024-12-19 05:42:37.436682: Validation loss did not improve from -0.29869. Patience: 21/50
2024-12-19 05:42:37.437794: train_loss -0.79
2024-12-19 05:42:37.438679: val_loss -0.2039
2024-12-19 05:42:37.439365: Pseudo dice [0.6181]
2024-12-19 05:42:37.440129: Epoch time: 90.01 s
2024-12-19 05:42:37.440813: Yayy! New best EMA pseudo Dice: 0.5882
2024-12-19 05:42:39.146152: 
2024-12-19 05:42:39.147929: Epoch 37
2024-12-19 05:42:39.148723: Current learning rate: 0.00775
2024-12-19 05:44:09.150372: Validation loss did not improve from -0.29869. Patience: 22/50
2024-12-19 05:44:09.151682: train_loss -0.7899
2024-12-19 05:44:09.153032: val_loss -0.1895
2024-12-19 05:44:09.153895: Pseudo dice [0.6144]
2024-12-19 05:44:09.154898: Epoch time: 90.01 s
2024-12-19 05:44:09.155751: Yayy! New best EMA pseudo Dice: 0.5908
2024-12-19 05:44:10.881588: 
2024-12-19 05:44:10.883522: Epoch 38
2024-12-19 05:44:10.884432: Current learning rate: 0.00769
2024-12-19 05:45:40.892725: Validation loss did not improve from -0.29869. Patience: 23/50
2024-12-19 05:45:40.893499: train_loss -0.794
2024-12-19 05:45:40.894279: val_loss -0.1856
2024-12-19 05:45:40.894962: Pseudo dice [0.5997]
2024-12-19 05:45:40.895602: Epoch time: 90.01 s
2024-12-19 05:45:40.896251: Yayy! New best EMA pseudo Dice: 0.5917
2024-12-19 05:45:42.972118: 
2024-12-19 05:45:42.973935: Epoch 39
2024-12-19 05:45:42.974746: Current learning rate: 0.00763
2024-12-19 05:47:13.045374: Validation loss did not improve from -0.29869. Patience: 24/50
2024-12-19 05:47:13.046304: train_loss -0.7908
2024-12-19 05:47:13.047245: val_loss -0.1409
2024-12-19 05:47:13.048022: Pseudo dice [0.5872]
2024-12-19 05:47:13.048897: Epoch time: 90.08 s
2024-12-19 05:47:14.761255: 
2024-12-19 05:47:14.762995: Epoch 40
2024-12-19 05:47:14.763833: Current learning rate: 0.00756
2024-12-19 05:48:44.815213: Validation loss did not improve from -0.29869. Patience: 25/50
2024-12-19 05:48:44.816662: train_loss -0.7974
2024-12-19 05:48:44.817635: val_loss -0.183
2024-12-19 05:48:44.818336: Pseudo dice [0.6073]
2024-12-19 05:48:44.819160: Epoch time: 90.06 s
2024-12-19 05:48:44.819790: Yayy! New best EMA pseudo Dice: 0.5928
2024-12-19 05:48:46.526986: 
2024-12-19 05:48:46.528996: Epoch 41
2024-12-19 05:48:46.529761: Current learning rate: 0.0075
2024-12-19 05:50:16.618067: Validation loss did not improve from -0.29869. Patience: 26/50
2024-12-19 05:50:16.619459: train_loss -0.8001
2024-12-19 05:50:16.620359: val_loss -0.0788
2024-12-19 05:50:16.621140: Pseudo dice [0.575]
2024-12-19 05:50:16.621775: Epoch time: 90.09 s
2024-12-19 05:50:17.880054: 
2024-12-19 05:50:17.881698: Epoch 42
2024-12-19 05:50:17.882313: Current learning rate: 0.00744
2024-12-19 05:51:48.265354: Validation loss did not improve from -0.29869. Patience: 27/50
2024-12-19 05:51:48.266591: train_loss -0.8013
2024-12-19 05:51:48.267410: val_loss -0.1727
2024-12-19 05:51:48.268087: Pseudo dice [0.5979]
2024-12-19 05:51:48.268772: Epoch time: 90.39 s
2024-12-19 05:51:49.528892: 
2024-12-19 05:51:49.530320: Epoch 43
2024-12-19 05:51:49.531034: Current learning rate: 0.00738
2024-12-19 05:53:19.863649: Validation loss did not improve from -0.29869. Patience: 28/50
2024-12-19 05:53:19.864936: train_loss -0.8035
2024-12-19 05:53:19.865988: val_loss -0.2102
2024-12-19 05:53:19.867011: Pseudo dice [0.6169]
2024-12-19 05:53:19.867883: Epoch time: 90.34 s
2024-12-19 05:53:19.868658: Yayy! New best EMA pseudo Dice: 0.5943
2024-12-19 05:53:21.748877: 
2024-12-19 05:53:21.750631: Epoch 44
2024-12-19 05:53:21.751587: Current learning rate: 0.00732
2024-12-19 05:54:52.145837: Validation loss did not improve from -0.29869. Patience: 29/50
2024-12-19 05:54:52.148251: train_loss -0.8048
2024-12-19 05:54:52.149546: val_loss -0.2029
2024-12-19 05:54:52.150315: Pseudo dice [0.6208]
2024-12-19 05:54:52.151042: Epoch time: 90.4 s
2024-12-19 05:54:52.538443: Yayy! New best EMA pseudo Dice: 0.5969
2024-12-19 05:54:54.207748: 
2024-12-19 05:54:54.209510: Epoch 45
2024-12-19 05:54:54.210315: Current learning rate: 0.00725
2024-12-19 05:56:24.515935: Validation loss did not improve from -0.29869. Patience: 30/50
2024-12-19 05:56:24.517096: train_loss -0.806
2024-12-19 05:56:24.517970: val_loss -0.1143
2024-12-19 05:56:24.518704: Pseudo dice [0.5901]
2024-12-19 05:56:24.519571: Epoch time: 90.31 s
2024-12-19 05:56:25.813625: 
2024-12-19 05:56:25.815884: Epoch 46
2024-12-19 05:56:25.816947: Current learning rate: 0.00719
2024-12-19 05:57:56.138453: Validation loss did not improve from -0.29869. Patience: 31/50
2024-12-19 05:57:56.140300: train_loss -0.806
2024-12-19 05:57:56.141900: val_loss -0.2254
2024-12-19 05:57:56.142992: Pseudo dice [0.6342]
2024-12-19 05:57:56.144128: Epoch time: 90.33 s
2024-12-19 05:57:56.145114: Yayy! New best EMA pseudo Dice: 0.6
2024-12-19 05:57:57.834643: 
2024-12-19 05:57:57.836362: Epoch 47
2024-12-19 05:57:57.837187: Current learning rate: 0.00713
2024-12-19 05:59:28.197387: Validation loss did not improve from -0.29869. Patience: 32/50
2024-12-19 05:59:28.198490: train_loss -0.8114
2024-12-19 05:59:28.199527: val_loss -0.0316
2024-12-19 05:59:28.200552: Pseudo dice [0.5379]
2024-12-19 05:59:28.201421: Epoch time: 90.36 s
2024-12-19 05:59:29.503716: 
2024-12-19 05:59:29.505600: Epoch 48
2024-12-19 05:59:29.506542: Current learning rate: 0.00707
2024-12-19 06:00:59.873271: Validation loss did not improve from -0.29869. Patience: 33/50
2024-12-19 06:00:59.874287: train_loss -0.8117
2024-12-19 06:00:59.875263: val_loss -0.1468
2024-12-19 06:00:59.876217: Pseudo dice [0.5774]
2024-12-19 06:00:59.877213: Epoch time: 90.37 s
2024-12-19 06:01:01.211077: 
2024-12-19 06:01:01.212911: Epoch 49
2024-12-19 06:01:01.213836: Current learning rate: 0.007
2024-12-19 06:02:31.245401: Validation loss did not improve from -0.29869. Patience: 34/50
2024-12-19 06:02:31.246773: train_loss -0.8148
2024-12-19 06:02:31.248027: val_loss -0.1581
2024-12-19 06:02:31.248949: Pseudo dice [0.6047]
2024-12-19 06:02:31.249746: Epoch time: 90.04 s
2024-12-19 06:02:33.349675: 
2024-12-19 06:02:33.351046: Epoch 50
2024-12-19 06:02:33.351866: Current learning rate: 0.00694
2024-12-19 06:04:03.407001: Validation loss did not improve from -0.29869. Patience: 35/50
2024-12-19 06:04:03.407743: train_loss -0.8131
2024-12-19 06:04:03.408616: val_loss -0.1571
2024-12-19 06:04:03.409327: Pseudo dice [0.5954]
2024-12-19 06:04:03.410042: Epoch time: 90.06 s
2024-12-19 06:04:04.716170: 
2024-12-19 06:04:04.717724: Epoch 51
2024-12-19 06:04:04.718487: Current learning rate: 0.00688
2024-12-19 06:05:34.838022: Validation loss did not improve from -0.29869. Patience: 36/50
2024-12-19 06:05:34.839362: train_loss -0.8138
2024-12-19 06:05:34.840225: val_loss -0.1765
2024-12-19 06:05:34.841053: Pseudo dice [0.5958]
2024-12-19 06:05:34.841732: Epoch time: 90.12 s
2024-12-19 06:05:36.178833: 
2024-12-19 06:05:36.180505: Epoch 52
2024-12-19 06:05:36.181357: Current learning rate: 0.00682
2024-12-19 06:07:06.261017: Validation loss did not improve from -0.29869. Patience: 37/50
2024-12-19 06:07:06.262187: train_loss -0.8159
2024-12-19 06:07:06.263180: val_loss -0.1328
2024-12-19 06:07:06.263862: Pseudo dice [0.6024]
2024-12-19 06:07:06.264655: Epoch time: 90.08 s
2024-12-19 06:07:07.587276: 
2024-12-19 06:07:07.589037: Epoch 53
2024-12-19 06:07:07.589729: Current learning rate: 0.00675
2024-12-19 06:08:37.585140: Validation loss did not improve from -0.29869. Patience: 38/50
2024-12-19 06:08:37.586103: train_loss -0.8143
2024-12-19 06:08:37.586931: val_loss -0.0746
2024-12-19 06:08:37.587827: Pseudo dice [0.5694]
2024-12-19 06:08:37.588665: Epoch time: 90.0 s
2024-12-19 06:08:38.936586: 
2024-12-19 06:08:38.938075: Epoch 54
2024-12-19 06:08:38.939142: Current learning rate: 0.00669
2024-12-19 06:10:08.892002: Validation loss did not improve from -0.29869. Patience: 39/50
2024-12-19 06:10:08.893030: train_loss -0.8165
2024-12-19 06:10:08.893817: val_loss -0.0091
2024-12-19 06:10:08.894460: Pseudo dice [0.5401]
2024-12-19 06:10:08.895081: Epoch time: 89.96 s
2024-12-19 06:10:10.532479: 
2024-12-19 06:10:10.534216: Epoch 55
2024-12-19 06:10:10.534990: Current learning rate: 0.00663
2024-12-19 06:11:40.447247: Validation loss did not improve from -0.29869. Patience: 40/50
2024-12-19 06:11:40.448227: train_loss -0.8133
2024-12-19 06:11:40.449288: val_loss -0.1581
2024-12-19 06:11:40.450017: Pseudo dice [0.604]
2024-12-19 06:11:40.450844: Epoch time: 89.92 s
2024-12-19 06:11:41.745450: 
2024-12-19 06:11:41.746818: Epoch 56
2024-12-19 06:11:41.747788: Current learning rate: 0.00657
2024-12-19 06:13:11.697220: Validation loss did not improve from -0.29869. Patience: 41/50
2024-12-19 06:13:11.698415: train_loss -0.8199
2024-12-19 06:13:11.699413: val_loss -0.1416
2024-12-19 06:13:11.700140: Pseudo dice [0.6077]
2024-12-19 06:13:11.700882: Epoch time: 89.95 s
2024-12-19 06:13:12.998496: 
2024-12-19 06:13:12.999923: Epoch 57
2024-12-19 06:13:13.000655: Current learning rate: 0.0065
2024-12-19 06:14:43.206836: Validation loss did not improve from -0.29869. Patience: 42/50
2024-12-19 06:14:43.207630: train_loss -0.8206
2024-12-19 06:14:43.208432: val_loss -0.1829
2024-12-19 06:14:43.209169: Pseudo dice [0.5981]
2024-12-19 06:14:43.209813: Epoch time: 90.21 s
2024-12-19 06:14:44.513501: 
2024-12-19 06:14:44.515223: Epoch 58
2024-12-19 06:14:44.516008: Current learning rate: 0.00644
2024-12-19 06:16:14.854948: Validation loss did not improve from -0.29869. Patience: 43/50
2024-12-19 06:16:14.855896: train_loss -0.819
2024-12-19 06:16:14.856687: val_loss 0.0033
2024-12-19 06:16:14.857321: Pseudo dice [0.5316]
2024-12-19 06:16:14.857929: Epoch time: 90.34 s
2024-12-19 06:16:16.178851: 
2024-12-19 06:16:16.180123: Epoch 59
2024-12-19 06:16:16.180752: Current learning rate: 0.00638
2024-12-19 06:17:46.450834: Validation loss did not improve from -0.29869. Patience: 44/50
2024-12-19 06:17:46.451724: train_loss -0.824
2024-12-19 06:17:46.452713: val_loss -0.144
2024-12-19 06:17:46.453365: Pseudo dice [0.5935]
2024-12-19 06:17:46.454116: Epoch time: 90.27 s
2024-12-19 06:17:48.643119: 
2024-12-19 06:17:48.644740: Epoch 60
2024-12-19 06:17:48.645504: Current learning rate: 0.00631
2024-12-19 06:19:18.803709: Validation loss did not improve from -0.29869. Patience: 45/50
2024-12-19 06:19:18.804516: train_loss -0.8259
2024-12-19 06:19:18.805159: val_loss -0.1353
2024-12-19 06:19:18.805751: Pseudo dice [0.5926]
2024-12-19 06:19:18.806572: Epoch time: 90.16 s
2024-12-19 06:19:20.126869: 
2024-12-19 06:19:20.128453: Epoch 61
2024-12-19 06:19:20.129188: Current learning rate: 0.00625
2024-12-19 06:20:50.385010: Validation loss did not improve from -0.29869. Patience: 46/50
2024-12-19 06:20:50.386157: train_loss -0.827
2024-12-19 06:20:50.387556: val_loss -0.1676
2024-12-19 06:20:50.388454: Pseudo dice [0.6209]
2024-12-19 06:20:50.389502: Epoch time: 90.26 s
2024-12-19 06:20:51.724025: 
2024-12-19 06:20:51.725616: Epoch 62
2024-12-19 06:20:51.726460: Current learning rate: 0.00619
2024-12-19 06:22:21.969042: Validation loss did not improve from -0.29869. Patience: 47/50
2024-12-19 06:22:21.970041: train_loss -0.8291
2024-12-19 06:22:21.970966: val_loss -0.1669
2024-12-19 06:22:21.971825: Pseudo dice [0.6233]
2024-12-19 06:22:21.972724: Epoch time: 90.25 s
2024-12-19 06:22:23.336507: 
2024-12-19 06:22:23.338519: Epoch 63
2024-12-19 06:22:23.339638: Current learning rate: 0.00612
2024-12-19 06:23:53.519189: Validation loss did not improve from -0.29869. Patience: 48/50
2024-12-19 06:23:53.520240: train_loss -0.8278
2024-12-19 06:23:53.521133: val_loss -0.0941
2024-12-19 06:23:53.521849: Pseudo dice [0.5844]
2024-12-19 06:23:53.522561: Epoch time: 90.19 s
2024-12-19 06:23:54.862072: 
2024-12-19 06:23:54.863940: Epoch 64
2024-12-19 06:23:54.864678: Current learning rate: 0.00606
2024-12-19 06:25:24.817821: Validation loss did not improve from -0.29869. Patience: 49/50
2024-12-19 06:25:24.818990: train_loss -0.8302
2024-12-19 06:25:24.819762: val_loss -0.131
2024-12-19 06:25:24.820414: Pseudo dice [0.6059]
2024-12-19 06:25:24.821004: Epoch time: 89.96 s
2024-12-19 06:25:26.606070: 
2024-12-19 06:25:26.607558: Epoch 65
2024-12-19 06:25:26.608238: Current learning rate: 0.006
2024-12-19 06:26:56.540043: Validation loss did not improve from -0.29869. Patience: 50/50
2024-12-19 06:26:56.541212: train_loss -0.8294
2024-12-19 06:26:56.542461: val_loss -0.0296
2024-12-19 06:26:56.543412: Pseudo dice [0.5539]
2024-12-19 06:26:56.544221: Epoch time: 89.94 s
2024-12-19 06:26:57.862744: 
2024-12-19 06:26:57.864545: Epoch 66
2024-12-19 06:26:57.865341: Current learning rate: 0.00593
2024-12-19 06:28:27.773293: Validation loss did not improve from -0.29869. Patience: 51/50
2024-12-19 06:28:27.774455: train_loss -0.8327
2024-12-19 06:28:27.775342: val_loss -0.0706
2024-12-19 06:28:27.776114: Pseudo dice [0.5752]
2024-12-19 06:28:27.776772: Epoch time: 89.91 s
2024-12-19 06:28:29.105067: 
2024-12-19 06:28:29.106943: Epoch 67
2024-12-19 06:28:29.107783: Current learning rate: 0.00587
2024-12-19 06:29:59.083598: Validation loss did not improve from -0.29869. Patience: 52/50
2024-12-19 06:29:59.084963: train_loss -0.8313
2024-12-19 06:29:59.085921: val_loss 0.1032
2024-12-19 06:29:59.086728: Pseudo dice [0.5293]
2024-12-19 06:29:59.087535: Epoch time: 89.98 s
2024-12-19 06:30:00.413500: 
2024-12-19 06:30:00.415069: Epoch 68
2024-12-19 06:30:00.415735: Current learning rate: 0.00581
2024-12-19 06:31:30.357379: Validation loss did not improve from -0.29869. Patience: 53/50
2024-12-19 06:31:30.358725: train_loss -0.833
2024-12-19 06:31:30.359724: val_loss -0.115
2024-12-19 06:31:30.360446: Pseudo dice [0.5835]
2024-12-19 06:31:30.361113: Epoch time: 89.95 s
2024-12-19 06:31:31.669572: 
2024-12-19 06:31:31.670819: Epoch 69
2024-12-19 06:31:31.671550: Current learning rate: 0.00574
2024-12-19 06:33:01.667287: Validation loss did not improve from -0.29869. Patience: 54/50
2024-12-19 06:33:01.668238: train_loss -0.833
2024-12-19 06:33:01.669343: val_loss -0.1123
2024-12-19 06:33:01.670175: Pseudo dice [0.5932]
2024-12-19 06:33:01.671264: Epoch time: 90.0 s
2024-12-19 06:33:03.404090: 
2024-12-19 06:33:03.405876: Epoch 70
2024-12-19 06:33:03.406911: Current learning rate: 0.00568
2024-12-19 06:34:33.394485: Validation loss did not improve from -0.29869. Patience: 55/50
2024-12-19 06:34:33.395406: train_loss -0.8357
2024-12-19 06:34:33.396287: val_loss -0.0195
2024-12-19 06:34:33.396917: Pseudo dice [0.5409]
2024-12-19 06:34:33.397663: Epoch time: 89.99 s
2024-12-19 06:34:35.192940: 
2024-12-19 06:34:35.194684: Epoch 71
2024-12-19 06:34:35.195633: Current learning rate: 0.00562
2024-12-19 06:36:05.419055: Validation loss did not improve from -0.29869. Patience: 56/50
2024-12-19 06:36:05.420224: train_loss -0.8357
2024-12-19 06:36:05.421103: val_loss -0.1591
2024-12-19 06:36:05.422022: Pseudo dice [0.6172]
2024-12-19 06:36:05.422760: Epoch time: 90.23 s
2024-12-19 06:36:06.773820: 
2024-12-19 06:36:06.775516: Epoch 72
2024-12-19 06:36:06.776180: Current learning rate: 0.00555
2024-12-19 06:37:37.093059: Validation loss did not improve from -0.29869. Patience: 57/50
2024-12-19 06:37:37.094167: train_loss -0.8369
2024-12-19 06:37:37.095145: val_loss -0.0803
2024-12-19 06:37:37.095894: Pseudo dice [0.5764]
2024-12-19 06:37:37.096644: Epoch time: 90.32 s
2024-12-19 06:37:38.383506: 
2024-12-19 06:37:38.385371: Epoch 73
2024-12-19 06:37:38.386497: Current learning rate: 0.00549
2024-12-19 06:39:08.667435: Validation loss did not improve from -0.29869. Patience: 58/50
2024-12-19 06:39:08.668782: train_loss -0.8389
2024-12-19 06:39:08.669753: val_loss -0.0466
2024-12-19 06:39:08.670448: Pseudo dice [0.5591]
2024-12-19 06:39:08.671101: Epoch time: 90.29 s
2024-12-19 06:39:10.020372: 
2024-12-19 06:39:10.022249: Epoch 74
2024-12-19 06:39:10.023356: Current learning rate: 0.00542
2024-12-19 06:40:40.423198: Validation loss did not improve from -0.29869. Patience: 59/50
2024-12-19 06:40:40.424232: train_loss -0.8393
2024-12-19 06:40:40.425169: val_loss -0.0759
2024-12-19 06:40:40.425900: Pseudo dice [0.5728]
2024-12-19 06:40:40.426543: Epoch time: 90.41 s
2024-12-19 06:40:42.097857: 
2024-12-19 06:40:42.099658: Epoch 75
2024-12-19 06:40:42.100410: Current learning rate: 0.00536
2024-12-19 06:42:12.408975: Validation loss did not improve from -0.29869. Patience: 60/50
2024-12-19 06:42:12.410144: train_loss -0.8368
2024-12-19 06:42:12.410954: val_loss -0.1128
2024-12-19 06:42:12.411817: Pseudo dice [0.5911]
2024-12-19 06:42:12.412575: Epoch time: 90.31 s
2024-12-19 06:42:13.735869: 
2024-12-19 06:42:13.737373: Epoch 76
2024-12-19 06:42:13.738140: Current learning rate: 0.00529
2024-12-19 06:43:43.987472: Validation loss did not improve from -0.29869. Patience: 61/50
2024-12-19 06:43:43.988588: train_loss -0.8439
2024-12-19 06:43:43.989484: val_loss -0.1798
2024-12-19 06:43:43.990280: Pseudo dice [0.6174]
2024-12-19 06:43:43.990952: Epoch time: 90.25 s
2024-12-19 06:43:45.316134: 
2024-12-19 06:43:45.318402: Epoch 77
2024-12-19 06:43:45.319211: Current learning rate: 0.00523
2024-12-19 06:45:15.585811: Validation loss did not improve from -0.29869. Patience: 62/50
2024-12-19 06:45:15.587146: train_loss -0.8433
2024-12-19 06:45:15.588024: val_loss -0.0656
2024-12-19 06:45:15.588751: Pseudo dice [0.5929]
2024-12-19 06:45:15.589567: Epoch time: 90.27 s
2024-12-19 06:45:17.090336: 
2024-12-19 06:45:17.091769: Epoch 78
2024-12-19 06:45:17.092456: Current learning rate: 0.00517
2024-12-19 06:46:47.360850: Validation loss did not improve from -0.29869. Patience: 63/50
2024-12-19 06:46:47.361813: train_loss -0.8434
2024-12-19 06:46:47.362679: val_loss -0.1214
2024-12-19 06:46:47.363310: Pseudo dice [0.596]
2024-12-19 06:46:47.364009: Epoch time: 90.27 s
2024-12-19 06:46:48.736493: 
2024-12-19 06:46:48.738264: Epoch 79
2024-12-19 06:46:48.739129: Current learning rate: 0.0051
2024-12-19 06:48:19.330635: Validation loss did not improve from -0.29869. Patience: 64/50
2024-12-19 06:48:19.331707: train_loss -0.8451
2024-12-19 06:48:19.332607: val_loss 0.0112
2024-12-19 06:48:19.333258: Pseudo dice [0.5606]
2024-12-19 06:48:19.333940: Epoch time: 90.6 s
2024-12-19 06:48:21.013193: 
2024-12-19 06:48:21.014581: Epoch 80
2024-12-19 06:48:21.015239: Current learning rate: 0.00504
2024-12-19 06:49:51.634296: Validation loss did not improve from -0.29869. Patience: 65/50
2024-12-19 06:49:51.635278: train_loss -0.8457
2024-12-19 06:49:51.636457: val_loss -0.0958
2024-12-19 06:49:51.637525: Pseudo dice [0.5924]
2024-12-19 06:49:51.638624: Epoch time: 90.62 s
2024-12-19 06:49:53.025408: 
2024-12-19 06:49:53.027210: Epoch 81
2024-12-19 06:49:53.028219: Current learning rate: 0.00497
2024-12-19 06:51:23.927264: Validation loss did not improve from -0.29869. Patience: 66/50
2024-12-19 06:51:23.928563: train_loss -0.8489
2024-12-19 06:51:23.929742: val_loss -0.1033
2024-12-19 06:51:23.930640: Pseudo dice [0.5817]
2024-12-19 06:51:23.931488: Epoch time: 90.9 s
2024-12-19 06:51:25.297253: 
2024-12-19 06:51:25.299073: Epoch 82
2024-12-19 06:51:25.300027: Current learning rate: 0.00491
2024-12-19 06:52:55.868048: Validation loss did not improve from -0.29869. Patience: 67/50
2024-12-19 06:52:55.869288: train_loss -0.8474
2024-12-19 06:52:55.870163: val_loss -0.192
2024-12-19 06:52:55.870901: Pseudo dice [0.6365]
2024-12-19 06:52:55.871633: Epoch time: 90.57 s
2024-12-19 06:52:57.143351: 
2024-12-19 06:52:57.145268: Epoch 83
2024-12-19 06:52:57.146204: Current learning rate: 0.00484
2024-12-19 06:54:27.807832: Validation loss did not improve from -0.29869. Patience: 68/50
2024-12-19 06:54:27.810776: train_loss -0.8475
2024-12-19 06:54:27.812379: val_loss -0.0733
2024-12-19 06:54:27.813447: Pseudo dice [0.5881]
2024-12-19 06:54:27.814305: Epoch time: 90.67 s
2024-12-19 06:54:29.092407: 
2024-12-19 06:54:29.094059: Epoch 84
2024-12-19 06:54:29.094881: Current learning rate: 0.00478
2024-12-19 06:55:59.444239: Validation loss did not improve from -0.29869. Patience: 69/50
2024-12-19 06:55:59.445338: train_loss -0.8492
2024-12-19 06:55:59.446396: val_loss -0.1106
2024-12-19 06:55:59.447164: Pseudo dice [0.5959]
2024-12-19 06:55:59.447897: Epoch time: 90.35 s
2024-12-19 06:56:01.163891: 
2024-12-19 06:56:01.166169: Epoch 85
2024-12-19 06:56:01.167074: Current learning rate: 0.00471
2024-12-19 06:57:31.475928: Validation loss did not improve from -0.29869. Patience: 70/50
2024-12-19 06:57:31.477008: train_loss -0.8516
2024-12-19 06:57:31.477806: val_loss -0.0595
2024-12-19 06:57:31.478484: Pseudo dice [0.5802]
2024-12-19 06:57:31.479336: Epoch time: 90.31 s
2024-12-19 06:57:32.754893: 
2024-12-19 06:57:32.756440: Epoch 86
2024-12-19 06:57:32.757161: Current learning rate: 0.00465
2024-12-19 06:59:03.139554: Validation loss did not improve from -0.29869. Patience: 71/50
2024-12-19 06:59:03.140327: train_loss -0.8507
2024-12-19 06:59:03.141125: val_loss -0.1195
2024-12-19 06:59:03.141834: Pseudo dice [0.589]
2024-12-19 06:59:03.142532: Epoch time: 90.39 s
2024-12-19 06:59:04.472162: 
2024-12-19 06:59:04.473985: Epoch 87
2024-12-19 06:59:04.474858: Current learning rate: 0.00458
2024-12-19 07:00:34.780638: Validation loss did not improve from -0.29869. Patience: 72/50
2024-12-19 07:00:34.781982: train_loss -0.853
2024-12-19 07:00:34.782806: val_loss -0.0255
2024-12-19 07:00:34.783788: Pseudo dice [0.5756]
2024-12-19 07:00:34.784834: Epoch time: 90.31 s
2024-12-19 07:00:36.054617: 
2024-12-19 07:00:36.056376: Epoch 88
2024-12-19 07:00:36.057303: Current learning rate: 0.00452
2024-12-19 07:02:06.415832: Validation loss did not improve from -0.29869. Patience: 73/50
2024-12-19 07:02:06.416847: train_loss -0.8535
2024-12-19 07:02:06.417646: val_loss -0.1112
2024-12-19 07:02:06.418319: Pseudo dice [0.6108]
2024-12-19 07:02:06.419115: Epoch time: 90.36 s
2024-12-19 07:02:07.717987: 
2024-12-19 07:02:07.719906: Epoch 89
2024-12-19 07:02:07.720674: Current learning rate: 0.00445
2024-12-19 07:03:38.038007: Validation loss did not improve from -0.29869. Patience: 74/50
2024-12-19 07:03:38.039123: train_loss -0.8544
2024-12-19 07:03:38.039954: val_loss -0.0878
2024-12-19 07:03:38.040656: Pseudo dice [0.5877]
2024-12-19 07:03:38.041360: Epoch time: 90.32 s
2024-12-19 07:03:39.787429: 
2024-12-19 07:03:39.789263: Epoch 90
2024-12-19 07:03:39.790240: Current learning rate: 0.00438
2024-12-19 07:05:10.080037: Validation loss did not improve from -0.29869. Patience: 75/50
2024-12-19 07:05:10.082616: train_loss -0.8538
2024-12-19 07:05:10.083968: val_loss -0.067
2024-12-19 07:05:10.084758: Pseudo dice [0.5926]
2024-12-19 07:05:10.085497: Epoch time: 90.3 s
2024-12-19 07:05:11.499264: 
2024-12-19 07:05:11.501304: Epoch 91
2024-12-19 07:05:11.502916: Current learning rate: 0.00432
2024-12-19 07:06:42.072981: Validation loss did not improve from -0.29869. Patience: 76/50
2024-12-19 07:06:42.074223: train_loss -0.8557
2024-12-19 07:06:42.075091: val_loss -0.0643
2024-12-19 07:06:42.075806: Pseudo dice [0.586]
2024-12-19 07:06:42.076462: Epoch time: 90.58 s
2024-12-19 07:06:43.788608: 
2024-12-19 07:06:43.790553: Epoch 92
2024-12-19 07:06:43.791299: Current learning rate: 0.00425
2024-12-19 07:08:14.292922: Validation loss did not improve from -0.29869. Patience: 77/50
2024-12-19 07:08:14.294210: train_loss -0.857
2024-12-19 07:08:14.295455: val_loss -0.0247
2024-12-19 07:08:14.296142: Pseudo dice [0.5687]
2024-12-19 07:08:14.296828: Epoch time: 90.51 s
2024-12-19 07:08:15.586087: 
2024-12-19 07:08:15.587941: Epoch 93
2024-12-19 07:08:15.589121: Current learning rate: 0.00419
2024-12-19 07:09:45.915203: Validation loss did not improve from -0.29869. Patience: 78/50
2024-12-19 07:09:45.916497: train_loss -0.8542
2024-12-19 07:09:45.917865: val_loss -0.1366
2024-12-19 07:09:45.918891: Pseudo dice [0.6248]
2024-12-19 07:09:45.919879: Epoch time: 90.33 s
2024-12-19 07:09:47.219818: 
2024-12-19 07:09:47.221625: Epoch 94
2024-12-19 07:09:47.222648: Current learning rate: 0.00412
2024-12-19 07:11:17.568603: Validation loss did not improve from -0.29869. Patience: 79/50
2024-12-19 07:11:17.569814: train_loss -0.8559
2024-12-19 07:11:17.570929: val_loss -0.0783
2024-12-19 07:11:17.572024: Pseudo dice [0.5849]
2024-12-19 07:11:17.573022: Epoch time: 90.35 s
2024-12-19 07:11:19.304567: 
2024-12-19 07:11:19.306330: Epoch 95
2024-12-19 07:11:19.307211: Current learning rate: 0.00405
2024-12-19 07:12:49.541412: Validation loss did not improve from -0.29869. Patience: 80/50
2024-12-19 07:12:49.542696: train_loss -0.8595
2024-12-19 07:12:49.544027: val_loss -0.1107
2024-12-19 07:12:49.544795: Pseudo dice [0.5978]
2024-12-19 07:12:49.545505: Epoch time: 90.24 s
2024-12-19 07:12:50.825281: 
2024-12-19 07:12:50.826869: Epoch 96
2024-12-19 07:12:50.827744: Current learning rate: 0.00399
2024-12-19 07:14:21.072199: Validation loss did not improve from -0.29869. Patience: 81/50
2024-12-19 07:14:21.073354: train_loss -0.8593
2024-12-19 07:14:21.074589: val_loss -0.1172
2024-12-19 07:14:21.075531: Pseudo dice [0.5921]
2024-12-19 07:14:21.076380: Epoch time: 90.25 s
2024-12-19 07:14:22.371897: 
2024-12-19 07:14:22.373839: Epoch 97
2024-12-19 07:14:22.374743: Current learning rate: 0.00392
2024-12-19 07:15:52.559618: Validation loss did not improve from -0.29869. Patience: 82/50
2024-12-19 07:15:52.560670: train_loss -0.8615
2024-12-19 07:15:52.561711: val_loss -0.1092
2024-12-19 07:15:52.562517: Pseudo dice [0.602]
2024-12-19 07:15:52.563195: Epoch time: 90.19 s
2024-12-19 07:15:53.867158: 
2024-12-19 07:15:53.869086: Epoch 98
2024-12-19 07:15:53.869916: Current learning rate: 0.00385
2024-12-19 07:17:24.107470: Validation loss did not improve from -0.29869. Patience: 83/50
2024-12-19 07:17:24.108356: train_loss -0.8611
2024-12-19 07:17:24.109185: val_loss -0.0658
2024-12-19 07:17:24.110103: Pseudo dice [0.5975]
2024-12-19 07:17:24.110826: Epoch time: 90.24 s
2024-12-19 07:17:25.446559: 
2024-12-19 07:17:25.448380: Epoch 99
2024-12-19 07:17:25.449236: Current learning rate: 0.00379
2024-12-19 07:18:55.711717: Validation loss did not improve from -0.29869. Patience: 84/50
2024-12-19 07:18:55.712804: train_loss -0.8621
2024-12-19 07:18:55.714052: val_loss -0.0687
2024-12-19 07:18:55.715091: Pseudo dice [0.5956]
2024-12-19 07:18:55.716096: Epoch time: 90.27 s
2024-12-19 07:18:57.358252: 
2024-12-19 07:18:57.359972: Epoch 100
2024-12-19 07:18:57.361134: Current learning rate: 0.00372
2024-12-19 07:20:27.757559: Validation loss did not improve from -0.29869. Patience: 85/50
2024-12-19 07:20:27.758370: train_loss -0.8616
2024-12-19 07:20:27.759370: val_loss -0.0209
2024-12-19 07:20:27.760133: Pseudo dice [0.5697]
2024-12-19 07:20:27.760729: Epoch time: 90.4 s
2024-12-19 07:20:29.100251: 
2024-12-19 07:20:29.101766: Epoch 101
2024-12-19 07:20:29.102532: Current learning rate: 0.00365
2024-12-19 07:21:59.676586: Validation loss did not improve from -0.29869. Patience: 86/50
2024-12-19 07:21:59.677734: train_loss -0.8618
2024-12-19 07:21:59.678512: val_loss -0.1612
2024-12-19 07:21:59.679131: Pseudo dice [0.6038]
2024-12-19 07:21:59.679784: Epoch time: 90.58 s
2024-12-19 07:22:00.958399: 
2024-12-19 07:22:00.959935: Epoch 102
2024-12-19 07:22:00.960631: Current learning rate: 0.00359
2024-12-19 07:23:31.571628: Validation loss did not improve from -0.29869. Patience: 87/50
2024-12-19 07:23:31.572796: train_loss -0.8649
2024-12-19 07:23:31.573795: val_loss -0.0605
2024-12-19 07:23:31.574679: Pseudo dice [0.5971]
2024-12-19 07:23:31.575419: Epoch time: 90.62 s
2024-12-19 07:23:32.846933: 
2024-12-19 07:23:32.848869: Epoch 103
2024-12-19 07:23:32.849886: Current learning rate: 0.00352
2024-12-19 07:25:03.313709: Validation loss did not improve from -0.29869. Patience: 88/50
2024-12-19 07:25:03.314802: train_loss -0.8652
2024-12-19 07:25:03.315865: val_loss -0.0667
2024-12-19 07:25:03.316850: Pseudo dice [0.5879]
2024-12-19 07:25:03.317733: Epoch time: 90.47 s
2024-12-19 07:25:05.068047: 
2024-12-19 07:25:05.069702: Epoch 104
2024-12-19 07:25:05.070797: Current learning rate: 0.00345
2024-12-19 07:26:35.588013: Validation loss did not improve from -0.29869. Patience: 89/50
2024-12-19 07:26:35.588887: train_loss -0.8642
2024-12-19 07:26:35.589762: val_loss -0.0913
2024-12-19 07:26:35.590410: Pseudo dice [0.6004]
2024-12-19 07:26:35.591170: Epoch time: 90.52 s
2024-12-19 07:26:37.265961: 
2024-12-19 07:26:37.267721: Epoch 105
2024-12-19 07:26:37.268745: Current learning rate: 0.00338
2024-12-19 07:28:07.775581: Validation loss did not improve from -0.29869. Patience: 90/50
2024-12-19 07:28:07.776458: train_loss -0.865
2024-12-19 07:28:07.777568: val_loss -0.0091
2024-12-19 07:28:07.778191: Pseudo dice [0.5854]
2024-12-19 07:28:07.778964: Epoch time: 90.51 s
2024-12-19 07:28:09.084322: 
2024-12-19 07:28:09.086148: Epoch 106
2024-12-19 07:28:09.087042: Current learning rate: 0.00332
2024-12-19 07:29:39.600692: Validation loss did not improve from -0.29869. Patience: 91/50
2024-12-19 07:29:39.601662: train_loss -0.8667
2024-12-19 07:29:39.602935: val_loss -0.0525
2024-12-19 07:29:39.603973: Pseudo dice [0.5805]
2024-12-19 07:29:39.604883: Epoch time: 90.52 s
2024-12-19 07:29:40.907607: 
2024-12-19 07:29:40.909430: Epoch 107
2024-12-19 07:29:40.910414: Current learning rate: 0.00325
2024-12-19 07:31:11.507997: Validation loss did not improve from -0.29869. Patience: 92/50
2024-12-19 07:31:11.508971: train_loss -0.8669
2024-12-19 07:31:11.509780: val_loss -0.0294
2024-12-19 07:31:11.510607: Pseudo dice [0.5624]
2024-12-19 07:31:11.511324: Epoch time: 90.6 s
2024-12-19 07:31:12.810430: 
2024-12-19 07:31:12.811737: Epoch 108
2024-12-19 07:31:12.812615: Current learning rate: 0.00318
2024-12-19 07:32:43.190681: Validation loss did not improve from -0.29869. Patience: 93/50
2024-12-19 07:32:43.191935: train_loss -0.8676
2024-12-19 07:32:43.192743: val_loss -0.1216
2024-12-19 07:32:43.193484: Pseudo dice [0.5954]
2024-12-19 07:32:43.194220: Epoch time: 90.38 s
2024-12-19 07:32:44.471693: 
2024-12-19 07:32:44.473236: Epoch 109
2024-12-19 07:32:44.473931: Current learning rate: 0.00311
2024-12-19 07:34:14.816509: Validation loss did not improve from -0.29869. Patience: 94/50
2024-12-19 07:34:14.817746: train_loss -0.8679
2024-12-19 07:34:14.818912: val_loss -0.0117
2024-12-19 07:34:14.819566: Pseudo dice [0.5683]
2024-12-19 07:34:14.820218: Epoch time: 90.35 s
2024-12-19 07:34:16.494033: 
2024-12-19 07:34:16.495855: Epoch 110
2024-12-19 07:34:16.496665: Current learning rate: 0.00304
2024-12-19 07:35:46.860423: Validation loss did not improve from -0.29869. Patience: 95/50
2024-12-19 07:35:46.861357: train_loss -0.8686
2024-12-19 07:35:46.862217: val_loss -0.0807
2024-12-19 07:35:46.862955: Pseudo dice [0.5918]
2024-12-19 07:35:46.863617: Epoch time: 90.37 s
2024-12-19 07:35:48.178045: 
2024-12-19 07:35:48.179924: Epoch 111
2024-12-19 07:35:48.180846: Current learning rate: 0.00297
2024-12-19 07:37:18.585968: Validation loss did not improve from -0.29869. Patience: 96/50
2024-12-19 07:37:18.587043: train_loss -0.8703
2024-12-19 07:37:18.588069: val_loss -0.0726
2024-12-19 07:37:18.589000: Pseudo dice [0.5878]
2024-12-19 07:37:18.589898: Epoch time: 90.41 s
2024-12-19 07:37:19.897236: 
2024-12-19 07:37:19.898968: Epoch 112
2024-12-19 07:37:19.899883: Current learning rate: 0.00291
2024-12-19 07:38:50.225938: Validation loss did not improve from -0.29869. Patience: 97/50
2024-12-19 07:38:50.226620: train_loss -0.8705
2024-12-19 07:38:50.227371: val_loss -0.0405
2024-12-19 07:38:50.228264: Pseudo dice [0.5833]
2024-12-19 07:38:50.228998: Epoch time: 90.33 s
2024-12-19 07:38:51.540561: 
2024-12-19 07:38:51.542173: Epoch 113
2024-12-19 07:38:51.542818: Current learning rate: 0.00284
2024-12-19 07:40:21.847787: Validation loss did not improve from -0.29869. Patience: 98/50
2024-12-19 07:40:21.848893: train_loss -0.8698
2024-12-19 07:40:21.849633: val_loss 0.0196
2024-12-19 07:40:21.850360: Pseudo dice [0.54]
2024-12-19 07:40:21.851160: Epoch time: 90.31 s
2024-12-19 07:40:23.566692: 
2024-12-19 07:40:23.568244: Epoch 114
2024-12-19 07:40:23.568865: Current learning rate: 0.00277
2024-12-19 07:41:53.876781: Validation loss did not improve from -0.29869. Patience: 99/50
2024-12-19 07:41:53.877906: train_loss -0.8726
2024-12-19 07:41:53.878818: val_loss -0.102
2024-12-19 07:41:53.879657: Pseudo dice [0.6189]
2024-12-19 07:41:53.880359: Epoch time: 90.31 s
2024-12-19 07:41:55.573361: 
2024-12-19 07:41:55.574980: Epoch 115
2024-12-19 07:41:55.575608: Current learning rate: 0.0027
2024-12-19 07:43:25.771640: Validation loss did not improve from -0.29869. Patience: 100/50
2024-12-19 07:43:25.772746: train_loss -0.8722
2024-12-19 07:43:25.773556: val_loss -0.0661
2024-12-19 07:43:25.774356: Pseudo dice [0.5887]
2024-12-19 07:43:25.775238: Epoch time: 90.2 s
2024-12-19 07:43:27.109111: 
2024-12-19 07:43:27.110390: Epoch 116
2024-12-19 07:43:27.111070: Current learning rate: 0.00263
2024-12-19 07:44:57.587148: Validation loss did not improve from -0.29869. Patience: 101/50
2024-12-19 07:44:57.588538: train_loss -0.8741
2024-12-19 07:44:57.589963: val_loss -0.0636
2024-12-19 07:44:57.591033: Pseudo dice [0.5932]
2024-12-19 07:44:57.592026: Epoch time: 90.48 s
2024-12-19 07:44:58.966007: 
2024-12-19 07:44:58.967789: Epoch 117
2024-12-19 07:44:58.969068: Current learning rate: 0.00256
2024-12-19 07:46:29.473992: Validation loss did not improve from -0.29869. Patience: 102/50
2024-12-19 07:46:29.475325: train_loss -0.8746
2024-12-19 07:46:29.476245: val_loss -0.0999
2024-12-19 07:46:29.476987: Pseudo dice [0.6053]
2024-12-19 07:46:29.477736: Epoch time: 90.51 s
2024-12-19 07:46:30.783427: 
2024-12-19 07:46:30.785035: Epoch 118
2024-12-19 07:46:30.785820: Current learning rate: 0.00249
2024-12-19 07:48:01.324596: Validation loss did not improve from -0.29869. Patience: 103/50
2024-12-19 07:48:01.325572: train_loss -0.8728
2024-12-19 07:48:01.326627: val_loss -0.015
2024-12-19 07:48:01.327431: Pseudo dice [0.5747]
2024-12-19 07:48:01.328264: Epoch time: 90.54 s
2024-12-19 07:48:02.634904: 
2024-12-19 07:48:02.636554: Epoch 119
2024-12-19 07:48:02.637360: Current learning rate: 0.00242
2024-12-19 07:49:33.184914: Validation loss did not improve from -0.29869. Patience: 104/50
2024-12-19 07:49:33.185998: train_loss -0.8743
2024-12-19 07:49:33.186951: val_loss -0.0825
2024-12-19 07:49:33.187545: Pseudo dice [0.5969]
2024-12-19 07:49:33.188283: Epoch time: 90.55 s
2024-12-19 07:49:34.859980: 
2024-12-19 07:49:34.861610: Epoch 120
2024-12-19 07:49:34.862279: Current learning rate: 0.00235
2024-12-19 07:51:05.282755: Validation loss did not improve from -0.29869. Patience: 105/50
2024-12-19 07:51:05.283831: train_loss -0.8752
2024-12-19 07:51:05.284637: val_loss -0.0784
2024-12-19 07:51:05.285463: Pseudo dice [0.5967]
2024-12-19 07:51:05.286171: Epoch time: 90.42 s
2024-12-19 07:51:06.602118: 
2024-12-19 07:51:06.603802: Epoch 121
2024-12-19 07:51:06.604579: Current learning rate: 0.00228
2024-12-19 07:52:36.968068: Validation loss did not improve from -0.29869. Patience: 106/50
2024-12-19 07:52:36.969292: train_loss -0.8753
2024-12-19 07:52:36.970305: val_loss -0.1257
2024-12-19 07:52:36.971268: Pseudo dice [0.6183]
2024-12-19 07:52:36.972093: Epoch time: 90.37 s
2024-12-19 07:52:38.271963: 
2024-12-19 07:52:38.273760: Epoch 122
2024-12-19 07:52:38.274658: Current learning rate: 0.00221
2024-12-19 07:54:08.609169: Validation loss did not improve from -0.29869. Patience: 107/50
2024-12-19 07:54:08.610198: train_loss -0.8756
2024-12-19 07:54:08.610975: val_loss -0.0883
2024-12-19 07:54:08.611741: Pseudo dice [0.5975]
2024-12-19 07:54:08.612712: Epoch time: 90.34 s
2024-12-19 07:54:09.912920: 
2024-12-19 07:54:09.914780: Epoch 123
2024-12-19 07:54:09.915690: Current learning rate: 0.00214
2024-12-19 07:55:40.240484: Validation loss did not improve from -0.29869. Patience: 108/50
2024-12-19 07:55:40.241423: train_loss -0.8784
2024-12-19 07:55:40.242254: val_loss -0.0723
2024-12-19 07:55:40.242882: Pseudo dice [0.5988]
2024-12-19 07:55:40.243562: Epoch time: 90.33 s
2024-12-19 07:55:41.889131: 
2024-12-19 07:55:41.890728: Epoch 124
2024-12-19 07:55:41.891645: Current learning rate: 0.00207
2024-12-19 07:57:12.285277: Validation loss did not improve from -0.29869. Patience: 109/50
2024-12-19 07:57:12.286225: train_loss -0.8772
2024-12-19 07:57:12.287200: val_loss -0.0843
2024-12-19 07:57:12.288037: Pseudo dice [0.597]
2024-12-19 07:57:12.288974: Epoch time: 90.4 s
2024-12-19 07:57:13.938127: 
2024-12-19 07:57:13.939693: Epoch 125
2024-12-19 07:57:13.940457: Current learning rate: 0.00199
2024-12-19 07:58:44.249495: Validation loss did not improve from -0.29869. Patience: 110/50
2024-12-19 07:58:44.250419: train_loss -0.8774
2024-12-19 07:58:44.251278: val_loss -0.0817
2024-12-19 07:58:44.252445: Pseudo dice [0.5917]
2024-12-19 07:58:44.253042: Epoch time: 90.31 s
2024-12-19 07:58:45.557806: 
2024-12-19 07:58:45.559415: Epoch 126
2024-12-19 07:58:45.560073: Current learning rate: 0.00192
2024-12-19 08:00:15.890105: Validation loss did not improve from -0.29869. Patience: 111/50
2024-12-19 08:00:15.893303: train_loss -0.8794
2024-12-19 08:00:15.894357: val_loss -0.0926
2024-12-19 08:00:15.895229: Pseudo dice [0.6082]
2024-12-19 08:00:15.895936: Epoch time: 90.34 s
2024-12-19 08:00:17.257615: 
2024-12-19 08:00:17.260063: Epoch 127
2024-12-19 08:00:17.261276: Current learning rate: 0.00185
2024-12-19 08:01:47.760340: Validation loss did not improve from -0.29869. Patience: 112/50
2024-12-19 08:01:47.761597: train_loss -0.8783
2024-12-19 08:01:47.763540: val_loss -0.0341
2024-12-19 08:01:47.764184: Pseudo dice [0.591]
2024-12-19 08:01:47.765255: Epoch time: 90.5 s
2024-12-19 08:01:49.156406: 
2024-12-19 08:01:49.158061: Epoch 128
2024-12-19 08:01:49.158782: Current learning rate: 0.00178
2024-12-19 08:03:19.485603: Validation loss did not improve from -0.29869. Patience: 113/50
2024-12-19 08:03:19.486479: train_loss -0.8782
2024-12-19 08:03:19.487645: val_loss -0.1659
2024-12-19 08:03:19.488774: Pseudo dice [0.6205]
2024-12-19 08:03:19.489904: Epoch time: 90.33 s
2024-12-19 08:03:20.797965: 
2024-12-19 08:03:20.799277: Epoch 129
2024-12-19 08:03:20.800003: Current learning rate: 0.0017
2024-12-19 08:04:51.570327: Validation loss did not improve from -0.29869. Patience: 114/50
2024-12-19 08:04:51.571549: train_loss -0.8794
2024-12-19 08:04:51.573111: val_loss -0.0757
2024-12-19 08:04:51.574136: Pseudo dice [0.5996]
2024-12-19 08:04:51.575098: Epoch time: 90.77 s
2024-12-19 08:04:53.252893: 
2024-12-19 08:04:53.255482: Epoch 130
2024-12-19 08:04:53.256625: Current learning rate: 0.00163
2024-12-19 08:06:23.683745: Validation loss did not improve from -0.29869. Patience: 115/50
2024-12-19 08:06:23.684418: train_loss -0.8818
2024-12-19 08:06:23.685107: val_loss -0.0337
2024-12-19 08:06:23.685890: Pseudo dice [0.5883]
2024-12-19 08:06:23.686612: Epoch time: 90.43 s
2024-12-19 08:06:24.977681: 
2024-12-19 08:06:24.979469: Epoch 131
2024-12-19 08:06:24.980595: Current learning rate: 0.00156
2024-12-19 08:07:55.561332: Validation loss did not improve from -0.29869. Patience: 116/50
2024-12-19 08:07:55.562134: train_loss -0.8801
2024-12-19 08:07:55.562944: val_loss -0.0621
2024-12-19 08:07:55.563644: Pseudo dice [0.6034]
2024-12-19 08:07:55.564249: Epoch time: 90.59 s
2024-12-19 08:07:56.923732: 
2024-12-19 08:07:56.927078: Epoch 132
2024-12-19 08:07:56.928857: Current learning rate: 0.00148
2024-12-19 08:09:27.303731: Validation loss did not improve from -0.29869. Patience: 117/50
2024-12-19 08:09:27.304499: train_loss -0.882
2024-12-19 08:09:27.305805: val_loss -0.026
2024-12-19 08:09:27.307144: Pseudo dice [0.586]
2024-12-19 08:09:27.308590: Epoch time: 90.38 s
2024-12-19 08:09:28.604762: 
2024-12-19 08:09:28.606417: Epoch 133
2024-12-19 08:09:28.607498: Current learning rate: 0.00141
2024-12-19 08:10:59.021500: Validation loss did not improve from -0.29869. Patience: 118/50
2024-12-19 08:10:59.022674: train_loss -0.8811
2024-12-19 08:10:59.023483: val_loss -0.0796
2024-12-19 08:10:59.024322: Pseudo dice [0.6]
2024-12-19 08:10:59.025194: Epoch time: 90.42 s
2024-12-19 08:11:00.337623: 
2024-12-19 08:11:00.339125: Epoch 134
2024-12-19 08:11:00.340250: Current learning rate: 0.00133
2024-12-19 08:12:30.904369: Validation loss did not improve from -0.29869. Patience: 119/50
2024-12-19 08:12:30.905053: train_loss -0.8816
2024-12-19 08:12:30.905879: val_loss -0.0234
2024-12-19 08:12:30.906903: Pseudo dice [0.5879]
2024-12-19 08:12:30.908011: Epoch time: 90.57 s
2024-12-19 08:12:33.081916: 
2024-12-19 08:12:33.083780: Epoch 135
2024-12-19 08:12:33.084685: Current learning rate: 0.00126
2024-12-19 08:14:03.561794: Validation loss did not improve from -0.29869. Patience: 120/50
2024-12-19 08:14:03.562909: train_loss -0.8827
2024-12-19 08:14:03.563787: val_loss -0.0183
2024-12-19 08:14:03.564457: Pseudo dice [0.5963]
2024-12-19 08:14:03.565195: Epoch time: 90.48 s
2024-12-19 08:14:04.841922: 
2024-12-19 08:14:04.843877: Epoch 136
2024-12-19 08:14:04.845051: Current learning rate: 0.00118
2024-12-19 08:15:35.406132: Validation loss did not improve from -0.29869. Patience: 121/50
2024-12-19 08:15:35.406904: train_loss -0.8827
2024-12-19 08:15:35.407997: val_loss -0.0704
2024-12-19 08:15:35.409080: Pseudo dice [0.5953]
2024-12-19 08:15:35.410248: Epoch time: 90.57 s
2024-12-19 08:15:36.771188: 
2024-12-19 08:15:36.773030: Epoch 137
2024-12-19 08:15:36.773996: Current learning rate: 0.00111
2024-12-19 08:17:07.434644: Validation loss did not improve from -0.29869. Patience: 122/50
2024-12-19 08:17:07.435785: train_loss -0.8831
2024-12-19 08:17:07.436702: val_loss -0.0604
2024-12-19 08:17:07.437726: Pseudo dice [0.5923]
2024-12-19 08:17:07.438589: Epoch time: 90.67 s
2024-12-19 08:17:08.723770: 
2024-12-19 08:17:08.725807: Epoch 138
2024-12-19 08:17:08.726556: Current learning rate: 0.00103
2024-12-19 08:18:39.258446: Validation loss did not improve from -0.29869. Patience: 123/50
2024-12-19 08:18:39.259211: train_loss -0.8846
2024-12-19 08:18:39.260563: val_loss -0.0399
2024-12-19 08:18:39.261528: Pseudo dice [0.5826]
2024-12-19 08:18:39.262392: Epoch time: 90.54 s
2024-12-19 08:18:40.578037: 
2024-12-19 08:18:40.579644: Epoch 139
2024-12-19 08:18:40.580455: Current learning rate: 0.00095
2024-12-19 08:20:11.198958: Validation loss did not improve from -0.29869. Patience: 124/50
2024-12-19 08:20:11.199867: train_loss -0.8822
2024-12-19 08:20:11.200635: val_loss -0.0287
2024-12-19 08:20:11.201515: Pseudo dice [0.5836]
2024-12-19 08:20:11.202297: Epoch time: 90.62 s
2024-12-19 08:20:12.916833: 
2024-12-19 08:20:12.918707: Epoch 140
2024-12-19 08:20:12.919617: Current learning rate: 0.00087
2024-12-19 08:21:43.439728: Validation loss did not improve from -0.29869. Patience: 125/50
2024-12-19 08:21:43.440888: train_loss -0.8852
2024-12-19 08:21:43.441737: val_loss 0.013
2024-12-19 08:21:43.442504: Pseudo dice [0.5755]
2024-12-19 08:21:43.443130: Epoch time: 90.53 s
2024-12-19 08:21:44.778298: 
2024-12-19 08:21:44.780190: Epoch 141
2024-12-19 08:21:44.780959: Current learning rate: 0.00079
2024-12-19 08:23:15.341472: Validation loss did not improve from -0.29869. Patience: 126/50
2024-12-19 08:23:15.342699: train_loss -0.8857
2024-12-19 08:23:15.343674: val_loss -0.0417
2024-12-19 08:23:15.344474: Pseudo dice [0.5863]
2024-12-19 08:23:15.345159: Epoch time: 90.57 s
2024-12-19 08:23:16.671694: 
2024-12-19 08:23:16.673300: Epoch 142
2024-12-19 08:23:16.674145: Current learning rate: 0.00071
2024-12-19 08:24:47.230339: Validation loss did not improve from -0.29869. Patience: 127/50
2024-12-19 08:24:47.231377: train_loss -0.8857
2024-12-19 08:24:47.232435: val_loss 0.028
2024-12-19 08:24:47.233408: Pseudo dice [0.5686]
2024-12-19 08:24:47.234431: Epoch time: 90.56 s
2024-12-19 08:24:48.580816: 
2024-12-19 08:24:48.582561: Epoch 143
2024-12-19 08:24:48.583308: Current learning rate: 0.00063
2024-12-19 08:26:19.017804: Validation loss did not improve from -0.29869. Patience: 128/50
2024-12-19 08:26:19.018982: train_loss -0.8853
2024-12-19 08:26:19.020135: val_loss -0.0174
2024-12-19 08:26:19.021009: Pseudo dice [0.5833]
2024-12-19 08:26:19.021790: Epoch time: 90.44 s
2024-12-19 08:26:20.334911: 
2024-12-19 08:26:20.336891: Epoch 144
2024-12-19 08:26:20.337713: Current learning rate: 0.00055
2024-12-19 08:27:50.833767: Validation loss did not improve from -0.29869. Patience: 129/50
2024-12-19 08:27:50.834823: train_loss -0.8852
2024-12-19 08:27:50.835860: val_loss -0.0355
2024-12-19 08:27:50.836598: Pseudo dice [0.6043]
2024-12-19 08:27:50.837495: Epoch time: 90.5 s
2024-12-19 08:27:52.587389: 
2024-12-19 08:27:52.589293: Epoch 145
2024-12-19 08:27:52.590108: Current learning rate: 0.00047
2024-12-19 08:29:22.860738: Validation loss did not improve from -0.29869. Patience: 130/50
2024-12-19 08:29:22.862015: train_loss -0.8854
2024-12-19 08:29:22.862954: val_loss -0.0613
2024-12-19 08:29:22.863673: Pseudo dice [0.6009]
2024-12-19 08:29:22.864448: Epoch time: 90.28 s
2024-12-19 08:29:24.600357: 
2024-12-19 08:29:24.601973: Epoch 146
2024-12-19 08:29:24.602721: Current learning rate: 0.00038
2024-12-19 08:30:54.765852: Validation loss did not improve from -0.29869. Patience: 131/50
2024-12-19 08:30:54.767099: train_loss -0.8866
2024-12-19 08:30:54.768056: val_loss -0.0747
2024-12-19 08:30:54.768705: Pseudo dice [0.6035]
2024-12-19 08:30:54.769545: Epoch time: 90.17 s
2024-12-19 08:30:56.109578: 
2024-12-19 08:30:56.111161: Epoch 147
2024-12-19 08:30:56.111932: Current learning rate: 0.0003
2024-12-19 08:32:26.321236: Validation loss did not improve from -0.29869. Patience: 132/50
2024-12-19 08:32:26.322102: train_loss -0.8872
2024-12-19 08:32:26.322943: val_loss -0.0471
2024-12-19 08:32:26.323729: Pseudo dice [0.5953]
2024-12-19 08:32:26.324583: Epoch time: 90.21 s
2024-12-19 08:32:27.666611: 
2024-12-19 08:32:27.667952: Epoch 148
2024-12-19 08:32:27.668749: Current learning rate: 0.00021
2024-12-19 08:33:57.908182: Validation loss did not improve from -0.29869. Patience: 133/50
2024-12-19 08:33:57.909317: train_loss -0.887
2024-12-19 08:33:57.910173: val_loss -0.0605
2024-12-19 08:33:57.910926: Pseudo dice [0.5952]
2024-12-19 08:33:57.911666: Epoch time: 90.24 s
2024-12-19 08:33:59.240527: 
2024-12-19 08:33:59.242324: Epoch 149
2024-12-19 08:33:59.243073: Current learning rate: 0.00011
2024-12-19 08:35:29.523905: Validation loss did not improve from -0.29869. Patience: 134/50
2024-12-19 08:35:29.525421: train_loss -0.8877
2024-12-19 08:35:29.526132: val_loss -0.0234
2024-12-19 08:35:29.526831: Pseudo dice [0.5923]
2024-12-19 08:35:29.527559: Epoch time: 90.29 s
2024-12-19 08:35:31.246085: Training done.
2024-12-19 08:35:28.965296: Using splits from existing split file: /home/gridsan/nchutisilp/datasets/nnUNet_Datasets/nnUNet_preprocessed/Dataset307_Sohee_Calcium_OCT_CrossValidation/splits_final_20.json
2024-12-19 08:35:29.055334: The split file contains 5 splits.
2024-12-19 08:35:29.056504: Desired fold for training: 2
2024-12-19 08:35:29.057214: This split has 1 training and 7 validation cases.
2024-12-19 08:35:29.058130: predicting 101-019
2024-12-19 08:35:29.079050: 101-019, shape torch.Size([1, 375, 498, 498]), rank 0
2024-12-19 08:37:28.417605: predicting 101-044
2024-12-19 08:37:28.438733: 101-044, shape torch.Size([1, 404, 498, 498]), rank 0
2024-12-19 08:39:04.228466: predicting 101-045
2024-12-19 08:39:04.255844: 101-045, shape torch.Size([1, 375, 498, 498]), rank 0
2024-12-19 08:40:32.388366: predicting 106-002
2024-12-19 08:40:32.414057: 106-002, shape torch.Size([1, 539, 498, 498]), rank 0
2024-12-19 08:42:38.881686: predicting 401-004
2024-12-19 08:42:38.914253: 401-004, shape torch.Size([1, 375, 498, 498]), rank 0
2024-12-19 08:44:07.127740: predicting 704-003
2024-12-19 08:44:07.151265: 704-003, shape torch.Size([1, 375, 498, 498]), rank 0
2024-12-19 08:45:35.635189: predicting 706-005
2024-12-19 08:45:35.657260: 706-005, shape torch.Size([1, 375, 498, 498]), rank 0
2024-12-19 08:35:31.386688: Using splits from existing split file: /home/gridsan/nchutisilp/datasets/nnUNet_Datasets/nnUNet_preprocessed/Dataset307_Sohee_Calcium_OCT_CrossValidation/splits_final_20.json
2024-12-19 08:35:31.388635: The split file contains 5 splits.
2024-12-19 08:35:31.389517: Desired fold for training: 3
2024-12-19 08:35:31.390153: This split has 1 training and 7 validation cases.
2024-12-19 08:35:31.391109: predicting 101-044
2024-12-19 08:35:31.401112: 101-044, shape torch.Size([1, 404, 498, 498]), rank 0
2024-12-19 08:37:35.960115: predicting 101-045
2024-12-19 08:37:35.984783: 101-045, shape torch.Size([1, 375, 498, 498]), rank 0
2024-12-19 08:39:04.032946: predicting 106-002
2024-12-19 08:39:04.061334: 106-002, shape torch.Size([1, 539, 498, 498]), rank 0
2024-12-19 08:41:10.529034: predicting 401-004
2024-12-19 08:41:10.561464: 401-004, shape torch.Size([1, 375, 498, 498]), rank 0
2024-12-19 08:42:38.734905: predicting 701-013
2024-12-19 08:42:38.758406: 701-013, shape torch.Size([1, 375, 498, 498]), rank 0
2024-12-19 08:44:07.304074: predicting 704-003
2024-12-19 08:44:07.323987: 704-003, shape torch.Size([1, 375, 498, 498]), rank 0
2024-12-19 08:45:35.534964: predicting 706-005
2024-12-19 08:45:35.555792: 706-005, shape torch.Size([1, 375, 498, 498]), rank 0
2024-12-19 08:47:24.234326: Validation complete
2024-12-19 08:47:24.235267: Mean Validation Dice:  0.5871492148903147
2024-12-19 08:47:24.338000: Validation complete
2024-12-19 08:47:24.338805: Mean Validation Dice:  0.5732403840118618

############################
INFO: You are using the old nnU-Net default plans. We have updated our recommendations. Please consider using those instead! Read more here: https://github.com/MIC-DKFZ/nnUNet/blob/master/documentation/resenc_presets.md
############################

Using device: cuda:0

#######################################################################
Please cite the following paper when using nnU-Net:
Isensee, F., Jaeger, P. F., Kohl, S. A., Petersen, J., & Maier-Hein, K. H. (2021). nnU-Net: a self-configuring method for deep learning-based biomedical image segmentation. Nature methods, 18(2), 203-211.
#######################################################################

2024-12-19 08:47:38.248931: Using torch.compile...
/home/gridsan/nchutisilp/.local/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn("The verbose parameter is deprecated. Please use get_last_lr() "
################### Loading pretrained weights from file  /home/gridsan/nchutisilp/projects/OpenAI-CLIP/clip_preivl_poststent_pretrained_nnUNet.pt ###################
Below is the list of overlapping blocks in pretrained model and nnUNet architecture:
encoder.stages.0.0.convs.0.conv.weight shape torch.Size([32, 1, 3, 3, 3])
encoder.stages.0.0.convs.0.conv.bias shape torch.Size([32])
encoder.stages.0.0.convs.0.norm.weight shape torch.Size([32])
encoder.stages.0.0.convs.0.norm.bias shape torch.Size([32])
encoder.stages.0.0.convs.0.all_modules.0.weight shape torch.Size([32, 1, 3, 3, 3])
encoder.stages.0.0.convs.0.all_modules.0.bias shape torch.Size([32])
encoder.stages.0.0.convs.0.all_modules.1.weight shape torch.Size([32])
encoder.stages.0.0.convs.0.all_modules.1.bias shape torch.Size([32])
encoder.stages.0.0.convs.1.conv.weight shape torch.Size([32, 32, 3, 3, 3])
encoder.stages.0.0.convs.1.conv.bias shape torch.Size([32])
encoder.stages.0.0.convs.1.norm.weight shape torch.Size([32])
encoder.stages.0.0.convs.1.norm.bias shape torch.Size([32])
encoder.stages.0.0.convs.1.all_modules.0.weight shape torch.Size([32, 32, 3, 3, 3])
encoder.stages.0.0.convs.1.all_modules.0.bias shape torch.Size([32])
encoder.stages.0.0.convs.1.all_modules.1.weight shape torch.Size([32])
encoder.stages.0.0.convs.1.all_modules.1.bias shape torch.Size([32])
encoder.stages.1.0.convs.0.conv.weight shape torch.Size([64, 32, 3, 3, 3])
encoder.stages.1.0.convs.0.conv.bias shape torch.Size([64])
encoder.stages.1.0.convs.0.norm.weight shape torch.Size([64])
encoder.stages.1.0.convs.0.norm.bias shape torch.Size([64])
encoder.stages.1.0.convs.0.all_modules.0.weight shape torch.Size([64, 32, 3, 3, 3])
encoder.stages.1.0.convs.0.all_modules.0.bias shape torch.Size([64])
encoder.stages.1.0.convs.0.all_modules.1.weight shape torch.Size([64])
encoder.stages.1.0.convs.0.all_modules.1.bias shape torch.Size([64])
encoder.stages.1.0.convs.1.conv.weight shape torch.Size([64, 64, 3, 3, 3])
encoder.stages.1.0.convs.1.conv.bias shape torch.Size([64])
encoder.stages.1.0.convs.1.norm.weight shape torch.Size([64])
encoder.stages.1.0.convs.1.norm.bias shape torch.Size([64])
encoder.stages.1.0.convs.1.all_modules.0.weight shape torch.Size([64, 64, 3, 3, 3])
encoder.stages.1.0.convs.1.all_modules.0.bias shape torch.Size([64])
encoder.stages.1.0.convs.1.all_modules.1.weight shape torch.Size([64])
encoder.stages.1.0.convs.1.all_modules.1.bias shape torch.Size([64])
encoder.stages.2.0.convs.0.conv.weight shape torch.Size([128, 64, 3, 3, 3])
encoder.stages.2.0.convs.0.conv.bias shape torch.Size([128])
encoder.stages.2.0.convs.0.norm.weight shape torch.Size([128])
encoder.stages.2.0.convs.0.norm.bias shape torch.Size([128])
encoder.stages.2.0.convs.0.all_modules.0.weight shape torch.Size([128, 64, 3, 3, 3])
encoder.stages.2.0.convs.0.all_modules.0.bias shape torch.Size([128])
encoder.stages.2.0.convs.0.all_modules.1.weight shape torch.Size([128])
encoder.stages.2.0.convs.0.all_modules.1.bias shape torch.Size([128])
encoder.stages.2.0.convs.1.conv.weight shape torch.Size([128, 128, 3, 3, 3])
encoder.stages.2.0.convs.1.conv.bias shape torch.Size([128])
encoder.stages.2.0.convs.1.norm.weight shape torch.Size([128])
encoder.stages.2.0.convs.1.norm.bias shape torch.Size([128])
encoder.stages.2.0.convs.1.all_modules.0.weight shape torch.Size([128, 128, 3, 3, 3])
encoder.stages.2.0.convs.1.all_modules.0.bias shape torch.Size([128])
encoder.stages.2.0.convs.1.all_modules.1.weight shape torch.Size([128])
encoder.stages.2.0.convs.1.all_modules.1.bias shape torch.Size([128])
encoder.stages.3.0.convs.0.conv.weight shape torch.Size([256, 128, 3, 3, 3])
encoder.stages.3.0.convs.0.conv.bias shape torch.Size([256])
encoder.stages.3.0.convs.0.norm.weight shape torch.Size([256])
encoder.stages.3.0.convs.0.norm.bias shape torch.Size([256])
encoder.stages.3.0.convs.0.all_modules.0.weight shape torch.Size([256, 128, 3, 3, 3])
encoder.stages.3.0.convs.0.all_modules.0.bias shape torch.Size([256])
encoder.stages.3.0.convs.0.all_modules.1.weight shape torch.Size([256])
encoder.stages.3.0.convs.0.all_modules.1.bias shape torch.Size([256])
encoder.stages.3.0.convs.1.conv.weight shape torch.Size([256, 256, 3, 3, 3])
encoder.stages.3.0.convs.1.conv.bias shape torch.Size([256])
encoder.stages.3.0.convs.1.norm.weight shape torch.Size([256])
encoder.stages.3.0.convs.1.norm.bias shape torch.Size([256])
encoder.stages.3.0.convs.1.all_modules.0.weight shape torch.Size([256, 256, 3, 3, 3])
encoder.stages.3.0.convs.1.all_modules.0.bias shape torch.Size([256])
encoder.stages.3.0.convs.1.all_modules.1.weight shape torch.Size([256])
encoder.stages.3.0.convs.1.all_modules.1.bias shape torch.Size([256])
encoder.stages.4.0.convs.0.conv.weight shape torch.Size([320, 256, 3, 3, 3])
encoder.stages.4.0.convs.0.conv.bias shape torch.Size([320])
encoder.stages.4.0.convs.0.norm.weight shape torch.Size([320])
encoder.stages.4.0.convs.0.norm.bias shape torch.Size([320])
encoder.stages.4.0.convs.0.all_modules.0.weight shape torch.Size([320, 256, 3, 3, 3])
encoder.stages.4.0.convs.0.all_modules.0.bias shape torch.Size([320])
encoder.stages.4.0.convs.0.all_modules.1.weight shape torch.Size([320])
encoder.stages.4.0.convs.0.all_modules.1.bias shape torch.Size([320])
encoder.stages.4.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.4.0.convs.1.conv.bias shape torch.Size([320])
encoder.stages.4.0.convs.1.norm.weight shape torch.Size([320])
encoder.stages.4.0.convs.1.norm.bias shape torch.Size([320])
encoder.stages.4.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.4.0.convs.1.all_modules.0.bias shape torch.Size([320])
encoder.stages.4.0.convs.1.all_modules.1.weight shape torch.Size([320])
encoder.stages.4.0.convs.1.all_modules.1.bias shape torch.Size([320])
encoder.stages.5.0.convs.0.conv.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.5.0.convs.0.conv.bias shape torch.Size([320])
encoder.stages.5.0.convs.0.norm.weight shape torch.Size([320])
encoder.stages.5.0.convs.0.norm.bias shape torch.Size([320])
encoder.stages.5.0.convs.0.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.5.0.convs.0.all_modules.0.bias shape torch.Size([320])
encoder.stages.5.0.convs.0.all_modules.1.weight shape torch.Size([320])
encoder.stages.5.0.convs.0.all_modules.1.bias shape torch.Size([320])
encoder.stages.5.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.5.0.convs.1.conv.bias shape torch.Size([320])
encoder.stages.5.0.convs.1.norm.weight shape torch.Size([320])
encoder.stages.5.0.convs.1.norm.bias shape torch.Size([320])
encoder.stages.5.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.5.0.convs.1.all_modules.0.bias shape torch.Size([320])
encoder.stages.5.0.convs.1.all_modules.1.weight shape torch.Size([320])
encoder.stages.5.0.convs.1.all_modules.1.bias shape torch.Size([320])
decoder.encoder.stages.0.0.convs.0.conv.weight shape torch.Size([32, 1, 3, 3, 3])
decoder.encoder.stages.0.0.convs.0.conv.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.norm.weight shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.norm.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.all_modules.0.weight shape torch.Size([32, 1, 3, 3, 3])
decoder.encoder.stages.0.0.convs.0.all_modules.0.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.all_modules.1.weight shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.all_modules.1.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.conv.weight shape torch.Size([32, 32, 3, 3, 3])
decoder.encoder.stages.0.0.convs.1.conv.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.norm.weight shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.norm.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.all_modules.0.weight shape torch.Size([32, 32, 3, 3, 3])
decoder.encoder.stages.0.0.convs.1.all_modules.0.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.all_modules.1.weight shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.all_modules.1.bias shape torch.Size([32])
decoder.encoder.stages.1.0.convs.0.conv.weight shape torch.Size([64, 32, 3, 3, 3])
decoder.encoder.stages.1.0.convs.0.conv.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.norm.weight shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.norm.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.all_modules.0.weight shape torch.Size([64, 32, 3, 3, 3])
decoder.encoder.stages.1.0.convs.0.all_modules.0.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.all_modules.1.weight shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.all_modules.1.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.conv.weight shape torch.Size([64, 64, 3, 3, 3])
decoder.encoder.stages.1.0.convs.1.conv.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.norm.weight shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.norm.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.all_modules.0.weight shape torch.Size([64, 64, 3, 3, 3])
decoder.encoder.stages.1.0.convs.1.all_modules.0.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.all_modules.1.weight shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.all_modules.1.bias shape torch.Size([64])
decoder.encoder.stages.2.0.convs.0.conv.weight shape torch.Size([128, 64, 3, 3, 3])
decoder.encoder.stages.2.0.convs.0.conv.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.norm.weight shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.norm.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.all_modules.0.weight shape torch.Size([128, 64, 3, 3, 3])
decoder.encoder.stages.2.0.convs.0.all_modules.0.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.all_modules.1.weight shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.all_modules.1.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.conv.weight shape torch.Size([128, 128, 3, 3, 3])
decoder.encoder.stages.2.0.convs.1.conv.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.norm.weight shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.norm.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.all_modules.0.weight shape torch.Size([128, 128, 3, 3, 3])
decoder.encoder.stages.2.0.convs.1.all_modules.0.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.all_modules.1.weight shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.all_modules.1.bias shape torch.Size([128])
decoder.encoder.stages.3.0.convs.0.conv.weight shape torch.Size([256, 128, 3, 3, 3])
decoder.encoder.stages.3.0.convs.0.conv.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.norm.weight shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.norm.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.all_modules.0.weight shape torch.Size([256, 128, 3, 3, 3])
decoder.encoder.stages.3.0.convs.0.all_modules.0.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.all_modules.1.weight shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.all_modules.1.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.conv.weight shape torch.Size([256, 256, 3, 3, 3])
decoder.encoder.stages.3.0.convs.1.conv.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.norm.weight shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.norm.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.all_modules.0.weight shape torch.Size([256, 256, 3, 3, 3])
decoder.encoder.stages.3.0.convs.1.all_modules.0.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.all_modules.1.weight shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.all_modules.1.bias shape torch.Size([256])
decoder.encoder.stages.4.0.convs.0.conv.weight shape torch.Size([320, 256, 3, 3, 3])
decoder.encoder.stages.4.0.convs.0.conv.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.norm.weight shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.norm.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.all_modules.0.weight shape torch.Size([320, 256, 3, 3, 3])
decoder.encoder.stages.4.0.convs.0.all_modules.0.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.all_modules.1.weight shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.all_modules.1.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.4.0.convs.1.conv.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.norm.weight shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.norm.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.4.0.convs.1.all_modules.0.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.all_modules.1.weight shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.all_modules.1.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.conv.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.5.0.convs.0.conv.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.norm.weight shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.norm.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.5.0.convs.0.all_modules.0.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.all_modules.1.weight shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.all_modules.1.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.5.0.convs.1.conv.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.norm.weight shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.norm.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.5.0.convs.1.all_modules.0.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.all_modules.1.weight shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.all_modules.1.bias shape torch.Size([320])
decoder.stages.0.convs.0.conv.weight shape torch.Size([320, 640, 3, 3, 3])
decoder.stages.0.convs.0.conv.bias shape torch.Size([320])
decoder.stages.0.convs.0.norm.weight shape torch.Size([320])
decoder.stages.0.convs.0.norm.bias shape torch.Size([320])
decoder.stages.0.convs.0.all_modules.0.weight shape torch.Size([320, 640, 3, 3, 3])
decoder.stages.0.convs.0.all_modules.0.bias shape torch.Size([320])
decoder.stages.0.convs.0.all_modules.1.weight shape torch.Size([320])
decoder.stages.0.convs.0.all_modules.1.bias shape torch.Size([320])
decoder.stages.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.stages.0.convs.1.conv.bias shape torch.Size([320])
decoder.stages.0.convs.1.norm.weight shape torch.Size([320])
decoder.stages.0.convs.1.norm.bias shape torch.Size([320])
decoder.stages.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.stages.0.convs.1.all_modules.0.bias shape torch.Size([320])
decoder.stages.0.convs.1.all_modules.1.weight shape torch.Size([320])
decoder.stages.0.convs.1.all_modules.1.bias shape torch.Size([320])
decoder.stages.1.convs.0.conv.weight shape torch.Size([256, 512, 3, 3, 3])
decoder.stages.1.convs.0.conv.bias shape torch.Size([256])
decoder.stages.1.convs.0.norm.weight shape torch.Size([256])
decoder.stages.1.convs.0.norm.bias shape torch.Size([256])
decoder.stages.1.convs.0.all_modules.0.weight shape torch.Size([256, 512, 3, 3, 3])
decoder.stages.1.convs.0.all_modules.0.bias shape torch.Size([256])
decoder.stages.1.convs.0.all_modules.1.weight shape torch.Size([256])
decoder.stages.1.convs.0.all_modules.1.bias shape torch.Size([256])
decoder.stages.1.convs.1.conv.weight shape torch.Size([256, 256, 3, 3, 3])
decoder.stages.1.convs.1.conv.bias shape torch.Size([256])
decoder.stages.1.convs.1.norm.weight shape torch.Size([256])
decoder.stages.1.convs.1.norm.bias shape torch.Size([256])
decoder.stages.1.convs.1.all_modules.0.weight shape torch.Size([256, 256, 3, 3, 3])
decoder.stages.1.convs.1.all_modules.0.bias shape torch.Size([256])
decoder.stages.1.convs.1.all_modules.1.weight shape torch.Size([256])
decoder.stages.1.convs.1.all_modules.1.bias shape torch.Size([256])
decoder.stages.2.convs.0.conv.weight shape torch.Size([128, 256, 3, 3, 3])
decoder.stages.2.convs.0.conv.bias shape torch.Size([128])
decoder.stages.2.convs.0.norm.weight shape torch.Size([128])
decoder.stages.2.convs.0.norm.bias shape torch.Size([128])
decoder.stages.2.convs.0.all_modules.0.weight shape torch.Size([128, 256, 3, 3, 3])
decoder.stages.2.convs.0.all_modules.0.bias shape torch.Size([128])
decoder.stages.2.convs.0.all_modules.1.weight shape torch.Size([128])
decoder.stages.2.convs.0.all_modules.1.bias shape torch.Size([128])
decoder.stages.2.convs.1.conv.weight shape torch.Size([128, 128, 3, 3, 3])
decoder.stages.2.convs.1.conv.bias shape torch.Size([128])
decoder.stages.2.convs.1.norm.weight shape torch.Size([128])
decoder.stages.2.convs.1.norm.bias shape torch.Size([128])
decoder.stages.2.convs.1.all_modules.0.weight shape torch.Size([128, 128, 3, 3, 3])
decoder.stages.2.convs.1.all_modules.0.bias shape torch.Size([128])
decoder.stages.2.convs.1.all_modules.1.weight shape torch.Size([128])
decoder.stages.2.convs.1.all_modules.1.bias shape torch.Size([128])
decoder.stages.3.convs.0.conv.weight shape torch.Size([64, 128, 3, 3, 3])
decoder.stages.3.convs.0.conv.bias shape torch.Size([64])
decoder.stages.3.convs.0.norm.weight shape torch.Size([64])
decoder.stages.3.convs.0.norm.bias shape torch.Size([64])
decoder.stages.3.convs.0.all_modules.0.weight shape torch.Size([64, 128, 3, 3, 3])
decoder.stages.3.convs.0.all_modules.0.bias shape torch.Size([64])
decoder.stages.3.convs.0.all_modules.1.weight shape torch.Size([64])
decoder.stages.3.convs.0.all_modules.1.bias shape torch.Size([64])
decoder.stages.3.convs.1.conv.weight shape torch.Size([64, 64, 3, 3, 3])
decoder.stages.3.convs.1.conv.bias shape torch.Size([64])
decoder.stages.3.convs.1.norm.weight shape torch.Size([64])
decoder.stages.3.convs.1.norm.bias shape torch.Size([64])
decoder.stages.3.convs.1.all_modules.0.weight shape torch.Size([64, 64, 3, 3, 3])
decoder.stages.3.convs.1.all_modules.0.bias shape torch.Size([64])
decoder.stages.3.convs.1.all_modules.1.weight shape torch.Size([64])
decoder.stages.3.convs.1.all_modules.1.bias shape torch.Size([64])
decoder.stages.4.convs.0.conv.weight shape torch.Size([32, 64, 3, 3, 3])
decoder.stages.4.convs.0.conv.bias shape torch.Size([32])
decoder.stages.4.convs.0.norm.weight shape torch.Size([32])
decoder.stages.4.convs.0.norm.bias shape torch.Size([32])
decoder.stages.4.convs.0.all_modules.0.weight shape torch.Size([32, 64, 3, 3, 3])
decoder.stages.4.convs.0.all_modules.0.bias shape torch.Size([32])
decoder.stages.4.convs.0.all_modules.1.weight shape torch.Size([32])
decoder.stages.4.convs.0.all_modules.1.bias shape torch.Size([32])
decoder.stages.4.convs.1.conv.weight shape torch.Size([32, 32, 3, 3, 3])
decoder.stages.4.convs.1.conv.bias shape torch.Size([32])
decoder.stages.4.convs.1.norm.weight shape torch.Size([32])
decoder.stages.4.convs.1.norm.bias shape torch.Size([32])
decoder.stages.4.convs.1.all_modules.0.weight shape torch.Size([32, 32, 3, 3, 3])
decoder.stages.4.convs.1.all_modules.0.bias shape torch.Size([32])
decoder.stages.4.convs.1.all_modules.1.weight shape torch.Size([32])
decoder.stages.4.convs.1.all_modules.1.bias shape torch.Size([32])
decoder.transpconvs.0.weight shape torch.Size([320, 320, 1, 2, 2])
decoder.transpconvs.0.bias shape torch.Size([320])
decoder.transpconvs.1.weight shape torch.Size([320, 256, 2, 2, 2])
decoder.transpconvs.1.bias shape torch.Size([256])
decoder.transpconvs.2.weight shape torch.Size([256, 128, 2, 2, 2])
decoder.transpconvs.2.bias shape torch.Size([128])
decoder.transpconvs.3.weight shape torch.Size([128, 64, 2, 2, 2])
decoder.transpconvs.3.bias shape torch.Size([64])
decoder.transpconvs.4.weight shape torch.Size([64, 32, 2, 2, 2])
decoder.transpconvs.4.bias shape torch.Size([32])
################### Done ###################
2024-12-19 08:48:01.555917: do_dummy_2d_data_aug: True
2024-12-19 08:48:01.557621: Using splits from existing split file: /home/gridsan/nchutisilp/datasets/nnUNet_Datasets/nnUNet_preprocessed/Dataset307_Sohee_Calcium_OCT_CrossValidation/splits_final_20.json
2024-12-19 08:48:01.559780: The split file contains 5 splits.
2024-12-19 08:48:01.560936: Desired fold for training: 4
2024-12-19 08:48:01.562054: This split has 1 training and 7 validation cases.
using pin_memory on device 0
using pin_memory on device 0

This is the configuration used by this training:
Configuration name: 3d_32x160x128_b10
 {'data_identifier': 'nnUNetPlans_3d_fullres', 'preprocessor_name': 'DefaultPreprocessor', 'batch_size': 10, 'patch_size': [32, 160, 128], 'median_image_size_in_voxels': [375.0, 498.0, 498.0], 'spacing': [1.0, 1.0, 1.0], 'normalization_schemes': ['ZScoreNormalization'], 'use_mask_for_norm': [False], 'resampling_fn_data': 'resample_data_or_seg_to_shape', 'resampling_fn_seg': 'resample_data_or_seg_to_shape', 'resampling_fn_data_kwargs': {'is_seg': False, 'order': 3, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_seg_kwargs': {'is_seg': True, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_probabilities': 'resample_data_or_seg_to_shape', 'resampling_fn_probabilities_kwargs': {'is_seg': False, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'architecture': {'network_class_name': 'dynamic_network_architectures.architectures.unet.PlainConvUNet', 'arch_kwargs': {'n_stages': 6, 'features_per_stage': [32, 64, 128, 256, 320, 320], 'conv_op': 'torch.nn.modules.conv.Conv3d', 'kernel_sizes': [[3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3]], 'strides': [[1, 1, 1], [2, 2, 2], [2, 2, 2], [2, 2, 2], [2, 2, 2], [1, 2, 2]], 'n_conv_per_stage': [2, 2, 2, 2, 2, 2], 'n_conv_per_stage_decoder': [2, 2, 2, 2, 2], 'conv_bias': True, 'norm_op': 'torch.nn.modules.instancenorm.InstanceNorm3d', 'norm_op_kwargs': {'eps': 1e-05, 'affine': True}, 'dropout_op': None, 'dropout_op_kwargs': None, 'nonlin': 'torch.nn.LeakyReLU', 'nonlin_kwargs': {'inplace': True}, 'deep_supervision': True}, '_kw_requires_import': ['conv_op', 'norm_op', 'dropout_op', 'nonlin']}, 'batch_dice': True, 'inherits_from': '3d_fullres'} 

These are the global plan.json settings:
 {'dataset_name': 'Dataset307_Sohee_Calcium_OCT_CrossValidation', 'plans_name': 'nnUNetPlans', 'original_median_spacing_after_transp': [1.0, 1.0, 1.0], 'original_median_shape_after_transp': [375, 498, 498], 'image_reader_writer': 'SimpleITKIO', 'transpose_forward': [0, 1, 2], 'transpose_backward': [0, 1, 2], 'experiment_planner_used': 'ExperimentPlanner', 'label_manager': 'LabelManager', 'foreground_intensity_properties_per_channel': {'0': {'max': 0.4977976381778717, 'mean': 0.13507525622844696, 'median': 0.09599608182907104, 'min': 0.0, 'percentile_00_5': 0.014754901640117168, 'percentile_99_5': 0.4977976381778717, 'std': 0.12025152146816254}}} 

2024-12-19 08:48:04.732896: unpacking dataset...
2024-12-19 08:48:08.663680: unpacking done...
2024-12-19 08:48:09.104162: Unable to plot network architecture: nnUNet_compile is enabled!
2024-12-19 08:48:09.224482: 
2024-12-19 08:48:09.226358: Epoch 0
2024-12-19 08:48:09.227970: Current learning rate: 0.01
concurrent.futures.process._RemoteTraceback: 
"""
Traceback (most recent call last):
  File "/home/gridsan/nchutisilp/.conda/envs/nnunet/lib/python3.9/concurrent/futures/process.py", line 246, in _process_worker
    r = call_item.fn(*call_item.args, **call_item.kwargs)
  File "/home/gridsan/nchutisilp/.local/lib/python3.9/site-packages/torch/_inductor/codecache.py", line 2492, in _worker_compile
    kernel = TritonCodeCache.load(kernel_name, source_code)
  File "/home/gridsan/nchutisilp/.local/lib/python3.9/site-packages/torch/_inductor/codecache.py", line 2205, in load
    mod = PyCodeCache.load(source_code)
  File "/home/gridsan/nchutisilp/.local/lib/python3.9/site-packages/torch/_inductor/codecache.py", line 2137, in load
    return cls.load_by_key_path(key, path, linemap, attrs)
  File "/home/gridsan/nchutisilp/.local/lib/python3.9/site-packages/torch/_inductor/codecache.py", line 2160, in load_by_key_path
    exec(code, mod.__dict__, mod.__dict__)
  File "/state/partition1/slurm_tmp/27617296.4294967291.0/torchinductor_nchutisilp/ad/cadzaybh4uknn7zkalumm5yaigcp3wp34ziefbua6uhu7gggoboy.py", line 12, in <module>
    @triton_heuristics.pointwise(
  File "/home/gridsan/nchutisilp/.local/lib/python3.9/site-packages/torch/_inductor/triton_heuristics.py", line 1188, in pointwise
    return cached_autotune(
  File "/home/gridsan/nchutisilp/.local/lib/python3.9/site-packages/torch/_inductor/triton_heuristics.py", line 883, in cached_autotune
    best_config = json.loads(fd.read())
  File "/home/gridsan/nchutisilp/.conda/envs/nnunet/lib/python3.9/json/__init__.py", line 346, in loads
    return _default_decoder.decode(s)
  File "/home/gridsan/nchutisilp/.conda/envs/nnunet/lib/python3.9/json/decoder.py", line 340, in decode
    raise JSONDecodeError("Extra data", s, end)
json.decoder.JSONDecodeError: Extra data: line 1 column 161 (char 160)
"""

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/gridsan/nchutisilp/.conda/envs/nnunet/bin/nnUNetv2_train", line 8, in <module>
    sys.exit(run_training_entry())
  File "/home/gridsan/nchutisilp/projects/nnUNet/nnunetv2/run/run_training.py", line 275, in run_training_entry
    run_training(args.dataset_name_or_id, args.configuration, args.fold, args.tr, args.p, args.pretrained_weights,
  File "/home/gridsan/nchutisilp/projects/nnUNet/nnunetv2/run/run_training.py", line 211, in run_training
    nnunet_trainer.run_training()
  File "/home/gridsan/nchutisilp/projects/nnUNet/nnunetv2/training/nnUNetTrainer/nnUNetTrainer.py", line 1399, in run_training
    train_outputs.append(self.train_step(next(self.dataloader_train)))
  File "/home/gridsan/nchutisilp/projects/nnUNet/nnunetv2/training/nnUNetTrainer/nnUNetTrainer.py", line 1014, in train_step
    self.grad_scaler.scale(l).backward()
  File "/home/gridsan/nchutisilp/.local/lib/python3.9/site-packages/torch/_tensor.py", line 525, in backward
    torch.autograd.backward(
  File "/home/gridsan/nchutisilp/.local/lib/python3.9/site-packages/torch/autograd/__init__.py", line 267, in backward
    _engine_run_backward(
  File "/home/gridsan/nchutisilp/.local/lib/python3.9/site-packages/torch/autograd/graph.py", line 744, in _engine_run_backward
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
  File "/home/gridsan/nchutisilp/.local/lib/python3.9/site-packages/torch/autograd/function.py", line 301, in apply
    return user_fn(self, *args)
  File "/home/gridsan/nchutisilp/.local/lib/python3.9/site-packages/torch/_functorch/_aot_autograd/jit_compile_runtime_wrappers.py", line 882, in backward
    out = call_compiled_backward()
  File "/home/gridsan/nchutisilp/.local/lib/python3.9/site-packages/torch/_functorch/_aot_autograd/jit_compile_runtime_wrappers.py", line 827, in call_compiled_backward
    CompiledFunction.compiled_bw = aot_config.bw_compiler(
  File "/home/gridsan/nchutisilp/.local/lib/python3.9/site-packages/torch/_dynamo/backends/common.py", line 36, in _wrapped_bw_compiler
    return disable(disable(bw_compiler)(*args, **kwargs))
  File "/home/gridsan/nchutisilp/.local/lib/python3.9/site-packages/torch/_dynamo/eval_frame.py", line 451, in _fn
    return fn(*args, **kwargs)
  File "/home/gridsan/nchutisilp/.local/lib/python3.9/site-packages/torch/_dynamo/external_utils.py", line 36, in inner
    return fn(*args, **kwargs)
  File "/home/gridsan/nchutisilp/.local/lib/python3.9/site-packages/torch/_dynamo/utils.py", line 262, in time_wrapper
    r = func(*args, **kwargs)
  File "/home/gridsan/nchutisilp/.local/lib/python3.9/site-packages/torch/_inductor/compile_fx.py", line 1293, in bw_compiler
    return inner_compile(
  File "/home/gridsan/nchutisilp/.local/lib/python3.9/site-packages/torch/_dynamo/repro/after_aot.py", line 83, in debug_wrapper
    inner_compiled_fn = compiler_fn(gm, example_inputs)
  File "/home/gridsan/nchutisilp/.local/lib/python3.9/site-packages/torch/_inductor/debug.py", line 304, in inner
    return fn(*args, **kwargs)
  File "/home/gridsan/nchutisilp/.conda/envs/nnunet/lib/python3.9/contextlib.py", line 79, in inner
    return func(*args, **kwds)
  File "/home/gridsan/nchutisilp/.conda/envs/nnunet/lib/python3.9/contextlib.py", line 79, in inner
    return func(*args, **kwds)
  File "/home/gridsan/nchutisilp/.local/lib/python3.9/site-packages/torch/_dynamo/utils.py", line 262, in time_wrapper
    r = func(*args, **kwargs)
  File "/home/gridsan/nchutisilp/.local/lib/python3.9/site-packages/torch/_inductor/compile_fx.py", line 438, in compile_fx_inner
    compiled_graph = fx_codegen_and_compile(
  File "/home/gridsan/nchutisilp/.local/lib/python3.9/site-packages/torch/_inductor/compile_fx.py", line 714, in fx_codegen_and_compile
    compiled_fn = graph.compile_to_fn()
  File "/home/gridsan/nchutisilp/.local/lib/python3.9/site-packages/torch/_inductor/graph.py", line 1307, in compile_to_fn
    return self.compile_to_module().call
  File "/home/gridsan/nchutisilp/.local/lib/python3.9/site-packages/torch/_dynamo/utils.py", line 262, in time_wrapper
    r = func(*args, **kwargs)
  File "/home/gridsan/nchutisilp/.local/lib/python3.9/site-packages/torch/_inductor/graph.py", line 1254, in compile_to_module
    mod = PyCodeCache.load_by_key_path(
  File "/home/gridsan/nchutisilp/.local/lib/python3.9/site-packages/torch/_inductor/codecache.py", line 2160, in load_by_key_path
    exec(code, mod.__dict__, mod.__dict__)
  File "/state/partition1/slurm_tmp/27617296.4294967291.0/torchinductor_nchutisilp/hj/chjladljqw4yc5cuq4fca35wj72srrp33u4utgnzjikhwrfyit4m.py", line 3651, in <module>
    async_compile.wait(globals())
  File "/home/gridsan/nchutisilp/.local/lib/python3.9/site-packages/torch/_inductor/codecache.py", line 2715, in wait
    scope[key] = result.result()
  File "/home/gridsan/nchutisilp/.local/lib/python3.9/site-packages/torch/_inductor/codecache.py", line 2522, in result
    self.future.result()
  File "/home/gridsan/nchutisilp/.conda/envs/nnunet/lib/python3.9/concurrent/futures/_base.py", line 439, in result
    return self.__get_result()
  File "/home/gridsan/nchutisilp/.conda/envs/nnunet/lib/python3.9/concurrent/futures/_base.py", line 391, in __get_result
    raise self._exception
json.decoder.JSONDecodeError: Extra data: line 1 column 161 (char 160)
Exception in thread Thread-3:
Traceback (most recent call last):
  File "/home/gridsan/nchutisilp/.conda/envs/nnunet/lib/python3.9/threading.py", line 980, in _bootstrap_inner
Exception in thread Thread-2:
Traceback (most recent call last):
  File "/home/gridsan/nchutisilp/.conda/envs/nnunet/lib/python3.9/threading.py", line 980, in _bootstrap_inner
