
CommandNotFoundError: Your shell has not been properly configured to use 'conda activate'.
To initialize your shell, run

    $ conda init <SHELL_NAME>

Currently supported shells are:
  - bash
  - fish
  - tcsh
  - xonsh
  - zsh
  - powershell

See 'conda init --help' for more information and options.

IMPORTANT: You may need to close and restart your shell after running 'conda init'.


/home/gridsan/nchutisilp/.conda/envs/nnunet/bin/python
Starting training with CONFIG=3d_32x160x128_b10, DATASET_ID=309, TRAINER=nnUNetTrainer

############################
INFO: You are using the old nnU-Net default plans. We have updated our recommendations. Please consider using those instead! Read more here: https://github.com/MIC-DKFZ/nnUNet/blob/master/documentation/resenc_presets.md
############################

Using device: cuda:0

#######################################################################
Please cite the following paper when using nnU-Net:
Isensee, F., Jaeger, P. F., Kohl, S. A., Petersen, J., & Maier-Hein, K. H. (2021). nnU-Net: a self-configuring method for deep learning-based biomedical image segmentation. Nature methods, 18(2), 203-211.
#######################################################################

2024-12-07 03:50:31.503805: Using torch.compile...

############################
INFO: You are using the old nnU-Net default plans. We have updated our recommendations. Please consider using those instead! Read more here: https://github.com/MIC-DKFZ/nnUNet/blob/master/documentation/resenc_presets.md
############################

Using device: cuda:0

#######################################################################
Please cite the following paper when using nnU-Net:
Isensee, F., Jaeger, P. F., Kohl, S. A., Petersen, J., & Maier-Hein, K. H. (2021). nnU-Net: a self-configuring method for deep learning-based biomedical image segmentation. Nature methods, 18(2), 203-211.
#######################################################################

2024-12-07 03:50:31.503762: Using torch.compile...
/home/gridsan/nchutisilp/.local/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn("The verbose parameter is deprecated. Please use get_last_lr() "
/home/gridsan/nchutisilp/.local/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn("The verbose parameter is deprecated. Please use get_last_lr() "
################### Loading pretrained weights from file  /home/gridsan/nchutisilp/projects/OpenAI-CLIP/clip_preivl_poststent_pretrained_nnUNet.pt ###################
Below is the list of overlapping blocks in pretrained model and nnUNet architecture:
encoder.stages.0.0.convs.0.conv.weight shape torch.Size([32, 1, 3, 3, 3])
encoder.stages.0.0.convs.0.conv.bias shape torch.Size([32])
encoder.stages.0.0.convs.0.norm.weight shape torch.Size([32])
encoder.stages.0.0.convs.0.norm.bias shape torch.Size([32])
encoder.stages.0.0.convs.0.all_modules.0.weight shape torch.Size([32, 1, 3, 3, 3])
encoder.stages.0.0.convs.0.all_modules.0.bias shape torch.Size([32])
encoder.stages.0.0.convs.0.all_modules.1.weight shape torch.Size([32])
encoder.stages.0.0.convs.0.all_modules.1.bias shape torch.Size([32])
encoder.stages.0.0.convs.1.conv.weight shape torch.Size([32, 32, 3, 3, 3])
encoder.stages.0.0.convs.1.conv.bias shape torch.Size([32])
encoder.stages.0.0.convs.1.norm.weight shape torch.Size([32])
encoder.stages.0.0.convs.1.norm.bias shape torch.Size([32])
encoder.stages.0.0.convs.1.all_modules.0.weight shape torch.Size([32, 32, 3, 3, 3])
encoder.stages.0.0.convs.1.all_modules.0.bias shape torch.Size([32])
encoder.stages.0.0.convs.1.all_modules.1.weight shape torch.Size([32])
encoder.stages.0.0.convs.1.all_modules.1.bias shape torch.Size([32])
encoder.stages.1.0.convs.0.conv.weight shape torch.Size([64, 32, 3, 3, 3])
encoder.stages.1.0.convs.0.conv.bias shape torch.Size([64])
encoder.stages.1.0.convs.0.norm.weight shape torch.Size([64])
encoder.stages.1.0.convs.0.norm.bias shape torch.Size([64])
encoder.stages.1.0.convs.0.all_modules.0.weight shape torch.Size([64, 32, 3, 3, 3])
encoder.stages.1.0.convs.0.all_modules.0.bias shape torch.Size([64])
encoder.stages.1.0.convs.0.all_modules.1.weight shape torch.Size([64])
encoder.stages.1.0.convs.0.all_modules.1.bias shape torch.Size([64])
encoder.stages.1.0.convs.1.conv.weight shape torch.Size([64, 64, 3, 3, 3])
encoder.stages.1.0.convs.1.conv.bias shape torch.Size([64])
encoder.stages.1.0.convs.1.norm.weight shape torch.Size([64])
encoder.stages.1.0.convs.1.norm.bias shape torch.Size([64])
encoder.stages.1.0.convs.1.all_modules.0.weight shape torch.Size([64, 64, 3, 3, 3])
encoder.stages.1.0.convs.1.all_modules.0.bias shape torch.Size([64])
encoder.stages.1.0.convs.1.all_modules.1.weight shape torch.Size([64])
encoder.stages.1.0.convs.1.all_modules.1.bias shape torch.Size([64])
encoder.stages.2.0.convs.0.conv.weight shape torch.Size([128, 64, 3, 3, 3])
encoder.stages.2.0.convs.0.conv.bias shape torch.Size([128])
encoder.stages.2.0.convs.0.norm.weight shape torch.Size([128])
encoder.stages.2.0.convs.0.norm.bias shape torch.Size([128])
encoder.stages.2.0.convs.0.all_modules.0.weight shape torch.Size([128, 64, 3, 3, 3])
encoder.stages.2.0.convs.0.all_modules.0.bias shape torch.Size([128])
encoder.stages.2.0.convs.0.all_modules.1.weight shape torch.Size([128])
encoder.stages.2.0.convs.0.all_modules.1.bias shape torch.Size([128])
encoder.stages.2.0.convs.1.conv.weight shape torch.Size([128, 128, 3, 3, 3])
encoder.stages.2.0.convs.1.conv.bias shape torch.Size([128])
encoder.stages.2.0.convs.1.norm.weight shape torch.Size([128])
encoder.stages.2.0.convs.1.norm.bias shape torch.Size([128])
encoder.stages.2.0.convs.1.all_modules.0.weight shape torch.Size([128, 128, 3, 3, 3])
encoder.stages.2.0.convs.1.all_modules.0.bias shape torch.Size([128])
encoder.stages.2.0.convs.1.all_modules.1.weight shape torch.Size([128])
encoder.stages.2.0.convs.1.all_modules.1.bias shape torch.Size([128])
encoder.stages.3.0.convs.0.conv.weight shape torch.Size([256, 128, 3, 3, 3])
encoder.stages.3.0.convs.0.conv.bias shape torch.Size([256])
encoder.stages.3.0.convs.0.norm.weight shape torch.Size([256])
encoder.stages.3.0.convs.0.norm.bias shape torch.Size([256])
encoder.stages.3.0.convs.0.all_modules.0.weight shape torch.Size([256, 128, 3, 3, 3])
encoder.stages.3.0.convs.0.all_modules.0.bias shape torch.Size([256])
encoder.stages.3.0.convs.0.all_modules.1.weight shape torch.Size([256])
encoder.stages.3.0.convs.0.all_modules.1.bias shape torch.Size([256])
encoder.stages.3.0.convs.1.conv.weight shape torch.Size([256, 256, 3, 3, 3])
encoder.stages.3.0.convs.1.conv.bias shape torch.Size([256])
encoder.stages.3.0.convs.1.norm.weight shape torch.Size([256])
encoder.stages.3.0.convs.1.norm.bias shape torch.Size([256])
encoder.stages.3.0.convs.1.all_modules.0.weight shape torch.Size([256, 256, 3, 3, 3])
encoder.stages.3.0.convs.1.all_modules.0.bias shape torch.Size([256])
encoder.stages.3.0.convs.1.all_modules.1.weight shape torch.Size([256])
encoder.stages.3.0.convs.1.all_modules.1.bias shape torch.Size([256])
encoder.stages.4.0.convs.0.conv.weight shape torch.Size([320, 256, 3, 3, 3])
encoder.stages.4.0.convs.0.conv.bias shape torch.Size([320])
encoder.stages.4.0.convs.0.norm.weight shape torch.Size([320])
encoder.stages.4.0.convs.0.norm.bias shape torch.Size([320])
encoder.stages.4.0.convs.0.all_modules.0.weight shape torch.Size([320, 256, 3, 3, 3])
encoder.stages.4.0.convs.0.all_modules.0.bias shape torch.Size([320])
encoder.stages.4.0.convs.0.all_modules.1.weight shape torch.Size([320])
encoder.stages.4.0.convs.0.all_modules.1.bias shape torch.Size([320])
encoder.stages.4.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.4.0.convs.1.conv.bias shape torch.Size([320])
encoder.stages.4.0.convs.1.norm.weight shape torch.Size([320])
encoder.stages.4.0.convs.1.norm.bias shape torch.Size([320])
encoder.stages.4.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.4.0.convs.1.all_modules.0.bias shape torch.Size([320])
encoder.stages.4.0.convs.1.all_modules.1.weight shape torch.Size([320])
encoder.stages.4.0.convs.1.all_modules.1.bias shape torch.Size([320])
encoder.stages.5.0.convs.0.conv.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.5.0.convs.0.conv.bias shape torch.Size([320])
encoder.stages.5.0.convs.0.norm.weight shape torch.Size([320])
encoder.stages.5.0.convs.0.norm.bias shape torch.Size([320])
encoder.stages.5.0.convs.0.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.5.0.convs.0.all_modules.0.bias shape torch.Size([320])
encoder.stages.5.0.convs.0.all_modules.1.weight shape torch.Size([320])
encoder.stages.5.0.convs.0.all_modules.1.bias shape torch.Size([320])
encoder.stages.5.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.5.0.convs.1.conv.bias shape torch.Size([320])
encoder.stages.5.0.convs.1.norm.weight shape torch.Size([320])
encoder.stages.5.0.convs.1.norm.bias shape torch.Size([320])
encoder.stages.5.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.5.0.convs.1.all_modules.0.bias shape torch.Size([320])
encoder.stages.5.0.convs.1.all_modules.1.weight shape torch.Size([320])
encoder.stages.5.0.convs.1.all_modules.1.bias shape torch.Size([320])
decoder.encoder.stages.0.0.convs.0.conv.weight shape torch.Size([32, 1, 3, 3, 3])
decoder.encoder.stages.0.0.convs.0.conv.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.norm.weight shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.norm.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.all_modules.0.weight shape torch.Size([32, 1, 3, 3, 3])
decoder.encoder.stages.0.0.convs.0.all_modules.0.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.all_modules.1.weight shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.all_modules.1.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.conv.weight shape torch.Size([32, 32, 3, 3, 3])
decoder.encoder.stages.0.0.convs.1.conv.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.norm.weight shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.norm.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.all_modules.0.weight shape torch.Size([32, 32, 3, 3, 3])
decoder.encoder.stages.0.0.convs.1.all_modules.0.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.all_modules.1.weight shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.all_modules.1.bias shape torch.Size([32])
decoder.encoder.stages.1.0.convs.0.conv.weight shape torch.Size([64, 32, 3, 3, 3])
decoder.encoder.stages.1.0.convs.0.conv.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.norm.weight shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.norm.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.all_modules.0.weight shape torch.Size([64, 32, 3, 3, 3])
decoder.encoder.stages.1.0.convs.0.all_modules.0.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.all_modules.1.weight shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.all_modules.1.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.conv.weight shape torch.Size([64, 64, 3, 3, 3])
decoder.encoder.stages.1.0.convs.1.conv.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.norm.weight shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.norm.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.all_modules.0.weight shape torch.Size([64, 64, 3, 3, 3])
decoder.encoder.stages.1.0.convs.1.all_modules.0.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.all_modules.1.weight shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.all_modules.1.bias shape torch.Size([64])
decoder.encoder.stages.2.0.convs.0.conv.weight shape torch.Size([128, 64, 3, 3, 3])
decoder.encoder.stages.2.0.convs.0.conv.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.norm.weight shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.norm.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.all_modules.0.weight shape torch.Size([128, 64, 3, 3, 3])
decoder.encoder.stages.2.0.convs.0.all_modules.0.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.all_modules.1.weight shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.all_modules.1.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.conv.weight shape torch.Size([128, 128, 3, 3, 3])
decoder.encoder.stages.2.0.convs.1.conv.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.norm.weight shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.norm.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.all_modules.0.weight shape torch.Size([128, 128, 3, 3, 3])
decoder.encoder.stages.2.0.convs.1.all_modules.0.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.all_modules.1.weight shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.all_modules.1.bias shape torch.Size([128])
decoder.encoder.stages.3.0.convs.0.conv.weight shape torch.Size([256, 128, 3, 3, 3])
decoder.encoder.stages.3.0.convs.0.conv.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.norm.weight shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.norm.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.all_modules.0.weight shape torch.Size([256, 128, 3, 3, 3])
decoder.encoder.stages.3.0.convs.0.all_modules.0.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.all_modules.1.weight shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.all_modules.1.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.conv.weight shape torch.Size([256, 256, 3, 3, 3])
decoder.encoder.stages.3.0.convs.1.conv.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.norm.weight shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.norm.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.all_modules.0.weight shape torch.Size([256, 256, 3, 3, 3])
decoder.encoder.stages.3.0.convs.1.all_modules.0.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.all_modules.1.weight shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.all_modules.1.bias shape torch.Size([256])
decoder.encoder.stages.4.0.convs.0.conv.weight shape torch.Size([320, 256, 3, 3, 3])
decoder.encoder.stages.4.0.convs.0.conv.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.norm.weight shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.norm.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.all_modules.0.weight shape torch.Size([320, 256, 3, 3, 3])
decoder.encoder.stages.4.0.convs.0.all_modules.0.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.all_modules.1.weight shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.all_modules.1.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.4.0.convs.1.conv.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.norm.weight shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.norm.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.4.0.convs.1.all_modules.0.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.all_modules.1.weight shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.all_modules.1.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.conv.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.5.0.convs.0.conv.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.norm.weight shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.norm.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.5.0.convs.0.all_modules.0.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.all_modules.1.weight shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.all_modules.1.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.5.0.convs.1.conv.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.norm.weight shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.norm.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.5.0.convs.1.all_modules.0.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.all_modules.1.weight shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.all_modules.1.bias shape torch.Size([320])
decoder.stages.0.convs.0.conv.weight shape torch.Size([320, 640, 3, 3, 3])
decoder.stages.0.convs.0.conv.bias shape torch.Size([320])
decoder.stages.0.convs.0.norm.weight shape torch.Size([320])
decoder.stages.0.convs.0.norm.bias shape torch.Size([320])
decoder.stages.0.convs.0.all_modules.0.weight shape torch.Size([320, 640, 3, 3, 3])
decoder.stages.0.convs.0.all_modules.0.bias shape torch.Size([320])
decoder.stages.0.convs.0.all_modules.1.weight shape torch.Size([320])
decoder.stages.0.convs.0.all_modules.1.bias shape torch.Size([320])
decoder.stages.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.stages.0.convs.1.conv.bias shape torch.Size([320])
decoder.stages.0.convs.1.norm.weight shape torch.Size([320])
decoder.stages.0.convs.1.norm.bias shape torch.Size([320])
decoder.stages.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.stages.0.convs.1.all_modules.0.bias shape torch.Size([320])
decoder.stages.0.convs.1.all_modules.1.weight shape torch.Size([320])
decoder.stages.0.convs.1.all_modules.1.bias shape torch.Size([320])
decoder.stages.1.convs.0.conv.weight shape torch.Size([256, 512, 3, 3, 3])
decoder.stages.1.convs.0.conv.bias shape torch.Size([256])
decoder.stages.1.convs.0.norm.weight shape torch.Size([256])
decoder.stages.1.convs.0.norm.bias shape torch.Size([256])
decoder.stages.1.convs.0.all_modules.0.weight shape torch.Size([256, 512, 3, 3, 3])
decoder.stages.1.convs.0.all_modules.0.bias shape torch.Size([256])
decoder.stages.1.convs.0.all_modules.1.weight shape torch.Size([256])
decoder.stages.1.convs.0.all_modules.1.bias shape torch.Size([256])
decoder.stages.1.convs.1.conv.weight shape torch.Size([256, 256, 3, 3, 3])
decoder.stages.1.convs.1.conv.bias shape torch.Size([256])
decoder.stages.1.convs.1.norm.weight shape torch.Size([256])
decoder.stages.1.convs.1.norm.bias shape torch.Size([256])
decoder.stages.1.convs.1.all_modules.0.weight shape torch.Size([256, 256, 3, 3, 3])
decoder.stages.1.convs.1.all_modules.0.bias shape torch.Size([256])
decoder.stages.1.convs.1.all_modules.1.weight shape torch.Size([256])
decoder.stages.1.convs.1.all_modules.1.bias shape torch.Size([256])
decoder.stages.2.convs.0.conv.weight shape torch.Size([128, 256, 3, 3, 3])
decoder.stages.2.convs.0.conv.bias shape torch.Size([128])
decoder.stages.2.convs.0.norm.weight shape torch.Size([128])
decoder.stages.2.convs.0.norm.bias shape torch.Size([128])
decoder.stages.2.convs.0.all_modules.0.weight shape torch.Size([128, 256, 3, 3, 3])
decoder.stages.2.convs.0.all_modules.0.bias shape torch.Size([128])
decoder.stages.2.convs.0.all_modules.1.weight shape torch.Size([128])
decoder.stages.2.convs.0.all_modules.1.bias shape torch.Size([128])
decoder.stages.2.convs.1.conv.weight shape torch.Size([128, 128, 3, 3, 3])
decoder.stages.2.convs.1.conv.bias shape torch.Size([128])
decoder.stages.2.convs.1.norm.weight shape torch.Size([128])
decoder.stages.2.convs.1.norm.bias shape torch.Size([128])
decoder.stages.2.convs.1.all_modules.0.weight shape torch.Size([128, 128, 3, 3, 3])
decoder.stages.2.convs.1.all_modules.0.bias shape torch.Size([128])
decoder.stages.2.convs.1.all_modules.1.weight shape torch.Size([128])
decoder.stages.2.convs.1.all_modules.1.bias shape torch.Size([128])
decoder.stages.3.convs.0.conv.weight shape torch.Size([64, 128, 3, 3, 3])
decoder.stages.3.convs.0.conv.bias shape torch.Size([64])
decoder.stages.3.convs.0.norm.weight shape torch.Size([64])
decoder.stages.3.convs.0.norm.bias shape torch.Size([64])
decoder.stages.3.convs.0.all_modules.0.weight shape torch.Size([64, 128, 3, 3, 3])
decoder.stages.3.convs.0.all_modules.0.bias shape torch.Size([64])
decoder.stages.3.convs.0.all_modules.1.weight shape torch.Size([64])
decoder.stages.3.convs.0.all_modules.1.bias shape torch.Size([64])
decoder.stages.3.convs.1.conv.weight shape torch.Size([64, 64, 3, 3, 3])
decoder.stages.3.convs.1.conv.bias shape torch.Size([64])
decoder.stages.3.convs.1.norm.weight shape torch.Size([64])
decoder.stages.3.convs.1.norm.bias shape torch.Size([64])
decoder.stages.3.convs.1.all_modules.0.weight shape torch.Size([64, 64, 3, 3, 3])
decoder.stages.3.convs.1.all_modules.0.bias shape torch.Size([64])
decoder.stages.3.convs.1.all_modules.1.weight shape torch.Size([64])
decoder.stages.3.convs.1.all_modules.1.bias shape torch.Size([64])
decoder.stages.4.convs.0.conv.weight shape torch.Size([32, 64, 3, 3, 3])
decoder.stages.4.convs.0.conv.bias shape torch.Size([32])
decoder.stages.4.convs.0.norm.weight shape torch.Size([32])
decoder.stages.4.convs.0.norm.bias shape torch.Size([32])
decoder.stages.4.convs.0.all_modules.0.weight shape torch.Size([32, 64, 3, 3, 3])
decoder.stages.4.convs.0.all_modules.0.bias shape torch.Size([32])
decoder.stages.4.convs.0.all_modules.1.weight shape torch.Size([32])
decoder.stages.4.convs.0.all_modules.1.bias shape torch.Size([32])
decoder.stages.4.convs.1.conv.weight shape torch.Size([32, 32, 3, 3, 3])
decoder.stages.4.convs.1.conv.bias shape torch.Size([32])
decoder.stages.4.convs.1.norm.weight shape torch.Size([32])
decoder.stages.4.convs.1.norm.bias shape torch.Size([32])
decoder.stages.4.convs.1.all_modules.0.weight shape torch.Size([32, 32, 3, 3, 3])
decoder.stages.4.convs.1.all_modules.0.bias shape torch.Size([32])
decoder.stages.4.convs.1.all_modules.1.weight shape torch.Size([32])
decoder.stages.4.convs.1.all_modules.1.bias shape torch.Size([32])
decoder.transpconvs.0.weight shape torch.Size([320, 320, 1, 2, 2])
decoder.transpconvs.0.bias shape torch.Size([320])
decoder.transpconvs.1.weight shape torch.Size([320, 256, 2, 2, 2])
decoder.transpconvs.1.bias shape torch.Size([256])
decoder.transpconvs.2.weight shape torch.Size([256, 128, 2, 2, 2])
decoder.transpconvs.2.bias shape torch.Size([128])
decoder.transpconvs.3.weight shape torch.Size([128, 64, 2, 2, 2])
decoder.transpconvs.3.bias shape torch.Size([64])
decoder.transpconvs.4.weight shape torch.Size([64, 32, 2, 2, 2])
decoder.transpconvs.4.bias shape torch.Size([32])
################### Done ###################
2024-12-07 03:50:36.196807: do_dummy_2d_data_aug: True
2024-12-07 03:50:36.201129: Using splits from existing split file: /home/gridsan/nchutisilp/datasets/nnUNet_Datasets/nnUNet_preprocessed/Dataset307_Sohee_Calcium_OCT_CrossValidation/splits_final.json
2024-12-07 03:50:36.203446: The split file contains 5 splits.
2024-12-07 03:50:36.204728: Desired fold for training: 1
2024-12-07 03:50:36.206102: This split has 6 training and 2 validation cases.
################### Loading pretrained weights from file  /home/gridsan/nchutisilp/projects/OpenAI-CLIP/clip_preivl_poststent_pretrained_nnUNet.pt ###################
Below is the list of overlapping blocks in pretrained model and nnUNet architecture:
encoder.stages.0.0.convs.0.conv.weight shape torch.Size([32, 1, 3, 3, 3])
encoder.stages.0.0.convs.0.conv.bias shape torch.Size([32])
encoder.stages.0.0.convs.0.norm.weight shape torch.Size([32])
encoder.stages.0.0.convs.0.norm.bias shape torch.Size([32])
encoder.stages.0.0.convs.0.all_modules.0.weight shape torch.Size([32, 1, 3, 3, 3])
encoder.stages.0.0.convs.0.all_modules.0.bias shape torch.Size([32])
encoder.stages.0.0.convs.0.all_modules.1.weight shape torch.Size([32])
encoder.stages.0.0.convs.0.all_modules.1.bias shape torch.Size([32])
encoder.stages.0.0.convs.1.conv.weight shape torch.Size([32, 32, 3, 3, 3])
encoder.stages.0.0.convs.1.conv.bias shape torch.Size([32])
encoder.stages.0.0.convs.1.norm.weight shape torch.Size([32])
encoder.stages.0.0.convs.1.norm.bias shape torch.Size([32])
encoder.stages.0.0.convs.1.all_modules.0.weight shape torch.Size([32, 32, 3, 3, 3])
encoder.stages.0.0.convs.1.all_modules.0.bias shape torch.Size([32])
encoder.stages.0.0.convs.1.all_modules.1.weight shape torch.Size([32])
encoder.stages.0.0.convs.1.all_modules.1.bias shape torch.Size([32])
encoder.stages.1.0.convs.0.conv.weight shape torch.Size([64, 32, 3, 3, 3])
encoder.stages.1.0.convs.0.conv.bias shape torch.Size([64])
encoder.stages.1.0.convs.0.norm.weight shape torch.Size([64])
encoder.stages.1.0.convs.0.norm.bias shape torch.Size([64])
encoder.stages.1.0.convs.0.all_modules.0.weight shape torch.Size([64, 32, 3, 3, 3])
encoder.stages.1.0.convs.0.all_modules.0.bias shape torch.Size([64])
encoder.stages.1.0.convs.0.all_modules.1.weight shape torch.Size([64])
encoder.stages.1.0.convs.0.all_modules.1.bias shape torch.Size([64])
encoder.stages.1.0.convs.1.conv.weight shape torch.Size([64, 64, 3, 3, 3])
encoder.stages.1.0.convs.1.conv.bias shape torch.Size([64])
encoder.stages.1.0.convs.1.norm.weight shape torch.Size([64])
encoder.stages.1.0.convs.1.norm.bias shape torch.Size([64])
encoder.stages.1.0.convs.1.all_modules.0.weight shape torch.Size([64, 64, 3, 3, 3])
encoder.stages.1.0.convs.1.all_modules.0.bias shape torch.Size([64])
encoder.stages.1.0.convs.1.all_modules.1.weight shape torch.Size([64])
encoder.stages.1.0.convs.1.all_modules.1.bias shape torch.Size([64])
encoder.stages.2.0.convs.0.conv.weight shape torch.Size([128, 64, 3, 3, 3])
encoder.stages.2.0.convs.0.conv.bias shape torch.Size([128])
encoder.stages.2.0.convs.0.norm.weight shape torch.Size([128])
encoder.stages.2.0.convs.0.norm.bias shape torch.Size([128])
encoder.stages.2.0.convs.0.all_modules.0.weight shape torch.Size([128, 64, 3, 3, 3])
encoder.stages.2.0.convs.0.all_modules.0.bias shape torch.Size([128])
encoder.stages.2.0.convs.0.all_modules.1.weight shape torch.Size([128])
encoder.stages.2.0.convs.0.all_modules.1.bias shape torch.Size([128])
encoder.stages.2.0.convs.1.conv.weight shape torch.Size([128, 128, 3, 3, 3])
encoder.stages.2.0.convs.1.conv.bias shape torch.Size([128])
encoder.stages.2.0.convs.1.norm.weight shape torch.Size([128])
encoder.stages.2.0.convs.1.norm.bias shape torch.Size([128])
encoder.stages.2.0.convs.1.all_modules.0.weight shape torch.Size([128, 128, 3, 3, 3])
encoder.stages.2.0.convs.1.all_modules.0.bias shape torch.Size([128])
encoder.stages.2.0.convs.1.all_modules.1.weight shape torch.Size([128])
encoder.stages.2.0.convs.1.all_modules.1.bias shape torch.Size([128])
encoder.stages.3.0.convs.0.conv.weight shape torch.Size([256, 128, 3, 3, 3])
encoder.stages.3.0.convs.0.conv.bias shape torch.Size([256])
encoder.stages.3.0.convs.0.norm.weight shape torch.Size([256])
encoder.stages.3.0.convs.0.norm.bias shape torch.Size([256])
encoder.stages.3.0.convs.0.all_modules.0.weight shape torch.Size([256, 128, 3, 3, 3])
encoder.stages.3.0.convs.0.all_modules.0.bias shape torch.Size([256])
encoder.stages.3.0.convs.0.all_modules.1.weight shape torch.Size([256])
encoder.stages.3.0.convs.0.all_modules.1.bias shape torch.Size([256])
encoder.stages.3.0.convs.1.conv.weight shape torch.Size([256, 256, 3, 3, 3])
encoder.stages.3.0.convs.1.conv.bias shape torch.Size([256])
encoder.stages.3.0.convs.1.norm.weight shape torch.Size([256])
encoder.stages.3.0.convs.1.norm.bias shape torch.Size([256])
encoder.stages.3.0.convs.1.all_modules.0.weight shape torch.Size([256, 256, 3, 3, 3])
encoder.stages.3.0.convs.1.all_modules.0.bias shape torch.Size([256])
encoder.stages.3.0.convs.1.all_modules.1.weight shape torch.Size([256])
encoder.stages.3.0.convs.1.all_modules.1.bias shape torch.Size([256])
encoder.stages.4.0.convs.0.conv.weight shape torch.Size([320, 256, 3, 3, 3])
encoder.stages.4.0.convs.0.conv.bias shape torch.Size([320])
encoder.stages.4.0.convs.0.norm.weight shape torch.Size([320])
encoder.stages.4.0.convs.0.norm.bias shape torch.Size([320])
encoder.stages.4.0.convs.0.all_modules.0.weight shape torch.Size([320, 256, 3, 3, 3])
encoder.stages.4.0.convs.0.all_modules.0.bias shape torch.Size([320])
encoder.stages.4.0.convs.0.all_modules.1.weight shape torch.Size([320])
encoder.stages.4.0.convs.0.all_modules.1.bias shape torch.Size([320])
encoder.stages.4.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.4.0.convs.1.conv.bias shape torch.Size([320])
encoder.stages.4.0.convs.1.norm.weight shape torch.Size([320])
encoder.stages.4.0.convs.1.norm.bias shape torch.Size([320])
encoder.stages.4.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.4.0.convs.1.all_modules.0.bias shape torch.Size([320])
encoder.stages.4.0.convs.1.all_modules.1.weight shape torch.Size([320])
encoder.stages.4.0.convs.1.all_modules.1.bias shape torch.Size([320])
encoder.stages.5.0.convs.0.conv.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.5.0.convs.0.conv.bias shape torch.Size([320])
encoder.stages.5.0.convs.0.norm.weight shape torch.Size([320])
encoder.stages.5.0.convs.0.norm.bias shape torch.Size([320])
encoder.stages.5.0.convs.0.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.5.0.convs.0.all_modules.0.bias shape torch.Size([320])
encoder.stages.5.0.convs.0.all_modules.1.weight shape torch.Size([320])
encoder.stages.5.0.convs.0.all_modules.1.bias shape torch.Size([320])
encoder.stages.5.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.5.0.convs.1.conv.bias shape torch.Size([320])
encoder.stages.5.0.convs.1.norm.weight shape torch.Size([320])
encoder.stages.5.0.convs.1.norm.bias shape torch.Size([320])
encoder.stages.5.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.5.0.convs.1.all_modules.0.bias shape torch.Size([320])
encoder.stages.5.0.convs.1.all_modules.1.weight shape torch.Size([320])
encoder.stages.5.0.convs.1.all_modules.1.bias shape torch.Size([320])
decoder.encoder.stages.0.0.convs.0.conv.weight shape torch.Size([32, 1, 3, 3, 3])
decoder.encoder.stages.0.0.convs.0.conv.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.norm.weight shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.norm.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.all_modules.0.weight shape torch.Size([32, 1, 3, 3, 3])
decoder.encoder.stages.0.0.convs.0.all_modules.0.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.all_modules.1.weight shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.all_modules.1.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.conv.weight shape torch.Size([32, 32, 3, 3, 3])
decoder.encoder.stages.0.0.convs.1.conv.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.norm.weight shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.norm.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.all_modules.0.weight shape torch.Size([32, 32, 3, 3, 3])
decoder.encoder.stages.0.0.convs.1.all_modules.0.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.all_modules.1.weight shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.all_modules.1.bias shape torch.Size([32])
decoder.encoder.stages.1.0.convs.0.conv.weight shape torch.Size([64, 32, 3, 3, 3])
decoder.encoder.stages.1.0.convs.0.conv.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.norm.weight shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.norm.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.all_modules.0.weight shape torch.Size([64, 32, 3, 3, 3])
decoder.encoder.stages.1.0.convs.0.all_modules.0.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.all_modules.1.weight shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.all_modules.1.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.conv.weight shape torch.Size([64, 64, 3, 3, 3])
decoder.encoder.stages.1.0.convs.1.conv.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.norm.weight shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.norm.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.all_modules.0.weight shape torch.Size([64, 64, 3, 3, 3])
decoder.encoder.stages.1.0.convs.1.all_modules.0.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.all_modules.1.weight shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.all_modules.1.bias shape torch.Size([64])
decoder.encoder.stages.2.0.convs.0.conv.weight shape torch.Size([128, 64, 3, 3, 3])
decoder.encoder.stages.2.0.convs.0.conv.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.norm.weight shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.norm.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.all_modules.0.weight shape torch.Size([128, 64, 3, 3, 3])
decoder.encoder.stages.2.0.convs.0.all_modules.0.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.all_modules.1.weight shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.all_modules.1.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.conv.weight shape torch.Size([128, 128, 3, 3, 3])
decoder.encoder.stages.2.0.convs.1.conv.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.norm.weight shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.norm.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.all_modules.0.weight shape torch.Size([128, 128, 3, 3, 3])
decoder.encoder.stages.2.0.convs.1.all_modules.0.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.all_modules.1.weight shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.all_modules.1.bias shape torch.Size([128])
decoder.encoder.stages.3.0.convs.0.conv.weight shape torch.Size([256, 128, 3, 3, 3])
decoder.encoder.stages.3.0.convs.0.conv.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.norm.weight shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.norm.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.all_modules.0.weight shape torch.Size([256, 128, 3, 3, 3])
decoder.encoder.stages.3.0.convs.0.all_modules.0.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.all_modules.1.weight shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.all_modules.1.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.conv.weight shape torch.Size([256, 256, 3, 3, 3])
decoder.encoder.stages.3.0.convs.1.conv.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.norm.weight shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.norm.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.all_modules.0.weight shape torch.Size([256, 256, 3, 3, 3])
decoder.encoder.stages.3.0.convs.1.all_modules.0.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.all_modules.1.weight shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.all_modules.1.bias shape torch.Size([256])
decoder.encoder.stages.4.0.convs.0.conv.weight shape torch.Size([320, 256, 3, 3, 3])
decoder.encoder.stages.4.0.convs.0.conv.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.norm.weight shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.norm.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.all_modules.0.weight shape torch.Size([320, 256, 3, 3, 3])
decoder.encoder.stages.4.0.convs.0.all_modules.0.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.all_modules.1.weight shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.all_modules.1.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.4.0.convs.1.conv.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.norm.weight shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.norm.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.4.0.convs.1.all_modules.0.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.all_modules.1.weight shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.all_modules.1.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.conv.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.5.0.convs.0.conv.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.norm.weight shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.norm.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.5.0.convs.0.all_modules.0.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.all_modules.1.weight shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.all_modules.1.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.5.0.convs.1.conv.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.norm.weight shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.norm.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.5.0.convs.1.all_modules.0.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.all_modules.1.weight shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.all_modules.1.bias shape torch.Size([320])
decoder.stages.0.convs.0.conv.weight shape torch.Size([320, 640, 3, 3, 3])
decoder.stages.0.convs.0.conv.bias shape torch.Size([320])
decoder.stages.0.convs.0.norm.weight shape torch.Size([320])
decoder.stages.0.convs.0.norm.bias shape torch.Size([320])
decoder.stages.0.convs.0.all_modules.0.weight shape torch.Size([320, 640, 3, 3, 3])
decoder.stages.0.convs.0.all_modules.0.bias shape torch.Size([320])
decoder.stages.0.convs.0.all_modules.1.weight shape torch.Size([320])
decoder.stages.0.convs.0.all_modules.1.bias shape torch.Size([320])
decoder.stages.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.stages.0.convs.1.conv.bias shape torch.Size([320])
decoder.stages.0.convs.1.norm.weight shape torch.Size([320])
decoder.stages.0.convs.1.norm.bias shape torch.Size([320])
decoder.stages.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.stages.0.convs.1.all_modules.0.bias shape torch.Size([320])
decoder.stages.0.convs.1.all_modules.1.weight shape torch.Size([320])
decoder.stages.0.convs.1.all_modules.1.bias shape torch.Size([320])
decoder.stages.1.convs.0.conv.weight shape torch.Size([256, 512, 3, 3, 3])
decoder.stages.1.convs.0.conv.bias shape torch.Size([256])
decoder.stages.1.convs.0.norm.weight shape torch.Size([256])
decoder.stages.1.convs.0.norm.bias shape torch.Size([256])
decoder.stages.1.convs.0.all_modules.0.weight shape torch.Size([256, 512, 3, 3, 3])
decoder.stages.1.convs.0.all_modules.0.bias shape torch.Size([256])
decoder.stages.1.convs.0.all_modules.1.weight shape torch.Size([256])
decoder.stages.1.convs.0.all_modules.1.bias shape torch.Size([256])
decoder.stages.1.convs.1.conv.weight shape torch.Size([256, 256, 3, 3, 3])
decoder.stages.1.convs.1.conv.bias shape torch.Size([256])
decoder.stages.1.convs.1.norm.weight shape torch.Size([256])
decoder.stages.1.convs.1.norm.bias shape torch.Size([256])
decoder.stages.1.convs.1.all_modules.0.weight shape torch.Size([256, 256, 3, 3, 3])
decoder.stages.1.convs.1.all_modules.0.bias shape torch.Size([256])
decoder.stages.1.convs.1.all_modules.1.weight shape torch.Size([256])
decoder.stages.1.convs.1.all_modules.1.bias shape torch.Size([256])
decoder.stages.2.convs.0.conv.weight shape torch.Size([128, 256, 3, 3, 3])
decoder.stages.2.convs.0.conv.bias shape torch.Size([128])
decoder.stages.2.convs.0.norm.weight shape torch.Size([128])
decoder.stages.2.convs.0.norm.bias shape torch.Size([128])
decoder.stages.2.convs.0.all_modules.0.weight shape torch.Size([128, 256, 3, 3, 3])
decoder.stages.2.convs.0.all_modules.0.bias shape torch.Size([128])
decoder.stages.2.convs.0.all_modules.1.weight shape torch.Size([128])
decoder.stages.2.convs.0.all_modules.1.bias shape torch.Size([128])
decoder.stages.2.convs.1.conv.weight shape torch.Size([128, 128, 3, 3, 3])
decoder.stages.2.convs.1.conv.bias shape torch.Size([128])
decoder.stages.2.convs.1.norm.weight shape torch.Size([128])
decoder.stages.2.convs.1.norm.bias shape torch.Size([128])
decoder.stages.2.convs.1.all_modules.0.weight shape torch.Size([128, 128, 3, 3, 3])
decoder.stages.2.convs.1.all_modules.0.bias shape torch.Size([128])
decoder.stages.2.convs.1.all_modules.1.weight shape torch.Size([128])
decoder.stages.2.convs.1.all_modules.1.bias shape torch.Size([128])
decoder.stages.3.convs.0.conv.weight shape torch.Size([64, 128, 3, 3, 3])
decoder.stages.3.convs.0.conv.bias shape torch.Size([64])
decoder.stages.3.convs.0.norm.weight shape torch.Size([64])
decoder.stages.3.convs.0.norm.bias shape torch.Size([64])
decoder.stages.3.convs.0.all_modules.0.weight shape torch.Size([64, 128, 3, 3, 3])
decoder.stages.3.convs.0.all_modules.0.bias shape torch.Size([64])
decoder.stages.3.convs.0.all_modules.1.weight shape torch.Size([64])
decoder.stages.3.convs.0.all_modules.1.bias shape torch.Size([64])
decoder.stages.3.convs.1.conv.weight shape torch.Size([64, 64, 3, 3, 3])
decoder.stages.3.convs.1.conv.bias shape torch.Size([64])
decoder.stages.3.convs.1.norm.weight shape torch.Size([64])
decoder.stages.3.convs.1.norm.bias shape torch.Size([64])
decoder.stages.3.convs.1.all_modules.0.weight shape torch.Size([64, 64, 3, 3, 3])
decoder.stages.3.convs.1.all_modules.0.bias shape torch.Size([64])
decoder.stages.3.convs.1.all_modules.1.weight shape torch.Size([64])
decoder.stages.3.convs.1.all_modules.1.bias shape torch.Size([64])
decoder.stages.4.convs.0.conv.weight shape torch.Size([32, 64, 3, 3, 3])
decoder.stages.4.convs.0.conv.bias shape torch.Size([32])
decoder.stages.4.convs.0.norm.weight shape torch.Size([32])
decoder.stages.4.convs.0.norm.bias shape torch.Size([32])
decoder.stages.4.convs.0.all_modules.0.weight shape torch.Size([32, 64, 3, 3, 3])
decoder.stages.4.convs.0.all_modules.0.bias shape torch.Size([32])
decoder.stages.4.convs.0.all_modules.1.weight shape torch.Size([32])
decoder.stages.4.convs.0.all_modules.1.bias shape torch.Size([32])
decoder.stages.4.convs.1.conv.weight shape torch.Size([32, 32, 3, 3, 3])
decoder.stages.4.convs.1.conv.bias shape torch.Size([32])
decoder.stages.4.convs.1.norm.weight shape torch.Size([32])
decoder.stages.4.convs.1.norm.bias shape torch.Size([32])
decoder.stages.4.convs.1.all_modules.0.weight shape torch.Size([32, 32, 3, 3, 3])
decoder.stages.4.convs.1.all_modules.0.bias shape torch.Size([32])
decoder.stages.4.convs.1.all_modules.1.weight shape torch.Size([32])
decoder.stages.4.convs.1.all_modules.1.bias shape torch.Size([32])
decoder.transpconvs.0.weight shape torch.Size([320, 320, 1, 2, 2])
decoder.transpconvs.0.bias shape torch.Size([320])
decoder.transpconvs.1.weight shape torch.Size([320, 256, 2, 2, 2])
decoder.transpconvs.1.bias shape torch.Size([256])
decoder.transpconvs.2.weight shape torch.Size([256, 128, 2, 2, 2])
decoder.transpconvs.2.bias shape torch.Size([128])
decoder.transpconvs.3.weight shape torch.Size([128, 64, 2, 2, 2])
decoder.transpconvs.3.bias shape torch.Size([64])
decoder.transpconvs.4.weight shape torch.Size([64, 32, 2, 2, 2])
decoder.transpconvs.4.bias shape torch.Size([32])
################### Done ###################
2024-12-07 03:50:36.196818: do_dummy_2d_data_aug: True
2024-12-07 03:50:36.201100: Using splits from existing split file: /home/gridsan/nchutisilp/datasets/nnUNet_Datasets/nnUNet_preprocessed/Dataset307_Sohee_Calcium_OCT_CrossValidation/splits_final.json
2024-12-07 03:50:36.203333: The split file contains 5 splits.
2024-12-07 03:50:36.204710: Desired fold for training: 0
2024-12-07 03:50:36.205782: This split has 6 training and 2 validation cases.
using pin_memory on device 0
using pin_memory on device 0
using pin_memory on device 0

This is the configuration used by this training:
Configuration name: 3d_32x160x128_b10
 {'data_identifier': 'nnUNetPlans_3d_fullres', 'preprocessor_name': 'DefaultPreprocessor', 'batch_size': 10, 'patch_size': [32, 160, 128], 'median_image_size_in_voxels': [375.0, 498.0, 498.0], 'spacing': [1.0, 1.0, 1.0], 'normalization_schemes': ['ZScoreNormalization'], 'use_mask_for_norm': [False], 'resampling_fn_data': 'resample_data_or_seg_to_shape', 'resampling_fn_seg': 'resample_data_or_seg_to_shape', 'resampling_fn_data_kwargs': {'is_seg': False, 'order': 3, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_seg_kwargs': {'is_seg': True, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_probabilities': 'resample_data_or_seg_to_shape', 'resampling_fn_probabilities_kwargs': {'is_seg': False, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'architecture': {'network_class_name': 'dynamic_network_architectures.architectures.unet.PlainConvUNet', 'arch_kwargs': {'n_stages': 6, 'features_per_stage': [32, 64, 128, 256, 320, 320], 'conv_op': 'torch.nn.modules.conv.Conv3d', 'kernel_sizes': [[3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3]], 'strides': [[1, 1, 1], [2, 2, 2], [2, 2, 2], [2, 2, 2], [2, 2, 2], [1, 2, 2]], 'n_conv_per_stage': [2, 2, 2, 2, 2, 2], 'n_conv_per_stage_decoder': [2, 2, 2, 2, 2], 'conv_bias': True, 'norm_op': 'torch.nn.modules.instancenorm.InstanceNorm3d', 'norm_op_kwargs': {'eps': 1e-05, 'affine': True}, 'dropout_op': None, 'dropout_op_kwargs': None, 'nonlin': 'torch.nn.LeakyReLU', 'nonlin_kwargs': {'inplace': True}, 'deep_supervision': True}, '_kw_requires_import': ['conv_op', 'norm_op', 'dropout_op', 'nonlin']}, 'batch_dice': True, 'inherits_from': '3d_fullres'} 

These are the global plan.json settings:
 {'dataset_name': 'Dataset307_Sohee_Calcium_OCT_CrossValidation', 'plans_name': 'nnUNetPlans', 'original_median_spacing_after_transp': [1.0, 1.0, 1.0], 'original_median_shape_after_transp': [375, 498, 498], 'image_reader_writer': 'SimpleITKIO', 'transpose_forward': [0, 1, 2], 'transpose_backward': [0, 1, 2], 'experiment_planner_used': 'ExperimentPlanner', 'label_manager': 'LabelManager', 'foreground_intensity_properties_per_channel': {'0': {'max': 0.4977976381778717, 'mean': 0.13507525622844696, 'median': 0.09599608182907104, 'min': 0.0, 'percentile_00_5': 0.014754901640117168, 'percentile_99_5': 0.4977976381778717, 'std': 0.12025152146816254}}} 

2024-12-07 03:50:56.624932: unpacking dataset...
using pin_memory on device 0

This is the configuration used by this training:
Configuration name: 3d_32x160x128_b10
 {'data_identifier': 'nnUNetPlans_3d_fullres', 'preprocessor_name': 'DefaultPreprocessor', 'batch_size': 10, 'patch_size': [32, 160, 128], 'median_image_size_in_voxels': [375.0, 498.0, 498.0], 'spacing': [1.0, 1.0, 1.0], 'normalization_schemes': ['ZScoreNormalization'], 'use_mask_for_norm': [False], 'resampling_fn_data': 'resample_data_or_seg_to_shape', 'resampling_fn_seg': 'resample_data_or_seg_to_shape', 'resampling_fn_data_kwargs': {'is_seg': False, 'order': 3, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_seg_kwargs': {'is_seg': True, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_probabilities': 'resample_data_or_seg_to_shape', 'resampling_fn_probabilities_kwargs': {'is_seg': False, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'architecture': {'network_class_name': 'dynamic_network_architectures.architectures.unet.PlainConvUNet', 'arch_kwargs': {'n_stages': 6, 'features_per_stage': [32, 64, 128, 256, 320, 320], 'conv_op': 'torch.nn.modules.conv.Conv3d', 'kernel_sizes': [[3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3]], 'strides': [[1, 1, 1], [2, 2, 2], [2, 2, 2], [2, 2, 2], [2, 2, 2], [1, 2, 2]], 'n_conv_per_stage': [2, 2, 2, 2, 2, 2], 'n_conv_per_stage_decoder': [2, 2, 2, 2, 2], 'conv_bias': True, 'norm_op': 'torch.nn.modules.instancenorm.InstanceNorm3d', 'norm_op_kwargs': {'eps': 1e-05, 'affine': True}, 'dropout_op': None, 'dropout_op_kwargs': None, 'nonlin': 'torch.nn.LeakyReLU', 'nonlin_kwargs': {'inplace': True}, 'deep_supervision': True}, '_kw_requires_import': ['conv_op', 'norm_op', 'dropout_op', 'nonlin']}, 'batch_dice': True, 'inherits_from': '3d_fullres'} 

These are the global plan.json settings:
 {'dataset_name': 'Dataset307_Sohee_Calcium_OCT_CrossValidation', 'plans_name': 'nnUNetPlans', 'original_median_spacing_after_transp': [1.0, 1.0, 1.0], 'original_median_shape_after_transp': [375, 498, 498], 'image_reader_writer': 'SimpleITKIO', 'transpose_forward': [0, 1, 2], 'transpose_backward': [0, 1, 2], 'experiment_planner_used': 'ExperimentPlanner', 'label_manager': 'LabelManager', 'foreground_intensity_properties_per_channel': {'0': {'max': 0.4977976381778717, 'mean': 0.13507525622844696, 'median': 0.09599608182907104, 'min': 0.0, 'percentile_00_5': 0.014754901640117168, 'percentile_99_5': 0.4977976381778717, 'std': 0.12025152146816254}}} 

2024-12-07 03:50:56.730646: unpacking dataset...
2024-12-07 03:51:02.424338: unpacking done...
2024-12-07 03:51:02.492093: Unable to plot network architecture: nnUNet_compile is enabled!
2024-12-07 03:51:02.580401: 
2024-12-07 03:51:02.581874: Epoch 0
2024-12-07 03:51:02.583104: Current learning rate: 0.01
2024-12-07 03:57:39.398434: Validation loss improved from 1000.00000 to -0.17367! Patience: 0/50
2024-12-07 03:57:39.399457: train_loss -0.1088
2024-12-07 03:57:39.400296: val_loss -0.1737
2024-12-07 03:57:39.401033: Pseudo dice [0.4957]
2024-12-07 03:57:39.401782: Epoch time: 396.82 s
2024-12-07 03:57:39.402543: Yayy! New best EMA pseudo Dice: 0.4957
2024-12-07 03:57:41.074264: 
2024-12-07 03:57:41.075393: Epoch 1
2024-12-07 03:57:41.076259: Current learning rate: 0.00999
2024-12-07 04:03:21.801554: Validation loss improved from -0.17367 to -0.19165! Patience: 0/50
2024-12-07 04:03:21.802554: train_loss -0.2581
2024-12-07 04:03:21.803347: val_loss -0.1917
2024-12-07 04:03:21.804042: Pseudo dice [0.4962]
2024-12-07 04:03:21.804709: Epoch time: 340.73 s
2024-12-07 04:03:21.805351: Yayy! New best EMA pseudo Dice: 0.4958
2024-12-07 04:03:23.607597: 
2024-12-07 04:03:23.608969: Epoch 2
2024-12-07 04:03:23.609879: Current learning rate: 0.00998
2024-12-07 04:09:04.984658: Validation loss improved from -0.19165 to -0.26893! Patience: 0/50
2024-12-07 04:09:04.985753: train_loss -0.312
2024-12-07 04:09:04.986620: val_loss -0.2689
2024-12-07 04:09:04.987406: Pseudo dice [0.5633]
2024-12-07 04:09:04.988270: Epoch time: 341.38 s
2024-12-07 04:09:04.989148: Yayy! New best EMA pseudo Dice: 0.5025
2024-12-07 04:09:06.931090: 
2024-12-07 04:09:06.932561: Epoch 3
2024-12-07 04:09:06.933285: Current learning rate: 0.00997
2024-12-07 04:14:51.586943: Validation loss improved from -0.26893 to -0.28430! Patience: 0/50
2024-12-07 04:14:51.587684: train_loss -0.3447
2024-12-07 04:14:51.588408: val_loss -0.2843
2024-12-07 04:14:51.589025: Pseudo dice [0.5976]
2024-12-07 04:14:51.589698: Epoch time: 344.66 s
2024-12-07 04:14:51.590351: Yayy! New best EMA pseudo Dice: 0.512
2024-12-07 04:14:53.376315: 
2024-12-07 04:14:53.377759: Epoch 4
2024-12-07 04:14:53.378632: Current learning rate: 0.00996
2024-12-07 04:20:38.053249: Validation loss did not improve from -0.28430. Patience: 1/50
2024-12-07 04:20:38.054377: train_loss -0.3891
2024-12-07 04:20:38.055171: val_loss -0.2697
2024-12-07 04:20:38.055835: Pseudo dice [0.585]
2024-12-07 04:20:38.056596: Epoch time: 344.68 s
2024-12-07 04:20:38.449357: Yayy! New best EMA pseudo Dice: 0.5193
2024-12-07 04:20:40.259984: 
2024-12-07 04:20:40.261434: Epoch 5
2024-12-07 04:20:40.262301: Current learning rate: 0.00995
2024-12-07 04:26:33.473421: Validation loss improved from -0.28430 to -0.33799! Patience: 1/50
2024-12-07 04:26:33.474194: train_loss -0.4017
2024-12-07 04:26:33.474862: val_loss -0.338
2024-12-07 04:26:33.475568: Pseudo dice [0.6117]
2024-12-07 04:26:33.476300: Epoch time: 353.22 s
2024-12-07 04:26:33.476981: Yayy! New best EMA pseudo Dice: 0.5285
2024-12-07 04:26:35.184507: 
2024-12-07 04:26:35.185919: Epoch 6
2024-12-07 04:26:35.186896: Current learning rate: 0.00995
2024-12-07 04:32:34.626596: Validation loss improved from -0.33799 to -0.34824! Patience: 0/50
2024-12-07 04:32:34.627645: train_loss -0.4026
2024-12-07 04:32:34.628381: val_loss -0.3482
2024-12-07 04:32:34.628948: Pseudo dice [0.6217]
2024-12-07 04:32:34.629605: Epoch time: 359.44 s
2024-12-07 04:32:34.630212: Yayy! New best EMA pseudo Dice: 0.5379
2024-12-07 04:32:36.423940: 
2024-12-07 04:32:36.425246: Epoch 7
2024-12-07 04:32:36.425976: Current learning rate: 0.00994
2024-12-07 04:38:22.019707: Validation loss improved from -0.34824 to -0.36322! Patience: 0/50
2024-12-07 04:38:22.020929: train_loss -0.4238
2024-12-07 04:38:22.022112: val_loss -0.3632
2024-12-07 04:38:22.023314: Pseudo dice [0.632]
2024-12-07 04:38:22.024108: Epoch time: 345.6 s
2024-12-07 04:38:22.025342: Yayy! New best EMA pseudo Dice: 0.5473
2024-12-07 04:38:24.160096: 
2024-12-07 04:38:24.161403: Epoch 8
2024-12-07 04:38:24.162210: Current learning rate: 0.00993
2024-12-07 04:43:47.385944: Validation loss did not improve from -0.36322. Patience: 1/50
2024-12-07 04:43:47.387051: train_loss -0.4406
2024-12-07 04:43:47.388015: val_loss -0.3617
2024-12-07 04:43:47.388790: Pseudo dice [0.6211]
2024-12-07 04:43:47.389606: Epoch time: 323.23 s
2024-12-07 04:43:47.390400: Yayy! New best EMA pseudo Dice: 0.5547
2024-12-07 04:43:49.218260: 
2024-12-07 04:43:49.219688: Epoch 9
2024-12-07 04:43:49.220563: Current learning rate: 0.00992
2024-12-07 04:49:14.276148: Validation loss improved from -0.36322 to -0.39942! Patience: 1/50
2024-12-07 04:49:14.277238: train_loss -0.4639
2024-12-07 04:49:14.278094: val_loss -0.3994
2024-12-07 04:49:14.278843: Pseudo dice [0.6583]
2024-12-07 04:49:14.279505: Epoch time: 325.06 s
2024-12-07 04:49:14.705339: Yayy! New best EMA pseudo Dice: 0.565
2024-12-07 04:49:16.516825: 
2024-12-07 04:49:16.518195: Epoch 10
2024-12-07 04:49:16.518999: Current learning rate: 0.00991
2024-12-07 04:55:00.526509: Validation loss improved from -0.39942 to -0.41951! Patience: 0/50
2024-12-07 04:55:00.527457: train_loss -0.4813
2024-12-07 04:55:00.528414: val_loss -0.4195
2024-12-07 04:55:00.529221: Pseudo dice [0.6606]
2024-12-07 04:55:00.529983: Epoch time: 344.01 s
2024-12-07 04:55:00.530784: Yayy! New best EMA pseudo Dice: 0.5746
2024-12-07 04:55:02.278317: 
2024-12-07 04:55:02.279733: Epoch 11
2024-12-07 04:55:02.280421: Current learning rate: 0.0099
2024-12-07 05:00:55.852574: Validation loss did not improve from -0.41951. Patience: 1/50
2024-12-07 05:00:55.854863: train_loss -0.4704
2024-12-07 05:00:55.855970: val_loss -0.3566
2024-12-07 05:00:55.856652: Pseudo dice [0.6306]
2024-12-07 05:00:55.857514: Epoch time: 353.58 s
2024-12-07 05:00:55.858317: Yayy! New best EMA pseudo Dice: 0.5802
2024-12-07 05:00:57.606226: 
2024-12-07 05:00:57.607515: Epoch 12
2024-12-07 05:00:57.608396: Current learning rate: 0.00989
2024-12-07 05:06:51.742779: Validation loss did not improve from -0.41951. Patience: 2/50
2024-12-07 05:06:51.743955: train_loss -0.4716
2024-12-07 05:06:51.745718: val_loss -0.3908
2024-12-07 05:06:51.746667: Pseudo dice [0.6593]
2024-12-07 05:06:51.747713: Epoch time: 354.14 s
2024-12-07 05:06:51.748469: Yayy! New best EMA pseudo Dice: 0.5881
2024-12-07 05:06:53.521493: 
2024-12-07 05:06:53.522764: Epoch 13
2024-12-07 05:06:53.523729: Current learning rate: 0.00988
2024-12-07 05:12:55.138604: Validation loss improved from -0.41951 to -0.43259! Patience: 2/50
2024-12-07 05:12:55.139616: train_loss -0.4767
2024-12-07 05:12:55.140479: val_loss -0.4326
2024-12-07 05:12:55.141267: Pseudo dice [0.6756]
2024-12-07 05:12:55.142052: Epoch time: 361.62 s
2024-12-07 05:12:55.142719: Yayy! New best EMA pseudo Dice: 0.5969
2024-12-07 05:12:56.993039: 
2024-12-07 05:12:56.994518: Epoch 14
2024-12-07 05:12:56.995607: Current learning rate: 0.00987
2024-12-07 05:18:59.307813: Validation loss did not improve from -0.43259. Patience: 1/50
2024-12-07 05:18:59.308778: train_loss -0.4944
2024-12-07 05:18:59.309690: val_loss -0.3691
2024-12-07 05:18:59.310411: Pseudo dice [0.6406]
2024-12-07 05:18:59.311246: Epoch time: 362.32 s
2024-12-07 05:18:59.740898: Yayy! New best EMA pseudo Dice: 0.6012
2024-12-07 05:19:01.579647: 
2024-12-07 05:19:01.580447: Epoch 15
2024-12-07 05:19:01.581166: Current learning rate: 0.00986
2024-12-07 05:24:58.150414: Validation loss improved from -0.43259 to -0.47488! Patience: 1/50
2024-12-07 05:24:58.151436: train_loss -0.5072
2024-12-07 05:24:58.152302: val_loss -0.4749
2024-12-07 05:24:58.152972: Pseudo dice [0.7112]
2024-12-07 05:24:58.153665: Epoch time: 356.57 s
2024-12-07 05:24:58.154394: Yayy! New best EMA pseudo Dice: 0.6122
2024-12-07 05:24:59.870132: 
2024-12-07 05:24:59.871343: Epoch 16
2024-12-07 05:24:59.872205: Current learning rate: 0.00986
2024-12-07 05:31:01.747197: Validation loss improved from -0.47488 to -0.48289! Patience: 0/50
2024-12-07 05:31:01.748275: train_loss -0.5164
2024-12-07 05:31:01.748990: val_loss -0.4829
2024-12-07 05:31:01.749666: Pseudo dice [0.7059]
2024-12-07 05:31:01.750300: Epoch time: 361.88 s
2024-12-07 05:31:01.750926: Yayy! New best EMA pseudo Dice: 0.6216
2024-12-07 05:31:03.591336: 
2024-12-07 05:31:03.592665: Epoch 17
2024-12-07 05:31:03.593675: Current learning rate: 0.00985
2024-12-07 05:37:03.834466: Validation loss did not improve from -0.48289. Patience: 1/50
2024-12-07 05:37:03.835491: train_loss -0.5195
2024-12-07 05:37:03.836541: val_loss -0.4498
2024-12-07 05:37:03.837621: Pseudo dice [0.689]
2024-12-07 05:37:03.838530: Epoch time: 360.25 s
2024-12-07 05:37:03.839387: Yayy! New best EMA pseudo Dice: 0.6283
2024-12-07 05:37:05.957609: 
2024-12-07 05:37:05.959017: Epoch 18
2024-12-07 05:37:05.959855: Current learning rate: 0.00984
2024-12-07 05:43:12.185103: Validation loss did not improve from -0.48289. Patience: 2/50
2024-12-07 05:43:12.186093: train_loss -0.5459
2024-12-07 05:43:12.187115: val_loss -0.4827
2024-12-07 05:43:12.188044: Pseudo dice [0.7052]
2024-12-07 05:43:12.188978: Epoch time: 366.23 s
2024-12-07 05:43:12.189853: Yayy! New best EMA pseudo Dice: 0.636
2024-12-07 05:43:13.996288: 
2024-12-07 05:43:13.998164: Epoch 19
2024-12-07 05:43:13.999353: Current learning rate: 0.00983
2024-12-07 05:49:36.673290: Validation loss did not improve from -0.48289. Patience: 3/50
2024-12-07 05:49:36.674221: train_loss -0.5342
2024-12-07 05:49:36.674965: val_loss -0.4413
2024-12-07 05:49:36.675701: Pseudo dice [0.6842]
2024-12-07 05:49:36.676439: Epoch time: 382.68 s
2024-12-07 05:49:37.083612: Yayy! New best EMA pseudo Dice: 0.6408
2024-12-07 05:49:38.847958: 
2024-12-07 05:49:38.849130: Epoch 20
2024-12-07 05:49:38.849936: Current learning rate: 0.00982
2024-12-07 05:56:08.761907: Validation loss did not improve from -0.48289. Patience: 4/50
2024-12-07 05:56:08.762998: train_loss -0.5386
2024-12-07 05:56:08.763934: val_loss -0.4542
2024-12-07 05:56:08.764789: Pseudo dice [0.6754]
2024-12-07 05:56:08.765757: Epoch time: 389.92 s
2024-12-07 05:56:08.766660: Yayy! New best EMA pseudo Dice: 0.6443
2024-12-07 05:56:10.890120: 
2024-12-07 05:56:10.892137: Epoch 21
2024-12-07 05:56:10.893116: Current learning rate: 0.00981
2024-12-07 06:02:37.376717: Validation loss improved from -0.48289 to -0.49114! Patience: 4/50
2024-12-07 06:02:37.378800: train_loss -0.5501
2024-12-07 06:02:37.379922: val_loss -0.4911
2024-12-07 06:02:37.380891: Pseudo dice [0.7149]
2024-12-07 06:02:37.381709: Epoch time: 386.49 s
2024-12-07 06:02:37.382426: Yayy! New best EMA pseudo Dice: 0.6514
2024-12-07 06:02:39.138443: 
2024-12-07 06:02:39.140374: Epoch 22
2024-12-07 06:02:39.141351: Current learning rate: 0.0098
2024-12-07 06:09:06.463840: Validation loss did not improve from -0.49114. Patience: 1/50
2024-12-07 06:09:06.464934: train_loss -0.5524
2024-12-07 06:09:06.465788: val_loss -0.4501
2024-12-07 06:09:06.466606: Pseudo dice [0.6879]
2024-12-07 06:09:06.467431: Epoch time: 387.33 s
2024-12-07 06:09:06.468235: Yayy! New best EMA pseudo Dice: 0.655
2024-12-07 06:09:08.193931: 
2024-12-07 06:09:08.195280: Epoch 23
2024-12-07 06:09:08.196084: Current learning rate: 0.00979
2024-12-07 06:15:13.414645: Validation loss did not improve from -0.49114. Patience: 2/50
2024-12-07 06:15:13.415711: train_loss -0.5633
2024-12-07 06:15:13.416486: val_loss -0.4592
2024-12-07 06:15:13.417252: Pseudo dice [0.6886]
2024-12-07 06:15:13.417931: Epoch time: 365.22 s
2024-12-07 06:15:13.418684: Yayy! New best EMA pseudo Dice: 0.6584
2024-12-07 06:15:15.192549: 
2024-12-07 06:15:15.193782: Epoch 24
2024-12-07 06:15:15.194754: Current learning rate: 0.00978
2024-12-07 06:21:07.739456: Validation loss did not improve from -0.49114. Patience: 3/50
2024-12-07 06:21:07.740516: train_loss -0.5667
2024-12-07 06:21:07.741377: val_loss -0.4588
2024-12-07 06:21:07.742060: Pseudo dice [0.6989]
2024-12-07 06:21:07.742747: Epoch time: 352.55 s
2024-12-07 06:21:08.175727: Yayy! New best EMA pseudo Dice: 0.6624
2024-12-07 06:21:09.900339: 
2024-12-07 06:21:09.901696: Epoch 25
2024-12-07 06:21:09.902411: Current learning rate: 0.00977
2024-12-07 06:26:46.941326: Validation loss did not improve from -0.49114. Patience: 4/50
2024-12-07 06:26:46.942302: train_loss -0.5557
2024-12-07 06:26:46.943163: val_loss -0.4812
2024-12-07 06:26:46.944007: Pseudo dice [0.7114]
2024-12-07 06:26:46.944761: Epoch time: 337.04 s
2024-12-07 06:26:46.945433: Yayy! New best EMA pseudo Dice: 0.6673
2024-12-07 06:26:48.691800: 
2024-12-07 06:26:48.693285: Epoch 26
2024-12-07 06:26:48.694345: Current learning rate: 0.00977
2024-12-07 06:32:40.827203: Validation loss did not improve from -0.49114. Patience: 5/50
2024-12-07 06:32:40.828242: train_loss -0.5779
2024-12-07 06:32:40.829276: val_loss -0.4795
2024-12-07 06:32:40.830275: Pseudo dice [0.7015]
2024-12-07 06:32:40.831317: Epoch time: 352.14 s
2024-12-07 06:32:40.832371: Yayy! New best EMA pseudo Dice: 0.6707
2024-12-07 06:32:42.582243: 
2024-12-07 06:32:42.583718: Epoch 27
2024-12-07 06:32:42.584779: Current learning rate: 0.00976
2024-12-07 06:38:43.391495: Validation loss did not improve from -0.49114. Patience: 6/50
2024-12-07 06:38:43.392479: train_loss -0.5772
2024-12-07 06:38:43.393203: val_loss -0.4855
2024-12-07 06:38:43.393889: Pseudo dice [0.7123]
2024-12-07 06:38:43.394597: Epoch time: 360.81 s
2024-12-07 06:38:43.395252: Yayy! New best EMA pseudo Dice: 0.6749
2024-12-07 06:38:45.148959: 
2024-12-07 06:38:45.150466: Epoch 28
2024-12-07 06:38:45.151401: Current learning rate: 0.00975
2024-12-07 06:44:55.650789: Validation loss improved from -0.49114 to -0.51132! Patience: 6/50
2024-12-07 06:44:55.652442: train_loss -0.5662
2024-12-07 06:44:55.653577: val_loss -0.5113
2024-12-07 06:44:55.654563: Pseudo dice [0.724]
2024-12-07 06:44:55.655347: Epoch time: 370.5 s
2024-12-07 06:44:55.656010: Yayy! New best EMA pseudo Dice: 0.6798
2024-12-07 06:44:58.261055: 
2024-12-07 06:44:58.262323: Epoch 29
2024-12-07 06:44:58.262989: Current learning rate: 0.00974
2024-12-07 06:51:10.271893: Validation loss did not improve from -0.51132. Patience: 1/50
2024-12-07 06:51:10.272913: train_loss -0.5878
2024-12-07 06:51:10.273734: val_loss -0.4687
2024-12-07 06:51:10.274643: Pseudo dice [0.6986]
2024-12-07 06:51:10.275457: Epoch time: 372.01 s
2024-12-07 06:51:10.692915: Yayy! New best EMA pseudo Dice: 0.6817
2024-12-07 06:51:12.467889: 
2024-12-07 06:51:12.469264: Epoch 30
2024-12-07 06:51:12.470053: Current learning rate: 0.00973
2024-12-07 06:57:20.058769: Validation loss did not improve from -0.51132. Patience: 2/50
2024-12-07 06:57:20.059621: train_loss -0.5936
2024-12-07 06:57:20.060427: val_loss -0.4556
2024-12-07 06:57:20.061191: Pseudo dice [0.6867]
2024-12-07 06:57:20.061943: Epoch time: 367.59 s
2024-12-07 06:57:20.062730: Yayy! New best EMA pseudo Dice: 0.6822
2024-12-07 06:57:21.806533: 
2024-12-07 06:57:21.808056: Epoch 31
2024-12-07 06:57:21.808891: Current learning rate: 0.00972
2024-12-07 07:03:26.494689: Validation loss did not improve from -0.51132. Patience: 3/50
2024-12-07 07:03:26.495723: train_loss -0.5932
2024-12-07 07:03:26.496655: val_loss -0.4758
2024-12-07 07:03:26.497439: Pseudo dice [0.6992]
2024-12-07 07:03:26.498176: Epoch time: 364.69 s
2024-12-07 07:03:26.498961: Yayy! New best EMA pseudo Dice: 0.6839
2024-12-07 07:03:28.254567: 
2024-12-07 07:03:28.256137: Epoch 32
2024-12-07 07:03:28.257074: Current learning rate: 0.00971
2024-12-07 07:09:51.753083: Validation loss did not improve from -0.51132. Patience: 4/50
2024-12-07 07:09:51.757925: train_loss -0.5998
2024-12-07 07:09:51.758919: val_loss -0.4753
2024-12-07 07:09:51.759720: Pseudo dice [0.6962]
2024-12-07 07:09:51.760683: Epoch time: 383.5 s
2024-12-07 07:09:51.761624: Yayy! New best EMA pseudo Dice: 0.6851
2024-12-07 07:09:53.578278: 
2024-12-07 07:09:53.579529: Epoch 33
2024-12-07 07:09:53.580278: Current learning rate: 0.0097
2024-12-07 07:16:31.062822: Validation loss did not improve from -0.51132. Patience: 5/50
2024-12-07 07:16:31.063913: train_loss -0.5905
2024-12-07 07:16:31.065438: val_loss -0.5003
2024-12-07 07:16:31.066221: Pseudo dice [0.7139]
2024-12-07 07:16:31.067066: Epoch time: 397.49 s
2024-12-07 07:16:31.067797: Yayy! New best EMA pseudo Dice: 0.688
2024-12-07 07:16:32.842850: 
2024-12-07 07:16:32.844591: Epoch 34
2024-12-07 07:16:32.845649: Current learning rate: 0.00969
2024-12-07 07:23:09.656581: Validation loss improved from -0.51132 to -0.51698! Patience: 5/50
2024-12-07 07:23:09.657758: train_loss -0.6027
2024-12-07 07:23:09.658647: val_loss -0.517
2024-12-07 07:23:09.659424: Pseudo dice [0.7247]
2024-12-07 07:23:09.660295: Epoch time: 396.82 s
2024-12-07 07:23:10.060236: Yayy! New best EMA pseudo Dice: 0.6917
2024-12-07 07:23:11.849170: 
2024-12-07 07:23:11.850615: Epoch 35
2024-12-07 07:23:11.851467: Current learning rate: 0.00968
2024-12-07 07:29:58.165554: Validation loss improved from -0.51698 to -0.52471! Patience: 0/50
2024-12-07 07:29:58.166614: train_loss -0.5989
2024-12-07 07:29:58.167468: val_loss -0.5247
2024-12-07 07:29:58.168267: Pseudo dice [0.7296]
2024-12-07 07:29:58.169035: Epoch time: 406.32 s
2024-12-07 07:29:58.169628: Yayy! New best EMA pseudo Dice: 0.6955
2024-12-07 07:29:59.931522: 
2024-12-07 07:29:59.932829: Epoch 36
2024-12-07 07:29:59.933565: Current learning rate: 0.00968
2024-12-07 07:36:37.368438: Validation loss did not improve from -0.52471. Patience: 1/50
2024-12-07 07:36:37.369561: train_loss -0.6086
2024-12-07 07:36:37.370357: val_loss -0.4971
2024-12-07 07:36:37.370986: Pseudo dice [0.713]
2024-12-07 07:36:37.371674: Epoch time: 397.44 s
2024-12-07 07:36:37.372392: Yayy! New best EMA pseudo Dice: 0.6972
2024-12-07 07:36:39.131935: 
2024-12-07 07:36:39.133305: Epoch 37
2024-12-07 07:36:39.133978: Current learning rate: 0.00967
2024-12-07 07:43:12.012336: Validation loss did not improve from -0.52471. Patience: 2/50
2024-12-07 07:43:12.013496: train_loss -0.6126
2024-12-07 07:43:12.014615: val_loss -0.49
2024-12-07 07:43:12.015342: Pseudo dice [0.7029]
2024-12-07 07:43:12.016148: Epoch time: 392.88 s
2024-12-07 07:43:12.017170: Yayy! New best EMA pseudo Dice: 0.6978
2024-12-07 07:43:13.781326: 
2024-12-07 07:43:13.782812: Epoch 38
2024-12-07 07:43:13.783587: Current learning rate: 0.00966
2024-12-07 07:49:39.341985: Validation loss did not improve from -0.52471. Patience: 3/50
2024-12-07 07:49:39.343199: train_loss -0.6019
2024-12-07 07:49:39.344039: val_loss -0.4914
2024-12-07 07:49:39.344692: Pseudo dice [0.7059]
2024-12-07 07:49:39.345377: Epoch time: 385.56 s
2024-12-07 07:49:39.346035: Yayy! New best EMA pseudo Dice: 0.6986
2024-12-07 07:49:41.179345: 
2024-12-07 07:49:41.180752: Epoch 39
2024-12-07 07:49:41.181670: Current learning rate: 0.00965
2024-12-07 07:55:57.136592: Validation loss did not improve from -0.52471. Patience: 4/50
2024-12-07 07:55:57.137741: train_loss -0.6075
2024-12-07 07:55:57.138726: val_loss -0.4701
2024-12-07 07:55:57.139668: Pseudo dice [0.7053]
2024-12-07 07:55:57.140489: Epoch time: 375.96 s
2024-12-07 07:55:57.555552: Yayy! New best EMA pseudo Dice: 0.6993
2024-12-07 07:55:59.787821: 
2024-12-07 07:55:59.789231: Epoch 40
2024-12-07 07:55:59.790138: Current learning rate: 0.00964
2024-12-07 08:02:16.836588: Validation loss did not improve from -0.52471. Patience: 5/50
2024-12-07 08:02:16.837681: train_loss -0.6091
2024-12-07 08:02:16.838502: val_loss -0.4981
2024-12-07 08:02:16.839234: Pseudo dice [0.7234]
2024-12-07 08:02:16.839919: Epoch time: 377.05 s
2024-12-07 08:02:16.840632: Yayy! New best EMA pseudo Dice: 0.7017
2024-12-07 08:02:18.687140: 
2024-12-07 08:02:18.688438: Epoch 41
2024-12-07 08:02:18.689428: Current learning rate: 0.00963
2024-12-07 08:08:20.668605: Validation loss did not improve from -0.52471. Patience: 6/50
2024-12-07 08:08:20.669556: train_loss -0.6225
2024-12-07 08:08:20.670530: val_loss -0.5031
2024-12-07 08:08:20.671296: Pseudo dice [0.7137]
2024-12-07 08:08:20.672035: Epoch time: 361.98 s
2024-12-07 08:08:20.672806: Yayy! New best EMA pseudo Dice: 0.7029
2024-12-07 08:08:22.399377: 
2024-12-07 08:08:22.400707: Epoch 42
2024-12-07 08:08:22.401507: Current learning rate: 0.00962
2024-12-07 08:14:25.235602: Validation loss did not improve from -0.52471. Patience: 7/50
2024-12-07 08:14:25.239374: train_loss -0.6153
2024-12-07 08:14:25.240624: val_loss -0.5093
2024-12-07 08:14:25.242074: Pseudo dice [0.7194]
2024-12-07 08:14:25.243093: Epoch time: 362.84 s
2024-12-07 08:14:25.244193: Yayy! New best EMA pseudo Dice: 0.7045
2024-12-07 08:14:27.025950: 
2024-12-07 08:14:27.027418: Epoch 43
2024-12-07 08:14:27.028235: Current learning rate: 0.00961
2024-12-07 08:20:59.029013: Validation loss improved from -0.52471 to -0.53606! Patience: 7/50
2024-12-07 08:20:59.030145: train_loss -0.6221
2024-12-07 08:20:59.031671: val_loss -0.5361
2024-12-07 08:20:59.032431: Pseudo dice [0.7342]
2024-12-07 08:20:59.033394: Epoch time: 392.01 s
2024-12-07 08:20:59.034239: Yayy! New best EMA pseudo Dice: 0.7075
2024-12-07 08:21:00.784228: 
2024-12-07 08:21:00.785612: Epoch 44
2024-12-07 08:21:00.786611: Current learning rate: 0.0096
2024-12-07 08:27:46.683605: Validation loss did not improve from -0.53606. Patience: 1/50
2024-12-07 08:27:46.684671: train_loss -0.6181
2024-12-07 08:27:46.685712: val_loss -0.5312
2024-12-07 08:27:46.686729: Pseudo dice [0.7381]
2024-12-07 08:27:46.687531: Epoch time: 405.9 s
2024-12-07 08:27:47.130116: Yayy! New best EMA pseudo Dice: 0.7106
2024-12-07 08:27:48.900476: 
2024-12-07 08:27:48.902076: Epoch 45
2024-12-07 08:27:48.903185: Current learning rate: 0.00959
2024-12-07 08:34:36.187195: Validation loss did not improve from -0.53606. Patience: 2/50
2024-12-07 08:34:36.188251: train_loss -0.6342
2024-12-07 08:34:36.189145: val_loss -0.5037
2024-12-07 08:34:36.189786: Pseudo dice [0.7274]
2024-12-07 08:34:36.190459: Epoch time: 407.29 s
2024-12-07 08:34:36.191106: Yayy! New best EMA pseudo Dice: 0.7122
2024-12-07 08:34:37.917744: 
2024-12-07 08:34:37.919038: Epoch 46
2024-12-07 08:34:37.919786: Current learning rate: 0.00959
2024-12-07 08:41:32.707211: Validation loss did not improve from -0.53606. Patience: 3/50
2024-12-07 08:41:32.708309: train_loss -0.6246
2024-12-07 08:41:32.709263: val_loss -0.511
2024-12-07 08:41:32.710002: Pseudo dice [0.7197]
2024-12-07 08:41:32.710736: Epoch time: 414.79 s
2024-12-07 08:41:32.711425: Yayy! New best EMA pseudo Dice: 0.713
2024-12-07 08:41:34.470443: 
2024-12-07 08:41:34.471668: Epoch 47
2024-12-07 08:41:34.472395: Current learning rate: 0.00958
2024-12-07 08:48:25.714405: Validation loss did not improve from -0.53606. Patience: 4/50
2024-12-07 08:48:25.715325: train_loss -0.6271
2024-12-07 08:48:25.716124: val_loss -0.4946
2024-12-07 08:48:25.716825: Pseudo dice [0.7142]
2024-12-07 08:48:25.717566: Epoch time: 411.25 s
2024-12-07 08:48:25.718334: Yayy! New best EMA pseudo Dice: 0.7131
2024-12-07 08:48:27.461292: 
2024-12-07 08:48:27.462686: Epoch 48
2024-12-07 08:48:27.463432: Current learning rate: 0.00957
2024-12-07 08:55:08.522892: Validation loss improved from -0.53606 to -0.55084! Patience: 4/50
2024-12-07 08:55:08.523961: train_loss -0.6327
2024-12-07 08:55:08.524783: val_loss -0.5508
2024-12-07 08:55:08.525527: Pseudo dice [0.7418]
2024-12-07 08:55:08.526279: Epoch time: 401.06 s
2024-12-07 08:55:08.527093: Yayy! New best EMA pseudo Dice: 0.716
2024-12-07 08:55:10.260531: 
2024-12-07 08:55:10.261746: Epoch 49
2024-12-07 08:55:10.262468: Current learning rate: 0.00956
2024-12-07 09:01:20.576901: Validation loss did not improve from -0.55084. Patience: 1/50
2024-12-07 09:01:20.577958: train_loss -0.6306
2024-12-07 09:01:20.578798: val_loss -0.4945
2024-12-07 09:01:20.579547: Pseudo dice [0.7039]
2024-12-07 09:01:20.580220: Epoch time: 370.32 s
2024-12-07 09:01:22.385965: 
2024-12-07 09:01:22.387270: Epoch 50
2024-12-07 09:01:22.388332: Current learning rate: 0.00955
2024-12-07 09:07:20.599886: Validation loss did not improve from -0.55084. Patience: 2/50
2024-12-07 09:07:20.600926: train_loss -0.6374
2024-12-07 09:07:20.602008: val_loss -0.5325
2024-12-07 09:07:20.602927: Pseudo dice [0.7267]
2024-12-07 09:07:20.603970: Epoch time: 358.22 s
2024-12-07 09:07:22.029053: 
2024-12-07 09:07:22.030470: Epoch 51
2024-12-07 09:07:22.031325: Current learning rate: 0.00954
2024-12-07 09:13:43.131518: Validation loss did not improve from -0.55084. Patience: 3/50
2024-12-07 09:13:43.132735: train_loss -0.6429
2024-12-07 09:13:43.133510: val_loss -0.5361
2024-12-07 09:13:43.134137: Pseudo dice [0.7366]
2024-12-07 09:13:43.134842: Epoch time: 381.1 s
2024-12-07 09:13:43.135484: Yayy! New best EMA pseudo Dice: 0.718
2024-12-07 09:13:44.898828: 
2024-12-07 09:13:44.900272: Epoch 52
2024-12-07 09:13:44.901184: Current learning rate: 0.00953
2024-12-07 09:19:58.182358: Validation loss did not improve from -0.55084. Patience: 4/50
2024-12-07 09:19:58.184094: train_loss -0.6496
2024-12-07 09:19:58.185035: val_loss -0.5073
2024-12-07 09:19:58.185831: Pseudo dice [0.7231]
2024-12-07 09:19:58.186544: Epoch time: 373.29 s
2024-12-07 09:19:58.187357: Yayy! New best EMA pseudo Dice: 0.7185
2024-12-07 09:19:59.939145: 
2024-12-07 09:19:59.940398: Epoch 53
2024-12-07 09:19:59.941174: Current learning rate: 0.00952
2024-12-07 09:25:54.795539: Validation loss did not improve from -0.55084. Patience: 5/50
2024-12-07 09:25:54.796713: train_loss -0.6437
2024-12-07 09:25:54.797590: val_loss -0.5267
2024-12-07 09:25:54.798279: Pseudo dice [0.728]
2024-12-07 09:25:54.798982: Epoch time: 354.86 s
2024-12-07 09:25:54.799647: Yayy! New best EMA pseudo Dice: 0.7195
2024-12-07 09:25:56.569837: 
2024-12-07 09:25:56.571361: Epoch 54
2024-12-07 09:25:56.572364: Current learning rate: 0.00951
2024-12-07 09:31:25.331326: Validation loss did not improve from -0.55084. Patience: 6/50
2024-12-07 09:31:25.332369: train_loss -0.649
2024-12-07 09:31:25.333103: val_loss -0.5148
2024-12-07 09:31:25.333755: Pseudo dice [0.7274]
2024-12-07 09:31:25.334454: Epoch time: 328.76 s
2024-12-07 09:31:25.698187: Yayy! New best EMA pseudo Dice: 0.7203
2024-12-07 09:31:27.454395: 
2024-12-07 09:31:27.455783: Epoch 55
2024-12-07 09:31:27.456562: Current learning rate: 0.0095
2024-12-07 09:36:39.617963: Validation loss did not improve from -0.55084. Patience: 7/50
2024-12-07 09:36:39.619031: train_loss -0.6445
2024-12-07 09:36:39.620006: val_loss -0.5042
2024-12-07 09:36:39.620838: Pseudo dice [0.7159]
2024-12-07 09:36:39.621675: Epoch time: 312.17 s
2024-12-07 09:36:41.103070: 
2024-12-07 09:36:41.104060: Epoch 56
2024-12-07 09:36:41.104787: Current learning rate: 0.00949
2024-12-07 09:41:48.346594: Validation loss did not improve from -0.55084. Patience: 8/50
2024-12-07 09:41:48.347625: train_loss -0.6486
2024-12-07 09:41:48.348678: val_loss -0.5502
2024-12-07 09:41:48.349512: Pseudo dice [0.7406]
2024-12-07 09:41:48.350301: Epoch time: 307.25 s
2024-12-07 09:41:48.351111: Yayy! New best EMA pseudo Dice: 0.7219
2024-12-07 09:41:50.116880: 
2024-12-07 09:41:50.118212: Epoch 57
2024-12-07 09:41:50.118906: Current learning rate: 0.00949
2024-12-07 09:47:10.274762: Validation loss improved from -0.55084 to -0.56096! Patience: 8/50
2024-12-07 09:47:10.276479: train_loss -0.6523
2024-12-07 09:47:10.277531: val_loss -0.561
2024-12-07 09:47:10.278262: Pseudo dice [0.7473]
2024-12-07 09:47:10.279032: Epoch time: 320.16 s
2024-12-07 09:47:10.279783: Yayy! New best EMA pseudo Dice: 0.7244
2024-12-07 09:47:12.018530: 
2024-12-07 09:47:12.019852: Epoch 58
2024-12-07 09:47:12.020587: Current learning rate: 0.00948
2024-12-07 09:53:02.498560: Validation loss did not improve from -0.56096. Patience: 1/50
2024-12-07 09:53:02.499534: train_loss -0.6552
2024-12-07 09:53:02.500473: val_loss -0.5114
2024-12-07 09:53:02.501180: Pseudo dice [0.7324]
2024-12-07 09:53:02.501926: Epoch time: 350.48 s
2024-12-07 09:53:02.502672: Yayy! New best EMA pseudo Dice: 0.7252
2024-12-07 09:53:04.293050: 
2024-12-07 09:53:04.294297: Epoch 59
2024-12-07 09:53:04.295074: Current learning rate: 0.00947
2024-12-07 09:59:09.859287: Validation loss did not improve from -0.56096. Patience: 2/50
2024-12-07 09:59:09.860897: train_loss -0.6529
2024-12-07 09:59:09.861912: val_loss -0.5258
2024-12-07 09:59:09.862672: Pseudo dice [0.7286]
2024-12-07 09:59:09.863313: Epoch time: 365.57 s
2024-12-07 09:59:10.313256: Yayy! New best EMA pseudo Dice: 0.7256
2024-12-07 09:59:12.086585: 
2024-12-07 09:59:12.088006: Epoch 60
2024-12-07 09:59:12.088813: Current learning rate: 0.00946
2024-12-07 10:04:18.239188: Validation loss did not improve from -0.56096. Patience: 3/50
2024-12-07 10:04:18.240212: train_loss -0.6519
2024-12-07 10:04:18.241197: val_loss -0.5257
2024-12-07 10:04:18.242211: Pseudo dice [0.7359]
2024-12-07 10:04:18.243085: Epoch time: 306.15 s
2024-12-07 10:04:18.244009: Yayy! New best EMA pseudo Dice: 0.7266
2024-12-07 10:04:21.670038: 
2024-12-07 10:04:21.671634: Epoch 61
2024-12-07 10:04:21.672654: Current learning rate: 0.00945
2024-12-07 10:09:30.166121: Validation loss did not improve from -0.56096. Patience: 4/50
2024-12-07 10:09:30.167167: train_loss -0.6633
2024-12-07 10:09:30.168062: val_loss -0.5402
2024-12-07 10:09:30.169064: Pseudo dice [0.7388]
2024-12-07 10:09:30.169956: Epoch time: 308.5 s
2024-12-07 10:09:30.170758: Yayy! New best EMA pseudo Dice: 0.7278
2024-12-07 10:09:31.949545: 
2024-12-07 10:09:31.951041: Epoch 62
2024-12-07 10:09:31.951950: Current learning rate: 0.00944
2024-12-07 10:14:37.808481: Validation loss did not improve from -0.56096. Patience: 5/50
2024-12-07 10:14:37.809366: train_loss -0.662
2024-12-07 10:14:37.810187: val_loss -0.5303
2024-12-07 10:14:37.810879: Pseudo dice [0.722]
2024-12-07 10:14:37.811538: Epoch time: 305.86 s
2024-12-07 10:14:39.220557: 
2024-12-07 10:14:39.221702: Epoch 63
2024-12-07 10:14:39.222499: Current learning rate: 0.00943
2024-12-07 10:20:14.777041: Validation loss did not improve from -0.56096. Patience: 6/50
2024-12-07 10:20:14.778117: train_loss -0.659
2024-12-07 10:20:14.778809: val_loss -0.5283
2024-12-07 10:20:14.779498: Pseudo dice [0.7243]
2024-12-07 10:20:14.780129: Epoch time: 335.56 s
2024-12-07 10:20:16.160941: 
2024-12-07 10:20:16.306638: Epoch 64
2024-12-07 10:20:16.307543: Current learning rate: 0.00942
2024-12-07 10:26:10.099869: Validation loss did not improve from -0.56096. Patience: 7/50
2024-12-07 10:26:10.102035: train_loss -0.6653
2024-12-07 10:26:10.103370: val_loss -0.5006
2024-12-07 10:26:10.104407: Pseudo dice [0.7266]
2024-12-07 10:26:10.105369: Epoch time: 353.94 s
2024-12-07 10:26:11.968619: 
2024-12-07 10:26:11.970019: Epoch 65
2024-12-07 10:26:11.971017: Current learning rate: 0.00941
2024-12-07 10:31:43.657628: Validation loss did not improve from -0.56096. Patience: 8/50
2024-12-07 10:31:43.658727: train_loss -0.6564
2024-12-07 10:31:43.659641: val_loss -0.4323
2024-12-07 10:31:43.660288: Pseudo dice [0.6823]
2024-12-07 10:31:43.661002: Epoch time: 331.69 s
2024-12-07 10:31:45.100310: 
2024-12-07 10:31:45.101578: Epoch 66
2024-12-07 10:31:45.102410: Current learning rate: 0.0094
2024-12-07 10:36:24.600782: Validation loss did not improve from -0.56096. Patience: 9/50
2024-12-07 10:36:24.601724: train_loss -0.6531
2024-12-07 10:36:24.604882: val_loss -0.4968
2024-12-07 10:36:24.605910: Pseudo dice [0.7185]
2024-12-07 10:36:24.607255: Epoch time: 279.5 s
2024-12-07 10:36:26.011527: 
2024-12-07 10:36:26.012883: Epoch 67
2024-12-07 10:36:26.014022: Current learning rate: 0.00939
2024-12-07 10:41:41.078449: Validation loss did not improve from -0.56096. Patience: 10/50
2024-12-07 10:41:41.079467: train_loss -0.6674
2024-12-07 10:41:41.080455: val_loss -0.5382
2024-12-07 10:41:41.081370: Pseudo dice [0.7422]
2024-12-07 10:41:41.082394: Epoch time: 315.07 s
2024-12-07 10:41:42.564192: 
2024-12-07 10:41:42.565665: Epoch 68
2024-12-07 10:41:42.566633: Current learning rate: 0.00939
2024-12-07 10:47:28.801384: Validation loss improved from -0.56096 to -0.57583! Patience: 10/50
2024-12-07 10:47:28.802380: train_loss -0.6713
2024-12-07 10:47:28.803491: val_loss -0.5758
2024-12-07 10:47:28.804519: Pseudo dice [0.7629]
2024-12-07 10:47:28.805495: Epoch time: 346.24 s
2024-12-07 10:47:28.806433: Yayy! New best EMA pseudo Dice: 0.728
2024-12-07 10:47:30.582760: 
2024-12-07 10:47:30.584244: Epoch 69
2024-12-07 10:47:30.585328: Current learning rate: 0.00938
2024-12-07 10:53:30.256431: Validation loss did not improve from -0.57583. Patience: 1/50
2024-12-07 10:53:30.257439: train_loss -0.6717
2024-12-07 10:53:30.258180: val_loss -0.5196
2024-12-07 10:53:30.258806: Pseudo dice [0.7308]
2024-12-07 10:53:30.259542: Epoch time: 359.68 s
2024-12-07 10:53:30.658660: Yayy! New best EMA pseudo Dice: 0.7282
2024-12-07 10:53:32.483397: 
2024-12-07 10:53:32.484877: Epoch 70
2024-12-07 10:53:32.485933: Current learning rate: 0.00937
2024-12-07 10:58:59.567262: Validation loss did not improve from -0.57583. Patience: 2/50
2024-12-07 10:58:59.568599: train_loss -0.6627
2024-12-07 10:58:59.569521: val_loss -0.5076
2024-12-07 10:58:59.570406: Pseudo dice [0.72]
2024-12-07 10:58:59.571475: Epoch time: 327.09 s
2024-12-07 10:59:01.483600: 
2024-12-07 10:59:01.485047: Epoch 71
2024-12-07 10:59:01.485819: Current learning rate: 0.00936
2024-12-07 11:04:03.582436: Validation loss did not improve from -0.57583. Patience: 3/50
2024-12-07 11:04:03.583915: train_loss -0.6763
2024-12-07 11:04:03.584840: val_loss -0.5202
2024-12-07 11:04:03.585524: Pseudo dice [0.7275]
2024-12-07 11:04:03.586302: Epoch time: 302.1 s
2024-12-07 11:04:05.161639: 
2024-12-07 11:04:05.163098: Epoch 72
2024-12-07 11:04:05.164092: Current learning rate: 0.00935
2024-12-07 11:09:07.153181: Validation loss did not improve from -0.57583. Patience: 4/50
2024-12-07 11:09:07.154103: train_loss -0.6836
2024-12-07 11:09:07.154977: val_loss -0.5343
2024-12-07 11:09:07.155817: Pseudo dice [0.7421]
2024-12-07 11:09:07.156651: Epoch time: 301.99 s
2024-12-07 11:09:07.157392: Yayy! New best EMA pseudo Dice: 0.7289
2024-12-07 11:09:08.997521: 
2024-12-07 11:09:08.998898: Epoch 73
2024-12-07 11:09:08.999659: Current learning rate: 0.00934
2024-12-07 11:14:48.343667: Validation loss did not improve from -0.57583. Patience: 5/50
2024-12-07 11:14:48.344623: train_loss -0.6697
2024-12-07 11:14:48.345433: val_loss -0.5344
2024-12-07 11:14:48.346122: Pseudo dice [0.7389]
2024-12-07 11:14:48.346984: Epoch time: 339.35 s
2024-12-07 11:14:48.347672: Yayy! New best EMA pseudo Dice: 0.7299
2024-12-07 11:14:50.223341: 
2024-12-07 11:14:50.224768: Epoch 74
2024-12-07 11:14:50.225507: Current learning rate: 0.00933
2024-12-07 11:20:47.455590: Validation loss did not improve from -0.57583. Patience: 6/50
2024-12-07 11:20:47.456655: train_loss -0.6658
2024-12-07 11:20:47.457794: val_loss -0.5429
2024-12-07 11:20:47.458717: Pseudo dice [0.7392]
2024-12-07 11:20:47.459441: Epoch time: 357.23 s
2024-12-07 11:20:47.905327: Yayy! New best EMA pseudo Dice: 0.7308
2024-12-07 11:20:49.764843: 
2024-12-07 11:20:49.766234: Epoch 75
2024-12-07 11:20:49.767110: Current learning rate: 0.00932
2024-12-07 11:26:32.764309: Validation loss did not improve from -0.57583. Patience: 7/50
2024-12-07 11:26:32.769598: train_loss -0.6749
2024-12-07 11:26:32.770624: val_loss -0.5336
2024-12-07 11:26:32.771441: Pseudo dice [0.7374]
2024-12-07 11:26:32.772329: Epoch time: 343.01 s
2024-12-07 11:26:32.773287: Yayy! New best EMA pseudo Dice: 0.7315
2024-12-07 11:26:34.622688: 
2024-12-07 11:26:34.624171: Epoch 76
2024-12-07 11:26:34.625179: Current learning rate: 0.00931
2024-12-07 11:31:12.490698: Validation loss did not improve from -0.57583. Patience: 8/50
2024-12-07 11:31:12.491929: train_loss -0.6787
2024-12-07 11:31:12.492839: val_loss -0.527
2024-12-07 11:31:12.493558: Pseudo dice [0.734]
2024-12-07 11:31:12.494427: Epoch time: 277.87 s
2024-12-07 11:31:12.495066: Yayy! New best EMA pseudo Dice: 0.7317
2024-12-07 11:31:14.383196: 
2024-12-07 11:31:14.384826: Epoch 77
2024-12-07 11:31:14.385943: Current learning rate: 0.0093
2024-12-07 11:36:12.787498: Validation loss did not improve from -0.57583. Patience: 9/50
2024-12-07 11:36:12.788501: train_loss -0.6867
2024-12-07 11:36:12.789494: val_loss -0.5409
2024-12-07 11:36:12.790302: Pseudo dice [0.7434]
2024-12-07 11:36:12.791132: Epoch time: 298.41 s
2024-12-07 11:36:12.791914: Yayy! New best EMA pseudo Dice: 0.7329
2024-12-07 11:36:14.713381: 
2024-12-07 11:36:14.714639: Epoch 78
2024-12-07 11:36:14.715511: Current learning rate: 0.0093
2024-12-07 11:41:57.872827: Validation loss did not improve from -0.57583. Patience: 10/50
2024-12-07 11:41:57.873604: train_loss -0.6852
2024-12-07 11:41:57.874510: val_loss -0.5265
2024-12-07 11:41:57.875335: Pseudo dice [0.7338]
2024-12-07 11:41:57.876161: Epoch time: 343.16 s
2024-12-07 11:41:57.876981: Yayy! New best EMA pseudo Dice: 0.733
2024-12-07 11:41:59.833040: 
2024-12-07 11:41:59.834369: Epoch 79
2024-12-07 11:41:59.835016: Current learning rate: 0.00929
2024-12-07 11:47:53.685482: Validation loss did not improve from -0.57583. Patience: 11/50
2024-12-07 11:47:53.686624: train_loss -0.6846
2024-12-07 11:47:53.687409: val_loss -0.5521
2024-12-07 11:47:53.688103: Pseudo dice [0.7482]
2024-12-07 11:47:53.688816: Epoch time: 353.85 s
2024-12-07 11:47:54.123734: Yayy! New best EMA pseudo Dice: 0.7345
2024-12-07 11:47:55.985373: 
2024-12-07 11:47:55.986741: Epoch 80
2024-12-07 11:47:55.987441: Current learning rate: 0.00928
2024-12-07 11:53:31.947493: Validation loss did not improve from -0.57583. Patience: 12/50
2024-12-07 11:53:31.948574: train_loss -0.6854
2024-12-07 11:53:31.949434: val_loss -0.5372
2024-12-07 11:53:31.950109: Pseudo dice [0.7345]
2024-12-07 11:53:31.950792: Epoch time: 335.96 s
2024-12-07 11:53:33.409021: 
2024-12-07 11:53:33.410208: Epoch 81
2024-12-07 11:53:33.410851: Current learning rate: 0.00927
2024-12-07 11:58:20.705554: Validation loss did not improve from -0.57583. Patience: 13/50
2024-12-07 11:58:20.706510: train_loss -0.6896
2024-12-07 11:58:20.707397: val_loss -0.5453
2024-12-07 11:58:20.708061: Pseudo dice [0.7441]
2024-12-07 11:58:20.708782: Epoch time: 287.3 s
2024-12-07 11:58:20.709422: Yayy! New best EMA pseudo Dice: 0.7355
2024-12-07 11:58:23.036463: 
2024-12-07 11:58:23.037882: Epoch 82
2024-12-07 11:58:23.038729: Current learning rate: 0.00926
2024-12-07 12:03:45.020036: Validation loss did not improve from -0.57583. Patience: 14/50
2024-12-07 12:03:45.021055: train_loss -0.6996
2024-12-07 12:03:45.021946: val_loss -0.5587
2024-12-07 12:03:45.022711: Pseudo dice [0.7527]
2024-12-07 12:03:45.023401: Epoch time: 321.99 s
2024-12-07 12:03:45.024076: Yayy! New best EMA pseudo Dice: 0.7372
2024-12-07 12:03:46.824095: 
2024-12-07 12:03:46.825347: Epoch 83
2024-12-07 12:03:46.826157: Current learning rate: 0.00925
2024-12-07 12:09:12.053432: Validation loss did not improve from -0.57583. Patience: 15/50
2024-12-07 12:09:12.054715: train_loss -0.686
2024-12-07 12:09:12.055529: val_loss -0.5426
2024-12-07 12:09:12.056240: Pseudo dice [0.7451]
2024-12-07 12:09:12.057055: Epoch time: 325.23 s
2024-12-07 12:09:12.057785: Yayy! New best EMA pseudo Dice: 0.738
2024-12-07 12:09:13.825800: 
2024-12-07 12:09:13.827072: Epoch 84
2024-12-07 12:09:13.827769: Current learning rate: 0.00924
2024-12-07 12:14:10.078284: failed to log:  (<class 'FileNotFoundError'>, FileNotFoundError(2, 'No such file or directory'), <traceback object at 0x7fc1b023fe40>)
2024-12-07 12:14:10.078284: failed to log:  (<class 'FileNotFoundError'>, FileNotFoundError(2, 'No such file or directory'), <traceback object at 0x7fc1b023fe40>)
2024-12-07 12:14:10.078284: failed to log:  (<class 'FileNotFoundError'>, FileNotFoundError(2, 'No such file or directory'), <traceback object at 0x7fc1b023fe40>)
2024-12-07 12:14:10.078284: failed to log:  (<class 'FileNotFoundError'>, FileNotFoundError(2, 'No such file or directory'), <traceback object at 0x7fc1b023fe40>)
2024-12-07 12:14:10.078284: failed to log:  (<class 'FileNotFoundError'>, FileNotFoundError(2, 'No such file or directory'), <traceback object at 0x7fc1b023fe40>)
2024-12-07 12:14:10.078284: Validation loss did not improve from -0.57583. Patience: 16/50
2024-12-07 12:14:12.582630: failed to log:  (<class 'FileNotFoundError'>, FileNotFoundError(2, 'No such file or directory'), <traceback object at 0x7fc1b023fe40>)
2024-12-07 12:14:12.582630: failed to log:  (<class 'FileNotFoundError'>, FileNotFoundError(2, 'No such file or directory'), <traceback object at 0x7fc1b023fe40>)
2024-12-07 12:14:12.582630: failed to log:  (<class 'FileNotFoundError'>, FileNotFoundError(2, 'No such file or directory'), <traceback object at 0x7fc1b023fe40>)
2024-12-07 12:14:12.582630: failed to log:  (<class 'FileNotFoundError'>, FileNotFoundError(2, 'No such file or directory'), <traceback object at 0x7fc1b023fe40>)
2024-12-07 12:14:12.582630: failed to log:  (<class 'FileNotFoundError'>, FileNotFoundError(2, 'No such file or directory'), <traceback object at 0x7fc1b023fe40>)
2024-12-07 12:14:12.582630: train_loss -0.6917
2024-12-07 12:14:15.085747: failed to log:  (<class 'FileNotFoundError'>, FileNotFoundError(2, 'No such file or directory'), <traceback object at 0x7fc1f433aa00>)
2024-12-07 12:14:15.085747: failed to log:  (<class 'FileNotFoundError'>, FileNotFoundError(2, 'No such file or directory'), <traceback object at 0x7fc1f433aa00>)
2024-12-07 12:14:15.085747: failed to log:  (<class 'FileNotFoundError'>, FileNotFoundError(2, 'No such file or directory'), <traceback object at 0x7fc1f433aa00>)
2024-12-07 12:14:15.085747: failed to log:  (<class 'FileNotFoundError'>, FileNotFoundError(2, 'No such file or directory'), <traceback object at 0x7fc1f433aa00>)
2024-12-07 12:14:15.085747: failed to log:  (<class 'FileNotFoundError'>, FileNotFoundError(2, 'No such file or directory'), <traceback object at 0x7fc1f433aa00>)
2024-12-07 12:14:15.085747: val_loss -0.5436
2024-12-07 12:14:17.588832: failed to log:  (<class 'FileNotFoundError'>, FileNotFoundError(2, 'No such file or directory'), <traceback object at 0x7fc1f6406fc0>)
2024-12-07 12:14:17.588832: failed to log:  (<class 'FileNotFoundError'>, FileNotFoundError(2, 'No such file or directory'), <traceback object at 0x7fc1f6406fc0>)
2024-12-07 12:14:17.588832: failed to log:  (<class 'FileNotFoundError'>, FileNotFoundError(2, 'No such file or directory'), <traceback object at 0x7fc1f6406fc0>)
2024-12-07 12:14:17.588832: failed to log:  (<class 'FileNotFoundError'>, FileNotFoundError(2, 'No such file or directory'), <traceback object at 0x7fc1f6406fc0>)
2024-12-07 12:14:17.588832: failed to log:  (<class 'FileNotFoundError'>, FileNotFoundError(2, 'No such file or directory'), <traceback object at 0x7fc1f6406fc0>)
2024-12-07 12:14:17.588832: Pseudo dice [0.733]
2024-12-07 12:14:20.092020: failed to log:  (<class 'FileNotFoundError'>, FileNotFoundError(2, 'No such file or directory'), <traceback object at 0x7fc1f41fcf00>)
2024-12-07 12:14:20.092020: failed to log:  (<class 'FileNotFoundError'>, FileNotFoundError(2, 'No such file or directory'), <traceback object at 0x7fc1f41fcf00>)
2024-12-07 12:14:20.092020: failed to log:  (<class 'FileNotFoundError'>, FileNotFoundError(2, 'No such file or directory'), <traceback object at 0x7fc1f41fcf00>)
2024-12-07 12:14:20.092020: failed to log:  (<class 'FileNotFoundError'>, FileNotFoundError(2, 'No such file or directory'), <traceback object at 0x7fc1f41fcf00>)
2024-12-07 12:14:20.092020: failed to log:  (<class 'FileNotFoundError'>, FileNotFoundError(2, 'No such file or directory'), <traceback object at 0x7fc1f41fcf00>)
2024-12-07 12:14:20.092020: Epoch time: 298.76 s
Traceback (most recent call last):
  File "/home/gridsan/nchutisilp/.conda/envs/nnunet/bin/nnUNetv2_train", line 8, in <module>
    sys.exit(run_training_entry())
  File "/home/gridsan/nchutisilp/projects/nnUNet/nnunetv2/run/run_training.py", line 275, in run_training_entry
    run_training(args.dataset_name_or_id, args.configuration, args.fold, args.tr, args.p, args.pretrained_weights,
  File "/home/gridsan/nchutisilp/projects/nnUNet/nnunetv2/run/run_training.py", line 211, in run_training
    nnunet_trainer.run_training()
  File "/home/gridsan/nchutisilp/projects/nnUNet/nnunetv2/training/nnUNetTrainer/nnUNetTrainer.py", line 1411, in run_training
    self.on_epoch_end()
  File "/home/gridsan/nchutisilp/projects/nnUNet/nnunetv2/training/nnUNetTrainer/nnUNetTrainer.py", line 1165, in on_epoch_end
    self.save_checkpoint(join(self.output_folder, 'checkpoint_latest.pth'))
  File "/home/gridsan/nchutisilp/projects/nnUNet/nnunetv2/training/nnUNetTrainer/nnUNetTrainer.py", line 1201, in save_checkpoint
    torch.save(checkpoint, filename)
  File "/home/gridsan/nchutisilp/.local/lib/python3.9/site-packages/torch/serialization.py", line 627, in save
    with _open_zipfile_writer(f) as opened_zipfile:
  File "/home/gridsan/nchutisilp/.local/lib/python3.9/site-packages/torch/serialization.py", line 501, in _open_zipfile_writer
    return container(name_or_buffer)
  File "/home/gridsan/nchutisilp/.local/lib/python3.9/site-packages/torch/serialization.py", line 472, in __init__
    super().__init__(torch._C.PyTorchFileWriter(self.name))
RuntimeError: Parent directory /home/gridsan/nchutisilp/datasets/nnUNet_Datasets/nnUNet_results/Dataset307_Sohee_Calcium_OCT_CrossValidation/nnUNetTrainer__nnUNetPlans__3d_32x160x128_b10/fold_1 does not exist.
Exception in thread Thread-2:
Traceback (most recent call last):
  File "/home/gridsan/nchutisilp/.conda/envs/nnunet/lib/python3.9/threading.py", line 980, in _bootstrap_inner
Exception in thread Thread-3:
Traceback (most recent call last):
  File "/home/gridsan/nchutisilp/.conda/envs/nnunet/lib/python3.9/threading.py", line 980, in _bootstrap_inner
    self.run()
  File "/home/gridsan/nchutisilp/.conda/envs/nnunet/lib/python3.9/threading.py", line 917, in run
    self.run()
  File "/home/gridsan/nchutisilp/.conda/envs/nnunet/lib/python3.9/threading.py", line 917, in run
    self._target(*self._args, **self._kwargs)
  File "/home/gridsan/nchutisilp/.local/lib/python3.9/site-packages/batchgenerators/dataloading/nondet_multi_threaded_augmenter.py", line 125, in results_loop
    self._target(*self._args, **self._kwargs)
  File "/home/gridsan/nchutisilp/.local/lib/python3.9/site-packages/batchgenerators/dataloading/nondet_multi_threaded_augmenter.py", line 125, in results_loop
    raise e
  File "/home/gridsan/nchutisilp/.local/lib/python3.9/site-packages/batchgenerators/dataloading/nondet_multi_threaded_augmenter.py", line 103, in results_loop
    raise e
  File "/home/gridsan/nchutisilp/.local/lib/python3.9/site-packages/batchgenerators/dataloading/nondet_multi_threaded_augmenter.py", line 103, in results_loop
    raise RuntimeError("One or more background workers are no longer alive. Exiting. Please check the "
RuntimeError: One or more background workers are no longer alive. Exiting. Please check the print statements above for the actual error message
    raise RuntimeError("One or more background workers are no longer alive. Exiting. Please check the "
RuntimeError: One or more background workers are no longer alive. Exiting. Please check the print statements above for the actual error message
2024-12-07 03:51:02.423550: unpacking done...
2024-12-07 03:51:02.492353: Unable to plot network architecture: nnUNet_compile is enabled!
2024-12-07 03:51:02.560128: 
2024-12-07 03:51:02.561481: Epoch 0
2024-12-07 03:51:02.562520: Current learning rate: 0.01
2024-12-07 03:57:24.773936: Validation loss improved from 1000.00000 to -0.17315! Patience: 0/50
2024-12-07 03:57:24.775453: train_loss -0.0655
2024-12-07 03:57:24.776501: val_loss -0.1731
2024-12-07 03:57:24.777174: Pseudo dice [0.5387]
2024-12-07 03:57:24.777745: Epoch time: 382.22 s
2024-12-07 03:57:24.778397: Yayy! New best EMA pseudo Dice: 0.5387
2024-12-07 03:57:26.519036: 
2024-12-07 03:57:26.520130: Epoch 1
2024-12-07 03:57:26.520847: Current learning rate: 0.00999
2024-12-07 04:02:51.689468: Validation loss improved from -0.17315 to -0.25894! Patience: 0/50
2024-12-07 04:02:51.690804: train_loss -0.248
2024-12-07 04:02:51.691655: val_loss -0.2589
2024-12-07 04:02:51.692407: Pseudo dice [0.6024]
2024-12-07 04:02:51.693097: Epoch time: 325.17 s
2024-12-07 04:02:51.693967: Yayy! New best EMA pseudo Dice: 0.5451
2024-12-07 04:02:53.425756: 
2024-12-07 04:02:53.427230: Epoch 2
2024-12-07 04:02:53.427980: Current learning rate: 0.00998
2024-12-07 04:08:36.853228: Validation loss improved from -0.25894 to -0.27147! Patience: 0/50
2024-12-07 04:08:36.854231: train_loss -0.2739
2024-12-07 04:08:36.855090: val_loss -0.2715
2024-12-07 04:08:36.856061: Pseudo dice [0.6078]
2024-12-07 04:08:36.856725: Epoch time: 343.43 s
2024-12-07 04:08:36.857671: Yayy! New best EMA pseudo Dice: 0.5513
2024-12-07 04:08:38.638711: 
2024-12-07 04:08:38.640179: Epoch 3
2024-12-07 04:08:38.641192: Current learning rate: 0.00997
2024-12-07 04:14:34.485357: Validation loss improved from -0.27147 to -0.28736! Patience: 0/50
2024-12-07 04:14:34.486377: train_loss -0.3187
2024-12-07 04:14:34.487208: val_loss -0.2874
2024-12-07 04:14:34.488016: Pseudo dice [0.6207]
2024-12-07 04:14:34.488829: Epoch time: 355.85 s
2024-12-07 04:14:34.489539: Yayy! New best EMA pseudo Dice: 0.5583
2024-12-07 04:14:36.210821: 
2024-12-07 04:14:36.212341: Epoch 4
2024-12-07 04:14:36.213655: Current learning rate: 0.00996
2024-12-07 04:20:37.481893: Validation loss improved from -0.28736 to -0.31425! Patience: 0/50
2024-12-07 04:20:37.482996: train_loss -0.3315
2024-12-07 04:20:37.483886: val_loss -0.3142
2024-12-07 04:20:37.484779: Pseudo dice [0.639]
2024-12-07 04:20:37.485589: Epoch time: 361.27 s
2024-12-07 04:20:37.836480: Yayy! New best EMA pseudo Dice: 0.5663
2024-12-07 04:20:39.620415: 
2024-12-07 04:20:39.621687: Epoch 5
2024-12-07 04:20:39.622384: Current learning rate: 0.00995
2024-12-07 04:26:45.680565: Validation loss improved from -0.31425 to -0.39258! Patience: 0/50
2024-12-07 04:26:45.681484: train_loss -0.3805
2024-12-07 04:26:45.682601: val_loss -0.3926
2024-12-07 04:26:45.683529: Pseudo dice [0.6845]
2024-12-07 04:26:45.684415: Epoch time: 366.06 s
2024-12-07 04:26:45.685328: Yayy! New best EMA pseudo Dice: 0.5781
2024-12-07 04:26:47.387242: 
2024-12-07 04:26:47.388729: Epoch 6
2024-12-07 04:26:47.389556: Current learning rate: 0.00995
2024-12-07 04:32:43.542343: Validation loss did not improve from -0.39258. Patience: 1/50
2024-12-07 04:32:43.543125: train_loss -0.4078
2024-12-07 04:32:43.543826: val_loss -0.3774
2024-12-07 04:32:43.544615: Pseudo dice [0.6731]
2024-12-07 04:32:43.545286: Epoch time: 356.16 s
2024-12-07 04:32:43.546041: Yayy! New best EMA pseudo Dice: 0.5876
2024-12-07 04:32:45.321785: 
2024-12-07 04:32:45.323253: Epoch 7
2024-12-07 04:32:45.324138: Current learning rate: 0.00994
2024-12-07 04:38:38.515568: Validation loss did not improve from -0.39258. Patience: 2/50
2024-12-07 04:38:38.516674: train_loss -0.4109
2024-12-07 04:38:38.517536: val_loss -0.383
2024-12-07 04:38:38.518357: Pseudo dice [0.6758]
2024-12-07 04:38:38.519155: Epoch time: 353.2 s
2024-12-07 04:38:38.520037: Yayy! New best EMA pseudo Dice: 0.5965
2024-12-07 04:38:40.724599: 
2024-12-07 04:38:40.726101: Epoch 8
2024-12-07 04:38:40.726845: Current learning rate: 0.00993
2024-12-07 04:44:14.013505: Validation loss did not improve from -0.39258. Patience: 3/50
2024-12-07 04:44:14.014492: train_loss -0.4223
2024-12-07 04:44:14.015209: val_loss -0.3915
2024-12-07 04:44:14.015991: Pseudo dice [0.691]
2024-12-07 04:44:14.016804: Epoch time: 333.29 s
2024-12-07 04:44:14.017565: Yayy! New best EMA pseudo Dice: 0.6059
2024-12-07 04:44:15.871327: 
2024-12-07 04:44:15.872772: Epoch 9
2024-12-07 04:44:15.873563: Current learning rate: 0.00992
2024-12-07 04:49:52.062823: Validation loss improved from -0.39258 to -0.43462! Patience: 3/50
2024-12-07 04:49:52.063954: train_loss -0.442
2024-12-07 04:49:52.064872: val_loss -0.4346
2024-12-07 04:49:52.065655: Pseudo dice [0.7127]
2024-12-07 04:49:52.066493: Epoch time: 336.19 s
2024-12-07 04:49:52.509459: Yayy! New best EMA pseudo Dice: 0.6166
2024-12-07 04:49:54.257857: 
2024-12-07 04:49:54.259277: Epoch 10
2024-12-07 04:49:54.260168: Current learning rate: 0.00991
2024-12-07 04:55:51.088436: Validation loss did not improve from -0.43462. Patience: 1/50
2024-12-07 04:55:51.089900: train_loss -0.4433
2024-12-07 04:55:51.090798: val_loss -0.4253
2024-12-07 04:55:51.091548: Pseudo dice [0.7053]
2024-12-07 04:55:51.092297: Epoch time: 356.83 s
2024-12-07 04:55:51.093009: Yayy! New best EMA pseudo Dice: 0.6255
2024-12-07 04:55:52.872306: 
2024-12-07 04:55:52.873748: Epoch 11
2024-12-07 04:55:52.874739: Current learning rate: 0.0099
2024-12-07 05:01:43.595996: Validation loss improved from -0.43462 to -0.43865! Patience: 1/50
2024-12-07 05:01:43.596996: train_loss -0.4707
2024-12-07 05:01:43.597866: val_loss -0.4387
2024-12-07 05:01:43.598664: Pseudo dice [0.7066]
2024-12-07 05:01:43.599443: Epoch time: 350.73 s
2024-12-07 05:01:43.600203: Yayy! New best EMA pseudo Dice: 0.6336
2024-12-07 05:01:45.343436: 
2024-12-07 05:01:45.344681: Epoch 12
2024-12-07 05:01:45.345459: Current learning rate: 0.00989
2024-12-07 05:07:40.103706: Validation loss did not improve from -0.43865. Patience: 1/50
2024-12-07 05:07:40.104782: train_loss -0.4786
2024-12-07 05:07:40.105740: val_loss -0.4344
2024-12-07 05:07:40.106419: Pseudo dice [0.7073]
2024-12-07 05:07:40.107260: Epoch time: 354.76 s
2024-12-07 05:07:40.107986: Yayy! New best EMA pseudo Dice: 0.641
2024-12-07 05:07:41.863053: 
2024-12-07 05:07:41.864534: Epoch 13
2024-12-07 05:07:41.865386: Current learning rate: 0.00988
2024-12-07 05:13:31.504534: Validation loss did not improve from -0.43865. Patience: 2/50
2024-12-07 05:13:31.505626: train_loss -0.4898
2024-12-07 05:13:31.506437: val_loss -0.4265
2024-12-07 05:13:31.507346: Pseudo dice [0.7053]
2024-12-07 05:13:31.508313: Epoch time: 349.64 s
2024-12-07 05:13:31.509265: Yayy! New best EMA pseudo Dice: 0.6474
2024-12-07 05:13:33.301453: 
2024-12-07 05:13:33.302991: Epoch 14
2024-12-07 05:13:33.303969: Current learning rate: 0.00987
2024-12-07 05:19:37.962422: Validation loss improved from -0.43865 to -0.46033! Patience: 2/50
2024-12-07 05:19:37.963417: train_loss -0.4919
2024-12-07 05:19:37.964342: val_loss -0.4603
2024-12-07 05:19:37.965332: Pseudo dice [0.7231]
2024-12-07 05:19:37.966259: Epoch time: 364.66 s
2024-12-07 05:19:38.395663: Yayy! New best EMA pseudo Dice: 0.655
2024-12-07 05:19:40.202328: 
2024-12-07 05:19:40.203771: Epoch 15
2024-12-07 05:19:40.204719: Current learning rate: 0.00986
2024-12-07 05:25:34.362460: Validation loss did not improve from -0.46033. Patience: 1/50
2024-12-07 05:25:34.363504: train_loss -0.5064
2024-12-07 05:25:34.364203: val_loss -0.4448
2024-12-07 05:25:34.364800: Pseudo dice [0.7102]
2024-12-07 05:25:34.365464: Epoch time: 354.16 s
2024-12-07 05:25:34.366239: Yayy! New best EMA pseudo Dice: 0.6605
2024-12-07 05:25:36.116010: 
2024-12-07 05:25:36.117163: Epoch 16
2024-12-07 05:25:36.117900: Current learning rate: 0.00986
2024-12-07 05:31:30.693287: Validation loss did not improve from -0.46033. Patience: 2/50
2024-12-07 05:31:30.694227: train_loss -0.5133
2024-12-07 05:31:30.695121: val_loss -0.4297
2024-12-07 05:31:30.696081: Pseudo dice [0.6954]
2024-12-07 05:31:30.696921: Epoch time: 354.58 s
2024-12-07 05:31:30.697758: Yayy! New best EMA pseudo Dice: 0.664
2024-12-07 05:31:32.606904: 
2024-12-07 05:31:32.608707: Epoch 17
2024-12-07 05:31:32.610160: Current learning rate: 0.00985
2024-12-07 05:37:30.658016: Validation loss improved from -0.46033 to -0.46419! Patience: 2/50
2024-12-07 05:37:30.659030: train_loss -0.5193
2024-12-07 05:37:30.659976: val_loss -0.4642
2024-12-07 05:37:30.660730: Pseudo dice [0.727]
2024-12-07 05:37:30.661552: Epoch time: 358.05 s
2024-12-07 05:37:30.662314: Yayy! New best EMA pseudo Dice: 0.6703
2024-12-07 05:37:32.449573: 
2024-12-07 05:37:32.451119: Epoch 18
2024-12-07 05:37:32.451941: Current learning rate: 0.00984
2024-12-07 05:43:31.559127: Validation loss did not improve from -0.46419. Patience: 1/50
2024-12-07 05:43:31.560178: train_loss -0.5317
2024-12-07 05:43:31.560881: val_loss -0.3914
2024-12-07 05:43:31.561592: Pseudo dice [0.6811]
2024-12-07 05:43:31.562293: Epoch time: 359.11 s
2024-12-07 05:43:31.562956: Yayy! New best EMA pseudo Dice: 0.6714
2024-12-07 05:43:33.783453: 
2024-12-07 05:43:33.785023: Epoch 19
2024-12-07 05:43:33.785935: Current learning rate: 0.00983
2024-12-07 05:49:41.644633: Validation loss improved from -0.46419 to -0.47103! Patience: 1/50
2024-12-07 05:49:41.645722: train_loss -0.5236
2024-12-07 05:49:41.646727: val_loss -0.471
2024-12-07 05:49:41.647486: Pseudo dice [0.7382]
2024-12-07 05:49:41.648309: Epoch time: 367.86 s
2024-12-07 05:49:42.048067: Yayy! New best EMA pseudo Dice: 0.678
2024-12-07 05:49:43.865096: 
2024-12-07 05:49:43.866551: Epoch 20
2024-12-07 05:49:43.867420: Current learning rate: 0.00982
2024-12-07 05:55:53.942341: Validation loss did not improve from -0.47103. Patience: 1/50
2024-12-07 05:55:53.943429: train_loss -0.5383
2024-12-07 05:55:53.944618: val_loss -0.4542
2024-12-07 05:55:53.945612: Pseudo dice [0.7354]
2024-12-07 05:55:53.946629: Epoch time: 370.08 s
2024-12-07 05:55:53.947613: Yayy! New best EMA pseudo Dice: 0.6838
2024-12-07 05:55:55.747314: 
2024-12-07 05:55:55.748917: Epoch 21
2024-12-07 05:55:55.749906: Current learning rate: 0.00981
2024-12-07 06:02:01.038039: Validation loss did not improve from -0.47103. Patience: 2/50
2024-12-07 06:02:01.043512: train_loss -0.5417
2024-12-07 06:02:01.044764: val_loss -0.4614
2024-12-07 06:02:01.045599: Pseudo dice [0.7214]
2024-12-07 06:02:01.046867: Epoch time: 365.3 s
2024-12-07 06:02:01.048455: Yayy! New best EMA pseudo Dice: 0.6875
2024-12-07 06:02:02.764154: 
2024-12-07 06:02:02.765586: Epoch 22
2024-12-07 06:02:02.766338: Current learning rate: 0.0098
2024-12-07 06:08:00.185669: Validation loss did not improve from -0.47103. Patience: 3/50
2024-12-07 06:08:00.187134: train_loss -0.5479
2024-12-07 06:08:00.188817: val_loss -0.4659
2024-12-07 06:08:00.189718: Pseudo dice [0.7337]
2024-12-07 06:08:00.190652: Epoch time: 357.42 s
2024-12-07 06:08:00.195782: Yayy! New best EMA pseudo Dice: 0.6922
2024-12-07 06:08:01.944111: 
2024-12-07 06:08:01.945597: Epoch 23
2024-12-07 06:08:01.946451: Current learning rate: 0.00979
2024-12-07 06:13:55.481601: Validation loss improved from -0.47103 to -0.51007! Patience: 3/50
2024-12-07 06:13:55.482700: train_loss -0.5656
2024-12-07 06:13:55.483621: val_loss -0.5101
2024-12-07 06:13:55.484450: Pseudo dice [0.7495]
2024-12-07 06:13:55.485416: Epoch time: 353.54 s
2024-12-07 06:13:55.486188: Yayy! New best EMA pseudo Dice: 0.6979
2024-12-07 06:13:57.150441: 
2024-12-07 06:13:57.151879: Epoch 24
2024-12-07 06:13:57.152755: Current learning rate: 0.00978
2024-12-07 06:19:44.800590: Validation loss did not improve from -0.51007. Patience: 1/50
2024-12-07 06:19:44.801779: train_loss -0.556
2024-12-07 06:19:44.803024: val_loss -0.4843
2024-12-07 06:19:44.804023: Pseudo dice [0.7444]
2024-12-07 06:19:44.805021: Epoch time: 347.65 s
2024-12-07 06:19:45.204181: Yayy! New best EMA pseudo Dice: 0.7026
2024-12-07 06:19:46.938376: 
2024-12-07 06:19:46.939751: Epoch 25
2024-12-07 06:19:46.940623: Current learning rate: 0.00977
2024-12-07 06:25:48.938764: Validation loss did not improve from -0.51007. Patience: 2/50
2024-12-07 06:25:48.939850: train_loss -0.5438
2024-12-07 06:25:48.940634: val_loss -0.4923
2024-12-07 06:25:48.941295: Pseudo dice [0.7464]
2024-12-07 06:25:48.941930: Epoch time: 362.0 s
2024-12-07 06:25:48.942628: Yayy! New best EMA pseudo Dice: 0.7069
2024-12-07 06:25:50.732928: 
2024-12-07 06:25:50.734435: Epoch 26
2024-12-07 06:25:50.735637: Current learning rate: 0.00977
2024-12-07 06:32:09.163931: Validation loss did not improve from -0.51007. Patience: 3/50
2024-12-07 06:32:09.165040: train_loss -0.5625
2024-12-07 06:32:09.165825: val_loss -0.5016
2024-12-07 06:32:09.166510: Pseudo dice [0.7491]
2024-12-07 06:32:09.167413: Epoch time: 378.43 s
2024-12-07 06:32:09.168283: Yayy! New best EMA pseudo Dice: 0.7112
2024-12-07 06:32:10.835229: 
2024-12-07 06:32:10.836708: Epoch 27
2024-12-07 06:32:10.837850: Current learning rate: 0.00976
2024-12-07 06:38:39.090576: Validation loss did not improve from -0.51007. Patience: 4/50
2024-12-07 06:38:39.091694: train_loss -0.5583
2024-12-07 06:38:39.092695: val_loss -0.4957
2024-12-07 06:38:39.093390: Pseudo dice [0.7469]
2024-12-07 06:38:39.094102: Epoch time: 388.26 s
2024-12-07 06:38:39.094762: Yayy! New best EMA pseudo Dice: 0.7147
2024-12-07 06:38:40.804111: 
2024-12-07 06:38:40.805471: Epoch 28
2024-12-07 06:38:40.806184: Current learning rate: 0.00975
2024-12-07 06:45:17.074665: Validation loss did not improve from -0.51007. Patience: 5/50
2024-12-07 06:45:17.075838: train_loss -0.5826
2024-12-07 06:45:17.076754: val_loss -0.4976
2024-12-07 06:45:17.077590: Pseudo dice [0.7523]
2024-12-07 06:45:17.078275: Epoch time: 396.27 s
2024-12-07 06:45:17.078996: Yayy! New best EMA pseudo Dice: 0.7185
2024-12-07 06:45:19.206980: 
2024-12-07 06:45:19.208438: Epoch 29
2024-12-07 06:45:19.209141: Current learning rate: 0.00974
2024-12-07 06:51:56.094992: Validation loss did not improve from -0.51007. Patience: 6/50
2024-12-07 06:51:56.096066: train_loss -0.5769
2024-12-07 06:51:56.096827: val_loss -0.4615
2024-12-07 06:51:56.097547: Pseudo dice [0.7205]
2024-12-07 06:51:56.098355: Epoch time: 396.89 s
2024-12-07 06:51:56.533880: Yayy! New best EMA pseudo Dice: 0.7187
2024-12-07 06:51:58.299394: 
2024-12-07 06:51:58.300889: Epoch 30
2024-12-07 06:51:58.301593: Current learning rate: 0.00973
2024-12-07 06:58:32.170556: Validation loss did not improve from -0.51007. Patience: 7/50
2024-12-07 06:58:32.171461: train_loss -0.568
2024-12-07 06:58:32.172144: val_loss -0.5035
2024-12-07 06:58:32.172864: Pseudo dice [0.7488]
2024-12-07 06:58:32.173545: Epoch time: 393.87 s
2024-12-07 06:58:32.174302: Yayy! New best EMA pseudo Dice: 0.7217
2024-12-07 06:58:34.008532: 
2024-12-07 06:58:34.009832: Epoch 31
2024-12-07 06:58:34.010641: Current learning rate: 0.00972
2024-12-07 07:04:47.492285: Validation loss did not improve from -0.51007. Patience: 8/50
2024-12-07 07:04:47.493437: train_loss -0.5847
2024-12-07 07:04:47.494393: val_loss -0.4815
2024-12-07 07:04:47.495251: Pseudo dice [0.7348]
2024-12-07 07:04:47.496129: Epoch time: 373.49 s
2024-12-07 07:04:47.496833: Yayy! New best EMA pseudo Dice: 0.723
2024-12-07 07:04:49.264610: 
2024-12-07 07:04:49.265933: Epoch 32
2024-12-07 07:04:49.266912: Current learning rate: 0.00971
2024-12-07 07:11:13.211643: Validation loss improved from -0.51007 to -0.52744! Patience: 8/50
2024-12-07 07:11:13.212865: train_loss -0.5861
2024-12-07 07:11:13.213640: val_loss -0.5274
2024-12-07 07:11:13.214390: Pseudo dice [0.7634]
2024-12-07 07:11:13.215205: Epoch time: 383.95 s
2024-12-07 07:11:13.216016: Yayy! New best EMA pseudo Dice: 0.7271
2024-12-07 07:11:14.981556: 
2024-12-07 07:11:14.982897: Epoch 33
2024-12-07 07:11:14.983607: Current learning rate: 0.0097
2024-12-07 07:17:41.074140: Validation loss did not improve from -0.52744. Patience: 1/50
2024-12-07 07:17:41.075192: train_loss -0.5936
2024-12-07 07:17:41.075977: val_loss -0.4809
2024-12-07 07:17:41.076768: Pseudo dice [0.7372]
2024-12-07 07:17:41.077599: Epoch time: 386.09 s
2024-12-07 07:17:41.078339: Yayy! New best EMA pseudo Dice: 0.7281
2024-12-07 07:17:42.829584: 
2024-12-07 07:17:42.831118: Epoch 34
2024-12-07 07:17:42.832164: Current learning rate: 0.00969
2024-12-07 07:24:12.573611: Validation loss did not improve from -0.52744. Patience: 2/50
2024-12-07 07:24:12.574784: train_loss -0.593
2024-12-07 07:24:12.575743: val_loss -0.5004
2024-12-07 07:24:12.576592: Pseudo dice [0.7488]
2024-12-07 07:24:12.577484: Epoch time: 389.75 s
2024-12-07 07:24:13.051434: Yayy! New best EMA pseudo Dice: 0.7301
2024-12-07 07:24:14.847905: 
2024-12-07 07:24:14.849562: Epoch 35
2024-12-07 07:24:14.850636: Current learning rate: 0.00968
2024-12-07 07:30:44.545137: Validation loss did not improve from -0.52744. Patience: 3/50
2024-12-07 07:30:44.546130: train_loss -0.598
2024-12-07 07:30:44.547172: val_loss -0.5011
2024-12-07 07:30:44.548182: Pseudo dice [0.7528]
2024-12-07 07:30:44.549002: Epoch time: 389.7 s
2024-12-07 07:30:44.549877: Yayy! New best EMA pseudo Dice: 0.7324
2024-12-07 07:30:46.338106: 
2024-12-07 07:30:46.339674: Epoch 36
2024-12-07 07:30:46.341066: Current learning rate: 0.00968
2024-12-07 07:37:21.907250: Validation loss did not improve from -0.52744. Patience: 4/50
2024-12-07 07:37:21.908170: train_loss -0.5949
2024-12-07 07:37:21.909017: val_loss -0.4918
2024-12-07 07:37:21.909790: Pseudo dice [0.7393]
2024-12-07 07:37:21.910597: Epoch time: 395.57 s
2024-12-07 07:37:21.911283: Yayy! New best EMA pseudo Dice: 0.7331
2024-12-07 07:37:23.702544: 
2024-12-07 07:37:23.703937: Epoch 37
2024-12-07 07:37:23.704767: Current learning rate: 0.00967
2024-12-07 07:44:08.593841: Validation loss did not improve from -0.52744. Patience: 5/50
2024-12-07 07:44:08.594721: train_loss -0.5976
2024-12-07 07:44:08.595607: val_loss -0.5091
2024-12-07 07:44:08.596465: Pseudo dice [0.7507]
2024-12-07 07:44:08.597182: Epoch time: 404.89 s
2024-12-07 07:44:08.598014: Yayy! New best EMA pseudo Dice: 0.7348
2024-12-07 07:44:10.410685: 
2024-12-07 07:44:10.412011: Epoch 38
2024-12-07 07:44:10.412829: Current learning rate: 0.00966
2024-12-07 07:50:42.855798: Validation loss did not improve from -0.52744. Patience: 6/50
2024-12-07 07:50:42.857468: train_loss -0.5988
2024-12-07 07:50:42.858366: val_loss -0.519
2024-12-07 07:50:42.859185: Pseudo dice [0.7636]
2024-12-07 07:50:42.860101: Epoch time: 392.45 s
2024-12-07 07:50:42.861000: Yayy! New best EMA pseudo Dice: 0.7377
2024-12-07 07:50:45.031334: 
2024-12-07 07:50:45.032768: Epoch 39
2024-12-07 07:50:45.033734: Current learning rate: 0.00965
2024-12-07 07:57:17.990230: Validation loss did not improve from -0.52744. Patience: 7/50
2024-12-07 07:57:17.991251: train_loss -0.6021
2024-12-07 07:57:17.992126: val_loss -0.4467
2024-12-07 07:57:17.992739: Pseudo dice [0.7163]
2024-12-07 07:57:17.993592: Epoch time: 392.96 s
2024-12-07 07:57:19.871762: 
2024-12-07 07:57:19.873445: Epoch 40
2024-12-07 07:57:19.874543: Current learning rate: 0.00964
2024-12-07 08:03:53.925161: Validation loss did not improve from -0.52744. Patience: 8/50
2024-12-07 08:03:53.926880: train_loss -0.6134
2024-12-07 08:03:53.927775: val_loss -0.4515
2024-12-07 08:03:53.928735: Pseudo dice [0.721]
2024-12-07 08:03:53.929482: Epoch time: 394.06 s
2024-12-07 08:03:55.471517: 
2024-12-07 08:03:55.472945: Epoch 41
2024-12-07 08:03:55.473775: Current learning rate: 0.00963
2024-12-07 08:10:14.573879: Validation loss did not improve from -0.52744. Patience: 9/50
2024-12-07 08:10:14.575675: train_loss -0.6224
2024-12-07 08:10:14.576654: val_loss -0.4753
2024-12-07 08:10:14.577367: Pseudo dice [0.7415]
2024-12-07 08:10:14.577990: Epoch time: 379.11 s
2024-12-07 08:10:15.905420: 
2024-12-07 08:10:15.906954: Epoch 42
2024-12-07 08:10:15.907689: Current learning rate: 0.00962
2024-12-07 08:16:27.766873: Validation loss did not improve from -0.52744. Patience: 10/50
2024-12-07 08:16:27.768073: train_loss -0.6194
2024-12-07 08:16:27.769054: val_loss -0.5019
2024-12-07 08:16:27.769968: Pseudo dice [0.7553]
2024-12-07 08:16:27.770875: Epoch time: 371.86 s
2024-12-07 08:16:29.130176: 
2024-12-07 08:16:29.131750: Epoch 43
2024-12-07 08:16:29.132674: Current learning rate: 0.00961
2024-12-07 08:22:29.611260: Validation loss did not improve from -0.52744. Patience: 11/50
2024-12-07 08:22:29.612205: train_loss -0.6137
2024-12-07 08:22:29.613296: val_loss -0.4953
2024-12-07 08:22:29.614545: Pseudo dice [0.7474]
2024-12-07 08:22:29.615659: Epoch time: 360.48 s
2024-12-07 08:22:29.616598: Yayy! New best EMA pseudo Dice: 0.738
2024-12-07 08:22:31.380924: 
2024-12-07 08:22:31.382333: Epoch 44
2024-12-07 08:22:31.383280: Current learning rate: 0.0096
2024-12-07 08:28:38.586425: Validation loss improved from -0.52744 to -0.54003! Patience: 11/50
2024-12-07 08:28:38.587575: train_loss -0.6163
2024-12-07 08:28:38.588457: val_loss -0.54
2024-12-07 08:28:38.589310: Pseudo dice [0.7776]
2024-12-07 08:28:38.590100: Epoch time: 367.21 s
2024-12-07 08:28:38.997760: Yayy! New best EMA pseudo Dice: 0.7419
2024-12-07 08:28:40.723309: 
2024-12-07 08:28:40.724663: Epoch 45
2024-12-07 08:28:40.725536: Current learning rate: 0.00959
2024-12-07 08:35:02.931104: Validation loss did not improve from -0.54003. Patience: 1/50
2024-12-07 08:35:02.932231: train_loss -0.6227
2024-12-07 08:35:02.933157: val_loss -0.487
2024-12-07 08:35:02.933899: Pseudo dice [0.7401]
2024-12-07 08:35:02.934657: Epoch time: 382.21 s
2024-12-07 08:35:04.252739: 
2024-12-07 08:35:04.254256: Epoch 46
2024-12-07 08:35:04.255071: Current learning rate: 0.00959
2024-12-07 08:41:15.828802: Validation loss did not improve from -0.54003. Patience: 2/50
2024-12-07 08:41:15.829895: train_loss -0.6269
2024-12-07 08:41:15.830836: val_loss -0.5252
2024-12-07 08:41:15.831492: Pseudo dice [0.7638]
2024-12-07 08:41:15.832155: Epoch time: 371.58 s
2024-12-07 08:41:15.833140: Yayy! New best EMA pseudo Dice: 0.744
2024-12-07 08:41:17.601036: 
2024-12-07 08:41:17.602389: Epoch 47
2024-12-07 08:41:17.603228: Current learning rate: 0.00958
2024-12-07 08:47:19.212786: Validation loss did not improve from -0.54003. Patience: 3/50
2024-12-07 08:47:19.213702: train_loss -0.6253
2024-12-07 08:47:19.214526: val_loss -0.5363
2024-12-07 08:47:19.215150: Pseudo dice [0.7703]
2024-12-07 08:47:19.215949: Epoch time: 361.61 s
2024-12-07 08:47:19.216700: Yayy! New best EMA pseudo Dice: 0.7466
2024-12-07 08:47:20.897878: 
2024-12-07 08:47:20.899227: Epoch 48
2024-12-07 08:47:20.899974: Current learning rate: 0.00957
2024-12-07 08:53:33.594342: Validation loss did not improve from -0.54003. Patience: 4/50
2024-12-07 08:53:33.595896: train_loss -0.6326
2024-12-07 08:53:33.597174: val_loss -0.4944
2024-12-07 08:53:33.598044: Pseudo dice [0.7465]
2024-12-07 08:53:33.598712: Epoch time: 372.7 s
2024-12-07 08:53:34.933464: 
2024-12-07 08:53:34.934757: Epoch 49
2024-12-07 08:53:34.935436: Current learning rate: 0.00956
2024-12-07 09:00:04.897189: Validation loss did not improve from -0.54003. Patience: 5/50
2024-12-07 09:00:04.898127: train_loss -0.6221
2024-12-07 09:00:04.898898: val_loss -0.5247
2024-12-07 09:00:04.899600: Pseudo dice [0.7591]
2024-12-07 09:00:04.900354: Epoch time: 389.97 s
2024-12-07 09:00:05.299659: Yayy! New best EMA pseudo Dice: 0.7478
2024-12-07 09:00:08.224837: 
2024-12-07 09:00:08.226159: Epoch 50
2024-12-07 09:00:08.226919: Current learning rate: 0.00955
2024-12-07 09:06:37.176932: Validation loss did not improve from -0.54003. Patience: 6/50
2024-12-07 09:06:37.177952: train_loss -0.6236
2024-12-07 09:06:37.178838: val_loss -0.4626
2024-12-07 09:06:37.179645: Pseudo dice [0.7311]
2024-12-07 09:06:37.180432: Epoch time: 388.95 s
2024-12-07 09:06:38.548559: 
2024-12-07 09:06:38.549879: Epoch 51
2024-12-07 09:06:38.550586: Current learning rate: 0.00954
2024-12-07 09:12:55.075122: Validation loss did not improve from -0.54003. Patience: 7/50
2024-12-07 09:12:55.076068: train_loss -0.6388
2024-12-07 09:12:55.076983: val_loss -0.5148
2024-12-07 09:12:55.077683: Pseudo dice [0.7539]
2024-12-07 09:12:55.078419: Epoch time: 376.53 s
2024-12-07 09:12:56.426809: 
2024-12-07 09:12:56.428315: Epoch 52
2024-12-07 09:12:56.429359: Current learning rate: 0.00953
2024-12-07 09:19:23.047750: Validation loss did not improve from -0.54003. Patience: 8/50
2024-12-07 09:19:23.052412: train_loss -0.6386
2024-12-07 09:19:23.053868: val_loss -0.5185
2024-12-07 09:19:23.054893: Pseudo dice [0.764]
2024-12-07 09:19:23.056010: Epoch time: 386.63 s
2024-12-07 09:19:23.057326: Yayy! New best EMA pseudo Dice: 0.7486
2024-12-07 09:19:24.794045: 
2024-12-07 09:19:24.795611: Epoch 53
2024-12-07 09:19:24.796836: Current learning rate: 0.00952
2024-12-07 09:25:26.822046: Validation loss improved from -0.54003 to -0.54110! Patience: 8/50
2024-12-07 09:25:26.823127: train_loss -0.6388
2024-12-07 09:25:26.824713: val_loss -0.5411
2024-12-07 09:25:26.825537: Pseudo dice [0.7649]
2024-12-07 09:25:26.826373: Epoch time: 362.03 s
2024-12-07 09:25:26.827039: Yayy! New best EMA pseudo Dice: 0.7503
2024-12-07 09:25:28.542651: 
2024-12-07 09:25:28.544012: Epoch 54
2024-12-07 09:25:28.544852: Current learning rate: 0.00951
2024-12-07 09:31:42.948340: Validation loss did not improve from -0.54110. Patience: 1/50
2024-12-07 09:31:42.949358: train_loss -0.6406
2024-12-07 09:31:42.950245: val_loss -0.4653
2024-12-07 09:31:42.951017: Pseudo dice [0.7283]
2024-12-07 09:31:42.951794: Epoch time: 374.41 s
2024-12-07 09:31:44.696187: 
2024-12-07 09:31:44.697502: Epoch 55
2024-12-07 09:31:44.698359: Current learning rate: 0.0095
2024-12-07 09:37:45.350850: Validation loss did not improve from -0.54110. Patience: 2/50
2024-12-07 09:37:45.351935: train_loss -0.6388
2024-12-07 09:37:45.353061: val_loss -0.541
2024-12-07 09:37:45.354329: Pseudo dice [0.7738]
2024-12-07 09:37:45.355380: Epoch time: 360.66 s
2024-12-07 09:37:45.356285: Yayy! New best EMA pseudo Dice: 0.7506
2024-12-07 09:37:47.107426: 
2024-12-07 09:37:47.108861: Epoch 56
2024-12-07 09:37:47.109780: Current learning rate: 0.00949
2024-12-07 09:43:42.366495: Validation loss did not improve from -0.54110. Patience: 3/50
2024-12-07 09:43:42.367582: train_loss -0.6475
2024-12-07 09:43:42.368750: val_loss -0.4773
2024-12-07 09:43:42.369655: Pseudo dice [0.7349]
2024-12-07 09:43:42.370402: Epoch time: 355.26 s
2024-12-07 09:43:43.702748: 
2024-12-07 09:43:43.704183: Epoch 57
2024-12-07 09:43:43.705000: Current learning rate: 0.00949
2024-12-07 09:49:40.423819: Validation loss did not improve from -0.54110. Patience: 4/50
2024-12-07 09:49:40.424909: train_loss -0.6455
2024-12-07 09:49:40.425833: val_loss -0.4771
2024-12-07 09:49:40.426620: Pseudo dice [0.7399]
2024-12-07 09:49:40.427437: Epoch time: 356.72 s
2024-12-07 09:49:41.833453: 
2024-12-07 09:49:41.834981: Epoch 58
2024-12-07 09:49:41.836214: Current learning rate: 0.00948
2024-12-07 09:55:32.619590: Validation loss did not improve from -0.54110. Patience: 5/50
2024-12-07 09:55:32.620745: train_loss -0.6595
2024-12-07 09:55:32.621729: val_loss -0.4686
2024-12-07 09:55:32.622539: Pseudo dice [0.7325]
2024-12-07 09:55:32.623239: Epoch time: 350.79 s
2024-12-07 09:55:34.011661: 
2024-12-07 09:55:34.013092: Epoch 59
2024-12-07 09:55:34.013841: Current learning rate: 0.00947
2024-12-07 10:01:19.738805: Validation loss did not improve from -0.54110. Patience: 6/50
2024-12-07 10:01:19.766858: train_loss -0.6555
2024-12-07 10:01:19.767890: val_loss -0.5334
2024-12-07 10:01:19.768648: Pseudo dice [0.7637]
2024-12-07 10:01:19.769448: Epoch time: 345.76 s
2024-12-07 10:01:21.590484: 
2024-12-07 10:01:21.591852: Epoch 60
2024-12-07 10:01:21.592677: Current learning rate: 0.00946
2024-12-07 10:06:37.348901: Validation loss did not improve from -0.54110. Patience: 7/50
2024-12-07 10:06:37.349953: train_loss -0.6619
2024-12-07 10:06:37.350711: val_loss -0.5391
2024-12-07 10:06:37.351347: Pseudo dice [0.7737]
2024-12-07 10:06:37.352050: Epoch time: 315.76 s
2024-12-07 10:06:37.352685: Yayy! New best EMA pseudo Dice: 0.7508
2024-12-07 10:06:39.490589: 
2024-12-07 10:06:39.492149: Epoch 61
2024-12-07 10:06:39.493127: Current learning rate: 0.00945
2024-12-07 10:11:43.724784: Validation loss did not improve from -0.54110. Patience: 8/50
2024-12-07 10:11:43.725697: train_loss -0.6514
2024-12-07 10:11:43.726458: val_loss -0.539
2024-12-07 10:11:43.727027: Pseudo dice [0.767]
2024-12-07 10:11:43.727666: Epoch time: 304.24 s
2024-12-07 10:11:43.728337: Yayy! New best EMA pseudo Dice: 0.7525
2024-12-07 10:11:45.486452: 
2024-12-07 10:11:45.487836: Epoch 62
2024-12-07 10:11:45.488578: Current learning rate: 0.00944
2024-12-07 10:17:03.239911: Validation loss did not improve from -0.54110. Patience: 9/50
2024-12-07 10:17:03.240979: train_loss -0.6591
2024-12-07 10:17:03.241766: val_loss -0.5186
2024-12-07 10:17:03.242376: Pseudo dice [0.7633]
2024-12-07 10:17:03.243077: Epoch time: 317.76 s
2024-12-07 10:17:03.243806: Yayy! New best EMA pseudo Dice: 0.7535
2024-12-07 10:17:05.030336: 
2024-12-07 10:17:05.031675: Epoch 63
2024-12-07 10:17:05.032695: Current learning rate: 0.00943
2024-12-07 10:22:27.435741: Validation loss did not improve from -0.54110. Patience: 10/50
2024-12-07 10:22:27.438780: train_loss -0.6691
2024-12-07 10:22:27.439945: val_loss -0.4982
2024-12-07 10:22:27.441075: Pseudo dice [0.7526]
2024-12-07 10:22:27.442193: Epoch time: 322.41 s
2024-12-07 10:22:28.836195: 
2024-12-07 10:22:28.837127: Epoch 64
2024-12-07 10:22:28.837866: Current learning rate: 0.00942
2024-12-07 10:28:16.617057: Validation loss did not improve from -0.54110. Patience: 11/50
2024-12-07 10:28:16.618140: train_loss -0.6566
2024-12-07 10:28:16.620100: val_loss -0.5209
2024-12-07 10:28:16.621131: Pseudo dice [0.7615]
2024-12-07 10:28:16.622132: Epoch time: 347.78 s
2024-12-07 10:28:17.040057: Yayy! New best EMA pseudo Dice: 0.7542
2024-12-07 10:28:18.785156: 
2024-12-07 10:28:18.786654: Epoch 65
2024-12-07 10:28:18.787555: Current learning rate: 0.00941
2024-12-07 10:33:23.313458: Validation loss did not improve from -0.54110. Patience: 12/50
2024-12-07 10:33:23.314461: train_loss -0.6617
2024-12-07 10:33:23.315577: val_loss -0.4842
2024-12-07 10:33:23.316455: Pseudo dice [0.7331]
2024-12-07 10:33:23.317422: Epoch time: 304.53 s
2024-12-07 10:33:24.708639: 
2024-12-07 10:33:24.709935: Epoch 66
2024-12-07 10:33:24.710842: Current learning rate: 0.0094
2024-12-07 10:38:49.420644: Validation loss did not improve from -0.54110. Patience: 13/50
2024-12-07 10:38:49.421690: train_loss -0.6452
2024-12-07 10:38:49.422625: val_loss -0.4947
2024-12-07 10:38:49.423622: Pseudo dice [0.745]
2024-12-07 10:38:49.424591: Epoch time: 324.71 s
2024-12-07 10:38:50.826315: 
2024-12-07 10:38:50.827855: Epoch 67
2024-12-07 10:38:50.828948: Current learning rate: 0.00939
2024-12-07 10:44:13.308116: Validation loss did not improve from -0.54110. Patience: 14/50
2024-12-07 10:44:13.309231: train_loss -0.6656
2024-12-07 10:44:13.310005: val_loss -0.5331
2024-12-07 10:44:13.310606: Pseudo dice [0.7696]
2024-12-07 10:44:13.311249: Epoch time: 322.48 s
2024-12-07 10:44:14.770511: 
2024-12-07 10:44:14.771786: Epoch 68
2024-12-07 10:44:14.772572: Current learning rate: 0.00939
2024-12-07 10:49:36.385532: Validation loss did not improve from -0.54110. Patience: 15/50
2024-12-07 10:49:36.386596: train_loss -0.6686
2024-12-07 10:49:36.387501: val_loss -0.5057
2024-12-07 10:49:36.388484: Pseudo dice [0.756]
2024-12-07 10:49:36.389302: Epoch time: 321.62 s
2024-12-07 10:49:37.802014: 
2024-12-07 10:49:37.803345: Epoch 69
2024-12-07 10:49:37.804175: Current learning rate: 0.00938
2024-12-07 10:55:28.933511: Validation loss did not improve from -0.54110. Patience: 16/50
2024-12-07 10:55:28.934662: train_loss -0.6739
2024-12-07 10:55:28.935626: val_loss -0.5162
2024-12-07 10:55:28.936395: Pseudo dice [0.7568]
2024-12-07 10:55:28.937202: Epoch time: 351.13 s
2024-12-07 10:55:30.721623: 
2024-12-07 10:55:30.723031: Epoch 70
2024-12-07 10:55:30.724296: Current learning rate: 0.00937
2024-12-07 11:00:31.466789: Validation loss did not improve from -0.54110. Patience: 17/50
2024-12-07 11:00:31.467839: train_loss -0.6737
2024-12-07 11:00:31.468552: val_loss -0.4408
2024-12-07 11:00:31.469281: Pseudo dice [0.7257]
2024-12-07 11:00:31.470075: Epoch time: 300.75 s
2024-12-07 11:00:33.277682: 
2024-12-07 11:00:33.279105: Epoch 71
2024-12-07 11:00:33.279887: Current learning rate: 0.00936
2024-12-07 11:06:01.171422: Validation loss did not improve from -0.54110. Patience: 18/50
2024-12-07 11:06:01.173047: train_loss -0.6764
2024-12-07 11:06:01.173969: val_loss -0.4522
2024-12-07 11:06:01.174749: Pseudo dice [0.7168]
2024-12-07 11:06:01.175536: Epoch time: 327.9 s
2024-12-07 11:06:02.733558: 
2024-12-07 11:06:02.735134: Epoch 72
2024-12-07 11:06:02.736024: Current learning rate: 0.00935
2024-12-07 11:11:26.462492: Validation loss improved from -0.54110 to -0.56458! Patience: 18/50
2024-12-07 11:11:26.463809: train_loss -0.672
2024-12-07 11:11:26.464727: val_loss -0.5646
2024-12-07 11:11:26.465530: Pseudo dice [0.7843]
2024-12-07 11:11:26.466419: Epoch time: 323.73 s
2024-12-07 11:11:27.907537: 
2024-12-07 11:11:27.908954: Epoch 73
2024-12-07 11:11:27.909739: Current learning rate: 0.00934
2024-12-07 11:16:58.466889: Validation loss did not improve from -0.56458. Patience: 1/50
2024-12-07 11:16:58.468024: train_loss -0.6768
2024-12-07 11:16:58.468978: val_loss -0.5449
2024-12-07 11:16:58.469734: Pseudo dice [0.776]
2024-12-07 11:16:58.470564: Epoch time: 330.56 s
2024-12-07 11:16:59.887405: 
2024-12-07 11:16:59.888790: Epoch 74
2024-12-07 11:16:59.889702: Current learning rate: 0.00933
2024-12-07 11:22:30.839555: Validation loss did not improve from -0.56458. Patience: 2/50
2024-12-07 11:22:30.840540: train_loss -0.6783
2024-12-07 11:22:30.841389: val_loss -0.4624
2024-12-07 11:22:30.842113: Pseudo dice [0.7303]
2024-12-07 11:22:30.842915: Epoch time: 330.95 s
2024-12-07 11:22:32.699046: 
2024-12-07 11:22:32.700756: Epoch 75
2024-12-07 11:22:32.701687: Current learning rate: 0.00932
2024-12-07 11:27:39.756504: Validation loss did not improve from -0.56458. Patience: 3/50
2024-12-07 11:27:39.757688: train_loss -0.676
2024-12-07 11:27:39.758490: val_loss -0.4974
2024-12-07 11:27:39.759199: Pseudo dice [0.7535]
2024-12-07 11:27:39.759969: Epoch time: 307.06 s
2024-12-07 11:27:41.236731: 
2024-12-07 11:27:41.238029: Epoch 76
2024-12-07 11:27:41.238982: Current learning rate: 0.00931
2024-12-07 11:33:12.185001: Validation loss did not improve from -0.56458. Patience: 4/50
2024-12-07 11:33:12.185931: train_loss -0.6845
2024-12-07 11:33:12.188226: val_loss -0.4878
2024-12-07 11:33:12.189299: Pseudo dice [0.7425]
2024-12-07 11:33:12.190801: Epoch time: 330.95 s
2024-12-07 11:33:13.637570: 
2024-12-07 11:33:13.638980: Epoch 77
2024-12-07 11:33:13.639867: Current learning rate: 0.0093
2024-12-07 11:38:45.568376: Validation loss did not improve from -0.56458. Patience: 5/50
2024-12-07 11:38:45.569344: train_loss -0.6824
2024-12-07 11:38:45.570126: val_loss -0.5163
2024-12-07 11:38:45.570826: Pseudo dice [0.7604]
2024-12-07 11:38:45.571575: Epoch time: 331.93 s
2024-12-07 11:38:47.099976: 
2024-12-07 11:38:47.101353: Epoch 78
2024-12-07 11:38:47.102318: Current learning rate: 0.0093
2024-12-07 11:44:31.031373: Validation loss did not improve from -0.56458. Patience: 6/50
2024-12-07 11:44:31.032394: train_loss -0.6797
2024-12-07 11:44:31.033185: val_loss -0.513
2024-12-07 11:44:31.033890: Pseudo dice [0.7582]
2024-12-07 11:44:31.034628: Epoch time: 343.93 s
2024-12-07 11:44:32.493813: 
2024-12-07 11:44:32.495190: Epoch 79
2024-12-07 11:44:32.495904: Current learning rate: 0.00929
2024-12-07 11:50:22.931639: Validation loss did not improve from -0.56458. Patience: 7/50
2024-12-07 11:50:22.932492: train_loss -0.6805
2024-12-07 11:50:22.933373: val_loss -0.5586
2024-12-07 11:50:22.934241: Pseudo dice [0.7811]
2024-12-07 11:50:22.935031: Epoch time: 350.44 s
2024-12-07 11:50:23.365786: Yayy! New best EMA pseudo Dice: 0.7552
2024-12-07 11:50:25.178397: 
2024-12-07 11:50:25.179815: Epoch 80
2024-12-07 11:50:25.180655: Current learning rate: 0.00928
2024-12-07 11:55:22.091611: Validation loss did not improve from -0.56458. Patience: 8/50
2024-12-07 11:55:22.092556: train_loss -0.688
2024-12-07 11:55:22.093686: val_loss -0.5145
2024-12-07 11:55:22.094778: Pseudo dice [0.7582]
2024-12-07 11:55:22.095605: Epoch time: 296.92 s
2024-12-07 11:55:22.096450: Yayy! New best EMA pseudo Dice: 0.7555
2024-12-07 11:55:23.958573: 
2024-12-07 11:55:23.969900: Epoch 81
2024-12-07 11:55:23.970969: Current learning rate: 0.00927
2024-12-07 12:00:51.744032: Validation loss did not improve from -0.56458. Patience: 9/50
2024-12-07 12:00:51.745015: train_loss -0.6896
2024-12-07 12:00:51.745898: val_loss -0.4968
2024-12-07 12:00:51.747019: Pseudo dice [0.7497]
2024-12-07 12:00:51.748014: Epoch time: 327.79 s
2024-12-07 12:00:53.585748: 
2024-12-07 12:00:53.586716: Epoch 82
2024-12-07 12:00:53.587494: Current learning rate: 0.00926
2024-12-07 12:06:20.016567: Validation loss did not improve from -0.56458. Patience: 10/50
2024-12-07 12:06:20.017645: train_loss -0.6914
2024-12-07 12:06:20.018507: val_loss -0.5118
2024-12-07 12:06:20.019322: Pseudo dice [0.7561]
2024-12-07 12:06:20.020106: Epoch time: 326.43 s
2024-12-07 12:06:21.369716: 
2024-12-07 12:06:21.371229: Epoch 83
2024-12-07 12:06:21.372184: Current learning rate: 0.00925
2024-12-07 12:11:29.621233: Validation loss did not improve from -0.56458. Patience: 11/50
2024-12-07 12:11:29.622913: train_loss -0.6873
2024-12-07 12:11:29.623922: val_loss -0.4728
2024-12-07 12:11:29.625009: Pseudo dice [0.7406]
2024-12-07 12:11:29.625991: Epoch time: 308.25 s
2024-12-07 12:11:30.999292: 
2024-12-07 12:11:31.000836: Epoch 84
2024-12-07 12:11:31.001919: Current learning rate: 0.00924
2024-12-07 12:16:45.851837: failed to log:  (<class 'FileNotFoundError'>, FileNotFoundError(2, 'No such file or directory'), <traceback object at 0x7f03800c7940>)
2024-12-07 12:16:45.851837: failed to log:  (<class 'FileNotFoundError'>, FileNotFoundError(2, 'No such file or directory'), <traceback object at 0x7f03800c7940>)
2024-12-07 12:16:45.851837: failed to log:  (<class 'FileNotFoundError'>, FileNotFoundError(2, 'No such file or directory'), <traceback object at 0x7f03800c7940>)
2024-12-07 12:16:45.851837: failed to log:  (<class 'FileNotFoundError'>, FileNotFoundError(2, 'No such file or directory'), <traceback object at 0x7f03800c7940>)
2024-12-07 12:16:45.851837: failed to log:  (<class 'FileNotFoundError'>, FileNotFoundError(2, 'No such file or directory'), <traceback object at 0x7f03800c7940>)
2024-12-07 12:16:45.851837: Validation loss did not improve from -0.56458. Patience: 12/50
2024-12-07 12:16:48.355176: failed to log:  (<class 'FileNotFoundError'>, FileNotFoundError(2, 'No such file or directory'), <traceback object at 0x7f0338408600>)
2024-12-07 12:16:48.355176: failed to log:  (<class 'FileNotFoundError'>, FileNotFoundError(2, 'No such file or directory'), <traceback object at 0x7f0338408600>)
2024-12-07 12:16:48.355176: failed to log:  (<class 'FileNotFoundError'>, FileNotFoundError(2, 'No such file or directory'), <traceback object at 0x7f0338408600>)
2024-12-07 12:16:48.355176: failed to log:  (<class 'FileNotFoundError'>, FileNotFoundError(2, 'No such file or directory'), <traceback object at 0x7f0338408600>)
2024-12-07 12:16:48.355176: failed to log:  (<class 'FileNotFoundError'>, FileNotFoundError(2, 'No such file or directory'), <traceback object at 0x7f0338408600>)
2024-12-07 12:16:48.355176: train_loss -0.6897
2024-12-07 12:16:50.858449: failed to log:  (<class 'FileNotFoundError'>, FileNotFoundError(2, 'No such file or directory'), <traceback object at 0x7f036dff1b40>)
2024-12-07 12:16:50.858449: failed to log:  (<class 'FileNotFoundError'>, FileNotFoundError(2, 'No such file or directory'), <traceback object at 0x7f036dff1b40>)
2024-12-07 12:16:50.858449: failed to log:  (<class 'FileNotFoundError'>, FileNotFoundError(2, 'No such file or directory'), <traceback object at 0x7f036dff1b40>)
2024-12-07 12:16:50.858449: failed to log:  (<class 'FileNotFoundError'>, FileNotFoundError(2, 'No such file or directory'), <traceback object at 0x7f036dff1b40>)
2024-12-07 12:16:50.858449: failed to log:  (<class 'FileNotFoundError'>, FileNotFoundError(2, 'No such file or directory'), <traceback object at 0x7f036dff1b40>)
2024-12-07 12:16:50.858449: val_loss -0.5212
2024-12-07 12:16:53.361692: failed to log:  (<class 'FileNotFoundError'>, FileNotFoundError(2, 'No such file or directory'), <traceback object at 0x7f036be07dc0>)
2024-12-07 12:16:53.361692: failed to log:  (<class 'FileNotFoundError'>, FileNotFoundError(2, 'No such file or directory'), <traceback object at 0x7f036be07dc0>)
2024-12-07 12:16:53.361692: failed to log:  (<class 'FileNotFoundError'>, FileNotFoundError(2, 'No such file or directory'), <traceback object at 0x7f036be07dc0>)
2024-12-07 12:16:53.361692: failed to log:  (<class 'FileNotFoundError'>, FileNotFoundError(2, 'No such file or directory'), <traceback object at 0x7f036be07dc0>)
2024-12-07 12:16:53.361692: failed to log:  (<class 'FileNotFoundError'>, FileNotFoundError(2, 'No such file or directory'), <traceback object at 0x7f036be07dc0>)
2024-12-07 12:16:53.361692: Pseudo dice [0.7591]
2024-12-07 12:16:55.865053: failed to log:  (<class 'FileNotFoundError'>, FileNotFoundError(2, 'No such file or directory'), <traceback object at 0x7f036be244c0>)
2024-12-07 12:16:55.865053: failed to log:  (<class 'FileNotFoundError'>, FileNotFoundError(2, 'No such file or directory'), <traceback object at 0x7f036be244c0>)
2024-12-07 12:16:55.865053: failed to log:  (<class 'FileNotFoundError'>, FileNotFoundError(2, 'No such file or directory'), <traceback object at 0x7f036be244c0>)
2024-12-07 12:16:55.865053: failed to log:  (<class 'FileNotFoundError'>, FileNotFoundError(2, 'No such file or directory'), <traceback object at 0x7f036be244c0>)
2024-12-07 12:16:55.865053: failed to log:  (<class 'FileNotFoundError'>, FileNotFoundError(2, 'No such file or directory'), <traceback object at 0x7f036be244c0>)
2024-12-07 12:16:55.865053: Epoch time: 317.36 s
Traceback (most recent call last):
  File "/home/gridsan/nchutisilp/.conda/envs/nnunet/bin/nnUNetv2_train", line 8, in <module>
    sys.exit(run_training_entry())
  File "/home/gridsan/nchutisilp/projects/nnUNet/nnunetv2/run/run_training.py", line 275, in run_training_entry
    run_training(args.dataset_name_or_id, args.configuration, args.fold, args.tr, args.p, args.pretrained_weights,
  File "/home/gridsan/nchutisilp/projects/nnUNet/nnunetv2/run/run_training.py", line 211, in run_training
    nnunet_trainer.run_training()
  File "/home/gridsan/nchutisilp/projects/nnUNet/nnunetv2/training/nnUNetTrainer/nnUNetTrainer.py", line 1411, in run_training
    self.on_epoch_end()
  File "/home/gridsan/nchutisilp/projects/nnUNet/nnunetv2/training/nnUNetTrainer/nnUNetTrainer.py", line 1165, in on_epoch_end
    self.save_checkpoint(join(self.output_folder, 'checkpoint_latest.pth'))
  File "/home/gridsan/nchutisilp/projects/nnUNet/nnunetv2/training/nnUNetTrainer/nnUNetTrainer.py", line 1201, in save_checkpoint
    torch.save(checkpoint, filename)
  File "/home/gridsan/nchutisilp/.local/lib/python3.9/site-packages/torch/serialization.py", line 627, in save
    with _open_zipfile_writer(f) as opened_zipfile:
  File "/home/gridsan/nchutisilp/.local/lib/python3.9/site-packages/torch/serialization.py", line 501, in _open_zipfile_writer
    return container(name_or_buffer)
  File "/home/gridsan/nchutisilp/.local/lib/python3.9/site-packages/torch/serialization.py", line 472, in __init__
    super().__init__(torch._C.PyTorchFileWriter(self.name))
RuntimeError: Parent directory /home/gridsan/nchutisilp/datasets/nnUNet_Datasets/nnUNet_results/Dataset307_Sohee_Calcium_OCT_CrossValidation/nnUNetTrainer__nnUNetPlans__3d_32x160x128_b10/fold_0 does not exist.
Exception in thread Thread-2:
Traceback (most recent call last):
  File "/home/gridsan/nchutisilp/.conda/envs/nnunet/lib/python3.9/threading.py", line 980, in _bootstrap_inner
    self.run()
  File "/home/gridsan/nchutisilp/.conda/envs/nnunet/lib/python3.9/threading.py", line 917, in run
    self._target(*self._args, **self._kwargs)
  File "/home/gridsan/nchutisilp/.local/lib/python3.9/site-packages/batchgenerators/dataloading/nondet_multi_threaded_augmenter.py", line 125, in results_loop
    raise e
  File "/home/gridsan/nchutisilp/.local/lib/python3.9/site-packages/batchgenerators/dataloading/nondet_multi_threaded_augmenter.py", line 103, in results_loop
    raise RuntimeError("One or more background workers are no longer alive. Exiting. Please check the "
RuntimeError: One or more background workers are no longer alive. Exiting. Please check the print statements above for the actual error message
Exception in thread Thread-3:
Traceback (most recent call last):
  File "/home/gridsan/nchutisilp/.conda/envs/nnunet/lib/python3.9/threading.py", line 980, in _bootstrap_inner
    self.run()
  File "/home/gridsan/nchutisilp/.conda/envs/nnunet/lib/python3.9/threading.py", line 917, in run
    self._target(*self._args, **self._kwargs)
  File "/home/gridsan/nchutisilp/.local/lib/python3.9/site-packages/batchgenerators/dataloading/nondet_multi_threaded_augmenter.py", line 125, in results_loop
    raise e
  File "/home/gridsan/nchutisilp/.local/lib/python3.9/site-packages/batchgenerators/dataloading/nondet_multi_threaded_augmenter.py", line 103, in results_loop
    raise RuntimeError("One or more background workers are no longer alive. Exiting. Please check the "
RuntimeError: One or more background workers are no longer alive. Exiting. Please check the print statements above for the actual error message
/var/spool/slurmd/job27567897/slurm_script: line 30: 1713484 Aborted                 CUDA_VISIBLE_DEVICES=1 nnUNetv2_train $DATASET_ID $CONFIG 1 -tr $TRAINER -device cuda -pretrained_weights $PATH_TO_CHECKPOINT
mv: cannot stat '/home/gridsan/nchutisilp/datasets/nnUNet_Datasets/nnUNet_results/Dataset309_Sohee_Calcium_OCT_CrossValidation/nnUNetTrainer__nnUNetPlans__3d_32x160x128_b10/fold_0': No such file or directory
mv: cannot stat '/home/gridsan/nchutisilp/datasets/nnUNet_Datasets/nnUNet_results/Dataset309_Sohee_Calcium_OCT_CrossValidation/nnUNetTrainer__nnUNetPlans__3d_32x160x128_b10/fold_1': No such file or directory

############################
INFO: You are using the old nnU-Net default plans. We have updated our recommendations. Please consider using those instead! Read more here: https://github.com/MIC-DKFZ/nnUNet/blob/master/documentation/resenc_presets.md
############################

Using device: cuda:0

#######################################################################
Please cite the following paper when using nnU-Net:
Isensee, F., Jaeger, P. F., Kohl, S. A., Petersen, J., & Maier-Hein, K. H. (2021). nnU-Net: a self-configuring method for deep learning-based biomedical image segmentation. Nature methods, 18(2), 203-211.
#######################################################################

2024-12-07 12:17:21.925284: Using torch.compile...

############################
INFO: You are using the old nnU-Net default plans. We have updated our recommendations. Please consider using those instead! Read more here: https://github.com/MIC-DKFZ/nnUNet/blob/master/documentation/resenc_presets.md
############################

Using device: cuda:0

#######################################################################
Please cite the following paper when using nnU-Net:
Isensee, F., Jaeger, P. F., Kohl, S. A., Petersen, J., & Maier-Hein, K. H. (2021). nnU-Net: a self-configuring method for deep learning-based biomedical image segmentation. Nature methods, 18(2), 203-211.
#######################################################################

2024-12-07 12:17:21.925416: Using torch.compile...
/home/gridsan/nchutisilp/.local/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn("The verbose parameter is deprecated. Please use get_last_lr() "
/home/gridsan/nchutisilp/.local/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn("The verbose parameter is deprecated. Please use get_last_lr() "
################### Loading pretrained weights from file  /home/gridsan/nchutisilp/projects/OpenAI-CLIP/clip_preivl_poststent_pretrained_nnUNet.pt ###################
Below is the list of overlapping blocks in pretrained model and nnUNet architecture:
encoder.stages.0.0.convs.0.conv.weight shape torch.Size([32, 1, 3, 3, 3])
encoder.stages.0.0.convs.0.conv.bias shape torch.Size([32])
encoder.stages.0.0.convs.0.norm.weight shape torch.Size([32])
encoder.stages.0.0.convs.0.norm.bias shape torch.Size([32])
encoder.stages.0.0.convs.0.all_modules.0.weight shape torch.Size([32, 1, 3, 3, 3])
encoder.stages.0.0.convs.0.all_modules.0.bias shape torch.Size([32])
encoder.stages.0.0.convs.0.all_modules.1.weight shape torch.Size([32])
encoder.stages.0.0.convs.0.all_modules.1.bias shape torch.Size([32])
encoder.stages.0.0.convs.1.conv.weight shape torch.Size([32, 32, 3, 3, 3])
encoder.stages.0.0.convs.1.conv.bias shape torch.Size([32])
encoder.stages.0.0.convs.1.norm.weight shape torch.Size([32])
encoder.stages.0.0.convs.1.norm.bias shape torch.Size([32])
encoder.stages.0.0.convs.1.all_modules.0.weight shape torch.Size([32, 32, 3, 3, 3])
encoder.stages.0.0.convs.1.all_modules.0.bias shape torch.Size([32])
encoder.stages.0.0.convs.1.all_modules.1.weight shape torch.Size([32])
encoder.stages.0.0.convs.1.all_modules.1.bias shape torch.Size([32])
encoder.stages.1.0.convs.0.conv.weight shape torch.Size([64, 32, 3, 3, 3])
encoder.stages.1.0.convs.0.conv.bias shape torch.Size([64])
encoder.stages.1.0.convs.0.norm.weight shape torch.Size([64])
encoder.stages.1.0.convs.0.norm.bias shape torch.Size([64])
encoder.stages.1.0.convs.0.all_modules.0.weight shape torch.Size([64, 32, 3, 3, 3])
encoder.stages.1.0.convs.0.all_modules.0.bias shape torch.Size([64])
encoder.stages.1.0.convs.0.all_modules.1.weight shape torch.Size([64])
encoder.stages.1.0.convs.0.all_modules.1.bias shape torch.Size([64])
encoder.stages.1.0.convs.1.conv.weight shape torch.Size([64, 64, 3, 3, 3])
encoder.stages.1.0.convs.1.conv.bias shape torch.Size([64])
encoder.stages.1.0.convs.1.norm.weight shape torch.Size([64])
encoder.stages.1.0.convs.1.norm.bias shape torch.Size([64])
encoder.stages.1.0.convs.1.all_modules.0.weight shape torch.Size([64, 64, 3, 3, 3])
encoder.stages.1.0.convs.1.all_modules.0.bias shape torch.Size([64])
encoder.stages.1.0.convs.1.all_modules.1.weight shape torch.Size([64])
encoder.stages.1.0.convs.1.all_modules.1.bias shape torch.Size([64])
encoder.stages.2.0.convs.0.conv.weight shape torch.Size([128, 64, 3, 3, 3])
encoder.stages.2.0.convs.0.conv.bias shape torch.Size([128])
encoder.stages.2.0.convs.0.norm.weight shape torch.Size([128])
encoder.stages.2.0.convs.0.norm.bias shape torch.Size([128])
encoder.stages.2.0.convs.0.all_modules.0.weight shape torch.Size([128, 64, 3, 3, 3])
encoder.stages.2.0.convs.0.all_modules.0.bias shape torch.Size([128])
encoder.stages.2.0.convs.0.all_modules.1.weight shape torch.Size([128])
encoder.stages.2.0.convs.0.all_modules.1.bias shape torch.Size([128])
encoder.stages.2.0.convs.1.conv.weight shape torch.Size([128, 128, 3, 3, 3])
encoder.stages.2.0.convs.1.conv.bias shape torch.Size([128])
encoder.stages.2.0.convs.1.norm.weight shape torch.Size([128])
encoder.stages.2.0.convs.1.norm.bias shape torch.Size([128])
encoder.stages.2.0.convs.1.all_modules.0.weight shape torch.Size([128, 128, 3, 3, 3])
encoder.stages.2.0.convs.1.all_modules.0.bias shape torch.Size([128])
encoder.stages.2.0.convs.1.all_modules.1.weight shape torch.Size([128])
encoder.stages.2.0.convs.1.all_modules.1.bias shape torch.Size([128])
encoder.stages.3.0.convs.0.conv.weight shape torch.Size([256, 128, 3, 3, 3])
encoder.stages.3.0.convs.0.conv.bias shape torch.Size([256])
encoder.stages.3.0.convs.0.norm.weight shape torch.Size([256])
encoder.stages.3.0.convs.0.norm.bias shape torch.Size([256])
encoder.stages.3.0.convs.0.all_modules.0.weight shape torch.Size([256, 128, 3, 3, 3])
encoder.stages.3.0.convs.0.all_modules.0.bias shape torch.Size([256])
encoder.stages.3.0.convs.0.all_modules.1.weight shape torch.Size([256])
encoder.stages.3.0.convs.0.all_modules.1.bias shape torch.Size([256])
encoder.stages.3.0.convs.1.conv.weight shape torch.Size([256, 256, 3, 3, 3])
encoder.stages.3.0.convs.1.conv.bias shape torch.Size([256])
encoder.stages.3.0.convs.1.norm.weight shape torch.Size([256])
encoder.stages.3.0.convs.1.norm.bias shape torch.Size([256])
encoder.stages.3.0.convs.1.all_modules.0.weight shape torch.Size([256, 256, 3, 3, 3])
encoder.stages.3.0.convs.1.all_modules.0.bias shape torch.Size([256])
encoder.stages.3.0.convs.1.all_modules.1.weight shape torch.Size([256])
encoder.stages.3.0.convs.1.all_modules.1.bias shape torch.Size([256])
encoder.stages.4.0.convs.0.conv.weight shape torch.Size([320, 256, 3, 3, 3])
encoder.stages.4.0.convs.0.conv.bias shape torch.Size([320])
encoder.stages.4.0.convs.0.norm.weight shape torch.Size([320])
encoder.stages.4.0.convs.0.norm.bias shape torch.Size([320])
encoder.stages.4.0.convs.0.all_modules.0.weight shape torch.Size([320, 256, 3, 3, 3])
encoder.stages.4.0.convs.0.all_modules.0.bias shape torch.Size([320])
encoder.stages.4.0.convs.0.all_modules.1.weight shape torch.Size([320])
encoder.stages.4.0.convs.0.all_modules.1.bias shape torch.Size([320])
encoder.stages.4.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.4.0.convs.1.conv.bias shape torch.Size([320])
encoder.stages.4.0.convs.1.norm.weight shape torch.Size([320])
encoder.stages.4.0.convs.1.norm.bias shape torch.Size([320])
encoder.stages.4.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.4.0.convs.1.all_modules.0.bias shape torch.Size([320])
encoder.stages.4.0.convs.1.all_modules.1.weight shape torch.Size([320])
encoder.stages.4.0.convs.1.all_modules.1.bias shape torch.Size([320])
encoder.stages.5.0.convs.0.conv.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.5.0.convs.0.conv.bias shape torch.Size([320])
encoder.stages.5.0.convs.0.norm.weight shape torch.Size([320])
encoder.stages.5.0.convs.0.norm.bias shape torch.Size([320])
encoder.stages.5.0.convs.0.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.5.0.convs.0.all_modules.0.bias shape torch.Size([320])
encoder.stages.5.0.convs.0.all_modules.1.weight shape torch.Size([320])
encoder.stages.5.0.convs.0.all_modules.1.bias shape torch.Size([320])
encoder.stages.5.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.5.0.convs.1.conv.bias shape torch.Size([320])
encoder.stages.5.0.convs.1.norm.weight shape torch.Size([320])
encoder.stages.5.0.convs.1.norm.bias shape torch.Size([320])
encoder.stages.5.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.5.0.convs.1.all_modules.0.bias shape torch.Size([320])
encoder.stages.5.0.convs.1.all_modules.1.weight shape torch.Size([320])
encoder.stages.5.0.convs.1.all_modules.1.bias shape torch.Size([320])
decoder.encoder.stages.0.0.convs.0.conv.weight shape torch.Size([32, 1, 3, 3, 3])
decoder.encoder.stages.0.0.convs.0.conv.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.norm.weight shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.norm.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.all_modules.0.weight shape torch.Size([32, 1, 3, 3, 3])
decoder.encoder.stages.0.0.convs.0.all_modules.0.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.all_modules.1.weight shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.all_modules.1.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.conv.weight shape torch.Size([32, 32, 3, 3, 3])
decoder.encoder.stages.0.0.convs.1.conv.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.norm.weight shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.norm.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.all_modules.0.weight shape torch.Size([32, 32, 3, 3, 3])
decoder.encoder.stages.0.0.convs.1.all_modules.0.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.all_modules.1.weight shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.all_modules.1.bias shape torch.Size([32])
decoder.encoder.stages.1.0.convs.0.conv.weight shape torch.Size([64, 32, 3, 3, 3])
decoder.encoder.stages.1.0.convs.0.conv.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.norm.weight shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.norm.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.all_modules.0.weight shape torch.Size([64, 32, 3, 3, 3])
decoder.encoder.stages.1.0.convs.0.all_modules.0.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.all_modules.1.weight shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.all_modules.1.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.conv.weight shape torch.Size([64, 64, 3, 3, 3])
decoder.encoder.stages.1.0.convs.1.conv.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.norm.weight shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.norm.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.all_modules.0.weight shape torch.Size([64, 64, 3, 3, 3])
decoder.encoder.stages.1.0.convs.1.all_modules.0.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.all_modules.1.weight shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.all_modules.1.bias shape torch.Size([64])
decoder.encoder.stages.2.0.convs.0.conv.weight shape torch.Size([128, 64, 3, 3, 3])
decoder.encoder.stages.2.0.convs.0.conv.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.norm.weight shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.norm.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.all_modules.0.weight shape torch.Size([128, 64, 3, 3, 3])
decoder.encoder.stages.2.0.convs.0.all_modules.0.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.all_modules.1.weight shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.all_modules.1.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.conv.weight shape torch.Size([128, 128, 3, 3, 3])
decoder.encoder.stages.2.0.convs.1.conv.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.norm.weight shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.norm.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.all_modules.0.weight shape torch.Size([128, 128, 3, 3, 3])
decoder.encoder.stages.2.0.convs.1.all_modules.0.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.all_modules.1.weight shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.all_modules.1.bias shape torch.Size([128])
decoder.encoder.stages.3.0.convs.0.conv.weight shape torch.Size([256, 128, 3, 3, 3])
decoder.encoder.stages.3.0.convs.0.conv.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.norm.weight shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.norm.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.all_modules.0.weight shape torch.Size([256, 128, 3, 3, 3])
decoder.encoder.stages.3.0.convs.0.all_modules.0.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.all_modules.1.weight shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.all_modules.1.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.conv.weight shape torch.Size([256, 256, 3, 3, 3])
decoder.encoder.stages.3.0.convs.1.conv.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.norm.weight shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.norm.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.all_modules.0.weight shape torch.Size([256, 256, 3, 3, 3])
decoder.encoder.stages.3.0.convs.1.all_modules.0.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.all_modules.1.weight shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.all_modules.1.bias shape torch.Size([256])
decoder.encoder.stages.4.0.convs.0.conv.weight shape torch.Size([320, 256, 3, 3, 3])
decoder.encoder.stages.4.0.convs.0.conv.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.norm.weight shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.norm.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.all_modules.0.weight shape torch.Size([320, 256, 3, 3, 3])
decoder.encoder.stages.4.0.convs.0.all_modules.0.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.all_modules.1.weight shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.all_modules.1.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.4.0.convs.1.conv.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.norm.weight shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.norm.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.4.0.convs.1.all_modules.0.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.all_modules.1.weight shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.all_modules.1.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.conv.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.5.0.convs.0.conv.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.norm.weight shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.norm.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.5.0.convs.0.all_modules.0.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.all_modules.1.weight shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.all_modules.1.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.5.0.convs.1.conv.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.norm.weight shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.norm.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.5.0.convs.1.all_modules.0.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.all_modules.1.weight shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.all_modules.1.bias shape torch.Size([320])
decoder.stages.0.convs.0.conv.weight shape torch.Size([320, 640, 3, 3, 3])
decoder.stages.0.convs.0.conv.bias shape torch.Size([320])
decoder.stages.0.convs.0.norm.weight shape torch.Size([320])
decoder.stages.0.convs.0.norm.bias shape torch.Size([320])
decoder.stages.0.convs.0.all_modules.0.weight shape torch.Size([320, 640, 3, 3, 3])
decoder.stages.0.convs.0.all_modules.0.bias shape torch.Size([320])
decoder.stages.0.convs.0.all_modules.1.weight shape torch.Size([320])
decoder.stages.0.convs.0.all_modules.1.bias shape torch.Size([320])
decoder.stages.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.stages.0.convs.1.conv.bias shape torch.Size([320])
decoder.stages.0.convs.1.norm.weight shape torch.Size([320])
decoder.stages.0.convs.1.norm.bias shape torch.Size([320])
decoder.stages.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.stages.0.convs.1.all_modules.0.bias shape torch.Size([320])
decoder.stages.0.convs.1.all_modules.1.weight shape torch.Size([320])
decoder.stages.0.convs.1.all_modules.1.bias shape torch.Size([320])
decoder.stages.1.convs.0.conv.weight shape torch.Size([256, 512, 3, 3, 3])
decoder.stages.1.convs.0.conv.bias shape torch.Size([256])
decoder.stages.1.convs.0.norm.weight shape torch.Size([256])
decoder.stages.1.convs.0.norm.bias shape torch.Size([256])
decoder.stages.1.convs.0.all_modules.0.weight shape torch.Size([256, 512, 3, 3, 3])
decoder.stages.1.convs.0.all_modules.0.bias shape torch.Size([256])
decoder.stages.1.convs.0.all_modules.1.weight shape torch.Size([256])
decoder.stages.1.convs.0.all_modules.1.bias shape torch.Size([256])
decoder.stages.1.convs.1.conv.weight shape torch.Size([256, 256, 3, 3, 3])
decoder.stages.1.convs.1.conv.bias shape torch.Size([256])
decoder.stages.1.convs.1.norm.weight shape torch.Size([256])
decoder.stages.1.convs.1.norm.bias shape torch.Size([256])
decoder.stages.1.convs.1.all_modules.0.weight shape torch.Size([256, 256, 3, 3, 3])
decoder.stages.1.convs.1.all_modules.0.bias shape torch.Size([256])
decoder.stages.1.convs.1.all_modules.1.weight shape torch.Size([256])
decoder.stages.1.convs.1.all_modules.1.bias shape torch.Size([256])
decoder.stages.2.convs.0.conv.weight shape torch.Size([128, 256, 3, 3, 3])
decoder.stages.2.convs.0.conv.bias shape torch.Size([128])
decoder.stages.2.convs.0.norm.weight shape torch.Size([128])
decoder.stages.2.convs.0.norm.bias shape torch.Size([128])
decoder.stages.2.convs.0.all_modules.0.weight shape torch.Size([128, 256, 3, 3, 3])
decoder.stages.2.convs.0.all_modules.0.bias shape torch.Size([128])
decoder.stages.2.convs.0.all_modules.1.weight shape torch.Size([128])
decoder.stages.2.convs.0.all_modules.1.bias shape torch.Size([128])
decoder.stages.2.convs.1.conv.weight shape torch.Size([128, 128, 3, 3, 3])
decoder.stages.2.convs.1.conv.bias shape torch.Size([128])
decoder.stages.2.convs.1.norm.weight shape torch.Size([128])
decoder.stages.2.convs.1.norm.bias shape torch.Size([128])
decoder.stages.2.convs.1.all_modules.0.weight shape torch.Size([128, 128, 3, 3, 3])
decoder.stages.2.convs.1.all_modules.0.bias shape torch.Size([128])
decoder.stages.2.convs.1.all_modules.1.weight shape torch.Size([128])
decoder.stages.2.convs.1.all_modules.1.bias shape torch.Size([128])
decoder.stages.3.convs.0.conv.weight shape torch.Size([64, 128, 3, 3, 3])
decoder.stages.3.convs.0.conv.bias shape torch.Size([64])
decoder.stages.3.convs.0.norm.weight shape torch.Size([64])
decoder.stages.3.convs.0.norm.bias shape torch.Size([64])
decoder.stages.3.convs.0.all_modules.0.weight shape torch.Size([64, 128, 3, 3, 3])
decoder.stages.3.convs.0.all_modules.0.bias shape torch.Size([64])
decoder.stages.3.convs.0.all_modules.1.weight shape torch.Size([64])
decoder.stages.3.convs.0.all_modules.1.bias shape torch.Size([64])
decoder.stages.3.convs.1.conv.weight shape torch.Size([64, 64, 3, 3, 3])
decoder.stages.3.convs.1.conv.bias shape torch.Size([64])
decoder.stages.3.convs.1.norm.weight shape torch.Size([64])
decoder.stages.3.convs.1.norm.bias shape torch.Size([64])
decoder.stages.3.convs.1.all_modules.0.weight shape torch.Size([64, 64, 3, 3, 3])
decoder.stages.3.convs.1.all_modules.0.bias shape torch.Size([64])
decoder.stages.3.convs.1.all_modules.1.weight shape torch.Size([64])
decoder.stages.3.convs.1.all_modules.1.bias shape torch.Size([64])
decoder.stages.4.convs.0.conv.weight shape torch.Size([32, 64, 3, 3, 3])
decoder.stages.4.convs.0.conv.bias shape torch.Size([32])
decoder.stages.4.convs.0.norm.weight shape torch.Size([32])
decoder.stages.4.convs.0.norm.bias shape torch.Size([32])
decoder.stages.4.convs.0.all_modules.0.weight shape torch.Size([32, 64, 3, 3, 3])
decoder.stages.4.convs.0.all_modules.0.bias shape torch.Size([32])
decoder.stages.4.convs.0.all_modules.1.weight shape torch.Size([32])
decoder.stages.4.convs.0.all_modules.1.bias shape torch.Size([32])
decoder.stages.4.convs.1.conv.weight shape torch.Size([32, 32, 3, 3, 3])
decoder.stages.4.convs.1.conv.bias shape torch.Size([32])
decoder.stages.4.convs.1.norm.weight shape torch.Size([32])
decoder.stages.4.convs.1.norm.bias shape torch.Size([32])
decoder.stages.4.convs.1.all_modules.0.weight shape torch.Size([32, 32, 3, 3, 3])
decoder.stages.4.convs.1.all_modules.0.bias shape torch.Size([32])
decoder.stages.4.convs.1.all_modules.1.weight shape torch.Size([32])
decoder.stages.4.convs.1.all_modules.1.bias shape torch.Size([32])
decoder.transpconvs.0.weight shape torch.Size([320, 320, 1, 2, 2])
decoder.transpconvs.0.bias shape torch.Size([320])
decoder.transpconvs.1.weight shape torch.Size([320, 256, 2, 2, 2])
decoder.transpconvs.1.bias shape torch.Size([256])
decoder.transpconvs.2.weight shape torch.Size([256, 128, 2, 2, 2])
decoder.transpconvs.2.bias shape torch.Size([128])
decoder.transpconvs.3.weight shape torch.Size([128, 64, 2, 2, 2])
decoder.transpconvs.3.bias shape torch.Size([64])
decoder.transpconvs.4.weight shape torch.Size([64, 32, 2, 2, 2])
decoder.transpconvs.4.bias shape torch.Size([32])
################### Done ###################
2024-12-07 12:17:25.432967: do_dummy_2d_data_aug: True
2024-12-07 12:17:25.435454: Using splits from existing split file: /home/gridsan/nchutisilp/datasets/nnUNet_Datasets/nnUNet_preprocessed/Dataset307_Sohee_Calcium_OCT_CrossValidation/splits_final.json
2024-12-07 12:17:25.437274: The split file contains 5 splits.
2024-12-07 12:17:25.438200: Desired fold for training: 3
2024-12-07 12:17:25.438884: This split has 7 training and 1 validation cases.
################### Loading pretrained weights from file  /home/gridsan/nchutisilp/projects/OpenAI-CLIP/clip_preivl_poststent_pretrained_nnUNet.pt ###################
Below is the list of overlapping blocks in pretrained model and nnUNet architecture:
encoder.stages.0.0.convs.0.conv.weight shape torch.Size([32, 1, 3, 3, 3])
encoder.stages.0.0.convs.0.conv.bias shape torch.Size([32])
encoder.stages.0.0.convs.0.norm.weight shape torch.Size([32])
encoder.stages.0.0.convs.0.norm.bias shape torch.Size([32])
encoder.stages.0.0.convs.0.all_modules.0.weight shape torch.Size([32, 1, 3, 3, 3])
encoder.stages.0.0.convs.0.all_modules.0.bias shape torch.Size([32])
encoder.stages.0.0.convs.0.all_modules.1.weight shape torch.Size([32])
encoder.stages.0.0.convs.0.all_modules.1.bias shape torch.Size([32])
encoder.stages.0.0.convs.1.conv.weight shape torch.Size([32, 32, 3, 3, 3])
encoder.stages.0.0.convs.1.conv.bias shape torch.Size([32])
encoder.stages.0.0.convs.1.norm.weight shape torch.Size([32])
encoder.stages.0.0.convs.1.norm.bias shape torch.Size([32])
encoder.stages.0.0.convs.1.all_modules.0.weight shape torch.Size([32, 32, 3, 3, 3])
encoder.stages.0.0.convs.1.all_modules.0.bias shape torch.Size([32])
encoder.stages.0.0.convs.1.all_modules.1.weight shape torch.Size([32])
encoder.stages.0.0.convs.1.all_modules.1.bias shape torch.Size([32])
encoder.stages.1.0.convs.0.conv.weight shape torch.Size([64, 32, 3, 3, 3])
encoder.stages.1.0.convs.0.conv.bias shape torch.Size([64])
encoder.stages.1.0.convs.0.norm.weight shape torch.Size([64])
encoder.stages.1.0.convs.0.norm.bias shape torch.Size([64])
encoder.stages.1.0.convs.0.all_modules.0.weight shape torch.Size([64, 32, 3, 3, 3])
encoder.stages.1.0.convs.0.all_modules.0.bias shape torch.Size([64])
encoder.stages.1.0.convs.0.all_modules.1.weight shape torch.Size([64])
encoder.stages.1.0.convs.0.all_modules.1.bias shape torch.Size([64])
encoder.stages.1.0.convs.1.conv.weight shape torch.Size([64, 64, 3, 3, 3])
encoder.stages.1.0.convs.1.conv.bias shape torch.Size([64])
encoder.stages.1.0.convs.1.norm.weight shape torch.Size([64])
encoder.stages.1.0.convs.1.norm.bias shape torch.Size([64])
encoder.stages.1.0.convs.1.all_modules.0.weight shape torch.Size([64, 64, 3, 3, 3])
encoder.stages.1.0.convs.1.all_modules.0.bias shape torch.Size([64])
encoder.stages.1.0.convs.1.all_modules.1.weight shape torch.Size([64])
encoder.stages.1.0.convs.1.all_modules.1.bias shape torch.Size([64])
encoder.stages.2.0.convs.0.conv.weight shape torch.Size([128, 64, 3, 3, 3])
encoder.stages.2.0.convs.0.conv.bias shape torch.Size([128])
encoder.stages.2.0.convs.0.norm.weight shape torch.Size([128])
encoder.stages.2.0.convs.0.norm.bias shape torch.Size([128])
encoder.stages.2.0.convs.0.all_modules.0.weight shape torch.Size([128, 64, 3, 3, 3])
encoder.stages.2.0.convs.0.all_modules.0.bias shape torch.Size([128])
encoder.stages.2.0.convs.0.all_modules.1.weight shape torch.Size([128])
encoder.stages.2.0.convs.0.all_modules.1.bias shape torch.Size([128])
encoder.stages.2.0.convs.1.conv.weight shape torch.Size([128, 128, 3, 3, 3])
encoder.stages.2.0.convs.1.conv.bias shape torch.Size([128])
encoder.stages.2.0.convs.1.norm.weight shape torch.Size([128])
encoder.stages.2.0.convs.1.norm.bias shape torch.Size([128])
encoder.stages.2.0.convs.1.all_modules.0.weight shape torch.Size([128, 128, 3, 3, 3])
encoder.stages.2.0.convs.1.all_modules.0.bias shape torch.Size([128])
encoder.stages.2.0.convs.1.all_modules.1.weight shape torch.Size([128])
encoder.stages.2.0.convs.1.all_modules.1.bias shape torch.Size([128])
encoder.stages.3.0.convs.0.conv.weight shape torch.Size([256, 128, 3, 3, 3])
encoder.stages.3.0.convs.0.conv.bias shape torch.Size([256])
encoder.stages.3.0.convs.0.norm.weight shape torch.Size([256])
encoder.stages.3.0.convs.0.norm.bias shape torch.Size([256])
encoder.stages.3.0.convs.0.all_modules.0.weight shape torch.Size([256, 128, 3, 3, 3])
encoder.stages.3.0.convs.0.all_modules.0.bias shape torch.Size([256])
encoder.stages.3.0.convs.0.all_modules.1.weight shape torch.Size([256])
encoder.stages.3.0.convs.0.all_modules.1.bias shape torch.Size([256])
encoder.stages.3.0.convs.1.conv.weight shape torch.Size([256, 256, 3, 3, 3])
encoder.stages.3.0.convs.1.conv.bias shape torch.Size([256])
encoder.stages.3.0.convs.1.norm.weight shape torch.Size([256])
encoder.stages.3.0.convs.1.norm.bias shape torch.Size([256])
encoder.stages.3.0.convs.1.all_modules.0.weight shape torch.Size([256, 256, 3, 3, 3])
encoder.stages.3.0.convs.1.all_modules.0.bias shape torch.Size([256])
encoder.stages.3.0.convs.1.all_modules.1.weight shape torch.Size([256])
encoder.stages.3.0.convs.1.all_modules.1.bias shape torch.Size([256])
encoder.stages.4.0.convs.0.conv.weight shape torch.Size([320, 256, 3, 3, 3])
encoder.stages.4.0.convs.0.conv.bias shape torch.Size([320])
encoder.stages.4.0.convs.0.norm.weight shape torch.Size([320])
encoder.stages.4.0.convs.0.norm.bias shape torch.Size([320])
encoder.stages.4.0.convs.0.all_modules.0.weight shape torch.Size([320, 256, 3, 3, 3])
encoder.stages.4.0.convs.0.all_modules.0.bias shape torch.Size([320])
encoder.stages.4.0.convs.0.all_modules.1.weight shape torch.Size([320])
encoder.stages.4.0.convs.0.all_modules.1.bias shape torch.Size([320])
encoder.stages.4.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.4.0.convs.1.conv.bias shape torch.Size([320])
encoder.stages.4.0.convs.1.norm.weight shape torch.Size([320])
encoder.stages.4.0.convs.1.norm.bias shape torch.Size([320])
encoder.stages.4.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.4.0.convs.1.all_modules.0.bias shape torch.Size([320])
encoder.stages.4.0.convs.1.all_modules.1.weight shape torch.Size([320])
encoder.stages.4.0.convs.1.all_modules.1.bias shape torch.Size([320])
encoder.stages.5.0.convs.0.conv.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.5.0.convs.0.conv.bias shape torch.Size([320])
encoder.stages.5.0.convs.0.norm.weight shape torch.Size([320])
encoder.stages.5.0.convs.0.norm.bias shape torch.Size([320])
encoder.stages.5.0.convs.0.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.5.0.convs.0.all_modules.0.bias shape torch.Size([320])
encoder.stages.5.0.convs.0.all_modules.1.weight shape torch.Size([320])
encoder.stages.5.0.convs.0.all_modules.1.bias shape torch.Size([320])
encoder.stages.5.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.5.0.convs.1.conv.bias shape torch.Size([320])
encoder.stages.5.0.convs.1.norm.weight shape torch.Size([320])
encoder.stages.5.0.convs.1.norm.bias shape torch.Size([320])
encoder.stages.5.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.5.0.convs.1.all_modules.0.bias shape torch.Size([320])
encoder.stages.5.0.convs.1.all_modules.1.weight shape torch.Size([320])
encoder.stages.5.0.convs.1.all_modules.1.bias shape torch.Size([320])
decoder.encoder.stages.0.0.convs.0.conv.weight shape torch.Size([32, 1, 3, 3, 3])
decoder.encoder.stages.0.0.convs.0.conv.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.norm.weight shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.norm.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.all_modules.0.weight shape torch.Size([32, 1, 3, 3, 3])
decoder.encoder.stages.0.0.convs.0.all_modules.0.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.all_modules.1.weight shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.all_modules.1.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.conv.weight shape torch.Size([32, 32, 3, 3, 3])
decoder.encoder.stages.0.0.convs.1.conv.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.norm.weight shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.norm.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.all_modules.0.weight shape torch.Size([32, 32, 3, 3, 3])
decoder.encoder.stages.0.0.convs.1.all_modules.0.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.all_modules.1.weight shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.all_modules.1.bias shape torch.Size([32])
decoder.encoder.stages.1.0.convs.0.conv.weight shape torch.Size([64, 32, 3, 3, 3])
decoder.encoder.stages.1.0.convs.0.conv.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.norm.weight shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.norm.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.all_modules.0.weight shape torch.Size([64, 32, 3, 3, 3])
decoder.encoder.stages.1.0.convs.0.all_modules.0.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.all_modules.1.weight shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.all_modules.1.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.conv.weight shape torch.Size([64, 64, 3, 3, 3])
decoder.encoder.stages.1.0.convs.1.conv.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.norm.weight shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.norm.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.all_modules.0.weight shape torch.Size([64, 64, 3, 3, 3])
decoder.encoder.stages.1.0.convs.1.all_modules.0.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.all_modules.1.weight shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.all_modules.1.bias shape torch.Size([64])
decoder.encoder.stages.2.0.convs.0.conv.weight shape torch.Size([128, 64, 3, 3, 3])
decoder.encoder.stages.2.0.convs.0.conv.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.norm.weight shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.norm.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.all_modules.0.weight shape torch.Size([128, 64, 3, 3, 3])
decoder.encoder.stages.2.0.convs.0.all_modules.0.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.all_modules.1.weight shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.all_modules.1.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.conv.weight shape torch.Size([128, 128, 3, 3, 3])
decoder.encoder.stages.2.0.convs.1.conv.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.norm.weight shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.norm.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.all_modules.0.weight shape torch.Size([128, 128, 3, 3, 3])
decoder.encoder.stages.2.0.convs.1.all_modules.0.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.all_modules.1.weight shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.all_modules.1.bias shape torch.Size([128])
decoder.encoder.stages.3.0.convs.0.conv.weight shape torch.Size([256, 128, 3, 3, 3])
decoder.encoder.stages.3.0.convs.0.conv.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.norm.weight shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.norm.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.all_modules.0.weight shape torch.Size([256, 128, 3, 3, 3])
decoder.encoder.stages.3.0.convs.0.all_modules.0.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.all_modules.1.weight shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.all_modules.1.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.conv.weight shape torch.Size([256, 256, 3, 3, 3])
decoder.encoder.stages.3.0.convs.1.conv.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.norm.weight shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.norm.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.all_modules.0.weight shape torch.Size([256, 256, 3, 3, 3])
decoder.encoder.stages.3.0.convs.1.all_modules.0.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.all_modules.1.weight shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.all_modules.1.bias shape torch.Size([256])
decoder.encoder.stages.4.0.convs.0.conv.weight shape torch.Size([320, 256, 3, 3, 3])
decoder.encoder.stages.4.0.convs.0.conv.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.norm.weight shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.norm.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.all_modules.0.weight shape torch.Size([320, 256, 3, 3, 3])
decoder.encoder.stages.4.0.convs.0.all_modules.0.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.all_modules.1.weight shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.all_modules.1.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.4.0.convs.1.conv.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.norm.weight shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.norm.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.4.0.convs.1.all_modules.0.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.all_modules.1.weight shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.all_modules.1.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.conv.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.5.0.convs.0.conv.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.norm.weight shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.norm.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.5.0.convs.0.all_modules.0.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.all_modules.1.weight shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.all_modules.1.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.5.0.convs.1.conv.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.norm.weight shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.norm.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.5.0.convs.1.all_modules.0.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.all_modules.1.weight shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.all_modules.1.bias shape torch.Size([320])
decoder.stages.0.convs.0.conv.weight shape torch.Size([320, 640, 3, 3, 3])
decoder.stages.0.convs.0.conv.bias shape torch.Size([320])
decoder.stages.0.convs.0.norm.weight shape torch.Size([320])
decoder.stages.0.convs.0.norm.bias shape torch.Size([320])
decoder.stages.0.convs.0.all_modules.0.weight shape torch.Size([320, 640, 3, 3, 3])
decoder.stages.0.convs.0.all_modules.0.bias shape torch.Size([320])
decoder.stages.0.convs.0.all_modules.1.weight shape torch.Size([320])
decoder.stages.0.convs.0.all_modules.1.bias shape torch.Size([320])
decoder.stages.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.stages.0.convs.1.conv.bias shape torch.Size([320])
decoder.stages.0.convs.1.norm.weight shape torch.Size([320])
decoder.stages.0.convs.1.norm.bias shape torch.Size([320])
decoder.stages.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.stages.0.convs.1.all_modules.0.bias shape torch.Size([320])
decoder.stages.0.convs.1.all_modules.1.weight shape torch.Size([320])
decoder.stages.0.convs.1.all_modules.1.bias shape torch.Size([320])
decoder.stages.1.convs.0.conv.weight shape torch.Size([256, 512, 3, 3, 3])
decoder.stages.1.convs.0.conv.bias shape torch.Size([256])
decoder.stages.1.convs.0.norm.weight shape torch.Size([256])
decoder.stages.1.convs.0.norm.bias shape torch.Size([256])
decoder.stages.1.convs.0.all_modules.0.weight shape torch.Size([256, 512, 3, 3, 3])
decoder.stages.1.convs.0.all_modules.0.bias shape torch.Size([256])
decoder.stages.1.convs.0.all_modules.1.weight shape torch.Size([256])
decoder.stages.1.convs.0.all_modules.1.bias shape torch.Size([256])
decoder.stages.1.convs.1.conv.weight shape torch.Size([256, 256, 3, 3, 3])
decoder.stages.1.convs.1.conv.bias shape torch.Size([256])
decoder.stages.1.convs.1.norm.weight shape torch.Size([256])
decoder.stages.1.convs.1.norm.bias shape torch.Size([256])
decoder.stages.1.convs.1.all_modules.0.weight shape torch.Size([256, 256, 3, 3, 3])
decoder.stages.1.convs.1.all_modules.0.bias shape torch.Size([256])
decoder.stages.1.convs.1.all_modules.1.weight shape torch.Size([256])
decoder.stages.1.convs.1.all_modules.1.bias shape torch.Size([256])
decoder.stages.2.convs.0.conv.weight shape torch.Size([128, 256, 3, 3, 3])
decoder.stages.2.convs.0.conv.bias shape torch.Size([128])
decoder.stages.2.convs.0.norm.weight shape torch.Size([128])
decoder.stages.2.convs.0.norm.bias shape torch.Size([128])
decoder.stages.2.convs.0.all_modules.0.weight shape torch.Size([128, 256, 3, 3, 3])
decoder.stages.2.convs.0.all_modules.0.bias shape torch.Size([128])
decoder.stages.2.convs.0.all_modules.1.weight shape torch.Size([128])
decoder.stages.2.convs.0.all_modules.1.bias shape torch.Size([128])
decoder.stages.2.convs.1.conv.weight shape torch.Size([128, 128, 3, 3, 3])
decoder.stages.2.convs.1.conv.bias shape torch.Size([128])
decoder.stages.2.convs.1.norm.weight shape torch.Size([128])
decoder.stages.2.convs.1.norm.bias shape torch.Size([128])
decoder.stages.2.convs.1.all_modules.0.weight shape torch.Size([128, 128, 3, 3, 3])
decoder.stages.2.convs.1.all_modules.0.bias shape torch.Size([128])
decoder.stages.2.convs.1.all_modules.1.weight shape torch.Size([128])
decoder.stages.2.convs.1.all_modules.1.bias shape torch.Size([128])
decoder.stages.3.convs.0.conv.weight shape torch.Size([64, 128, 3, 3, 3])
decoder.stages.3.convs.0.conv.bias shape torch.Size([64])
decoder.stages.3.convs.0.norm.weight shape torch.Size([64])
decoder.stages.3.convs.0.norm.bias shape torch.Size([64])
decoder.stages.3.convs.0.all_modules.0.weight shape torch.Size([64, 128, 3, 3, 3])
decoder.stages.3.convs.0.all_modules.0.bias shape torch.Size([64])
decoder.stages.3.convs.0.all_modules.1.weight shape torch.Size([64])
decoder.stages.3.convs.0.all_modules.1.bias shape torch.Size([64])
decoder.stages.3.convs.1.conv.weight shape torch.Size([64, 64, 3, 3, 3])
decoder.stages.3.convs.1.conv.bias shape torch.Size([64])
decoder.stages.3.convs.1.norm.weight shape torch.Size([64])
decoder.stages.3.convs.1.norm.bias shape torch.Size([64])
decoder.stages.3.convs.1.all_modules.0.weight shape torch.Size([64, 64, 3, 3, 3])
decoder.stages.3.convs.1.all_modules.0.bias shape torch.Size([64])
decoder.stages.3.convs.1.all_modules.1.weight shape torch.Size([64])
decoder.stages.3.convs.1.all_modules.1.bias shape torch.Size([64])
decoder.stages.4.convs.0.conv.weight shape torch.Size([32, 64, 3, 3, 3])
decoder.stages.4.convs.0.conv.bias shape torch.Size([32])
decoder.stages.4.convs.0.norm.weight shape torch.Size([32])
decoder.stages.4.convs.0.norm.bias shape torch.Size([32])
decoder.stages.4.convs.0.all_modules.0.weight shape torch.Size([32, 64, 3, 3, 3])
decoder.stages.4.convs.0.all_modules.0.bias shape torch.Size([32])
decoder.stages.4.convs.0.all_modules.1.weight shape torch.Size([32])
decoder.stages.4.convs.0.all_modules.1.bias shape torch.Size([32])
decoder.stages.4.convs.1.conv.weight shape torch.Size([32, 32, 3, 3, 3])
decoder.stages.4.convs.1.conv.bias shape torch.Size([32])
decoder.stages.4.convs.1.norm.weight shape torch.Size([32])
decoder.stages.4.convs.1.norm.bias shape torch.Size([32])
decoder.stages.4.convs.1.all_modules.0.weight shape torch.Size([32, 32, 3, 3, 3])
decoder.stages.4.convs.1.all_modules.0.bias shape torch.Size([32])
decoder.stages.4.convs.1.all_modules.1.weight shape torch.Size([32])
decoder.stages.4.convs.1.all_modules.1.bias shape torch.Size([32])
decoder.transpconvs.0.weight shape torch.Size([320, 320, 1, 2, 2])
decoder.transpconvs.0.bias shape torch.Size([320])
decoder.transpconvs.1.weight shape torch.Size([320, 256, 2, 2, 2])
decoder.transpconvs.1.bias shape torch.Size([256])
decoder.transpconvs.2.weight shape torch.Size([256, 128, 2, 2, 2])
decoder.transpconvs.2.bias shape torch.Size([128])
decoder.transpconvs.3.weight shape torch.Size([128, 64, 2, 2, 2])
decoder.transpconvs.3.bias shape torch.Size([64])
decoder.transpconvs.4.weight shape torch.Size([64, 32, 2, 2, 2])
decoder.transpconvs.4.bias shape torch.Size([32])
################### Done ###################
2024-12-07 12:17:25.432932: do_dummy_2d_data_aug: True
2024-12-07 12:17:25.435467: Using splits from existing split file: /home/gridsan/nchutisilp/datasets/nnUNet_Datasets/nnUNet_preprocessed/Dataset307_Sohee_Calcium_OCT_CrossValidation/splits_final.json
2024-12-07 12:17:25.437432: The split file contains 5 splits.
2024-12-07 12:17:25.438385: Desired fold for training: 2
2024-12-07 12:17:25.439178: This split has 6 training and 2 validation cases.
using pin_memory on device 0
using pin_memory on device 0
using pin_memory on device 0

This is the configuration used by this training:
Configuration name: 3d_32x160x128_b10
 {'data_identifier': 'nnUNetPlans_3d_fullres', 'preprocessor_name': 'DefaultPreprocessor', 'batch_size': 10, 'patch_size': [32, 160, 128], 'median_image_size_in_voxels': [375.0, 498.0, 498.0], 'spacing': [1.0, 1.0, 1.0], 'normalization_schemes': ['ZScoreNormalization'], 'use_mask_for_norm': [False], 'resampling_fn_data': 'resample_data_or_seg_to_shape', 'resampling_fn_seg': 'resample_data_or_seg_to_shape', 'resampling_fn_data_kwargs': {'is_seg': False, 'order': 3, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_seg_kwargs': {'is_seg': True, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_probabilities': 'resample_data_or_seg_to_shape', 'resampling_fn_probabilities_kwargs': {'is_seg': False, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'architecture': {'network_class_name': 'dynamic_network_architectures.architectures.unet.PlainConvUNet', 'arch_kwargs': {'n_stages': 6, 'features_per_stage': [32, 64, 128, 256, 320, 320], 'conv_op': 'torch.nn.modules.conv.Conv3d', 'kernel_sizes': [[3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3]], 'strides': [[1, 1, 1], [2, 2, 2], [2, 2, 2], [2, 2, 2], [2, 2, 2], [1, 2, 2]], 'n_conv_per_stage': [2, 2, 2, 2, 2, 2], 'n_conv_per_stage_decoder': [2, 2, 2, 2, 2], 'conv_bias': True, 'norm_op': 'torch.nn.modules.instancenorm.InstanceNorm3d', 'norm_op_kwargs': {'eps': 1e-05, 'affine': True}, 'dropout_op': None, 'dropout_op_kwargs': None, 'nonlin': 'torch.nn.LeakyReLU', 'nonlin_kwargs': {'inplace': True}, 'deep_supervision': True}, '_kw_requires_import': ['conv_op', 'norm_op', 'dropout_op', 'nonlin']}, 'batch_dice': True, 'inherits_from': '3d_fullres'} 

These are the global plan.json settings:
 {'dataset_name': 'Dataset307_Sohee_Calcium_OCT_CrossValidation', 'plans_name': 'nnUNetPlans', 'original_median_spacing_after_transp': [1.0, 1.0, 1.0], 'original_median_shape_after_transp': [375, 498, 498], 'image_reader_writer': 'SimpleITKIO', 'transpose_forward': [0, 1, 2], 'transpose_backward': [0, 1, 2], 'experiment_planner_used': 'ExperimentPlanner', 'label_manager': 'LabelManager', 'foreground_intensity_properties_per_channel': {'0': {'max': 0.4977976381778717, 'mean': 0.13507525622844696, 'median': 0.09599608182907104, 'min': 0.0, 'percentile_00_5': 0.014754901640117168, 'percentile_99_5': 0.4977976381778717, 'std': 0.12025152146816254}}} 

2024-12-07 12:17:58.998674: unpacking dataset...
using pin_memory on device 0

This is the configuration used by this training:
Configuration name: 3d_32x160x128_b10
 {'data_identifier': 'nnUNetPlans_3d_fullres', 'preprocessor_name': 'DefaultPreprocessor', 'batch_size': 10, 'patch_size': [32, 160, 128], 'median_image_size_in_voxels': [375.0, 498.0, 498.0], 'spacing': [1.0, 1.0, 1.0], 'normalization_schemes': ['ZScoreNormalization'], 'use_mask_for_norm': [False], 'resampling_fn_data': 'resample_data_or_seg_to_shape', 'resampling_fn_seg': 'resample_data_or_seg_to_shape', 'resampling_fn_data_kwargs': {'is_seg': False, 'order': 3, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_seg_kwargs': {'is_seg': True, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_probabilities': 'resample_data_or_seg_to_shape', 'resampling_fn_probabilities_kwargs': {'is_seg': False, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'architecture': {'network_class_name': 'dynamic_network_architectures.architectures.unet.PlainConvUNet', 'arch_kwargs': {'n_stages': 6, 'features_per_stage': [32, 64, 128, 256, 320, 320], 'conv_op': 'torch.nn.modules.conv.Conv3d', 'kernel_sizes': [[3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3]], 'strides': [[1, 1, 1], [2, 2, 2], [2, 2, 2], [2, 2, 2], [2, 2, 2], [1, 2, 2]], 'n_conv_per_stage': [2, 2, 2, 2, 2, 2], 'n_conv_per_stage_decoder': [2, 2, 2, 2, 2], 'conv_bias': True, 'norm_op': 'torch.nn.modules.instancenorm.InstanceNorm3d', 'norm_op_kwargs': {'eps': 1e-05, 'affine': True}, 'dropout_op': None, 'dropout_op_kwargs': None, 'nonlin': 'torch.nn.LeakyReLU', 'nonlin_kwargs': {'inplace': True}, 'deep_supervision': True}, '_kw_requires_import': ['conv_op', 'norm_op', 'dropout_op', 'nonlin']}, 'batch_dice': True, 'inherits_from': '3d_fullres'} 

These are the global plan.json settings:
 {'dataset_name': 'Dataset307_Sohee_Calcium_OCT_CrossValidation', 'plans_name': 'nnUNetPlans', 'original_median_spacing_after_transp': [1.0, 1.0, 1.0], 'original_median_shape_after_transp': [375, 498, 498], 'image_reader_writer': 'SimpleITKIO', 'transpose_forward': [0, 1, 2], 'transpose_backward': [0, 1, 2], 'experiment_planner_used': 'ExperimentPlanner', 'label_manager': 'LabelManager', 'foreground_intensity_properties_per_channel': {'0': {'max': 0.4977976381778717, 'mean': 0.13507525622844696, 'median': 0.09599608182907104, 'min': 0.0, 'percentile_00_5': 0.014754901640117168, 'percentile_99_5': 0.4977976381778717, 'std': 0.12025152146816254}}} 

2024-12-07 12:18:01.684631: unpacking dataset...
2024-12-07 12:18:04.685343: unpacking done...
2024-12-07 12:18:04.704507: Unable to plot network architecture: nnUNet_compile is enabled!
2024-12-07 12:18:04.745959: 
2024-12-07 12:18:04.747446: Epoch 0
2024-12-07 12:18:04.748984: Current learning rate: 0.01
2024-12-07 12:25:44.459060: Validation loss improved from 1000.00000 to -0.20976! Patience: 0/50
2024-12-07 12:25:44.460067: train_loss -0.0585
2024-12-07 12:25:44.460896: val_loss -0.2098
2024-12-07 12:25:44.461542: Pseudo dice [0.5422]
2024-12-07 12:25:44.462142: Epoch time: 459.72 s
2024-12-07 12:25:44.462813: Yayy! New best EMA pseudo Dice: 0.5422
2024-12-07 12:25:46.051120: 
2024-12-07 12:25:46.052798: Epoch 1
2024-12-07 12:25:46.054015: Current learning rate: 0.00999
2024-12-07 12:32:50.403793: Validation loss improved from -0.20976 to -0.22814! Patience: 0/50
2024-12-07 12:32:50.404818: train_loss -0.2087
2024-12-07 12:32:50.405658: val_loss -0.2281
2024-12-07 12:32:50.406381: Pseudo dice [0.5454]
2024-12-07 12:32:50.407186: Epoch time: 424.36 s
2024-12-07 12:32:50.407891: Yayy! New best EMA pseudo Dice: 0.5425
2024-12-07 12:32:52.118053: 
2024-12-07 12:32:52.119363: Epoch 2
2024-12-07 12:32:52.120128: Current learning rate: 0.00998
2024-12-07 12:39:54.942293: Validation loss improved from -0.22814 to -0.31871! Patience: 0/50
2024-12-07 12:39:54.943246: train_loss -0.261
2024-12-07 12:39:54.944458: val_loss -0.3187
2024-12-07 12:39:54.945356: Pseudo dice [0.6045]
2024-12-07 12:39:54.946208: Epoch time: 422.83 s
2024-12-07 12:39:54.946892: Yayy! New best EMA pseudo Dice: 0.5487
2024-12-07 12:39:56.788406: 
2024-12-07 12:39:56.790071: Epoch 3
2024-12-07 12:39:56.791051: Current learning rate: 0.00997
2024-12-07 12:46:57.343376: Validation loss improved from -0.31871 to -0.32874! Patience: 0/50
2024-12-07 12:46:57.344087: train_loss -0.3031
2024-12-07 12:46:57.345080: val_loss -0.3287
2024-12-07 12:46:57.345918: Pseudo dice [0.6151]
2024-12-07 12:46:57.346773: Epoch time: 420.56 s
2024-12-07 12:46:57.347522: Yayy! New best EMA pseudo Dice: 0.5554
2024-12-07 12:46:59.135411: 
2024-12-07 12:46:59.136821: Epoch 4
2024-12-07 12:46:59.137597: Current learning rate: 0.00996
2024-12-07 12:53:27.814639: Validation loss improved from -0.32874 to -0.39017! Patience: 0/50
2024-12-07 12:53:27.815660: train_loss -0.3305
2024-12-07 12:53:27.816676: val_loss -0.3902
2024-12-07 12:53:27.817662: Pseudo dice [0.6519]
2024-12-07 12:53:27.818598: Epoch time: 388.68 s
2024-12-07 12:53:28.236672: Yayy! New best EMA pseudo Dice: 0.565
2024-12-07 12:53:30.074757: 
2024-12-07 12:53:30.076389: Epoch 5
2024-12-07 12:53:30.077542: Current learning rate: 0.00995
2024-12-07 13:00:11.654145: Validation loss improved from -0.39017 to -0.41791! Patience: 0/50
2024-12-07 13:00:11.655156: train_loss -0.3552
2024-12-07 13:00:11.656131: val_loss -0.4179
2024-12-07 13:00:11.656960: Pseudo dice [0.6639]
2024-12-07 13:00:11.657827: Epoch time: 401.58 s
2024-12-07 13:00:11.658657: Yayy! New best EMA pseudo Dice: 0.5749
2024-12-07 13:00:13.434684: 
2024-12-07 13:00:13.435985: Epoch 6
2024-12-07 13:00:13.436771: Current learning rate: 0.00995
2024-12-07 13:06:45.417238: Validation loss improved from -0.41791 to -0.43996! Patience: 0/50
2024-12-07 13:06:45.418298: train_loss -0.378
2024-12-07 13:06:45.419147: val_loss -0.44
2024-12-07 13:06:45.420021: Pseudo dice [0.677]
2024-12-07 13:06:45.420887: Epoch time: 391.99 s
2024-12-07 13:06:45.421718: Yayy! New best EMA pseudo Dice: 0.5851
2024-12-07 13:06:47.211674: 
2024-12-07 13:06:47.213049: Epoch 7
2024-12-07 13:06:47.214156: Current learning rate: 0.00994
2024-12-07 13:13:10.436426: Validation loss did not improve from -0.43996. Patience: 1/50
2024-12-07 13:13:10.437427: train_loss -0.407
2024-12-07 13:13:10.438197: val_loss -0.381
2024-12-07 13:13:10.438867: Pseudo dice [0.6341]
2024-12-07 13:13:10.439556: Epoch time: 383.23 s
2024-12-07 13:13:10.440167: Yayy! New best EMA pseudo Dice: 0.59
2024-12-07 13:13:12.709251: 
2024-12-07 13:13:12.710883: Epoch 8
2024-12-07 13:13:12.712029: Current learning rate: 0.00993
2024-12-07 13:19:47.634028: Validation loss improved from -0.43996 to -0.47486! Patience: 1/50
2024-12-07 13:19:47.634980: train_loss -0.4216
2024-12-07 13:19:47.635886: val_loss -0.4749
2024-12-07 13:19:47.636765: Pseudo dice [0.7023]
2024-12-07 13:19:47.637645: Epoch time: 394.93 s
2024-12-07 13:19:47.638653: Yayy! New best EMA pseudo Dice: 0.6012
2024-12-07 13:19:49.477845: 
2024-12-07 13:19:49.479439: Epoch 9
2024-12-07 13:19:49.480512: Current learning rate: 0.00992
2024-12-07 13:26:30.508262: Validation loss did not improve from -0.47486. Patience: 1/50
2024-12-07 13:26:30.511082: train_loss -0.4335
2024-12-07 13:26:30.512097: val_loss -0.4429
2024-12-07 13:26:30.512818: Pseudo dice [0.6774]
2024-12-07 13:26:30.513601: Epoch time: 401.04 s
2024-12-07 13:26:30.932439: Yayy! New best EMA pseudo Dice: 0.6088
2024-12-07 13:26:32.654580: 
2024-12-07 13:26:32.656002: Epoch 10
2024-12-07 13:26:32.656786: Current learning rate: 0.00991
2024-12-07 13:33:04.243140: Validation loss improved from -0.47486 to -0.48508! Patience: 1/50
2024-12-07 13:33:04.244167: train_loss -0.4521
2024-12-07 13:33:04.245188: val_loss -0.4851
2024-12-07 13:33:04.245981: Pseudo dice [0.6945]
2024-12-07 13:33:04.246761: Epoch time: 391.59 s
2024-12-07 13:33:04.247447: Yayy! New best EMA pseudo Dice: 0.6174
2024-12-07 13:33:06.008016: 
2024-12-07 13:33:06.009513: Epoch 11
2024-12-07 13:33:06.010293: Current learning rate: 0.0099
2024-12-07 13:39:45.759969: Validation loss improved from -0.48508 to -0.51034! Patience: 0/50
2024-12-07 13:39:45.760943: train_loss -0.4701
2024-12-07 13:39:45.761770: val_loss -0.5103
2024-12-07 13:39:45.762537: Pseudo dice [0.7197]
2024-12-07 13:39:45.763335: Epoch time: 399.75 s
2024-12-07 13:39:45.764189: Yayy! New best EMA pseudo Dice: 0.6276
2024-12-07 13:39:47.496636: 
2024-12-07 13:39:47.497870: Epoch 12
2024-12-07 13:39:47.498625: Current learning rate: 0.00989
2024-12-07 13:46:21.952556: Validation loss did not improve from -0.51034. Patience: 1/50
2024-12-07 13:46:21.953552: train_loss -0.4782
2024-12-07 13:46:21.954529: val_loss -0.4703
2024-12-07 13:46:21.955469: Pseudo dice [0.7012]
2024-12-07 13:46:21.956200: Epoch time: 394.46 s
2024-12-07 13:46:21.957126: Yayy! New best EMA pseudo Dice: 0.635
2024-12-07 13:46:23.717463: 
2024-12-07 13:46:23.718839: Epoch 13
2024-12-07 13:46:23.719739: Current learning rate: 0.00988
2024-12-07 13:53:26.813216: Validation loss did not improve from -0.51034. Patience: 2/50
2024-12-07 13:53:26.814284: train_loss -0.4746
2024-12-07 13:53:26.815092: val_loss -0.4613
2024-12-07 13:53:26.815769: Pseudo dice [0.6923]
2024-12-07 13:53:26.816548: Epoch time: 423.1 s
2024-12-07 13:53:26.817211: Yayy! New best EMA pseudo Dice: 0.6407
2024-12-07 13:53:28.585658: 
2024-12-07 13:53:28.586931: Epoch 14
2024-12-07 13:53:28.587728: Current learning rate: 0.00987
2024-12-07 14:00:24.670864: Validation loss did not improve from -0.51034. Patience: 3/50
2024-12-07 14:00:24.672002: train_loss -0.4792
2024-12-07 14:00:24.672801: val_loss -0.5087
2024-12-07 14:00:24.673474: Pseudo dice [0.7219]
2024-12-07 14:00:24.674129: Epoch time: 416.09 s
2024-12-07 14:00:25.079297: Yayy! New best EMA pseudo Dice: 0.6488
2024-12-07 14:00:26.929608: 
2024-12-07 14:00:26.931208: Epoch 15
2024-12-07 14:00:26.932231: Current learning rate: 0.00986
2024-12-07 14:07:23.375687: Validation loss did not improve from -0.51034. Patience: 4/50
2024-12-07 14:07:23.376637: train_loss -0.4963
2024-12-07 14:07:23.377963: val_loss -0.4915
2024-12-07 14:07:23.378985: Pseudo dice [0.7046]
2024-12-07 14:07:23.379878: Epoch time: 416.45 s
2024-12-07 14:07:23.380691: Yayy! New best EMA pseudo Dice: 0.6544
2024-12-07 14:07:25.197053: 
2024-12-07 14:07:25.198521: Epoch 16
2024-12-07 14:07:25.199389: Current learning rate: 0.00986
2024-12-07 14:14:23.178133: Validation loss did not improve from -0.51034. Patience: 5/50
2024-12-07 14:14:23.179066: train_loss -0.4853
2024-12-07 14:14:23.180068: val_loss -0.4726
2024-12-07 14:14:23.180758: Pseudo dice [0.6947]
2024-12-07 14:14:23.181577: Epoch time: 417.98 s
2024-12-07 14:14:23.182333: Yayy! New best EMA pseudo Dice: 0.6585
2024-12-07 14:14:25.014491: 
2024-12-07 14:14:25.015963: Epoch 17
2024-12-07 14:14:25.016757: Current learning rate: 0.00985
2024-12-07 14:21:33.486057: Validation loss did not improve from -0.51034. Patience: 6/50
2024-12-07 14:21:33.486948: train_loss -0.5049
2024-12-07 14:21:33.487786: val_loss -0.4975
2024-12-07 14:21:33.488568: Pseudo dice [0.7058]
2024-12-07 14:21:33.489422: Epoch time: 428.47 s
2024-12-07 14:21:33.490091: Yayy! New best EMA pseudo Dice: 0.6632
2024-12-07 14:21:35.672001: 
2024-12-07 14:21:35.673601: Epoch 18
2024-12-07 14:21:35.674644: Current learning rate: 0.00984
2024-12-07 14:28:36.187191: Validation loss did not improve from -0.51034. Patience: 7/50
2024-12-07 14:28:36.189103: train_loss -0.506
2024-12-07 14:28:36.190216: val_loss -0.4847
2024-12-07 14:28:36.190930: Pseudo dice [0.7089]
2024-12-07 14:28:36.191873: Epoch time: 420.52 s
2024-12-07 14:28:36.192723: Yayy! New best EMA pseudo Dice: 0.6678
2024-12-07 14:28:38.013359: 
2024-12-07 14:28:38.014639: Epoch 19
2024-12-07 14:28:38.015359: Current learning rate: 0.00983
2024-12-07 14:35:35.689492: Validation loss did not improve from -0.51034. Patience: 8/50
2024-12-07 14:35:35.690521: train_loss -0.5122
2024-12-07 14:35:35.691378: val_loss -0.4913
2024-12-07 14:35:35.692149: Pseudo dice [0.7044]
2024-12-07 14:35:35.692900: Epoch time: 417.68 s
2024-12-07 14:35:36.098296: Yayy! New best EMA pseudo Dice: 0.6714
2024-12-07 14:35:37.922793: 
2024-12-07 14:35:37.924147: Epoch 20
2024-12-07 14:35:37.924985: Current learning rate: 0.00982
2024-12-07 14:42:32.104413: Validation loss improved from -0.51034 to -0.51584! Patience: 8/50
2024-12-07 14:42:32.105155: train_loss -0.5273
2024-12-07 14:42:32.106008: val_loss -0.5158
2024-12-07 14:42:32.106714: Pseudo dice [0.7167]
2024-12-07 14:42:32.107533: Epoch time: 414.18 s
2024-12-07 14:42:32.108236: Yayy! New best EMA pseudo Dice: 0.6759
2024-12-07 14:42:33.960125: 
2024-12-07 14:42:33.961214: Epoch 21
2024-12-07 14:42:33.962049: Current learning rate: 0.00981
2024-12-07 14:49:17.394984: Validation loss did not improve from -0.51584. Patience: 1/50
2024-12-07 14:49:17.395971: train_loss -0.5265
2024-12-07 14:49:17.396803: val_loss -0.4879
2024-12-07 14:49:17.397569: Pseudo dice [0.6971]
2024-12-07 14:49:17.398223: Epoch time: 403.44 s
2024-12-07 14:49:17.398831: Yayy! New best EMA pseudo Dice: 0.6781
2024-12-07 14:49:19.161766: 
2024-12-07 14:49:19.163063: Epoch 22
2024-12-07 14:49:19.163880: Current learning rate: 0.0098
2024-12-07 14:56:01.368220: Validation loss did not improve from -0.51584. Patience: 2/50
2024-12-07 14:56:01.369154: train_loss -0.531
2024-12-07 14:56:01.369924: val_loss -0.4948
2024-12-07 14:56:01.370632: Pseudo dice [0.7065]
2024-12-07 14:56:01.371299: Epoch time: 402.21 s
2024-12-07 14:56:01.372044: Yayy! New best EMA pseudo Dice: 0.6809
2024-12-07 14:56:03.116755: 
2024-12-07 14:56:03.118171: Epoch 23
2024-12-07 14:56:03.118968: Current learning rate: 0.00979
2024-12-07 15:02:47.080242: Validation loss improved from -0.51584 to -0.52806! Patience: 2/50
2024-12-07 15:02:47.081244: train_loss -0.5376
2024-12-07 15:02:47.082065: val_loss -0.5281
2024-12-07 15:02:47.082824: Pseudo dice [0.7243]
2024-12-07 15:02:47.083525: Epoch time: 403.97 s
2024-12-07 15:02:47.084211: Yayy! New best EMA pseudo Dice: 0.6853
2024-12-07 15:02:48.849097: 
2024-12-07 15:02:48.850468: Epoch 24
2024-12-07 15:02:48.851244: Current learning rate: 0.00978
2024-12-07 15:09:33.209947: Validation loss did not improve from -0.52806. Patience: 1/50
2024-12-07 15:09:33.210907: train_loss -0.5379
2024-12-07 15:09:33.211627: val_loss -0.4889
2024-12-07 15:09:33.212356: Pseudo dice [0.6988]
2024-12-07 15:09:33.213229: Epoch time: 404.36 s
2024-12-07 15:09:33.632082: Yayy! New best EMA pseudo Dice: 0.6866
2024-12-07 15:09:35.344436: 
2024-12-07 15:09:35.345928: Epoch 25
2024-12-07 15:09:35.346841: Current learning rate: 0.00977
2024-12-07 15:16:14.610825: Validation loss did not improve from -0.52806. Patience: 2/50
2024-12-07 15:16:14.611787: train_loss -0.5502
2024-12-07 15:16:14.612524: val_loss -0.4905
2024-12-07 15:16:14.613187: Pseudo dice [0.7036]
2024-12-07 15:16:14.613867: Epoch time: 399.27 s
2024-12-07 15:16:14.614519: Yayy! New best EMA pseudo Dice: 0.6883
2024-12-07 15:16:16.362283: 
2024-12-07 15:16:16.363563: Epoch 26
2024-12-07 15:16:16.364304: Current learning rate: 0.00977
2024-12-07 15:22:59.970265: Validation loss improved from -0.52806 to -0.54938! Patience: 2/50
2024-12-07 15:22:59.971221: train_loss -0.5486
2024-12-07 15:22:59.971998: val_loss -0.5494
2024-12-07 15:22:59.972697: Pseudo dice [0.7376]
2024-12-07 15:22:59.973379: Epoch time: 403.61 s
2024-12-07 15:22:59.974104: Yayy! New best EMA pseudo Dice: 0.6932
2024-12-07 15:23:01.747368: 
2024-12-07 15:23:01.748570: Epoch 27
2024-12-07 15:23:01.749411: Current learning rate: 0.00976
2024-12-07 15:29:38.471342: Validation loss did not improve from -0.54938. Patience: 1/50
2024-12-07 15:29:38.473067: train_loss -0.565
2024-12-07 15:29:38.473925: val_loss -0.5297
2024-12-07 15:29:38.474735: Pseudo dice [0.7278]
2024-12-07 15:29:38.475498: Epoch time: 396.73 s
2024-12-07 15:29:38.476228: Yayy! New best EMA pseudo Dice: 0.6967
2024-12-07 15:29:40.241767: 
2024-12-07 15:29:40.243000: Epoch 28
2024-12-07 15:29:40.243807: Current learning rate: 0.00975
2024-12-07 15:36:29.974082: Validation loss did not improve from -0.54938. Patience: 2/50
2024-12-07 15:36:29.974932: train_loss -0.5743
2024-12-07 15:36:29.975741: val_loss -0.5366
2024-12-07 15:36:29.976592: Pseudo dice [0.7281]
2024-12-07 15:36:29.977345: Epoch time: 409.73 s
2024-12-07 15:36:29.978168: Yayy! New best EMA pseudo Dice: 0.6998
2024-12-07 15:36:32.148074: 
2024-12-07 15:36:32.149554: Epoch 29
2024-12-07 15:36:32.150386: Current learning rate: 0.00974
2024-12-07 15:43:26.999279: Validation loss did not improve from -0.54938. Patience: 3/50
2024-12-07 15:43:27.000369: train_loss -0.5715
2024-12-07 15:43:27.001449: val_loss -0.4756
2024-12-07 15:43:27.002270: Pseudo dice [0.6977]
2024-12-07 15:43:27.003078: Epoch time: 414.85 s
2024-12-07 15:43:28.805441: 
2024-12-07 15:43:28.806987: Epoch 30
2024-12-07 15:43:28.808060: Current learning rate: 0.00973
2024-12-07 15:50:39.544338: Validation loss improved from -0.54938 to -0.55691! Patience: 3/50
2024-12-07 15:50:39.545497: train_loss -0.5604
2024-12-07 15:50:39.546513: val_loss -0.5569
2024-12-07 15:50:39.547554: Pseudo dice [0.7411]
2024-12-07 15:50:39.548571: Epoch time: 430.74 s
2024-12-07 15:50:39.549570: Yayy! New best EMA pseudo Dice: 0.7038
2024-12-07 15:50:41.346112: 
2024-12-07 15:50:41.347624: Epoch 31
2024-12-07 15:50:41.348797: Current learning rate: 0.00972
2024-12-07 15:57:29.361812: Validation loss did not improve from -0.55691. Patience: 1/50
2024-12-07 15:57:29.362820: train_loss -0.5652
2024-12-07 15:57:29.363637: val_loss -0.5552
2024-12-07 15:57:29.364412: Pseudo dice [0.7501]
2024-12-07 15:57:29.365116: Epoch time: 408.02 s
2024-12-07 15:57:29.365878: Yayy! New best EMA pseudo Dice: 0.7084
2024-12-07 15:57:31.124980: 
2024-12-07 15:57:31.126493: Epoch 32
2024-12-07 15:57:31.127258: Current learning rate: 0.00971
2024-12-07 16:04:24.627643: Validation loss did not improve from -0.55691. Patience: 2/50
2024-12-07 16:04:24.628448: train_loss -0.5757
2024-12-07 16:04:24.629322: val_loss -0.5309
2024-12-07 16:04:24.630081: Pseudo dice [0.7242]
2024-12-07 16:04:24.630806: Epoch time: 413.51 s
2024-12-07 16:04:24.631497: Yayy! New best EMA pseudo Dice: 0.71
2024-12-07 16:04:26.365880: 
2024-12-07 16:04:26.367183: Epoch 33
2024-12-07 16:04:26.368148: Current learning rate: 0.0097
2024-12-07 16:11:19.737932: Validation loss improved from -0.55691 to -0.57890! Patience: 2/50
2024-12-07 16:11:19.738893: train_loss -0.5701
2024-12-07 16:11:19.739557: val_loss -0.5789
2024-12-07 16:11:19.740318: Pseudo dice [0.7554]
2024-12-07 16:11:19.740964: Epoch time: 413.37 s
2024-12-07 16:11:19.741555: Yayy! New best EMA pseudo Dice: 0.7145
2024-12-07 16:11:21.524813: 
2024-12-07 16:11:21.525973: Epoch 34
2024-12-07 16:11:21.526775: Current learning rate: 0.00969
2024-12-07 16:18:16.938425: Validation loss did not improve from -0.57890. Patience: 1/50
2024-12-07 16:18:16.939415: train_loss -0.5802
2024-12-07 16:18:16.940514: val_loss -0.5331
2024-12-07 16:18:16.941439: Pseudo dice [0.7296]
2024-12-07 16:18:16.942309: Epoch time: 415.42 s
2024-12-07 16:18:17.344424: Yayy! New best EMA pseudo Dice: 0.716
2024-12-07 16:18:19.178834: 
2024-12-07 16:18:19.179995: Epoch 35
2024-12-07 16:18:19.180804: Current learning rate: 0.00968
2024-12-07 16:25:10.296518: Validation loss did not improve from -0.57890. Patience: 2/50
2024-12-07 16:25:10.297530: train_loss -0.5885
2024-12-07 16:25:10.298618: val_loss -0.5475
2024-12-07 16:25:10.299382: Pseudo dice [0.7412]
2024-12-07 16:25:10.300172: Epoch time: 411.12 s
2024-12-07 16:25:10.300895: Yayy! New best EMA pseudo Dice: 0.7186
2024-12-07 16:25:12.147817: 
2024-12-07 16:25:12.149097: Epoch 36
2024-12-07 16:25:12.149937: Current learning rate: 0.00968
2024-12-07 16:31:59.520216: Validation loss did not improve from -0.57890. Patience: 3/50
2024-12-07 16:31:59.521853: train_loss -0.5954
2024-12-07 16:31:59.522905: val_loss -0.5525
2024-12-07 16:31:59.523890: Pseudo dice [0.7406]
2024-12-07 16:31:59.524837: Epoch time: 407.38 s
2024-12-07 16:31:59.525718: Yayy! New best EMA pseudo Dice: 0.7208
2024-12-07 16:32:01.327697: 
2024-12-07 16:32:01.329205: Epoch 37
2024-12-07 16:32:01.330302: Current learning rate: 0.00967
2024-12-07 16:38:48.288709: Validation loss did not improve from -0.57890. Patience: 4/50
2024-12-07 16:38:48.290441: train_loss -0.5888
2024-12-07 16:38:48.291642: val_loss -0.5671
2024-12-07 16:38:48.292408: Pseudo dice [0.7459]
2024-12-07 16:38:48.293314: Epoch time: 406.96 s
2024-12-07 16:38:48.294347: Yayy! New best EMA pseudo Dice: 0.7233
2024-12-07 16:38:50.099571: 
2024-12-07 16:38:50.101030: Epoch 38
2024-12-07 16:38:50.102038: Current learning rate: 0.00966
2024-12-07 16:45:29.680041: Validation loss improved from -0.57890 to -0.57915! Patience: 4/50
2024-12-07 16:45:29.682745: train_loss -0.5882
2024-12-07 16:45:29.683624: val_loss -0.5792
2024-12-07 16:45:29.684334: Pseudo dice [0.7594]
2024-12-07 16:45:29.685076: Epoch time: 399.58 s
2024-12-07 16:45:29.685758: Yayy! New best EMA pseudo Dice: 0.7269
2024-12-07 16:45:31.450928: 
2024-12-07 16:45:31.452510: Epoch 39
2024-12-07 16:45:31.453365: Current learning rate: 0.00965
2024-12-07 16:52:31.446878: Validation loss did not improve from -0.57915. Patience: 1/50
2024-12-07 16:52:31.447689: train_loss -0.6084
2024-12-07 16:52:31.448777: val_loss -0.5489
2024-12-07 16:52:31.449725: Pseudo dice [0.7411]
2024-12-07 16:52:31.450667: Epoch time: 420.0 s
2024-12-07 16:52:31.864514: Yayy! New best EMA pseudo Dice: 0.7283
2024-12-07 16:52:34.086303: 
2024-12-07 16:52:34.087598: Epoch 40
2024-12-07 16:52:34.088586: Current learning rate: 0.00964
2024-12-07 16:59:29.048849: Validation loss did not improve from -0.57915. Patience: 2/50
2024-12-07 16:59:29.049642: train_loss -0.6039
2024-12-07 16:59:29.050573: val_loss -0.528
2024-12-07 16:59:29.051332: Pseudo dice [0.7176]
2024-12-07 16:59:29.052099: Epoch time: 414.96 s
2024-12-07 16:59:30.475576: 
2024-12-07 16:59:30.477066: Epoch 41
2024-12-07 16:59:30.478042: Current learning rate: 0.00963
2024-12-07 17:06:23.673386: Validation loss did not improve from -0.57915. Patience: 3/50
2024-12-07 17:06:23.674304: train_loss -0.6127
2024-12-07 17:06:23.675228: val_loss -0.5271
2024-12-07 17:06:23.675930: Pseudo dice [0.7274]
2024-12-07 17:06:23.676628: Epoch time: 413.2 s
2024-12-07 17:06:24.996715: 
2024-12-07 17:06:24.998134: Epoch 42
2024-12-07 17:06:24.998867: Current learning rate: 0.00962
2024-12-07 17:13:23.003676: Validation loss did not improve from -0.57915. Patience: 4/50
2024-12-07 17:13:23.004696: train_loss -0.6116
2024-12-07 17:13:23.005465: val_loss -0.5666
2024-12-07 17:13:23.006179: Pseudo dice [0.7465]
2024-12-07 17:13:23.006949: Epoch time: 418.01 s
2024-12-07 17:13:23.007580: Yayy! New best EMA pseudo Dice: 0.7292
2024-12-07 17:13:24.732029: 
2024-12-07 17:13:24.733207: Epoch 43
2024-12-07 17:13:24.733955: Current learning rate: 0.00961
2024-12-07 17:20:17.783926: Validation loss did not improve from -0.57915. Patience: 5/50
2024-12-07 17:20:17.784954: train_loss -0.6077
2024-12-07 17:20:17.785906: val_loss -0.515
2024-12-07 17:20:17.786753: Pseudo dice [0.7215]
2024-12-07 17:20:17.787543: Epoch time: 413.05 s
2024-12-07 17:20:19.107636: 
2024-12-07 17:20:19.108541: Epoch 44
2024-12-07 17:20:19.109365: Current learning rate: 0.0096
2024-12-07 17:27:20.462770: Validation loss did not improve from -0.57915. Patience: 6/50
2024-12-07 17:27:20.463898: train_loss -0.608
2024-12-07 17:27:20.464854: val_loss -0.5454
2024-12-07 17:27:20.465764: Pseudo dice [0.7378]
2024-12-07 17:27:20.466695: Epoch time: 421.36 s
2024-12-07 17:27:20.880749: Yayy! New best EMA pseudo Dice: 0.7293
2024-12-07 17:27:22.585404: 
2024-12-07 17:27:22.586645: Epoch 45
2024-12-07 17:27:22.587407: Current learning rate: 0.00959
2024-12-07 17:34:08.951572: Validation loss did not improve from -0.57915. Patience: 7/50
2024-12-07 17:34:08.952628: train_loss -0.604
2024-12-07 17:34:08.953407: val_loss -0.5586
2024-12-07 17:34:08.954173: Pseudo dice [0.7431]
2024-12-07 17:34:08.954827: Epoch time: 406.37 s
2024-12-07 17:34:08.955450: Yayy! New best EMA pseudo Dice: 0.7307
2024-12-07 17:34:10.702919: 
2024-12-07 17:34:10.704083: Epoch 46
2024-12-07 17:34:10.704853: Current learning rate: 0.00959
2024-12-07 17:40:57.562866: Validation loss did not improve from -0.57915. Patience: 8/50
2024-12-07 17:40:57.564962: train_loss -0.6168
2024-12-07 17:40:57.566168: val_loss -0.5535
2024-12-07 17:40:57.567155: Pseudo dice [0.7367]
2024-12-07 17:40:57.568103: Epoch time: 406.86 s
2024-12-07 17:40:57.568909: Yayy! New best EMA pseudo Dice: 0.7313
2024-12-07 17:40:59.318387: 
2024-12-07 17:40:59.319660: Epoch 47
2024-12-07 17:40:59.320650: Current learning rate: 0.00958
2024-12-07 17:47:41.564260: Validation loss did not improve from -0.57915. Patience: 9/50
2024-12-07 17:47:41.565252: train_loss -0.6265
2024-12-07 17:47:41.566272: val_loss -0.5285
2024-12-07 17:47:41.567187: Pseudo dice [0.7247]
2024-12-07 17:47:41.568129: Epoch time: 402.25 s
2024-12-07 17:47:42.876932: 
2024-12-07 17:47:42.878223: Epoch 48
2024-12-07 17:47:42.879089: Current learning rate: 0.00957
2024-12-07 17:54:30.532555: Validation loss did not improve from -0.57915. Patience: 10/50
2024-12-07 17:54:30.533691: train_loss -0.6228
2024-12-07 17:54:30.534459: val_loss -0.5594
2024-12-07 17:54:30.535145: Pseudo dice [0.7505]
2024-12-07 17:54:30.535859: Epoch time: 407.66 s
2024-12-07 17:54:30.536506: Yayy! New best EMA pseudo Dice: 0.7326
2024-12-07 17:54:32.282936: 
2024-12-07 17:54:32.284222: Epoch 49
2024-12-07 17:54:32.284904: Current learning rate: 0.00956
2024-12-07 18:01:11.588883: Validation loss did not improve from -0.57915. Patience: 11/50
2024-12-07 18:01:11.589760: train_loss -0.622
2024-12-07 18:01:11.590466: val_loss -0.5714
2024-12-07 18:01:11.591231: Pseudo dice [0.7514]
2024-12-07 18:01:11.591954: Epoch time: 399.31 s
2024-12-07 18:01:11.997158: Yayy! New best EMA pseudo Dice: 0.7345
2024-12-07 18:01:13.725239: 
2024-12-07 18:01:13.726582: Epoch 50
2024-12-07 18:01:13.727385: Current learning rate: 0.00955
2024-12-07 18:07:59.968689: Validation loss did not improve from -0.57915. Patience: 12/50
2024-12-07 18:07:59.969703: train_loss -0.6233
2024-12-07 18:07:59.970489: val_loss -0.5203
2024-12-07 18:07:59.971239: Pseudo dice [0.7189]
2024-12-07 18:07:59.972051: Epoch time: 406.25 s
2024-12-07 18:08:01.317011: 
2024-12-07 18:08:01.318464: Epoch 51
2024-12-07 18:08:01.319376: Current learning rate: 0.00954
2024-12-07 18:14:53.693357: Validation loss did not improve from -0.57915. Patience: 13/50
2024-12-07 18:14:53.694527: train_loss -0.6253
2024-12-07 18:14:53.695709: val_loss -0.5709
2024-12-07 18:14:53.696740: Pseudo dice [0.751]
2024-12-07 18:14:53.697747: Epoch time: 412.38 s
2024-12-07 18:14:53.698631: Yayy! New best EMA pseudo Dice: 0.7348
2024-12-07 18:14:55.823030: 
2024-12-07 18:14:55.824431: Epoch 52
2024-12-07 18:14:55.825369: Current learning rate: 0.00953
2024-12-07 18:21:52.610450: Validation loss did not improve from -0.57915. Patience: 14/50
2024-12-07 18:21:52.611549: train_loss -0.6298
2024-12-07 18:21:52.612814: val_loss -0.5747
2024-12-07 18:21:52.613817: Pseudo dice [0.7534]
2024-12-07 18:21:52.614871: Epoch time: 416.79 s
2024-12-07 18:21:52.615970: Yayy! New best EMA pseudo Dice: 0.7366
2024-12-07 18:21:54.349404: 
2024-12-07 18:21:54.350806: Epoch 53
2024-12-07 18:21:54.351645: Current learning rate: 0.00952
2024-12-07 18:28:47.359753: Validation loss did not improve from -0.57915. Patience: 15/50
2024-12-07 18:28:47.360721: train_loss -0.6357
2024-12-07 18:28:47.361590: val_loss -0.5728
2024-12-07 18:28:47.362249: Pseudo dice [0.7542]
2024-12-07 18:28:47.363075: Epoch time: 413.01 s
2024-12-07 18:28:47.363823: Yayy! New best EMA pseudo Dice: 0.7384
2024-12-07 18:28:49.097805: 
2024-12-07 18:28:49.099116: Epoch 54
2024-12-07 18:28:49.099974: Current learning rate: 0.00951
2024-12-07 18:35:54.378520: Validation loss did not improve from -0.57915. Patience: 16/50
2024-12-07 18:35:54.379508: train_loss -0.6307
2024-12-07 18:35:54.380580: val_loss -0.5636
2024-12-07 18:35:54.381585: Pseudo dice [0.7515]
2024-12-07 18:35:54.382475: Epoch time: 425.28 s
2024-12-07 18:35:54.791699: Yayy! New best EMA pseudo Dice: 0.7397
2024-12-07 18:35:56.555413: 
2024-12-07 18:35:56.556786: Epoch 55
2024-12-07 18:35:56.557757: Current learning rate: 0.0095
2024-12-07 18:43:00.151230: Validation loss did not improve from -0.57915. Patience: 17/50
2024-12-07 18:43:00.152823: train_loss -0.6339
2024-12-07 18:43:00.153777: val_loss -0.5663
2024-12-07 18:43:00.154554: Pseudo dice [0.7493]
2024-12-07 18:43:00.155339: Epoch time: 423.6 s
2024-12-07 18:43:00.156147: Yayy! New best EMA pseudo Dice: 0.7406
2024-12-07 18:43:01.911853: 
2024-12-07 18:43:01.913214: Epoch 56
2024-12-07 18:43:01.913897: Current learning rate: 0.00949
2024-12-07 18:49:44.068236: Validation loss did not improve from -0.57915. Patience: 18/50
2024-12-07 18:49:44.069355: train_loss -0.6298
2024-12-07 18:49:44.070367: val_loss -0.5174
2024-12-07 18:49:44.071139: Pseudo dice [0.7207]
2024-12-07 18:49:44.071955: Epoch time: 402.16 s
2024-12-07 18:49:45.381515: 
2024-12-07 18:49:45.382943: Epoch 57
2024-12-07 18:49:45.383956: Current learning rate: 0.00949
2024-12-07 18:56:41.286350: Validation loss did not improve from -0.57915. Patience: 19/50
2024-12-07 18:56:41.287567: train_loss -0.6335
2024-12-07 18:56:41.288428: val_loss -0.5591
2024-12-07 18:56:41.289351: Pseudo dice [0.7485]
2024-12-07 18:56:41.290267: Epoch time: 415.91 s
2024-12-07 18:56:42.649926: 
2024-12-07 18:56:42.651411: Epoch 58
2024-12-07 18:56:42.652479: Current learning rate: 0.00948
2024-12-07 19:03:39.519175: Validation loss did not improve from -0.57915. Patience: 20/50
2024-12-07 19:03:39.520145: train_loss -0.6403
2024-12-07 19:03:39.521117: val_loss -0.5736
2024-12-07 19:03:39.521984: Pseudo dice [0.7521]
2024-12-07 19:03:39.522926: Epoch time: 416.87 s
2024-12-07 19:03:39.523725: Yayy! New best EMA pseudo Dice: 0.7409
2024-12-07 19:03:41.278003: 
2024-12-07 19:03:41.279440: Epoch 59
2024-12-07 19:03:41.280239: Current learning rate: 0.00947
2024-12-07 19:10:39.957849: Validation loss did not improve from -0.57915. Patience: 21/50
2024-12-07 19:10:39.958857: train_loss -0.6441
2024-12-07 19:10:39.959703: val_loss -0.5744
2024-12-07 19:10:39.960487: Pseudo dice [0.7494]
2024-12-07 19:10:39.961264: Epoch time: 418.68 s
2024-12-07 19:10:40.375907: Yayy! New best EMA pseudo Dice: 0.7417
2024-12-07 19:10:42.123323: 
2024-12-07 19:10:42.124622: Epoch 60
2024-12-07 19:10:42.125487: Current learning rate: 0.00946
2024-12-07 19:17:43.033365: Validation loss did not improve from -0.57915. Patience: 22/50
2024-12-07 19:17:43.034450: train_loss -0.6464
2024-12-07 19:17:43.035169: val_loss -0.5657
2024-12-07 19:17:43.035802: Pseudo dice [0.7488]
2024-12-07 19:17:43.036472: Epoch time: 420.91 s
2024-12-07 19:17:43.037129: Yayy! New best EMA pseudo Dice: 0.7425
2024-12-07 19:17:44.841391: 
2024-12-07 19:17:44.842685: Epoch 61
2024-12-07 19:17:44.843474: Current learning rate: 0.00945
2024-12-07 19:24:34.220395: Validation loss did not improve from -0.57915. Patience: 23/50
2024-12-07 19:24:34.221404: train_loss -0.6466
2024-12-07 19:24:34.222278: val_loss -0.5761
2024-12-07 19:24:34.223237: Pseudo dice [0.7522]
2024-12-07 19:24:34.224055: Epoch time: 409.38 s
2024-12-07 19:24:34.224967: Yayy! New best EMA pseudo Dice: 0.7434
2024-12-07 19:24:36.349571: 
2024-12-07 19:24:36.350962: Epoch 62
2024-12-07 19:24:36.351775: Current learning rate: 0.00944
2024-12-07 19:31:47.245582: Validation loss did not improve from -0.57915. Patience: 24/50
2024-12-07 19:31:47.246355: train_loss -0.6515
2024-12-07 19:31:47.247198: val_loss -0.545
2024-12-07 19:31:47.247943: Pseudo dice [0.7411]
2024-12-07 19:31:47.248633: Epoch time: 430.9 s
2024-12-07 19:31:48.614366: 
2024-12-07 19:31:48.615645: Epoch 63
2024-12-07 19:31:48.616328: Current learning rate: 0.00943
2024-12-07 19:38:47.325312: Validation loss improved from -0.57915 to -0.57927! Patience: 24/50
2024-12-07 19:38:47.326277: train_loss -0.6536
2024-12-07 19:38:47.327011: val_loss -0.5793
2024-12-07 19:38:47.327679: Pseudo dice [0.7538]
2024-12-07 19:38:47.328399: Epoch time: 418.71 s
2024-12-07 19:38:47.329133: Yayy! New best EMA pseudo Dice: 0.7443
2024-12-07 19:38:49.211814: 
2024-12-07 19:38:49.213242: Epoch 64
2024-12-07 19:38:49.213978: Current learning rate: 0.00942
2024-12-07 19:45:38.140389: Validation loss did not improve from -0.57927. Patience: 1/50
2024-12-07 19:45:38.141336: train_loss -0.6557
2024-12-07 19:45:38.142175: val_loss -0.5392
2024-12-07 19:45:38.142888: Pseudo dice [0.7321]
2024-12-07 19:45:38.143600: Epoch time: 408.93 s
2024-12-07 19:45:39.896294: 
2024-12-07 19:45:39.897888: Epoch 65
2024-12-07 19:45:39.898995: Current learning rate: 0.00941
2024-12-07 19:52:30.571836: Validation loss improved from -0.57927 to -0.58644! Patience: 1/50
2024-12-07 19:52:30.602481: train_loss -0.6578
2024-12-07 19:52:30.604302: val_loss -0.5864
2024-12-07 19:52:30.605243: Pseudo dice [0.7633]
2024-12-07 19:52:30.606157: Epoch time: 410.71 s
2024-12-07 19:52:30.606887: Yayy! New best EMA pseudo Dice: 0.7451
2024-12-07 19:52:32.394279: 
2024-12-07 19:52:32.395602: Epoch 66
2024-12-07 19:52:32.396751: Current learning rate: 0.0094
2024-12-07 19:59:21.813950: Validation loss did not improve from -0.58644. Patience: 1/50
2024-12-07 19:59:21.815271: train_loss -0.6499
2024-12-07 19:59:21.816373: val_loss -0.5677
2024-12-07 19:59:21.817253: Pseudo dice [0.7543]
2024-12-07 19:59:21.818254: Epoch time: 409.42 s
2024-12-07 19:59:21.819046: Yayy! New best EMA pseudo Dice: 0.746
2024-12-07 19:59:23.555139: 
2024-12-07 19:59:23.556545: Epoch 67
2024-12-07 19:59:23.557291: Current learning rate: 0.00939
2024-12-07 20:06:14.265895: Validation loss did not improve from -0.58644. Patience: 2/50
2024-12-07 20:06:14.266817: train_loss -0.6519
2024-12-07 20:06:14.267612: val_loss -0.5711
2024-12-07 20:06:14.268367: Pseudo dice [0.7492]
2024-12-07 20:06:14.269118: Epoch time: 410.71 s
2024-12-07 20:06:14.269784: Yayy! New best EMA pseudo Dice: 0.7463
2024-12-07 20:06:16.053923: 
2024-12-07 20:06:16.055222: Epoch 68
2024-12-07 20:06:16.056083: Current learning rate: 0.00939
2024-12-07 20:12:59.904943: Validation loss did not improve from -0.58644. Patience: 3/50
2024-12-07 20:12:59.905695: train_loss -0.6535
2024-12-07 20:12:59.906643: val_loss -0.5314
2024-12-07 20:12:59.907319: Pseudo dice [0.7271]
2024-12-07 20:12:59.908027: Epoch time: 403.85 s
2024-12-07 20:13:01.297139: 
2024-12-07 20:13:01.298283: Epoch 69
2024-12-07 20:13:01.299188: Current learning rate: 0.00938
2024-12-07 20:19:56.798709: Validation loss did not improve from -0.58644. Patience: 4/50
2024-12-07 20:19:56.799672: train_loss -0.6539
2024-12-07 20:19:56.800555: val_loss -0.5474
2024-12-07 20:19:56.801426: Pseudo dice [0.7384]
2024-12-07 20:19:56.802396: Epoch time: 415.5 s
2024-12-07 20:19:58.625778: 
2024-12-07 20:19:58.626972: Epoch 70
2024-12-07 20:19:58.627928: Current learning rate: 0.00937
2024-12-07 20:26:53.630775: Validation loss did not improve from -0.58644. Patience: 5/50
2024-12-07 20:26:53.631838: train_loss -0.6596
2024-12-07 20:26:53.632659: val_loss -0.5785
2024-12-07 20:26:53.633404: Pseudo dice [0.7528]
2024-12-07 20:26:53.634433: Epoch time: 415.01 s
2024-12-07 20:26:55.004962: 
2024-12-07 20:26:55.006562: Epoch 71
2024-12-07 20:26:55.007613: Current learning rate: 0.00936
2024-12-07 20:33:45.134936: Validation loss did not improve from -0.58644. Patience: 6/50
2024-12-07 20:33:45.135848: train_loss -0.6541
2024-12-07 20:33:45.136819: val_loss -0.5729
2024-12-07 20:33:45.137738: Pseudo dice [0.7552]
2024-12-07 20:33:45.138641: Epoch time: 410.13 s
2024-12-07 20:33:46.504565: 
2024-12-07 20:33:46.505701: Epoch 72
2024-12-07 20:33:46.506357: Current learning rate: 0.00935
2024-12-07 20:40:29.621433: Validation loss did not improve from -0.58644. Patience: 7/50
2024-12-07 20:40:29.622481: train_loss -0.6604
2024-12-07 20:40:29.623693: val_loss -0.5624
2024-12-07 20:40:29.624730: Pseudo dice [0.7483]
2024-12-07 20:40:29.625676: Epoch time: 403.12 s
2024-12-07 20:40:31.393722: 
2024-12-07 20:40:31.394950: Epoch 73
2024-12-07 20:40:31.396108: Current learning rate: 0.00934
2024-12-07 20:47:23.213990: Validation loss did not improve from -0.58644. Patience: 8/50
2024-12-07 20:47:23.214918: train_loss -0.6579
2024-12-07 20:47:23.215792: val_loss -0.5857
2024-12-07 20:47:23.216566: Pseudo dice [0.7548]
2024-12-07 20:47:23.217378: Epoch time: 411.82 s
2024-12-07 20:47:23.218136: Yayy! New best EMA pseudo Dice: 0.7469
2024-12-07 20:47:25.021415: 
2024-12-07 20:47:25.022571: Epoch 74
2024-12-07 20:47:25.023265: Current learning rate: 0.00933
2024-12-07 20:54:14.794492: Validation loss did not improve from -0.58644. Patience: 9/50
2024-12-07 20:54:14.796576: train_loss -0.6693
2024-12-07 20:54:14.797492: val_loss -0.5801
2024-12-07 20:54:14.798162: Pseudo dice [0.7558]
2024-12-07 20:54:14.798956: Epoch time: 409.78 s
2024-12-07 20:54:15.243662: Yayy! New best EMA pseudo Dice: 0.7478
2024-12-07 20:54:17.063707: 
2024-12-07 20:54:17.065134: Epoch 75
2024-12-07 20:54:17.065903: Current learning rate: 0.00932
2024-12-07 21:01:01.353685: Validation loss did not improve from -0.58644. Patience: 10/50
2024-12-07 21:01:01.355535: train_loss -0.6714
2024-12-07 21:01:01.357076: val_loss -0.5763
2024-12-07 21:01:01.358090: Pseudo dice [0.7534]
2024-12-07 21:01:01.359052: Epoch time: 404.29 s
2024-12-07 21:01:01.359955: Yayy! New best EMA pseudo Dice: 0.7483
2024-12-07 21:01:03.213176: 
2024-12-07 21:01:03.214539: Epoch 76
2024-12-07 21:01:03.215314: Current learning rate: 0.00931
2024-12-07 21:07:59.110359: Validation loss did not improve from -0.58644. Patience: 11/50
2024-12-07 21:07:59.111282: train_loss -0.6726
2024-12-07 21:07:59.112305: val_loss -0.5605
2024-12-07 21:07:59.113278: Pseudo dice [0.743]
2024-12-07 21:07:59.114138: Epoch time: 415.9 s
2024-12-07 21:08:00.509714: 
2024-12-07 21:08:00.511181: Epoch 77
2024-12-07 21:08:00.512020: Current learning rate: 0.0093
2024-12-07 21:14:44.482622: Validation loss did not improve from -0.58644. Patience: 12/50
2024-12-07 21:14:44.483674: train_loss -0.6713
2024-12-07 21:14:44.484471: val_loss -0.5669
2024-12-07 21:14:44.485217: Pseudo dice [0.7449]
2024-12-07 21:14:44.486037: Epoch time: 403.98 s
2024-12-07 21:14:45.893685: 
2024-12-07 21:14:45.894977: Epoch 78
2024-12-07 21:14:45.895794: Current learning rate: 0.0093
2024-12-07 21:21:32.005838: Validation loss improved from -0.58644 to -0.58947! Patience: 12/50
2024-12-07 21:21:32.006756: train_loss -0.6763
2024-12-07 21:21:32.007550: val_loss -0.5895
2024-12-07 21:21:32.008308: Pseudo dice [0.7629]
2024-12-07 21:21:32.008955: Epoch time: 406.11 s
2024-12-07 21:21:32.009685: Yayy! New best EMA pseudo Dice: 0.749
2024-12-07 21:21:33.908388: 
2024-12-07 21:21:33.909781: Epoch 79
2024-12-07 21:21:33.910744: Current learning rate: 0.00929
2024-12-07 21:28:22.342020: Validation loss improved from -0.58947 to -0.59482! Patience: 0/50
2024-12-07 21:28:22.343102: train_loss -0.6778
2024-12-07 21:28:22.343863: val_loss -0.5948
2024-12-07 21:28:22.344568: Pseudo dice [0.7664]
2024-12-07 21:28:22.345198: Epoch time: 408.44 s
2024-12-07 21:28:22.758472: Yayy! New best EMA pseudo Dice: 0.7508
2024-12-07 21:28:24.564813: 
2024-12-07 21:28:24.566233: Epoch 80
2024-12-07 21:28:24.567248: Current learning rate: 0.00928
2024-12-07 21:35:12.861147: Validation loss did not improve from -0.59482. Patience: 1/50
2024-12-07 21:35:12.862198: train_loss -0.6757
2024-12-07 21:35:12.863016: val_loss -0.5754
2024-12-07 21:35:12.863731: Pseudo dice [0.7641]
2024-12-07 21:35:12.864421: Epoch time: 408.3 s
2024-12-07 21:35:12.865041: Yayy! New best EMA pseudo Dice: 0.7521
2024-12-07 21:35:15.001516: 
2024-12-07 21:35:15.003666: Epoch 81
2024-12-07 21:35:15.005092: Current learning rate: 0.00927
2024-12-07 21:42:11.041198: Validation loss did not improve from -0.59482. Patience: 2/50
2024-12-07 21:42:11.042668: train_loss -0.6822
2024-12-07 21:42:11.043476: val_loss -0.5563
2024-12-07 21:42:11.044166: Pseudo dice [0.7324]
2024-12-07 21:42:11.044910: Epoch time: 416.04 s
2024-12-07 21:42:12.559160: 
2024-12-07 21:42:12.560245: Epoch 82
2024-12-07 21:42:12.561219: Current learning rate: 0.00926
2024-12-07 21:49:00.346674: Validation loss did not improve from -0.59482. Patience: 3/50
2024-12-07 21:49:00.347499: train_loss -0.6712
2024-12-07 21:49:00.348243: val_loss -0.5662
2024-12-07 21:49:00.349305: Pseudo dice [0.7531]
2024-12-07 21:49:00.350042: Epoch time: 407.79 s
2024-12-07 21:49:01.723546: 
2024-12-07 21:49:01.724765: Epoch 83
2024-12-07 21:49:01.725548: Current learning rate: 0.00925
2024-12-07 21:55:47.368257: Validation loss did not improve from -0.59482. Patience: 4/50
2024-12-07 21:55:47.369278: train_loss -0.6851
2024-12-07 21:55:47.369984: val_loss -0.545
2024-12-07 21:55:47.370784: Pseudo dice [0.7414]
2024-12-07 21:55:47.371638: Epoch time: 405.65 s
2024-12-07 21:55:49.179985: 
2024-12-07 21:55:49.181157: Epoch 84
2024-12-07 21:55:49.182300: Current learning rate: 0.00924
2024-12-07 22:02:35.967211: Validation loss did not improve from -0.59482. Patience: 5/50
2024-12-07 22:02:35.968764: train_loss -0.6782
2024-12-07 22:02:35.970473: val_loss -0.5745
2024-12-07 22:02:35.971177: Pseudo dice [0.7519]
2024-12-07 22:02:35.972172: Epoch time: 406.79 s
2024-12-07 22:02:37.857989: 
2024-12-07 22:02:37.859482: Epoch 85
2024-12-07 22:02:37.860296: Current learning rate: 0.00923
2024-12-07 22:09:25.766532: Validation loss did not improve from -0.59482. Patience: 6/50
2024-12-07 22:09:25.767767: train_loss -0.6743
2024-12-07 22:09:25.768639: val_loss -0.5787
2024-12-07 22:09:25.769422: Pseudo dice [0.756]
2024-12-07 22:09:25.770250: Epoch time: 407.91 s
2024-12-07 22:09:27.175606: 
2024-12-07 22:09:27.176888: Epoch 86
2024-12-07 22:09:27.177706: Current learning rate: 0.00922
2024-12-07 22:16:13.233861: Validation loss did not improve from -0.59482. Patience: 7/50
2024-12-07 22:16:13.235024: train_loss -0.6788
2024-12-07 22:16:13.235973: val_loss -0.5699
2024-12-07 22:16:13.236656: Pseudo dice [0.7563]
2024-12-07 22:16:13.237409: Epoch time: 406.06 s
2024-12-07 22:16:14.640016: 
2024-12-07 22:16:14.642525: Epoch 87
2024-12-07 22:16:14.643985: Current learning rate: 0.00921
2024-12-07 22:23:10.188002: Validation loss did not improve from -0.59482. Patience: 8/50
2024-12-07 22:23:10.189155: train_loss -0.6854
2024-12-07 22:23:10.189960: val_loss -0.5606
2024-12-07 22:23:10.190654: Pseudo dice [0.7551]
2024-12-07 22:23:10.191544: Epoch time: 415.55 s
2024-12-07 22:23:11.528376: 
2024-12-07 22:23:11.529738: Epoch 88
2024-12-07 22:23:11.530582: Current learning rate: 0.0092
2024-12-07 22:30:00.913878: Validation loss did not improve from -0.59482. Patience: 9/50
2024-12-07 22:30:00.914902: train_loss -0.6876
2024-12-07 22:30:00.915831: val_loss -0.5518
2024-12-07 22:30:00.916654: Pseudo dice [0.7522]
2024-12-07 22:30:00.917563: Epoch time: 409.39 s
2024-12-07 22:30:02.232766: 
2024-12-07 22:30:02.234384: Epoch 89
2024-12-07 22:30:02.235314: Current learning rate: 0.0092
2024-12-07 22:36:55.974146: Validation loss did not improve from -0.59482. Patience: 10/50
2024-12-07 22:36:55.975083: train_loss -0.679
2024-12-07 22:36:55.975828: val_loss -0.5683
2024-12-07 22:36:55.976541: Pseudo dice [0.752]
2024-12-07 22:36:55.977283: Epoch time: 413.74 s
2024-12-07 22:36:57.704571: 
2024-12-07 22:36:57.705924: Epoch 90
2024-12-07 22:36:57.706628: Current learning rate: 0.00919
2024-12-07 22:43:44.035175: Validation loss did not improve from -0.59482. Patience: 11/50
2024-12-07 22:43:44.036054: train_loss -0.6748
2024-12-07 22:43:44.036865: val_loss -0.566
2024-12-07 22:43:44.037611: Pseudo dice [0.746]
2024-12-07 22:43:44.038234: Epoch time: 406.33 s
2024-12-07 22:43:45.373715: 
2024-12-07 22:43:45.374493: Epoch 91
2024-12-07 22:43:45.375238: Current learning rate: 0.00918
2024-12-07 22:50:56.803885: Validation loss did not improve from -0.59482. Patience: 12/50
2024-12-07 22:50:56.804737: train_loss -0.6847
2024-12-07 22:50:56.805522: val_loss -0.5918
2024-12-07 22:50:56.806232: Pseudo dice [0.7647]
2024-12-07 22:50:56.806982: Epoch time: 431.43 s
2024-12-07 22:50:56.807765: Yayy! New best EMA pseudo Dice: 0.7524
2024-12-07 22:50:58.528558: 
2024-12-07 22:50:58.530003: Epoch 92
2024-12-07 22:50:58.530811: Current learning rate: 0.00917
2024-12-07 22:57:56.214593: Validation loss did not improve from -0.59482. Patience: 13/50
2024-12-07 22:57:56.215443: train_loss -0.6864
2024-12-07 22:57:56.216209: val_loss -0.5729
2024-12-07 22:57:56.216943: Pseudo dice [0.7622]
2024-12-07 22:57:56.217731: Epoch time: 417.69 s
2024-12-07 22:57:56.218467: Yayy! New best EMA pseudo Dice: 0.7533
2024-12-07 22:57:57.927185: 
2024-12-07 22:57:57.928122: Epoch 93
2024-12-07 22:57:57.928987: Current learning rate: 0.00916
2024-12-07 23:04:49.166244: Validation loss improved from -0.59482 to -0.59567! Patience: 13/50
2024-12-07 23:04:49.167750: train_loss -0.6945
2024-12-07 23:04:49.168427: val_loss -0.5957
2024-12-07 23:04:49.169125: Pseudo dice [0.7742]
2024-12-07 23:04:49.169882: Epoch time: 411.24 s
2024-12-07 23:04:49.170547: Yayy! New best EMA pseudo Dice: 0.7554
2024-12-07 23:04:50.907068: 
2024-12-07 23:04:50.908057: Epoch 94
2024-12-07 23:04:50.908855: Current learning rate: 0.00915
2024-12-07 23:11:39.386290: Validation loss did not improve from -0.59567. Patience: 1/50
2024-12-07 23:11:39.387291: train_loss -0.6867
2024-12-07 23:11:39.388105: val_loss -0.5375
2024-12-07 23:11:39.388819: Pseudo dice [0.7368]
2024-12-07 23:11:39.389609: Epoch time: 408.48 s
2024-12-07 23:11:41.675831: 
2024-12-07 23:11:41.677125: Epoch 95
2024-12-07 23:11:41.677879: Current learning rate: 0.00914
2024-12-07 23:18:37.532247: Validation loss improved from -0.59567 to -0.59853! Patience: 1/50
2024-12-07 23:18:37.533352: train_loss -0.7025
2024-12-07 23:18:37.534373: val_loss -0.5985
2024-12-07 23:18:37.535350: Pseudo dice [0.7784]
2024-12-07 23:18:37.536276: Epoch time: 415.86 s
2024-12-07 23:18:37.537193: Yayy! New best EMA pseudo Dice: 0.756
2024-12-07 23:18:39.203458: 
2024-12-07 23:18:39.204662: Epoch 96
2024-12-07 23:18:39.205318: Current learning rate: 0.00913
2024-12-07 23:25:26.492526: Validation loss did not improve from -0.59853. Patience: 1/50
2024-12-07 23:25:26.493432: train_loss -0.6941
2024-12-07 23:25:26.494393: val_loss -0.5732
2024-12-07 23:25:26.495296: Pseudo dice [0.7572]
2024-12-07 23:25:26.496342: Epoch time: 407.29 s
2024-12-07 23:25:26.497362: Yayy! New best EMA pseudo Dice: 0.7562
2024-12-07 23:25:28.272158: 
2024-12-07 23:25:28.273454: Epoch 97
2024-12-07 23:25:28.274178: Current learning rate: 0.00912
2024-12-07 23:32:26.653836: Validation loss did not improve from -0.59853. Patience: 2/50
2024-12-07 23:32:26.654799: train_loss -0.7002
2024-12-07 23:32:26.655586: val_loss -0.5572
2024-12-07 23:32:26.656279: Pseudo dice [0.7486]
2024-12-07 23:32:26.657166: Epoch time: 418.38 s
2024-12-07 23:32:28.000453: 
2024-12-07 23:32:28.001724: Epoch 98
2024-12-07 23:32:28.002438: Current learning rate: 0.00911
2024-12-07 23:39:24.252521: Validation loss did not improve from -0.59853. Patience: 3/50
2024-12-07 23:39:24.253504: train_loss -0.6961
2024-12-07 23:39:24.254287: val_loss -0.5847
2024-12-07 23:39:24.255139: Pseudo dice [0.7645]
2024-12-07 23:39:24.255786: Epoch time: 416.25 s
2024-12-07 23:39:24.256520: Yayy! New best EMA pseudo Dice: 0.7563
2024-12-07 23:39:26.002144: 
2024-12-07 23:39:26.003596: Epoch 99
2024-12-07 23:39:26.004559: Current learning rate: 0.0091
2024-12-07 23:46:21.491309: Validation loss did not improve from -0.59853. Patience: 4/50
2024-12-07 23:46:21.492224: train_loss -0.7021
2024-12-07 23:46:21.493104: val_loss -0.5957
2024-12-07 23:46:21.493842: Pseudo dice [0.765]
2024-12-07 23:46:21.494658: Epoch time: 415.49 s
2024-12-07 23:46:21.918033: Yayy! New best EMA pseudo Dice: 0.7572
2024-12-07 23:46:23.637572: 
2024-12-07 23:46:23.638968: Epoch 100
2024-12-07 23:46:23.639749: Current learning rate: 0.0091
2024-12-07 23:53:12.385778: Validation loss did not improve from -0.59853. Patience: 5/50
2024-12-07 23:53:12.386747: train_loss -0.6996
2024-12-07 23:53:12.387640: val_loss -0.5692
2024-12-07 23:53:12.388536: Pseudo dice [0.7583]
2024-12-07 23:53:12.389247: Epoch time: 408.75 s
2024-12-07 23:53:12.389901: Yayy! New best EMA pseudo Dice: 0.7573
2024-12-07 23:53:14.159196: 
2024-12-07 23:53:14.160680: Epoch 101
2024-12-07 23:53:14.161648: Current learning rate: 0.00909
2024-12-07 23:59:58.325550: Validation loss did not improve from -0.59853. Patience: 6/50
2024-12-07 23:59:58.326583: train_loss -0.6931
2024-12-07 23:59:58.327424: val_loss -0.5527
2024-12-07 23:59:58.328264: Pseudo dice [0.7491]
2024-12-07 23:59:58.329031: Epoch time: 404.17 s
2024-12-07 23:59:59.653517: 
2024-12-07 23:59:59.654859: Epoch 102
2024-12-07 23:59:59.655749: Current learning rate: 0.00908
2024-12-08 00:06:46.281701: Validation loss did not improve from -0.59853. Patience: 7/50
2024-12-08 00:06:46.283679: train_loss -0.6909
2024-12-08 00:06:46.285650: val_loss -0.5721
2024-12-08 00:06:46.286822: Pseudo dice [0.7529]
2024-12-08 00:06:46.287925: Epoch time: 406.63 s
2024-12-08 00:06:47.742609: 
2024-12-08 00:06:47.744052: Epoch 103
2024-12-08 00:06:47.745199: Current learning rate: 0.00907
2024-12-08 00:13:38.777798: Validation loss did not improve from -0.59853. Patience: 8/50
2024-12-08 00:13:38.780742: train_loss -0.7069
2024-12-08 00:13:38.781736: val_loss -0.5795
2024-12-08 00:13:38.782426: Pseudo dice [0.7586]
2024-12-08 00:13:38.783510: Epoch time: 411.04 s
2024-12-08 00:13:40.136350: 
2024-12-08 00:13:40.137690: Epoch 104
2024-12-08 00:13:40.138558: Current learning rate: 0.00906
2024-12-08 00:20:26.159473: Validation loss improved from -0.59853 to -0.60341! Patience: 8/50
2024-12-08 00:20:26.161919: train_loss -0.7035
2024-12-08 00:20:26.162678: val_loss -0.6034
2024-12-08 00:20:26.163362: Pseudo dice [0.7741]
2024-12-08 00:20:26.164049: Epoch time: 406.03 s
2024-12-08 00:20:26.592451: Yayy! New best EMA pseudo Dice: 0.7582
2024-12-08 00:20:28.300050: 
2024-12-08 00:20:28.301351: Epoch 105
2024-12-08 00:20:28.302094: Current learning rate: 0.00905
2024-12-08 00:27:11.587161: Validation loss did not improve from -0.60341. Patience: 1/50
2024-12-08 00:27:11.588156: train_loss -0.7095
2024-12-08 00:27:11.589290: val_loss -0.5877
2024-12-08 00:27:11.590261: Pseudo dice [0.7643]
2024-12-08 00:27:11.591041: Epoch time: 403.29 s
2024-12-08 00:27:11.591792: Yayy! New best EMA pseudo Dice: 0.7588
2024-12-08 00:27:13.338353: 
2024-12-08 00:27:13.339519: Epoch 106
2024-12-08 00:27:13.340312: Current learning rate: 0.00904
2024-12-08 00:34:10.581321: Validation loss did not improve from -0.60341. Patience: 2/50
2024-12-08 00:34:10.582357: train_loss -0.7152
2024-12-08 00:34:10.583157: val_loss -0.5921
2024-12-08 00:34:10.583954: Pseudo dice [0.7684]
2024-12-08 00:34:10.584599: Epoch time: 417.25 s
2024-12-08 00:34:10.585306: Yayy! New best EMA pseudo Dice: 0.7597
2024-12-08 00:34:12.958222: 
2024-12-08 00:34:12.959821: Epoch 107
2024-12-08 00:34:12.960950: Current learning rate: 0.00903
2024-12-08 00:41:09.070775: Validation loss improved from -0.60341 to -0.61103! Patience: 2/50
2024-12-08 00:41:09.071769: train_loss -0.7083
2024-12-08 00:41:09.072759: val_loss -0.611
2024-12-08 00:41:09.073444: Pseudo dice [0.7747]
2024-12-08 00:41:09.074215: Epoch time: 416.11 s
2024-12-08 00:41:09.075015: Yayy! New best EMA pseudo Dice: 0.7612
2024-12-08 00:41:10.815739: 
2024-12-08 00:41:10.816994: Epoch 108
2024-12-08 00:41:10.817937: Current learning rate: 0.00902
2024-12-08 00:48:15.169201: Validation loss did not improve from -0.61103. Patience: 1/50
2024-12-08 00:48:15.170238: train_loss -0.7107
2024-12-08 00:48:15.171106: val_loss -0.578
2024-12-08 00:48:15.172062: Pseudo dice [0.7596]
2024-12-08 00:48:15.172940: Epoch time: 424.36 s
2024-12-08 00:48:16.497808: 
2024-12-08 00:48:16.499248: Epoch 109
2024-12-08 00:48:16.500089: Current learning rate: 0.00901
2024-12-08 00:55:22.724281: Validation loss did not improve from -0.61103. Patience: 2/50
2024-12-08 00:55:22.725308: train_loss -0.7108
2024-12-08 00:55:22.726184: val_loss -0.5416
2024-12-08 00:55:22.726900: Pseudo dice [0.7504]
2024-12-08 00:55:22.727600: Epoch time: 426.23 s
2024-12-08 00:55:24.478484: 
2024-12-08 00:55:24.479982: Epoch 110
2024-12-08 00:55:24.480724: Current learning rate: 0.009
2024-12-08 01:02:37.900012: Validation loss did not improve from -0.61103. Patience: 3/50
2024-12-08 01:02:37.901019: train_loss -0.7079
2024-12-08 01:02:37.901750: val_loss -0.5706
2024-12-08 01:02:37.902528: Pseudo dice [0.754]
2024-12-08 01:02:37.903175: Epoch time: 433.42 s
2024-12-08 01:02:39.235135: 
2024-12-08 01:02:39.236540: Epoch 111
2024-12-08 01:02:39.237751: Current learning rate: 0.009
2024-12-08 01:09:54.788637: Validation loss did not improve from -0.61103. Patience: 4/50
2024-12-08 01:09:54.789556: train_loss -0.7069
2024-12-08 01:09:54.790520: val_loss -0.5896
2024-12-08 01:09:54.791396: Pseudo dice [0.7614]
2024-12-08 01:09:54.792393: Epoch time: 435.56 s
2024-12-08 01:09:56.111424: 
2024-12-08 01:09:56.112731: Epoch 112
2024-12-08 01:09:56.113411: Current learning rate: 0.00899
2024-12-08 01:17:15.590165: Validation loss did not improve from -0.61103. Patience: 5/50
2024-12-08 01:17:15.591567: train_loss -0.712
2024-12-08 01:17:15.592499: val_loss -0.5875
2024-12-08 01:17:15.593139: Pseudo dice [0.7723]
2024-12-08 01:17:15.593904: Epoch time: 439.48 s
2024-12-08 01:17:16.919950: 
2024-12-08 01:17:16.921183: Epoch 113
2024-12-08 01:17:16.921940: Current learning rate: 0.00898
2024-12-08 01:24:42.439015: Validation loss did not improve from -0.61103. Patience: 6/50
2024-12-08 01:24:42.440965: train_loss -0.7128
2024-12-08 01:24:42.441943: val_loss -0.5979
2024-12-08 01:24:42.442667: Pseudo dice [0.7725]
2024-12-08 01:24:42.443410: Epoch time: 445.52 s
2024-12-08 01:24:42.444194: Yayy! New best EMA pseudo Dice: 0.762
2024-12-08 01:24:44.202666: 
2024-12-08 01:24:44.204038: Epoch 114
2024-12-08 01:24:44.205006: Current learning rate: 0.00897
2024-12-08 01:32:09.272848: Validation loss did not improve from -0.61103. Patience: 7/50
2024-12-08 01:32:09.273812: train_loss -0.7108
2024-12-08 01:32:09.274536: val_loss -0.5773
2024-12-08 01:32:09.275214: Pseudo dice [0.7678]
2024-12-08 01:32:09.275956: Epoch time: 445.07 s
2024-12-08 01:32:09.692012: Yayy! New best EMA pseudo Dice: 0.7626
2024-12-08 01:32:11.422146: 
2024-12-08 01:32:11.423542: Epoch 115
2024-12-08 01:32:11.424300: Current learning rate: 0.00896
2024-12-08 01:39:38.347487: Validation loss did not improve from -0.61103. Patience: 8/50
2024-12-08 01:39:38.348596: train_loss -0.7053
2024-12-08 01:39:38.349406: val_loss -0.5903
2024-12-08 01:39:38.350124: Pseudo dice [0.7692]
2024-12-08 01:39:38.350851: Epoch time: 446.93 s
2024-12-08 01:39:38.351511: Yayy! New best EMA pseudo Dice: 0.7633
2024-12-08 01:39:40.096670: 
2024-12-08 01:39:40.098104: Epoch 116
2024-12-08 01:39:40.098840: Current learning rate: 0.00895
2024-12-08 01:47:07.014518: Validation loss did not improve from -0.61103. Patience: 9/50
2024-12-08 01:47:07.015475: train_loss -0.7066
2024-12-08 01:47:07.016682: val_loss -0.5915
2024-12-08 01:47:07.017843: Pseudo dice [0.7601]
2024-12-08 01:47:07.018980: Epoch time: 446.92 s
2024-12-08 01:47:08.717526: 
2024-12-08 01:47:08.718861: Epoch 117
2024-12-08 01:47:08.719976: Current learning rate: 0.00894
2024-12-08 01:54:32.597561: Validation loss did not improve from -0.61103. Patience: 10/50
2024-12-08 01:54:32.598548: train_loss -0.7123
2024-12-08 01:54:32.599184: val_loss -0.5543
2024-12-08 01:54:32.599945: Pseudo dice [0.7545]
2024-12-08 01:54:32.600722: Epoch time: 443.88 s
2024-12-08 01:54:33.942177: 
2024-12-08 01:54:33.943540: Epoch 118
2024-12-08 01:54:33.944385: Current learning rate: 0.00893
2024-12-08 02:01:59.424782: Validation loss did not improve from -0.61103. Patience: 11/50
2024-12-08 02:01:59.425690: train_loss -0.7185
2024-12-08 02:01:59.426625: val_loss -0.5635
2024-12-08 02:01:59.427391: Pseudo dice [0.7541]
2024-12-08 02:01:59.428058: Epoch time: 445.48 s
2024-12-08 02:02:00.778076: 
2024-12-08 02:02:00.779634: Epoch 119
2024-12-08 02:02:00.780589: Current learning rate: 0.00892
2024-12-08 02:09:09.245192: Validation loss did not improve from -0.61103. Patience: 12/50
2024-12-08 02:09:09.246103: train_loss -0.7141
2024-12-08 02:09:09.246995: val_loss -0.6057
2024-12-08 02:09:09.247764: Pseudo dice [0.7711]
2024-12-08 02:09:09.248558: Epoch time: 428.47 s
2024-12-08 02:09:10.986067: 
2024-12-08 02:09:10.987361: Epoch 120
2024-12-08 02:09:10.988159: Current learning rate: 0.00891
2024-12-08 02:15:31.552084: failed to log:  (<class 'FileNotFoundError'>, FileNotFoundError(2, 'No such file or directory'), <traceback object at 0x7fcdd0a28d00>)
2024-12-08 02:15:31.552084: failed to log:  (<class 'FileNotFoundError'>, FileNotFoundError(2, 'No such file or directory'), <traceback object at 0x7fcdd0a28d00>)
2024-12-08 02:15:31.552084: failed to log:  (<class 'FileNotFoundError'>, FileNotFoundError(2, 'No such file or directory'), <traceback object at 0x7fcdd0a28d00>)
2024-12-08 02:15:31.552084: failed to log:  (<class 'FileNotFoundError'>, FileNotFoundError(2, 'No such file or directory'), <traceback object at 0x7fcdd0a28d00>)
2024-12-08 02:15:31.552084: failed to log:  (<class 'FileNotFoundError'>, FileNotFoundError(2, 'No such file or directory'), <traceback object at 0x7fcdd0a28d00>)
2024-12-08 02:15:31.552084: Validation loss did not improve from -0.61103. Patience: 13/50
2024-12-08 02:15:34.057377: failed to log:  (<class 'FileNotFoundError'>, FileNotFoundError(2, 'No such file or directory'), <traceback object at 0x7fcdd0a28d00>)
2024-12-08 02:15:34.057377: failed to log:  (<class 'FileNotFoundError'>, FileNotFoundError(2, 'No such file or directory'), <traceback object at 0x7fcdd0a28d00>)
2024-12-08 02:15:34.057377: failed to log:  (<class 'FileNotFoundError'>, FileNotFoundError(2, 'No such file or directory'), <traceback object at 0x7fcdd0a28d00>)
2024-12-08 02:15:34.057377: failed to log:  (<class 'FileNotFoundError'>, FileNotFoundError(2, 'No such file or directory'), <traceback object at 0x7fcdd0a28d00>)
2024-12-08 02:15:34.057377: failed to log:  (<class 'FileNotFoundError'>, FileNotFoundError(2, 'No such file or directory'), <traceback object at 0x7fcdd0a28d00>)
2024-12-08 02:15:34.057377: train_loss -0.7232
2024-12-08 02:15:36.562237: failed to log:  (<class 'FileNotFoundError'>, FileNotFoundError(2, 'No such file or directory'), <traceback object at 0x7fcdc03de480>)
2024-12-08 02:15:36.562237: failed to log:  (<class 'FileNotFoundError'>, FileNotFoundError(2, 'No such file or directory'), <traceback object at 0x7fcdc03de480>)
2024-12-08 02:15:36.562237: failed to log:  (<class 'FileNotFoundError'>, FileNotFoundError(2, 'No such file or directory'), <traceback object at 0x7fcdc03de480>)
2024-12-08 02:15:36.562237: failed to log:  (<class 'FileNotFoundError'>, FileNotFoundError(2, 'No such file or directory'), <traceback object at 0x7fcdc03de480>)
2024-12-08 02:15:36.562237: failed to log:  (<class 'FileNotFoundError'>, FileNotFoundError(2, 'No such file or directory'), <traceback object at 0x7fcdc03de480>)
2024-12-08 02:15:36.562237: val_loss -0.6087
2024-12-08 02:15:39.067075: failed to log:  (<class 'FileNotFoundError'>, FileNotFoundError(2, 'No such file or directory'), <traceback object at 0x7fcddc67ee80>)
2024-12-08 02:15:39.067075: failed to log:  (<class 'FileNotFoundError'>, FileNotFoundError(2, 'No such file or directory'), <traceback object at 0x7fcddc67ee80>)
2024-12-08 02:15:39.067075: failed to log:  (<class 'FileNotFoundError'>, FileNotFoundError(2, 'No such file or directory'), <traceback object at 0x7fcddc67ee80>)
2024-12-08 02:15:39.067075: failed to log:  (<class 'FileNotFoundError'>, FileNotFoundError(2, 'No such file or directory'), <traceback object at 0x7fcddc67ee80>)
2024-12-08 02:15:39.067075: failed to log:  (<class 'FileNotFoundError'>, FileNotFoundError(2, 'No such file or directory'), <traceback object at 0x7fcddc67ee80>)
2024-12-08 02:15:39.067075: Pseudo dice [0.7791]
2024-12-08 02:15:41.572472: failed to log:  (<class 'FileNotFoundError'>, FileNotFoundError(2, 'No such file or directory'), <traceback object at 0x7fcdccd199c0>)
2024-12-08 02:15:41.572472: failed to log:  (<class 'FileNotFoundError'>, FileNotFoundError(2, 'No such file or directory'), <traceback object at 0x7fcdccd199c0>)
2024-12-08 02:15:41.572472: failed to log:  (<class 'FileNotFoundError'>, FileNotFoundError(2, 'No such file or directory'), <traceback object at 0x7fcdccd199c0>)
2024-12-08 02:15:41.572472: failed to log:  (<class 'FileNotFoundError'>, FileNotFoundError(2, 'No such file or directory'), <traceback object at 0x7fcdccd199c0>)
2024-12-08 02:15:41.572472: failed to log:  (<class 'FileNotFoundError'>, FileNotFoundError(2, 'No such file or directory'), <traceback object at 0x7fcdccd199c0>)
2024-12-08 02:15:41.572472: Epoch time: 383.07 s
2024-12-08 02:15:44.077123: failed to log:  (<class 'FileNotFoundError'>, FileNotFoundError(2, 'No such file or directory'), <traceback object at 0x7fcd99bde040>)
2024-12-08 02:15:44.077123: failed to log:  (<class 'FileNotFoundError'>, FileNotFoundError(2, 'No such file or directory'), <traceback object at 0x7fcd99bde040>)
2024-12-08 02:15:44.077123: failed to log:  (<class 'FileNotFoundError'>, FileNotFoundError(2, 'No such file or directory'), <traceback object at 0x7fcd99bde040>)
2024-12-08 02:15:44.077123: failed to log:  (<class 'FileNotFoundError'>, FileNotFoundError(2, 'No such file or directory'), <traceback object at 0x7fcd99bde040>)
2024-12-08 02:15:44.077123: failed to log:  (<class 'FileNotFoundError'>, FileNotFoundError(2, 'No such file or directory'), <traceback object at 0x7fcd99bde040>)
2024-12-08 02:15:44.077123: Yayy! New best EMA pseudo Dice: 0.764
Traceback (most recent call last):
  File "/home/gridsan/nchutisilp/.conda/envs/nnunet/bin/nnUNetv2_train", line 8, in <module>
    sys.exit(run_training_entry())
  File "/home/gridsan/nchutisilp/projects/nnUNet/nnunetv2/run/run_training.py", line 275, in run_training_entry
    run_training(args.dataset_name_or_id, args.configuration, args.fold, args.tr, args.p, args.pretrained_weights,
  File "/home/gridsan/nchutisilp/projects/nnUNet/nnunetv2/run/run_training.py", line 211, in run_training
    nnunet_trainer.run_training()
  File "/home/gridsan/nchutisilp/projects/nnUNet/nnunetv2/training/nnUNetTrainer/nnUNetTrainer.py", line 1411, in run_training
    self.on_epoch_end()
  File "/home/gridsan/nchutisilp/projects/nnUNet/nnunetv2/training/nnUNetTrainer/nnUNetTrainer.py", line 1171, in on_epoch_end
    self.save_checkpoint(join(self.output_folder, 'checkpoint_best.pth'))
  File "/home/gridsan/nchutisilp/projects/nnUNet/nnunetv2/training/nnUNetTrainer/nnUNetTrainer.py", line 1201, in save_checkpoint
    torch.save(checkpoint, filename)
  File "/home/gridsan/nchutisilp/.local/lib/python3.9/site-packages/torch/serialization.py", line 627, in save
    with _open_zipfile_writer(f) as opened_zipfile:
  File "/home/gridsan/nchutisilp/.local/lib/python3.9/site-packages/torch/serialization.py", line 501, in _open_zipfile_writer
    return container(name_or_buffer)
  File "/home/gridsan/nchutisilp/.local/lib/python3.9/site-packages/torch/serialization.py", line 472, in __init__
    super().__init__(torch._C.PyTorchFileWriter(self.name))
RuntimeError: Parent directory /home/gridsan/nchutisilp/datasets/nnUNet_Datasets/nnUNet_results/Dataset307_Sohee_Calcium_OCT_CrossValidation/nnUNetTrainer__nnUNetPlans__3d_32x160x128_b10/fold_2 does not exist.
Exception in thread Thread-2:
Traceback (most recent call last):
  File "/home/gridsan/nchutisilp/.conda/envs/nnunet/lib/python3.9/threading.py", line 980, in _bootstrap_inner
Exception in thread Thread-3:
Traceback (most recent call last):
  File "/home/gridsan/nchutisilp/.conda/envs/nnunet/lib/python3.9/threading.py", line 980, in _bootstrap_inner
2024-12-07 12:18:06.299693: unpacking done...
2024-12-07 12:18:06.310533: Unable to plot network architecture: nnUNet_compile is enabled!
2024-12-07 12:18:06.356685: 
2024-12-07 12:18:06.358119: Epoch 0
2024-12-07 12:18:06.359286: Current learning rate: 0.01
2024-12-07 12:25:55.919611: Validation loss improved from 1000.00000 to -0.15880! Patience: 0/50
2024-12-07 12:25:55.920616: train_loss -0.0896
2024-12-07 12:25:55.921575: val_loss -0.1588
2024-12-07 12:25:55.922338: Pseudo dice [0.5135]
2024-12-07 12:25:55.923160: Epoch time: 469.57 s
2024-12-07 12:25:55.923981: Yayy! New best EMA pseudo Dice: 0.5135
2024-12-07 12:25:57.583355: 
2024-12-07 12:25:57.584888: Epoch 1
2024-12-07 12:25:57.585921: Current learning rate: 0.00999
2024-12-07 12:32:59.300551: Validation loss improved from -0.15880 to -0.19432! Patience: 0/50
2024-12-07 12:32:59.301559: train_loss -0.2398
2024-12-07 12:32:59.302248: val_loss -0.1943
2024-12-07 12:32:59.302888: Pseudo dice [0.528]
2024-12-07 12:32:59.303536: Epoch time: 421.72 s
2024-12-07 12:32:59.304312: Yayy! New best EMA pseudo Dice: 0.515
2024-12-07 12:33:01.074126: 
2024-12-07 12:33:01.075493: Epoch 2
2024-12-07 12:33:01.076208: Current learning rate: 0.00998
2024-12-07 12:39:30.549000: Validation loss improved from -0.19432 to -0.22900! Patience: 0/50
2024-12-07 12:39:30.550160: train_loss -0.293
2024-12-07 12:39:30.551212: val_loss -0.229
2024-12-07 12:39:30.551983: Pseudo dice [0.5454]
2024-12-07 12:39:30.552809: Epoch time: 389.48 s
2024-12-07 12:39:30.553490: Yayy! New best EMA pseudo Dice: 0.518
2024-12-07 12:39:32.379423: 
2024-12-07 12:39:32.380801: Epoch 3
2024-12-07 12:39:32.381613: Current learning rate: 0.00997
2024-12-07 12:46:03.441181: Validation loss did not improve from -0.22900. Patience: 1/50
2024-12-07 12:46:03.442269: train_loss -0.3336
2024-12-07 12:46:03.443326: val_loss -0.1943
2024-12-07 12:46:03.444197: Pseudo dice [0.5238]
2024-12-07 12:46:03.445190: Epoch time: 391.06 s
2024-12-07 12:46:03.446188: Yayy! New best EMA pseudo Dice: 0.5186
2024-12-07 12:46:05.201140: 
2024-12-07 12:46:05.202595: Epoch 4
2024-12-07 12:46:05.203424: Current learning rate: 0.00996
2024-12-07 12:52:37.493857: Validation loss improved from -0.22900 to -0.27758! Patience: 1/50
2024-12-07 12:52:37.494919: train_loss -0.3761
2024-12-07 12:52:37.496130: val_loss -0.2776
2024-12-07 12:52:37.497054: Pseudo dice [0.5781]
2024-12-07 12:52:37.497943: Epoch time: 392.3 s
2024-12-07 12:52:37.993190: Yayy! New best EMA pseudo Dice: 0.5246
2024-12-07 12:52:39.840463: 
2024-12-07 12:52:39.841891: Epoch 5
2024-12-07 12:52:39.842813: Current learning rate: 0.00995
2024-12-07 12:59:10.002855: Validation loss did not improve from -0.27758. Patience: 1/50
2024-12-07 12:59:10.003845: train_loss -0.3794
2024-12-07 12:59:10.004728: val_loss -0.1409
2024-12-07 12:59:10.005711: Pseudo dice [0.4962]
2024-12-07 12:59:10.006831: Epoch time: 390.16 s
2024-12-07 12:59:11.315415: 
2024-12-07 12:59:11.316653: Epoch 6
2024-12-07 12:59:11.317389: Current learning rate: 0.00995
2024-12-07 13:05:20.639244: Validation loss did not improve from -0.27758. Patience: 2/50
2024-12-07 13:05:20.641390: train_loss -0.4278
2024-12-07 13:05:20.642292: val_loss -0.2686
2024-12-07 13:05:20.642974: Pseudo dice [0.5785]
2024-12-07 13:05:20.643750: Epoch time: 369.33 s
2024-12-07 13:05:20.644419: Yayy! New best EMA pseudo Dice: 0.5274
2024-12-07 13:05:22.415352: 
2024-12-07 13:05:22.416104: Epoch 7
2024-12-07 13:05:22.416780: Current learning rate: 0.00994
2024-12-07 13:11:41.721121: Validation loss improved from -0.27758 to -0.34665! Patience: 2/50
2024-12-07 13:11:41.722176: train_loss -0.4403
2024-12-07 13:11:41.723277: val_loss -0.3467
2024-12-07 13:11:41.724393: Pseudo dice [0.6429]
2024-12-07 13:11:41.725393: Epoch time: 379.31 s
2024-12-07 13:11:41.726388: Yayy! New best EMA pseudo Dice: 0.539
2024-12-07 13:11:44.157275: 
2024-12-07 13:11:44.158596: Epoch 8
2024-12-07 13:11:44.159515: Current learning rate: 0.00993
2024-12-07 13:17:51.022082: Validation loss improved from -0.34665 to -0.39049! Patience: 0/50
2024-12-07 13:17:51.023185: train_loss -0.4566
2024-12-07 13:17:51.023903: val_loss -0.3905
2024-12-07 13:17:51.024583: Pseudo dice [0.6632]
2024-12-07 13:17:51.025393: Epoch time: 366.87 s
2024-12-07 13:17:51.026122: Yayy! New best EMA pseudo Dice: 0.5514
2024-12-07 13:17:52.879157: 
2024-12-07 13:17:52.880281: Epoch 9
2024-12-07 13:17:52.881147: Current learning rate: 0.00992
2024-12-07 13:23:58.969155: Validation loss improved from -0.39049 to -0.42035! Patience: 0/50
2024-12-07 13:23:58.971909: train_loss -0.4773
2024-12-07 13:23:58.973578: val_loss -0.4204
2024-12-07 13:23:58.974569: Pseudo dice [0.6877]
2024-12-07 13:23:58.975579: Epoch time: 366.09 s
2024-12-07 13:23:59.387171: Yayy! New best EMA pseudo Dice: 0.565
2024-12-07 13:24:01.168364: 
2024-12-07 13:24:01.169951: Epoch 10
2024-12-07 13:24:01.170939: Current learning rate: 0.00991
2024-12-07 13:30:14.007954: Validation loss did not improve from -0.42035. Patience: 1/50
2024-12-07 13:30:14.009926: train_loss -0.4837
2024-12-07 13:30:14.011187: val_loss -0.3931
2024-12-07 13:30:14.012197: Pseudo dice [0.6792]
2024-12-07 13:30:14.013163: Epoch time: 372.84 s
2024-12-07 13:30:14.014059: Yayy! New best EMA pseudo Dice: 0.5764
2024-12-07 13:30:15.785299: 
2024-12-07 13:30:15.786758: Epoch 11
2024-12-07 13:30:15.787580: Current learning rate: 0.0099
2024-12-07 13:36:36.995925: Validation loss did not improve from -0.42035. Patience: 2/50
2024-12-07 13:36:36.996975: train_loss -0.4849
2024-12-07 13:36:36.997871: val_loss -0.304
2024-12-07 13:36:36.998588: Pseudo dice [0.6227]
2024-12-07 13:36:36.999341: Epoch time: 381.21 s
2024-12-07 13:36:37.000027: Yayy! New best EMA pseudo Dice: 0.5811
2024-12-07 13:36:38.768231: 
2024-12-07 13:36:38.769438: Epoch 12
2024-12-07 13:36:38.770210: Current learning rate: 0.00989
2024-12-07 13:42:48.885244: Validation loss did not improve from -0.42035. Patience: 3/50
2024-12-07 13:42:48.886355: train_loss -0.5046
2024-12-07 13:42:48.887134: val_loss -0.3443
2024-12-07 13:42:48.887883: Pseudo dice [0.6393]
2024-12-07 13:42:48.888715: Epoch time: 370.12 s
2024-12-07 13:42:48.889534: Yayy! New best EMA pseudo Dice: 0.5869
2024-12-07 13:42:50.670278: 
2024-12-07 13:42:50.671637: Epoch 13
2024-12-07 13:42:50.672696: Current learning rate: 0.00988
2024-12-07 13:49:20.945724: Validation loss did not improve from -0.42035. Patience: 4/50
2024-12-07 13:49:20.946774: train_loss -0.5003
2024-12-07 13:49:20.947568: val_loss -0.403
2024-12-07 13:49:20.948198: Pseudo dice [0.6792]
2024-12-07 13:49:20.948931: Epoch time: 390.28 s
2024-12-07 13:49:20.949670: Yayy! New best EMA pseudo Dice: 0.5961
2024-12-07 13:49:22.744289: 
2024-12-07 13:49:22.745509: Epoch 14
2024-12-07 13:49:22.746357: Current learning rate: 0.00987
2024-12-07 13:55:55.541104: Validation loss did not improve from -0.42035. Patience: 5/50
2024-12-07 13:55:55.542148: train_loss -0.5021
2024-12-07 13:55:55.542945: val_loss -0.308
2024-12-07 13:55:55.543605: Pseudo dice [0.5901]
2024-12-07 13:55:55.544333: Epoch time: 392.8 s
2024-12-07 13:55:57.423541: 
2024-12-07 13:55:57.424824: Epoch 15
2024-12-07 13:55:57.425552: Current learning rate: 0.00986
2024-12-07 14:02:20.463973: Validation loss did not improve from -0.42035. Patience: 6/50
2024-12-07 14:02:20.465007: train_loss -0.5179
2024-12-07 14:02:20.465963: val_loss -0.3673
2024-12-07 14:02:20.466707: Pseudo dice [0.6421]
2024-12-07 14:02:20.467484: Epoch time: 383.04 s
2024-12-07 14:02:20.468153: Yayy! New best EMA pseudo Dice: 0.6002
2024-12-07 14:02:22.290572: 
2024-12-07 14:02:22.291807: Epoch 16
2024-12-07 14:02:22.292547: Current learning rate: 0.00986
2024-12-07 14:08:46.587338: Validation loss did not improve from -0.42035. Patience: 7/50
2024-12-07 14:08:46.588257: train_loss -0.5304
2024-12-07 14:08:46.589018: val_loss -0.3394
2024-12-07 14:08:46.589687: Pseudo dice [0.6454]
2024-12-07 14:08:46.590432: Epoch time: 384.3 s
2024-12-07 14:08:46.591083: Yayy! New best EMA pseudo Dice: 0.6047
2024-12-07 14:08:48.422191: 
2024-12-07 14:08:48.423497: Epoch 17
2024-12-07 14:08:48.424232: Current learning rate: 0.00985
2024-12-07 14:15:09.502846: Validation loss did not improve from -0.42035. Patience: 8/50
2024-12-07 14:15:09.504114: train_loss -0.5358
2024-12-07 14:15:09.505050: val_loss -0.3748
2024-12-07 14:15:09.505972: Pseudo dice [0.6538]
2024-12-07 14:15:09.506948: Epoch time: 381.08 s
2024-12-07 14:15:09.507727: Yayy! New best EMA pseudo Dice: 0.6096
2024-12-07 14:15:11.368256: 
2024-12-07 14:15:11.369522: Epoch 18
2024-12-07 14:15:11.370527: Current learning rate: 0.00984
2024-12-07 14:21:21.958698: Validation loss did not improve from -0.42035. Patience: 9/50
2024-12-07 14:21:21.960205: train_loss -0.5346
2024-12-07 14:21:21.961167: val_loss -0.3126
2024-12-07 14:21:21.961940: Pseudo dice [0.6178]
2024-12-07 14:21:21.962679: Epoch time: 370.59 s
2024-12-07 14:21:21.963434: Yayy! New best EMA pseudo Dice: 0.6104
2024-12-07 14:21:24.377843: 
2024-12-07 14:21:24.379401: Epoch 19
2024-12-07 14:21:24.380368: Current learning rate: 0.00983
2024-12-07 14:27:56.753595: Validation loss improved from -0.42035 to -0.43718! Patience: 9/50
2024-12-07 14:27:56.754765: train_loss -0.5529
2024-12-07 14:27:56.756300: val_loss -0.4372
2024-12-07 14:27:56.757098: Pseudo dice [0.6949]
2024-12-07 14:27:56.758045: Epoch time: 392.38 s
2024-12-07 14:27:57.164748: Yayy! New best EMA pseudo Dice: 0.6189
2024-12-07 14:27:58.986343: 
2024-12-07 14:27:58.987437: Epoch 20
2024-12-07 14:27:58.988242: Current learning rate: 0.00982
2024-12-07 14:34:53.718623: Validation loss did not improve from -0.43718. Patience: 1/50
2024-12-07 14:34:53.721381: train_loss -0.5383
2024-12-07 14:34:53.722502: val_loss -0.3567
2024-12-07 14:34:53.723332: Pseudo dice [0.6482]
2024-12-07 14:34:53.724175: Epoch time: 414.74 s
2024-12-07 14:34:53.724945: Yayy! New best EMA pseudo Dice: 0.6218
2024-12-07 14:34:55.610311: 
2024-12-07 14:34:55.611506: Epoch 21
2024-12-07 14:34:55.612508: Current learning rate: 0.00981
2024-12-07 14:41:28.653849: Validation loss did not improve from -0.43718. Patience: 2/50
2024-12-07 14:41:28.654845: train_loss -0.5439
2024-12-07 14:41:28.655673: val_loss -0.3168
2024-12-07 14:41:28.656361: Pseudo dice [0.6241]
2024-12-07 14:41:28.657064: Epoch time: 393.05 s
2024-12-07 14:41:28.657692: Yayy! New best EMA pseudo Dice: 0.622
2024-12-07 14:41:30.474357: 
2024-12-07 14:41:30.475763: Epoch 22
2024-12-07 14:41:30.476491: Current learning rate: 0.0098
2024-12-07 14:48:00.125760: Validation loss did not improve from -0.43718. Patience: 3/50
2024-12-07 14:48:00.126835: train_loss -0.5567
2024-12-07 14:48:00.127679: val_loss -0.3394
2024-12-07 14:48:00.128459: Pseudo dice [0.6414]
2024-12-07 14:48:00.129132: Epoch time: 389.65 s
2024-12-07 14:48:00.129787: Yayy! New best EMA pseudo Dice: 0.624
2024-12-07 14:48:01.910785: 
2024-12-07 14:48:01.911971: Epoch 23
2024-12-07 14:48:01.912772: Current learning rate: 0.00979
2024-12-07 14:54:56.186474: Validation loss did not improve from -0.43718. Patience: 4/50
2024-12-07 14:54:56.187156: train_loss -0.5513
2024-12-07 14:54:56.188185: val_loss -0.3818
2024-12-07 14:54:56.188868: Pseudo dice [0.6558]
2024-12-07 14:54:56.189509: Epoch time: 414.28 s
2024-12-07 14:54:56.190155: Yayy! New best EMA pseudo Dice: 0.6272
2024-12-07 14:54:57.941185: 
2024-12-07 14:54:57.942523: Epoch 24
2024-12-07 14:54:57.943377: Current learning rate: 0.00978
2024-12-07 15:01:37.494751: Validation loss did not improve from -0.43718. Patience: 5/50
2024-12-07 15:01:37.495539: train_loss -0.56
2024-12-07 15:01:37.496629: val_loss -0.4234
2024-12-07 15:01:37.497686: Pseudo dice [0.6863]
2024-12-07 15:01:37.498970: Epoch time: 399.56 s
2024-12-07 15:01:37.910724: Yayy! New best EMA pseudo Dice: 0.6331
2024-12-07 15:01:39.664114: 
2024-12-07 15:01:39.665864: Epoch 25
2024-12-07 15:01:39.667050: Current learning rate: 0.00977
2024-12-07 15:08:26.999293: Validation loss did not improve from -0.43718. Patience: 6/50
2024-12-07 15:08:27.000310: train_loss -0.5688
2024-12-07 15:08:27.001191: val_loss -0.4117
2024-12-07 15:08:27.001855: Pseudo dice [0.6822]
2024-12-07 15:08:27.002565: Epoch time: 407.34 s
2024-12-07 15:08:27.003365: Yayy! New best EMA pseudo Dice: 0.638
2024-12-07 15:08:28.777095: 
2024-12-07 15:08:28.778329: Epoch 26
2024-12-07 15:08:28.779087: Current learning rate: 0.00977
2024-12-07 15:15:20.268706: Validation loss did not improve from -0.43718. Patience: 7/50
2024-12-07 15:15:20.269490: train_loss -0.5772
2024-12-07 15:15:20.270441: val_loss -0.356
2024-12-07 15:15:20.271168: Pseudo dice [0.6489]
2024-12-07 15:15:20.271797: Epoch time: 411.49 s
2024-12-07 15:15:20.272662: Yayy! New best EMA pseudo Dice: 0.6391
2024-12-07 15:15:22.061186: 
2024-12-07 15:15:22.062383: Epoch 27
2024-12-07 15:15:22.063092: Current learning rate: 0.00976
2024-12-07 15:21:50.291056: Validation loss did not improve from -0.43718. Patience: 8/50
2024-12-07 15:21:50.292141: train_loss -0.5786
2024-12-07 15:21:50.293125: val_loss -0.1123
2024-12-07 15:21:50.294007: Pseudo dice [0.5237]
2024-12-07 15:21:50.294876: Epoch time: 388.23 s
2024-12-07 15:21:51.667190: 
2024-12-07 15:21:51.668573: Epoch 28
2024-12-07 15:21:51.669316: Current learning rate: 0.00975
2024-12-07 15:28:44.960105: Validation loss did not improve from -0.43718. Patience: 9/50
2024-12-07 15:28:44.961174: train_loss -0.5888
2024-12-07 15:28:44.962108: val_loss -0.3685
2024-12-07 15:28:44.962879: Pseudo dice [0.6552]
2024-12-07 15:28:44.963617: Epoch time: 413.3 s
2024-12-07 15:28:47.023212: 
2024-12-07 15:28:47.024733: Epoch 29
2024-12-07 15:28:47.025799: Current learning rate: 0.00974
2024-12-07 15:35:45.055126: Validation loss did not improve from -0.43718. Patience: 10/50
2024-12-07 15:35:45.057949: train_loss -0.5865
2024-12-07 15:35:45.060159: val_loss -0.279
2024-12-07 15:35:45.061077: Pseudo dice [0.5986]
2024-12-07 15:35:45.062207: Epoch time: 418.04 s
2024-12-07 15:35:46.901918: 
2024-12-07 15:35:46.903375: Epoch 30
2024-12-07 15:35:46.904529: Current learning rate: 0.00973
2024-12-07 15:42:50.111114: Validation loss did not improve from -0.43718. Patience: 11/50
2024-12-07 15:42:50.125115: train_loss -0.582
2024-12-07 15:42:50.126146: val_loss -0.3964
2024-12-07 15:42:50.126910: Pseudo dice [0.6758]
2024-12-07 15:42:50.127603: Epoch time: 423.22 s
2024-12-07 15:42:51.544632: 
2024-12-07 15:42:51.546000: Epoch 31
2024-12-07 15:42:51.546794: Current learning rate: 0.00972
2024-12-07 15:49:40.479303: Validation loss did not improve from -0.43718. Patience: 12/50
2024-12-07 15:49:40.480352: train_loss -0.5823
2024-12-07 15:49:40.481289: val_loss -0.3262
2024-12-07 15:49:40.482217: Pseudo dice [0.6315]
2024-12-07 15:49:40.483110: Epoch time: 408.94 s
2024-12-07 15:49:41.890001: 
2024-12-07 15:49:41.891479: Epoch 32
2024-12-07 15:49:41.892305: Current learning rate: 0.00971
2024-12-07 15:56:36.380489: Validation loss did not improve from -0.43718. Patience: 13/50
2024-12-07 15:56:36.381538: train_loss -0.6005
2024-12-07 15:56:36.382368: val_loss -0.3428
2024-12-07 15:56:36.383134: Pseudo dice [0.6224]
2024-12-07 15:56:36.383812: Epoch time: 414.49 s
2024-12-07 15:56:37.825595: 
2024-12-07 15:56:37.826676: Epoch 33
2024-12-07 15:56:37.827539: Current learning rate: 0.0097
2024-12-07 16:03:30.767138: Validation loss did not improve from -0.43718. Patience: 14/50
2024-12-07 16:03:30.768338: train_loss -0.6115
2024-12-07 16:03:30.769356: val_loss -0.3934
2024-12-07 16:03:30.770244: Pseudo dice [0.6693]
2024-12-07 16:03:30.771160: Epoch time: 412.94 s
2024-12-07 16:03:32.184364: 
2024-12-07 16:03:32.185805: Epoch 34
2024-12-07 16:03:32.186598: Current learning rate: 0.00969
2024-12-07 16:10:29.272667: Validation loss did not improve from -0.43718. Patience: 15/50
2024-12-07 16:10:29.273723: train_loss -0.5943
2024-12-07 16:10:29.274438: val_loss -0.3381
2024-12-07 16:10:29.275118: Pseudo dice [0.6397]
2024-12-07 16:10:29.275779: Epoch time: 417.09 s
2024-12-07 16:10:31.132060: 
2024-12-07 16:10:31.133456: Epoch 35
2024-12-07 16:10:31.134214: Current learning rate: 0.00968
2024-12-07 16:17:24.184550: Validation loss did not improve from -0.43718. Patience: 16/50
2024-12-07 16:17:24.185313: train_loss -0.6146
2024-12-07 16:17:24.186106: val_loss -0.2765
2024-12-07 16:17:24.186903: Pseudo dice [0.5885]
2024-12-07 16:17:24.187582: Epoch time: 413.05 s
2024-12-07 16:17:25.609444: 
2024-12-07 16:17:25.610844: Epoch 36
2024-12-07 16:17:25.611512: Current learning rate: 0.00968
2024-12-07 16:23:39.017610: Validation loss did not improve from -0.43718. Patience: 17/50
2024-12-07 16:23:39.018548: train_loss -0.6068
2024-12-07 16:23:39.019336: val_loss -0.4086
2024-12-07 16:23:39.020250: Pseudo dice [0.6785]
2024-12-07 16:23:39.021128: Epoch time: 373.41 s
2024-12-07 16:23:40.451903: 
2024-12-07 16:23:40.453120: Epoch 37
2024-12-07 16:23:40.454010: Current learning rate: 0.00967
2024-12-07 16:30:41.324383: Validation loss did not improve from -0.43718. Patience: 18/50
2024-12-07 16:30:41.325365: train_loss -0.621
2024-12-07 16:30:41.326060: val_loss -0.4005
2024-12-07 16:30:41.326716: Pseudo dice [0.6783]
2024-12-07 16:30:41.327412: Epoch time: 420.87 s
2024-12-07 16:30:41.328031: Yayy! New best EMA pseudo Dice: 0.6397
2024-12-07 16:30:43.149939: 
2024-12-07 16:30:43.151409: Epoch 38
2024-12-07 16:30:43.152201: Current learning rate: 0.00966
2024-12-07 16:37:49.630657: Validation loss did not improve from -0.43718. Patience: 19/50
2024-12-07 16:37:49.631851: train_loss -0.6124
2024-12-07 16:37:49.633532: val_loss -0.3178
2024-12-07 16:37:49.634530: Pseudo dice [0.6423]
2024-12-07 16:37:49.635551: Epoch time: 426.48 s
2024-12-07 16:37:49.636384: Yayy! New best EMA pseudo Dice: 0.64
2024-12-07 16:37:51.488597: 
2024-12-07 16:37:51.489782: Epoch 39
2024-12-07 16:37:51.490598: Current learning rate: 0.00965
2024-12-07 16:44:18.894959: Validation loss did not improve from -0.43718. Patience: 20/50
2024-12-07 16:44:18.896348: train_loss -0.6167
2024-12-07 16:44:18.897090: val_loss -0.3543
2024-12-07 16:44:18.897859: Pseudo dice [0.6572]
2024-12-07 16:44:18.898606: Epoch time: 387.41 s
2024-12-07 16:44:19.335524: Yayy! New best EMA pseudo Dice: 0.6417
2024-12-07 16:44:21.687291: 
2024-12-07 16:44:21.688595: Epoch 40
2024-12-07 16:44:21.689392: Current learning rate: 0.00964
2024-12-07 16:51:14.167324: Validation loss did not improve from -0.43718. Patience: 21/50
2024-12-07 16:51:14.168320: train_loss -0.6257
2024-12-07 16:51:14.169259: val_loss -0.3918
2024-12-07 16:51:14.170129: Pseudo dice [0.6751]
2024-12-07 16:51:14.170937: Epoch time: 412.48 s
2024-12-07 16:51:14.171833: Yayy! New best EMA pseudo Dice: 0.645
2024-12-07 16:51:16.012041: 
2024-12-07 16:51:16.013231: Epoch 41
2024-12-07 16:51:16.013877: Current learning rate: 0.00963
2024-12-07 16:58:16.474131: Validation loss did not improve from -0.43718. Patience: 22/50
2024-12-07 16:58:16.475035: train_loss -0.6229
2024-12-07 16:58:16.475834: val_loss -0.3266
2024-12-07 16:58:16.476667: Pseudo dice [0.6274]
2024-12-07 16:58:16.477432: Epoch time: 420.46 s
2024-12-07 16:58:17.880895: 
2024-12-07 16:58:17.882185: Epoch 42
2024-12-07 16:58:17.883016: Current learning rate: 0.00962
2024-12-07 17:05:20.592957: Validation loss did not improve from -0.43718. Patience: 23/50
2024-12-07 17:05:20.593840: train_loss -0.6157
2024-12-07 17:05:20.594699: val_loss -0.4308
2024-12-07 17:05:20.595493: Pseudo dice [0.6861]
2024-12-07 17:05:20.596297: Epoch time: 422.71 s
2024-12-07 17:05:20.596978: Yayy! New best EMA pseudo Dice: 0.6476
2024-12-07 17:05:22.406192: 
2024-12-07 17:05:22.407340: Epoch 43
2024-12-07 17:05:22.408109: Current learning rate: 0.00961
2024-12-07 17:12:03.557819: Validation loss did not improve from -0.43718. Patience: 24/50
2024-12-07 17:12:03.558769: train_loss -0.6135
2024-12-07 17:12:03.559455: val_loss -0.3745
2024-12-07 17:12:03.560137: Pseudo dice [0.6529]
2024-12-07 17:12:03.560800: Epoch time: 401.15 s
2024-12-07 17:12:03.561571: Yayy! New best EMA pseudo Dice: 0.6481
2024-12-07 17:12:05.317297: 
2024-12-07 17:12:05.318770: Epoch 44
2024-12-07 17:12:05.319540: Current learning rate: 0.0096
2024-12-07 17:19:01.303931: Validation loss did not improve from -0.43718. Patience: 25/50
2024-12-07 17:19:01.304853: train_loss -0.627
2024-12-07 17:19:01.305538: val_loss -0.3708
2024-12-07 17:19:01.306319: Pseudo dice [0.6504]
2024-12-07 17:19:01.306969: Epoch time: 415.99 s
2024-12-07 17:19:01.714831: Yayy! New best EMA pseudo Dice: 0.6483
2024-12-07 17:19:03.457446: 
2024-12-07 17:19:03.458297: Epoch 45
2024-12-07 17:19:03.458905: Current learning rate: 0.00959
2024-12-07 17:25:47.011492: Validation loss did not improve from -0.43718. Patience: 26/50
2024-12-07 17:25:47.012537: train_loss -0.632
2024-12-07 17:25:47.013759: val_loss -0.4145
2024-12-07 17:25:47.014863: Pseudo dice [0.6798]
2024-12-07 17:25:47.015723: Epoch time: 403.56 s
2024-12-07 17:25:47.016719: Yayy! New best EMA pseudo Dice: 0.6515
2024-12-07 17:25:48.789903: 
2024-12-07 17:25:48.790966: Epoch 46
2024-12-07 17:25:48.791610: Current learning rate: 0.00959
2024-12-07 17:32:37.969687: Validation loss did not improve from -0.43718. Patience: 27/50
2024-12-07 17:32:37.970826: train_loss -0.6333
2024-12-07 17:32:37.971890: val_loss -0.3644
2024-12-07 17:32:37.973035: Pseudo dice [0.6485]
2024-12-07 17:32:37.974114: Epoch time: 409.18 s
2024-12-07 17:32:39.347736: 
2024-12-07 17:32:39.349257: Epoch 47
2024-12-07 17:32:39.350096: Current learning rate: 0.00958
2024-12-07 17:39:23.522091: Validation loss did not improve from -0.43718. Patience: 28/50
2024-12-07 17:39:23.524164: train_loss -0.6363
2024-12-07 17:39:23.525023: val_loss -0.4288
2024-12-07 17:39:23.525679: Pseudo dice [0.6897]
2024-12-07 17:39:23.526495: Epoch time: 404.18 s
2024-12-07 17:39:23.527200: Yayy! New best EMA pseudo Dice: 0.655
2024-12-07 17:39:25.266308: 
2024-12-07 17:39:25.267733: Epoch 48
2024-12-07 17:39:25.268641: Current learning rate: 0.00957
2024-12-07 17:46:21.580690: Validation loss improved from -0.43718 to -0.45699! Patience: 28/50
2024-12-07 17:46:21.583457: train_loss -0.6408
2024-12-07 17:46:21.585371: val_loss -0.457
2024-12-07 17:46:21.586221: Pseudo dice [0.7079]
2024-12-07 17:46:21.587219: Epoch time: 416.32 s
2024-12-07 17:46:21.588153: Yayy! New best EMA pseudo Dice: 0.6603
2024-12-07 17:46:23.360212: 
2024-12-07 17:46:23.361386: Epoch 49
2024-12-07 17:46:23.362156: Current learning rate: 0.00956
2024-12-07 17:53:10.143398: Validation loss did not improve from -0.45699. Patience: 1/50
2024-12-07 17:53:10.159564: train_loss -0.6299
2024-12-07 17:53:10.160615: val_loss -0.3869
2024-12-07 17:53:10.161415: Pseudo dice [0.6697]
2024-12-07 17:53:10.162115: Epoch time: 406.8 s
2024-12-07 17:53:10.596858: Yayy! New best EMA pseudo Dice: 0.6613
2024-12-07 17:53:12.391258: 
2024-12-07 17:53:12.392497: Epoch 50
2024-12-07 17:53:12.393255: Current learning rate: 0.00955
2024-12-07 18:00:09.046214: Validation loss did not improve from -0.45699. Patience: 2/50
2024-12-07 18:00:09.047193: train_loss -0.6417
2024-12-07 18:00:09.048155: val_loss -0.4235
2024-12-07 18:00:09.048925: Pseudo dice [0.6833]
2024-12-07 18:00:09.049689: Epoch time: 416.66 s
2024-12-07 18:00:09.050405: Yayy! New best EMA pseudo Dice: 0.6635
2024-12-07 18:00:11.811127: 
2024-12-07 18:00:11.812610: Epoch 51
2024-12-07 18:00:11.813507: Current learning rate: 0.00954
2024-12-07 18:06:52.446572: Validation loss did not improve from -0.45699. Patience: 3/50
2024-12-07 18:06:52.447540: train_loss -0.6423
2024-12-07 18:06:52.448320: val_loss -0.3505
2024-12-07 18:06:52.448983: Pseudo dice [0.6581]
2024-12-07 18:06:52.449666: Epoch time: 400.64 s
2024-12-07 18:06:53.807742: 
2024-12-07 18:06:53.809236: Epoch 52
2024-12-07 18:06:53.810101: Current learning rate: 0.00953
2024-12-07 18:13:46.274747: Validation loss did not improve from -0.45699. Patience: 4/50
2024-12-07 18:13:46.275812: train_loss -0.6429
2024-12-07 18:13:46.276684: val_loss -0.4022
2024-12-07 18:13:46.277508: Pseudo dice [0.6709]
2024-12-07 18:13:46.278237: Epoch time: 412.47 s
2024-12-07 18:13:46.278894: Yayy! New best EMA pseudo Dice: 0.6637
2024-12-07 18:13:48.039559: 
2024-12-07 18:13:48.040964: Epoch 53
2024-12-07 18:13:48.041831: Current learning rate: 0.00952
2024-12-07 18:20:41.752151: Validation loss did not improve from -0.45699. Patience: 5/50
2024-12-07 18:20:41.753065: train_loss -0.6424
2024-12-07 18:20:41.753997: val_loss -0.4367
2024-12-07 18:20:41.754914: Pseudo dice [0.704]
2024-12-07 18:20:41.755665: Epoch time: 413.71 s
2024-12-07 18:20:41.756448: Yayy! New best EMA pseudo Dice: 0.6677
2024-12-07 18:20:43.522639: 
2024-12-07 18:20:43.523801: Epoch 54
2024-12-07 18:20:43.524551: Current learning rate: 0.00951
2024-12-07 18:27:31.480970: Validation loss did not improve from -0.45699. Patience: 6/50
2024-12-07 18:27:31.482061: train_loss -0.6507
2024-12-07 18:27:31.483054: val_loss -0.2754
2024-12-07 18:27:31.483983: Pseudo dice [0.6298]
2024-12-07 18:27:31.484928: Epoch time: 407.96 s
2024-12-07 18:27:33.270325: 
2024-12-07 18:27:33.271878: Epoch 55
2024-12-07 18:27:33.272969: Current learning rate: 0.0095
2024-12-07 18:34:27.285522: Validation loss did not improve from -0.45699. Patience: 7/50
2024-12-07 18:34:27.286667: train_loss -0.6635
2024-12-07 18:34:27.287544: val_loss -0.3559
2024-12-07 18:34:27.288445: Pseudo dice [0.6529]
2024-12-07 18:34:27.289357: Epoch time: 414.02 s
2024-12-07 18:34:28.667480: 
2024-12-07 18:34:28.669047: Epoch 56
2024-12-07 18:34:28.670020: Current learning rate: 0.00949
2024-12-07 18:41:18.226158: Validation loss did not improve from -0.45699. Patience: 8/50
2024-12-07 18:41:18.227197: train_loss -0.6493
2024-12-07 18:41:18.227938: val_loss -0.2091
2024-12-07 18:41:18.228590: Pseudo dice [0.5702]
2024-12-07 18:41:18.229339: Epoch time: 409.56 s
2024-12-07 18:41:19.598984: 
2024-12-07 18:41:19.600350: Epoch 57
2024-12-07 18:41:19.601095: Current learning rate: 0.00949
2024-12-07 18:48:14.208453: Validation loss did not improve from -0.45699. Patience: 9/50
2024-12-07 18:48:14.210559: train_loss -0.6547
2024-12-07 18:48:14.212295: val_loss -0.353
2024-12-07 18:48:14.213077: Pseudo dice [0.6579]
2024-12-07 18:48:14.213967: Epoch time: 414.61 s
2024-12-07 18:48:15.592828: 
2024-12-07 18:48:15.594322: Epoch 58
2024-12-07 18:48:15.595089: Current learning rate: 0.00948
2024-12-07 18:55:03.707679: Validation loss did not improve from -0.45699. Patience: 10/50
2024-12-07 18:55:03.711214: train_loss -0.6572
2024-12-07 18:55:03.711906: val_loss -0.4187
2024-12-07 18:55:03.712562: Pseudo dice [0.6863]
2024-12-07 18:55:03.713315: Epoch time: 408.12 s
2024-12-07 18:55:05.083674: 
2024-12-07 18:55:05.085234: Epoch 59
2024-12-07 18:55:05.086363: Current learning rate: 0.00947
2024-12-07 19:01:42.071721: Validation loss did not improve from -0.45699. Patience: 11/50
2024-12-07 19:01:42.072682: train_loss -0.6573
2024-12-07 19:01:42.073420: val_loss -0.4036
2024-12-07 19:01:42.074141: Pseudo dice [0.6736]
2024-12-07 19:01:42.074936: Epoch time: 396.99 s
2024-12-07 19:01:43.909459: 
2024-12-07 19:01:43.910929: Epoch 60
2024-12-07 19:01:43.912011: Current learning rate: 0.00946
2024-12-07 19:08:33.269066: Validation loss did not improve from -0.45699. Patience: 12/50
2024-12-07 19:08:33.270107: train_loss -0.6567
2024-12-07 19:08:33.271112: val_loss -0.3492
2024-12-07 19:08:33.272086: Pseudo dice [0.6397]
2024-12-07 19:08:33.272981: Epoch time: 409.36 s
2024-12-07 19:08:34.671744: 
2024-12-07 19:08:34.673240: Epoch 61
2024-12-07 19:08:34.674337: Current learning rate: 0.00945
2024-12-07 19:15:51.990623: Validation loss did not improve from -0.45699. Patience: 13/50
2024-12-07 19:15:51.991713: train_loss -0.6553
2024-12-07 19:15:51.992458: val_loss -0.442
2024-12-07 19:15:51.993258: Pseudo dice [0.7017]
2024-12-07 19:15:51.993942: Epoch time: 437.32 s
2024-12-07 19:15:54.121886: 
2024-12-07 19:15:54.123194: Epoch 62
2024-12-07 19:15:54.123986: Current learning rate: 0.00944
2024-12-07 19:22:29.719309: Validation loss did not improve from -0.45699. Patience: 14/50
2024-12-07 19:22:29.720495: train_loss -0.6584
2024-12-07 19:22:29.721365: val_loss -0.4301
2024-12-07 19:22:29.722377: Pseudo dice [0.6902]
2024-12-07 19:22:29.723374: Epoch time: 395.6 s
2024-12-07 19:22:31.094226: 
2024-12-07 19:22:31.095986: Epoch 63
2024-12-07 19:22:31.097016: Current learning rate: 0.00943
2024-12-07 19:29:41.750638: Validation loss did not improve from -0.45699. Patience: 15/50
2024-12-07 19:29:41.751703: train_loss -0.655
2024-12-07 19:29:41.752501: val_loss -0.4038
2024-12-07 19:29:41.753178: Pseudo dice [0.6635]
2024-12-07 19:29:41.753891: Epoch time: 430.66 s
2024-12-07 19:29:43.158299: 
2024-12-07 19:29:43.159706: Epoch 64
2024-12-07 19:29:43.160471: Current learning rate: 0.00942
2024-12-07 19:36:49.218075: Validation loss did not improve from -0.45699. Patience: 16/50
2024-12-07 19:36:49.219104: train_loss -0.6681
2024-12-07 19:36:49.219968: val_loss -0.3869
2024-12-07 19:36:49.220752: Pseudo dice [0.6702]
2024-12-07 19:36:49.221590: Epoch time: 426.06 s
2024-12-07 19:36:51.026495: 
2024-12-07 19:36:51.027626: Epoch 65
2024-12-07 19:36:51.028331: Current learning rate: 0.00941
2024-12-07 19:43:23.290870: Validation loss did not improve from -0.45699. Patience: 17/50
2024-12-07 19:43:23.291899: train_loss -0.6634
2024-12-07 19:43:23.292806: val_loss -0.3188
2024-12-07 19:43:23.293744: Pseudo dice [0.6253]
2024-12-07 19:43:23.294543: Epoch time: 392.27 s
2024-12-07 19:43:24.717329: 
2024-12-07 19:43:24.718914: Epoch 66
2024-12-07 19:43:24.720093: Current learning rate: 0.0094
2024-12-07 19:50:18.201943: Validation loss did not improve from -0.45699. Patience: 18/50
2024-12-07 19:50:18.203688: train_loss -0.6644
2024-12-07 19:50:18.204731: val_loss -0.2217
2024-12-07 19:50:18.205704: Pseudo dice [0.5758]
2024-12-07 19:50:18.206698: Epoch time: 413.49 s
2024-12-07 19:50:19.624543: 
2024-12-07 19:50:19.625938: Epoch 67
2024-12-07 19:50:19.626892: Current learning rate: 0.00939
2024-12-07 19:57:07.650776: Validation loss did not improve from -0.45699. Patience: 19/50
2024-12-07 19:57:07.654566: train_loss -0.6612
2024-12-07 19:57:07.655900: val_loss -0.1746
2024-12-07 19:57:07.656683: Pseudo dice [0.5507]
2024-12-07 19:57:07.657821: Epoch time: 408.03 s
2024-12-07 19:57:09.094018: 
2024-12-07 19:57:09.095325: Epoch 68
2024-12-07 19:57:09.096020: Current learning rate: 0.00939
2024-12-07 20:03:37.990500: Validation loss did not improve from -0.45699. Patience: 20/50
2024-12-07 20:03:37.992743: train_loss -0.6684
2024-12-07 20:03:37.993593: val_loss -0.3364
2024-12-07 20:03:37.994495: Pseudo dice [0.6474]
2024-12-07 20:03:37.995280: Epoch time: 388.9 s
2024-12-07 20:03:39.458102: 
2024-12-07 20:03:39.459537: Epoch 69
2024-12-07 20:03:39.460268: Current learning rate: 0.00938
2024-12-07 20:10:46.778871: Validation loss did not improve from -0.45699. Patience: 21/50
2024-12-07 20:10:46.779718: train_loss -0.6716
2024-12-07 20:10:46.780979: val_loss -0.2989
2024-12-07 20:10:46.782310: Pseudo dice [0.6276]
2024-12-07 20:10:46.783429: Epoch time: 427.32 s
2024-12-07 20:10:48.614274: 
2024-12-07 20:10:48.615863: Epoch 70
2024-12-07 20:10:48.616893: Current learning rate: 0.00937
2024-12-07 20:17:42.517180: Validation loss did not improve from -0.45699. Patience: 22/50
2024-12-07 20:17:42.518306: train_loss -0.6754
2024-12-07 20:17:42.519285: val_loss -0.419
2024-12-07 20:17:42.520224: Pseudo dice [0.68]
2024-12-07 20:17:42.521113: Epoch time: 413.91 s
2024-12-07 20:17:43.928360: 
2024-12-07 20:17:43.929870: Epoch 71
2024-12-07 20:17:43.930959: Current learning rate: 0.00936
2024-12-07 20:24:47.656971: Validation loss did not improve from -0.45699. Patience: 23/50
2024-12-07 20:24:47.658063: train_loss -0.6729
2024-12-07 20:24:47.658808: val_loss -0.3387
2024-12-07 20:24:47.659475: Pseudo dice [0.6203]
2024-12-07 20:24:47.660144: Epoch time: 423.73 s
2024-12-07 20:24:49.067633: 
2024-12-07 20:24:49.068990: Epoch 72
2024-12-07 20:24:49.069761: Current learning rate: 0.00935
2024-12-07 20:31:53.117131: Validation loss improved from -0.45699 to -0.46357! Patience: 23/50
2024-12-07 20:31:53.118141: train_loss -0.6849
2024-12-07 20:31:53.119291: val_loss -0.4636
2024-12-07 20:31:53.120407: Pseudo dice [0.7028]
2024-12-07 20:31:53.121390: Epoch time: 424.05 s
2024-12-07 20:31:55.171509: 
2024-12-07 20:31:55.172943: Epoch 73
2024-12-07 20:31:55.173999: Current learning rate: 0.00934
2024-12-07 20:38:14.047909: Validation loss did not improve from -0.46357. Patience: 1/50
2024-12-07 20:38:14.048712: train_loss -0.6776
2024-12-07 20:38:14.049626: val_loss -0.3359
2024-12-07 20:38:14.050436: Pseudo dice [0.6482]
2024-12-07 20:38:14.051270: Epoch time: 378.88 s
2024-12-07 20:38:15.471090: 
2024-12-07 20:38:15.472723: Epoch 74
2024-12-07 20:38:15.473745: Current learning rate: 0.00933
2024-12-07 20:45:15.668379: Validation loss did not improve from -0.46357. Patience: 2/50
2024-12-07 20:45:15.669290: train_loss -0.688
2024-12-07 20:45:15.670041: val_loss -0.4018
2024-12-07 20:45:15.670805: Pseudo dice [0.6699]
2024-12-07 20:45:15.671491: Epoch time: 420.2 s
2024-12-07 20:45:17.489869: 
2024-12-07 20:45:17.491101: Epoch 75
2024-12-07 20:45:17.491958: Current learning rate: 0.00932
2024-12-07 20:52:09.990360: Validation loss did not improve from -0.46357. Patience: 3/50
2024-12-07 20:52:09.991316: train_loss -0.6837
2024-12-07 20:52:09.992142: val_loss -0.4253
2024-12-07 20:52:09.992824: Pseudo dice [0.6971]
2024-12-07 20:52:09.993528: Epoch time: 412.5 s
2024-12-07 20:52:11.410223: 
2024-12-07 20:52:11.411434: Epoch 76
2024-12-07 20:52:11.412236: Current learning rate: 0.00931
2024-12-07 20:58:57.036507: Validation loss did not improve from -0.46357. Patience: 4/50
2024-12-07 20:58:57.038842: train_loss -0.6753
2024-12-07 20:58:57.040892: val_loss -0.2975
2024-12-07 20:58:57.041682: Pseudo dice [0.6075]
2024-12-07 20:58:57.042732: Epoch time: 405.63 s
2024-12-07 20:58:58.501380: 
2024-12-07 20:58:58.502848: Epoch 77
2024-12-07 20:58:58.503626: Current learning rate: 0.0093
2024-12-07 21:05:59.158834: Validation loss did not improve from -0.46357. Patience: 5/50
2024-12-07 21:05:59.162545: train_loss -0.6805
2024-12-07 21:05:59.163415: val_loss -0.1395
2024-12-07 21:05:59.164216: Pseudo dice [0.5129]
2024-12-07 21:05:59.164934: Epoch time: 420.66 s
2024-12-07 21:06:00.598368: 
2024-12-07 21:06:00.599871: Epoch 78
2024-12-07 21:06:00.600789: Current learning rate: 0.0093
2024-12-07 21:12:43.252998: Validation loss did not improve from -0.46357. Patience: 6/50
2024-12-07 21:12:43.254094: train_loss -0.6782
2024-12-07 21:12:43.254942: val_loss -0.3968
2024-12-07 21:12:43.255738: Pseudo dice [0.6897]
2024-12-07 21:12:43.256489: Epoch time: 402.66 s
2024-12-07 21:12:44.706897: 
2024-12-07 21:12:44.708091: Epoch 79
2024-12-07 21:12:44.708909: Current learning rate: 0.00929
2024-12-07 21:19:37.428010: Validation loss did not improve from -0.46357. Patience: 7/50
2024-12-07 21:19:37.428982: train_loss -0.6782
2024-12-07 21:19:37.429951: val_loss -0.3642
2024-12-07 21:19:37.430819: Pseudo dice [0.6566]
2024-12-07 21:19:37.431583: Epoch time: 412.72 s
2024-12-07 21:19:39.298236: 
2024-12-07 21:19:39.299887: Epoch 80
2024-12-07 21:19:39.301005: Current learning rate: 0.00928
2024-12-07 21:26:33.484087: Validation loss did not improve from -0.46357. Patience: 8/50
2024-12-07 21:26:33.485224: train_loss -0.6834
2024-12-07 21:26:33.486033: val_loss -0.4171
2024-12-07 21:26:33.486865: Pseudo dice [0.6857]
2024-12-07 21:26:33.487630: Epoch time: 414.19 s
2024-12-07 21:26:34.962683: 
2024-12-07 21:26:34.963943: Epoch 81
2024-12-07 21:26:34.964850: Current learning rate: 0.00927
2024-12-07 21:33:11.046535: Validation loss did not improve from -0.46357. Patience: 9/50
2024-12-07 21:33:11.047231: train_loss -0.6819
2024-12-07 21:33:11.047956: val_loss -0.3161
2024-12-07 21:33:11.048617: Pseudo dice [0.6234]
2024-12-07 21:33:11.049265: Epoch time: 396.09 s
2024-12-07 21:33:12.504868: 
2024-12-07 21:33:12.505939: Epoch 82
2024-12-07 21:33:12.506744: Current learning rate: 0.00926
2024-12-07 21:40:10.184549: Validation loss did not improve from -0.46357. Patience: 10/50
2024-12-07 21:40:10.185569: train_loss -0.6831
2024-12-07 21:40:10.186475: val_loss -0.3578
2024-12-07 21:40:10.187380: Pseudo dice [0.6467]
2024-12-07 21:40:10.188173: Epoch time: 417.68 s
2024-12-07 21:40:11.521923: 
2024-12-07 21:40:11.523256: Epoch 83
2024-12-07 21:40:11.523964: Current learning rate: 0.00925
2024-12-07 21:47:03.644414: Validation loss did not improve from -0.46357. Patience: 11/50
2024-12-07 21:47:03.645444: train_loss -0.6973
2024-12-07 21:47:03.646259: val_loss -0.3817
2024-12-07 21:47:03.646909: Pseudo dice [0.6774]
2024-12-07 21:47:03.647635: Epoch time: 412.12 s
2024-12-07 21:47:05.371887: 
2024-12-07 21:47:05.373148: Epoch 84
2024-12-07 21:47:05.373921: Current learning rate: 0.00924
2024-12-07 21:53:58.586033: Validation loss did not improve from -0.46357. Patience: 12/50
2024-12-07 21:53:58.586681: train_loss -0.6952
2024-12-07 21:53:58.587724: val_loss -0.4116
2024-12-07 21:53:58.588789: Pseudo dice [0.6828]
2024-12-07 21:53:58.589797: Epoch time: 413.22 s
2024-12-07 21:54:00.314355: 
2024-12-07 21:54:00.315578: Epoch 85
2024-12-07 21:54:00.316402: Current learning rate: 0.00923
2024-12-07 22:00:44.080704: Validation loss did not improve from -0.46357. Patience: 13/50
2024-12-07 22:00:44.082189: train_loss -0.6917
2024-12-07 22:00:44.083186: val_loss -0.2939
2024-12-07 22:00:44.084037: Pseudo dice [0.6176]
2024-12-07 22:00:44.084909: Epoch time: 403.77 s
2024-12-07 22:00:45.504408: 
2024-12-07 22:00:45.505385: Epoch 86
2024-12-07 22:00:45.506108: Current learning rate: 0.00922
2024-12-07 22:07:38.617609: Validation loss did not improve from -0.46357. Patience: 14/50
2024-12-07 22:07:38.619766: train_loss -0.6968
2024-12-07 22:07:38.621376: val_loss -0.2979
2024-12-07 22:07:38.622496: Pseudo dice [0.6189]
2024-12-07 22:07:38.623778: Epoch time: 413.12 s
2024-12-07 22:07:40.079928: 
2024-12-07 22:07:40.081398: Epoch 87
2024-12-07 22:07:40.082240: Current learning rate: 0.00921
2024-12-07 22:14:39.420418: Validation loss improved from -0.46357 to -0.46408! Patience: 14/50
2024-12-07 22:14:39.423951: train_loss -0.6896
2024-12-07 22:14:39.424942: val_loss -0.4641
2024-12-07 22:14:39.425777: Pseudo dice [0.7126]
2024-12-07 22:14:39.426488: Epoch time: 419.35 s
2024-12-07 22:14:40.784929: 
2024-12-07 22:14:40.786036: Epoch 88
2024-12-07 22:14:40.786724: Current learning rate: 0.0092
2024-12-07 22:21:18.046951: Validation loss did not improve from -0.46408. Patience: 1/50
2024-12-07 22:21:18.047905: train_loss -0.6899
2024-12-07 22:21:18.048753: val_loss -0.4147
2024-12-07 22:21:18.049638: Pseudo dice [0.6798]
2024-12-07 22:21:18.050485: Epoch time: 397.26 s
2024-12-07 22:21:19.384276: 
2024-12-07 22:21:19.385605: Epoch 89
2024-12-07 22:21:19.386289: Current learning rate: 0.0092
2024-12-07 22:28:11.942683: Validation loss did not improve from -0.46408. Patience: 2/50
2024-12-07 22:28:11.943607: train_loss -0.6915
2024-12-07 22:28:11.944430: val_loss -0.3307
2024-12-07 22:28:11.945141: Pseudo dice [0.6256]
2024-12-07 22:28:11.945916: Epoch time: 412.56 s
2024-12-07 22:28:13.729735: 
2024-12-07 22:28:13.731009: Epoch 90
2024-12-07 22:28:13.731837: Current learning rate: 0.00919
2024-12-07 22:34:55.674142: Validation loss did not improve from -0.46408. Patience: 3/50
2024-12-07 22:34:55.674819: train_loss -0.6891
2024-12-07 22:34:55.675796: val_loss -0.3632
2024-12-07 22:34:55.676723: Pseudo dice [0.672]
2024-12-07 22:34:55.677745: Epoch time: 401.95 s
2024-12-07 22:34:57.081337: 
2024-12-07 22:34:57.082504: Epoch 91
2024-12-07 22:34:57.083210: Current learning rate: 0.00918
2024-12-07 22:41:41.642511: Validation loss did not improve from -0.46408. Patience: 4/50
2024-12-07 22:41:41.643586: train_loss -0.6961
2024-12-07 22:41:41.644423: val_loss -0.3817
2024-12-07 22:41:41.645146: Pseudo dice [0.655]
2024-12-07 22:41:41.645922: Epoch time: 404.56 s
2024-12-07 22:41:42.989415: 
2024-12-07 22:41:42.990590: Epoch 92
2024-12-07 22:41:42.991251: Current learning rate: 0.00917
2024-12-07 22:48:43.790834: Validation loss did not improve from -0.46408. Patience: 5/50
2024-12-07 22:48:43.791682: train_loss -0.7024
2024-12-07 22:48:43.792598: val_loss -0.3037
2024-12-07 22:48:43.793235: Pseudo dice [0.6196]
2024-12-07 22:48:43.793986: Epoch time: 420.8 s
2024-12-07 22:48:45.140977: 
2024-12-07 22:48:45.141948: Epoch 93
2024-12-07 22:48:45.142687: Current learning rate: 0.00916
2024-12-07 22:55:49.640510: Validation loss did not improve from -0.46408. Patience: 6/50
2024-12-07 22:55:49.641397: train_loss -0.7066
2024-12-07 22:55:49.642228: val_loss -0.3428
2024-12-07 22:55:49.643038: Pseudo dice [0.6452]
2024-12-07 22:55:49.643783: Epoch time: 424.5 s
2024-12-07 22:55:50.951795: 
2024-12-07 22:55:50.953084: Epoch 94
2024-12-07 22:55:50.954077: Current learning rate: 0.00915
2024-12-07 23:02:29.972429: Validation loss improved from -0.46408 to -0.49820! Patience: 6/50
2024-12-07 23:02:29.973588: train_loss -0.6982
2024-12-07 23:02:29.974786: val_loss -0.4982
2024-12-07 23:02:29.975935: Pseudo dice [0.733]
2024-12-07 23:02:29.976986: Epoch time: 399.02 s
2024-12-07 23:02:32.254163: 
2024-12-07 23:02:32.255308: Epoch 95
2024-12-07 23:02:32.256077: Current learning rate: 0.00914
2024-12-07 23:09:00.959708: Validation loss improved from -0.49820 to -0.50318! Patience: 0/50
2024-12-07 23:09:00.961966: train_loss -0.6922
2024-12-07 23:09:00.963867: val_loss -0.5032
2024-12-07 23:09:00.964729: Pseudo dice [0.731]
2024-12-07 23:09:00.965838: Epoch time: 388.71 s
2024-12-07 23:09:02.341222: 
2024-12-07 23:09:02.342592: Epoch 96
2024-12-07 23:09:02.343284: Current learning rate: 0.00913
2024-12-07 23:16:06.534318: Validation loss did not improve from -0.50318. Patience: 1/50
2024-12-07 23:16:06.537548: train_loss -0.6882
2024-12-07 23:16:06.538673: val_loss -0.4445
2024-12-07 23:16:06.539406: Pseudo dice [0.7032]
2024-12-07 23:16:06.540196: Epoch time: 424.2 s
2024-12-07 23:16:06.540858: Yayy! New best EMA pseudo Dice: 0.6695
2024-12-07 23:16:08.320436: 
2024-12-07 23:16:08.321704: Epoch 97
2024-12-07 23:16:08.322443: Current learning rate: 0.00912
2024-12-07 23:23:03.031758: Validation loss did not improve from -0.50318. Patience: 2/50
2024-12-07 23:23:03.032823: train_loss -0.7003
2024-12-07 23:23:03.033679: val_loss -0.2993
2024-12-07 23:23:03.034517: Pseudo dice [0.618]
2024-12-07 23:23:03.035196: Epoch time: 414.71 s
2024-12-07 23:23:04.377439: 
2024-12-07 23:23:04.378838: Epoch 98
2024-12-07 23:23:04.379735: Current learning rate: 0.00911
2024-12-07 23:30:11.167171: Validation loss did not improve from -0.50318. Patience: 3/50
2024-12-07 23:30:11.168117: train_loss -0.6893
2024-12-07 23:30:11.168924: val_loss -0.3563
2024-12-07 23:30:11.169776: Pseudo dice [0.6511]
2024-12-07 23:30:11.170608: Epoch time: 426.79 s
2024-12-07 23:30:12.545359: 
2024-12-07 23:30:12.546820: Epoch 99
2024-12-07 23:30:12.547829: Current learning rate: 0.0091
2024-12-07 23:36:55.441105: Validation loss did not improve from -0.50318. Patience: 4/50
2024-12-07 23:36:55.442533: train_loss -0.6963
2024-12-07 23:36:55.443377: val_loss -0.4361
2024-12-07 23:36:55.444056: Pseudo dice [0.7016]
2024-12-07 23:36:55.444770: Epoch time: 402.9 s
2024-12-07 23:36:57.202038: 
2024-12-07 23:36:57.203320: Epoch 100
2024-12-07 23:36:57.204021: Current learning rate: 0.0091
2024-12-07 23:43:43.355899: Validation loss did not improve from -0.50318. Patience: 5/50
2024-12-07 23:43:43.356872: train_loss -0.7005
2024-12-07 23:43:43.357751: val_loss -0.3136
2024-12-07 23:43:43.358696: Pseudo dice [0.6313]
2024-12-07 23:43:43.359670: Epoch time: 406.16 s
2024-12-07 23:43:44.720970: 
2024-12-07 23:43:44.722442: Epoch 101
2024-12-07 23:43:44.723547: Current learning rate: 0.00909
2024-12-07 23:50:14.988224: Validation loss did not improve from -0.50318. Patience: 6/50
2024-12-07 23:50:14.989009: train_loss -0.7111
2024-12-07 23:50:14.989772: val_loss -0.4034
2024-12-07 23:50:14.990522: Pseudo dice [0.6866]
2024-12-07 23:50:14.991272: Epoch time: 390.27 s
2024-12-07 23:50:16.356427: 
2024-12-07 23:50:16.357813: Epoch 102
2024-12-07 23:50:16.358651: Current learning rate: 0.00908
2024-12-07 23:57:10.773822: Validation loss did not improve from -0.50318. Patience: 7/50
2024-12-07 23:57:10.774489: train_loss -0.7111
2024-12-07 23:57:10.775189: val_loss -0.3992
2024-12-07 23:57:10.775859: Pseudo dice [0.6827]
2024-12-07 23:57:10.776561: Epoch time: 414.42 s
2024-12-07 23:57:12.127115: 
2024-12-07 23:57:12.128659: Epoch 103
2024-12-07 23:57:12.129521: Current learning rate: 0.00907
2024-12-08 00:04:01.834213: Validation loss did not improve from -0.50318. Patience: 8/50
2024-12-08 00:04:01.835049: train_loss -0.7103
2024-12-08 00:04:01.836277: val_loss -0.3497
2024-12-08 00:04:01.837446: Pseudo dice [0.6624]
2024-12-08 00:04:01.838533: Epoch time: 409.71 s
2024-12-08 00:04:03.176278: 
2024-12-08 00:04:03.177574: Epoch 104
2024-12-08 00:04:03.178582: Current learning rate: 0.00906
2024-12-08 00:11:03.804637: Validation loss did not improve from -0.50318. Patience: 9/50
2024-12-08 00:11:03.805728: train_loss -0.71
2024-12-08 00:11:03.806989: val_loss -0.3146
2024-12-08 00:11:03.808152: Pseudo dice [0.641]
2024-12-08 00:11:03.809245: Epoch time: 420.63 s
2024-12-08 00:11:05.587020: 
2024-12-08 00:11:05.588590: Epoch 105
2024-12-08 00:11:05.589796: Current learning rate: 0.00905
2024-12-08 00:17:52.330110: Validation loss did not improve from -0.50318. Patience: 10/50
2024-12-08 00:17:52.331663: train_loss -0.7057
2024-12-08 00:17:52.333427: val_loss -0.3542
2024-12-08 00:17:52.334211: Pseudo dice [0.6644]
2024-12-08 00:17:52.335036: Epoch time: 406.75 s
2024-12-08 00:17:53.733034: 
2024-12-08 00:17:53.734513: Epoch 106
2024-12-08 00:17:53.735612: Current learning rate: 0.00904
2024-12-08 00:24:25.288986: Validation loss did not improve from -0.50318. Patience: 11/50
2024-12-08 00:24:25.289937: train_loss -0.7136
2024-12-08 00:24:25.290675: val_loss -0.3458
2024-12-08 00:24:25.291428: Pseudo dice [0.6452]
2024-12-08 00:24:25.292076: Epoch time: 391.56 s
2024-12-08 00:24:27.203327: 
2024-12-08 00:24:27.204744: Epoch 107
2024-12-08 00:24:27.205519: Current learning rate: 0.00903
2024-12-08 00:31:26.468558: Validation loss did not improve from -0.50318. Patience: 12/50
2024-12-08 00:31:26.469829: train_loss -0.7096
2024-12-08 00:31:26.470644: val_loss -0.1979
2024-12-08 00:31:26.471492: Pseudo dice [0.5776]
2024-12-08 00:31:26.472254: Epoch time: 419.27 s
2024-12-08 00:31:27.876209: 
2024-12-08 00:31:27.877789: Epoch 108
2024-12-08 00:31:27.878524: Current learning rate: 0.00902
2024-12-08 00:38:16.848928: Validation loss did not improve from -0.50318. Patience: 13/50
2024-12-08 00:38:16.850001: train_loss -0.7127
2024-12-08 00:38:16.850753: val_loss -0.3697
2024-12-08 00:38:16.851437: Pseudo dice [0.6687]
2024-12-08 00:38:16.852068: Epoch time: 408.98 s
2024-12-08 00:38:18.223045: 
2024-12-08 00:38:18.224564: Epoch 109
2024-12-08 00:38:18.225441: Current learning rate: 0.00901
2024-12-08 00:45:07.490071: Validation loss did not improve from -0.50318. Patience: 14/50
2024-12-08 00:45:07.490848: train_loss -0.7132
2024-12-08 00:45:07.491846: val_loss -0.351
2024-12-08 00:45:07.492731: Pseudo dice [0.6481]
2024-12-08 00:45:07.493670: Epoch time: 409.27 s
2024-12-08 00:45:09.295713: 
2024-12-08 00:45:09.297067: Epoch 110
2024-12-08 00:45:09.297915: Current learning rate: 0.009
2024-12-08 00:51:48.762945: Validation loss did not improve from -0.50318. Patience: 15/50
2024-12-08 00:51:48.763922: train_loss -0.7111
2024-12-08 00:51:48.764812: val_loss -0.3864
2024-12-08 00:51:48.765577: Pseudo dice [0.6625]
2024-12-08 00:51:48.766329: Epoch time: 399.47 s
2024-12-08 00:51:50.127706: 
2024-12-08 00:51:50.128870: Epoch 111
2024-12-08 00:51:50.129637: Current learning rate: 0.009
2024-12-08 00:58:21.926206: Validation loss did not improve from -0.50318. Patience: 16/50
2024-12-08 00:58:21.927210: train_loss -0.7189
2024-12-08 00:58:21.927936: val_loss -0.4224
2024-12-08 00:58:21.928548: Pseudo dice [0.6977]
2024-12-08 00:58:21.929208: Epoch time: 391.8 s
2024-12-08 00:58:23.288448: 
2024-12-08 00:58:23.289838: Epoch 112
2024-12-08 00:58:23.290859: Current learning rate: 0.00899
2024-12-08 01:04:57.371546: Validation loss did not improve from -0.50318. Patience: 17/50
2024-12-08 01:04:57.372520: train_loss -0.7157
2024-12-08 01:04:57.373265: val_loss -0.3351
2024-12-08 01:04:57.374009: Pseudo dice [0.6559]
2024-12-08 01:04:57.374779: Epoch time: 394.09 s
2024-12-08 01:04:58.743227: 
2024-12-08 01:04:58.744478: Epoch 113
2024-12-08 01:04:58.745214: Current learning rate: 0.00898
2024-12-08 01:11:37.215422: Validation loss did not improve from -0.50318. Patience: 18/50
2024-12-08 01:11:37.216507: train_loss -0.7195
2024-12-08 01:11:37.217399: val_loss -0.3866
2024-12-08 01:11:37.218202: Pseudo dice [0.6871]
2024-12-08 01:11:37.218923: Epoch time: 398.48 s
2024-12-08 01:11:38.578162: 
2024-12-08 01:11:38.579450: Epoch 114
2024-12-08 01:11:38.580505: Current learning rate: 0.00897
2024-12-08 01:18:16.639941: Validation loss did not improve from -0.50318. Patience: 19/50
2024-12-08 01:18:16.640905: train_loss -0.7164
2024-12-08 01:18:16.641749: val_loss -0.3571
2024-12-08 01:18:16.642560: Pseudo dice [0.6474]
2024-12-08 01:18:16.643323: Epoch time: 398.06 s
2024-12-08 01:18:18.450510: 
2024-12-08 01:18:18.451528: Epoch 115
2024-12-08 01:18:18.452311: Current learning rate: 0.00896
2024-12-08 01:24:32.145246: Validation loss did not improve from -0.50318. Patience: 20/50
2024-12-08 01:24:32.147130: train_loss -0.7245
2024-12-08 01:24:32.148614: val_loss -0.4553
2024-12-08 01:24:32.149389: Pseudo dice [0.7144]
2024-12-08 01:24:32.150400: Epoch time: 373.7 s
2024-12-08 01:24:33.535369: 
2024-12-08 01:24:33.536291: Epoch 116
2024-12-08 01:24:33.536955: Current learning rate: 0.00895
2024-12-08 01:30:49.115077: Validation loss did not improve from -0.50318. Patience: 21/50
2024-12-08 01:30:49.117230: train_loss -0.7205
2024-12-08 01:30:49.118224: val_loss -0.2949
2024-12-08 01:30:49.119025: Pseudo dice [0.6135]
2024-12-08 01:30:49.119742: Epoch time: 375.58 s
2024-12-08 01:30:50.536779: 
2024-12-08 01:30:50.538048: Epoch 117
2024-12-08 01:30:50.538918: Current learning rate: 0.00894
2024-12-08 01:37:18.924920: Validation loss did not improve from -0.50318. Patience: 22/50
2024-12-08 01:37:18.926141: train_loss -0.7214
2024-12-08 01:37:18.926932: val_loss -0.3995
2024-12-08 01:37:18.927873: Pseudo dice [0.6774]
2024-12-08 01:37:18.928641: Epoch time: 388.39 s
2024-12-08 01:37:20.306795: 
2024-12-08 01:37:20.308190: Epoch 118
2024-12-08 01:37:20.309189: Current learning rate: 0.00893
2024-12-08 01:43:50.793927: Validation loss did not improve from -0.50318. Patience: 23/50
2024-12-08 01:43:50.795017: train_loss -0.7261
2024-12-08 01:43:50.795736: val_loss -0.3182
2024-12-08 01:43:50.796418: Pseudo dice [0.6419]
2024-12-08 01:43:50.797073: Epoch time: 390.49 s
2024-12-08 01:43:52.204426: 
2024-12-08 01:43:52.205840: Epoch 119
2024-12-08 01:43:52.206585: Current learning rate: 0.00892
2024-12-08 01:50:32.551689: Validation loss did not improve from -0.50318. Patience: 24/50
2024-12-08 01:50:32.552703: train_loss -0.7251
2024-12-08 01:50:32.553500: val_loss -0.4075
2024-12-08 01:50:32.554292: Pseudo dice [0.7006]
2024-12-08 01:50:32.555030: Epoch time: 400.35 s
2024-12-08 01:50:34.395397: 
2024-12-08 01:50:34.396604: Epoch 120
2024-12-08 01:50:34.397495: Current learning rate: 0.00891
2024-12-08 01:57:03.290836: Validation loss did not improve from -0.50318. Patience: 25/50
2024-12-08 01:57:03.291753: train_loss -0.7296
2024-12-08 01:57:03.292631: val_loss -0.2972
2024-12-08 01:57:03.293647: Pseudo dice [0.6162]
2024-12-08 01:57:03.294624: Epoch time: 388.9 s
2024-12-08 01:57:04.680627: 
2024-12-08 01:57:04.681813: Epoch 121
2024-12-08 01:57:04.682553: Current learning rate: 0.0089
2024-12-08 02:03:36.372569: Validation loss did not improve from -0.50318. Patience: 26/50
2024-12-08 02:03:36.373606: train_loss -0.7279
2024-12-08 02:03:36.374486: val_loss -0.2882
2024-12-08 02:03:36.375293: Pseudo dice [0.618]
2024-12-08 02:03:36.376081: Epoch time: 391.69 s
2024-12-08 02:03:37.749578: 
2024-12-08 02:03:37.750938: Epoch 122
2024-12-08 02:03:37.751814: Current learning rate: 0.00889
2024-12-08 02:09:54.811600: Validation loss did not improve from -0.50318. Patience: 27/50
2024-12-08 02:09:54.812588: train_loss -0.727
2024-12-08 02:09:54.813693: val_loss -0.3999
2024-12-08 02:09:54.814659: Pseudo dice [0.6927]
2024-12-08 02:09:54.815656: Epoch time: 377.06 s
2024-12-08 02:09:56.233666: 
2024-12-08 02:09:56.235180: Epoch 123
2024-12-08 02:09:56.236123: Current learning rate: 0.00889
2024-12-08 02:15:53.617722: Validation loss did not improve from -0.50318. Patience: 28/50
2024-12-08 02:15:53.618623: train_loss -0.7294
2024-12-08 02:15:53.619308: val_loss -0.3015
2024-12-08 02:15:53.619925: Pseudo dice [0.6354]
2024-12-08 02:15:53.620612: Epoch time: 357.39 s
2024-12-08 02:15:55.096909: 
2024-12-08 02:15:55.098166: Epoch 124
2024-12-08 02:15:55.098966: Current learning rate: 0.00888
2024-12-08 02:21:08.985189: Validation loss did not improve from -0.50318. Patience: 29/50
2024-12-08 02:21:08.986231: train_loss -0.7245
2024-12-08 02:21:08.987080: val_loss -0.307
2024-12-08 02:21:08.987783: Pseudo dice [0.6284]
2024-12-08 02:21:08.988560: Epoch time: 313.89 s
2024-12-08 02:21:10.816939: 
2024-12-08 02:21:10.818414: Epoch 125
2024-12-08 02:21:10.819484: Current learning rate: 0.00887
2024-12-08 02:25:26.836918: Validation loss did not improve from -0.50318. Patience: 30/50
2024-12-08 02:25:26.838248: train_loss -0.7316
2024-12-08 02:25:26.840330: val_loss -0.3049
2024-12-08 02:25:26.841034: Pseudo dice [0.6382]
2024-12-08 02:25:26.842058: Epoch time: 256.02 s
2024-12-08 02:25:28.282526: 
2024-12-08 02:25:28.283908: Epoch 126
2024-12-08 02:25:28.284606: Current learning rate: 0.00886
2024-12-08 02:29:45.951793: Validation loss did not improve from -0.50318. Patience: 31/50
2024-12-08 02:29:45.953366: train_loss -0.7304
2024-12-08 02:29:45.954421: val_loss -0.3642
2024-12-08 02:29:45.955130: Pseudo dice [0.6594]
2024-12-08 02:29:45.955799: Epoch time: 257.67 s
2024-12-08 02:29:47.394152: 
2024-12-08 02:29:47.395751: Epoch 127
2024-12-08 02:29:47.396487: Current learning rate: 0.00885
2024-12-08 02:34:03.317562: Validation loss did not improve from -0.50318. Patience: 32/50
2024-12-08 02:34:03.318497: train_loss -0.7282
2024-12-08 02:34:03.319308: val_loss -0.3597
2024-12-08 02:34:03.319938: Pseudo dice [0.6599]
2024-12-08 02:34:03.320618: Epoch time: 255.93 s
2024-12-08 02:34:04.712084: 
2024-12-08 02:34:04.713349: Epoch 128
2024-12-08 02:34:04.714182: Current learning rate: 0.00884
2024-12-08 02:38:13.409280: Validation loss did not improve from -0.50318. Patience: 33/50
2024-12-08 02:38:13.411491: train_loss -0.734
2024-12-08 02:38:13.412207: val_loss -0.3783
2024-12-08 02:38:13.412839: Pseudo dice [0.675]
2024-12-08 02:38:13.413621: Epoch time: 248.7 s
2024-12-08 02:38:14.814713: 
2024-12-08 02:38:14.815858: Epoch 129
2024-12-08 02:38:14.816666: Current learning rate: 0.00883
2024-12-08 02:42:30.426456: Validation loss did not improve from -0.50318. Patience: 34/50
2024-12-08 02:42:30.427433: train_loss -0.7277
2024-12-08 02:42:30.428228: val_loss -0.4217
2024-12-08 02:42:30.429051: Pseudo dice [0.6919]
2024-12-08 02:42:30.429838: Epoch time: 255.61 s
2024-12-08 02:42:32.695172: 
2024-12-08 02:42:32.696368: Epoch 130
2024-12-08 02:42:32.697196: Current learning rate: 0.00882
2024-12-08 02:46:51.214116: Validation loss did not improve from -0.50318. Patience: 35/50
2024-12-08 02:46:51.214927: train_loss -0.7283
2024-12-08 02:46:51.215638: val_loss -0.2955
2024-12-08 02:46:51.216359: Pseudo dice [0.6058]
2024-12-08 02:46:51.216996: Epoch time: 258.52 s
2024-12-08 02:46:52.628452: 
2024-12-08 02:46:52.629744: Epoch 131
2024-12-08 02:46:52.630511: Current learning rate: 0.00881
2024-12-08 02:51:08.936136: Validation loss did not improve from -0.50318. Patience: 36/50
2024-12-08 02:51:08.937124: train_loss -0.7281
2024-12-08 02:51:08.937940: val_loss -0.2807
2024-12-08 02:51:08.938697: Pseudo dice [0.6034]
2024-12-08 02:51:08.939452: Epoch time: 256.31 s
2024-12-08 02:51:10.461080: 
2024-12-08 02:51:10.462264: Epoch 132
2024-12-08 02:51:10.463176: Current learning rate: 0.0088
2024-12-08 02:55:29.491349: Validation loss did not improve from -0.50318. Patience: 37/50
2024-12-08 02:55:29.492332: train_loss -0.7299
2024-12-08 02:55:29.493181: val_loss -0.3735
2024-12-08 02:55:29.494073: Pseudo dice [0.6695]
2024-12-08 02:55:29.494925: Epoch time: 259.03 s
2024-12-08 02:55:30.885189: 
2024-12-08 02:55:30.886668: Epoch 133
2024-12-08 02:55:30.887731: Current learning rate: 0.00879
2024-12-08 02:59:49.377842: Validation loss did not improve from -0.50318. Patience: 38/50
2024-12-08 02:59:49.378512: train_loss -0.7321
2024-12-08 02:59:49.379429: val_loss -0.3703
2024-12-08 02:59:49.380160: Pseudo dice [0.6676]
2024-12-08 02:59:49.380845: Epoch time: 258.49 s
2024-12-08 02:59:50.784310: 
2024-12-08 02:59:50.785598: Epoch 134
2024-12-08 02:59:50.786302: Current learning rate: 0.00879
2024-12-08 03:04:10.350932: Validation loss did not improve from -0.50318. Patience: 39/50
2024-12-08 03:04:10.351991: train_loss -0.7353
2024-12-08 03:04:10.352861: val_loss -0.2567
2024-12-08 03:04:10.353493: Pseudo dice [0.6178]
2024-12-08 03:04:10.354280: Epoch time: 259.57 s
2024-12-08 03:04:12.208944: 
2024-12-08 03:04:12.210359: Epoch 135
2024-12-08 03:04:12.211143: Current learning rate: 0.00878
2024-12-08 03:08:35.809546: Validation loss did not improve from -0.50318. Patience: 40/50
2024-12-08 03:08:35.810670: train_loss -0.7351
2024-12-08 03:08:35.811594: val_loss -0.2465
2024-12-08 03:08:35.812346: Pseudo dice [0.5811]
2024-12-08 03:08:35.813131: Epoch time: 263.6 s
2024-12-08 03:08:37.225532: 
2024-12-08 03:08:37.226799: Epoch 136
2024-12-08 03:08:37.227523: Current learning rate: 0.00877
2024-12-08 03:13:01.306950: Validation loss did not improve from -0.50318. Patience: 41/50
2024-12-08 03:13:01.307995: train_loss -0.7351
2024-12-08 03:13:01.309281: val_loss -0.3524
2024-12-08 03:13:01.310247: Pseudo dice [0.6556]
2024-12-08 03:13:01.311158: Epoch time: 264.08 s
2024-12-08 03:13:02.753654: 
2024-12-08 03:13:02.755088: Epoch 137
2024-12-08 03:13:02.755991: Current learning rate: 0.00876
2024-12-08 03:17:30.600651: Validation loss did not improve from -0.50318. Patience: 42/50
2024-12-08 03:17:30.601666: train_loss -0.7309
2024-12-08 03:17:30.602389: val_loss -0.3217
2024-12-08 03:17:30.603204: Pseudo dice [0.6498]
2024-12-08 03:17:30.603956: Epoch time: 267.85 s
2024-12-08 03:17:32.035825: 
2024-12-08 03:17:32.037024: Epoch 138
2024-12-08 03:17:32.037863: Current learning rate: 0.00875
2024-12-08 03:21:58.340410: Validation loss did not improve from -0.50318. Patience: 43/50
2024-12-08 03:21:58.343009: train_loss -0.7306
2024-12-08 03:21:58.344171: val_loss -0.2459
2024-12-08 03:21:58.344943: Pseudo dice [0.5997]
2024-12-08 03:21:58.345979: Epoch time: 266.31 s
2024-12-08 03:21:59.770116: 
2024-12-08 03:21:59.771587: Epoch 139
2024-12-08 03:21:59.772344: Current learning rate: 0.00874
2024-12-08 03:26:14.744509: Validation loss did not improve from -0.50318. Patience: 44/50
2024-12-08 03:26:14.746374: train_loss -0.7345
2024-12-08 03:26:14.747279: val_loss -0.2709
2024-12-08 03:26:14.748111: Pseudo dice [0.6204]
2024-12-08 03:26:14.748880: Epoch time: 254.98 s
2024-12-08 03:26:16.669304: 
2024-12-08 03:26:16.670397: Epoch 140
2024-12-08 03:26:16.671057: Current learning rate: 0.00873
2024-12-08 03:30:36.079797: Validation loss did not improve from -0.50318. Patience: 45/50
2024-12-08 03:30:36.081102: train_loss -0.7351
2024-12-08 03:30:36.082741: val_loss -0.3533
2024-12-08 03:30:36.083499: Pseudo dice [0.6643]
2024-12-08 03:30:36.084424: Epoch time: 259.41 s
2024-12-08 03:30:38.087538: 
2024-12-08 03:30:38.088907: Epoch 141
2024-12-08 03:30:38.089615: Current learning rate: 0.00872
2024-12-08 03:34:58.208071: Validation loss did not improve from -0.50318. Patience: 46/50
2024-12-08 03:34:58.209789: train_loss -0.7322
2024-12-08 03:34:58.211036: val_loss -0.3742
2024-12-08 03:34:58.211913: Pseudo dice [0.6617]
2024-12-08 03:34:58.212802: Epoch time: 260.12 s
2024-12-08 03:34:59.606587: 
2024-12-08 03:34:59.607890: Epoch 142
2024-12-08 03:34:59.608809: Current learning rate: 0.00871
2024-12-08 03:39:23.843258: Validation loss did not improve from -0.50318. Patience: 47/50
2024-12-08 03:39:23.844386: train_loss -0.7341
2024-12-08 03:39:23.845167: val_loss -0.4053
2024-12-08 03:39:23.845862: Pseudo dice [0.6894]
2024-12-08 03:39:23.846618: Epoch time: 264.24 s
2024-12-08 03:39:25.270832: 
2024-12-08 03:39:25.272441: Epoch 143
2024-12-08 03:39:25.273318: Current learning rate: 0.0087
2024-12-08 03:43:47.070951: Validation loss did not improve from -0.50318. Patience: 48/50
2024-12-08 03:43:47.073144: train_loss -0.7341
2024-12-08 03:43:47.073992: val_loss -0.3472
2024-12-08 03:43:47.074714: Pseudo dice [0.661]
2024-12-08 03:43:47.075468: Epoch time: 261.8 s
2024-12-08 03:43:48.493333: 
2024-12-08 03:43:48.494727: Epoch 144
2024-12-08 03:43:48.495535: Current learning rate: 0.00869
2024-12-08 03:48:05.445090: Validation loss did not improve from -0.50318. Patience: 49/50
2024-12-08 03:48:05.446315: train_loss -0.742
2024-12-08 03:48:05.447184: val_loss -0.3851
2024-12-08 03:48:05.448021: Pseudo dice [0.6668]
2024-12-08 03:48:05.448774: Epoch time: 256.95 s
2024-12-08 03:48:07.296701: 
2024-12-08 03:48:07.298132: Epoch 145
2024-12-08 03:48:07.298916: Current learning rate: 0.00868
2024-12-08 03:52:26.692432: Validation loss did not improve from -0.50318. Patience: 50/50
2024-12-08 03:52:26.693491: train_loss -0.7486
2024-12-08 03:52:26.694237: val_loss -0.2011
2024-12-08 03:52:26.694923: Pseudo dice [0.5711]
2024-12-08 03:52:26.695602: Epoch time: 259.4 s
2024-12-08 03:52:28.102598: Patience reached. Stopping training.
2024-12-08 03:52:28.621558: Training done.
2024-12-08 03:52:28.799829: Using splits from existing split file: /home/gridsan/nchutisilp/datasets/nnUNet_Datasets/nnUNet_preprocessed/Dataset307_Sohee_Calcium_OCT_CrossValidation/splits_final.json
2024-12-08 03:52:28.802385: The split file contains 5 splits.
2024-12-08 03:52:28.803227: Desired fold for training: 3
2024-12-08 03:52:28.803951: This split has 7 training and 1 validation cases.
2024-12-08 03:52:28.804956: predicting 701-013
2024-12-08 03:52:28.899897: 701-013, shape torch.Size([1, 375, 498, 498]), rank 0
2024-12-08 03:54:55.384660: Validation complete
2024-12-08 03:54:55.385499: Mean Validation Dice:  0.6018660146220571
mv: cannot stat '/home/gridsan/nchutisilp/datasets/nnUNet_Datasets/nnUNet_results/Dataset309_Sohee_Calcium_OCT_CrossValidation/nnUNetTrainer__nnUNetPlans__3d_32x160x128_b10/fold_2': No such file or directory
mv: cannot stat '/home/gridsan/nchutisilp/datasets/nnUNet_Datasets/nnUNet_results/Dataset309_Sohee_Calcium_OCT_CrossValidation/nnUNetTrainer__nnUNetPlans__3d_32x160x128_b10/fold_3': No such file or directory

############################
INFO: You are using the old nnU-Net default plans. We have updated our recommendations. Please consider using those instead! Read more here: https://github.com/MIC-DKFZ/nnUNet/blob/master/documentation/resenc_presets.md
############################

Using device: cuda:0

#######################################################################
Please cite the following paper when using nnU-Net:
Isensee, F., Jaeger, P. F., Kohl, S. A., Petersen, J., & Maier-Hein, K. H. (2021). nnU-Net: a self-configuring method for deep learning-based biomedical image segmentation. Nature methods, 18(2), 203-211.
#######################################################################

2024-12-08 03:55:04.260310: Using torch.compile...
/home/gridsan/nchutisilp/.local/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn("The verbose parameter is deprecated. Please use get_last_lr() "
################### Loading pretrained weights from file  /home/gridsan/nchutisilp/projects/OpenAI-CLIP/clip_preivl_poststent_pretrained_nnUNet.pt ###################
Below is the list of overlapping blocks in pretrained model and nnUNet architecture:
encoder.stages.0.0.convs.0.conv.weight shape torch.Size([32, 1, 3, 3, 3])
encoder.stages.0.0.convs.0.conv.bias shape torch.Size([32])
encoder.stages.0.0.convs.0.norm.weight shape torch.Size([32])
encoder.stages.0.0.convs.0.norm.bias shape torch.Size([32])
encoder.stages.0.0.convs.0.all_modules.0.weight shape torch.Size([32, 1, 3, 3, 3])
encoder.stages.0.0.convs.0.all_modules.0.bias shape torch.Size([32])
encoder.stages.0.0.convs.0.all_modules.1.weight shape torch.Size([32])
encoder.stages.0.0.convs.0.all_modules.1.bias shape torch.Size([32])
encoder.stages.0.0.convs.1.conv.weight shape torch.Size([32, 32, 3, 3, 3])
encoder.stages.0.0.convs.1.conv.bias shape torch.Size([32])
encoder.stages.0.0.convs.1.norm.weight shape torch.Size([32])
encoder.stages.0.0.convs.1.norm.bias shape torch.Size([32])
encoder.stages.0.0.convs.1.all_modules.0.weight shape torch.Size([32, 32, 3, 3, 3])
encoder.stages.0.0.convs.1.all_modules.0.bias shape torch.Size([32])
encoder.stages.0.0.convs.1.all_modules.1.weight shape torch.Size([32])
encoder.stages.0.0.convs.1.all_modules.1.bias shape torch.Size([32])
encoder.stages.1.0.convs.0.conv.weight shape torch.Size([64, 32, 3, 3, 3])
encoder.stages.1.0.convs.0.conv.bias shape torch.Size([64])
encoder.stages.1.0.convs.0.norm.weight shape torch.Size([64])
encoder.stages.1.0.convs.0.norm.bias shape torch.Size([64])
encoder.stages.1.0.convs.0.all_modules.0.weight shape torch.Size([64, 32, 3, 3, 3])
encoder.stages.1.0.convs.0.all_modules.0.bias shape torch.Size([64])
encoder.stages.1.0.convs.0.all_modules.1.weight shape torch.Size([64])
encoder.stages.1.0.convs.0.all_modules.1.bias shape torch.Size([64])
encoder.stages.1.0.convs.1.conv.weight shape torch.Size([64, 64, 3, 3, 3])
encoder.stages.1.0.convs.1.conv.bias shape torch.Size([64])
encoder.stages.1.0.convs.1.norm.weight shape torch.Size([64])
encoder.stages.1.0.convs.1.norm.bias shape torch.Size([64])
encoder.stages.1.0.convs.1.all_modules.0.weight shape torch.Size([64, 64, 3, 3, 3])
encoder.stages.1.0.convs.1.all_modules.0.bias shape torch.Size([64])
encoder.stages.1.0.convs.1.all_modules.1.weight shape torch.Size([64])
encoder.stages.1.0.convs.1.all_modules.1.bias shape torch.Size([64])
encoder.stages.2.0.convs.0.conv.weight shape torch.Size([128, 64, 3, 3, 3])
encoder.stages.2.0.convs.0.conv.bias shape torch.Size([128])
encoder.stages.2.0.convs.0.norm.weight shape torch.Size([128])
encoder.stages.2.0.convs.0.norm.bias shape torch.Size([128])
encoder.stages.2.0.convs.0.all_modules.0.weight shape torch.Size([128, 64, 3, 3, 3])
encoder.stages.2.0.convs.0.all_modules.0.bias shape torch.Size([128])
encoder.stages.2.0.convs.0.all_modules.1.weight shape torch.Size([128])
encoder.stages.2.0.convs.0.all_modules.1.bias shape torch.Size([128])
encoder.stages.2.0.convs.1.conv.weight shape torch.Size([128, 128, 3, 3, 3])
encoder.stages.2.0.convs.1.conv.bias shape torch.Size([128])
encoder.stages.2.0.convs.1.norm.weight shape torch.Size([128])
encoder.stages.2.0.convs.1.norm.bias shape torch.Size([128])
encoder.stages.2.0.convs.1.all_modules.0.weight shape torch.Size([128, 128, 3, 3, 3])
encoder.stages.2.0.convs.1.all_modules.0.bias shape torch.Size([128])
encoder.stages.2.0.convs.1.all_modules.1.weight shape torch.Size([128])
encoder.stages.2.0.convs.1.all_modules.1.bias shape torch.Size([128])
encoder.stages.3.0.convs.0.conv.weight shape torch.Size([256, 128, 3, 3, 3])
encoder.stages.3.0.convs.0.conv.bias shape torch.Size([256])
encoder.stages.3.0.convs.0.norm.weight shape torch.Size([256])
encoder.stages.3.0.convs.0.norm.bias shape torch.Size([256])
encoder.stages.3.0.convs.0.all_modules.0.weight shape torch.Size([256, 128, 3, 3, 3])
encoder.stages.3.0.convs.0.all_modules.0.bias shape torch.Size([256])
encoder.stages.3.0.convs.0.all_modules.1.weight shape torch.Size([256])
encoder.stages.3.0.convs.0.all_modules.1.bias shape torch.Size([256])
encoder.stages.3.0.convs.1.conv.weight shape torch.Size([256, 256, 3, 3, 3])
encoder.stages.3.0.convs.1.conv.bias shape torch.Size([256])
encoder.stages.3.0.convs.1.norm.weight shape torch.Size([256])
encoder.stages.3.0.convs.1.norm.bias shape torch.Size([256])
encoder.stages.3.0.convs.1.all_modules.0.weight shape torch.Size([256, 256, 3, 3, 3])
encoder.stages.3.0.convs.1.all_modules.0.bias shape torch.Size([256])
encoder.stages.3.0.convs.1.all_modules.1.weight shape torch.Size([256])
encoder.stages.3.0.convs.1.all_modules.1.bias shape torch.Size([256])
encoder.stages.4.0.convs.0.conv.weight shape torch.Size([320, 256, 3, 3, 3])
encoder.stages.4.0.convs.0.conv.bias shape torch.Size([320])
encoder.stages.4.0.convs.0.norm.weight shape torch.Size([320])
encoder.stages.4.0.convs.0.norm.bias shape torch.Size([320])
encoder.stages.4.0.convs.0.all_modules.0.weight shape torch.Size([320, 256, 3, 3, 3])
encoder.stages.4.0.convs.0.all_modules.0.bias shape torch.Size([320])
encoder.stages.4.0.convs.0.all_modules.1.weight shape torch.Size([320])
encoder.stages.4.0.convs.0.all_modules.1.bias shape torch.Size([320])
encoder.stages.4.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.4.0.convs.1.conv.bias shape torch.Size([320])
encoder.stages.4.0.convs.1.norm.weight shape torch.Size([320])
encoder.stages.4.0.convs.1.norm.bias shape torch.Size([320])
encoder.stages.4.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.4.0.convs.1.all_modules.0.bias shape torch.Size([320])
encoder.stages.4.0.convs.1.all_modules.1.weight shape torch.Size([320])
encoder.stages.4.0.convs.1.all_modules.1.bias shape torch.Size([320])
encoder.stages.5.0.convs.0.conv.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.5.0.convs.0.conv.bias shape torch.Size([320])
encoder.stages.5.0.convs.0.norm.weight shape torch.Size([320])
encoder.stages.5.0.convs.0.norm.bias shape torch.Size([320])
encoder.stages.5.0.convs.0.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.5.0.convs.0.all_modules.0.bias shape torch.Size([320])
encoder.stages.5.0.convs.0.all_modules.1.weight shape torch.Size([320])
encoder.stages.5.0.convs.0.all_modules.1.bias shape torch.Size([320])
encoder.stages.5.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.5.0.convs.1.conv.bias shape torch.Size([320])
encoder.stages.5.0.convs.1.norm.weight shape torch.Size([320])
encoder.stages.5.0.convs.1.norm.bias shape torch.Size([320])
encoder.stages.5.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.5.0.convs.1.all_modules.0.bias shape torch.Size([320])
encoder.stages.5.0.convs.1.all_modules.1.weight shape torch.Size([320])
encoder.stages.5.0.convs.1.all_modules.1.bias shape torch.Size([320])
decoder.encoder.stages.0.0.convs.0.conv.weight shape torch.Size([32, 1, 3, 3, 3])
decoder.encoder.stages.0.0.convs.0.conv.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.norm.weight shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.norm.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.all_modules.0.weight shape torch.Size([32, 1, 3, 3, 3])
decoder.encoder.stages.0.0.convs.0.all_modules.0.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.all_modules.1.weight shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.all_modules.1.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.conv.weight shape torch.Size([32, 32, 3, 3, 3])
decoder.encoder.stages.0.0.convs.1.conv.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.norm.weight shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.norm.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.all_modules.0.weight shape torch.Size([32, 32, 3, 3, 3])
decoder.encoder.stages.0.0.convs.1.all_modules.0.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.all_modules.1.weight shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.all_modules.1.bias shape torch.Size([32])
decoder.encoder.stages.1.0.convs.0.conv.weight shape torch.Size([64, 32, 3, 3, 3])
decoder.encoder.stages.1.0.convs.0.conv.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.norm.weight shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.norm.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.all_modules.0.weight shape torch.Size([64, 32, 3, 3, 3])
decoder.encoder.stages.1.0.convs.0.all_modules.0.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.all_modules.1.weight shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.all_modules.1.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.conv.weight shape torch.Size([64, 64, 3, 3, 3])
decoder.encoder.stages.1.0.convs.1.conv.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.norm.weight shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.norm.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.all_modules.0.weight shape torch.Size([64, 64, 3, 3, 3])
decoder.encoder.stages.1.0.convs.1.all_modules.0.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.all_modules.1.weight shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.all_modules.1.bias shape torch.Size([64])
decoder.encoder.stages.2.0.convs.0.conv.weight shape torch.Size([128, 64, 3, 3, 3])
decoder.encoder.stages.2.0.convs.0.conv.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.norm.weight shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.norm.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.all_modules.0.weight shape torch.Size([128, 64, 3, 3, 3])
decoder.encoder.stages.2.0.convs.0.all_modules.0.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.all_modules.1.weight shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.all_modules.1.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.conv.weight shape torch.Size([128, 128, 3, 3, 3])
decoder.encoder.stages.2.0.convs.1.conv.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.norm.weight shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.norm.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.all_modules.0.weight shape torch.Size([128, 128, 3, 3, 3])
decoder.encoder.stages.2.0.convs.1.all_modules.0.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.all_modules.1.weight shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.all_modules.1.bias shape torch.Size([128])
decoder.encoder.stages.3.0.convs.0.conv.weight shape torch.Size([256, 128, 3, 3, 3])
decoder.encoder.stages.3.0.convs.0.conv.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.norm.weight shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.norm.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.all_modules.0.weight shape torch.Size([256, 128, 3, 3, 3])
decoder.encoder.stages.3.0.convs.0.all_modules.0.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.all_modules.1.weight shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.all_modules.1.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.conv.weight shape torch.Size([256, 256, 3, 3, 3])
decoder.encoder.stages.3.0.convs.1.conv.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.norm.weight shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.norm.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.all_modules.0.weight shape torch.Size([256, 256, 3, 3, 3])
decoder.encoder.stages.3.0.convs.1.all_modules.0.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.all_modules.1.weight shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.all_modules.1.bias shape torch.Size([256])
decoder.encoder.stages.4.0.convs.0.conv.weight shape torch.Size([320, 256, 3, 3, 3])
decoder.encoder.stages.4.0.convs.0.conv.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.norm.weight shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.norm.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.all_modules.0.weight shape torch.Size([320, 256, 3, 3, 3])
decoder.encoder.stages.4.0.convs.0.all_modules.0.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.all_modules.1.weight shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.all_modules.1.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.4.0.convs.1.conv.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.norm.weight shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.norm.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.4.0.convs.1.all_modules.0.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.all_modules.1.weight shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.all_modules.1.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.conv.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.5.0.convs.0.conv.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.norm.weight shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.norm.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.5.0.convs.0.all_modules.0.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.all_modules.1.weight shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.all_modules.1.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.5.0.convs.1.conv.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.norm.weight shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.norm.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.5.0.convs.1.all_modules.0.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.all_modules.1.weight shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.all_modules.1.bias shape torch.Size([320])
decoder.stages.0.convs.0.conv.weight shape torch.Size([320, 640, 3, 3, 3])
decoder.stages.0.convs.0.conv.bias shape torch.Size([320])
decoder.stages.0.convs.0.norm.weight shape torch.Size([320])
decoder.stages.0.convs.0.norm.bias shape torch.Size([320])
decoder.stages.0.convs.0.all_modules.0.weight shape torch.Size([320, 640, 3, 3, 3])
decoder.stages.0.convs.0.all_modules.0.bias shape torch.Size([320])
decoder.stages.0.convs.0.all_modules.1.weight shape torch.Size([320])
decoder.stages.0.convs.0.all_modules.1.bias shape torch.Size([320])
decoder.stages.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.stages.0.convs.1.conv.bias shape torch.Size([320])
decoder.stages.0.convs.1.norm.weight shape torch.Size([320])
decoder.stages.0.convs.1.norm.bias shape torch.Size([320])
decoder.stages.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.stages.0.convs.1.all_modules.0.bias shape torch.Size([320])
decoder.stages.0.convs.1.all_modules.1.weight shape torch.Size([320])
decoder.stages.0.convs.1.all_modules.1.bias shape torch.Size([320])
decoder.stages.1.convs.0.conv.weight shape torch.Size([256, 512, 3, 3, 3])
decoder.stages.1.convs.0.conv.bias shape torch.Size([256])
decoder.stages.1.convs.0.norm.weight shape torch.Size([256])
decoder.stages.1.convs.0.norm.bias shape torch.Size([256])
decoder.stages.1.convs.0.all_modules.0.weight shape torch.Size([256, 512, 3, 3, 3])
decoder.stages.1.convs.0.all_modules.0.bias shape torch.Size([256])
decoder.stages.1.convs.0.all_modules.1.weight shape torch.Size([256])
decoder.stages.1.convs.0.all_modules.1.bias shape torch.Size([256])
decoder.stages.1.convs.1.conv.weight shape torch.Size([256, 256, 3, 3, 3])
decoder.stages.1.convs.1.conv.bias shape torch.Size([256])
decoder.stages.1.convs.1.norm.weight shape torch.Size([256])
decoder.stages.1.convs.1.norm.bias shape torch.Size([256])
decoder.stages.1.convs.1.all_modules.0.weight shape torch.Size([256, 256, 3, 3, 3])
decoder.stages.1.convs.1.all_modules.0.bias shape torch.Size([256])
decoder.stages.1.convs.1.all_modules.1.weight shape torch.Size([256])
decoder.stages.1.convs.1.all_modules.1.bias shape torch.Size([256])
decoder.stages.2.convs.0.conv.weight shape torch.Size([128, 256, 3, 3, 3])
decoder.stages.2.convs.0.conv.bias shape torch.Size([128])
decoder.stages.2.convs.0.norm.weight shape torch.Size([128])
decoder.stages.2.convs.0.norm.bias shape torch.Size([128])
decoder.stages.2.convs.0.all_modules.0.weight shape torch.Size([128, 256, 3, 3, 3])
decoder.stages.2.convs.0.all_modules.0.bias shape torch.Size([128])
decoder.stages.2.convs.0.all_modules.1.weight shape torch.Size([128])
decoder.stages.2.convs.0.all_modules.1.bias shape torch.Size([128])
decoder.stages.2.convs.1.conv.weight shape torch.Size([128, 128, 3, 3, 3])
decoder.stages.2.convs.1.conv.bias shape torch.Size([128])
decoder.stages.2.convs.1.norm.weight shape torch.Size([128])
decoder.stages.2.convs.1.norm.bias shape torch.Size([128])
decoder.stages.2.convs.1.all_modules.0.weight shape torch.Size([128, 128, 3, 3, 3])
decoder.stages.2.convs.1.all_modules.0.bias shape torch.Size([128])
decoder.stages.2.convs.1.all_modules.1.weight shape torch.Size([128])
decoder.stages.2.convs.1.all_modules.1.bias shape torch.Size([128])
decoder.stages.3.convs.0.conv.weight shape torch.Size([64, 128, 3, 3, 3])
decoder.stages.3.convs.0.conv.bias shape torch.Size([64])
decoder.stages.3.convs.0.norm.weight shape torch.Size([64])
decoder.stages.3.convs.0.norm.bias shape torch.Size([64])
decoder.stages.3.convs.0.all_modules.0.weight shape torch.Size([64, 128, 3, 3, 3])
decoder.stages.3.convs.0.all_modules.0.bias shape torch.Size([64])
decoder.stages.3.convs.0.all_modules.1.weight shape torch.Size([64])
decoder.stages.3.convs.0.all_modules.1.bias shape torch.Size([64])
decoder.stages.3.convs.1.conv.weight shape torch.Size([64, 64, 3, 3, 3])
decoder.stages.3.convs.1.conv.bias shape torch.Size([64])
decoder.stages.3.convs.1.norm.weight shape torch.Size([64])
decoder.stages.3.convs.1.norm.bias shape torch.Size([64])
decoder.stages.3.convs.1.all_modules.0.weight shape torch.Size([64, 64, 3, 3, 3])
decoder.stages.3.convs.1.all_modules.0.bias shape torch.Size([64])
decoder.stages.3.convs.1.all_modules.1.weight shape torch.Size([64])
decoder.stages.3.convs.1.all_modules.1.bias shape torch.Size([64])
decoder.stages.4.convs.0.conv.weight shape torch.Size([32, 64, 3, 3, 3])
decoder.stages.4.convs.0.conv.bias shape torch.Size([32])
decoder.stages.4.convs.0.norm.weight shape torch.Size([32])
decoder.stages.4.convs.0.norm.bias shape torch.Size([32])
decoder.stages.4.convs.0.all_modules.0.weight shape torch.Size([32, 64, 3, 3, 3])
decoder.stages.4.convs.0.all_modules.0.bias shape torch.Size([32])
decoder.stages.4.convs.0.all_modules.1.weight shape torch.Size([32])
decoder.stages.4.convs.0.all_modules.1.bias shape torch.Size([32])
decoder.stages.4.convs.1.conv.weight shape torch.Size([32, 32, 3, 3, 3])
decoder.stages.4.convs.1.conv.bias shape torch.Size([32])
decoder.stages.4.convs.1.norm.weight shape torch.Size([32])
decoder.stages.4.convs.1.norm.bias shape torch.Size([32])
decoder.stages.4.convs.1.all_modules.0.weight shape torch.Size([32, 32, 3, 3, 3])
decoder.stages.4.convs.1.all_modules.0.bias shape torch.Size([32])
decoder.stages.4.convs.1.all_modules.1.weight shape torch.Size([32])
decoder.stages.4.convs.1.all_modules.1.bias shape torch.Size([32])
decoder.transpconvs.0.weight shape torch.Size([320, 320, 1, 2, 2])
decoder.transpconvs.0.bias shape torch.Size([320])
decoder.transpconvs.1.weight shape torch.Size([320, 256, 2, 2, 2])
decoder.transpconvs.1.bias shape torch.Size([256])
decoder.transpconvs.2.weight shape torch.Size([256, 128, 2, 2, 2])
decoder.transpconvs.2.bias shape torch.Size([128])
decoder.transpconvs.3.weight shape torch.Size([128, 64, 2, 2, 2])
decoder.transpconvs.3.bias shape torch.Size([64])
decoder.transpconvs.4.weight shape torch.Size([64, 32, 2, 2, 2])
decoder.transpconvs.4.bias shape torch.Size([32])
################### Done ###################
2024-12-08 03:55:30.110745: do_dummy_2d_data_aug: True
2024-12-08 03:55:30.113144: Using splits from existing split file: /home/gridsan/nchutisilp/datasets/nnUNet_Datasets/nnUNet_preprocessed/Dataset307_Sohee_Calcium_OCT_CrossValidation/splits_final.json
2024-12-08 03:55:30.115546: The split file contains 5 splits.
2024-12-08 03:55:30.116568: Desired fold for training: 4
2024-12-08 03:55:30.117538: This split has 7 training and 1 validation cases.
using pin_memory on device 0
using pin_memory on device 0

This is the configuration used by this training:
Configuration name: 3d_32x160x128_b10
 {'data_identifier': 'nnUNetPlans_3d_fullres', 'preprocessor_name': 'DefaultPreprocessor', 'batch_size': 10, 'patch_size': [32, 160, 128], 'median_image_size_in_voxels': [375.0, 498.0, 498.0], 'spacing': [1.0, 1.0, 1.0], 'normalization_schemes': ['ZScoreNormalization'], 'use_mask_for_norm': [False], 'resampling_fn_data': 'resample_data_or_seg_to_shape', 'resampling_fn_seg': 'resample_data_or_seg_to_shape', 'resampling_fn_data_kwargs': {'is_seg': False, 'order': 3, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_seg_kwargs': {'is_seg': True, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_probabilities': 'resample_data_or_seg_to_shape', 'resampling_fn_probabilities_kwargs': {'is_seg': False, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'architecture': {'network_class_name': 'dynamic_network_architectures.architectures.unet.PlainConvUNet', 'arch_kwargs': {'n_stages': 6, 'features_per_stage': [32, 64, 128, 256, 320, 320], 'conv_op': 'torch.nn.modules.conv.Conv3d', 'kernel_sizes': [[3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3]], 'strides': [[1, 1, 1], [2, 2, 2], [2, 2, 2], [2, 2, 2], [2, 2, 2], [1, 2, 2]], 'n_conv_per_stage': [2, 2, 2, 2, 2, 2], 'n_conv_per_stage_decoder': [2, 2, 2, 2, 2], 'conv_bias': True, 'norm_op': 'torch.nn.modules.instancenorm.InstanceNorm3d', 'norm_op_kwargs': {'eps': 1e-05, 'affine': True}, 'dropout_op': None, 'dropout_op_kwargs': None, 'nonlin': 'torch.nn.LeakyReLU', 'nonlin_kwargs': {'inplace': True}, 'deep_supervision': True}, '_kw_requires_import': ['conv_op', 'norm_op', 'dropout_op', 'nonlin']}, 'batch_dice': True, 'inherits_from': '3d_fullres'} 

These are the global plan.json settings:
 {'dataset_name': 'Dataset307_Sohee_Calcium_OCT_CrossValidation', 'plans_name': 'nnUNetPlans', 'original_median_spacing_after_transp': [1.0, 1.0, 1.0], 'original_median_shape_after_transp': [375, 498, 498], 'image_reader_writer': 'SimpleITKIO', 'transpose_forward': [0, 1, 2], 'transpose_backward': [0, 1, 2], 'experiment_planner_used': 'ExperimentPlanner', 'label_manager': 'LabelManager', 'foreground_intensity_properties_per_channel': {'0': {'max': 0.4977976381778717, 'mean': 0.13507525622844696, 'median': 0.09599608182907104, 'min': 0.0, 'percentile_00_5': 0.014754901640117168, 'percentile_99_5': 0.4977976381778717, 'std': 0.12025152146816254}}} 

2024-12-08 03:55:52.904984: unpacking dataset...
2024-12-08 03:55:57.330439: unpacking done...
2024-12-08 03:55:57.608516: Unable to plot network architecture: nnUNet_compile is enabled!
2024-12-08 03:55:57.937046: 
2024-12-08 03:55:57.938394: Epoch 0
2024-12-08 03:55:57.939441: Current learning rate: 0.01
2024-12-08 04:02:06.651655: Validation loss improved from 1000.00000 to -0.23830! Patience: 0/50
2024-12-08 04:02:06.652789: train_loss -0.0619
2024-12-08 04:02:06.653697: val_loss -0.2383
2024-12-08 04:02:06.654551: Pseudo dice [0.5706]
2024-12-08 04:02:06.655301: Epoch time: 368.72 s
2024-12-08 04:02:06.656150: Yayy! New best EMA pseudo Dice: 0.5706
2024-12-08 04:02:09.110926: 
2024-12-08 04:02:09.112121: Epoch 1
2024-12-08 04:02:09.112771: Current learning rate: 0.00999
2024-12-08 04:06:39.437405: Validation loss improved from -0.23830 to -0.26822! Patience: 0/50
2024-12-08 04:06:39.438490: train_loss -0.218
2024-12-08 04:06:39.439242: val_loss -0.2682
2024-12-08 04:06:39.439992: Pseudo dice [0.5606]
2024-12-08 04:06:39.440727: Epoch time: 270.33 s
2024-12-08 04:06:40.782331: 
2024-12-08 04:06:40.783578: Epoch 2
2024-12-08 04:06:40.784274: Current learning rate: 0.00998
2024-12-08 04:11:14.600004: Validation loss did not improve from -0.26822. Patience: 1/50
2024-12-08 04:11:14.601057: train_loss -0.2443
2024-12-08 04:11:14.601861: val_loss -0.2545
2024-12-08 04:11:14.602647: Pseudo dice [0.5597]
2024-12-08 04:11:14.603355: Epoch time: 273.82 s
2024-12-08 04:11:16.008720: 
2024-12-08 04:11:16.009860: Epoch 3
2024-12-08 04:11:16.010539: Current learning rate: 0.00997
2024-12-08 04:15:03.446585: Validation loss improved from -0.26822 to -0.32698! Patience: 1/50
2024-12-08 04:15:03.447679: train_loss -0.3215
2024-12-08 04:15:03.448603: val_loss -0.327
2024-12-08 04:15:03.449309: Pseudo dice [0.616]
2024-12-08 04:15:03.450166: Epoch time: 227.44 s
2024-12-08 04:15:03.450959: Yayy! New best EMA pseudo Dice: 0.5733
2024-12-08 04:15:05.306049: 
2024-12-08 04:15:05.309236: Epoch 4
2024-12-08 04:15:05.310970: Current learning rate: 0.00996
2024-12-08 04:19:26.876659: Validation loss did not improve from -0.32698. Patience: 1/50
2024-12-08 04:19:26.877735: train_loss -0.3322
2024-12-08 04:19:26.878563: val_loss -0.3132
2024-12-08 04:19:26.879312: Pseudo dice [0.6142]
2024-12-08 04:19:26.880084: Epoch time: 261.58 s
2024-12-08 04:19:27.296213: Yayy! New best EMA pseudo Dice: 0.5774
2024-12-08 04:19:29.165725: 
2024-12-08 04:19:29.167535: Epoch 5
2024-12-08 04:19:29.168515: Current learning rate: 0.00995
2024-12-08 04:23:55.597236: Validation loss did not improve from -0.32698. Patience: 2/50
2024-12-08 04:23:55.598234: train_loss -0.3586
2024-12-08 04:23:55.599265: val_loss -0.3104
2024-12-08 04:23:55.600127: Pseudo dice [0.6114]
2024-12-08 04:23:55.600939: Epoch time: 266.43 s
2024-12-08 04:23:55.601717: Yayy! New best EMA pseudo Dice: 0.5808
2024-12-08 04:23:57.379112: 
2024-12-08 04:23:57.380455: Epoch 6
2024-12-08 04:23:57.381077: Current learning rate: 0.00995
2024-12-08 04:28:28.355458: Validation loss did not improve from -0.32698. Patience: 3/50
2024-12-08 04:28:28.356547: train_loss -0.3762
2024-12-08 04:28:28.357389: val_loss -0.2884
2024-12-08 04:28:28.358027: Pseudo dice [0.6155]
2024-12-08 04:28:28.358703: Epoch time: 270.98 s
2024-12-08 04:28:28.359335: Yayy! New best EMA pseudo Dice: 0.5843
2024-12-08 04:28:30.130639: 
2024-12-08 04:28:30.132017: Epoch 7
2024-12-08 04:28:30.132757: Current learning rate: 0.00994
2024-12-08 04:33:13.543854: Validation loss improved from -0.32698 to -0.38183! Patience: 3/50
2024-12-08 04:33:13.544851: train_loss -0.4017
2024-12-08 04:33:13.545659: val_loss -0.3818
2024-12-08 04:33:13.546370: Pseudo dice [0.6534]
2024-12-08 04:33:13.547001: Epoch time: 283.42 s
2024-12-08 04:33:13.547716: Yayy! New best EMA pseudo Dice: 0.5912
2024-12-08 04:33:15.688403: 
2024-12-08 04:33:15.689895: Epoch 8
2024-12-08 04:33:15.690886: Current learning rate: 0.00993
2024-12-08 04:37:50.343135: Validation loss improved from -0.38183 to -0.39537! Patience: 0/50
2024-12-08 04:37:50.344273: train_loss -0.421
2024-12-08 04:37:50.345083: val_loss -0.3954
2024-12-08 04:37:50.353606: Pseudo dice [0.6568]
2024-12-08 04:37:50.354496: Epoch time: 274.66 s
2024-12-08 04:37:50.355525: Yayy! New best EMA pseudo Dice: 0.5978
2024-12-08 04:37:52.234294: 
2024-12-08 04:37:52.235580: Epoch 9
2024-12-08 04:37:52.236519: Current learning rate: 0.00992
2024-12-08 04:42:31.077711: Validation loss improved from -0.39537 to -0.41417! Patience: 0/50
2024-12-08 04:42:31.086589: train_loss -0.4465
2024-12-08 04:42:31.087990: val_loss -0.4142
2024-12-08 04:42:31.088989: Pseudo dice [0.666]
2024-12-08 04:42:31.089730: Epoch time: 278.86 s
2024-12-08 04:42:31.532774: Yayy! New best EMA pseudo Dice: 0.6046
2024-12-08 04:42:33.224393: 
2024-12-08 04:42:33.225638: Epoch 10
2024-12-08 04:42:33.226316: Current learning rate: 0.00991
2024-12-08 04:47:07.588382: Validation loss did not improve from -0.41417. Patience: 1/50
2024-12-08 04:47:07.589675: train_loss -0.4545
2024-12-08 04:47:07.590645: val_loss -0.4016
2024-12-08 04:47:07.591329: Pseudo dice [0.6616]
2024-12-08 04:47:07.592288: Epoch time: 274.37 s
2024-12-08 04:47:07.593023: Yayy! New best EMA pseudo Dice: 0.6103
2024-12-08 04:47:09.341973: 
2024-12-08 04:47:09.343345: Epoch 11
2024-12-08 04:47:09.344133: Current learning rate: 0.0099
2024-12-08 04:51:45.598893: Validation loss improved from -0.41417 to -0.42587! Patience: 1/50
2024-12-08 04:51:45.599783: train_loss -0.471
2024-12-08 04:51:45.600685: val_loss -0.4259
2024-12-08 04:51:45.601391: Pseudo dice [0.6845]
2024-12-08 04:51:45.602362: Epoch time: 276.26 s
2024-12-08 04:51:45.603091: Yayy! New best EMA pseudo Dice: 0.6177
2024-12-08 04:51:47.390580: 
2024-12-08 04:51:47.391899: Epoch 12
2024-12-08 04:51:47.392678: Current learning rate: 0.00989
2024-12-08 04:56:14.865107: Validation loss did not improve from -0.42587. Patience: 1/50
2024-12-08 04:56:14.866151: train_loss -0.49
2024-12-08 04:56:14.867069: val_loss -0.4181
2024-12-08 04:56:14.867895: Pseudo dice [0.684]
2024-12-08 04:56:14.868786: Epoch time: 267.48 s
2024-12-08 04:56:14.869534: Yayy! New best EMA pseudo Dice: 0.6243
2024-12-08 04:56:16.759117: 
2024-12-08 04:56:16.761972: Epoch 13
2024-12-08 04:56:16.763179: Current learning rate: 0.00988
2024-12-08 05:00:47.489915: Validation loss improved from -0.42587 to -0.43659! Patience: 1/50
2024-12-08 05:00:47.491163: train_loss -0.4903
2024-12-08 05:00:47.491953: val_loss -0.4366
2024-12-08 05:00:47.492678: Pseudo dice [0.6838]
2024-12-08 05:00:47.493354: Epoch time: 270.74 s
2024-12-08 05:00:47.494050: Yayy! New best EMA pseudo Dice: 0.6303
2024-12-08 05:00:49.313557: 
2024-12-08 05:00:49.315990: Epoch 14
2024-12-08 05:00:49.316882: Current learning rate: 0.00987
2024-12-08 05:05:24.639490: Validation loss did not improve from -0.43659. Patience: 1/50
2024-12-08 05:05:24.643365: train_loss -0.4953
2024-12-08 05:05:24.645867: val_loss -0.4119
2024-12-08 05:05:24.646601: Pseudo dice [0.6572]
2024-12-08 05:05:24.647911: Epoch time: 275.33 s
2024-12-08 05:05:25.077967: Yayy! New best EMA pseudo Dice: 0.633
2024-12-08 05:05:26.851006: 
2024-12-08 05:05:26.852319: Epoch 15
2024-12-08 05:05:26.853098: Current learning rate: 0.00986
2024-12-08 05:09:52.114589: Validation loss improved from -0.43659 to -0.47566! Patience: 1/50
2024-12-08 05:09:52.115453: train_loss -0.4985
2024-12-08 05:09:52.116312: val_loss -0.4757
2024-12-08 05:09:52.116966: Pseudo dice [0.7084]
2024-12-08 05:09:52.117626: Epoch time: 265.27 s
2024-12-08 05:09:52.118236: Yayy! New best EMA pseudo Dice: 0.6405
2024-12-08 05:09:53.912597: 
2024-12-08 05:09:53.914089: Epoch 16
2024-12-08 05:09:53.914916: Current learning rate: 0.00986
2024-12-08 05:14:23.838126: Validation loss did not improve from -0.47566. Patience: 1/50
2024-12-08 05:14:23.838997: train_loss -0.5225
2024-12-08 05:14:23.839849: val_loss -0.4207
2024-12-08 05:14:23.840749: Pseudo dice [0.6916]
2024-12-08 05:14:23.841848: Epoch time: 269.93 s
2024-12-08 05:14:23.842860: Yayy! New best EMA pseudo Dice: 0.6456
2024-12-08 05:14:25.661724: 
2024-12-08 05:14:25.663292: Epoch 17
2024-12-08 05:14:25.664378: Current learning rate: 0.00985
2024-12-08 05:19:26.721500: Validation loss did not improve from -0.47566. Patience: 2/50
2024-12-08 05:19:26.722692: train_loss -0.5275
2024-12-08 05:19:26.723577: val_loss -0.4215
2024-12-08 05:19:26.724332: Pseudo dice [0.6645]
2024-12-08 05:19:26.725014: Epoch time: 301.06 s
2024-12-08 05:19:26.725787: Yayy! New best EMA pseudo Dice: 0.6475
2024-12-08 05:19:28.616712: 
2024-12-08 05:19:28.617999: Epoch 18
2024-12-08 05:19:28.618766: Current learning rate: 0.00984
2024-12-08 05:24:00.278563: Validation loss did not improve from -0.47566. Patience: 3/50
2024-12-08 05:24:00.279569: train_loss -0.5277
2024-12-08 05:24:00.280344: val_loss -0.3945
2024-12-08 05:24:00.281065: Pseudo dice [0.6738]
2024-12-08 05:24:00.281876: Epoch time: 271.66 s
2024-12-08 05:24:00.282789: Yayy! New best EMA pseudo Dice: 0.6502
2024-12-08 05:24:03.391572: 
2024-12-08 05:24:03.392930: Epoch 19
2024-12-08 05:24:03.393765: Current learning rate: 0.00983
2024-12-08 05:28:29.587578: Validation loss did not improve from -0.47566. Patience: 4/50
2024-12-08 05:28:29.588496: train_loss -0.5229
2024-12-08 05:28:29.589234: val_loss -0.4177
2024-12-08 05:28:29.589973: Pseudo dice [0.675]
2024-12-08 05:28:29.590739: Epoch time: 266.2 s
2024-12-08 05:28:30.008705: Yayy! New best EMA pseudo Dice: 0.6526
2024-12-08 05:28:31.837330: 
2024-12-08 05:28:31.838570: Epoch 20
2024-12-08 05:28:31.839409: Current learning rate: 0.00982
2024-12-08 05:32:54.805743: Validation loss did not improve from -0.47566. Patience: 5/50
2024-12-08 05:32:54.806416: train_loss -0.5253
2024-12-08 05:32:54.807204: val_loss -0.4043
2024-12-08 05:32:54.807825: Pseudo dice [0.6934]
2024-12-08 05:32:54.808435: Epoch time: 262.97 s
2024-12-08 05:32:54.809059: Yayy! New best EMA pseudo Dice: 0.6567
2024-12-08 05:32:56.678670: 
2024-12-08 05:32:56.679598: Epoch 21
2024-12-08 05:32:56.680324: Current learning rate: 0.00981
2024-12-08 05:37:20.562472: Validation loss did not improve from -0.47566. Patience: 6/50
2024-12-08 05:37:20.563463: train_loss -0.5355
2024-12-08 05:37:20.564467: val_loss -0.4468
2024-12-08 05:37:20.565313: Pseudo dice [0.6873]
2024-12-08 05:37:20.566145: Epoch time: 263.89 s
2024-12-08 05:37:20.566906: Yayy! New best EMA pseudo Dice: 0.6598
2024-12-08 05:37:22.341109: 
2024-12-08 05:37:22.342248: Epoch 22
2024-12-08 05:37:22.343143: Current learning rate: 0.0098
2024-12-08 05:41:47.365080: Validation loss did not improve from -0.47566. Patience: 7/50
2024-12-08 05:41:47.366075: train_loss -0.5307
2024-12-08 05:41:47.367080: val_loss -0.4378
2024-12-08 05:41:47.367936: Pseudo dice [0.6868]
2024-12-08 05:41:47.368813: Epoch time: 265.03 s
2024-12-08 05:41:47.369601: Yayy! New best EMA pseudo Dice: 0.6625
2024-12-08 05:41:49.125304: 
2024-12-08 05:41:49.126951: Epoch 23
2024-12-08 05:41:49.127748: Current learning rate: 0.00979
2024-12-08 05:46:15.520693: Validation loss did not improve from -0.47566. Patience: 8/50
2024-12-08 05:46:15.521649: train_loss -0.5542
2024-12-08 05:46:15.522334: val_loss -0.4298
2024-12-08 05:46:15.522996: Pseudo dice [0.6756]
2024-12-08 05:46:15.523726: Epoch time: 266.4 s
2024-12-08 05:46:15.524313: Yayy! New best EMA pseudo Dice: 0.6638
2024-12-08 05:46:17.325870: 
2024-12-08 05:46:17.327173: Epoch 24
2024-12-08 05:46:17.328193: Current learning rate: 0.00978
2024-12-08 05:50:43.797468: Validation loss did not improve from -0.47566. Patience: 9/50
2024-12-08 05:50:43.798294: train_loss -0.5701
2024-12-08 05:50:43.798969: val_loss -0.4452
2024-12-08 05:50:43.799694: Pseudo dice [0.6941]
2024-12-08 05:50:43.800303: Epoch time: 266.47 s
2024-12-08 05:50:44.140698: Yayy! New best EMA pseudo Dice: 0.6668
2024-12-08 05:50:45.890053: 
2024-12-08 05:50:45.891226: Epoch 25
2024-12-08 05:50:45.891924: Current learning rate: 0.00977
2024-12-08 05:53:39.706135: failed to log:  (<class 'FileNotFoundError'>, FileNotFoundError(2, 'No such file or directory'), <traceback object at 0x7f69a1b876c0>)
2024-12-08 05:53:39.706135: failed to log:  (<class 'FileNotFoundError'>, FileNotFoundError(2, 'No such file or directory'), <traceback object at 0x7f69a1b876c0>)
2024-12-08 05:53:39.706135: failed to log:  (<class 'FileNotFoundError'>, FileNotFoundError(2, 'No such file or directory'), <traceback object at 0x7f69a1b876c0>)
2024-12-08 05:53:39.706135: failed to log:  (<class 'FileNotFoundError'>, FileNotFoundError(2, 'No such file or directory'), <traceback object at 0x7f69a1b876c0>)
2024-12-08 05:53:39.706135: failed to log:  (<class 'FileNotFoundError'>, FileNotFoundError(2, 'No such file or directory'), <traceback object at 0x7f69a1b876c0>)
2024-12-08 05:53:39.706135: Validation loss did not improve from -0.47566. Patience: 10/50
2024-12-08 05:53:42.210211: failed to log:  (<class 'FileNotFoundError'>, FileNotFoundError(2, 'No such file or directory'), <traceback object at 0x7f69a1b876c0>)
2024-12-08 05:53:42.210211: failed to log:  (<class 'FileNotFoundError'>, FileNotFoundError(2, 'No such file or directory'), <traceback object at 0x7f69a1b876c0>)
2024-12-08 05:53:42.210211: failed to log:  (<class 'FileNotFoundError'>, FileNotFoundError(2, 'No such file or directory'), <traceback object at 0x7f69a1b876c0>)
2024-12-08 05:53:42.210211: failed to log:  (<class 'FileNotFoundError'>, FileNotFoundError(2, 'No such file or directory'), <traceback object at 0x7f69a1b876c0>)
2024-12-08 05:53:42.210211: failed to log:  (<class 'FileNotFoundError'>, FileNotFoundError(2, 'No such file or directory'), <traceback object at 0x7f69a1b876c0>)
2024-12-08 05:53:42.210211: train_loss -0.5651
2024-12-08 05:53:44.713349: failed to log:  (<class 'FileNotFoundError'>, FileNotFoundError(2, 'No such file or directory'), <traceback object at 0x7f69a0460500>)
2024-12-08 05:53:44.713349: failed to log:  (<class 'FileNotFoundError'>, FileNotFoundError(2, 'No such file or directory'), <traceback object at 0x7f69a0460500>)
2024-12-08 05:53:44.713349: failed to log:  (<class 'FileNotFoundError'>, FileNotFoundError(2, 'No such file or directory'), <traceback object at 0x7f69a0460500>)
2024-12-08 05:53:44.713349: failed to log:  (<class 'FileNotFoundError'>, FileNotFoundError(2, 'No such file or directory'), <traceback object at 0x7f69a0460500>)
2024-12-08 05:53:44.713349: failed to log:  (<class 'FileNotFoundError'>, FileNotFoundError(2, 'No such file or directory'), <traceback object at 0x7f69a0460500>)
2024-12-08 05:53:44.713349: val_loss -0.4538
2024-12-08 05:53:47.216297: failed to log:  (<class 'FileNotFoundError'>, FileNotFoundError(2, 'No such file or directory'), <traceback object at 0x7f69a1d92180>)
2024-12-08 05:53:47.216297: failed to log:  (<class 'FileNotFoundError'>, FileNotFoundError(2, 'No such file or directory'), <traceback object at 0x7f69a1d92180>)
2024-12-08 05:53:47.216297: failed to log:  (<class 'FileNotFoundError'>, FileNotFoundError(2, 'No such file or directory'), <traceback object at 0x7f69a1d92180>)
2024-12-08 05:53:47.216297: failed to log:  (<class 'FileNotFoundError'>, FileNotFoundError(2, 'No such file or directory'), <traceback object at 0x7f69a1d92180>)
2024-12-08 05:53:47.216297: failed to log:  (<class 'FileNotFoundError'>, FileNotFoundError(2, 'No such file or directory'), <traceback object at 0x7f69a1d92180>)
2024-12-08 05:53:47.216297: Pseudo dice [0.686]
2024-12-08 05:53:49.719526: failed to log:  (<class 'FileNotFoundError'>, FileNotFoundError(2, 'No such file or directory'), <traceback object at 0x7f69a183f500>)
2024-12-08 05:53:49.719526: failed to log:  (<class 'FileNotFoundError'>, FileNotFoundError(2, 'No such file or directory'), <traceback object at 0x7f69a183f500>)
2024-12-08 05:53:49.719526: failed to log:  (<class 'FileNotFoundError'>, FileNotFoundError(2, 'No such file or directory'), <traceback object at 0x7f69a183f500>)
2024-12-08 05:53:49.719526: failed to log:  (<class 'FileNotFoundError'>, FileNotFoundError(2, 'No such file or directory'), <traceback object at 0x7f69a183f500>)
2024-12-08 05:53:49.719526: failed to log:  (<class 'FileNotFoundError'>, FileNotFoundError(2, 'No such file or directory'), <traceback object at 0x7f69a183f500>)
2024-12-08 05:53:49.719526: Epoch time: 176.32 s
2024-12-08 05:53:52.222927: failed to log:  (<class 'FileNotFoundError'>, FileNotFoundError(2, 'No such file or directory'), <traceback object at 0x7f69ee16c080>)
2024-12-08 05:53:52.222927: failed to log:  (<class 'FileNotFoundError'>, FileNotFoundError(2, 'No such file or directory'), <traceback object at 0x7f69ee16c080>)
2024-12-08 05:53:52.222927: failed to log:  (<class 'FileNotFoundError'>, FileNotFoundError(2, 'No such file or directory'), <traceback object at 0x7f69ee16c080>)
2024-12-08 05:53:52.222927: failed to log:  (<class 'FileNotFoundError'>, FileNotFoundError(2, 'No such file or directory'), <traceback object at 0x7f69ee16c080>)
2024-12-08 05:53:52.222927: failed to log:  (<class 'FileNotFoundError'>, FileNotFoundError(2, 'No such file or directory'), <traceback object at 0x7f69ee16c080>)
2024-12-08 05:53:52.222927: Yayy! New best EMA pseudo Dice: 0.6687
Traceback (most recent call last):
  File "/home/gridsan/nchutisilp/.conda/envs/nnunet/bin/nnUNetv2_train", line 8, in <module>
    sys.exit(run_training_entry())
  File "/home/gridsan/nchutisilp/projects/nnUNet/nnunetv2/run/run_training.py", line 275, in run_training_entry
    run_training(args.dataset_name_or_id, args.configuration, args.fold, args.tr, args.p, args.pretrained_weights,
  File "/home/gridsan/nchutisilp/projects/nnUNet/nnunetv2/run/run_training.py", line 211, in run_training
    nnunet_trainer.run_training()
  File "/home/gridsan/nchutisilp/projects/nnUNet/nnunetv2/training/nnUNetTrainer/nnUNetTrainer.py", line 1411, in run_training
    self.on_epoch_end()
  File "/home/gridsan/nchutisilp/projects/nnUNet/nnunetv2/training/nnUNetTrainer/nnUNetTrainer.py", line 1171, in on_epoch_end
    self.save_checkpoint(join(self.output_folder, 'checkpoint_best.pth'))
  File "/home/gridsan/nchutisilp/projects/nnUNet/nnunetv2/training/nnUNetTrainer/nnUNetTrainer.py", line 1201, in save_checkpoint
    torch.save(checkpoint, filename)
  File "/home/gridsan/nchutisilp/.local/lib/python3.9/site-packages/torch/serialization.py", line 627, in save
    with _open_zipfile_writer(f) as opened_zipfile:
  File "/home/gridsan/nchutisilp/.local/lib/python3.9/site-packages/torch/serialization.py", line 501, in _open_zipfile_writer
    return container(name_or_buffer)
  File "/home/gridsan/nchutisilp/.local/lib/python3.9/site-packages/torch/serialization.py", line 472, in __init__
    super().__init__(torch._C.PyTorchFileWriter(self.name))
RuntimeError: Parent directory /home/gridsan/nchutisilp/datasets/nnUNet_Datasets/nnUNet_results/Dataset307_Sohee_Calcium_OCT_CrossValidation/nnUNetTrainer__nnUNetPlans__3d_32x160x128_b10/fold_4 does not exist.
Exception in thread Thread-2:
Traceback (most recent call last):
  File "/home/gridsan/nchutisilp/.conda/envs/nnunet/lib/python3.9/threading.py", line 980, in _bootstrap_inner
Exception in thread Thread-3:
Traceback (most recent call last):
  File "/home/gridsan/nchutisilp/.conda/envs/nnunet/lib/python3.9/threading.py", line 980, in _bootstrap_inner
    self.run()
  File "/home/gridsan/nchutisilp/.conda/envs/nnunet/lib/python3.9/threading.py", line 917, in run
    self.run()
  File "/home/gridsan/nchutisilp/.conda/envs/nnunet/lib/python3.9/threading.py", line 917, in run
    self._target(*self._args, **self._kwargs)
  File "/home/gridsan/nchutisilp/.local/lib/python3.9/site-packages/batchgenerators/dataloading/nondet_multi_threaded_augmenter.py", line 125, in results_loop
    self._target(*self._args, **self._kwargs)
  File "/home/gridsan/nchutisilp/.local/lib/python3.9/site-packages/batchgenerators/dataloading/nondet_multi_threaded_augmenter.py", line 125, in results_loop
    raise e
  File "/home/gridsan/nchutisilp/.local/lib/python3.9/site-packages/batchgenerators/dataloading/nondet_multi_threaded_augmenter.py", line 103, in results_loop
    raise e
  File "/home/gridsan/nchutisilp/.local/lib/python3.9/site-packages/batchgenerators/dataloading/nondet_multi_threaded_augmenter.py", line 103, in results_loop
    raise RuntimeError("One or more background workers are no longer alive. Exiting. Please check the "
RuntimeError: One or more background workers are no longer alive. Exiting. Please check the print statements above for the actual error message
    raise RuntimeError("One or more background workers are no longer alive. Exiting. Please check the "
RuntimeError: One or more background workers are no longer alive. Exiting. Please check the print statements above for the actual error message
/var/spool/slurmd/job27567897/slurm_script: line 44: 1766056 Aborted                 CUDA_VISIBLE_DEVICES=0 nnUNetv2_train $DATASET_ID $CONFIG 4 -tr $TRAINER -device cuda -pretrained_weights $PATH_TO_CHECKPOINT
mv: cannot stat '/home/gridsan/nchutisilp/datasets/nnUNet_Datasets/nnUNet_results/Dataset309_Sohee_Calcium_OCT_CrossValidation/nnUNetTrainer__nnUNetPlans__3d_32x160x128_b10/fold_4': No such file or directory
