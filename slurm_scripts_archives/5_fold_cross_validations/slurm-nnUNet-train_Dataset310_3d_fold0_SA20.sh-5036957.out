/nfs/erelab001/shared/Computational_Group/Naravich/nnUNet/.venv/bin/python
Starting training with CONFIG=3d_32x160x128_b10, DATASET_ID=310, TRAINER=nnUNetTrainerScaleAnalysis20
/nfs/erelab001/shared/Computational_Group/Naravich/nnUNet/nnunetv2/training/nnUNetTrainer/nnUNetTrainer.py:169: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  self.grad_scaler = GradScaler() if self.device.type == 'cuda' else None

############################
INFO: You are using the old nnU-Net default plans. We have updated our recommendations. Please consider using those instead! Read more here: https://github.com/MIC-DKFZ/nnUNet/blob/master/documentation/resenc_presets.md
############################

Using device: cuda:0

#######################################################################
Please cite the following paper when using nnU-Net:
Isensee, F., Jaeger, P. F., Kohl, S. A., Petersen, J., & Maier-Hein, K. H. (2021). nnU-Net: a self-configuring method for deep learning-based biomedical image segmentation. Nature methods, 18(2), 203-211.
#######################################################################

2025-10-05 08:43:43.243564: do_dummy_2d_data_aug: True
2025-10-05 08:43:43.244634: Using splits from existing split file: /nfs/erelab001/shared/Computational_Group/Naravich/datasets/nnUNet_Datasets/nnUNet_preprocessed/Dataset310_nnInteractive_Calcium_OCT_CrossValidation/splits_final_20.json
2025-10-05 08:43:43.246330: The split file contains 5 splits.
2025-10-05 08:43:43.246575: Desired fold for training: 0
2025-10-05 08:43:43.246841: This split has 1 training and 7 validation cases.
/nfs/erelab001/shared/Computational_Group/Naravich/nnUNet/.venv/lib/python3.12/site-packages/fft_conv_pytorch/fft_conv.py:139: UserWarning: Using a non-tuple sequence for multidimensional indexing is deprecated and will be changed in pytorch 2.9; use x[tuple(seq)] instead of x[seq]. In pytorch 2.9 this will be interpreted as tensor index, x[torch.tensor(seq)], which will result either in an error or a different result (Triggered internally at /pytorch/torch/csrc/autograd/python_variable_indexing.cpp:306.)
  output = output[crop_slices].contiguous()
/nfs/erelab001/shared/Computational_Group/Naravich/nnUNet/.venv/lib/python3.12/site-packages/fft_conv_pytorch/fft_conv.py:139: UserWarning: Using a non-tuple sequence for multidimensional indexing is deprecated and will be changed in pytorch 2.9; use x[tuple(seq)] instead of x[seq]. In pytorch 2.9 this will be interpreted as tensor index, x[torch.tensor(seq)], which will result either in an error or a different result (Triggered internally at /pytorch/torch/csrc/autograd/python_variable_indexing.cpp:306.)
  output = output[crop_slices].contiguous()
/nfs/erelab001/shared/Computational_Group/Naravich/nnUNet/.venv/lib/python3.12/site-packages/fft_conv_pytorch/fft_conv.py:139: UserWarning: Using a non-tuple sequence for multidimensional indexing is deprecated and will be changed in pytorch 2.9; use x[tuple(seq)] instead of x[seq]. In pytorch 2.9 this will be interpreted as tensor index, x[torch.tensor(seq)], which will result either in an error or a different result (Triggered internally at /pytorch/torch/csrc/autograd/python_variable_indexing.cpp:306.)
  output = output[crop_slices].contiguous()
/nfs/erelab001/shared/Computational_Group/Naravich/nnUNet/.venv/lib/python3.12/site-packages/fft_conv_pytorch/fft_conv.py:139: UserWarning: Using a non-tuple sequence for multidimensional indexing is deprecated and will be changed in pytorch 2.9; use x[tuple(seq)] instead of x[seq]. In pytorch 2.9 this will be interpreted as tensor index, x[torch.tensor(seq)], which will result either in an error or a different result (Triggered internally at /pytorch/torch/csrc/autograd/python_variable_indexing.cpp:306.)
  output = output[crop_slices].contiguous()
/nfs/erelab001/shared/Computational_Group/Naravich/nnUNet/.venv/lib/python3.12/site-packages/fft_conv_pytorch/fft_conv.py:139: UserWarning: Using a non-tuple sequence for multidimensional indexing is deprecated and will be changed in pytorch 2.9; use x[tuple(seq)] instead of x[seq]. In pytorch 2.9 this will be interpreted as tensor index, x[torch.tensor(seq)], which will result either in an error or a different result (Triggered internally at /pytorch/torch/csrc/autograd/python_variable_indexing.cpp:306.)
  output = output[crop_slices].contiguous()
/nfs/erelab001/shared/Computational_Group/Naravich/nnUNet/.venv/lib/python3.12/site-packages/fft_conv_pytorch/fft_conv.py:139: UserWarning: Using a non-tuple sequence for multidimensional indexing is deprecated and will be changed in pytorch 2.9; use x[tuple(seq)] instead of x[seq]. In pytorch 2.9 this will be interpreted as tensor index, x[torch.tensor(seq)], which will result either in an error or a different result (Triggered internally at /pytorch/torch/csrc/autograd/python_variable_indexing.cpp:306.)
  output = output[crop_slices].contiguous()
/nfs/erelab001/shared/Computational_Group/Naravich/nnUNet/.venv/lib/python3.12/site-packages/fft_conv_pytorch/fft_conv.py:139: UserWarning: Using a non-tuple sequence for multidimensional indexing is deprecated and will be changed in pytorch 2.9; use x[tuple(seq)] instead of x[seq]. In pytorch 2.9 this will be interpreted as tensor index, x[torch.tensor(seq)], which will result either in an error or a different result (Triggered internally at /pytorch/torch/csrc/autograd/python_variable_indexing.cpp:306.)
  output = output[crop_slices].contiguous()
/nfs/erelab001/shared/Computational_Group/Naravich/nnUNet/.venv/lib/python3.12/site-packages/fft_conv_pytorch/fft_conv.py:139: UserWarning: Using a non-tuple sequence for multidimensional indexing is deprecated and will be changed in pytorch 2.9; use x[tuple(seq)] instead of x[seq]. In pytorch 2.9 this will be interpreted as tensor index, x[torch.tensor(seq)], which will result either in an error or a different result (Triggered internally at /pytorch/torch/csrc/autograd/python_variable_indexing.cpp:306.)
  output = output[crop_slices].contiguous()
/nfs/erelab001/shared/Computational_Group/Naravich/nnUNet/.venv/lib/python3.12/site-packages/fft_conv_pytorch/fft_conv.py:139: UserWarning: Using a non-tuple sequence for multidimensional indexing is deprecated and will be changed in pytorch 2.9; use x[tuple(seq)] instead of x[seq]. In pytorch 2.9 this will be interpreted as tensor index, x[torch.tensor(seq)], which will result either in an error or a different result (Triggered internally at /pytorch/torch/csrc/autograd/python_variable_indexing.cpp:306.)
  output = output[crop_slices].contiguous()
/nfs/erelab001/shared/Computational_Group/Naravich/nnUNet/.venv/lib/python3.12/site-packages/fft_conv_pytorch/fft_conv.py:139: UserWarning: Using a non-tuple sequence for multidimensional indexing is deprecated and will be changed in pytorch 2.9; use x[tuple(seq)] instead of x[seq]. In pytorch 2.9 this will be interpreted as tensor index, x[torch.tensor(seq)], which will result either in an error or a different result (Triggered internally at /pytorch/torch/csrc/autograd/python_variable_indexing.cpp:306.)
  output = output[crop_slices].contiguous()
/nfs/erelab001/shared/Computational_Group/Naravich/nnUNet/.venv/lib/python3.12/site-packages/fft_conv_pytorch/fft_conv.py:139: UserWarning: Using a non-tuple sequence for multidimensional indexing is deprecated and will be changed in pytorch 2.9; use x[tuple(seq)] instead of x[seq]. In pytorch 2.9 this will be interpreted as tensor index, x[torch.tensor(seq)], which will result either in an error or a different result (Triggered internally at /pytorch/torch/csrc/autograd/python_variable_indexing.cpp:306.)
  output = output[crop_slices].contiguous()
/nfs/erelab001/shared/Computational_Group/Naravich/nnUNet/.venv/lib/python3.12/site-packages/fft_conv_pytorch/fft_conv.py:139: UserWarning: Using a non-tuple sequence for multidimensional indexing is deprecated and will be changed in pytorch 2.9; use x[tuple(seq)] instead of x[seq]. In pytorch 2.9 this will be interpreted as tensor index, x[torch.tensor(seq)], which will result either in an error or a different result (Triggered internally at /pytorch/torch/csrc/autograd/python_variable_indexing.cpp:306.)
  output = output[crop_slices].contiguous()
using pin_memory on device 0
using pin_memory on device 0
2025-10-05 08:43:49.870244: Using torch.compile...

This is the configuration used by this training:
Configuration name: 3d_32x160x128_b10
 {'data_identifier': 'nnUNetPlans_3d_fullres', 'preprocessor_name': 'DefaultPreprocessor', 'batch_size': 10, 'patch_size': [32, 160, 128], 'median_image_size_in_voxels': [375.0, 498.0, 498.0], 'spacing': [1.0, 1.0, 1.0], 'normalization_schemes': ['ZScoreNormalization'], 'use_mask_for_norm': [False], 'resampling_fn_data': 'resample_data_or_seg_to_shape', 'resampling_fn_seg': 'resample_data_or_seg_to_shape', 'resampling_fn_data_kwargs': {'is_seg': False, 'order': 3, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_seg_kwargs': {'is_seg': True, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_probabilities': 'resample_data_or_seg_to_shape', 'resampling_fn_probabilities_kwargs': {'is_seg': False, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'architecture': {'network_class_name': 'dynamic_network_architectures.architectures.unet.PlainConvUNet', 'arch_kwargs': {'n_stages': 6, 'features_per_stage': [32, 64, 128, 256, 320, 320], 'conv_op': 'torch.nn.modules.conv.Conv3d', 'kernel_sizes': [[3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3]], 'strides': [[1, 1, 1], [2, 2, 2], [2, 2, 2], [2, 2, 2], [2, 2, 2], [1, 2, 2]], 'n_conv_per_stage': [2, 2, 2, 2, 2, 2], 'n_conv_per_stage_decoder': [2, 2, 2, 2, 2], 'conv_bias': True, 'norm_op': 'torch.nn.modules.instancenorm.InstanceNorm3d', 'norm_op_kwargs': {'eps': 1e-05, 'affine': True}, 'dropout_op': None, 'dropout_op_kwargs': None, 'nonlin': 'torch.nn.LeakyReLU', 'nonlin_kwargs': {'inplace': True}, 'deep_supervision': True}, '_kw_requires_import': ['conv_op', 'norm_op', 'dropout_op', 'nonlin']}, 'batch_dice': True, 'inherits_from': '3d_fullres'} 

These are the global plan.json settings:
 {'dataset_name': 'Dataset310_nnInteractive_Calcium_OCT_CrossValidation', 'plans_name': 'nnUNetPlans', 'original_median_spacing_after_transp': [1.0, 1.0, 1.0], 'original_median_shape_after_transp': [375, 498, 498], 'image_reader_writer': 'SimpleITKIO', 'transpose_forward': [0, 1, 2], 'transpose_backward': [0, 1, 2], 'experiment_planner_used': 'ExperimentPlanner', 'label_manager': 'LabelManager', 'foreground_intensity_properties_per_channel': {'0': {'max': 0.4977976381778717, 'mean': 0.1394134759902954, 'median': 0.09849607944488525, 'min': 0.0, 'percentile_00_5': 0.015305490233004093, 'percentile_99_5': 0.4977976381778717, 'std': 0.121165432035923}}} 

2025-10-05 08:43:56.171836: unpacking dataset...
/nfs/erelab001/shared/Computational_Group/Naravich/nnUNet/.venv/lib/python3.12/site-packages/torch/_inductor/lowering.py:7095: UserWarning: 
Online softmax is disabled on the fly since Inductor decides to
split the reduction. Cut an issue to PyTorch if this is an
important use case and you want to speed it up with online
softmax.

  warnings.warn(
/nfs/erelab001/shared/Computational_Group/Naravich/nnUNet/.venv/lib/python3.12/site-packages/torch/_inductor/lowering.py:7095: UserWarning: 
Online softmax is disabled on the fly since Inductor decides to
split the reduction. Cut an issue to PyTorch if this is an
important use case and you want to speed it up with online
softmax.

  warnings.warn(
/nfs/erelab001/shared/Computational_Group/Naravich/nnUNet/.venv/lib/python3.12/site-packages/torch/_inductor/lowering.py:7095: UserWarning: 
Online softmax is disabled on the fly since Inductor decides to
split the reduction. Cut an issue to PyTorch if this is an
important use case and you want to speed it up with online
softmax.

  warnings.warn(
/nfs/erelab001/shared/Computational_Group/Naravich/nnUNet/.venv/lib/python3.12/site-packages/torch/_inductor/lowering.py:7095: UserWarning: 
Online softmax is disabled on the fly since Inductor decides to
split the reduction. Cut an issue to PyTorch if this is an
important use case and you want to speed it up with online
softmax.

  warnings.warn(
2025-10-05 08:44:00.567346: unpacking done...
2025-10-05 08:44:00.569522: Unable to plot network architecture: nnUNet_compile is enabled!
2025-10-05 08:44:00.589177: 
2025-10-05 08:44:00.589460: Epoch 0
2025-10-05 08:44:00.589655: Current learning rate: 0.01
2025-10-05 08:45:22.718659: Validation loss improved from 1000.00000 to 0.01830! Patience: 0/50
2025-10-05 08:45:22.719277: train_loss -0.1625
2025-10-05 08:45:22.719505: val_loss 0.0183
2025-10-05 08:45:22.719699: Pseudo dice [np.float32(0.4164)]
2025-10-05 08:45:22.719883: Epoch time: 82.13 s
2025-10-05 08:45:22.720071: Yayy! New best EMA pseudo Dice: 0.4163999855518341
2025-10-05 08:45:24.280153: 
2025-10-05 08:45:24.280496: Epoch 1
2025-10-05 08:45:24.280749: Current learning rate: 0.00994
2025-10-05 08:46:10.194193: Validation loss improved from 0.01830 to -0.02031! Patience: 0/50
2025-10-05 08:46:10.194777: train_loss -0.3474
2025-10-05 08:46:10.194987: val_loss -0.0203
2025-10-05 08:46:10.195143: Pseudo dice [np.float32(0.4263)]
2025-10-05 08:46:10.195297: Epoch time: 45.92 s
2025-10-05 08:46:10.195430: Yayy! New best EMA pseudo Dice: 0.4174000024795532
2025-10-05 08:46:11.489556: 
2025-10-05 08:46:11.489859: Epoch 2
2025-10-05 08:46:11.490131: Current learning rate: 0.00988
2025-10-05 08:46:57.521073: Validation loss did not improve from -0.02031. Patience: 1/50
2025-10-05 08:46:57.522153: train_loss -0.4413
2025-10-05 08:46:57.522529: val_loss 0.0157
2025-10-05 08:46:57.522870: Pseudo dice [np.float32(0.4439)]
2025-10-05 08:46:57.523227: Epoch time: 46.03 s
2025-10-05 08:46:57.523575: Yayy! New best EMA pseudo Dice: 0.41999998688697815
2025-10-05 08:46:58.686229: 
2025-10-05 08:46:58.686583: Epoch 3
2025-10-05 08:46:58.686804: Current learning rate: 0.00982
2025-10-05 08:47:44.755951: Validation loss did not improve from -0.02031. Patience: 2/50
2025-10-05 08:47:44.756465: train_loss -0.4904
2025-10-05 08:47:44.756630: val_loss 0.0057
2025-10-05 08:47:44.756860: Pseudo dice [np.float32(0.4692)]
2025-10-05 08:47:44.757048: Epoch time: 46.07 s
2025-10-05 08:47:44.757188: Yayy! New best EMA pseudo Dice: 0.42489999532699585
2025-10-05 08:47:45.843898: 
2025-10-05 08:47:45.844219: Epoch 4
2025-10-05 08:47:45.844422: Current learning rate: 0.00976
2025-10-05 08:48:31.857819: Validation loss improved from -0.02031 to -0.13552! Patience: 2/50
2025-10-05 08:48:31.870041: train_loss -0.5262
2025-10-05 08:48:31.878359: val_loss -0.1355
2025-10-05 08:48:31.884777: Pseudo dice [np.float32(0.5203)]
2025-10-05 08:48:31.893896: Epoch time: 46.03 s
2025-10-05 08:48:32.408475: Yayy! New best EMA pseudo Dice: 0.4345000088214874
2025-10-05 08:48:33.532493: 
2025-10-05 08:48:33.533180: Epoch 5
2025-10-05 08:48:33.533606: Current learning rate: 0.0097
2025-10-05 08:49:19.575668: Validation loss did not improve from -0.13552. Patience: 1/50
2025-10-05 08:49:19.576139: train_loss -0.5754
2025-10-05 08:49:19.576319: val_loss -0.1238
2025-10-05 08:49:19.576479: Pseudo dice [np.float32(0.5224)]
2025-10-05 08:49:19.576631: Epoch time: 46.04 s
2025-10-05 08:49:19.576753: Yayy! New best EMA pseudo Dice: 0.4433000087738037
2025-10-05 08:49:20.614380: 
2025-10-05 08:49:20.614796: Epoch 6
2025-10-05 08:49:20.615076: Current learning rate: 0.00964
2025-10-05 08:50:06.612764: Validation loss improved from -0.13552 to -0.15415! Patience: 1/50
2025-10-05 08:50:06.613544: train_loss -0.598
2025-10-05 08:50:06.613798: val_loss -0.1542
2025-10-05 08:50:06.614005: Pseudo dice [np.float32(0.5498)]
2025-10-05 08:50:06.614208: Epoch time: 46.0 s
2025-10-05 08:50:06.614387: Yayy! New best EMA pseudo Dice: 0.453900009393692
2025-10-05 08:50:07.736843: 
2025-10-05 08:50:07.737184: Epoch 7
2025-10-05 08:50:07.737457: Current learning rate: 0.00958
2025-10-05 08:50:53.783140: Validation loss did not improve from -0.15415. Patience: 1/50
2025-10-05 08:50:53.783685: train_loss -0.6375
2025-10-05 08:50:53.783836: val_loss -0.0258
2025-10-05 08:50:53.783986: Pseudo dice [np.float32(0.503)]
2025-10-05 08:50:53.784210: Epoch time: 46.05 s
2025-10-05 08:50:53.784370: Yayy! New best EMA pseudo Dice: 0.45879998803138733
2025-10-05 08:50:54.902786: 
2025-10-05 08:50:54.903103: Epoch 8
2025-10-05 08:50:54.903328: Current learning rate: 0.00952
2025-10-05 08:51:40.994267: Validation loss did not improve from -0.15415. Patience: 2/50
2025-10-05 08:51:40.995510: train_loss -0.6559
2025-10-05 08:51:40.996036: val_loss -0.0754
2025-10-05 08:51:40.996483: Pseudo dice [np.float32(0.5139)]
2025-10-05 08:51:40.996989: Epoch time: 46.09 s
2025-10-05 08:51:40.997455: Yayy! New best EMA pseudo Dice: 0.4643000066280365
2025-10-05 08:51:42.101403: 
2025-10-05 08:51:42.101972: Epoch 9
2025-10-05 08:51:42.102426: Current learning rate: 0.00946
2025-10-05 08:52:28.189505: Validation loss did not improve from -0.15415. Patience: 3/50
2025-10-05 08:52:28.190147: train_loss -0.6758
2025-10-05 08:52:28.190510: val_loss 0.0534
2025-10-05 08:52:28.190976: Pseudo dice [np.float32(0.4445)]
2025-10-05 08:52:28.191195: Epoch time: 46.09 s
2025-10-05 08:52:29.314157: 
2025-10-05 08:52:29.314544: Epoch 10
2025-10-05 08:52:29.314916: Current learning rate: 0.0094
2025-10-05 08:53:15.391087: Validation loss did not improve from -0.15415. Patience: 4/50
2025-10-05 08:53:15.391725: train_loss -0.6955
2025-10-05 08:53:15.391896: val_loss -0.0507
2025-10-05 08:53:15.392070: Pseudo dice [np.float32(0.5293)]
2025-10-05 08:53:15.392219: Epoch time: 46.08 s
2025-10-05 08:53:15.392342: Yayy! New best EMA pseudo Dice: 0.4690000116825104
2025-10-05 08:53:16.500732: 
2025-10-05 08:53:16.501140: Epoch 11
2025-10-05 08:53:16.501360: Current learning rate: 0.00934
2025-10-05 08:54:02.583297: Validation loss did not improve from -0.15415. Patience: 5/50
2025-10-05 08:54:02.584105: train_loss -0.7212
2025-10-05 08:54:02.584560: val_loss 0.0158
2025-10-05 08:54:02.584948: Pseudo dice [np.float32(0.4704)]
2025-10-05 08:54:02.585371: Epoch time: 46.08 s
2025-10-05 08:54:02.585766: Yayy! New best EMA pseudo Dice: 0.4691999852657318
2025-10-05 08:54:03.686842: 
2025-10-05 08:54:03.687264: Epoch 12
2025-10-05 08:54:03.687513: Current learning rate: 0.00928
2025-10-05 08:54:49.780675: Validation loss did not improve from -0.15415. Patience: 6/50
2025-10-05 08:54:49.781330: train_loss -0.7376
2025-10-05 08:54:49.781551: val_loss 0.0789
2025-10-05 08:54:49.781740: Pseudo dice [np.float32(0.4751)]
2025-10-05 08:54:49.781913: Epoch time: 46.1 s
2025-10-05 08:54:49.782112: Yayy! New best EMA pseudo Dice: 0.4697999954223633
2025-10-05 08:54:51.441730: 
2025-10-05 08:54:51.442320: Epoch 13
2025-10-05 08:54:51.442824: Current learning rate: 0.00922
2025-10-05 08:55:37.524690: Validation loss did not improve from -0.15415. Patience: 7/50
2025-10-05 08:55:37.525127: train_loss -0.7467
2025-10-05 08:55:37.525283: val_loss 0.1104
2025-10-05 08:55:37.525418: Pseudo dice [np.float32(0.4382)]
2025-10-05 08:55:37.525581: Epoch time: 46.08 s
2025-10-05 08:55:38.159881: 
2025-10-05 08:55:38.160249: Epoch 14
2025-10-05 08:55:38.160470: Current learning rate: 0.00916
2025-10-05 08:56:24.216094: Validation loss did not improve from -0.15415. Patience: 8/50
2025-10-05 08:56:24.216808: train_loss -0.7736
2025-10-05 08:56:24.217041: val_loss 0.0778
2025-10-05 08:56:24.217286: Pseudo dice [np.float32(0.4879)]
2025-10-05 08:56:24.217523: Epoch time: 46.06 s
2025-10-05 08:56:25.373868: 
2025-10-05 08:56:25.374221: Epoch 15
2025-10-05 08:56:25.374426: Current learning rate: 0.0091
2025-10-05 08:57:11.473193: Validation loss did not improve from -0.15415. Patience: 9/50
2025-10-05 08:57:11.473946: train_loss -0.7826
2025-10-05 08:57:11.474147: val_loss 0.1714
2025-10-05 08:57:11.474317: Pseudo dice [np.float32(0.4373)]
2025-10-05 08:57:11.474558: Epoch time: 46.1 s
2025-10-05 08:57:12.122344: 
2025-10-05 08:57:12.122669: Epoch 16
2025-10-05 08:57:12.122868: Current learning rate: 0.00903
2025-10-05 08:57:58.255742: Validation loss did not improve from -0.15415. Patience: 10/50
2025-10-05 08:57:58.256328: train_loss -0.7897
2025-10-05 08:57:58.256592: val_loss 0.1388
2025-10-05 08:57:58.256816: Pseudo dice [np.float32(0.5142)]
2025-10-05 08:57:58.256990: Epoch time: 46.13 s
2025-10-05 08:57:58.257169: Yayy! New best EMA pseudo Dice: 0.47040000557899475
2025-10-05 08:57:59.339964: 
2025-10-05 08:57:59.340327: Epoch 17
2025-10-05 08:57:59.340588: Current learning rate: 0.00897
2025-10-05 08:58:45.455554: Validation loss did not improve from -0.15415. Patience: 11/50
2025-10-05 08:58:45.456105: train_loss -0.7994
2025-10-05 08:58:45.456490: val_loss 0.1235
2025-10-05 08:58:45.456776: Pseudo dice [np.float32(0.4595)]
2025-10-05 08:58:45.457065: Epoch time: 46.12 s
2025-10-05 08:58:46.111608: 
2025-10-05 08:58:46.112235: Epoch 18
2025-10-05 08:58:46.112773: Current learning rate: 0.00891
2025-10-05 08:59:32.291558: Validation loss did not improve from -0.15415. Patience: 12/50
2025-10-05 08:59:32.292124: train_loss -0.8093
2025-10-05 08:59:32.292352: val_loss 0.1295
2025-10-05 08:59:32.292487: Pseudo dice [np.float32(0.4818)]
2025-10-05 08:59:32.292623: Epoch time: 46.18 s
2025-10-05 08:59:32.292747: Yayy! New best EMA pseudo Dice: 0.4706000089645386
2025-10-05 08:59:33.409868: 
2025-10-05 08:59:33.410209: Epoch 19
2025-10-05 08:59:33.410442: Current learning rate: 0.00885
2025-10-05 09:00:19.561168: Validation loss did not improve from -0.15415. Patience: 13/50
2025-10-05 09:00:19.570476: train_loss -0.8145
2025-10-05 09:00:19.578022: val_loss 0.1916
2025-10-05 09:00:19.585433: Pseudo dice [np.float32(0.4546)]
2025-10-05 09:00:19.595335: Epoch time: 46.16 s
2025-10-05 09:00:20.734838: 
2025-10-05 09:00:20.735181: Epoch 20
2025-10-05 09:00:20.735496: Current learning rate: 0.00879
2025-10-05 09:01:06.914530: Validation loss did not improve from -0.15415. Patience: 14/50
2025-10-05 09:01:06.915480: train_loss -0.8274
2025-10-05 09:01:06.915843: val_loss 0.1003
2025-10-05 09:01:06.916193: Pseudo dice [np.float32(0.4949)]
2025-10-05 09:01:06.916629: Epoch time: 46.18 s
2025-10-05 09:01:06.916986: Yayy! New best EMA pseudo Dice: 0.4715999960899353
2025-10-05 09:01:08.015018: 
2025-10-05 09:01:08.015408: Epoch 21
2025-10-05 09:01:08.015584: Current learning rate: 0.00873
2025-10-05 09:01:54.161174: Validation loss did not improve from -0.15415. Patience: 15/50
2025-10-05 09:01:54.161665: train_loss -0.8308
2025-10-05 09:01:54.161847: val_loss 0.0836
2025-10-05 09:01:54.161969: Pseudo dice [np.float32(0.5113)]
2025-10-05 09:01:54.162143: Epoch time: 46.15 s
2025-10-05 09:01:54.162295: Yayy! New best EMA pseudo Dice: 0.475600004196167
2025-10-05 09:01:55.226729: 
2025-10-05 09:01:55.227082: Epoch 22
2025-10-05 09:01:55.227429: Current learning rate: 0.00867
2025-10-05 09:02:41.337381: Validation loss did not improve from -0.15415. Patience: 16/50
2025-10-05 09:02:41.338264: train_loss -0.8344
2025-10-05 09:02:41.338541: val_loss 0.1247
2025-10-05 09:02:41.338926: Pseudo dice [np.float32(0.5017)]
2025-10-05 09:02:41.339231: Epoch time: 46.11 s
2025-10-05 09:02:41.339668: Yayy! New best EMA pseudo Dice: 0.4781999886035919
2025-10-05 09:02:42.382376: 
2025-10-05 09:02:42.382626: Epoch 23
2025-10-05 09:02:42.382812: Current learning rate: 0.00861
2025-10-05 09:03:28.518536: Validation loss did not improve from -0.15415. Patience: 17/50
2025-10-05 09:03:28.518926: train_loss -0.8476
2025-10-05 09:03:28.519076: val_loss 0.2038
2025-10-05 09:03:28.519214: Pseudo dice [np.float32(0.4594)]
2025-10-05 09:03:28.519368: Epoch time: 46.14 s
2025-10-05 09:03:29.158123: 
2025-10-05 09:03:29.158489: Epoch 24
2025-10-05 09:03:29.158695: Current learning rate: 0.00855
2025-10-05 09:04:15.343671: Validation loss did not improve from -0.15415. Patience: 18/50
2025-10-05 09:04:15.344209: train_loss -0.849
2025-10-05 09:04:15.344351: val_loss 0.0523
2025-10-05 09:04:15.344470: Pseudo dice [np.float32(0.5341)]
2025-10-05 09:04:15.344631: Epoch time: 46.19 s
2025-10-05 09:04:15.961867: Yayy! New best EMA pseudo Dice: 0.4821000099182129
2025-10-05 09:04:17.218284: 
2025-10-05 09:04:17.218637: Epoch 25
2025-10-05 09:04:17.218905: Current learning rate: 0.00849
2025-10-05 09:05:03.356748: Validation loss did not improve from -0.15415. Patience: 19/50
2025-10-05 09:05:03.357345: train_loss -0.8512
2025-10-05 09:05:03.357605: val_loss 0.2303
2025-10-05 09:05:03.357756: Pseudo dice [np.float32(0.4314)]
2025-10-05 09:05:03.357931: Epoch time: 46.14 s
2025-10-05 09:05:03.994730: 
2025-10-05 09:05:03.995027: Epoch 26
2025-10-05 09:05:03.995237: Current learning rate: 0.00843
2025-10-05 09:05:50.128836: Validation loss did not improve from -0.15415. Patience: 20/50
2025-10-05 09:05:50.129483: train_loss -0.8544
2025-10-05 09:05:50.129643: val_loss 0.1823
2025-10-05 09:05:50.129777: Pseudo dice [np.float32(0.4872)]
2025-10-05 09:05:50.129956: Epoch time: 46.14 s
2025-10-05 09:05:50.770870: 
2025-10-05 09:05:50.771208: Epoch 27
2025-10-05 09:05:50.771436: Current learning rate: 0.00836
2025-10-05 09:06:36.894352: Validation loss did not improve from -0.15415. Patience: 21/50
2025-10-05 09:06:36.894795: train_loss -0.8596
2025-10-05 09:06:36.894946: val_loss 0.1814
2025-10-05 09:06:36.895060: Pseudo dice [np.float32(0.4672)]
2025-10-05 09:06:36.895207: Epoch time: 46.12 s
2025-10-05 09:06:38.060701: 
2025-10-05 09:06:38.061020: Epoch 28
2025-10-05 09:06:38.061220: Current learning rate: 0.0083
2025-10-05 09:07:24.218998: Validation loss did not improve from -0.15415. Patience: 22/50
2025-10-05 09:07:24.220036: train_loss -0.8669
2025-10-05 09:07:24.220343: val_loss 0.1518
2025-10-05 09:07:24.220670: Pseudo dice [np.float32(0.5111)]
2025-10-05 09:07:24.220948: Epoch time: 46.16 s
2025-10-05 09:07:24.863811: 
2025-10-05 09:07:24.864286: Epoch 29
2025-10-05 09:07:24.864685: Current learning rate: 0.00824
2025-10-05 09:08:11.025058: Validation loss did not improve from -0.15415. Patience: 23/50
2025-10-05 09:08:11.025574: train_loss -0.8713
2025-10-05 09:08:11.025742: val_loss 0.2356
2025-10-05 09:08:11.025859: Pseudo dice [np.float32(0.4549)]
2025-10-05 09:08:11.025987: Epoch time: 46.16 s
2025-10-05 09:08:12.127735: 
2025-10-05 09:08:12.128066: Epoch 30
2025-10-05 09:08:12.128332: Current learning rate: 0.00818
2025-10-05 09:08:58.321031: Validation loss did not improve from -0.15415. Patience: 24/50
2025-10-05 09:08:58.322016: train_loss -0.8686
2025-10-05 09:08:58.322265: val_loss 0.1842
2025-10-05 09:08:58.322553: Pseudo dice [np.float32(0.4843)]
2025-10-05 09:08:58.322761: Epoch time: 46.19 s
2025-10-05 09:08:58.973586: 
2025-10-05 09:08:58.974026: Epoch 31
2025-10-05 09:08:58.974311: Current learning rate: 0.00812
2025-10-05 09:09:45.125372: Validation loss did not improve from -0.15415. Patience: 25/50
2025-10-05 09:09:45.126131: train_loss -0.8737
2025-10-05 09:09:45.126543: val_loss 0.1872
2025-10-05 09:09:45.126904: Pseudo dice [np.float32(0.4681)]
2025-10-05 09:09:45.127303: Epoch time: 46.15 s
2025-10-05 09:09:45.778000: 
2025-10-05 09:09:45.778327: Epoch 32
2025-10-05 09:09:45.778533: Current learning rate: 0.00806
2025-10-05 09:10:31.911438: Validation loss did not improve from -0.15415. Patience: 26/50
2025-10-05 09:10:31.912737: train_loss -0.8781
2025-10-05 09:10:31.913019: val_loss 0.301
2025-10-05 09:10:31.913385: Pseudo dice [np.float32(0.4473)]
2025-10-05 09:10:31.913527: Epoch time: 46.14 s
2025-10-05 09:10:32.561764: 
2025-10-05 09:10:32.562086: Epoch 33
2025-10-05 09:10:32.562340: Current learning rate: 0.008
2025-10-05 09:11:18.807928: Validation loss did not improve from -0.15415. Patience: 27/50
2025-10-05 09:11:18.808432: train_loss -0.8775
2025-10-05 09:11:18.808620: val_loss 0.2707
2025-10-05 09:11:18.808824: Pseudo dice [np.float32(0.4574)]
2025-10-05 09:11:18.809033: Epoch time: 46.25 s
2025-10-05 09:11:19.461041: 
2025-10-05 09:11:19.461295: Epoch 34
2025-10-05 09:11:19.461496: Current learning rate: 0.00793
2025-10-05 09:12:05.699833: Validation loss did not improve from -0.15415. Patience: 28/50
2025-10-05 09:12:05.700448: train_loss -0.8806
2025-10-05 09:12:05.700598: val_loss 0.2603
2025-10-05 09:12:05.700719: Pseudo dice [np.float32(0.4815)]
2025-10-05 09:12:05.700878: Epoch time: 46.24 s
2025-10-05 09:12:06.802530: 
2025-10-05 09:12:06.802959: Epoch 35
2025-10-05 09:12:06.803218: Current learning rate: 0.00787
2025-10-05 09:12:53.020205: Validation loss did not improve from -0.15415. Patience: 29/50
2025-10-05 09:12:53.020854: train_loss -0.8864
2025-10-05 09:12:53.021171: val_loss 0.2784
2025-10-05 09:12:53.021420: Pseudo dice [np.float32(0.4688)]
2025-10-05 09:12:53.021938: Epoch time: 46.22 s
2025-10-05 09:12:53.682500: 
2025-10-05 09:12:53.682823: Epoch 36
2025-10-05 09:12:53.683083: Current learning rate: 0.00781
2025-10-05 09:13:39.869585: Validation loss did not improve from -0.15415. Patience: 30/50
2025-10-05 09:13:39.870465: train_loss -0.8894
2025-10-05 09:13:39.870880: val_loss 0.3277
2025-10-05 09:13:39.871188: Pseudo dice [np.float32(0.4208)]
2025-10-05 09:13:39.871433: Epoch time: 46.19 s
2025-10-05 09:13:40.523409: 
2025-10-05 09:13:40.523685: Epoch 37
2025-10-05 09:13:40.523922: Current learning rate: 0.00775
2025-10-05 09:14:26.716470: Validation loss did not improve from -0.15415. Patience: 31/50
2025-10-05 09:14:26.717046: train_loss -0.89
2025-10-05 09:14:26.717405: val_loss 0.2417
2025-10-05 09:14:26.717730: Pseudo dice [np.float32(0.4533)]
2025-10-05 09:14:26.718095: Epoch time: 46.19 s
2025-10-05 09:14:27.369553: 
2025-10-05 09:14:27.369774: Epoch 38
2025-10-05 09:14:27.369962: Current learning rate: 0.00769
2025-10-05 09:15:13.577469: Validation loss did not improve from -0.15415. Patience: 32/50
2025-10-05 09:15:13.578036: train_loss -0.893
2025-10-05 09:15:13.578197: val_loss 0.1947
2025-10-05 09:15:13.578320: Pseudo dice [np.float32(0.483)]
2025-10-05 09:15:13.578448: Epoch time: 46.21 s
2025-10-05 09:15:14.225640: 
2025-10-05 09:15:14.225878: Epoch 39
2025-10-05 09:15:14.226059: Current learning rate: 0.00763
2025-10-05 09:16:00.413491: Validation loss did not improve from -0.15415. Patience: 33/50
2025-10-05 09:16:00.413939: train_loss -0.894
2025-10-05 09:16:00.414079: val_loss 0.3064
2025-10-05 09:16:00.414224: Pseudo dice [np.float32(0.486)]
2025-10-05 09:16:00.414356: Epoch time: 46.19 s
2025-10-05 09:16:01.522036: 
2025-10-05 09:16:01.522294: Epoch 40
2025-10-05 09:16:01.522684: Current learning rate: 0.00756
2025-10-05 09:16:47.699775: Validation loss did not improve from -0.15415. Patience: 34/50
2025-10-05 09:16:47.700463: train_loss -0.8992
2025-10-05 09:16:47.700621: val_loss 0.311
2025-10-05 09:16:47.700791: Pseudo dice [np.float32(0.4193)]
2025-10-05 09:16:47.701030: Epoch time: 46.18 s
2025-10-05 09:16:48.369179: 
2025-10-05 09:16:48.369451: Epoch 41
2025-10-05 09:16:48.369687: Current learning rate: 0.0075
2025-10-05 09:17:34.519280: Validation loss did not improve from -0.15415. Patience: 35/50
2025-10-05 09:17:34.519804: train_loss -0.8983
2025-10-05 09:17:34.519985: val_loss 0.3431
2025-10-05 09:17:34.520154: Pseudo dice [np.float32(0.4676)]
2025-10-05 09:17:34.520334: Epoch time: 46.15 s
2025-10-05 09:17:35.158433: 
2025-10-05 09:17:35.158866: Epoch 42
2025-10-05 09:17:35.159130: Current learning rate: 0.00744
2025-10-05 09:18:21.320290: Validation loss did not improve from -0.15415. Patience: 36/50
2025-10-05 09:18:21.320839: train_loss -0.8994
2025-10-05 09:18:21.321013: val_loss 0.3206
2025-10-05 09:18:21.321192: Pseudo dice [np.float32(0.4384)]
2025-10-05 09:18:21.321323: Epoch time: 46.16 s
2025-10-05 09:18:22.447741: 
2025-10-05 09:18:22.448110: Epoch 43
2025-10-05 09:18:22.448299: Current learning rate: 0.00738
2025-10-05 09:19:08.617592: Validation loss did not improve from -0.15415. Patience: 37/50
2025-10-05 09:19:08.618066: train_loss -0.8981
2025-10-05 09:19:08.618255: val_loss 0.3503
2025-10-05 09:19:08.618479: Pseudo dice [np.float32(0.4253)]
2025-10-05 09:19:08.618704: Epoch time: 46.17 s
2025-10-05 09:19:09.251187: 
2025-10-05 09:19:09.251562: Epoch 44
2025-10-05 09:19:09.251841: Current learning rate: 0.00732
2025-10-05 09:19:55.465686: Validation loss did not improve from -0.15415. Patience: 38/50
2025-10-05 09:19:55.466351: train_loss -0.9
2025-10-05 09:19:55.466541: val_loss 0.2713
2025-10-05 09:19:55.466687: Pseudo dice [np.float32(0.4759)]
2025-10-05 09:19:55.466863: Epoch time: 46.22 s
2025-10-05 09:19:56.568348: 
2025-10-05 09:19:56.568659: Epoch 45
2025-10-05 09:19:56.568841: Current learning rate: 0.00725
2025-10-05 09:20:42.851266: Validation loss did not improve from -0.15415. Patience: 39/50
2025-10-05 09:20:42.851704: train_loss -0.9044
2025-10-05 09:20:42.851851: val_loss 0.3717
2025-10-05 09:20:42.851962: Pseudo dice [np.float32(0.4272)]
2025-10-05 09:20:42.852214: Epoch time: 46.28 s
2025-10-05 09:20:43.489285: 
2025-10-05 09:20:43.489674: Epoch 46
2025-10-05 09:20:43.489925: Current learning rate: 0.00719
2025-10-05 09:21:29.771730: Validation loss did not improve from -0.15415. Patience: 40/50
2025-10-05 09:21:29.772621: train_loss -0.908
2025-10-05 09:21:29.772794: val_loss 0.3148
2025-10-05 09:21:29.772963: Pseudo dice [np.float32(0.4585)]
2025-10-05 09:21:29.773172: Epoch time: 46.28 s
2025-10-05 09:21:30.414452: 
2025-10-05 09:21:30.414725: Epoch 47
2025-10-05 09:21:30.414929: Current learning rate: 0.00713
2025-10-05 09:22:16.628798: Validation loss did not improve from -0.15415. Patience: 41/50
2025-10-05 09:22:16.629288: train_loss -0.9075
2025-10-05 09:22:16.629429: val_loss 0.3649
2025-10-05 09:22:16.629555: Pseudo dice [np.float32(0.4327)]
2025-10-05 09:22:16.629701: Epoch time: 46.22 s
2025-10-05 09:22:17.271731: 
2025-10-05 09:22:17.272002: Epoch 48
2025-10-05 09:22:17.272176: Current learning rate: 0.00707
2025-10-05 09:23:03.456675: Validation loss did not improve from -0.15415. Patience: 42/50
2025-10-05 09:23:03.457377: train_loss -0.904
2025-10-05 09:23:03.457514: val_loss 0.324
2025-10-05 09:23:03.457633: Pseudo dice [np.float32(0.4833)]
2025-10-05 09:23:03.457810: Epoch time: 46.19 s
2025-10-05 09:23:04.103710: 
2025-10-05 09:23:04.103944: Epoch 49
2025-10-05 09:23:04.104134: Current learning rate: 0.007
2025-10-05 09:23:50.331261: Validation loss did not improve from -0.15415. Patience: 43/50
2025-10-05 09:23:50.331798: train_loss -0.9091
2025-10-05 09:23:50.332029: val_loss 0.4004
2025-10-05 09:23:50.332267: Pseudo dice [np.float32(0.4504)]
2025-10-05 09:23:50.332556: Epoch time: 46.23 s
2025-10-05 09:23:51.587183: 
2025-10-05 09:23:51.587522: Epoch 50
2025-10-05 09:23:51.587762: Current learning rate: 0.00694
2025-10-05 09:24:37.828352: Validation loss did not improve from -0.15415. Patience: 44/50
2025-10-05 09:24:37.829237: train_loss -0.9132
2025-10-05 09:24:37.829487: val_loss 0.4334
2025-10-05 09:24:37.829723: Pseudo dice [np.float32(0.4347)]
2025-10-05 09:24:37.829958: Epoch time: 46.24 s
2025-10-05 09:24:38.481237: 
2025-10-05 09:24:38.481614: Epoch 51
2025-10-05 09:24:38.481901: Current learning rate: 0.00688
2025-10-05 09:25:24.762527: Validation loss did not improve from -0.15415. Patience: 45/50
2025-10-05 09:25:24.763035: train_loss -0.9123
2025-10-05 09:25:24.763201: val_loss 0.3836
2025-10-05 09:25:24.763319: Pseudo dice [np.float32(0.4503)]
2025-10-05 09:25:24.763475: Epoch time: 46.28 s
2025-10-05 09:25:25.408605: 
2025-10-05 09:25:25.408967: Epoch 52
2025-10-05 09:25:25.409177: Current learning rate: 0.00682
2025-10-05 09:26:11.584948: Validation loss did not improve from -0.15415. Patience: 46/50
2025-10-05 09:26:11.585720: train_loss -0.9149
2025-10-05 09:26:11.585914: val_loss 0.274
2025-10-05 09:26:11.586028: Pseudo dice [np.float32(0.4842)]
2025-10-05 09:26:11.586178: Epoch time: 46.18 s
2025-10-05 09:26:12.235260: 
2025-10-05 09:26:12.235766: Epoch 53
2025-10-05 09:26:12.236184: Current learning rate: 0.00675
2025-10-05 09:26:58.442063: Validation loss did not improve from -0.15415. Patience: 47/50
2025-10-05 09:26:58.442618: train_loss -0.9156
2025-10-05 09:26:58.442836: val_loss 0.4318
2025-10-05 09:26:58.443080: Pseudo dice [np.float32(0.4371)]
2025-10-05 09:26:58.443341: Epoch time: 46.21 s
2025-10-05 09:26:59.105092: 
2025-10-05 09:26:59.105391: Epoch 54
2025-10-05 09:26:59.105647: Current learning rate: 0.00669
2025-10-05 09:27:45.345796: Validation loss did not improve from -0.15415. Patience: 48/50
2025-10-05 09:27:45.346524: train_loss -0.9196
2025-10-05 09:27:45.346722: val_loss 0.2928
2025-10-05 09:27:45.346866: Pseudo dice [np.float32(0.4737)]
2025-10-05 09:27:45.347024: Epoch time: 46.24 s
2025-10-05 09:27:46.426761: 
2025-10-05 09:27:46.427133: Epoch 55
2025-10-05 09:27:46.427389: Current learning rate: 0.00663
2025-10-05 09:28:32.647775: Validation loss did not improve from -0.15415. Patience: 49/50
2025-10-05 09:28:32.648368: train_loss -0.9167
2025-10-05 09:28:32.648909: val_loss 0.3095
2025-10-05 09:28:32.649108: Pseudo dice [np.float32(0.4658)]
2025-10-05 09:28:32.649282: Epoch time: 46.22 s
2025-10-05 09:28:33.297349: 
2025-10-05 09:28:33.297676: Epoch 56
2025-10-05 09:28:33.298061: Current learning rate: 0.00657
2025-10-05 09:29:19.476589: Validation loss did not improve from -0.15415. Patience: 50/50
2025-10-05 09:29:19.477546: train_loss -0.9181
2025-10-05 09:29:19.477787: val_loss 0.3539
2025-10-05 09:29:19.477928: Pseudo dice [np.float32(0.4496)]
2025-10-05 09:29:19.478078: Epoch time: 46.18 s
2025-10-05 09:29:20.126150: 
2025-10-05 09:29:20.126553: Epoch 57
2025-10-05 09:29:20.126973: Current learning rate: 0.0065
2025-10-05 09:30:06.323002: Validation loss did not improve from -0.15415. Patience: 51/50
2025-10-05 09:30:06.323502: train_loss -0.9191
2025-10-05 09:30:06.323712: val_loss 0.5494
2025-10-05 09:30:06.323830: Pseudo dice [np.float32(0.3828)]
2025-10-05 09:30:06.324018: Epoch time: 46.2 s
2025-10-05 09:30:06.976386: 
2025-10-05 09:30:06.976749: Epoch 58
2025-10-05 09:30:06.976980: Current learning rate: 0.00644
2025-10-05 09:30:53.203439: Validation loss did not improve from -0.15415. Patience: 52/50
2025-10-05 09:30:53.204156: train_loss -0.9197
2025-10-05 09:30:53.204326: val_loss 0.4847
2025-10-05 09:30:53.204459: Pseudo dice [np.float32(0.4101)]
2025-10-05 09:30:53.204622: Epoch time: 46.23 s
2025-10-05 09:30:54.432211: 
2025-10-05 09:30:54.432562: Epoch 59
2025-10-05 09:30:54.432745: Current learning rate: 0.00638
2025-10-05 09:31:40.622752: Validation loss did not improve from -0.15415. Patience: 53/50
2025-10-05 09:31:40.623280: train_loss -0.9212
2025-10-05 09:31:40.623439: val_loss 0.4577
2025-10-05 09:31:40.623574: Pseudo dice [np.float32(0.4205)]
2025-10-05 09:31:40.623744: Epoch time: 46.19 s
2025-10-05 09:31:41.729184: 
2025-10-05 09:31:41.729519: Epoch 60
2025-10-05 09:31:41.729723: Current learning rate: 0.00631
2025-10-05 09:32:27.942384: Validation loss did not improve from -0.15415. Patience: 54/50
2025-10-05 09:32:27.943190: train_loss -0.9205
2025-10-05 09:32:27.943367: val_loss 0.3764
2025-10-05 09:32:27.943505: Pseudo dice [np.float32(0.443)]
2025-10-05 09:32:27.943636: Epoch time: 46.21 s
2025-10-05 09:32:28.600938: 
2025-10-05 09:32:28.601305: Epoch 61
2025-10-05 09:32:28.601548: Current learning rate: 0.00625
2025-10-05 09:33:14.865521: Validation loss did not improve from -0.15415. Patience: 55/50
2025-10-05 09:33:14.866313: train_loss -0.9239
2025-10-05 09:33:14.866814: val_loss 0.4106
2025-10-05 09:33:14.867274: Pseudo dice [np.float32(0.4584)]
2025-10-05 09:33:14.867721: Epoch time: 46.27 s
2025-10-05 09:33:15.527561: 
2025-10-05 09:33:15.528053: Epoch 62
2025-10-05 09:33:15.528268: Current learning rate: 0.00619
2025-10-05 09:34:01.789920: Validation loss did not improve from -0.15415. Patience: 56/50
2025-10-05 09:34:01.790608: train_loss -0.9236
2025-10-05 09:34:01.790784: val_loss 0.4403
2025-10-05 09:34:01.790938: Pseudo dice [np.float32(0.4231)]
2025-10-05 09:34:01.791087: Epoch time: 46.26 s
2025-10-05 09:34:02.461970: 
2025-10-05 09:34:02.462443: Epoch 63
2025-10-05 09:34:02.462683: Current learning rate: 0.00612
2025-10-05 09:34:48.713644: Validation loss did not improve from -0.15415. Patience: 57/50
2025-10-05 09:34:48.714175: train_loss -0.924
2025-10-05 09:34:48.714327: val_loss 0.398
2025-10-05 09:34:48.714461: Pseudo dice [np.float32(0.4549)]
2025-10-05 09:34:48.714660: Epoch time: 46.25 s
2025-10-05 09:34:49.381075: 
2025-10-05 09:34:49.381415: Epoch 64
2025-10-05 09:34:49.381618: Current learning rate: 0.00606
2025-10-05 09:35:35.605882: Validation loss did not improve from -0.15415. Patience: 58/50
2025-10-05 09:35:35.606640: train_loss -0.9243
2025-10-05 09:35:35.606878: val_loss 0.4505
2025-10-05 09:35:35.607090: Pseudo dice [np.float32(0.4361)]
2025-10-05 09:35:35.607324: Epoch time: 46.23 s
2025-10-05 09:35:36.701349: 
2025-10-05 09:35:36.701781: Epoch 65
2025-10-05 09:35:36.702094: Current learning rate: 0.006
2025-10-05 09:36:22.929121: Validation loss did not improve from -0.15415. Patience: 59/50
2025-10-05 09:36:22.929639: train_loss -0.9265
2025-10-05 09:36:22.929810: val_loss 0.4225
2025-10-05 09:36:22.929944: Pseudo dice [np.float32(0.44)]
2025-10-05 09:36:22.930112: Epoch time: 46.23 s
2025-10-05 09:36:23.598599: 
2025-10-05 09:36:23.598979: Epoch 66
2025-10-05 09:36:23.599198: Current learning rate: 0.00593
2025-10-05 09:37:09.824749: Validation loss did not improve from -0.15415. Patience: 60/50
2025-10-05 09:37:09.825618: train_loss -0.9256
2025-10-05 09:37:09.825834: val_loss 0.368
2025-10-05 09:37:09.825986: Pseudo dice [np.float32(0.4481)]
2025-10-05 09:37:09.826182: Epoch time: 46.23 s
2025-10-05 09:37:10.502771: 
2025-10-05 09:37:10.503127: Epoch 67
2025-10-05 09:37:10.503406: Current learning rate: 0.00587
2025-10-05 09:37:56.756397: Validation loss did not improve from -0.15415. Patience: 61/50
2025-10-05 09:37:56.756908: train_loss -0.9282
2025-10-05 09:37:56.757050: val_loss 0.3171
2025-10-05 09:37:56.757254: Pseudo dice [np.float32(0.4632)]
2025-10-05 09:37:56.757471: Epoch time: 46.25 s
2025-10-05 09:37:57.428185: 
2025-10-05 09:37:57.428512: Epoch 68
2025-10-05 09:37:57.428760: Current learning rate: 0.00581
2025-10-05 09:38:43.645608: Validation loss did not improve from -0.15415. Patience: 62/50
2025-10-05 09:38:43.646277: train_loss -0.9292
2025-10-05 09:38:43.646422: val_loss 0.3719
2025-10-05 09:38:43.646546: Pseudo dice [np.float32(0.4574)]
2025-10-05 09:38:43.646751: Epoch time: 46.22 s
2025-10-05 09:38:44.304786: 
2025-10-05 09:38:44.305140: Epoch 69
2025-10-05 09:38:44.305322: Current learning rate: 0.00574
2025-10-05 09:39:30.541854: Validation loss did not improve from -0.15415. Patience: 63/50
2025-10-05 09:39:30.542500: train_loss -0.9285
2025-10-05 09:39:30.542708: val_loss 0.4189
2025-10-05 09:39:30.542821: Pseudo dice [np.float32(0.4391)]
2025-10-05 09:39:30.543075: Epoch time: 46.24 s
2025-10-05 09:39:31.642936: 
2025-10-05 09:39:31.643325: Epoch 70
2025-10-05 09:39:31.643602: Current learning rate: 0.00568
2025-10-05 09:40:17.833858: Validation loss did not improve from -0.15415. Patience: 64/50
2025-10-05 09:40:17.834574: train_loss -0.9281
2025-10-05 09:40:17.834718: val_loss 0.4484
2025-10-05 09:40:17.834891: Pseudo dice [np.float32(0.4429)]
2025-10-05 09:40:17.835043: Epoch time: 46.19 s
2025-10-05 09:40:18.492077: 
2025-10-05 09:40:18.492389: Epoch 71
2025-10-05 09:40:18.492616: Current learning rate: 0.00562
2025-10-05 09:41:04.671080: Validation loss did not improve from -0.15415. Patience: 65/50
2025-10-05 09:41:04.671582: train_loss -0.9306
2025-10-05 09:41:04.671737: val_loss 0.4185
2025-10-05 09:41:04.671851: Pseudo dice [np.float32(0.4526)]
2025-10-05 09:41:04.672019: Epoch time: 46.18 s
2025-10-05 09:41:05.330219: 
2025-10-05 09:41:05.330539: Epoch 72
2025-10-05 09:41:05.330743: Current learning rate: 0.00555
2025-10-05 09:41:51.533941: Validation loss did not improve from -0.15415. Patience: 66/50
2025-10-05 09:41:51.534803: train_loss -0.9309
2025-10-05 09:41:51.535141: val_loss 0.3611
2025-10-05 09:41:51.535447: Pseudo dice [np.float32(0.4468)]
2025-10-05 09:41:51.535699: Epoch time: 46.21 s
2025-10-05 09:41:52.208744: 
2025-10-05 09:41:52.209168: Epoch 73
2025-10-05 09:41:52.209378: Current learning rate: 0.00549
2025-10-05 09:42:38.407913: Validation loss did not improve from -0.15415. Patience: 67/50
2025-10-05 09:42:38.408434: train_loss -0.931
2025-10-05 09:42:38.408619: val_loss 0.5246
2025-10-05 09:42:38.408788: Pseudo dice [np.float32(0.4319)]
2025-10-05 09:42:38.408958: Epoch time: 46.2 s
2025-10-05 09:42:39.595827: 
2025-10-05 09:42:39.596203: Epoch 74
2025-10-05 09:42:39.596410: Current learning rate: 0.00542
2025-10-05 09:43:25.774557: Validation loss did not improve from -0.15415. Patience: 68/50
2025-10-05 09:43:25.775434: train_loss -0.9318
2025-10-05 09:43:25.775615: val_loss 0.4109
2025-10-05 09:43:25.775752: Pseudo dice [np.float32(0.4473)]
2025-10-05 09:43:25.775899: Epoch time: 46.18 s
2025-10-05 09:43:26.904980: 
2025-10-05 09:43:26.905367: Epoch 75
2025-10-05 09:43:26.905580: Current learning rate: 0.00536
2025-10-05 09:44:13.093054: Validation loss did not improve from -0.15415. Patience: 69/50
2025-10-05 09:44:13.093938: train_loss -0.9319
2025-10-05 09:44:13.094121: val_loss 0.3654
2025-10-05 09:44:13.094279: Pseudo dice [np.float32(0.4314)]
2025-10-05 09:44:13.094524: Epoch time: 46.19 s
2025-10-05 09:44:13.757548: 
2025-10-05 09:44:13.757893: Epoch 76
2025-10-05 09:44:13.758111: Current learning rate: 0.00529
2025-10-05 09:44:59.929882: Validation loss did not improve from -0.15415. Patience: 70/50
2025-10-05 09:44:59.930682: train_loss -0.9317
2025-10-05 09:44:59.930817: val_loss 0.51
2025-10-05 09:44:59.930963: Pseudo dice [np.float32(0.4216)]
2025-10-05 09:44:59.931148: Epoch time: 46.17 s
2025-10-05 09:45:00.585898: 
2025-10-05 09:45:00.586135: Epoch 77
2025-10-05 09:45:00.586389: Current learning rate: 0.00523
2025-10-05 09:45:46.833667: Validation loss did not improve from -0.15415. Patience: 71/50
2025-10-05 09:45:46.834074: train_loss -0.9319
2025-10-05 09:45:46.834216: val_loss 0.5215
2025-10-05 09:45:46.834336: Pseudo dice [np.float32(0.4004)]
2025-10-05 09:45:46.834491: Epoch time: 46.25 s
2025-10-05 09:45:47.510201: 
2025-10-05 09:45:47.510445: Epoch 78
2025-10-05 09:45:47.510661: Current learning rate: 0.00517
2025-10-05 09:46:33.832539: Validation loss did not improve from -0.15415. Patience: 72/50
2025-10-05 09:46:33.833196: train_loss -0.9327
2025-10-05 09:46:33.833368: val_loss 0.5269
2025-10-05 09:46:33.833505: Pseudo dice [np.float32(0.3983)]
2025-10-05 09:46:33.833633: Epoch time: 46.32 s
2025-10-05 09:46:34.510285: 
2025-10-05 09:46:34.510548: Epoch 79
2025-10-05 09:46:34.510763: Current learning rate: 0.0051
2025-10-05 09:47:20.710335: Validation loss did not improve from -0.15415. Patience: 73/50
2025-10-05 09:47:20.710794: train_loss -0.9357
2025-10-05 09:47:20.711034: val_loss 0.4101
2025-10-05 09:47:20.711365: Pseudo dice [np.float32(0.4475)]
2025-10-05 09:47:20.711606: Epoch time: 46.2 s
2025-10-05 09:47:21.819489: 
2025-10-05 09:47:21.819783: Epoch 80
2025-10-05 09:47:21.819992: Current learning rate: 0.00504
2025-10-05 09:48:08.025303: Validation loss did not improve from -0.15415. Patience: 74/50
2025-10-05 09:48:08.026119: train_loss -0.9351
2025-10-05 09:48:08.026371: val_loss 0.5463
2025-10-05 09:48:08.026555: Pseudo dice [np.float32(0.4164)]
2025-10-05 09:48:08.026768: Epoch time: 46.21 s
2025-10-05 09:48:08.707955: 
2025-10-05 09:48:08.708358: Epoch 81
2025-10-05 09:48:08.708583: Current learning rate: 0.00497
2025-10-05 09:48:54.940496: Validation loss did not improve from -0.15415. Patience: 75/50
2025-10-05 09:48:54.940990: train_loss -0.9347
2025-10-05 09:48:54.941200: val_loss 0.5298
2025-10-05 09:48:54.941378: Pseudo dice [np.float32(0.412)]
2025-10-05 09:48:54.941554: Epoch time: 46.23 s
2025-10-05 09:48:55.608914: 
2025-10-05 09:48:55.609190: Epoch 82
2025-10-05 09:48:55.609405: Current learning rate: 0.00491
2025-10-05 09:49:41.873406: Validation loss did not improve from -0.15415. Patience: 76/50
2025-10-05 09:49:41.874279: train_loss -0.9369
2025-10-05 09:49:41.874641: val_loss 0.4469
2025-10-05 09:49:41.874771: Pseudo dice [np.float32(0.4511)]
2025-10-05 09:49:41.874986: Epoch time: 46.27 s
2025-10-05 09:49:42.529354: 
2025-10-05 09:49:42.530004: Epoch 83
2025-10-05 09:49:42.530242: Current learning rate: 0.00484
2025-10-05 09:50:28.783853: Validation loss did not improve from -0.15415. Patience: 77/50
2025-10-05 09:50:28.784616: train_loss -0.9354
2025-10-05 09:50:28.785111: val_loss 0.4071
2025-10-05 09:50:28.785501: Pseudo dice [np.float32(0.4791)]
2025-10-05 09:50:28.785751: Epoch time: 46.26 s
2025-10-05 09:50:29.445990: 
2025-10-05 09:50:29.446313: Epoch 84
2025-10-05 09:50:29.446543: Current learning rate: 0.00478
2025-10-05 09:51:15.684655: Validation loss did not improve from -0.15415. Patience: 78/50
2025-10-05 09:51:15.686051: train_loss -0.9368
2025-10-05 09:51:15.686511: val_loss 0.4742
2025-10-05 09:51:15.686861: Pseudo dice [np.float32(0.4419)]
2025-10-05 09:51:15.687249: Epoch time: 46.24 s
2025-10-05 09:51:16.794681: 
2025-10-05 09:51:16.795151: Epoch 85
2025-10-05 09:51:16.795610: Current learning rate: 0.00471
2025-10-05 09:52:03.006318: Validation loss did not improve from -0.15415. Patience: 79/50
2025-10-05 09:52:03.006908: train_loss -0.9363
2025-10-05 09:52:03.007248: val_loss 0.4825
2025-10-05 09:52:03.007455: Pseudo dice [np.float32(0.399)]
2025-10-05 09:52:03.007751: Epoch time: 46.21 s
2025-10-05 09:52:03.658831: 
2025-10-05 09:52:03.659200: Epoch 86
2025-10-05 09:52:03.659431: Current learning rate: 0.00465
2025-10-05 09:52:49.799546: Validation loss did not improve from -0.15415. Patience: 80/50
2025-10-05 09:52:49.800742: train_loss -0.9369
2025-10-05 09:52:49.801132: val_loss 0.4975
2025-10-05 09:52:49.801376: Pseudo dice [np.float32(0.4307)]
2025-10-05 09:52:49.801563: Epoch time: 46.14 s
2025-10-05 09:52:50.454116: 
2025-10-05 09:52:50.454369: Epoch 87
2025-10-05 09:52:50.454550: Current learning rate: 0.00458
2025-10-05 09:53:36.624682: Validation loss did not improve from -0.15415. Patience: 81/50
2025-10-05 09:53:36.625172: train_loss -0.9383
2025-10-05 09:53:36.625324: val_loss 0.4534
2025-10-05 09:53:36.625488: Pseudo dice [np.float32(0.4218)]
2025-10-05 09:53:36.625673: Epoch time: 46.17 s
2025-10-05 09:53:37.278448: 
2025-10-05 09:53:37.278821: Epoch 88
2025-10-05 09:53:37.279020: Current learning rate: 0.00452
2025-10-05 09:54:23.480638: Validation loss did not improve from -0.15415. Patience: 82/50
2025-10-05 09:54:23.481352: train_loss -0.9387
2025-10-05 09:54:23.481535: val_loss 0.404
2025-10-05 09:54:23.481688: Pseudo dice [np.float32(0.4721)]
2025-10-05 09:54:23.481824: Epoch time: 46.2 s
2025-10-05 09:54:24.687398: 
2025-10-05 09:54:24.687835: Epoch 89
2025-10-05 09:54:24.688220: Current learning rate: 0.00445
2025-10-05 09:55:10.831960: Validation loss did not improve from -0.15415. Patience: 83/50
2025-10-05 09:55:10.832496: train_loss -0.9382
2025-10-05 09:55:10.832799: val_loss 0.414
2025-10-05 09:55:10.833059: Pseudo dice [np.float32(0.4358)]
2025-10-05 09:55:10.833378: Epoch time: 46.15 s
2025-10-05 09:55:11.961539: 
2025-10-05 09:55:11.961874: Epoch 90
2025-10-05 09:55:11.962108: Current learning rate: 0.00438
2025-10-05 09:55:58.089268: Validation loss did not improve from -0.15415. Patience: 84/50
2025-10-05 09:55:58.090405: train_loss -0.9374
2025-10-05 09:55:58.090791: val_loss 0.4739
2025-10-05 09:55:58.091155: Pseudo dice [np.float32(0.4407)]
2025-10-05 09:55:58.091663: Epoch time: 46.13 s
2025-10-05 09:55:58.751868: 
2025-10-05 09:55:58.752188: Epoch 91
2025-10-05 09:55:58.752439: Current learning rate: 0.00432
2025-10-05 09:56:44.861820: Validation loss did not improve from -0.15415. Patience: 85/50
2025-10-05 09:56:44.862531: train_loss -0.939
2025-10-05 09:56:44.862906: val_loss 0.5309
2025-10-05 09:56:44.863124: Pseudo dice [np.float32(0.4135)]
2025-10-05 09:56:44.863346: Epoch time: 46.11 s
2025-10-05 09:56:45.520621: 
2025-10-05 09:56:45.520985: Epoch 92
2025-10-05 09:56:45.521216: Current learning rate: 0.00425
2025-10-05 09:57:31.701043: Validation loss did not improve from -0.15415. Patience: 86/50
2025-10-05 09:57:31.701934: train_loss -0.9385
2025-10-05 09:57:31.702282: val_loss 0.4015
2025-10-05 09:57:31.702538: Pseudo dice [np.float32(0.4829)]
2025-10-05 09:57:31.702839: Epoch time: 46.18 s
2025-10-05 09:57:32.372455: 
2025-10-05 09:57:32.372770: Epoch 93
2025-10-05 09:57:32.372991: Current learning rate: 0.00419
2025-10-05 09:58:18.579569: Validation loss did not improve from -0.15415. Patience: 87/50
2025-10-05 09:58:18.580087: train_loss -0.9389
2025-10-05 09:58:18.580340: val_loss 0.4572
2025-10-05 09:58:18.580504: Pseudo dice [np.float32(0.4396)]
2025-10-05 09:58:18.580686: Epoch time: 46.21 s
2025-10-05 09:58:19.243209: 
2025-10-05 09:58:19.243454: Epoch 94
2025-10-05 09:58:19.243666: Current learning rate: 0.00412
2025-10-05 09:59:05.418476: Validation loss did not improve from -0.15415. Patience: 88/50
2025-10-05 09:59:05.419195: train_loss -0.9401
2025-10-05 09:59:05.419482: val_loss 0.4761
2025-10-05 09:59:05.419763: Pseudo dice [np.float32(0.442)]
2025-10-05 09:59:05.420005: Epoch time: 46.18 s
2025-10-05 09:59:06.525794: 
2025-10-05 09:59:06.526070: Epoch 95
2025-10-05 09:59:06.526522: Current learning rate: 0.00405
2025-10-05 09:59:52.681722: Validation loss did not improve from -0.15415. Patience: 89/50
2025-10-05 09:59:52.682191: train_loss -0.9395
2025-10-05 09:59:52.682443: val_loss 0.4518
2025-10-05 09:59:52.682642: Pseudo dice [np.float32(0.439)]
2025-10-05 09:59:52.682767: Epoch time: 46.16 s
2025-10-05 09:59:53.352483: 
2025-10-05 09:59:53.352900: Epoch 96
2025-10-05 09:59:53.353155: Current learning rate: 0.00399
2025-10-05 10:00:39.452500: Validation loss did not improve from -0.15415. Patience: 90/50
2025-10-05 10:00:39.454025: train_loss -0.9401
2025-10-05 10:00:39.454508: val_loss 0.5693
2025-10-05 10:00:39.454916: Pseudo dice [np.float32(0.4374)]
2025-10-05 10:00:39.455389: Epoch time: 46.1 s
2025-10-05 10:00:40.117567: 
2025-10-05 10:00:40.117977: Epoch 97
2025-10-05 10:00:40.118219: Current learning rate: 0.00392
2025-10-05 10:01:26.269564: Validation loss did not improve from -0.15415. Patience: 91/50
2025-10-05 10:01:26.270029: train_loss -0.9415
2025-10-05 10:01:26.270254: val_loss 0.4701
2025-10-05 10:01:26.270448: Pseudo dice [np.float32(0.4599)]
2025-10-05 10:01:26.270652: Epoch time: 46.15 s
2025-10-05 10:01:26.931316: 
2025-10-05 10:01:26.931586: Epoch 98
2025-10-05 10:01:26.931818: Current learning rate: 0.00385
2025-10-05 10:02:13.168508: Validation loss did not improve from -0.15415. Patience: 92/50
2025-10-05 10:02:13.169242: train_loss -0.9416
2025-10-05 10:02:13.169424: val_loss 0.4937
2025-10-05 10:02:13.169569: Pseudo dice [np.float32(0.4466)]
2025-10-05 10:02:13.169734: Epoch time: 46.24 s
2025-10-05 10:02:13.827437: 
2025-10-05 10:02:13.827737: Epoch 99
2025-10-05 10:02:13.827918: Current learning rate: 0.00379
2025-10-05 10:02:59.996478: Validation loss did not improve from -0.15415. Patience: 93/50
2025-10-05 10:02:59.996967: train_loss -0.9427
2025-10-05 10:02:59.997128: val_loss 0.5067
2025-10-05 10:02:59.997336: Pseudo dice [np.float32(0.423)]
2025-10-05 10:02:59.997484: Epoch time: 46.17 s
2025-10-05 10:03:01.113623: 
2025-10-05 10:03:01.113991: Epoch 100
2025-10-05 10:03:01.114275: Current learning rate: 0.00372
2025-10-05 10:03:47.237108: Validation loss did not improve from -0.15415. Patience: 94/50
2025-10-05 10:03:47.237814: train_loss -0.9422
2025-10-05 10:03:47.237988: val_loss 0.5039
2025-10-05 10:03:47.238185: Pseudo dice [np.float32(0.4426)]
2025-10-05 10:03:47.238376: Epoch time: 46.12 s
2025-10-05 10:03:47.894666: 
2025-10-05 10:03:47.895116: Epoch 101
2025-10-05 10:03:47.895420: Current learning rate: 0.00365
2025-10-05 10:04:33.977406: Validation loss did not improve from -0.15415. Patience: 95/50
2025-10-05 10:04:33.977876: train_loss -0.9437
2025-10-05 10:04:33.978058: val_loss 0.5472
2025-10-05 10:04:33.978197: Pseudo dice [np.float32(0.4242)]
2025-10-05 10:04:33.978365: Epoch time: 46.08 s
2025-10-05 10:04:34.628803: 
2025-10-05 10:04:34.629121: Epoch 102
2025-10-05 10:04:34.629302: Current learning rate: 0.00359
2025-10-05 10:05:20.809069: Validation loss did not improve from -0.15415. Patience: 96/50
2025-10-05 10:05:20.809847: train_loss -0.943
2025-10-05 10:05:20.810189: val_loss 0.4669
2025-10-05 10:05:20.810431: Pseudo dice [np.float32(0.4438)]
2025-10-05 10:05:20.810634: Epoch time: 46.18 s
2025-10-05 10:05:21.489298: 
2025-10-05 10:05:21.489616: Epoch 103
2025-10-05 10:05:21.489804: Current learning rate: 0.00352
2025-10-05 10:06:07.670694: Validation loss did not improve from -0.15415. Patience: 97/50
2025-10-05 10:06:07.671359: train_loss -0.9437
2025-10-05 10:06:07.671782: val_loss 0.5345
2025-10-05 10:06:07.672204: Pseudo dice [np.float32(0.4565)]
2025-10-05 10:06:07.672618: Epoch time: 46.18 s
2025-10-05 10:06:08.839512: 
2025-10-05 10:06:08.839941: Epoch 104
2025-10-05 10:06:08.840374: Current learning rate: 0.00345
2025-10-05 10:06:54.957016: Validation loss did not improve from -0.15415. Patience: 98/50
2025-10-05 10:06:54.957759: train_loss -0.9438
2025-10-05 10:06:54.957953: val_loss 0.5264
2025-10-05 10:06:54.958068: Pseudo dice [np.float32(0.4181)]
2025-10-05 10:06:54.958226: Epoch time: 46.12 s
2025-10-05 10:06:56.068459: 
2025-10-05 10:06:56.068771: Epoch 105
2025-10-05 10:06:56.068965: Current learning rate: 0.00338
2025-10-05 10:07:42.119594: Validation loss did not improve from -0.15415. Patience: 99/50
2025-10-05 10:07:42.120270: train_loss -0.9428
2025-10-05 10:07:42.120461: val_loss 0.5821
2025-10-05 10:07:42.120594: Pseudo dice [np.float32(0.4022)]
2025-10-05 10:07:42.120915: Epoch time: 46.05 s
2025-10-05 10:07:42.780980: 
2025-10-05 10:07:42.781685: Epoch 106
2025-10-05 10:07:42.782124: Current learning rate: 0.00332
2025-10-05 10:08:28.908492: Validation loss did not improve from -0.15415. Patience: 100/50
2025-10-05 10:08:28.909530: train_loss -0.945
2025-10-05 10:08:28.909866: val_loss 0.4645
2025-10-05 10:08:28.910064: Pseudo dice [np.float32(0.4392)]
2025-10-05 10:08:28.910496: Epoch time: 46.13 s
2025-10-05 10:08:29.571303: 
2025-10-05 10:08:29.571781: Epoch 107
2025-10-05 10:08:29.572085: Current learning rate: 0.00325
2025-10-05 10:09:15.742675: Validation loss did not improve from -0.15415. Patience: 101/50
2025-10-05 10:09:15.743245: train_loss -0.9449
2025-10-05 10:09:15.743391: val_loss 0.4726
2025-10-05 10:09:15.743515: Pseudo dice [np.float32(0.46)]
2025-10-05 10:09:15.743651: Epoch time: 46.17 s
2025-10-05 10:09:16.409208: 
2025-10-05 10:09:16.409685: Epoch 108
2025-10-05 10:09:16.410024: Current learning rate: 0.00318
2025-10-05 10:10:02.613001: Validation loss did not improve from -0.15415. Patience: 102/50
2025-10-05 10:10:02.613792: train_loss -0.9464
2025-10-05 10:10:02.614031: val_loss 0.5624
2025-10-05 10:10:02.614192: Pseudo dice [np.float32(0.4161)]
2025-10-05 10:10:02.614440: Epoch time: 46.21 s
2025-10-05 10:10:03.294479: 
2025-10-05 10:10:03.294938: Epoch 109
2025-10-05 10:10:03.295254: Current learning rate: 0.00311
2025-10-05 10:10:49.428000: Validation loss did not improve from -0.15415. Patience: 103/50
2025-10-05 10:10:49.428450: train_loss -0.9457
2025-10-05 10:10:49.428669: val_loss 0.5592
2025-10-05 10:10:49.428885: Pseudo dice [np.float32(0.4015)]
2025-10-05 10:10:49.429074: Epoch time: 46.13 s
2025-10-05 10:10:50.604940: 
2025-10-05 10:10:50.605395: Epoch 110
2025-10-05 10:10:50.605763: Current learning rate: 0.00304
2025-10-05 10:11:36.673356: Validation loss did not improve from -0.15415. Patience: 104/50
2025-10-05 10:11:36.673910: train_loss -0.9454
2025-10-05 10:11:36.674048: val_loss 0.5574
2025-10-05 10:11:36.674206: Pseudo dice [np.float32(0.4206)]
2025-10-05 10:11:36.674421: Epoch time: 46.07 s
2025-10-05 10:11:37.335150: 
2025-10-05 10:11:37.335789: Epoch 111
2025-10-05 10:11:37.336159: Current learning rate: 0.00297
2025-10-05 10:12:23.433269: Validation loss did not improve from -0.15415. Patience: 105/50
2025-10-05 10:12:23.433832: train_loss -0.9451
2025-10-05 10:12:23.433988: val_loss 0.5241
2025-10-05 10:12:23.434178: Pseudo dice [np.float32(0.4224)]
2025-10-05 10:12:23.434325: Epoch time: 46.1 s
2025-10-05 10:12:24.100738: 
2025-10-05 10:12:24.101110: Epoch 112
2025-10-05 10:12:24.101293: Current learning rate: 0.00291
2025-10-05 10:13:10.297658: Validation loss did not improve from -0.15415. Patience: 106/50
2025-10-05 10:13:10.298572: train_loss -0.9458
2025-10-05 10:13:10.299002: val_loss 0.4798
2025-10-05 10:13:10.299372: Pseudo dice [np.float32(0.4541)]
2025-10-05 10:13:10.299943: Epoch time: 46.2 s
2025-10-05 10:13:10.955781: 
2025-10-05 10:13:10.956269: Epoch 113
2025-10-05 10:13:10.956758: Current learning rate: 0.00284
2025-10-05 10:13:57.115984: Validation loss did not improve from -0.15415. Patience: 107/50
2025-10-05 10:13:57.116641: train_loss -0.9455
2025-10-05 10:13:57.116967: val_loss 0.5209
2025-10-05 10:13:57.117238: Pseudo dice [np.float32(0.4183)]
2025-10-05 10:13:57.117430: Epoch time: 46.16 s
2025-10-05 10:13:57.787243: 
2025-10-05 10:13:57.787491: Epoch 114
2025-10-05 10:13:57.787656: Current learning rate: 0.00277
2025-10-05 10:14:43.955790: Validation loss did not improve from -0.15415. Patience: 108/50
2025-10-05 10:14:43.956647: train_loss -0.9461
2025-10-05 10:14:43.956790: val_loss 0.4687
2025-10-05 10:14:43.956926: Pseudo dice [np.float32(0.4319)]
2025-10-05 10:14:43.957244: Epoch time: 46.17 s
2025-10-05 10:14:45.052470: 
2025-10-05 10:14:45.052992: Epoch 115
2025-10-05 10:14:45.053383: Current learning rate: 0.0027
2025-10-05 10:15:31.139462: Validation loss did not improve from -0.15415. Patience: 109/50
2025-10-05 10:15:31.139956: train_loss -0.946
2025-10-05 10:15:31.140139: val_loss 0.4642
2025-10-05 10:15:31.140329: Pseudo dice [np.float32(0.4502)]
2025-10-05 10:15:31.140465: Epoch time: 46.09 s
2025-10-05 10:15:31.809729: 
2025-10-05 10:15:31.810095: Epoch 116
2025-10-05 10:15:31.810297: Current learning rate: 0.00263
2025-10-05 10:16:17.966989: Validation loss did not improve from -0.15415. Patience: 110/50
2025-10-05 10:16:17.967546: train_loss -0.9457
2025-10-05 10:16:17.967753: val_loss 0.518
2025-10-05 10:16:17.967863: Pseudo dice [np.float32(0.4254)]
2025-10-05 10:16:17.968005: Epoch time: 46.16 s
2025-10-05 10:16:18.642781: 
2025-10-05 10:16:18.643091: Epoch 117
2025-10-05 10:16:18.643268: Current learning rate: 0.00256
2025-10-05 10:17:04.813517: Validation loss did not improve from -0.15415. Patience: 111/50
2025-10-05 10:17:04.814515: train_loss -0.9473
2025-10-05 10:17:04.815316: val_loss 0.6762
2025-10-05 10:17:04.815951: Pseudo dice [np.float32(0.3976)]
2025-10-05 10:17:04.816510: Epoch time: 46.17 s
2025-10-05 10:17:05.503409: 
2025-10-05 10:17:05.503934: Epoch 118
2025-10-05 10:17:05.504265: Current learning rate: 0.00249
2025-10-05 10:17:51.725887: Validation loss did not improve from -0.15415. Patience: 112/50
2025-10-05 10:17:51.726656: train_loss -0.9482
2025-10-05 10:17:51.726887: val_loss 0.4228
2025-10-05 10:17:51.727011: Pseudo dice [np.float32(0.4551)]
2025-10-05 10:17:51.727149: Epoch time: 46.22 s
2025-10-05 10:17:52.400463: 
2025-10-05 10:17:52.400762: Epoch 119
2025-10-05 10:17:52.400971: Current learning rate: 0.00242
2025-10-05 10:18:38.587155: Validation loss did not improve from -0.15415. Patience: 113/50
2025-10-05 10:18:38.587818: train_loss -0.9486
2025-10-05 10:18:38.588167: val_loss 0.5254
2025-10-05 10:18:38.588513: Pseudo dice [np.float32(0.4296)]
2025-10-05 10:18:38.588871: Epoch time: 46.19 s
2025-10-05 10:18:40.273007: 
2025-10-05 10:18:40.273410: Epoch 120
2025-10-05 10:18:40.273697: Current learning rate: 0.00235
2025-10-05 10:19:26.436524: Validation loss did not improve from -0.15415. Patience: 114/50
2025-10-05 10:19:26.437238: train_loss -0.9481
2025-10-05 10:19:26.437407: val_loss 0.4708
2025-10-05 10:19:26.437565: Pseudo dice [np.float32(0.4508)]
2025-10-05 10:19:26.437732: Epoch time: 46.16 s
2025-10-05 10:19:27.103010: 
2025-10-05 10:19:27.103352: Epoch 121
2025-10-05 10:19:27.103596: Current learning rate: 0.00228
2025-10-05 10:20:13.221977: Validation loss did not improve from -0.15415. Patience: 115/50
2025-10-05 10:20:13.222537: train_loss -0.9485
2025-10-05 10:20:13.222702: val_loss 0.5718
2025-10-05 10:20:13.222903: Pseudo dice [np.float32(0.4151)]
2025-10-05 10:20:13.223060: Epoch time: 46.12 s
2025-10-05 10:20:13.891666: 
2025-10-05 10:20:13.891941: Epoch 122
2025-10-05 10:20:13.892119: Current learning rate: 0.00221
2025-10-05 10:21:00.034559: Validation loss did not improve from -0.15415. Patience: 116/50
2025-10-05 10:21:00.035447: train_loss -0.9489
2025-10-05 10:21:00.035723: val_loss 0.5946
2025-10-05 10:21:00.035966: Pseudo dice [np.float32(0.417)]
2025-10-05 10:21:00.036235: Epoch time: 46.14 s
2025-10-05 10:21:00.712313: 
2025-10-05 10:21:00.712867: Epoch 123
2025-10-05 10:21:00.713350: Current learning rate: 0.00214
2025-10-05 10:21:46.893089: Validation loss did not improve from -0.15415. Patience: 117/50
2025-10-05 10:21:46.893971: train_loss -0.9502
2025-10-05 10:21:46.894500: val_loss 0.5912
2025-10-05 10:21:46.894968: Pseudo dice [np.float32(0.4168)]
2025-10-05 10:21:46.895415: Epoch time: 46.18 s
2025-10-05 10:21:47.566575: 
2025-10-05 10:21:47.567141: Epoch 124
2025-10-05 10:21:47.567644: Current learning rate: 0.00207
2025-10-05 10:22:33.740904: Validation loss did not improve from -0.15415. Patience: 118/50
2025-10-05 10:22:33.741612: train_loss -0.9499
2025-10-05 10:22:33.741832: val_loss 0.5822
2025-10-05 10:22:33.742011: Pseudo dice [np.float32(0.4257)]
2025-10-05 10:22:33.742184: Epoch time: 46.18 s
2025-10-05 10:22:34.865256: 
2025-10-05 10:22:34.865565: Epoch 125
2025-10-05 10:22:34.865826: Current learning rate: 0.00199
2025-10-05 10:23:21.073634: Validation loss did not improve from -0.15415. Patience: 119/50
2025-10-05 10:23:21.074066: train_loss -0.9497
2025-10-05 10:23:21.074206: val_loss 0.5701
2025-10-05 10:23:21.074318: Pseudo dice [np.float32(0.4357)]
2025-10-05 10:23:21.074470: Epoch time: 46.21 s
2025-10-05 10:23:21.734775: 
2025-10-05 10:23:21.735034: Epoch 126
2025-10-05 10:23:21.735203: Current learning rate: 0.00192
2025-10-05 10:24:08.002356: Validation loss did not improve from -0.15415. Patience: 120/50
2025-10-05 10:24:08.002993: train_loss -0.9494
2025-10-05 10:24:08.003143: val_loss 0.5872
2025-10-05 10:24:08.003285: Pseudo dice [np.float32(0.4193)]
2025-10-05 10:24:08.003427: Epoch time: 46.27 s
2025-10-05 10:24:08.674110: 
2025-10-05 10:24:08.674479: Epoch 127
2025-10-05 10:24:08.674685: Current learning rate: 0.00185
2025-10-05 10:24:54.881919: Validation loss did not improve from -0.15415. Patience: 121/50
2025-10-05 10:24:54.882456: train_loss -0.9499
2025-10-05 10:24:54.882597: val_loss 0.5628
2025-10-05 10:24:54.882714: Pseudo dice [np.float32(0.4102)]
2025-10-05 10:24:54.882944: Epoch time: 46.21 s
2025-10-05 10:24:55.563119: 
2025-10-05 10:24:55.563570: Epoch 128
2025-10-05 10:24:55.563841: Current learning rate: 0.00178
2025-10-05 10:25:41.784080: Validation loss did not improve from -0.15415. Patience: 122/50
2025-10-05 10:25:41.785614: train_loss -0.9498
2025-10-05 10:25:41.786063: val_loss 0.5668
2025-10-05 10:25:41.786442: Pseudo dice [np.float32(0.414)]
2025-10-05 10:25:41.786998: Epoch time: 46.22 s
2025-10-05 10:25:42.461500: 
2025-10-05 10:25:42.461868: Epoch 129
2025-10-05 10:25:42.462101: Current learning rate: 0.0017
2025-10-05 10:26:28.664739: Validation loss did not improve from -0.15415. Patience: 123/50
2025-10-05 10:26:28.665219: train_loss -0.95
2025-10-05 10:26:28.665476: val_loss 0.5977
2025-10-05 10:26:28.665618: Pseudo dice [np.float32(0.3888)]
2025-10-05 10:26:28.665785: Epoch time: 46.2 s
2025-10-05 10:26:29.769155: 
2025-10-05 10:26:29.769578: Epoch 130
2025-10-05 10:26:29.769827: Current learning rate: 0.00163
2025-10-05 10:27:15.922105: Validation loss did not improve from -0.15415. Patience: 124/50
2025-10-05 10:27:15.922783: train_loss -0.9507
2025-10-05 10:27:15.922962: val_loss 0.6488
2025-10-05 10:27:15.923124: Pseudo dice [np.float32(0.3881)]
2025-10-05 10:27:15.923303: Epoch time: 46.15 s
2025-10-05 10:27:16.584765: 
2025-10-05 10:27:16.585126: Epoch 131
2025-10-05 10:27:16.585360: Current learning rate: 0.00156
2025-10-05 10:28:02.740806: Validation loss did not improve from -0.15415. Patience: 125/50
2025-10-05 10:28:02.741293: train_loss -0.9508
2025-10-05 10:28:02.741467: val_loss 0.5468
2025-10-05 10:28:02.741590: Pseudo dice [np.float32(0.3888)]
2025-10-05 10:28:02.741742: Epoch time: 46.16 s
2025-10-05 10:28:03.401044: 
2025-10-05 10:28:03.401369: Epoch 132
2025-10-05 10:28:03.401575: Current learning rate: 0.00148
2025-10-05 10:28:49.545970: Validation loss did not improve from -0.15415. Patience: 126/50
2025-10-05 10:28:49.546558: train_loss -0.9513
2025-10-05 10:28:49.546700: val_loss 0.5896
2025-10-05 10:28:49.546837: Pseudo dice [np.float32(0.4144)]
2025-10-05 10:28:49.547057: Epoch time: 46.15 s
2025-10-05 10:28:50.213135: 
2025-10-05 10:28:50.213390: Epoch 133
2025-10-05 10:28:50.213595: Current learning rate: 0.00141
2025-10-05 10:29:36.383375: Validation loss did not improve from -0.15415. Patience: 127/50
2025-10-05 10:29:36.383883: train_loss -0.9514
2025-10-05 10:29:36.384051: val_loss 0.5229
2025-10-05 10:29:36.384176: Pseudo dice [np.float32(0.4391)]
2025-10-05 10:29:36.384310: Epoch time: 46.17 s
2025-10-05 10:29:37.044255: 
2025-10-05 10:29:37.044689: Epoch 134
2025-10-05 10:29:37.044934: Current learning rate: 0.00133
2025-10-05 10:30:23.166480: Validation loss did not improve from -0.15415. Patience: 128/50
2025-10-05 10:30:23.168071: train_loss -0.9502
2025-10-05 10:30:23.168460: val_loss 0.5961
2025-10-05 10:30:23.168823: Pseudo dice [np.float32(0.4215)]
2025-10-05 10:30:23.169305: Epoch time: 46.12 s
2025-10-05 10:30:24.909356: 
2025-10-05 10:30:24.910006: Epoch 135
2025-10-05 10:30:24.910422: Current learning rate: 0.00126
2025-10-05 10:31:11.031084: Validation loss did not improve from -0.15415. Patience: 129/50
2025-10-05 10:31:11.031858: train_loss -0.9515
2025-10-05 10:31:11.032285: val_loss 0.6355
2025-10-05 10:31:11.032650: Pseudo dice [np.float32(0.4096)]
2025-10-05 10:31:11.033066: Epoch time: 46.12 s
2025-10-05 10:31:11.703483: 
2025-10-05 10:31:11.704025: Epoch 136
2025-10-05 10:31:11.704551: Current learning rate: 0.00118
2025-10-05 10:31:57.841928: Validation loss did not improve from -0.15415. Patience: 130/50
2025-10-05 10:31:57.843123: train_loss -0.9511
2025-10-05 10:31:57.843585: val_loss 0.6013
2025-10-05 10:31:57.843844: Pseudo dice [np.float32(0.3962)]
2025-10-05 10:31:57.844245: Epoch time: 46.14 s
2025-10-05 10:31:58.521226: 
2025-10-05 10:31:58.521766: Epoch 137
2025-10-05 10:31:58.522115: Current learning rate: 0.00111
2025-10-05 10:32:44.733272: Validation loss did not improve from -0.15415. Patience: 131/50
2025-10-05 10:32:44.733918: train_loss -0.9516
2025-10-05 10:32:44.734299: val_loss 0.5331
2025-10-05 10:32:44.734622: Pseudo dice [np.float32(0.4218)]
2025-10-05 10:32:44.735001: Epoch time: 46.21 s
2025-10-05 10:32:45.405865: 
2025-10-05 10:32:45.406187: Epoch 138
2025-10-05 10:32:45.406390: Current learning rate: 0.00103
2025-10-05 10:33:31.548435: Validation loss did not improve from -0.15415. Patience: 132/50
2025-10-05 10:33:31.549753: train_loss -0.9516
2025-10-05 10:33:31.550123: val_loss 0.6322
2025-10-05 10:33:31.550482: Pseudo dice [np.float32(0.4211)]
2025-10-05 10:33:31.550833: Epoch time: 46.14 s
2025-10-05 10:33:32.226239: 
2025-10-05 10:33:32.226742: Epoch 139
2025-10-05 10:33:32.227059: Current learning rate: 0.00095
2025-10-05 10:34:18.347155: Validation loss did not improve from -0.15415. Patience: 133/50
2025-10-05 10:34:18.347853: train_loss -0.9521
2025-10-05 10:34:18.348226: val_loss 0.5658
2025-10-05 10:34:18.348749: Pseudo dice [np.float32(0.4188)]
2025-10-05 10:34:18.349188: Epoch time: 46.12 s
2025-10-05 10:34:19.458317: 
2025-10-05 10:34:19.458900: Epoch 140
2025-10-05 10:34:19.459326: Current learning rate: 0.00087
2025-10-05 10:35:05.553681: Validation loss did not improve from -0.15415. Patience: 134/50
2025-10-05 10:35:05.555235: train_loss -0.9519
2025-10-05 10:35:05.555716: val_loss 0.534
2025-10-05 10:35:05.556071: Pseudo dice [np.float32(0.4241)]
2025-10-05 10:35:05.556451: Epoch time: 46.1 s
2025-10-05 10:35:06.226771: 
2025-10-05 10:35:06.227296: Epoch 141
2025-10-05 10:35:06.227706: Current learning rate: 0.00079
2025-10-05 10:35:52.334138: Validation loss did not improve from -0.15415. Patience: 135/50
2025-10-05 10:35:52.334655: train_loss -0.9526
2025-10-05 10:35:52.334850: val_loss 0.5142
2025-10-05 10:35:52.335060: Pseudo dice [np.float32(0.4251)]
2025-10-05 10:35:52.335294: Epoch time: 46.11 s
2025-10-05 10:35:53.007470: 
2025-10-05 10:35:53.007753: Epoch 142
2025-10-05 10:35:53.007954: Current learning rate: 0.00071
2025-10-05 10:36:39.125222: Validation loss did not improve from -0.15415. Patience: 136/50
2025-10-05 10:36:39.125936: train_loss -0.9525
2025-10-05 10:36:39.126141: val_loss 0.564
2025-10-05 10:36:39.126350: Pseudo dice [np.float32(0.4115)]
2025-10-05 10:36:39.126510: Epoch time: 46.12 s
2025-10-05 10:36:39.804256: 
2025-10-05 10:36:39.804610: Epoch 143
2025-10-05 10:36:39.804804: Current learning rate: 0.00063
2025-10-05 10:37:25.949606: Validation loss did not improve from -0.15415. Patience: 137/50
2025-10-05 10:37:25.950321: train_loss -0.9526
2025-10-05 10:37:25.950497: val_loss 0.5436
2025-10-05 10:37:25.950636: Pseudo dice [np.float32(0.4169)]
2025-10-05 10:37:25.950796: Epoch time: 46.15 s
2025-10-05 10:37:26.627238: 
2025-10-05 10:37:26.627617: Epoch 144
2025-10-05 10:37:26.627805: Current learning rate: 0.00055
2025-10-05 10:38:12.836788: Validation loss did not improve from -0.15415. Patience: 138/50
2025-10-05 10:38:12.837501: train_loss -0.9529
2025-10-05 10:38:12.837664: val_loss 0.6114
2025-10-05 10:38:12.837829: Pseudo dice [np.float32(0.3967)]
2025-10-05 10:38:12.837966: Epoch time: 46.21 s
2025-10-05 10:38:13.958126: 
2025-10-05 10:38:13.958395: Epoch 145
2025-10-05 10:38:13.958559: Current learning rate: 0.00047
2025-10-05 10:39:00.126744: Validation loss did not improve from -0.15415. Patience: 139/50
2025-10-05 10:39:00.127470: train_loss -0.9531
2025-10-05 10:39:00.127758: val_loss 0.5173
2025-10-05 10:39:00.128052: Pseudo dice [np.float32(0.4336)]
2025-10-05 10:39:00.128215: Epoch time: 46.17 s
2025-10-05 10:39:00.793178: 
2025-10-05 10:39:00.793514: Epoch 146
2025-10-05 10:39:00.793877: Current learning rate: 0.00038
2025-10-05 10:39:46.887654: Validation loss did not improve from -0.15415. Patience: 140/50
2025-10-05 10:39:46.888580: train_loss -0.9534
2025-10-05 10:39:46.888776: val_loss 0.5522
2025-10-05 10:39:46.888906: Pseudo dice [np.float32(0.4283)]
2025-10-05 10:39:46.889052: Epoch time: 46.1 s
2025-10-05 10:39:47.564686: 
2025-10-05 10:39:47.565468: Epoch 147
2025-10-05 10:39:47.566176: Current learning rate: 0.0003
2025-10-05 10:40:33.715923: Validation loss did not improve from -0.15415. Patience: 141/50
2025-10-05 10:40:33.716665: train_loss -0.9526
2025-10-05 10:40:33.717091: val_loss 0.49
2025-10-05 10:40:33.717483: Pseudo dice [np.float32(0.4458)]
2025-10-05 10:40:33.717974: Epoch time: 46.15 s
2025-10-05 10:40:34.396151: 
2025-10-05 10:40:34.396741: Epoch 148
2025-10-05 10:40:34.397151: Current learning rate: 0.00021
2025-10-05 10:41:20.567865: Validation loss did not improve from -0.15415. Patience: 142/50
2025-10-05 10:41:20.568834: train_loss -0.9528
2025-10-05 10:41:20.569044: val_loss 0.5188
2025-10-05 10:41:20.569299: Pseudo dice [np.float32(0.4222)]
2025-10-05 10:41:20.569532: Epoch time: 46.17 s
2025-10-05 10:41:21.748002: 
2025-10-05 10:41:21.748409: Epoch 149
2025-10-05 10:41:21.748667: Current learning rate: 0.00011
2025-10-05 10:42:07.916118: Validation loss did not improve from -0.15415. Patience: 143/50
2025-10-05 10:42:07.916924: train_loss -0.9535
2025-10-05 10:42:07.917257: val_loss 0.5468
2025-10-05 10:42:07.917487: Pseudo dice [np.float32(0.4359)]
2025-10-05 10:42:07.917722: Epoch time: 46.17 s
2025-10-05 10:42:09.107306: Training done.
2025-10-05 10:42:09.124733: Using splits from existing split file: /nfs/erelab001/shared/Computational_Group/Naravich/datasets/nnUNet_Datasets/nnUNet_preprocessed/Dataset310_nnInteractive_Calcium_OCT_CrossValidation/splits_final_20.json
2025-10-05 10:42:09.125019: The split file contains 5 splits.
2025-10-05 10:42:09.125130: Desired fold for training: 0
2025-10-05 10:42:09.125287: This split has 1 training and 7 validation cases.
2025-10-05 10:42:09.125488: predicting 101-019
2025-10-05 10:42:09.127649: 101-019, shape torch.Size([1, 375, 498, 498]), rank 0
2025-10-05 10:42:56.984908: predicting 101-044
2025-10-05 10:42:56.994417: 101-044, shape torch.Size([1, 404, 498, 498]), rank 0
2025-10-05 10:43:33.473469: predicting 101-045
2025-10-05 10:43:33.484275: 101-045, shape torch.Size([1, 375, 498, 498]), rank 0
2025-10-05 10:44:07.408020: predicting 106-002
2025-10-05 10:44:07.418306: 106-002, shape torch.Size([1, 539, 498, 498]), rank 0
2025-10-05 10:44:55.589245: predicting 701-013
2025-10-05 10:44:55.602322: 701-013, shape torch.Size([1, 375, 498, 498]), rank 0
2025-10-05 10:45:29.505750: predicting 704-003
2025-10-05 10:45:29.516126: 704-003, shape torch.Size([1, 375, 498, 498]), rank 0
2025-10-05 10:46:03.298056: predicting 706-005
2025-10-05 10:46:03.308971: 706-005, shape torch.Size([1, 375, 498, 498]), rank 0
2025-10-05 10:46:51.181489: Validation complete
2025-10-05 10:46:51.181828: Mean Validation Dice:  0.39241556125897353
Finished training fold 0 saving to /nfs/erelab001/shared/Computational_Group/Naravich/datasets/nnUNet_Datasets/nnUNet_results/Dataset310_nnInteractive_Calcium_OCT_CrossValidation/nnUNetTrainerScaleAnalysis20__nnUNetPlans__3d_32x160x128_b10/fold_0_No_Pretrained
