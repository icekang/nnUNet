/nfs/erelab001/shared/Computational_Group/Naravich/nnUNet/.venv/bin/python
Starting training with CONFIG=3d_32x160x128_b10, DATASET_ID=310, TRAINER=nnUNetTrainerScaleAnalysis80
/nfs/erelab001/shared/Computational_Group/Naravich/nnUNet/nnunetv2/training/nnUNetTrainer/nnUNetTrainer.py:169: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  self.grad_scaler = GradScaler() if self.device.type == 'cuda' else None

############################
INFO: You are using the old nnU-Net default plans. We have updated our recommendations. Please consider using those instead! Read more here: https://github.com/MIC-DKFZ/nnUNet/blob/master/documentation/resenc_presets.md
############################

Using device: cuda:0

#######################################################################
Please cite the following paper when using nnU-Net:
Isensee, F., Jaeger, P. F., Kohl, S. A., Petersen, J., & Maier-Hein, K. H. (2021). nnU-Net: a self-configuring method for deep learning-based biomedical image segmentation. Nature methods, 18(2), 203-211.
#######################################################################

2025-10-06 03:16:37.188273: do_dummy_2d_data_aug: True
2025-10-06 03:16:37.188661: Using splits from existing split file: /nfs/erelab001/shared/Computational_Group/Naravich/datasets/nnUNet_Datasets/nnUNet_preprocessed/Dataset310_nnInteractive_Calcium_OCT_CrossValidation/splits_final_80.json
2025-10-06 03:16:37.188850: The split file contains 5 splits.
2025-10-06 03:16:37.188954: Desired fold for training: 4
2025-10-06 03:16:37.189051: This split has 6 training and 5 validation cases.
/nfs/erelab001/shared/Computational_Group/Naravich/nnUNet/.venv/lib/python3.12/site-packages/fft_conv_pytorch/fft_conv.py:139: UserWarning: Using a non-tuple sequence for multidimensional indexing is deprecated and will be changed in pytorch 2.9; use x[tuple(seq)] instead of x[seq]. In pytorch 2.9 this will be interpreted as tensor index, x[torch.tensor(seq)], which will result either in an error or a different result (Triggered internally at /pytorch/torch/csrc/autograd/python_variable_indexing.cpp:306.)
  output = output[crop_slices].contiguous()
/nfs/erelab001/shared/Computational_Group/Naravich/nnUNet/.venv/lib/python3.12/site-packages/fft_conv_pytorch/fft_conv.py:139: UserWarning: Using a non-tuple sequence for multidimensional indexing is deprecated and will be changed in pytorch 2.9; use x[tuple(seq)] instead of x[seq]. In pytorch 2.9 this will be interpreted as tensor index, x[torch.tensor(seq)], which will result either in an error or a different result (Triggered internally at /pytorch/torch/csrc/autograd/python_variable_indexing.cpp:306.)
  output = output[crop_slices].contiguous()
/nfs/erelab001/shared/Computational_Group/Naravich/nnUNet/.venv/lib/python3.12/site-packages/fft_conv_pytorch/fft_conv.py:139: UserWarning: Using a non-tuple sequence for multidimensional indexing is deprecated and will be changed in pytorch 2.9; use x[tuple(seq)] instead of x[seq]. In pytorch 2.9 this will be interpreted as tensor index, x[torch.tensor(seq)], which will result either in an error or a different result (Triggered internally at /pytorch/torch/csrc/autograd/python_variable_indexing.cpp:306.)
  output = output[crop_slices].contiguous()
/nfs/erelab001/shared/Computational_Group/Naravich/nnUNet/.venv/lib/python3.12/site-packages/fft_conv_pytorch/fft_conv.py:139: UserWarning: Using a non-tuple sequence for multidimensional indexing is deprecated and will be changed in pytorch 2.9; use x[tuple(seq)] instead of x[seq]. In pytorch 2.9 this will be interpreted as tensor index, x[torch.tensor(seq)], which will result either in an error or a different result (Triggered internally at /pytorch/torch/csrc/autograd/python_variable_indexing.cpp:306.)
  output = output[crop_slices].contiguous()
/nfs/erelab001/shared/Computational_Group/Naravich/nnUNet/.venv/lib/python3.12/site-packages/fft_conv_pytorch/fft_conv.py:139: UserWarning: Using a non-tuple sequence for multidimensional indexing is deprecated and will be changed in pytorch 2.9; use x[tuple(seq)] instead of x[seq]. In pytorch 2.9 this will be interpreted as tensor index, x[torch.tensor(seq)], which will result either in an error or a different result (Triggered internally at /pytorch/torch/csrc/autograd/python_variable_indexing.cpp:306.)
  output = output[crop_slices].contiguous()
/nfs/erelab001/shared/Computational_Group/Naravich/nnUNet/.venv/lib/python3.12/site-packages/fft_conv_pytorch/fft_conv.py:139: UserWarning: Using a non-tuple sequence for multidimensional indexing is deprecated and will be changed in pytorch 2.9; use x[tuple(seq)] instead of x[seq]. In pytorch 2.9 this will be interpreted as tensor index, x[torch.tensor(seq)], which will result either in an error or a different result (Triggered internally at /pytorch/torch/csrc/autograd/python_variable_indexing.cpp:306.)
  output = output[crop_slices].contiguous()
/nfs/erelab001/shared/Computational_Group/Naravich/nnUNet/.venv/lib/python3.12/site-packages/fft_conv_pytorch/fft_conv.py:139: UserWarning: Using a non-tuple sequence for multidimensional indexing is deprecated and will be changed in pytorch 2.9; use x[tuple(seq)] instead of x[seq]. In pytorch 2.9 this will be interpreted as tensor index, x[torch.tensor(seq)], which will result either in an error or a different result (Triggered internally at /pytorch/torch/csrc/autograd/python_variable_indexing.cpp:306.)
  output = output[crop_slices].contiguous()
/nfs/erelab001/shared/Computational_Group/Naravich/nnUNet/.venv/lib/python3.12/site-packages/fft_conv_pytorch/fft_conv.py:139: UserWarning: Using a non-tuple sequence for multidimensional indexing is deprecated and will be changed in pytorch 2.9; use x[tuple(seq)] instead of x[seq]. In pytorch 2.9 this will be interpreted as tensor index, x[torch.tensor(seq)], which will result either in an error or a different result (Triggered internally at /pytorch/torch/csrc/autograd/python_variable_indexing.cpp:306.)
  output = output[crop_slices].contiguous()
/nfs/erelab001/shared/Computational_Group/Naravich/nnUNet/.venv/lib/python3.12/site-packages/fft_conv_pytorch/fft_conv.py:139: UserWarning: Using a non-tuple sequence for multidimensional indexing is deprecated and will be changed in pytorch 2.9; use x[tuple(seq)] instead of x[seq]. In pytorch 2.9 this will be interpreted as tensor index, x[torch.tensor(seq)], which will result either in an error or a different result (Triggered internally at /pytorch/torch/csrc/autograd/python_variable_indexing.cpp:306.)
  output = output[crop_slices].contiguous()
/nfs/erelab001/shared/Computational_Group/Naravich/nnUNet/.venv/lib/python3.12/site-packages/fft_conv_pytorch/fft_conv.py:139: UserWarning: Using a non-tuple sequence for multidimensional indexing is deprecated and will be changed in pytorch 2.9; use x[tuple(seq)] instead of x[seq]. In pytorch 2.9 this will be interpreted as tensor index, x[torch.tensor(seq)], which will result either in an error or a different result (Triggered internally at /pytorch/torch/csrc/autograd/python_variable_indexing.cpp:306.)
  output = output[crop_slices].contiguous()
/nfs/erelab001/shared/Computational_Group/Naravich/nnUNet/.venv/lib/python3.12/site-packages/fft_conv_pytorch/fft_conv.py:139: UserWarning: Using a non-tuple sequence for multidimensional indexing is deprecated and will be changed in pytorch 2.9; use x[tuple(seq)] instead of x[seq]. In pytorch 2.9 this will be interpreted as tensor index, x[torch.tensor(seq)], which will result either in an error or a different result (Triggered internally at /pytorch/torch/csrc/autograd/python_variable_indexing.cpp:306.)
  output = output[crop_slices].contiguous()
/nfs/erelab001/shared/Computational_Group/Naravich/nnUNet/.venv/lib/python3.12/site-packages/fft_conv_pytorch/fft_conv.py:139: UserWarning: Using a non-tuple sequence for multidimensional indexing is deprecated and will be changed in pytorch 2.9; use x[tuple(seq)] instead of x[seq]. In pytorch 2.9 this will be interpreted as tensor index, x[torch.tensor(seq)], which will result either in an error or a different result (Triggered internally at /pytorch/torch/csrc/autograd/python_variable_indexing.cpp:306.)
  output = output[crop_slices].contiguous()
using pin_memory on device 0
using pin_memory on device 0
2025-10-06 03:16:42.139860: Using torch.compile...

This is the configuration used by this training:
Configuration name: 3d_32x160x128_b10
 {'data_identifier': 'nnUNetPlans_3d_fullres', 'preprocessor_name': 'DefaultPreprocessor', 'batch_size': 10, 'patch_size': [32, 160, 128], 'median_image_size_in_voxels': [375.0, 498.0, 498.0], 'spacing': [1.0, 1.0, 1.0], 'normalization_schemes': ['ZScoreNormalization'], 'use_mask_for_norm': [False], 'resampling_fn_data': 'resample_data_or_seg_to_shape', 'resampling_fn_seg': 'resample_data_or_seg_to_shape', 'resampling_fn_data_kwargs': {'is_seg': False, 'order': 3, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_seg_kwargs': {'is_seg': True, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_probabilities': 'resample_data_or_seg_to_shape', 'resampling_fn_probabilities_kwargs': {'is_seg': False, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'architecture': {'network_class_name': 'dynamic_network_architectures.architectures.unet.PlainConvUNet', 'arch_kwargs': {'n_stages': 6, 'features_per_stage': [32, 64, 128, 256, 320, 320], 'conv_op': 'torch.nn.modules.conv.Conv3d', 'kernel_sizes': [[3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3]], 'strides': [[1, 1, 1], [2, 2, 2], [2, 2, 2], [2, 2, 2], [2, 2, 2], [1, 2, 2]], 'n_conv_per_stage': [2, 2, 2, 2, 2, 2], 'n_conv_per_stage_decoder': [2, 2, 2, 2, 2], 'conv_bias': True, 'norm_op': 'torch.nn.modules.instancenorm.InstanceNorm3d', 'norm_op_kwargs': {'eps': 1e-05, 'affine': True}, 'dropout_op': None, 'dropout_op_kwargs': None, 'nonlin': 'torch.nn.LeakyReLU', 'nonlin_kwargs': {'inplace': True}, 'deep_supervision': True}, '_kw_requires_import': ['conv_op', 'norm_op', 'dropout_op', 'nonlin']}, 'batch_dice': True, 'inherits_from': '3d_fullres'} 

These are the global plan.json settings:
 {'dataset_name': 'Dataset310_nnInteractive_Calcium_OCT_CrossValidation', 'plans_name': 'nnUNetPlans', 'original_median_spacing_after_transp': [1.0, 1.0, 1.0], 'original_median_shape_after_transp': [375, 498, 498], 'image_reader_writer': 'SimpleITKIO', 'transpose_forward': [0, 1, 2], 'transpose_backward': [0, 1, 2], 'experiment_planner_used': 'ExperimentPlanner', 'label_manager': 'LabelManager', 'foreground_intensity_properties_per_channel': {'0': {'max': 0.4977976381778717, 'mean': 0.1394134759902954, 'median': 0.09849607944488525, 'min': 0.0, 'percentile_00_5': 0.015305490233004093, 'percentile_99_5': 0.4977976381778717, 'std': 0.121165432035923}}} 

2025-10-06 03:16:43.756600: unpacking dataset...
/nfs/erelab001/shared/Computational_Group/Naravich/nnUNet/.venv/lib/python3.12/site-packages/torch/_inductor/lowering.py:7095: UserWarning: 
Online softmax is disabled on the fly since Inductor decides to
split the reduction. Cut an issue to PyTorch if this is an
important use case and you want to speed it up with online
softmax.

  warnings.warn(
/nfs/erelab001/shared/Computational_Group/Naravich/nnUNet/.venv/lib/python3.12/site-packages/torch/_inductor/lowering.py:7095: UserWarning: 
Online softmax is disabled on the fly since Inductor decides to
split the reduction. Cut an issue to PyTorch if this is an
important use case and you want to speed it up with online
softmax.

  warnings.warn(
/nfs/erelab001/shared/Computational_Group/Naravich/nnUNet/.venv/lib/python3.12/site-packages/torch/_inductor/lowering.py:7095: UserWarning: 
Online softmax is disabled on the fly since Inductor decides to
split the reduction. Cut an issue to PyTorch if this is an
important use case and you want to speed it up with online
softmax.

  warnings.warn(
/nfs/erelab001/shared/Computational_Group/Naravich/nnUNet/.venv/lib/python3.12/site-packages/torch/_inductor/lowering.py:7095: UserWarning: 
Online softmax is disabled on the fly since Inductor decides to
split the reduction. Cut an issue to PyTorch if this is an
important use case and you want to speed it up with online
softmax.

  warnings.warn(
2025-10-06 03:16:48.146439: unpacking done...
2025-10-06 03:16:48.148417: Unable to plot network architecture: nnUNet_compile is enabled!
2025-10-06 03:16:48.153235: 
2025-10-06 03:16:48.153444: Epoch 0
2025-10-06 03:16:48.153646: Current learning rate: 0.01
2025-10-06 03:18:10.028476: Validation loss improved from 1000.00000 to -0.18644! Patience: 0/50
2025-10-06 03:18:10.028914: train_loss -0.1507
2025-10-06 03:18:10.029092: val_loss -0.1864
2025-10-06 03:18:10.029215: Pseudo dice [np.float32(0.5429)]
2025-10-06 03:18:10.029374: Epoch time: 81.88 s
2025-10-06 03:18:10.029518: Yayy! New best EMA pseudo Dice: 0.542900025844574
2025-10-06 03:18:11.044874: 
2025-10-06 03:18:11.045145: Epoch 1
2025-10-06 03:18:11.045323: Current learning rate: 0.00994
2025-10-06 03:18:57.843333: Validation loss improved from -0.18644 to -0.22599! Patience: 0/50
2025-10-06 03:18:57.843839: train_loss -0.287
2025-10-06 03:18:57.843996: val_loss -0.226
2025-10-06 03:18:57.844185: Pseudo dice [np.float32(0.5583)]
2025-10-06 03:18:57.844332: Epoch time: 46.8 s
2025-10-06 03:18:57.844443: Yayy! New best EMA pseudo Dice: 0.5443999767303467
2025-10-06 03:18:58.920668: 
2025-10-06 03:18:58.920947: Epoch 2
2025-10-06 03:18:58.921136: Current learning rate: 0.00988
2025-10-06 03:19:45.823035: Validation loss did not improve from -0.22599. Patience: 1/50
2025-10-06 03:19:45.823617: train_loss -0.3328
2025-10-06 03:19:45.823763: val_loss -0.1715
2025-10-06 03:19:45.823958: Pseudo dice [np.float32(0.5394)]
2025-10-06 03:19:45.824176: Epoch time: 46.9 s
2025-10-06 03:19:46.492187: 
2025-10-06 03:19:46.492568: Epoch 3
2025-10-06 03:19:46.492777: Current learning rate: 0.00982
2025-10-06 03:20:33.290912: Validation loss did not improve from -0.22599. Patience: 2/50
2025-10-06 03:20:33.291448: train_loss -0.3389
2025-10-06 03:20:33.291615: val_loss -0.2237
2025-10-06 03:20:33.291759: Pseudo dice [np.float32(0.5592)]
2025-10-06 03:20:33.291912: Epoch time: 46.8 s
2025-10-06 03:20:33.292166: Yayy! New best EMA pseudo Dice: 0.5454999804496765
2025-10-06 03:20:34.402395: 
2025-10-06 03:20:34.402751: Epoch 4
2025-10-06 03:20:34.402940: Current learning rate: 0.00976
2025-10-06 03:21:21.216848: Validation loss improved from -0.22599 to -0.24397! Patience: 2/50
2025-10-06 03:21:21.217558: train_loss -0.3757
2025-10-06 03:21:21.217802: val_loss -0.244
2025-10-06 03:21:21.217949: Pseudo dice [np.float32(0.5666)]
2025-10-06 03:21:21.218112: Epoch time: 46.82 s
2025-10-06 03:21:21.627421: Yayy! New best EMA pseudo Dice: 0.5475999712944031
2025-10-06 03:21:22.723639: 
2025-10-06 03:21:22.723951: Epoch 5
2025-10-06 03:21:22.724185: Current learning rate: 0.0097
2025-10-06 03:22:09.576778: Validation loss improved from -0.24397 to -0.28250! Patience: 0/50
2025-10-06 03:22:09.577253: train_loss -0.4205
2025-10-06 03:22:09.577435: val_loss -0.2825
2025-10-06 03:22:09.577561: Pseudo dice [np.float32(0.5882)]
2025-10-06 03:22:09.577698: Epoch time: 46.85 s
2025-10-06 03:22:09.577824: Yayy! New best EMA pseudo Dice: 0.5515999794006348
2025-10-06 03:22:10.664950: 
2025-10-06 03:22:10.665310: Epoch 6
2025-10-06 03:22:10.665509: Current learning rate: 0.00964
2025-10-06 03:22:57.544163: Validation loss improved from -0.28250 to -0.31021! Patience: 0/50
2025-10-06 03:22:57.544888: train_loss -0.4319
2025-10-06 03:22:57.545238: val_loss -0.3102
2025-10-06 03:22:57.545476: Pseudo dice [np.float32(0.6186)]
2025-10-06 03:22:57.545766: Epoch time: 46.88 s
2025-10-06 03:22:57.545996: Yayy! New best EMA pseudo Dice: 0.5583000183105469
2025-10-06 03:22:58.645732: 
2025-10-06 03:22:58.646001: Epoch 7
2025-10-06 03:22:58.646270: Current learning rate: 0.00958
2025-10-06 03:23:45.588147: Validation loss improved from -0.31021 to -0.37917! Patience: 0/50
2025-10-06 03:23:45.588633: train_loss -0.4506
2025-10-06 03:23:45.588835: val_loss -0.3792
2025-10-06 03:23:45.589010: Pseudo dice [np.float32(0.6584)]
2025-10-06 03:23:45.589189: Epoch time: 46.94 s
2025-10-06 03:23:45.589359: Yayy! New best EMA pseudo Dice: 0.5683000087738037
2025-10-06 03:23:46.712428: 
2025-10-06 03:23:46.712839: Epoch 8
2025-10-06 03:23:46.713105: Current learning rate: 0.00952
2025-10-06 03:24:33.736856: Validation loss did not improve from -0.37917. Patience: 1/50
2025-10-06 03:24:33.737460: train_loss -0.4819
2025-10-06 03:24:33.737630: val_loss -0.3424
2025-10-06 03:24:33.737792: Pseudo dice [np.float32(0.6317)]
2025-10-06 03:24:33.737972: Epoch time: 47.03 s
2025-10-06 03:24:33.738123: Yayy! New best EMA pseudo Dice: 0.5746999979019165
2025-10-06 03:24:34.858064: 
2025-10-06 03:24:34.858347: Epoch 9
2025-10-06 03:24:34.858515: Current learning rate: 0.00946
2025-10-06 03:25:21.877970: Validation loss did not improve from -0.37917. Patience: 2/50
2025-10-06 03:25:21.878725: train_loss -0.4835
2025-10-06 03:25:21.879087: val_loss -0.3295
2025-10-06 03:25:21.879369: Pseudo dice [np.float32(0.6323)]
2025-10-06 03:25:21.879715: Epoch time: 47.02 s
2025-10-06 03:25:22.335459: Yayy! New best EMA pseudo Dice: 0.5803999900817871
2025-10-06 03:25:23.448027: 
2025-10-06 03:25:23.448382: Epoch 10
2025-10-06 03:25:23.448597: Current learning rate: 0.0094
2025-10-06 03:26:10.435859: Validation loss improved from -0.37917 to -0.40728! Patience: 2/50
2025-10-06 03:26:10.436386: train_loss -0.503
2025-10-06 03:26:10.436551: val_loss -0.4073
2025-10-06 03:26:10.436684: Pseudo dice [np.float32(0.6617)]
2025-10-06 03:26:10.436831: Epoch time: 46.99 s
2025-10-06 03:26:10.436938: Yayy! New best EMA pseudo Dice: 0.5885999798774719
2025-10-06 03:26:11.531368: 
2025-10-06 03:26:11.531698: Epoch 11
2025-10-06 03:26:11.531893: Current learning rate: 0.00934
2025-10-06 03:26:58.455057: Validation loss did not improve from -0.40728. Patience: 1/50
2025-10-06 03:26:58.455477: train_loss -0.5127
2025-10-06 03:26:58.455644: val_loss -0.2994
2025-10-06 03:26:58.455809: Pseudo dice [np.float32(0.6168)]
2025-10-06 03:26:58.455973: Epoch time: 46.92 s
2025-10-06 03:26:58.456107: Yayy! New best EMA pseudo Dice: 0.5914000272750854
2025-10-06 03:26:59.546783: 
2025-10-06 03:26:59.547131: Epoch 12
2025-10-06 03:26:59.547312: Current learning rate: 0.00928
2025-10-06 03:27:46.480852: Validation loss did not improve from -0.40728. Patience: 2/50
2025-10-06 03:27:46.481605: train_loss -0.5232
2025-10-06 03:27:46.481941: val_loss -0.3866
2025-10-06 03:27:46.482156: Pseudo dice [np.float32(0.6629)]
2025-10-06 03:27:46.482445: Epoch time: 46.94 s
2025-10-06 03:27:46.482656: Yayy! New best EMA pseudo Dice: 0.5985000133514404
2025-10-06 03:27:48.140597: 
2025-10-06 03:27:48.141032: Epoch 13
2025-10-06 03:27:48.141322: Current learning rate: 0.00922
2025-10-06 03:28:35.081954: Validation loss did not improve from -0.40728. Patience: 3/50
2025-10-06 03:28:35.082321: train_loss -0.5294
2025-10-06 03:28:35.082505: val_loss -0.3974
2025-10-06 03:28:35.082647: Pseudo dice [np.float32(0.6707)]
2025-10-06 03:28:35.082784: Epoch time: 46.94 s
2025-10-06 03:28:35.082903: Yayy! New best EMA pseudo Dice: 0.6057000160217285
2025-10-06 03:28:36.194942: 
2025-10-06 03:28:36.195341: Epoch 14
2025-10-06 03:28:36.195598: Current learning rate: 0.00916
2025-10-06 03:29:23.230740: Validation loss did not improve from -0.40728. Patience: 4/50
2025-10-06 03:29:23.231487: train_loss -0.5405
2025-10-06 03:29:23.231726: val_loss -0.3604
2025-10-06 03:29:23.232063: Pseudo dice [np.float32(0.6318)]
2025-10-06 03:29:23.232320: Epoch time: 47.04 s
2025-10-06 03:29:23.667128: Yayy! New best EMA pseudo Dice: 0.6083999872207642
2025-10-06 03:29:24.752924: 
2025-10-06 03:29:24.753304: Epoch 15
2025-10-06 03:29:24.753529: Current learning rate: 0.0091
2025-10-06 03:30:11.718258: Validation loss did not improve from -0.40728. Patience: 5/50
2025-10-06 03:30:11.718921: train_loss -0.5568
2025-10-06 03:30:11.719148: val_loss -0.3528
2025-10-06 03:30:11.719347: Pseudo dice [np.float32(0.6375)]
2025-10-06 03:30:11.719584: Epoch time: 46.97 s
2025-10-06 03:30:11.719795: Yayy! New best EMA pseudo Dice: 0.611299991607666
2025-10-06 03:30:12.822138: 
2025-10-06 03:30:12.822485: Epoch 16
2025-10-06 03:30:12.822701: Current learning rate: 0.00903
2025-10-06 03:30:59.773052: Validation loss did not improve from -0.40728. Patience: 6/50
2025-10-06 03:30:59.774163: train_loss -0.5551
2025-10-06 03:30:59.774594: val_loss -0.3998
2025-10-06 03:30:59.774920: Pseudo dice [np.float32(0.6695)]
2025-10-06 03:30:59.775229: Epoch time: 46.95 s
2025-10-06 03:30:59.775479: Yayy! New best EMA pseudo Dice: 0.6171000003814697
2025-10-06 03:31:00.877290: 
2025-10-06 03:31:00.877594: Epoch 17
2025-10-06 03:31:00.877789: Current learning rate: 0.00897
2025-10-06 03:31:47.877937: Validation loss did not improve from -0.40728. Patience: 7/50
2025-10-06 03:31:47.878545: train_loss -0.5756
2025-10-06 03:31:47.878722: val_loss -0.3887
2025-10-06 03:31:47.878881: Pseudo dice [np.float32(0.6549)]
2025-10-06 03:31:47.879034: Epoch time: 47.0 s
2025-10-06 03:31:47.879142: Yayy! New best EMA pseudo Dice: 0.6208999752998352
2025-10-06 03:31:48.984845: 
2025-10-06 03:31:48.985208: Epoch 18
2025-10-06 03:31:48.985414: Current learning rate: 0.00891
2025-10-06 03:32:36.004310: Validation loss did not improve from -0.40728. Patience: 8/50
2025-10-06 03:32:36.004998: train_loss -0.5737
2025-10-06 03:32:36.005141: val_loss -0.3946
2025-10-06 03:32:36.005300: Pseudo dice [np.float32(0.6604)]
2025-10-06 03:32:36.005497: Epoch time: 47.02 s
2025-10-06 03:32:36.005610: Yayy! New best EMA pseudo Dice: 0.6248000264167786
2025-10-06 03:32:37.092296: 
2025-10-06 03:32:37.092678: Epoch 19
2025-10-06 03:32:37.092897: Current learning rate: 0.00885
2025-10-06 03:33:24.122119: Validation loss did not improve from -0.40728. Patience: 9/50
2025-10-06 03:33:24.122569: train_loss -0.5831
2025-10-06 03:33:24.122709: val_loss -0.3845
2025-10-06 03:33:24.122841: Pseudo dice [np.float32(0.6576)]
2025-10-06 03:33:24.122969: Epoch time: 47.03 s
2025-10-06 03:33:24.573910: Yayy! New best EMA pseudo Dice: 0.6280999779701233
2025-10-06 03:33:25.676613: 
2025-10-06 03:33:25.676962: Epoch 20
2025-10-06 03:33:25.677198: Current learning rate: 0.00879
2025-10-06 03:34:12.777464: Validation loss improved from -0.40728 to -0.43863! Patience: 9/50
2025-10-06 03:34:12.778049: train_loss -0.5831
2025-10-06 03:34:12.778208: val_loss -0.4386
2025-10-06 03:34:12.778344: Pseudo dice [np.float32(0.6764)]
2025-10-06 03:34:12.778540: Epoch time: 47.1 s
2025-10-06 03:34:12.778691: Yayy! New best EMA pseudo Dice: 0.6328999996185303
2025-10-06 03:34:13.877088: 
2025-10-06 03:34:13.877377: Epoch 21
2025-10-06 03:34:13.877568: Current learning rate: 0.00873
2025-10-06 03:35:00.865616: Validation loss did not improve from -0.43863. Patience: 1/50
2025-10-06 03:35:00.866151: train_loss -0.5979
2025-10-06 03:35:00.866299: val_loss -0.3617
2025-10-06 03:35:00.866431: Pseudo dice [np.float32(0.647)]
2025-10-06 03:35:00.866571: Epoch time: 46.99 s
2025-10-06 03:35:00.866748: Yayy! New best EMA pseudo Dice: 0.6342999935150146
2025-10-06 03:35:01.957242: 
2025-10-06 03:35:01.957603: Epoch 22
2025-10-06 03:35:01.957808: Current learning rate: 0.00867
2025-10-06 03:35:48.861469: Validation loss did not improve from -0.43863. Patience: 2/50
2025-10-06 03:35:48.862157: train_loss -0.6066
2025-10-06 03:35:48.862396: val_loss -0.3966
2025-10-06 03:35:48.862599: Pseudo dice [np.float32(0.6628)]
2025-10-06 03:35:48.862907: Epoch time: 46.91 s
2025-10-06 03:35:48.863140: Yayy! New best EMA pseudo Dice: 0.6371999979019165
2025-10-06 03:35:49.953981: 
2025-10-06 03:35:49.954274: Epoch 23
2025-10-06 03:35:49.954493: Current learning rate: 0.00861
2025-10-06 03:36:36.920217: Validation loss did not improve from -0.43863. Patience: 3/50
2025-10-06 03:36:36.920687: train_loss -0.6132
2025-10-06 03:36:36.920888: val_loss -0.4119
2025-10-06 03:36:36.921025: Pseudo dice [np.float32(0.6735)]
2025-10-06 03:36:36.921218: Epoch time: 46.97 s
2025-10-06 03:36:36.921335: Yayy! New best EMA pseudo Dice: 0.6407999992370605
2025-10-06 03:36:38.000684: 
2025-10-06 03:36:38.001263: Epoch 24
2025-10-06 03:36:38.001523: Current learning rate: 0.00855
2025-10-06 03:37:25.044035: Validation loss did not improve from -0.43863. Patience: 4/50
2025-10-06 03:37:25.044536: train_loss -0.6101
2025-10-06 03:37:25.044721: val_loss -0.4188
2025-10-06 03:37:25.044849: Pseudo dice [np.float32(0.677)]
2025-10-06 03:37:25.045003: Epoch time: 47.04 s
2025-10-06 03:37:25.484873: Yayy! New best EMA pseudo Dice: 0.6444000005722046
2025-10-06 03:37:26.575208: 
2025-10-06 03:37:26.575570: Epoch 25
2025-10-06 03:37:26.575785: Current learning rate: 0.00849
2025-10-06 03:38:13.488281: Validation loss did not improve from -0.43863. Patience: 5/50
2025-10-06 03:38:13.488977: train_loss -0.6246
2025-10-06 03:38:13.489302: val_loss -0.404
2025-10-06 03:38:13.489549: Pseudo dice [np.float32(0.6694)]
2025-10-06 03:38:13.489755: Epoch time: 46.91 s
2025-10-06 03:38:13.490283: Yayy! New best EMA pseudo Dice: 0.6468999981880188
2025-10-06 03:38:14.603861: 
2025-10-06 03:38:14.604189: Epoch 26
2025-10-06 03:38:14.604382: Current learning rate: 0.00843
2025-10-06 03:39:01.591941: Validation loss did not improve from -0.43863. Patience: 6/50
2025-10-06 03:39:01.592664: train_loss -0.6228
2025-10-06 03:39:01.592858: val_loss -0.3999
2025-10-06 03:39:01.592978: Pseudo dice [np.float32(0.6702)]
2025-10-06 03:39:01.593118: Epoch time: 46.99 s
2025-10-06 03:39:01.593268: Yayy! New best EMA pseudo Dice: 0.6492999792098999
2025-10-06 03:39:02.679565: 
2025-10-06 03:39:02.679908: Epoch 27
2025-10-06 03:39:02.680116: Current learning rate: 0.00836
2025-10-06 03:39:49.616261: Validation loss did not improve from -0.43863. Patience: 7/50
2025-10-06 03:39:49.616770: train_loss -0.6255
2025-10-06 03:39:49.616927: val_loss -0.3955
2025-10-06 03:39:49.617062: Pseudo dice [np.float32(0.6699)]
2025-10-06 03:39:49.617227: Epoch time: 46.94 s
2025-10-06 03:39:49.617349: Yayy! New best EMA pseudo Dice: 0.6513000130653381
2025-10-06 03:39:51.247608: 
2025-10-06 03:39:51.247876: Epoch 28
2025-10-06 03:39:51.248060: Current learning rate: 0.0083
2025-10-06 03:40:38.170273: Validation loss did not improve from -0.43863. Patience: 8/50
2025-10-06 03:40:38.171075: train_loss -0.6316
2025-10-06 03:40:38.171382: val_loss -0.4062
2025-10-06 03:40:38.171605: Pseudo dice [np.float32(0.6806)]
2025-10-06 03:40:38.171829: Epoch time: 46.92 s
2025-10-06 03:40:38.172031: Yayy! New best EMA pseudo Dice: 0.65420001745224
2025-10-06 03:40:39.263544: 
2025-10-06 03:40:39.263891: Epoch 29
2025-10-06 03:40:39.264104: Current learning rate: 0.00824
2025-10-06 03:41:26.263593: Validation loss did not improve from -0.43863. Patience: 9/50
2025-10-06 03:41:26.264143: train_loss -0.641
2025-10-06 03:41:26.264344: val_loss -0.3756
2025-10-06 03:41:26.264532: Pseudo dice [np.float32(0.6534)]
2025-10-06 03:41:26.264714: Epoch time: 47.0 s
2025-10-06 03:41:27.385451: 
2025-10-06 03:41:27.385870: Epoch 30
2025-10-06 03:41:27.386097: Current learning rate: 0.00818
2025-10-06 03:42:14.393484: Validation loss did not improve from -0.43863. Patience: 10/50
2025-10-06 03:42:14.394067: train_loss -0.6428
2025-10-06 03:42:14.394218: val_loss -0.3966
2025-10-06 03:42:14.394337: Pseudo dice [np.float32(0.6639)]
2025-10-06 03:42:14.394515: Epoch time: 47.01 s
2025-10-06 03:42:14.394665: Yayy! New best EMA pseudo Dice: 0.6550999879837036
2025-10-06 03:42:15.493054: 
2025-10-06 03:42:15.493346: Epoch 31
2025-10-06 03:42:15.493514: Current learning rate: 0.00812
2025-10-06 03:43:02.492052: Validation loss did not improve from -0.43863. Patience: 11/50
2025-10-06 03:43:02.492771: train_loss -0.6455
2025-10-06 03:43:02.493024: val_loss -0.4025
2025-10-06 03:43:02.493197: Pseudo dice [np.float32(0.6566)]
2025-10-06 03:43:02.493396: Epoch time: 47.0 s
2025-10-06 03:43:02.493548: Yayy! New best EMA pseudo Dice: 0.6553000211715698
2025-10-06 03:43:03.586934: 
2025-10-06 03:43:03.587290: Epoch 32
2025-10-06 03:43:03.587528: Current learning rate: 0.00806
2025-10-06 03:43:50.535787: Validation loss did not improve from -0.43863. Patience: 12/50
2025-10-06 03:43:50.536497: train_loss -0.6521
2025-10-06 03:43:50.536662: val_loss -0.4287
2025-10-06 03:43:50.536819: Pseudo dice [np.float32(0.6886)]
2025-10-06 03:43:50.537090: Epoch time: 46.95 s
2025-10-06 03:43:50.537211: Yayy! New best EMA pseudo Dice: 0.6585999727249146
2025-10-06 03:43:51.637566: 
2025-10-06 03:43:51.637974: Epoch 33
2025-10-06 03:43:51.638203: Current learning rate: 0.008
2025-10-06 03:44:38.605204: Validation loss did not improve from -0.43863. Patience: 13/50
2025-10-06 03:44:38.605673: train_loss -0.6623
2025-10-06 03:44:38.605839: val_loss -0.4062
2025-10-06 03:44:38.605964: Pseudo dice [np.float32(0.665)]
2025-10-06 03:44:38.606099: Epoch time: 46.97 s
2025-10-06 03:44:38.606245: Yayy! New best EMA pseudo Dice: 0.6592000126838684
2025-10-06 03:44:39.702864: 
2025-10-06 03:44:39.703147: Epoch 34
2025-10-06 03:44:39.703354: Current learning rate: 0.00793
2025-10-06 03:45:26.686010: Validation loss did not improve from -0.43863. Patience: 14/50
2025-10-06 03:45:26.686859: train_loss -0.6582
2025-10-06 03:45:26.687201: val_loss -0.4254
2025-10-06 03:45:26.687517: Pseudo dice [np.float32(0.6775)]
2025-10-06 03:45:26.687834: Epoch time: 46.98 s
2025-10-06 03:45:27.120607: Yayy! New best EMA pseudo Dice: 0.6610999703407288
2025-10-06 03:45:28.205337: 
2025-10-06 03:45:28.205689: Epoch 35
2025-10-06 03:45:28.205873: Current learning rate: 0.00787
2025-10-06 03:46:15.157850: Validation loss did not improve from -0.43863. Patience: 15/50
2025-10-06 03:46:15.158344: train_loss -0.6615
2025-10-06 03:46:15.158477: val_loss -0.3625
2025-10-06 03:46:15.158607: Pseudo dice [np.float32(0.6452)]
2025-10-06 03:46:15.158732: Epoch time: 46.95 s
2025-10-06 03:46:15.808218: 
2025-10-06 03:46:15.808466: Epoch 36
2025-10-06 03:46:15.808642: Current learning rate: 0.00781
2025-10-06 03:47:02.770832: Validation loss did not improve from -0.43863. Patience: 16/50
2025-10-06 03:47:02.771435: train_loss -0.6724
2025-10-06 03:47:02.771586: val_loss -0.3839
2025-10-06 03:47:02.771696: Pseudo dice [np.float32(0.658)]
2025-10-06 03:47:02.771845: Epoch time: 46.96 s
2025-10-06 03:47:03.427717: 
2025-10-06 03:47:03.428035: Epoch 37
2025-10-06 03:47:03.428216: Current learning rate: 0.00775
2025-10-06 03:47:50.369526: Validation loss did not improve from -0.43863. Patience: 17/50
2025-10-06 03:47:50.369990: train_loss -0.6703
2025-10-06 03:47:50.370185: val_loss -0.3973
2025-10-06 03:47:50.370343: Pseudo dice [np.float32(0.6719)]
2025-10-06 03:47:50.370481: Epoch time: 46.94 s
2025-10-06 03:47:51.022722: 
2025-10-06 03:47:51.022988: Epoch 38
2025-10-06 03:47:51.023172: Current learning rate: 0.00769
2025-10-06 03:48:38.011962: Validation loss did not improve from -0.43863. Patience: 18/50
2025-10-06 03:48:38.012731: train_loss -0.6736
2025-10-06 03:48:38.013035: val_loss -0.3801
2025-10-06 03:48:38.013278: Pseudo dice [np.float32(0.6572)]
2025-10-06 03:48:38.013541: Epoch time: 46.99 s
2025-10-06 03:48:38.684466: 
2025-10-06 03:48:38.684852: Epoch 39
2025-10-06 03:48:38.685076: Current learning rate: 0.00763
2025-10-06 03:49:25.740026: Validation loss did not improve from -0.43863. Patience: 19/50
2025-10-06 03:49:25.740669: train_loss -0.6894
2025-10-06 03:49:25.740873: val_loss -0.3853
2025-10-06 03:49:25.741044: Pseudo dice [np.float32(0.6611)]
2025-10-06 03:49:25.741240: Epoch time: 47.06 s
2025-10-06 03:49:26.850181: 
2025-10-06 03:49:26.850784: Epoch 40
2025-10-06 03:49:26.851029: Current learning rate: 0.00756
2025-10-06 03:50:13.806324: Validation loss did not improve from -0.43863. Patience: 20/50
2025-10-06 03:50:13.807011: train_loss -0.6894
2025-10-06 03:50:13.807165: val_loss -0.4192
2025-10-06 03:50:13.807303: Pseudo dice [np.float32(0.6822)]
2025-10-06 03:50:13.807474: Epoch time: 46.96 s
2025-10-06 03:50:13.807592: Yayy! New best EMA pseudo Dice: 0.6625000238418579
2025-10-06 03:50:14.918043: 
2025-10-06 03:50:14.918356: Epoch 41
2025-10-06 03:50:14.918535: Current learning rate: 0.0075
2025-10-06 03:51:01.869260: Validation loss did not improve from -0.43863. Patience: 21/50
2025-10-06 03:51:01.869753: train_loss -0.6949
2025-10-06 03:51:01.869907: val_loss -0.4074
2025-10-06 03:51:01.870030: Pseudo dice [np.float32(0.6692)]
2025-10-06 03:51:01.870178: Epoch time: 46.95 s
2025-10-06 03:51:01.870290: Yayy! New best EMA pseudo Dice: 0.6632000207901001
2025-10-06 03:51:02.945718: 
2025-10-06 03:51:02.946100: Epoch 42
2025-10-06 03:51:02.946320: Current learning rate: 0.00744
2025-10-06 03:51:49.693338: Validation loss did not improve from -0.43863. Patience: 22/50
2025-10-06 03:51:49.694176: train_loss -0.6983
2025-10-06 03:51:49.694429: val_loss -0.359
2025-10-06 03:51:49.694612: Pseudo dice [np.float32(0.6464)]
2025-10-06 03:51:49.694817: Epoch time: 46.75 s
2025-10-06 03:51:50.905968: 
2025-10-06 03:51:50.906335: Epoch 43
2025-10-06 03:51:50.906538: Current learning rate: 0.00738
2025-10-06 03:52:37.739908: Validation loss did not improve from -0.43863. Patience: 23/50
2025-10-06 03:52:37.740505: train_loss -0.6972
2025-10-06 03:52:37.740645: val_loss -0.4032
2025-10-06 03:52:37.740867: Pseudo dice [np.float32(0.6625)]
2025-10-06 03:52:37.741182: Epoch time: 46.84 s
2025-10-06 03:52:38.388405: 
2025-10-06 03:52:38.388733: Epoch 44
2025-10-06 03:52:38.388914: Current learning rate: 0.00732
2025-10-06 03:53:25.439817: Validation loss did not improve from -0.43863. Patience: 24/50
2025-10-06 03:53:25.440513: train_loss -0.7099
2025-10-06 03:53:25.440685: val_loss -0.4249
2025-10-06 03:53:25.440841: Pseudo dice [np.float32(0.6875)]
2025-10-06 03:53:25.440995: Epoch time: 47.05 s
2025-10-06 03:53:25.890942: Yayy! New best EMA pseudo Dice: 0.6642000079154968
2025-10-06 03:53:26.980663: 
2025-10-06 03:53:26.981045: Epoch 45
2025-10-06 03:53:26.981222: Current learning rate: 0.00725
2025-10-06 03:54:13.972759: Validation loss improved from -0.43863 to -0.44003! Patience: 24/50
2025-10-06 03:54:13.973258: train_loss -0.7069
2025-10-06 03:54:13.973395: val_loss -0.44
2025-10-06 03:54:13.973528: Pseudo dice [np.float32(0.6862)]
2025-10-06 03:54:13.973675: Epoch time: 46.99 s
2025-10-06 03:54:13.973819: Yayy! New best EMA pseudo Dice: 0.6664000153541565
2025-10-06 03:54:15.074257: 
2025-10-06 03:54:15.074574: Epoch 46
2025-10-06 03:54:15.074841: Current learning rate: 0.00719
2025-10-06 03:55:02.034207: Validation loss did not improve from -0.44003. Patience: 1/50
2025-10-06 03:55:02.034791: train_loss -0.7139
2025-10-06 03:55:02.035037: val_loss -0.3782
2025-10-06 03:55:02.035232: Pseudo dice [np.float32(0.6537)]
2025-10-06 03:55:02.035463: Epoch time: 46.96 s
2025-10-06 03:55:02.694134: 
2025-10-06 03:55:02.694463: Epoch 47
2025-10-06 03:55:02.694658: Current learning rate: 0.00713
2025-10-06 03:55:49.645119: Validation loss did not improve from -0.44003. Patience: 2/50
2025-10-06 03:55:49.645627: train_loss -0.7193
2025-10-06 03:55:49.645796: val_loss -0.4156
2025-10-06 03:55:49.645941: Pseudo dice [np.float32(0.6756)]
2025-10-06 03:55:49.646093: Epoch time: 46.95 s
2025-10-06 03:55:50.300169: 
2025-10-06 03:55:50.300545: Epoch 48
2025-10-06 03:55:50.300767: Current learning rate: 0.00707
2025-10-06 03:56:37.224142: Validation loss did not improve from -0.44003. Patience: 3/50
2025-10-06 03:56:37.224638: train_loss -0.7258
2025-10-06 03:56:37.224851: val_loss -0.4079
2025-10-06 03:56:37.225011: Pseudo dice [np.float32(0.6823)]
2025-10-06 03:56:37.225184: Epoch time: 46.93 s
2025-10-06 03:56:37.225328: Yayy! New best EMA pseudo Dice: 0.6678000092506409
2025-10-06 03:56:38.324187: 
2025-10-06 03:56:38.324558: Epoch 49
2025-10-06 03:56:38.324739: Current learning rate: 0.007
2025-10-06 03:57:25.334643: Validation loss did not improve from -0.44003. Patience: 4/50
2025-10-06 03:57:25.335173: train_loss -0.7227
2025-10-06 03:57:25.335353: val_loss -0.4061
2025-10-06 03:57:25.335523: Pseudo dice [np.float32(0.6749)]
2025-10-06 03:57:25.335724: Epoch time: 47.01 s
2025-10-06 03:57:25.780043: Yayy! New best EMA pseudo Dice: 0.6685000061988831
2025-10-06 03:57:26.873000: 
2025-10-06 03:57:26.873312: Epoch 50
2025-10-06 03:57:26.873549: Current learning rate: 0.00694
2025-10-06 03:58:13.879182: Validation loss did not improve from -0.44003. Patience: 5/50
2025-10-06 03:58:13.879810: train_loss -0.7335
2025-10-06 03:58:13.880016: val_loss -0.42
2025-10-06 03:58:13.880171: Pseudo dice [np.float32(0.6838)]
2025-10-06 03:58:13.880322: Epoch time: 47.01 s
2025-10-06 03:58:13.880454: Yayy! New best EMA pseudo Dice: 0.6700000166893005
2025-10-06 03:58:14.977722: 
2025-10-06 03:58:14.977944: Epoch 51
2025-10-06 03:58:14.978134: Current learning rate: 0.00688
2025-10-06 03:59:01.941687: Validation loss did not improve from -0.44003. Patience: 6/50
2025-10-06 03:59:01.942178: train_loss -0.7351
2025-10-06 03:59:01.942413: val_loss -0.3846
2025-10-06 03:59:01.942685: Pseudo dice [np.float32(0.6768)]
2025-10-06 03:59:01.942953: Epoch time: 46.97 s
2025-10-06 03:59:01.943168: Yayy! New best EMA pseudo Dice: 0.6707000136375427
2025-10-06 03:59:03.029330: 
2025-10-06 03:59:03.029645: Epoch 52
2025-10-06 03:59:03.029810: Current learning rate: 0.00682
2025-10-06 03:59:49.972758: Validation loss did not improve from -0.44003. Patience: 7/50
2025-10-06 03:59:49.973416: train_loss -0.7402
2025-10-06 03:59:49.973589: val_loss -0.4243
2025-10-06 03:59:49.973792: Pseudo dice [np.float32(0.6866)]
2025-10-06 03:59:49.973934: Epoch time: 46.94 s
2025-10-06 03:59:49.974123: Yayy! New best EMA pseudo Dice: 0.6722999811172485
2025-10-06 03:59:51.100276: 
2025-10-06 03:59:51.100595: Epoch 53
2025-10-06 03:59:51.100803: Current learning rate: 0.00675
2025-10-06 04:00:38.131972: Validation loss did not improve from -0.44003. Patience: 8/50
2025-10-06 04:00:38.132720: train_loss -0.7422
2025-10-06 04:00:38.133032: val_loss -0.4041
2025-10-06 04:00:38.133277: Pseudo dice [np.float32(0.6777)]
2025-10-06 04:00:38.133474: Epoch time: 47.03 s
2025-10-06 04:00:38.133629: Yayy! New best EMA pseudo Dice: 0.6728000044822693
2025-10-06 04:00:39.249816: 
2025-10-06 04:00:39.250135: Epoch 54
2025-10-06 04:00:39.250304: Current learning rate: 0.00669
2025-10-06 04:01:26.298244: Validation loss did not improve from -0.44003. Patience: 9/50
2025-10-06 04:01:26.298756: train_loss -0.7445
2025-10-06 04:01:26.298927: val_loss -0.4124
2025-10-06 04:01:26.299067: Pseudo dice [np.float32(0.6802)]
2025-10-06 04:01:26.299218: Epoch time: 47.05 s
2025-10-06 04:01:26.738991: Yayy! New best EMA pseudo Dice: 0.6736000180244446
2025-10-06 04:01:27.833886: 
2025-10-06 04:01:27.834242: Epoch 55
2025-10-06 04:01:27.834438: Current learning rate: 0.00663
2025-10-06 04:02:14.866387: Validation loss did not improve from -0.44003. Patience: 10/50
2025-10-06 04:02:14.866940: train_loss -0.7519
2025-10-06 04:02:14.867207: val_loss -0.4031
2025-10-06 04:02:14.867397: Pseudo dice [np.float32(0.67)]
2025-10-06 04:02:14.867617: Epoch time: 47.03 s
2025-10-06 04:02:15.525397: 
2025-10-06 04:02:15.525805: Epoch 56
2025-10-06 04:02:15.526135: Current learning rate: 0.00657
2025-10-06 04:03:02.574029: Validation loss did not improve from -0.44003. Patience: 11/50
2025-10-06 04:03:02.574865: train_loss -0.7485
2025-10-06 04:03:02.575114: val_loss -0.4256
2025-10-06 04:03:02.575306: Pseudo dice [np.float32(0.6937)]
2025-10-06 04:03:02.575531: Epoch time: 47.05 s
2025-10-06 04:03:02.575748: Yayy! New best EMA pseudo Dice: 0.6753000020980835
2025-10-06 04:03:03.676533: 
2025-10-06 04:03:03.676900: Epoch 57
2025-10-06 04:03:03.677119: Current learning rate: 0.0065
2025-10-06 04:03:50.674096: Validation loss did not improve from -0.44003. Patience: 12/50
2025-10-06 04:03:50.674644: train_loss -0.7509
2025-10-06 04:03:50.674827: val_loss -0.4077
2025-10-06 04:03:50.674947: Pseudo dice [np.float32(0.6808)]
2025-10-06 04:03:50.675145: Epoch time: 47.0 s
2025-10-06 04:03:50.675269: Yayy! New best EMA pseudo Dice: 0.6758000254631042
2025-10-06 04:03:51.801260: 
2025-10-06 04:03:51.801618: Epoch 58
2025-10-06 04:03:51.801823: Current learning rate: 0.00644
2025-10-06 04:04:38.835801: Validation loss did not improve from -0.44003. Patience: 13/50
2025-10-06 04:04:38.836426: train_loss -0.7565
2025-10-06 04:04:38.836592: val_loss -0.3754
2025-10-06 04:04:38.836721: Pseudo dice [np.float32(0.6769)]
2025-10-06 04:04:38.836863: Epoch time: 47.04 s
2025-10-06 04:04:38.837043: Yayy! New best EMA pseudo Dice: 0.6758999824523926
2025-10-06 04:04:40.555580: 
2025-10-06 04:04:40.556046: Epoch 59
2025-10-06 04:04:40.556341: Current learning rate: 0.00638
2025-10-06 04:05:27.633262: Validation loss did not improve from -0.44003. Patience: 14/50
2025-10-06 04:05:27.633847: train_loss -0.7577
2025-10-06 04:05:27.634001: val_loss -0.3967
2025-10-06 04:05:27.634163: Pseudo dice [np.float32(0.6744)]
2025-10-06 04:05:27.634311: Epoch time: 47.08 s
2025-10-06 04:05:28.760561: 
2025-10-06 04:05:28.760846: Epoch 60
2025-10-06 04:05:28.761039: Current learning rate: 0.00631
2025-10-06 04:06:15.800323: Validation loss improved from -0.44003 to -0.44786! Patience: 14/50
2025-10-06 04:06:15.800904: train_loss -0.7639
2025-10-06 04:06:15.801084: val_loss -0.4479
2025-10-06 04:06:15.801279: Pseudo dice [np.float32(0.7084)]
2025-10-06 04:06:15.801504: Epoch time: 47.04 s
2025-10-06 04:06:15.801686: Yayy! New best EMA pseudo Dice: 0.6790000200271606
2025-10-06 04:06:16.920341: 
2025-10-06 04:06:16.920723: Epoch 61
2025-10-06 04:06:16.920911: Current learning rate: 0.00625
2025-10-06 04:07:03.936019: Validation loss did not improve from -0.44786. Patience: 1/50
2025-10-06 04:07:03.936389: train_loss -0.7679
2025-10-06 04:07:03.936525: val_loss -0.3636
2025-10-06 04:07:03.936643: Pseudo dice [np.float32(0.6582)]
2025-10-06 04:07:03.936781: Epoch time: 47.02 s
2025-10-06 04:07:04.607111: 
2025-10-06 04:07:04.607345: Epoch 62
2025-10-06 04:07:04.607514: Current learning rate: 0.00619
2025-10-06 04:07:51.548380: Validation loss did not improve from -0.44786. Patience: 2/50
2025-10-06 04:07:51.549066: train_loss -0.7654
2025-10-06 04:07:51.549242: val_loss -0.4015
2025-10-06 04:07:51.549401: Pseudo dice [np.float32(0.6707)]
2025-10-06 04:07:51.549564: Epoch time: 46.94 s
2025-10-06 04:07:52.207361: 
2025-10-06 04:07:52.207692: Epoch 63
2025-10-06 04:07:52.207879: Current learning rate: 0.00612
2025-10-06 04:08:39.146451: Validation loss did not improve from -0.44786. Patience: 3/50
2025-10-06 04:08:39.147012: train_loss -0.7712
2025-10-06 04:08:39.147163: val_loss -0.3918
2025-10-06 04:08:39.147287: Pseudo dice [np.float32(0.6722)]
2025-10-06 04:08:39.147446: Epoch time: 46.94 s
2025-10-06 04:08:39.823482: 
2025-10-06 04:08:39.823837: Epoch 64
2025-10-06 04:08:39.824048: Current learning rate: 0.00606
2025-10-06 04:09:26.837831: Validation loss did not improve from -0.44786. Patience: 4/50
2025-10-06 04:09:26.838484: train_loss -0.7759
2025-10-06 04:09:26.838679: val_loss -0.3857
2025-10-06 04:09:26.838871: Pseudo dice [np.float32(0.6738)]
2025-10-06 04:09:26.839058: Epoch time: 47.02 s
2025-10-06 04:09:27.935457: 
2025-10-06 04:09:27.935904: Epoch 65
2025-10-06 04:09:27.936187: Current learning rate: 0.006
2025-10-06 04:10:14.954484: Validation loss did not improve from -0.44786. Patience: 5/50
2025-10-06 04:10:14.955212: train_loss -0.7779
2025-10-06 04:10:14.955441: val_loss -0.4022
2025-10-06 04:10:14.955565: Pseudo dice [np.float32(0.6826)]
2025-10-06 04:10:14.955799: Epoch time: 47.02 s
2025-10-06 04:10:15.619217: 
2025-10-06 04:10:15.619588: Epoch 66
2025-10-06 04:10:15.619806: Current learning rate: 0.00593
2025-10-06 04:11:02.575451: Validation loss did not improve from -0.44786. Patience: 6/50
2025-10-06 04:11:02.576171: train_loss -0.7851
2025-10-06 04:11:02.576403: val_loss -0.4022
2025-10-06 04:11:02.576601: Pseudo dice [np.float32(0.6694)]
2025-10-06 04:11:02.576837: Epoch time: 46.96 s
2025-10-06 04:11:03.247653: 
2025-10-06 04:11:03.248009: Epoch 67
2025-10-06 04:11:03.248265: Current learning rate: 0.00587
2025-10-06 04:11:50.115365: Validation loss did not improve from -0.44786. Patience: 7/50
2025-10-06 04:11:50.115849: train_loss -0.7777
2025-10-06 04:11:50.116132: val_loss -0.427
2025-10-06 04:11:50.116363: Pseudo dice [np.float32(0.6882)]
2025-10-06 04:11:50.116582: Epoch time: 46.87 s
2025-10-06 04:11:50.779161: 
2025-10-06 04:11:50.779597: Epoch 68
2025-10-06 04:11:50.779883: Current learning rate: 0.00581
2025-10-06 04:12:37.767563: Validation loss did not improve from -0.44786. Patience: 8/50
2025-10-06 04:12:37.768218: train_loss -0.7836
2025-10-06 04:12:37.768398: val_loss -0.3903
2025-10-06 04:12:37.768545: Pseudo dice [np.float32(0.6768)]
2025-10-06 04:12:37.768675: Epoch time: 46.99 s
2025-10-06 04:12:38.431777: 
2025-10-06 04:12:38.432047: Epoch 69
2025-10-06 04:12:38.432228: Current learning rate: 0.00574
2025-10-06 04:13:25.518553: Validation loss did not improve from -0.44786. Patience: 9/50
2025-10-06 04:13:25.519256: train_loss -0.7901
2025-10-06 04:13:25.519427: val_loss -0.3919
2025-10-06 04:13:25.519543: Pseudo dice [np.float32(0.6787)]
2025-10-06 04:13:25.519670: Epoch time: 47.09 s
2025-10-06 04:13:26.638919: 
2025-10-06 04:13:26.639249: Epoch 70
2025-10-06 04:13:26.639428: Current learning rate: 0.00568
2025-10-06 04:14:13.725019: Validation loss did not improve from -0.44786. Patience: 10/50
2025-10-06 04:14:13.725693: train_loss -0.7893
2025-10-06 04:14:13.726032: val_loss -0.3862
2025-10-06 04:14:13.726231: Pseudo dice [np.float32(0.6857)]
2025-10-06 04:14:13.726439: Epoch time: 47.09 s
2025-10-06 04:14:14.400037: 
2025-10-06 04:14:14.400409: Epoch 71
2025-10-06 04:14:14.400633: Current learning rate: 0.00562
2025-10-06 04:15:01.404861: Validation loss did not improve from -0.44786. Patience: 11/50
2025-10-06 04:15:01.405473: train_loss -0.7914
2025-10-06 04:15:01.405632: val_loss -0.4064
2025-10-06 04:15:01.405784: Pseudo dice [np.float32(0.6861)]
2025-10-06 04:15:01.405913: Epoch time: 47.01 s
2025-10-06 04:15:02.065243: 
2025-10-06 04:15:02.065609: Epoch 72
2025-10-06 04:15:02.065826: Current learning rate: 0.00555
2025-10-06 04:15:49.014087: Validation loss did not improve from -0.44786. Patience: 12/50
2025-10-06 04:15:49.014625: train_loss -0.7924
2025-10-06 04:15:49.014768: val_loss -0.3554
2025-10-06 04:15:49.014882: Pseudo dice [np.float32(0.6673)]
2025-10-06 04:15:49.015011: Epoch time: 46.95 s
2025-10-06 04:15:49.675729: 
2025-10-06 04:15:49.676081: Epoch 73
2025-10-06 04:15:49.676269: Current learning rate: 0.00549
2025-10-06 04:16:36.624983: Validation loss did not improve from -0.44786. Patience: 13/50
2025-10-06 04:16:36.625588: train_loss -0.7919
2025-10-06 04:16:36.625887: val_loss -0.3495
2025-10-06 04:16:36.626126: Pseudo dice [np.float32(0.653)]
2025-10-06 04:16:36.626462: Epoch time: 46.95 s
2025-10-06 04:16:37.853026: 
2025-10-06 04:16:37.853603: Epoch 74
2025-10-06 04:16:37.853885: Current learning rate: 0.00542
2025-10-06 04:17:24.873279: Validation loss did not improve from -0.44786. Patience: 14/50
2025-10-06 04:17:24.873963: train_loss -0.7933
2025-10-06 04:17:24.874126: val_loss -0.3899
2025-10-06 04:17:24.874256: Pseudo dice [np.float32(0.6678)]
2025-10-06 04:17:24.874385: Epoch time: 47.02 s
2025-10-06 04:17:25.991982: 
2025-10-06 04:17:25.992266: Epoch 75
2025-10-06 04:17:25.992518: Current learning rate: 0.00536
2025-10-06 04:18:12.978896: Validation loss did not improve from -0.44786. Patience: 15/50
2025-10-06 04:18:12.979492: train_loss -0.801
2025-10-06 04:18:12.979781: val_loss -0.3997
2025-10-06 04:18:12.980042: Pseudo dice [np.float32(0.6845)]
2025-10-06 04:18:12.980229: Epoch time: 46.99 s
2025-10-06 04:18:13.643878: 
2025-10-06 04:18:13.644407: Epoch 76
2025-10-06 04:18:13.644630: Current learning rate: 0.00529
2025-10-06 04:19:00.590251: Validation loss did not improve from -0.44786. Patience: 16/50
2025-10-06 04:19:00.590729: train_loss -0.8009
2025-10-06 04:19:00.590920: val_loss -0.3644
2025-10-06 04:19:00.591067: Pseudo dice [np.float32(0.6637)]
2025-10-06 04:19:00.591250: Epoch time: 46.95 s
2025-10-06 04:19:01.244277: 
2025-10-06 04:19:01.244597: Epoch 77
2025-10-06 04:19:01.244803: Current learning rate: 0.00523
2025-10-06 04:19:48.159809: Validation loss did not improve from -0.44786. Patience: 17/50
2025-10-06 04:19:48.160308: train_loss -0.7986
2025-10-06 04:19:48.160509: val_loss -0.4007
2025-10-06 04:19:48.160657: Pseudo dice [np.float32(0.6838)]
2025-10-06 04:19:48.160841: Epoch time: 46.92 s
2025-10-06 04:19:48.820203: 
2025-10-06 04:19:48.820480: Epoch 78
2025-10-06 04:19:48.820642: Current learning rate: 0.00517
2025-10-06 04:20:35.803772: Validation loss did not improve from -0.44786. Patience: 18/50
2025-10-06 04:20:35.804362: train_loss -0.8091
2025-10-06 04:20:35.804590: val_loss -0.4232
2025-10-06 04:20:35.804766: Pseudo dice [np.float32(0.7037)]
2025-10-06 04:20:35.804960: Epoch time: 46.98 s
2025-10-06 04:20:36.472804: 
2025-10-06 04:20:36.473123: Epoch 79
2025-10-06 04:20:36.473354: Current learning rate: 0.0051
2025-10-06 04:21:23.484450: Validation loss did not improve from -0.44786. Patience: 19/50
2025-10-06 04:21:23.484948: train_loss -0.8106
2025-10-06 04:21:23.485120: val_loss -0.3726
2025-10-06 04:21:23.485261: Pseudo dice [np.float32(0.6813)]
2025-10-06 04:21:23.485413: Epoch time: 47.01 s
2025-10-06 04:21:24.625102: 
2025-10-06 04:21:24.625327: Epoch 80
2025-10-06 04:21:24.625489: Current learning rate: 0.00504
2025-10-06 04:22:11.643215: Validation loss did not improve from -0.44786. Patience: 20/50
2025-10-06 04:22:11.643932: train_loss -0.8105
2025-10-06 04:22:11.644091: val_loss -0.4029
2025-10-06 04:22:11.644295: Pseudo dice [np.float32(0.6823)]
2025-10-06 04:22:11.644528: Epoch time: 47.02 s
2025-10-06 04:22:12.309963: 
2025-10-06 04:22:12.310333: Epoch 81
2025-10-06 04:22:12.310587: Current learning rate: 0.00497
2025-10-06 04:22:59.205763: Validation loss did not improve from -0.44786. Patience: 21/50
2025-10-06 04:22:59.206278: train_loss -0.8068
2025-10-06 04:22:59.206418: val_loss -0.3778
2025-10-06 04:22:59.206532: Pseudo dice [np.float32(0.6725)]
2025-10-06 04:22:59.206654: Epoch time: 46.9 s
2025-10-06 04:22:59.866969: 
2025-10-06 04:22:59.867301: Epoch 82
2025-10-06 04:22:59.867502: Current learning rate: 0.00491
2025-10-06 04:23:46.810114: Validation loss did not improve from -0.44786. Patience: 22/50
2025-10-06 04:23:46.810789: train_loss -0.8064
2025-10-06 04:23:46.810958: val_loss -0.3976
2025-10-06 04:23:46.811196: Pseudo dice [np.float32(0.6761)]
2025-10-06 04:23:46.811359: Epoch time: 46.94 s
2025-10-06 04:23:47.454951: 
2025-10-06 04:23:47.455235: Epoch 83
2025-10-06 04:23:47.455438: Current learning rate: 0.00484
2025-10-06 04:24:34.434266: Validation loss did not improve from -0.44786. Patience: 23/50
2025-10-06 04:24:34.435012: train_loss -0.8206
2025-10-06 04:24:34.435289: val_loss -0.348
2025-10-06 04:24:34.435452: Pseudo dice [np.float32(0.6581)]
2025-10-06 04:24:34.435625: Epoch time: 46.98 s
2025-10-06 04:24:35.104971: 
2025-10-06 04:24:35.105333: Epoch 84
2025-10-06 04:24:35.105576: Current learning rate: 0.00478
2025-10-06 04:25:22.153308: Validation loss did not improve from -0.44786. Patience: 24/50
2025-10-06 04:25:22.154054: train_loss -0.8155
2025-10-06 04:25:22.154199: val_loss -0.3545
2025-10-06 04:25:22.154317: Pseudo dice [np.float32(0.6381)]
2025-10-06 04:25:22.154508: Epoch time: 47.05 s
2025-10-06 04:25:23.266097: 
2025-10-06 04:25:23.266460: Epoch 85
2025-10-06 04:25:23.266716: Current learning rate: 0.00471
2025-10-06 04:26:10.291375: Validation loss did not improve from -0.44786. Patience: 25/50
2025-10-06 04:26:10.291946: train_loss -0.8167
2025-10-06 04:26:10.292180: val_loss -0.3478
2025-10-06 04:26:10.292455: Pseudo dice [np.float32(0.6687)]
2025-10-06 04:26:10.292686: Epoch time: 47.03 s
2025-10-06 04:26:10.934536: 
2025-10-06 04:26:10.934970: Epoch 86
2025-10-06 04:26:10.935228: Current learning rate: 0.00465
2025-10-06 04:26:57.800788: Validation loss did not improve from -0.44786. Patience: 26/50
2025-10-06 04:26:57.801322: train_loss -0.8184
2025-10-06 04:26:57.801486: val_loss -0.3654
2025-10-06 04:26:57.801617: Pseudo dice [np.float32(0.6751)]
2025-10-06 04:26:57.801764: Epoch time: 46.87 s
2025-10-06 04:26:58.445437: 
2025-10-06 04:26:58.445830: Epoch 87
2025-10-06 04:26:58.446067: Current learning rate: 0.00458
2025-10-06 04:27:45.350528: Validation loss did not improve from -0.44786. Patience: 27/50
2025-10-06 04:27:45.351042: train_loss -0.8175
2025-10-06 04:27:45.351221: val_loss -0.3742
2025-10-06 04:27:45.351367: Pseudo dice [np.float32(0.6831)]
2025-10-06 04:27:45.351569: Epoch time: 46.91 s
2025-10-06 04:27:45.995324: 
2025-10-06 04:27:45.995740: Epoch 88
2025-10-06 04:27:45.995946: Current learning rate: 0.00452
2025-10-06 04:28:32.923745: Validation loss did not improve from -0.44786. Patience: 28/50
2025-10-06 04:28:32.924561: train_loss -0.8212
2025-10-06 04:28:32.924805: val_loss -0.3538
2025-10-06 04:28:32.924998: Pseudo dice [np.float32(0.6646)]
2025-10-06 04:28:32.925199: Epoch time: 46.93 s
2025-10-06 04:28:33.568710: 
2025-10-06 04:28:33.569039: Epoch 89
2025-10-06 04:28:33.569262: Current learning rate: 0.00445
2025-10-06 04:29:20.546751: Validation loss did not improve from -0.44786. Patience: 29/50
2025-10-06 04:29:20.547149: train_loss -0.819
2025-10-06 04:29:20.547381: val_loss -0.3701
2025-10-06 04:29:20.547800: Pseudo dice [np.float32(0.6615)]
2025-10-06 04:29:20.547936: Epoch time: 46.98 s
2025-10-06 04:29:22.235568: 
2025-10-06 04:29:22.236109: Epoch 90
2025-10-06 04:29:22.236569: Current learning rate: 0.00438
2025-10-06 04:30:09.214984: Validation loss did not improve from -0.44786. Patience: 30/50
2025-10-06 04:30:09.215616: train_loss -0.8205
2025-10-06 04:30:09.215766: val_loss -0.3771
2025-10-06 04:30:09.215883: Pseudo dice [np.float32(0.678)]
2025-10-06 04:30:09.216016: Epoch time: 46.98 s
2025-10-06 04:30:09.867537: 
2025-10-06 04:30:09.867826: Epoch 91
2025-10-06 04:30:09.868045: Current learning rate: 0.00432
2025-10-06 04:30:56.738982: Validation loss did not improve from -0.44786. Patience: 31/50
2025-10-06 04:30:56.739509: train_loss -0.823
2025-10-06 04:30:56.739685: val_loss -0.3237
2025-10-06 04:30:56.739836: Pseudo dice [np.float32(0.6501)]
2025-10-06 04:30:56.740005: Epoch time: 46.87 s
2025-10-06 04:30:57.386147: 
2025-10-06 04:30:57.386511: Epoch 92
2025-10-06 04:30:57.386691: Current learning rate: 0.00425
2025-10-06 04:31:44.290365: Validation loss did not improve from -0.44786. Patience: 32/50
2025-10-06 04:31:44.291096: train_loss -0.823
2025-10-06 04:31:44.291245: val_loss -0.3967
2025-10-06 04:31:44.291383: Pseudo dice [np.float32(0.6887)]
2025-10-06 04:31:44.291515: Epoch time: 46.91 s
2025-10-06 04:31:44.937894: 
2025-10-06 04:31:44.938325: Epoch 93
2025-10-06 04:31:44.938542: Current learning rate: 0.00419
2025-10-06 04:32:31.852160: Validation loss did not improve from -0.44786. Patience: 33/50
2025-10-06 04:32:31.852685: train_loss -0.8277
2025-10-06 04:32:31.852927: val_loss -0.3711
2025-10-06 04:32:31.853161: Pseudo dice [np.float32(0.6853)]
2025-10-06 04:32:31.853413: Epoch time: 46.92 s
2025-10-06 04:32:32.501972: 
2025-10-06 04:32:32.502320: Epoch 94
2025-10-06 04:32:32.502582: Current learning rate: 0.00412
2025-10-06 04:33:19.505839: Validation loss did not improve from -0.44786. Patience: 34/50
2025-10-06 04:33:19.506553: train_loss -0.8326
2025-10-06 04:33:19.506715: val_loss -0.4161
2025-10-06 04:33:19.506851: Pseudo dice [np.float32(0.6857)]
2025-10-06 04:33:19.506999: Epoch time: 47.01 s
2025-10-06 04:33:20.618731: 
2025-10-06 04:33:20.619025: Epoch 95
2025-10-06 04:33:20.619232: Current learning rate: 0.00405
2025-10-06 04:34:07.666619: Validation loss did not improve from -0.44786. Patience: 35/50
2025-10-06 04:34:07.667079: train_loss -0.8318
2025-10-06 04:34:07.667336: val_loss -0.3811
2025-10-06 04:34:07.667511: Pseudo dice [np.float32(0.6816)]
2025-10-06 04:34:07.667688: Epoch time: 47.05 s
2025-10-06 04:34:08.315672: 
2025-10-06 04:34:08.316072: Epoch 96
2025-10-06 04:34:08.316288: Current learning rate: 0.00399
2025-10-06 04:34:55.265518: Validation loss did not improve from -0.44786. Patience: 36/50
2025-10-06 04:34:55.266355: train_loss -0.8357
2025-10-06 04:34:55.266593: val_loss -0.3839
2025-10-06 04:34:55.266812: Pseudo dice [np.float32(0.685)]
2025-10-06 04:34:55.267077: Epoch time: 46.95 s
2025-10-06 04:34:55.923412: 
2025-10-06 04:34:55.923748: Epoch 97
2025-10-06 04:34:55.924024: Current learning rate: 0.00392
2025-10-06 04:35:42.866051: Validation loss did not improve from -0.44786. Patience: 37/50
2025-10-06 04:35:42.866564: train_loss -0.8322
2025-10-06 04:35:42.866764: val_loss -0.3915
2025-10-06 04:35:42.866945: Pseudo dice [np.float32(0.6796)]
2025-10-06 04:35:42.867135: Epoch time: 46.94 s
2025-10-06 04:35:43.514961: 
2025-10-06 04:35:43.515288: Epoch 98
2025-10-06 04:35:43.515480: Current learning rate: 0.00385
2025-10-06 04:36:30.516216: Validation loss did not improve from -0.44786. Patience: 38/50
2025-10-06 04:36:30.516880: train_loss -0.8341
2025-10-06 04:36:30.517049: val_loss -0.3852
2025-10-06 04:36:30.517204: Pseudo dice [np.float32(0.6871)]
2025-10-06 04:36:30.517334: Epoch time: 47.0 s
2025-10-06 04:36:31.179723: 
2025-10-06 04:36:31.180000: Epoch 99
2025-10-06 04:36:31.180190: Current learning rate: 0.00379
2025-10-06 04:37:18.164472: Validation loss did not improve from -0.44786. Patience: 39/50
2025-10-06 04:37:18.164970: train_loss -0.835
2025-10-06 04:37:18.165131: val_loss -0.3493
2025-10-06 04:37:18.165249: Pseudo dice [np.float32(0.6545)]
2025-10-06 04:37:18.165380: Epoch time: 46.99 s
2025-10-06 04:37:19.266483: 
2025-10-06 04:37:19.266753: Epoch 100
2025-10-06 04:37:19.266930: Current learning rate: 0.00372
2025-10-06 04:38:06.075568: Validation loss did not improve from -0.44786. Patience: 40/50
2025-10-06 04:38:06.076550: train_loss -0.8406
2025-10-06 04:38:06.076706: val_loss -0.3464
2025-10-06 04:38:06.076902: Pseudo dice [np.float32(0.6721)]
2025-10-06 04:38:06.077123: Epoch time: 46.81 s
2025-10-06 04:38:06.731755: 
2025-10-06 04:38:06.732127: Epoch 101
2025-10-06 04:38:06.732323: Current learning rate: 0.00365
2025-10-06 04:38:53.575594: Validation loss did not improve from -0.44786. Patience: 41/50
2025-10-06 04:38:53.576086: train_loss -0.8446
2025-10-06 04:38:53.576250: val_loss -0.4151
2025-10-06 04:38:53.576465: Pseudo dice [np.float32(0.6915)]
2025-10-06 04:38:53.576615: Epoch time: 46.85 s
2025-10-06 04:38:54.233225: 
2025-10-06 04:38:54.233624: Epoch 102
2025-10-06 04:38:54.233882: Current learning rate: 0.00359
2025-10-06 04:39:41.091400: Validation loss did not improve from -0.44786. Patience: 42/50
2025-10-06 04:39:41.092045: train_loss -0.8369
2025-10-06 04:39:41.092187: val_loss -0.3994
2025-10-06 04:39:41.092321: Pseudo dice [np.float32(0.6897)]
2025-10-06 04:39:41.092458: Epoch time: 46.86 s
2025-10-06 04:39:41.740668: 
2025-10-06 04:39:41.740958: Epoch 103
2025-10-06 04:39:41.741131: Current learning rate: 0.00352
2025-10-06 04:40:28.598503: Validation loss did not improve from -0.44786. Patience: 43/50
2025-10-06 04:40:28.599008: train_loss -0.8387
2025-10-06 04:40:28.599263: val_loss -0.3672
2025-10-06 04:40:28.599473: Pseudo dice [np.float32(0.6844)]
2025-10-06 04:40:28.599701: Epoch time: 46.86 s
2025-10-06 04:40:29.253839: 
2025-10-06 04:40:29.254225: Epoch 104
2025-10-06 04:40:29.254408: Current learning rate: 0.00345
2025-10-06 04:41:16.157164: Validation loss did not improve from -0.44786. Patience: 44/50
2025-10-06 04:41:16.157832: train_loss -0.8456
2025-10-06 04:41:16.158050: val_loss -0.4246
2025-10-06 04:41:16.158213: Pseudo dice [np.float32(0.7058)]
2025-10-06 04:41:16.158391: Epoch time: 46.9 s
2025-10-06 04:41:16.602808: Yayy! New best EMA pseudo Dice: 0.6812000274658203
2025-10-06 04:41:18.291792: 
2025-10-06 04:41:18.292209: Epoch 105
2025-10-06 04:41:18.292421: Current learning rate: 0.00338
2025-10-06 04:42:05.235666: Validation loss did not improve from -0.44786. Patience: 45/50
2025-10-06 04:42:05.236226: train_loss -0.8435
2025-10-06 04:42:05.236485: val_loss -0.3706
2025-10-06 04:42:05.236707: Pseudo dice [np.float32(0.6751)]
2025-10-06 04:42:05.236938: Epoch time: 46.95 s
2025-10-06 04:42:05.897727: 
2025-10-06 04:42:05.898179: Epoch 106
2025-10-06 04:42:05.898435: Current learning rate: 0.00332
2025-10-06 04:42:52.858127: Validation loss did not improve from -0.44786. Patience: 46/50
2025-10-06 04:42:52.858754: train_loss -0.8466
2025-10-06 04:42:52.858952: val_loss -0.3753
2025-10-06 04:42:52.859100: Pseudo dice [np.float32(0.6742)]
2025-10-06 04:42:52.859271: Epoch time: 46.96 s
2025-10-06 04:42:53.511950: 
2025-10-06 04:42:53.512252: Epoch 107
2025-10-06 04:42:53.512477: Current learning rate: 0.00325
2025-10-06 04:43:40.473917: Validation loss did not improve from -0.44786. Patience: 47/50
2025-10-06 04:43:40.474420: train_loss -0.8455
2025-10-06 04:43:40.474590: val_loss -0.3627
2025-10-06 04:43:40.474718: Pseudo dice [np.float32(0.6684)]
2025-10-06 04:43:40.474907: Epoch time: 46.96 s
2025-10-06 04:43:41.128351: 
2025-10-06 04:43:41.128604: Epoch 108
2025-10-06 04:43:41.128785: Current learning rate: 0.00318
2025-10-06 04:44:28.086553: Validation loss did not improve from -0.44786. Patience: 48/50
2025-10-06 04:44:28.087318: train_loss -0.8445
2025-10-06 04:44:28.087562: val_loss -0.3584
2025-10-06 04:44:28.088010: Pseudo dice [np.float32(0.6885)]
2025-10-06 04:44:28.088220: Epoch time: 46.96 s
2025-10-06 04:44:28.759879: 
2025-10-06 04:44:28.760162: Epoch 109
2025-10-06 04:44:28.760341: Current learning rate: 0.00311
2025-10-06 04:45:15.716938: Validation loss did not improve from -0.44786. Patience: 49/50
2025-10-06 04:45:15.717465: train_loss -0.8468
2025-10-06 04:45:15.717635: val_loss -0.3463
2025-10-06 04:45:15.717755: Pseudo dice [np.float32(0.6581)]
2025-10-06 04:45:15.717891: Epoch time: 46.96 s
2025-10-06 04:45:16.834690: 
2025-10-06 04:45:16.835030: Epoch 110
2025-10-06 04:45:16.835202: Current learning rate: 0.00304
2025-10-06 04:46:03.816895: Validation loss did not improve from -0.44786. Patience: 50/50
2025-10-06 04:46:03.817475: train_loss -0.8482
2025-10-06 04:46:03.817632: val_loss -0.34
2025-10-06 04:46:03.817781: Pseudo dice [np.float32(0.6751)]
2025-10-06 04:46:03.817903: Epoch time: 46.98 s
2025-10-06 04:46:04.483757: 
2025-10-06 04:46:04.484051: Epoch 111
2025-10-06 04:46:04.484267: Current learning rate: 0.00297
2025-10-06 04:46:51.458241: Validation loss did not improve from -0.44786. Patience: 51/50
2025-10-06 04:46:51.458665: train_loss -0.8489
2025-10-06 04:46:51.458863: val_loss -0.3253
2025-10-06 04:46:51.459028: Pseudo dice [np.float32(0.6754)]
2025-10-06 04:46:51.459190: Epoch time: 46.98 s
2025-10-06 04:46:52.111307: 
2025-10-06 04:46:52.111642: Epoch 112
2025-10-06 04:46:52.111804: Current learning rate: 0.00291
2025-10-06 04:47:39.111584: Validation loss did not improve from -0.44786. Patience: 52/50
2025-10-06 04:47:39.112246: train_loss -0.8506
2025-10-06 04:47:39.112426: val_loss -0.3481
2025-10-06 04:47:39.112572: Pseudo dice [np.float32(0.6693)]
2025-10-06 04:47:39.112733: Epoch time: 47.0 s
2025-10-06 04:47:39.764676: 
2025-10-06 04:47:39.765047: Epoch 113
2025-10-06 04:47:39.765286: Current learning rate: 0.00284
2025-10-06 04:48:26.794933: Validation loss did not improve from -0.44786. Patience: 53/50
2025-10-06 04:48:26.795675: train_loss -0.8523
2025-10-06 04:48:26.796013: val_loss -0.3453
2025-10-06 04:48:26.796235: Pseudo dice [np.float32(0.6697)]
2025-10-06 04:48:26.796514: Epoch time: 47.03 s
2025-10-06 04:48:27.458277: 
2025-10-06 04:48:27.458613: Epoch 114
2025-10-06 04:48:27.458854: Current learning rate: 0.00277
2025-10-06 04:49:14.476197: Validation loss did not improve from -0.44786. Patience: 54/50
2025-10-06 04:49:14.476777: train_loss -0.856
2025-10-06 04:49:14.476955: val_loss -0.3443
2025-10-06 04:49:14.477090: Pseudo dice [np.float32(0.6633)]
2025-10-06 04:49:14.477221: Epoch time: 47.02 s
2025-10-06 04:49:15.601240: 
2025-10-06 04:49:15.601607: Epoch 115
2025-10-06 04:49:15.601891: Current learning rate: 0.0027
2025-10-06 04:50:02.679388: Validation loss did not improve from -0.44786. Patience: 55/50
2025-10-06 04:50:02.679795: train_loss -0.8561
2025-10-06 04:50:02.679952: val_loss -0.3659
2025-10-06 04:50:02.680092: Pseudo dice [np.float32(0.6763)]
2025-10-06 04:50:02.680235: Epoch time: 47.08 s
2025-10-06 04:50:03.336126: 
2025-10-06 04:50:03.336344: Epoch 116
2025-10-06 04:50:03.336506: Current learning rate: 0.00263
2025-10-06 04:50:50.312751: Validation loss did not improve from -0.44786. Patience: 56/50
2025-10-06 04:50:50.313364: train_loss -0.8578
2025-10-06 04:50:50.313572: val_loss -0.3596
2025-10-06 04:50:50.313755: Pseudo dice [np.float32(0.6792)]
2025-10-06 04:50:50.314002: Epoch time: 46.98 s
2025-10-06 04:50:50.977420: 
2025-10-06 04:50:50.977865: Epoch 117
2025-10-06 04:50:50.978126: Current learning rate: 0.00256
2025-10-06 04:51:37.955885: Validation loss did not improve from -0.44786. Patience: 57/50
2025-10-06 04:51:37.956329: train_loss -0.8543
2025-10-06 04:51:37.956515: val_loss -0.372
2025-10-06 04:51:37.956632: Pseudo dice [np.float32(0.6925)]
2025-10-06 04:51:37.956790: Epoch time: 46.98 s
2025-10-06 04:51:38.621644: 
2025-10-06 04:51:38.622057: Epoch 118
2025-10-06 04:51:38.622232: Current learning rate: 0.00249
2025-10-06 04:52:25.589696: Validation loss did not improve from -0.44786. Patience: 58/50
2025-10-06 04:52:25.590318: train_loss -0.8586
2025-10-06 04:52:25.590457: val_loss -0.3608
2025-10-06 04:52:25.590569: Pseudo dice [np.float32(0.6676)]
2025-10-06 04:52:25.590698: Epoch time: 46.97 s
2025-10-06 04:52:26.256006: 
2025-10-06 04:52:26.256392: Epoch 119
2025-10-06 04:52:26.256595: Current learning rate: 0.00242
2025-10-06 04:53:13.286696: Validation loss did not improve from -0.44786. Patience: 59/50
2025-10-06 04:53:13.287152: train_loss -0.8612
2025-10-06 04:53:13.287320: val_loss -0.3458
2025-10-06 04:53:13.287457: Pseudo dice [np.float32(0.6837)]
2025-10-06 04:53:13.287612: Epoch time: 47.03 s
2025-10-06 04:53:14.392026: 
2025-10-06 04:53:14.392372: Epoch 120
2025-10-06 04:53:14.392562: Current learning rate: 0.00235
2025-10-06 04:54:01.449401: Validation loss did not improve from -0.44786. Patience: 60/50
2025-10-06 04:54:01.449845: train_loss -0.8582
2025-10-06 04:54:01.449984: val_loss -0.3675
2025-10-06 04:54:01.450094: Pseudo dice [np.float32(0.6723)]
2025-10-06 04:54:01.450231: Epoch time: 47.06 s
2025-10-06 04:54:02.690045: 
2025-10-06 04:54:02.690382: Epoch 121
2025-10-06 04:54:02.690555: Current learning rate: 0.00228
2025-10-06 04:54:49.607063: Validation loss did not improve from -0.44786. Patience: 61/50
2025-10-06 04:54:49.607535: train_loss -0.8602
2025-10-06 04:54:49.607681: val_loss -0.378
2025-10-06 04:54:49.607868: Pseudo dice [np.float32(0.7021)]
2025-10-06 04:54:49.608040: Epoch time: 46.92 s
2025-10-06 04:54:50.270607: 
2025-10-06 04:54:50.270973: Epoch 122
2025-10-06 04:54:50.271165: Current learning rate: 0.00221
2025-10-06 04:55:37.191135: Validation loss did not improve from -0.44786. Patience: 62/50
2025-10-06 04:55:37.191543: train_loss -0.8627
2025-10-06 04:55:37.191745: val_loss -0.3874
2025-10-06 04:55:37.191890: Pseudo dice [np.float32(0.7007)]
2025-10-06 04:55:37.192052: Epoch time: 46.92 s
2025-10-06 04:55:37.848267: 
2025-10-06 04:55:37.848625: Epoch 123
2025-10-06 04:55:37.848817: Current learning rate: 0.00214
2025-10-06 04:56:24.795274: Validation loss did not improve from -0.44786. Patience: 63/50
2025-10-06 04:56:24.795721: train_loss -0.8636
2025-10-06 04:56:24.795945: val_loss -0.3708
2025-10-06 04:56:24.796082: Pseudo dice [np.float32(0.6872)]
2025-10-06 04:56:24.796233: Epoch time: 46.95 s
2025-10-06 04:56:24.796386: Yayy! New best EMA pseudo Dice: 0.6815999746322632
2025-10-06 04:56:25.931753: 
2025-10-06 04:56:25.932141: Epoch 124
2025-10-06 04:56:25.932387: Current learning rate: 0.00207
2025-10-06 04:57:12.930490: Validation loss did not improve from -0.44786. Patience: 64/50
2025-10-06 04:57:12.931050: train_loss -0.8632
2025-10-06 04:57:12.931230: val_loss -0.3125
2025-10-06 04:57:12.931377: Pseudo dice [np.float32(0.6561)]
2025-10-06 04:57:12.931534: Epoch time: 47.0 s
2025-10-06 04:57:14.059868: 
2025-10-06 04:57:14.060301: Epoch 125
2025-10-06 04:57:14.060580: Current learning rate: 0.00199
2025-10-06 04:58:01.074988: Validation loss did not improve from -0.44786. Patience: 65/50
2025-10-06 04:58:01.075452: train_loss -0.869
2025-10-06 04:58:01.075626: val_loss -0.3464
2025-10-06 04:58:01.075767: Pseudo dice [np.float32(0.6812)]
2025-10-06 04:58:01.075911: Epoch time: 47.02 s
2025-10-06 04:58:01.747541: 
2025-10-06 04:58:01.747930: Epoch 126
2025-10-06 04:58:01.748149: Current learning rate: 0.00192
2025-10-06 04:58:48.619851: Validation loss did not improve from -0.44786. Patience: 66/50
2025-10-06 04:58:48.620316: train_loss -0.8651
2025-10-06 04:58:48.620461: val_loss -0.3239
2025-10-06 04:58:48.620575: Pseudo dice [np.float32(0.6646)]
2025-10-06 04:58:48.620718: Epoch time: 46.87 s
2025-10-06 04:58:49.287517: 
2025-10-06 04:58:49.287796: Epoch 127
2025-10-06 04:58:49.288001: Current learning rate: 0.00185
2025-10-06 04:59:36.184291: Validation loss did not improve from -0.44786. Patience: 67/50
2025-10-06 04:59:36.184808: train_loss -0.8633
2025-10-06 04:59:36.185003: val_loss -0.3672
2025-10-06 04:59:36.185210: Pseudo dice [np.float32(0.6754)]
2025-10-06 04:59:36.185411: Epoch time: 46.9 s
2025-10-06 04:59:36.861294: 
2025-10-06 04:59:36.861615: Epoch 128
2025-10-06 04:59:36.861909: Current learning rate: 0.00178
2025-10-06 05:00:23.843915: Validation loss did not improve from -0.44786. Patience: 68/50
2025-10-06 05:00:23.844688: train_loss -0.8678
2025-10-06 05:00:23.844964: val_loss -0.3767
2025-10-06 05:00:23.845188: Pseudo dice [np.float32(0.6881)]
2025-10-06 05:00:23.845436: Epoch time: 46.98 s
2025-10-06 05:00:24.522292: 
2025-10-06 05:00:24.522639: Epoch 129
2025-10-06 05:00:24.522886: Current learning rate: 0.0017
2025-10-06 05:01:11.517302: Validation loss did not improve from -0.44786. Patience: 69/50
2025-10-06 05:01:11.517742: train_loss -0.8703
2025-10-06 05:01:11.517909: val_loss -0.3644
2025-10-06 05:01:11.518022: Pseudo dice [np.float32(0.6886)]
2025-10-06 05:01:11.518173: Epoch time: 47.0 s
2025-10-06 05:01:12.677829: 
2025-10-06 05:01:12.678131: Epoch 130
2025-10-06 05:01:12.678363: Current learning rate: 0.00163
2025-10-06 05:01:59.703655: Validation loss did not improve from -0.44786. Patience: 70/50
2025-10-06 05:01:59.704245: train_loss -0.8685
2025-10-06 05:01:59.704525: val_loss -0.3358
2025-10-06 05:01:59.704679: Pseudo dice [np.float32(0.6634)]
2025-10-06 05:01:59.704846: Epoch time: 47.03 s
2025-10-06 05:02:00.358787: 
2025-10-06 05:02:00.359076: Epoch 131
2025-10-06 05:02:00.359272: Current learning rate: 0.00156
2025-10-06 05:02:47.286070: Validation loss did not improve from -0.44786. Patience: 71/50
2025-10-06 05:02:47.286542: train_loss -0.8692
2025-10-06 05:02:47.286689: val_loss -0.3717
2025-10-06 05:02:47.286848: Pseudo dice [np.float32(0.6793)]
2025-10-06 05:02:47.286998: Epoch time: 46.93 s
2025-10-06 05:02:47.949162: 
2025-10-06 05:02:47.949527: Epoch 132
2025-10-06 05:02:47.949750: Current learning rate: 0.00148
2025-10-06 05:03:34.893136: Validation loss did not improve from -0.44786. Patience: 72/50
2025-10-06 05:03:34.893707: train_loss -0.8691
2025-10-06 05:03:34.893905: val_loss -0.3435
2025-10-06 05:03:34.894123: Pseudo dice [np.float32(0.6718)]
2025-10-06 05:03:34.894301: Epoch time: 46.95 s
2025-10-06 05:03:35.549924: 
2025-10-06 05:03:35.550171: Epoch 133
2025-10-06 05:03:35.550366: Current learning rate: 0.00141
2025-10-06 05:04:22.490526: Validation loss did not improve from -0.44786. Patience: 73/50
2025-10-06 05:04:22.491108: train_loss -0.8729
2025-10-06 05:04:22.491352: val_loss -0.3277
2025-10-06 05:04:22.491552: Pseudo dice [np.float32(0.6614)]
2025-10-06 05:04:22.491813: Epoch time: 46.94 s
2025-10-06 05:04:23.149610: 
2025-10-06 05:04:23.150051: Epoch 134
2025-10-06 05:04:23.150316: Current learning rate: 0.00133
2025-10-06 05:05:10.148748: Validation loss did not improve from -0.44786. Patience: 74/50
2025-10-06 05:05:10.149222: train_loss -0.8755
2025-10-06 05:05:10.149359: val_loss -0.3725
2025-10-06 05:05:10.149503: Pseudo dice [np.float32(0.6927)]
2025-10-06 05:05:10.149629: Epoch time: 47.0 s
2025-10-06 05:05:11.188395: 
2025-10-06 05:05:11.188723: Epoch 135
2025-10-06 05:05:11.188909: Current learning rate: 0.00126
2025-10-06 05:05:58.147621: Validation loss did not improve from -0.44786. Patience: 75/50
2025-10-06 05:05:58.148052: train_loss -0.8686
2025-10-06 05:05:58.148215: val_loss -0.3235
2025-10-06 05:05:58.148340: Pseudo dice [np.float32(0.6848)]
2025-10-06 05:05:58.148478: Epoch time: 46.96 s
2025-10-06 05:05:59.364309: 
2025-10-06 05:05:59.364727: Epoch 136
2025-10-06 05:05:59.364917: Current learning rate: 0.00118
2025-10-06 05:06:46.343434: Validation loss did not improve from -0.44786. Patience: 76/50
2025-10-06 05:06:46.344280: train_loss -0.8754
2025-10-06 05:06:46.344644: val_loss -0.3395
2025-10-06 05:06:46.344893: Pseudo dice [np.float32(0.6809)]
2025-10-06 05:06:46.345141: Epoch time: 46.98 s
2025-10-06 05:06:47.017300: 
2025-10-06 05:06:47.017871: Epoch 137
2025-10-06 05:06:47.018262: Current learning rate: 0.00111
2025-10-06 05:07:33.997760: Validation loss did not improve from -0.44786. Patience: 77/50
2025-10-06 05:07:33.998265: train_loss -0.8745
2025-10-06 05:07:33.998455: val_loss -0.3268
2025-10-06 05:07:33.998576: Pseudo dice [np.float32(0.6757)]
2025-10-06 05:07:33.998749: Epoch time: 46.98 s
2025-10-06 05:07:34.661869: 
2025-10-06 05:07:34.662225: Epoch 138
2025-10-06 05:07:34.662495: Current learning rate: 0.00103
2025-10-06 05:08:21.666430: Validation loss did not improve from -0.44786. Patience: 78/50
2025-10-06 05:08:21.666998: train_loss -0.8708
2025-10-06 05:08:21.667155: val_loss -0.3025
2025-10-06 05:08:21.667295: Pseudo dice [np.float32(0.6649)]
2025-10-06 05:08:21.667430: Epoch time: 47.01 s
2025-10-06 05:08:22.333387: 
2025-10-06 05:08:22.333784: Epoch 139
2025-10-06 05:08:22.334010: Current learning rate: 0.00095
2025-10-06 05:09:09.390481: Validation loss did not improve from -0.44786. Patience: 79/50
2025-10-06 05:09:09.391014: train_loss -0.8714
2025-10-06 05:09:09.391323: val_loss -0.3123
2025-10-06 05:09:09.391522: Pseudo dice [np.float32(0.6711)]
2025-10-06 05:09:09.391809: Epoch time: 47.06 s
2025-10-06 05:09:10.511441: 
2025-10-06 05:09:10.511775: Epoch 140
2025-10-06 05:09:10.511946: Current learning rate: 0.00087
2025-10-06 05:09:57.370795: Validation loss did not improve from -0.44786. Patience: 80/50
2025-10-06 05:09:57.371382: train_loss -0.8738
2025-10-06 05:09:57.371537: val_loss -0.3463
2025-10-06 05:09:57.371652: Pseudo dice [np.float32(0.6835)]
2025-10-06 05:09:57.371789: Epoch time: 46.86 s
2025-10-06 05:09:58.037182: 
2025-10-06 05:09:58.037586: Epoch 141
2025-10-06 05:09:58.037802: Current learning rate: 0.00079
2025-10-06 05:10:45.020555: Validation loss did not improve from -0.44786. Patience: 81/50
2025-10-06 05:10:45.021045: train_loss -0.8758
2025-10-06 05:10:45.021233: val_loss -0.38
2025-10-06 05:10:45.021420: Pseudo dice [np.float32(0.6883)]
2025-10-06 05:10:45.021627: Epoch time: 46.98 s
2025-10-06 05:10:45.688312: 
2025-10-06 05:10:45.688700: Epoch 142
2025-10-06 05:10:45.688915: Current learning rate: 0.00071
2025-10-06 05:11:32.715397: Validation loss did not improve from -0.44786. Patience: 82/50
2025-10-06 05:11:32.715900: train_loss -0.8733
2025-10-06 05:11:32.716121: val_loss -0.3534
2025-10-06 05:11:32.716268: Pseudo dice [np.float32(0.6877)]
2025-10-06 05:11:32.716419: Epoch time: 47.03 s
2025-10-06 05:11:33.378554: 
2025-10-06 05:11:33.378858: Epoch 143
2025-10-06 05:11:33.379075: Current learning rate: 0.00063
2025-10-06 05:12:20.420918: Validation loss did not improve from -0.44786. Patience: 83/50
2025-10-06 05:12:20.421434: train_loss -0.8768
2025-10-06 05:12:20.421588: val_loss -0.3229
2025-10-06 05:12:20.421754: Pseudo dice [np.float32(0.6671)]
2025-10-06 05:12:20.421916: Epoch time: 47.04 s
2025-10-06 05:12:21.094131: 
2025-10-06 05:12:21.094506: Epoch 144
2025-10-06 05:12:21.094719: Current learning rate: 0.00055
2025-10-06 05:13:08.121690: Validation loss did not improve from -0.44786. Patience: 84/50
2025-10-06 05:13:08.122217: train_loss -0.878
2025-10-06 05:13:08.122437: val_loss -0.351
2025-10-06 05:13:08.122585: Pseudo dice [np.float32(0.6763)]
2025-10-06 05:13:08.122752: Epoch time: 47.03 s
2025-10-06 05:13:09.243058: 
2025-10-06 05:13:09.243297: Epoch 145
2025-10-06 05:13:09.243494: Current learning rate: 0.00047
2025-10-06 05:13:56.233584: Validation loss did not improve from -0.44786. Patience: 85/50
2025-10-06 05:13:56.234076: train_loss -0.8783
2025-10-06 05:13:56.234289: val_loss -0.3567
2025-10-06 05:13:56.234460: Pseudo dice [np.float32(0.694)]
2025-10-06 05:13:56.234629: Epoch time: 46.99 s
2025-10-06 05:13:56.901755: 
2025-10-06 05:13:56.902105: Epoch 146
2025-10-06 05:13:56.902314: Current learning rate: 0.00038
2025-10-06 05:14:43.918144: Validation loss did not improve from -0.44786. Patience: 86/50
2025-10-06 05:14:43.918852: train_loss -0.8774
2025-10-06 05:14:43.919152: val_loss -0.374
2025-10-06 05:14:43.919371: Pseudo dice [np.float32(0.6826)]
2025-10-06 05:14:43.919562: Epoch time: 47.02 s
2025-10-06 05:14:44.600859: 
2025-10-06 05:14:44.601235: Epoch 147
2025-10-06 05:14:44.601438: Current learning rate: 0.0003
2025-10-06 05:15:31.688254: Validation loss did not improve from -0.44786. Patience: 87/50
2025-10-06 05:15:31.688985: train_loss -0.8817
2025-10-06 05:15:31.689321: val_loss -0.3726
2025-10-06 05:15:31.689538: Pseudo dice [np.float32(0.6833)]
2025-10-06 05:15:31.689842: Epoch time: 47.09 s
2025-10-06 05:15:32.350407: 
2025-10-06 05:15:32.350737: Epoch 148
2025-10-06 05:15:32.350906: Current learning rate: 0.00021
2025-10-06 05:16:19.369278: Validation loss did not improve from -0.44786. Patience: 88/50
2025-10-06 05:16:19.369869: train_loss -0.8792
2025-10-06 05:16:19.370027: val_loss -0.3252
2025-10-06 05:16:19.370235: Pseudo dice [np.float32(0.6726)]
2025-10-06 05:16:19.370382: Epoch time: 47.02 s
2025-10-06 05:16:20.030508: 
2025-10-06 05:16:20.030867: Epoch 149
2025-10-06 05:16:20.031079: Current learning rate: 0.00011
2025-10-06 05:17:06.977391: Validation loss did not improve from -0.44786. Patience: 89/50
2025-10-06 05:17:06.977893: train_loss -0.8784
2025-10-06 05:17:06.978081: val_loss -0.3504
2025-10-06 05:17:06.978228: Pseudo dice [np.float32(0.6809)]
2025-10-06 05:17:06.978383: Epoch time: 46.95 s
2025-10-06 05:17:08.103068: Training done.
2025-10-06 05:17:08.149910: Using splits from existing split file: /nfs/erelab001/shared/Computational_Group/Naravich/datasets/nnUNet_Datasets/nnUNet_preprocessed/Dataset310_nnInteractive_Calcium_OCT_CrossValidation/splits_final_80.json
2025-10-06 05:17:08.150517: The split file contains 5 splits.
2025-10-06 05:17:08.150828: Desired fold for training: 4
2025-10-06 05:17:08.151105: This split has 6 training and 5 validation cases.
2025-10-06 05:17:08.151422: predicting 101-044
2025-10-06 05:17:08.155274: 101-044, shape torch.Size([1, 404, 498, 498]), rank 0
2025-10-06 05:17:49.521364: predicting 401-004
2025-10-06 05:17:49.534769: 401-004, shape torch.Size([1, 375, 498, 498]), rank 0
2025-10-06 05:18:24.611272: predicting 701-013
2025-10-06 05:18:24.625542: 701-013, shape torch.Size([1, 375, 498, 498]), rank 0
2025-10-06 05:18:59.613549: predicting 704-003
2025-10-06 05:18:59.627178: 704-003, shape torch.Size([1, 375, 498, 498]), rank 0
2025-10-06 05:19:34.592485: predicting 706-005
2025-10-06 05:19:34.606544: 706-005, shape torch.Size([1, 375, 498, 498]), rank 0
2025-10-06 05:20:23.166923: Validation complete
2025-10-06 05:20:23.167177: Mean Validation Dice:  0.6627079082122835
Finished training fold 4 saving to /nfs/erelab001/shared/Computational_Group/Naravich/datasets/nnUNet_Datasets/nnUNet_results/Dataset310_nnInteractive_Calcium_OCT_CrossValidation/nnUNetTrainerScaleAnalysis80__nnUNetPlans__3d_32x160x128_b10/fold_4_No_Pretrained
