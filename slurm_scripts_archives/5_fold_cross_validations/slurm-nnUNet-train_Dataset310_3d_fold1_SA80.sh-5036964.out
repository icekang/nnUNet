/nfs/erelab001/shared/Computational_Group/Naravich/nnUNet/.venv/bin/python
Starting training with CONFIG=3d_32x160x128_b10, DATASET_ID=310, TRAINER=nnUNetTrainerScaleAnalysis80
/nfs/erelab001/shared/Computational_Group/Naravich/nnUNet/nnunetv2/training/nnUNetTrainer/nnUNetTrainer.py:169: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  self.grad_scaler = GradScaler() if self.device.type == 'cuda' else None

############################
INFO: You are using the old nnU-Net default plans. We have updated our recommendations. Please consider using those instead! Read more here: https://github.com/MIC-DKFZ/nnUNet/blob/master/documentation/resenc_presets.md
############################

Using device: cuda:0

#######################################################################
Please cite the following paper when using nnU-Net:
Isensee, F., Jaeger, P. F., Kohl, S. A., Petersen, J., & Maier-Hein, K. H. (2021). nnU-Net: a self-configuring method for deep learning-based biomedical image segmentation. Nature methods, 18(2), 203-211.
#######################################################################

2025-10-05 14:52:57.070546: do_dummy_2d_data_aug: True
2025-10-05 14:52:57.070944: Using splits from existing split file: /nfs/erelab001/shared/Computational_Group/Naravich/datasets/nnUNet_Datasets/nnUNet_preprocessed/Dataset310_nnInteractive_Calcium_OCT_CrossValidation/splits_final_80.json
2025-10-05 14:52:57.071211: The split file contains 5 splits.
2025-10-05 14:52:57.071366: Desired fold for training: 1
2025-10-05 14:52:57.071495: This split has 6 training and 4 validation cases.
/nfs/erelab001/shared/Computational_Group/Naravich/nnUNet/.venv/lib/python3.12/site-packages/fft_conv_pytorch/fft_conv.py:139: UserWarning: Using a non-tuple sequence for multidimensional indexing is deprecated and will be changed in pytorch 2.9; use x[tuple(seq)] instead of x[seq]. In pytorch 2.9 this will be interpreted as tensor index, x[torch.tensor(seq)], which will result either in an error or a different result (Triggered internally at /pytorch/torch/csrc/autograd/python_variable_indexing.cpp:306.)
  output = output[crop_slices].contiguous()
/nfs/erelab001/shared/Computational_Group/Naravich/nnUNet/.venv/lib/python3.12/site-packages/fft_conv_pytorch/fft_conv.py:139: UserWarning: Using a non-tuple sequence for multidimensional indexing is deprecated and will be changed in pytorch 2.9; use x[tuple(seq)] instead of x[seq]. In pytorch 2.9 this will be interpreted as tensor index, x[torch.tensor(seq)], which will result either in an error or a different result (Triggered internally at /pytorch/torch/csrc/autograd/python_variable_indexing.cpp:306.)
  output = output[crop_slices].contiguous()
/nfs/erelab001/shared/Computational_Group/Naravich/nnUNet/.venv/lib/python3.12/site-packages/fft_conv_pytorch/fft_conv.py:139: UserWarning: Using a non-tuple sequence for multidimensional indexing is deprecated and will be changed in pytorch 2.9; use x[tuple(seq)] instead of x[seq]. In pytorch 2.9 this will be interpreted as tensor index, x[torch.tensor(seq)], which will result either in an error or a different result (Triggered internally at /pytorch/torch/csrc/autograd/python_variable_indexing.cpp:306.)
  output = output[crop_slices].contiguous()
/nfs/erelab001/shared/Computational_Group/Naravich/nnUNet/.venv/lib/python3.12/site-packages/fft_conv_pytorch/fft_conv.py:139: UserWarning: Using a non-tuple sequence for multidimensional indexing is deprecated and will be changed in pytorch 2.9; use x[tuple(seq)] instead of x[seq]. In pytorch 2.9 this will be interpreted as tensor index, x[torch.tensor(seq)], which will result either in an error or a different result (Triggered internally at /pytorch/torch/csrc/autograd/python_variable_indexing.cpp:306.)
  output = output[crop_slices].contiguous()
/nfs/erelab001/shared/Computational_Group/Naravich/nnUNet/.venv/lib/python3.12/site-packages/fft_conv_pytorch/fft_conv.py:139: UserWarning: Using a non-tuple sequence for multidimensional indexing is deprecated and will be changed in pytorch 2.9; use x[tuple(seq)] instead of x[seq]. In pytorch 2.9 this will be interpreted as tensor index, x[torch.tensor(seq)], which will result either in an error or a different result (Triggered internally at /pytorch/torch/csrc/autograd/python_variable_indexing.cpp:306.)
  output = output[crop_slices].contiguous()
/nfs/erelab001/shared/Computational_Group/Naravich/nnUNet/.venv/lib/python3.12/site-packages/fft_conv_pytorch/fft_conv.py:139: UserWarning: Using a non-tuple sequence for multidimensional indexing is deprecated and will be changed in pytorch 2.9; use x[tuple(seq)] instead of x[seq]. In pytorch 2.9 this will be interpreted as tensor index, x[torch.tensor(seq)], which will result either in an error or a different result (Triggered internally at /pytorch/torch/csrc/autograd/python_variable_indexing.cpp:306.)
  output = output[crop_slices].contiguous()
/nfs/erelab001/shared/Computational_Group/Naravich/nnUNet/.venv/lib/python3.12/site-packages/fft_conv_pytorch/fft_conv.py:139: UserWarning: Using a non-tuple sequence for multidimensional indexing is deprecated and will be changed in pytorch 2.9; use x[tuple(seq)] instead of x[seq]. In pytorch 2.9 this will be interpreted as tensor index, x[torch.tensor(seq)], which will result either in an error or a different result (Triggered internally at /pytorch/torch/csrc/autograd/python_variable_indexing.cpp:306.)
  output = output[crop_slices].contiguous()
/nfs/erelab001/shared/Computational_Group/Naravich/nnUNet/.venv/lib/python3.12/site-packages/fft_conv_pytorch/fft_conv.py:139: UserWarning: Using a non-tuple sequence for multidimensional indexing is deprecated and will be changed in pytorch 2.9; use x[tuple(seq)] instead of x[seq]. In pytorch 2.9 this will be interpreted as tensor index, x[torch.tensor(seq)], which will result either in an error or a different result (Triggered internally at /pytorch/torch/csrc/autograd/python_variable_indexing.cpp:306.)
  output = output[crop_slices].contiguous()
/nfs/erelab001/shared/Computational_Group/Naravich/nnUNet/.venv/lib/python3.12/site-packages/fft_conv_pytorch/fft_conv.py:139: UserWarning: Using a non-tuple sequence for multidimensional indexing is deprecated and will be changed in pytorch 2.9; use x[tuple(seq)] instead of x[seq]. In pytorch 2.9 this will be interpreted as tensor index, x[torch.tensor(seq)], which will result either in an error or a different result (Triggered internally at /pytorch/torch/csrc/autograd/python_variable_indexing.cpp:306.)
  output = output[crop_slices].contiguous()
/nfs/erelab001/shared/Computational_Group/Naravich/nnUNet/.venv/lib/python3.12/site-packages/fft_conv_pytorch/fft_conv.py:139: UserWarning: Using a non-tuple sequence for multidimensional indexing is deprecated and will be changed in pytorch 2.9; use x[tuple(seq)] instead of x[seq]. In pytorch 2.9 this will be interpreted as tensor index, x[torch.tensor(seq)], which will result either in an error or a different result (Triggered internally at /pytorch/torch/csrc/autograd/python_variable_indexing.cpp:306.)
  output = output[crop_slices].contiguous()
/nfs/erelab001/shared/Computational_Group/Naravich/nnUNet/.venv/lib/python3.12/site-packages/fft_conv_pytorch/fft_conv.py:139: UserWarning: Using a non-tuple sequence for multidimensional indexing is deprecated and will be changed in pytorch 2.9; use x[tuple(seq)] instead of x[seq]. In pytorch 2.9 this will be interpreted as tensor index, x[torch.tensor(seq)], which will result either in an error or a different result (Triggered internally at /pytorch/torch/csrc/autograd/python_variable_indexing.cpp:306.)
  output = output[crop_slices].contiguous()
/nfs/erelab001/shared/Computational_Group/Naravich/nnUNet/.venv/lib/python3.12/site-packages/fft_conv_pytorch/fft_conv.py:139: UserWarning: Using a non-tuple sequence for multidimensional indexing is deprecated and will be changed in pytorch 2.9; use x[tuple(seq)] instead of x[seq]. In pytorch 2.9 this will be interpreted as tensor index, x[torch.tensor(seq)], which will result either in an error or a different result (Triggered internally at /pytorch/torch/csrc/autograd/python_variable_indexing.cpp:306.)
  output = output[crop_slices].contiguous()
using pin_memory on device 0
using pin_memory on device 0
2025-10-05 14:53:01.363314: Using torch.compile...

This is the configuration used by this training:
Configuration name: 3d_32x160x128_b10
 {'data_identifier': 'nnUNetPlans_3d_fullres', 'preprocessor_name': 'DefaultPreprocessor', 'batch_size': 10, 'patch_size': [32, 160, 128], 'median_image_size_in_voxels': [375.0, 498.0, 498.0], 'spacing': [1.0, 1.0, 1.0], 'normalization_schemes': ['ZScoreNormalization'], 'use_mask_for_norm': [False], 'resampling_fn_data': 'resample_data_or_seg_to_shape', 'resampling_fn_seg': 'resample_data_or_seg_to_shape', 'resampling_fn_data_kwargs': {'is_seg': False, 'order': 3, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_seg_kwargs': {'is_seg': True, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_probabilities': 'resample_data_or_seg_to_shape', 'resampling_fn_probabilities_kwargs': {'is_seg': False, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'architecture': {'network_class_name': 'dynamic_network_architectures.architectures.unet.PlainConvUNet', 'arch_kwargs': {'n_stages': 6, 'features_per_stage': [32, 64, 128, 256, 320, 320], 'conv_op': 'torch.nn.modules.conv.Conv3d', 'kernel_sizes': [[3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3]], 'strides': [[1, 1, 1], [2, 2, 2], [2, 2, 2], [2, 2, 2], [2, 2, 2], [1, 2, 2]], 'n_conv_per_stage': [2, 2, 2, 2, 2, 2], 'n_conv_per_stage_decoder': [2, 2, 2, 2, 2], 'conv_bias': True, 'norm_op': 'torch.nn.modules.instancenorm.InstanceNorm3d', 'norm_op_kwargs': {'eps': 1e-05, 'affine': True}, 'dropout_op': None, 'dropout_op_kwargs': None, 'nonlin': 'torch.nn.LeakyReLU', 'nonlin_kwargs': {'inplace': True}, 'deep_supervision': True}, '_kw_requires_import': ['conv_op', 'norm_op', 'dropout_op', 'nonlin']}, 'batch_dice': True, 'inherits_from': '3d_fullres'} 

These are the global plan.json settings:
 {'dataset_name': 'Dataset310_nnInteractive_Calcium_OCT_CrossValidation', 'plans_name': 'nnUNetPlans', 'original_median_spacing_after_transp': [1.0, 1.0, 1.0], 'original_median_shape_after_transp': [375, 498, 498], 'image_reader_writer': 'SimpleITKIO', 'transpose_forward': [0, 1, 2], 'transpose_backward': [0, 1, 2], 'experiment_planner_used': 'ExperimentPlanner', 'label_manager': 'LabelManager', 'foreground_intensity_properties_per_channel': {'0': {'max': 0.4977976381778717, 'mean': 0.1394134759902954, 'median': 0.09849607944488525, 'min': 0.0, 'percentile_00_5': 0.015305490233004093, 'percentile_99_5': 0.4977976381778717, 'std': 0.121165432035923}}} 

2025-10-05 14:53:03.125790: unpacking dataset...
/nfs/erelab001/shared/Computational_Group/Naravich/nnUNet/.venv/lib/python3.12/site-packages/torch/_inductor/lowering.py:7095: UserWarning: 
Online softmax is disabled on the fly since Inductor decides to
split the reduction. Cut an issue to PyTorch if this is an
important use case and you want to speed it up with online
softmax.

  warnings.warn(
/nfs/erelab001/shared/Computational_Group/Naravich/nnUNet/.venv/lib/python3.12/site-packages/torch/_inductor/lowering.py:7095: UserWarning: 
Online softmax is disabled on the fly since Inductor decides to
split the reduction. Cut an issue to PyTorch if this is an
important use case and you want to speed it up with online
softmax.

  warnings.warn(
/nfs/erelab001/shared/Computational_Group/Naravich/nnUNet/.venv/lib/python3.12/site-packages/torch/_inductor/lowering.py:7095: UserWarning: 
Online softmax is disabled on the fly since Inductor decides to
split the reduction. Cut an issue to PyTorch if this is an
important use case and you want to speed it up with online
softmax.

  warnings.warn(
/nfs/erelab001/shared/Computational_Group/Naravich/nnUNet/.venv/lib/python3.12/site-packages/torch/_inductor/lowering.py:7095: UserWarning: 
Online softmax is disabled on the fly since Inductor decides to
split the reduction. Cut an issue to PyTorch if this is an
important use case and you want to speed it up with online
softmax.

  warnings.warn(
2025-10-05 14:53:07.193063: unpacking done...
2025-10-05 14:53:07.195236: Unable to plot network architecture: nnUNet_compile is enabled!
2025-10-05 14:53:07.199890: 
2025-10-05 14:53:07.200058: Epoch 0
2025-10-05 14:53:07.200236: Current learning rate: 0.01
2025-10-05 14:54:27.170785: Validation loss improved from 1000.00000 to -0.02184! Patience: 0/50
2025-10-05 14:54:27.171641: train_loss -0.1497
2025-10-05 14:54:27.171921: val_loss -0.0218
2025-10-05 14:54:27.172240: Pseudo dice [np.float32(0.4995)]
2025-10-05 14:54:27.172479: Epoch time: 79.97 s
2025-10-05 14:54:27.172770: Yayy! New best EMA pseudo Dice: 0.49950000643730164
2025-10-05 14:54:28.125530: 
2025-10-05 14:54:28.126132: Epoch 1
2025-10-05 14:54:28.126327: Current learning rate: 0.00994
2025-10-05 14:55:14.396045: Validation loss improved from -0.02184 to -0.14735! Patience: 0/50
2025-10-05 14:55:14.396457: train_loss -0.2727
2025-10-05 14:55:14.396623: val_loss -0.1474
2025-10-05 14:55:14.396741: Pseudo dice [np.float32(0.5202)]
2025-10-05 14:55:14.396885: Epoch time: 46.27 s
2025-10-05 14:55:14.396994: Yayy! New best EMA pseudo Dice: 0.5015000104904175
2025-10-05 14:55:15.463072: 
2025-10-05 14:55:15.463427: Epoch 2
2025-10-05 14:55:15.463632: Current learning rate: 0.00988
2025-10-05 14:56:01.752659: Validation loss improved from -0.14735 to -0.17537! Patience: 0/50
2025-10-05 14:56:01.753314: train_loss -0.3073
2025-10-05 14:56:01.753504: val_loss -0.1754
2025-10-05 14:56:01.753613: Pseudo dice [np.float32(0.5722)]
2025-10-05 14:56:01.753780: Epoch time: 46.29 s
2025-10-05 14:56:01.753932: Yayy! New best EMA pseudo Dice: 0.5085999965667725
2025-10-05 14:56:02.835536: 
2025-10-05 14:56:02.835851: Epoch 3
2025-10-05 14:56:02.836070: Current learning rate: 0.00982
2025-10-05 14:56:49.157749: Validation loss did not improve from -0.17537. Patience: 1/50
2025-10-05 14:56:49.158220: train_loss -0.3529
2025-10-05 14:56:49.158371: val_loss -0.1668
2025-10-05 14:56:49.158507: Pseudo dice [np.float32(0.5805)]
2025-10-05 14:56:49.158638: Epoch time: 46.32 s
2025-10-05 14:56:49.158797: Yayy! New best EMA pseudo Dice: 0.5157999992370605
2025-10-05 14:56:50.242428: 
2025-10-05 14:56:50.242766: Epoch 4
2025-10-05 14:56:50.242989: Current learning rate: 0.00976
2025-10-05 14:57:36.591156: Validation loss did not improve from -0.17537. Patience: 2/50
2025-10-05 14:57:36.591713: train_loss -0.3606
2025-10-05 14:57:36.591885: val_loss -0.0951
2025-10-05 14:57:36.592017: Pseudo dice [np.float32(0.4997)]
2025-10-05 14:57:36.592167: Epoch time: 46.35 s
2025-10-05 14:57:37.618436: 
2025-10-05 14:57:37.618725: Epoch 5
2025-10-05 14:57:37.618963: Current learning rate: 0.0097
2025-10-05 14:58:23.999269: Validation loss improved from -0.17537 to -0.21729! Patience: 2/50
2025-10-05 14:58:23.999748: train_loss -0.4129
2025-10-05 14:58:23.999974: val_loss -0.2173
2025-10-05 14:58:24.000147: Pseudo dice [np.float32(0.5933)]
2025-10-05 14:58:24.000430: Epoch time: 46.38 s
2025-10-05 14:58:24.000687: Yayy! New best EMA pseudo Dice: 0.5220999717712402
2025-10-05 14:58:25.114074: 
2025-10-05 14:58:25.114525: Epoch 6
2025-10-05 14:58:25.114787: Current learning rate: 0.00964
2025-10-05 14:59:11.496972: Validation loss did not improve from -0.21729. Patience: 1/50
2025-10-05 14:59:11.497531: train_loss -0.4316
2025-10-05 14:59:11.497675: val_loss -0.2013
2025-10-05 14:59:11.497808: Pseudo dice [np.float32(0.5867)]
2025-10-05 14:59:11.497953: Epoch time: 46.38 s
2025-10-05 14:59:11.498083: Yayy! New best EMA pseudo Dice: 0.5285000205039978
2025-10-05 14:59:12.575349: 
2025-10-05 14:59:12.575651: Epoch 7
2025-10-05 14:59:12.575903: Current learning rate: 0.00958
2025-10-05 14:59:58.904279: Validation loss improved from -0.21729 to -0.23352! Patience: 1/50
2025-10-05 14:59:58.904804: train_loss -0.4434
2025-10-05 14:59:58.905035: val_loss -0.2335
2025-10-05 14:59:58.905239: Pseudo dice [np.float32(0.5836)]
2025-10-05 14:59:58.905498: Epoch time: 46.33 s
2025-10-05 14:59:58.905712: Yayy! New best EMA pseudo Dice: 0.5340999960899353
2025-10-05 14:59:59.963262: 
2025-10-05 14:59:59.963642: Epoch 8
2025-10-05 14:59:59.963969: Current learning rate: 0.00952
2025-10-05 15:00:46.380985: Validation loss improved from -0.23352 to -0.27850! Patience: 0/50
2025-10-05 15:00:46.381613: train_loss -0.4664
2025-10-05 15:00:46.381832: val_loss -0.2785
2025-10-05 15:00:46.381973: Pseudo dice [np.float32(0.632)]
2025-10-05 15:00:46.382137: Epoch time: 46.42 s
2025-10-05 15:00:46.382284: Yayy! New best EMA pseudo Dice: 0.5437999963760376
2025-10-05 15:00:47.471109: 
2025-10-05 15:00:47.471422: Epoch 9
2025-10-05 15:00:47.471625: Current learning rate: 0.00946
2025-10-05 15:01:33.851483: Validation loss did not improve from -0.27850. Patience: 1/50
2025-10-05 15:01:33.852087: train_loss -0.4886
2025-10-05 15:01:33.852501: val_loss -0.219
2025-10-05 15:01:33.852757: Pseudo dice [np.float32(0.6116)]
2025-10-05 15:01:33.852966: Epoch time: 46.38 s
2025-10-05 15:01:34.297386: Yayy! New best EMA pseudo Dice: 0.550599992275238
2025-10-05 15:01:35.339757: 
2025-10-05 15:01:35.340055: Epoch 10
2025-10-05 15:01:35.340293: Current learning rate: 0.0094
2025-10-05 15:02:21.719372: Validation loss did not improve from -0.27850. Patience: 2/50
2025-10-05 15:02:21.719930: train_loss -0.4855
2025-10-05 15:02:21.720123: val_loss -0.2676
2025-10-05 15:02:21.720286: Pseudo dice [np.float32(0.6285)]
2025-10-05 15:02:21.720438: Epoch time: 46.38 s
2025-10-05 15:02:21.720570: Yayy! New best EMA pseudo Dice: 0.5583999752998352
2025-10-05 15:02:22.785826: 
2025-10-05 15:02:22.786219: Epoch 11
2025-10-05 15:02:22.786495: Current learning rate: 0.00934
2025-10-05 15:03:09.151630: Validation loss did not improve from -0.27850. Patience: 3/50
2025-10-05 15:03:09.152079: train_loss -0.5088
2025-10-05 15:03:09.152292: val_loss -0.2712
2025-10-05 15:03:09.152433: Pseudo dice [np.float32(0.6563)]
2025-10-05 15:03:09.152557: Epoch time: 46.37 s
2025-10-05 15:03:09.152703: Yayy! New best EMA pseudo Dice: 0.5681999921798706
2025-10-05 15:03:10.191723: 
2025-10-05 15:03:10.192051: Epoch 12
2025-10-05 15:03:10.192223: Current learning rate: 0.00928
2025-10-05 15:03:56.629657: Validation loss improved from -0.27850 to -0.28752! Patience: 3/50
2025-10-05 15:03:56.630249: train_loss -0.5203
2025-10-05 15:03:56.630423: val_loss -0.2875
2025-10-05 15:03:56.630611: Pseudo dice [np.float32(0.6467)]
2025-10-05 15:03:56.630802: Epoch time: 46.44 s
2025-10-05 15:03:56.630953: Yayy! New best EMA pseudo Dice: 0.5759999752044678
2025-10-05 15:03:58.057207: 
2025-10-05 15:03:58.057510: Epoch 13
2025-10-05 15:03:58.057656: Current learning rate: 0.00922
2025-10-05 15:04:44.454693: Validation loss improved from -0.28752 to -0.30357! Patience: 0/50
2025-10-05 15:04:44.455201: train_loss -0.5276
2025-10-05 15:04:44.455409: val_loss -0.3036
2025-10-05 15:04:44.455560: Pseudo dice [np.float32(0.6374)]
2025-10-05 15:04:44.455703: Epoch time: 46.4 s
2025-10-05 15:04:44.455826: Yayy! New best EMA pseudo Dice: 0.5821999907493591
2025-10-05 15:04:45.518517: 
2025-10-05 15:04:45.518860: Epoch 14
2025-10-05 15:04:45.519178: Current learning rate: 0.00916
2025-10-05 15:05:31.925853: Validation loss improved from -0.30357 to -0.30731! Patience: 0/50
2025-10-05 15:05:31.926374: train_loss -0.5187
2025-10-05 15:05:31.926530: val_loss -0.3073
2025-10-05 15:05:31.926665: Pseudo dice [np.float32(0.6469)]
2025-10-05 15:05:31.926837: Epoch time: 46.41 s
2025-10-05 15:05:32.365470: Yayy! New best EMA pseudo Dice: 0.588699996471405
2025-10-05 15:05:33.407610: 
2025-10-05 15:05:33.407881: Epoch 15
2025-10-05 15:05:33.408063: Current learning rate: 0.0091
2025-10-05 15:06:19.794737: Validation loss improved from -0.30731 to -0.34542! Patience: 0/50
2025-10-05 15:06:19.795196: train_loss -0.5362
2025-10-05 15:06:19.795341: val_loss -0.3454
2025-10-05 15:06:19.795456: Pseudo dice [np.float32(0.6545)]
2025-10-05 15:06:19.795588: Epoch time: 46.39 s
2025-10-05 15:06:19.795700: Yayy! New best EMA pseudo Dice: 0.5952000021934509
2025-10-05 15:06:20.855946: 
2025-10-05 15:06:20.856274: Epoch 16
2025-10-05 15:06:20.856439: Current learning rate: 0.00903
2025-10-05 15:07:07.283550: Validation loss did not improve from -0.34542. Patience: 1/50
2025-10-05 15:07:07.284452: train_loss -0.5548
2025-10-05 15:07:07.284636: val_loss -0.2732
2025-10-05 15:07:07.284782: Pseudo dice [np.float32(0.6342)]
2025-10-05 15:07:07.284935: Epoch time: 46.43 s
2025-10-05 15:07:07.285050: Yayy! New best EMA pseudo Dice: 0.5990999937057495
2025-10-05 15:07:08.362258: 
2025-10-05 15:07:08.362687: Epoch 17
2025-10-05 15:07:08.362955: Current learning rate: 0.00897
2025-10-05 15:07:54.769859: Validation loss did not improve from -0.34542. Patience: 2/50
2025-10-05 15:07:54.770254: train_loss -0.561
2025-10-05 15:07:54.770437: val_loss -0.2786
2025-10-05 15:07:54.770553: Pseudo dice [np.float32(0.6472)]
2025-10-05 15:07:54.770706: Epoch time: 46.41 s
2025-10-05 15:07:54.770898: Yayy! New best EMA pseudo Dice: 0.6039000153541565
2025-10-05 15:07:55.837292: 
2025-10-05 15:07:55.837593: Epoch 18
2025-10-05 15:07:55.837792: Current learning rate: 0.00891
2025-10-05 15:08:42.250173: Validation loss did not improve from -0.34542. Patience: 3/50
2025-10-05 15:08:42.250783: train_loss -0.5553
2025-10-05 15:08:42.250978: val_loss -0.3405
2025-10-05 15:08:42.251089: Pseudo dice [np.float32(0.663)]
2025-10-05 15:08:42.251210: Epoch time: 46.41 s
2025-10-05 15:08:42.251317: Yayy! New best EMA pseudo Dice: 0.6097999811172485
2025-10-05 15:08:43.332250: 
2025-10-05 15:08:43.332600: Epoch 19
2025-10-05 15:08:43.332838: Current learning rate: 0.00885
2025-10-05 15:09:29.784062: Validation loss did not improve from -0.34542. Patience: 4/50
2025-10-05 15:09:29.784411: train_loss -0.5636
2025-10-05 15:09:29.784658: val_loss -0.2995
2025-10-05 15:09:29.784802: Pseudo dice [np.float32(0.6517)]
2025-10-05 15:09:29.784958: Epoch time: 46.45 s
2025-10-05 15:09:30.232485: Yayy! New best EMA pseudo Dice: 0.6140000224113464
2025-10-05 15:09:31.318294: 
2025-10-05 15:09:31.318624: Epoch 20
2025-10-05 15:09:31.318821: Current learning rate: 0.00879
2025-10-05 15:10:17.743286: Validation loss did not improve from -0.34542. Patience: 5/50
2025-10-05 15:10:17.743984: train_loss -0.5749
2025-10-05 15:10:17.744160: val_loss -0.3403
2025-10-05 15:10:17.744285: Pseudo dice [np.float32(0.6589)]
2025-10-05 15:10:17.744410: Epoch time: 46.43 s
2025-10-05 15:10:17.744548: Yayy! New best EMA pseudo Dice: 0.6184999942779541
2025-10-05 15:10:18.902775: 
2025-10-05 15:10:18.903184: Epoch 21
2025-10-05 15:10:18.903409: Current learning rate: 0.00873
2025-10-05 15:11:05.359170: Validation loss improved from -0.34542 to -0.36946! Patience: 5/50
2025-10-05 15:11:05.359607: train_loss -0.5818
2025-10-05 15:11:05.359797: val_loss -0.3695
2025-10-05 15:11:05.359998: Pseudo dice [np.float32(0.6698)]
2025-10-05 15:11:05.360169: Epoch time: 46.46 s
2025-10-05 15:11:05.360337: Yayy! New best EMA pseudo Dice: 0.6236000061035156
2025-10-05 15:11:06.512009: 
2025-10-05 15:11:06.512342: Epoch 22
2025-10-05 15:11:06.512640: Current learning rate: 0.00867
2025-10-05 15:11:52.950816: Validation loss did not improve from -0.36946. Patience: 1/50
2025-10-05 15:11:52.951453: train_loss -0.5749
2025-10-05 15:11:52.951686: val_loss -0.3146
2025-10-05 15:11:52.951908: Pseudo dice [np.float32(0.6578)]
2025-10-05 15:11:52.952063: Epoch time: 46.44 s
2025-10-05 15:11:52.952227: Yayy! New best EMA pseudo Dice: 0.6270999908447266
2025-10-05 15:11:54.035375: 
2025-10-05 15:11:54.035748: Epoch 23
2025-10-05 15:11:54.036039: Current learning rate: 0.00861
2025-10-05 15:12:40.538229: Validation loss improved from -0.36946 to -0.38022! Patience: 1/50
2025-10-05 15:12:40.538711: train_loss -0.5848
2025-10-05 15:12:40.538858: val_loss -0.3802
2025-10-05 15:12:40.539002: Pseudo dice [np.float32(0.6899)]
2025-10-05 15:12:40.539166: Epoch time: 46.5 s
2025-10-05 15:12:40.539293: Yayy! New best EMA pseudo Dice: 0.6333000063896179
2025-10-05 15:12:41.614407: 
2025-10-05 15:12:41.614743: Epoch 24
2025-10-05 15:12:41.614956: Current learning rate: 0.00855
2025-10-05 15:13:28.071203: Validation loss did not improve from -0.38022. Patience: 1/50
2025-10-05 15:13:28.071720: train_loss -0.5974
2025-10-05 15:13:28.071887: val_loss -0.3703
2025-10-05 15:13:28.072057: Pseudo dice [np.float32(0.6681)]
2025-10-05 15:13:28.072188: Epoch time: 46.46 s
2025-10-05 15:13:28.498124: Yayy! New best EMA pseudo Dice: 0.6367999911308289
2025-10-05 15:13:29.547051: 
2025-10-05 15:13:29.547275: Epoch 25
2025-10-05 15:13:29.547472: Current learning rate: 0.00849
2025-10-05 15:14:15.986730: Validation loss did not improve from -0.38022. Patience: 2/50
2025-10-05 15:14:15.987162: train_loss -0.6041
2025-10-05 15:14:15.987367: val_loss -0.336
2025-10-05 15:14:15.987539: Pseudo dice [np.float32(0.6722)]
2025-10-05 15:14:15.987662: Epoch time: 46.44 s
2025-10-05 15:14:15.987774: Yayy! New best EMA pseudo Dice: 0.6403999924659729
2025-10-05 15:14:17.057972: 
2025-10-05 15:14:17.058213: Epoch 26
2025-10-05 15:14:17.058444: Current learning rate: 0.00843
2025-10-05 15:15:03.486732: Validation loss did not improve from -0.38022. Patience: 3/50
2025-10-05 15:15:03.487312: train_loss -0.6037
2025-10-05 15:15:03.487449: val_loss -0.3345
2025-10-05 15:15:03.487558: Pseudo dice [np.float32(0.653)]
2025-10-05 15:15:03.487676: Epoch time: 46.43 s
2025-10-05 15:15:03.487786: Yayy! New best EMA pseudo Dice: 0.6416000127792358
2025-10-05 15:15:04.551647: 
2025-10-05 15:15:04.551964: Epoch 27
2025-10-05 15:15:04.552140: Current learning rate: 0.00836
2025-10-05 15:15:50.993379: Validation loss did not improve from -0.38022. Patience: 4/50
2025-10-05 15:15:50.993801: train_loss -0.6027
2025-10-05 15:15:50.993937: val_loss -0.3453
2025-10-05 15:15:50.994050: Pseudo dice [np.float32(0.6678)]
2025-10-05 15:15:50.994182: Epoch time: 46.44 s
2025-10-05 15:15:50.994287: Yayy! New best EMA pseudo Dice: 0.6442000269889832
2025-10-05 15:15:52.435885: 
2025-10-05 15:15:52.436231: Epoch 28
2025-10-05 15:15:52.436439: Current learning rate: 0.0083
2025-10-05 15:16:38.894884: Validation loss did not improve from -0.38022. Patience: 5/50
2025-10-05 15:16:38.895456: train_loss -0.6072
2025-10-05 15:16:38.895592: val_loss -0.3505
2025-10-05 15:16:38.895702: Pseudo dice [np.float32(0.6767)]
2025-10-05 15:16:38.895859: Epoch time: 46.46 s
2025-10-05 15:16:38.895982: Yayy! New best EMA pseudo Dice: 0.6474999785423279
2025-10-05 15:16:39.983547: 
2025-10-05 15:16:39.983916: Epoch 29
2025-10-05 15:16:39.984132: Current learning rate: 0.00824
2025-10-05 15:17:26.406867: Validation loss did not improve from -0.38022. Patience: 6/50
2025-10-05 15:17:26.407371: train_loss -0.6017
2025-10-05 15:17:26.407543: val_loss -0.3396
2025-10-05 15:17:26.407692: Pseudo dice [np.float32(0.6731)]
2025-10-05 15:17:26.407831: Epoch time: 46.42 s
2025-10-05 15:17:26.889635: Yayy! New best EMA pseudo Dice: 0.6499999761581421
2025-10-05 15:17:27.963121: 
2025-10-05 15:17:27.963399: Epoch 30
2025-10-05 15:17:27.963657: Current learning rate: 0.00818
2025-10-05 15:18:14.414104: Validation loss did not improve from -0.38022. Patience: 7/50
2025-10-05 15:18:14.414794: train_loss -0.6192
2025-10-05 15:18:14.414987: val_loss -0.3102
2025-10-05 15:18:14.415144: Pseudo dice [np.float32(0.66)]
2025-10-05 15:18:14.415274: Epoch time: 46.45 s
2025-10-05 15:18:14.415407: Yayy! New best EMA pseudo Dice: 0.6510000228881836
2025-10-05 15:18:15.513173: 
2025-10-05 15:18:15.513450: Epoch 31
2025-10-05 15:18:15.513672: Current learning rate: 0.00812
2025-10-05 15:19:01.934433: Validation loss did not improve from -0.38022. Patience: 8/50
2025-10-05 15:19:01.934827: train_loss -0.6255
2025-10-05 15:19:01.934977: val_loss -0.331
2025-10-05 15:19:01.935127: Pseudo dice [np.float32(0.6683)]
2025-10-05 15:19:01.935269: Epoch time: 46.42 s
2025-10-05 15:19:01.935383: Yayy! New best EMA pseudo Dice: 0.6528000235557556
2025-10-05 15:19:03.011360: 
2025-10-05 15:19:03.011703: Epoch 32
2025-10-05 15:19:03.011907: Current learning rate: 0.00806
2025-10-05 15:19:49.482517: Validation loss did not improve from -0.38022. Patience: 9/50
2025-10-05 15:19:49.483279: train_loss -0.6347
2025-10-05 15:19:49.483523: val_loss -0.328
2025-10-05 15:19:49.483806: Pseudo dice [np.float32(0.6621)]
2025-10-05 15:19:49.484101: Epoch time: 46.47 s
2025-10-05 15:19:49.484348: Yayy! New best EMA pseudo Dice: 0.6536999940872192
2025-10-05 15:19:50.559509: 
2025-10-05 15:19:50.559873: Epoch 33
2025-10-05 15:19:50.560109: Current learning rate: 0.008
2025-10-05 15:20:37.036805: Validation loss did not improve from -0.38022. Patience: 10/50
2025-10-05 15:20:37.037226: train_loss -0.6438
2025-10-05 15:20:37.037435: val_loss -0.3461
2025-10-05 15:20:37.037645: Pseudo dice [np.float32(0.6763)]
2025-10-05 15:20:37.037920: Epoch time: 46.48 s
2025-10-05 15:20:37.038084: Yayy! New best EMA pseudo Dice: 0.656000018119812
2025-10-05 15:20:38.140969: 
2025-10-05 15:20:38.141340: Epoch 34
2025-10-05 15:20:38.141533: Current learning rate: 0.00793
2025-10-05 15:21:24.580600: Validation loss did not improve from -0.38022. Patience: 11/50
2025-10-05 15:21:24.581284: train_loss -0.6479
2025-10-05 15:21:24.581418: val_loss -0.3176
2025-10-05 15:21:24.581551: Pseudo dice [np.float32(0.6516)]
2025-10-05 15:21:24.581670: Epoch time: 46.44 s
2025-10-05 15:21:25.654618: 
2025-10-05 15:21:25.654916: Epoch 35
2025-10-05 15:21:25.655082: Current learning rate: 0.00787
2025-10-05 15:22:12.088731: Validation loss did not improve from -0.38022. Patience: 12/50
2025-10-05 15:22:12.089130: train_loss -0.6493
2025-10-05 15:22:12.089267: val_loss -0.3419
2025-10-05 15:22:12.089380: Pseudo dice [np.float32(0.6786)]
2025-10-05 15:22:12.089784: Epoch time: 46.44 s
2025-10-05 15:22:12.089906: Yayy! New best EMA pseudo Dice: 0.657800018787384
2025-10-05 15:22:13.173573: 
2025-10-05 15:22:13.173849: Epoch 36
2025-10-05 15:22:13.174034: Current learning rate: 0.00781
2025-10-05 15:22:59.612843: Validation loss did not improve from -0.38022. Patience: 13/50
2025-10-05 15:22:59.613438: train_loss -0.6501
2025-10-05 15:22:59.613576: val_loss -0.2803
2025-10-05 15:22:59.613683: Pseudo dice [np.float32(0.6684)]
2025-10-05 15:22:59.613809: Epoch time: 46.44 s
2025-10-05 15:22:59.613934: Yayy! New best EMA pseudo Dice: 0.6589000225067139
2025-10-05 15:23:00.686634: 
2025-10-05 15:23:00.686904: Epoch 37
2025-10-05 15:23:00.687046: Current learning rate: 0.00775
2025-10-05 15:23:47.085171: Validation loss did not improve from -0.38022. Patience: 14/50
2025-10-05 15:23:47.085574: train_loss -0.6604
2025-10-05 15:23:47.085936: val_loss -0.2933
2025-10-05 15:23:47.086220: Pseudo dice [np.float32(0.6359)]
2025-10-05 15:23:47.086563: Epoch time: 46.4 s
2025-10-05 15:23:47.723994: 
2025-10-05 15:23:47.724246: Epoch 38
2025-10-05 15:23:47.724405: Current learning rate: 0.00769
2025-10-05 15:24:34.130763: Validation loss did not improve from -0.38022. Patience: 15/50
2025-10-05 15:24:34.131554: train_loss -0.6596
2025-10-05 15:24:34.131899: val_loss -0.3191
2025-10-05 15:24:34.132146: Pseudo dice [np.float32(0.6575)]
2025-10-05 15:24:34.132405: Epoch time: 46.41 s
2025-10-05 15:24:34.774757: 
2025-10-05 15:24:34.775116: Epoch 39
2025-10-05 15:24:34.775356: Current learning rate: 0.00763
2025-10-05 15:25:21.156124: Validation loss did not improve from -0.38022. Patience: 16/50
2025-10-05 15:25:21.156608: train_loss -0.6616
2025-10-05 15:25:21.156818: val_loss -0.3277
2025-10-05 15:25:21.157042: Pseudo dice [np.float32(0.6692)]
2025-10-05 15:25:21.157244: Epoch time: 46.38 s
2025-10-05 15:25:22.231319: 
2025-10-05 15:25:22.231573: Epoch 40
2025-10-05 15:25:22.231759: Current learning rate: 0.00756
2025-10-05 15:26:08.651042: Validation loss did not improve from -0.38022. Patience: 17/50
2025-10-05 15:26:08.651627: train_loss -0.6541
2025-10-05 15:26:08.651793: val_loss -0.3224
2025-10-05 15:26:08.651951: Pseudo dice [np.float32(0.6644)]
2025-10-05 15:26:08.652135: Epoch time: 46.42 s
2025-10-05 15:26:09.296475: 
2025-10-05 15:26:09.296883: Epoch 41
2025-10-05 15:26:09.297093: Current learning rate: 0.0075
2025-10-05 15:26:55.708732: Validation loss did not improve from -0.38022. Patience: 18/50
2025-10-05 15:26:55.709180: train_loss -0.6653
2025-10-05 15:26:55.709357: val_loss -0.3466
2025-10-05 15:26:55.709544: Pseudo dice [np.float32(0.673)]
2025-10-05 15:26:55.709717: Epoch time: 46.41 s
2025-10-05 15:26:55.709898: Yayy! New best EMA pseudo Dice: 0.6600000262260437
2025-10-05 15:26:56.760027: 
2025-10-05 15:26:56.760288: Epoch 42
2025-10-05 15:26:56.760436: Current learning rate: 0.00744
2025-10-05 15:27:43.201114: Validation loss did not improve from -0.38022. Patience: 19/50
2025-10-05 15:27:43.201873: train_loss -0.684
2025-10-05 15:27:43.202072: val_loss -0.3215
2025-10-05 15:27:43.202280: Pseudo dice [np.float32(0.6589)]
2025-10-05 15:27:43.202476: Epoch time: 46.44 s
2025-10-05 15:27:44.199197: 
2025-10-05 15:27:44.199470: Epoch 43
2025-10-05 15:27:44.199655: Current learning rate: 0.00738
2025-10-05 15:28:30.645221: Validation loss did not improve from -0.38022. Patience: 20/50
2025-10-05 15:28:30.645662: train_loss -0.681
2025-10-05 15:28:30.645889: val_loss -0.3176
2025-10-05 15:28:30.646072: Pseudo dice [np.float32(0.6722)]
2025-10-05 15:28:30.646251: Epoch time: 46.45 s
2025-10-05 15:28:30.646427: Yayy! New best EMA pseudo Dice: 0.6610999703407288
2025-10-05 15:28:31.702908: 
2025-10-05 15:28:31.703171: Epoch 44
2025-10-05 15:28:31.703342: Current learning rate: 0.00732
2025-10-05 15:29:18.160258: Validation loss did not improve from -0.38022. Patience: 21/50
2025-10-05 15:29:18.160937: train_loss -0.6868
2025-10-05 15:29:18.161229: val_loss -0.3293
2025-10-05 15:29:18.161412: Pseudo dice [np.float32(0.6589)]
2025-10-05 15:29:18.161644: Epoch time: 46.46 s
2025-10-05 15:29:19.248927: 
2025-10-05 15:29:19.249240: Epoch 45
2025-10-05 15:29:19.249490: Current learning rate: 0.00725
2025-10-05 15:30:05.680696: Validation loss did not improve from -0.38022. Patience: 22/50
2025-10-05 15:30:05.681157: train_loss -0.6911
2025-10-05 15:30:05.681338: val_loss -0.3316
2025-10-05 15:30:05.681513: Pseudo dice [np.float32(0.6745)]
2025-10-05 15:30:05.681672: Epoch time: 46.43 s
2025-10-05 15:30:05.681870: Yayy! New best EMA pseudo Dice: 0.6622999906539917
2025-10-05 15:30:06.755715: 
2025-10-05 15:30:06.756021: Epoch 46
2025-10-05 15:30:06.756279: Current learning rate: 0.00719
2025-10-05 15:30:53.168377: Validation loss did not improve from -0.38022. Patience: 23/50
2025-10-05 15:30:53.169092: train_loss -0.6819
2025-10-05 15:30:53.169330: val_loss -0.3432
2025-10-05 15:30:53.169503: Pseudo dice [np.float32(0.6728)]
2025-10-05 15:30:53.169697: Epoch time: 46.41 s
2025-10-05 15:30:53.169847: Yayy! New best EMA pseudo Dice: 0.6632999777793884
2025-10-05 15:30:54.232843: 
2025-10-05 15:30:54.233210: Epoch 47
2025-10-05 15:30:54.233474: Current learning rate: 0.00713
2025-10-05 15:31:40.652498: Validation loss did not improve from -0.38022. Patience: 24/50
2025-10-05 15:31:40.652953: train_loss -0.6891
2025-10-05 15:31:40.653186: val_loss -0.3205
2025-10-05 15:31:40.653408: Pseudo dice [np.float32(0.6646)]
2025-10-05 15:31:40.653619: Epoch time: 46.42 s
2025-10-05 15:31:40.653822: Yayy! New best EMA pseudo Dice: 0.6635000109672546
2025-10-05 15:31:41.752564: 
2025-10-05 15:31:41.752881: Epoch 48
2025-10-05 15:31:41.753160: Current learning rate: 0.00707
2025-10-05 15:32:28.159202: Validation loss did not improve from -0.38022. Patience: 25/50
2025-10-05 15:32:28.160110: train_loss -0.6846
2025-10-05 15:32:28.160320: val_loss -0.3469
2025-10-05 15:32:28.160500: Pseudo dice [np.float32(0.6773)]
2025-10-05 15:32:28.160754: Epoch time: 46.41 s
2025-10-05 15:32:28.160980: Yayy! New best EMA pseudo Dice: 0.6647999882698059
2025-10-05 15:32:29.248981: 
2025-10-05 15:32:29.249384: Epoch 49
2025-10-05 15:32:29.249789: Current learning rate: 0.007
2025-10-05 15:33:15.678776: Validation loss did not improve from -0.38022. Patience: 26/50
2025-10-05 15:33:15.679268: train_loss -0.7009
2025-10-05 15:33:15.679451: val_loss -0.3379
2025-10-05 15:33:15.679631: Pseudo dice [np.float32(0.6679)]
2025-10-05 15:33:15.679821: Epoch time: 46.43 s
2025-10-05 15:33:16.112205: Yayy! New best EMA pseudo Dice: 0.6650999784469604
2025-10-05 15:33:17.190073: 
2025-10-05 15:33:17.190518: Epoch 50
2025-10-05 15:33:17.190840: Current learning rate: 0.00694
2025-10-05 15:34:03.689541: Validation loss did not improve from -0.38022. Patience: 27/50
2025-10-05 15:34:03.690180: train_loss -0.7075
2025-10-05 15:34:03.690443: val_loss -0.3513
2025-10-05 15:34:03.690653: Pseudo dice [np.float32(0.6748)]
2025-10-05 15:34:03.691030: Epoch time: 46.5 s
2025-10-05 15:34:03.691257: Yayy! New best EMA pseudo Dice: 0.666100025177002
2025-10-05 15:34:04.773161: 
2025-10-05 15:34:04.773502: Epoch 51
2025-10-05 15:34:04.773753: Current learning rate: 0.00688
2025-10-05 15:34:51.217946: Validation loss did not improve from -0.38022. Patience: 28/50
2025-10-05 15:34:51.218436: train_loss -0.701
2025-10-05 15:34:51.218672: val_loss -0.3219
2025-10-05 15:34:51.218876: Pseudo dice [np.float32(0.6693)]
2025-10-05 15:34:51.219074: Epoch time: 46.45 s
2025-10-05 15:34:51.219257: Yayy! New best EMA pseudo Dice: 0.6664000153541565
2025-10-05 15:34:52.305584: 
2025-10-05 15:34:52.305950: Epoch 52
2025-10-05 15:34:52.306221: Current learning rate: 0.00682
2025-10-05 15:35:38.737059: Validation loss did not improve from -0.38022. Patience: 29/50
2025-10-05 15:35:38.737676: train_loss -0.7126
2025-10-05 15:35:38.737861: val_loss -0.3427
2025-10-05 15:35:38.738036: Pseudo dice [np.float32(0.678)]
2025-10-05 15:35:38.738256: Epoch time: 46.43 s
2025-10-05 15:35:38.738420: Yayy! New best EMA pseudo Dice: 0.6675999760627747
2025-10-05 15:35:39.837384: 
2025-10-05 15:35:39.837744: Epoch 53
2025-10-05 15:35:39.837928: Current learning rate: 0.00675
2025-10-05 15:36:26.264134: Validation loss did not improve from -0.38022. Patience: 30/50
2025-10-05 15:36:26.264598: train_loss -0.7155
2025-10-05 15:36:26.264740: val_loss -0.2936
2025-10-05 15:36:26.264859: Pseudo dice [np.float32(0.667)]
2025-10-05 15:36:26.265032: Epoch time: 46.43 s
2025-10-05 15:36:26.897446: 
2025-10-05 15:36:26.897762: Epoch 54
2025-10-05 15:36:26.897942: Current learning rate: 0.00669
2025-10-05 15:37:13.335892: Validation loss did not improve from -0.38022. Patience: 31/50
2025-10-05 15:37:13.336537: train_loss -0.7125
2025-10-05 15:37:13.336682: val_loss -0.3337
2025-10-05 15:37:13.336817: Pseudo dice [np.float32(0.6812)]
2025-10-05 15:37:13.336942: Epoch time: 46.44 s
2025-10-05 15:37:13.801275: Yayy! New best EMA pseudo Dice: 0.6689000129699707
2025-10-05 15:37:14.887355: 
2025-10-05 15:37:14.887664: Epoch 55
2025-10-05 15:37:14.887894: Current learning rate: 0.00663
2025-10-05 15:38:01.331309: Validation loss did not improve from -0.38022. Patience: 32/50
2025-10-05 15:38:01.331732: train_loss -0.7262
2025-10-05 15:38:01.331884: val_loss -0.3155
2025-10-05 15:38:01.332010: Pseudo dice [np.float32(0.6721)]
2025-10-05 15:38:01.332209: Epoch time: 46.45 s
2025-10-05 15:38:01.332331: Yayy! New best EMA pseudo Dice: 0.6692000031471252
2025-10-05 15:38:02.428193: 
2025-10-05 15:38:02.428530: Epoch 56
2025-10-05 15:38:02.428828: Current learning rate: 0.00657
2025-10-05 15:38:48.831619: Validation loss did not improve from -0.38022. Patience: 33/50
2025-10-05 15:38:48.832359: train_loss -0.7205
2025-10-05 15:38:48.832686: val_loss -0.2943
2025-10-05 15:38:48.832871: Pseudo dice [np.float32(0.6705)]
2025-10-05 15:38:48.833133: Epoch time: 46.4 s
2025-10-05 15:38:48.833284: Yayy! New best EMA pseudo Dice: 0.6693000197410583
2025-10-05 15:38:49.932080: 
2025-10-05 15:38:49.932378: Epoch 57
2025-10-05 15:38:49.932602: Current learning rate: 0.0065
2025-10-05 15:39:36.398406: Validation loss did not improve from -0.38022. Patience: 34/50
2025-10-05 15:39:36.398879: train_loss -0.7263
2025-10-05 15:39:36.399094: val_loss -0.3255
2025-10-05 15:39:36.399357: Pseudo dice [np.float32(0.6809)]
2025-10-05 15:39:36.399520: Epoch time: 46.47 s
2025-10-05 15:39:36.399707: Yayy! New best EMA pseudo Dice: 0.6704999804496765
2025-10-05 15:39:37.933503: 
2025-10-05 15:39:37.933870: Epoch 58
2025-10-05 15:39:37.934142: Current learning rate: 0.00644
2025-10-05 15:40:24.308389: Validation loss did not improve from -0.38022. Patience: 35/50
2025-10-05 15:40:24.309325: train_loss -0.7195
2025-10-05 15:40:24.309630: val_loss -0.3291
2025-10-05 15:40:24.309922: Pseudo dice [np.float32(0.671)]
2025-10-05 15:40:24.310143: Epoch time: 46.38 s
2025-10-05 15:40:24.310467: Yayy! New best EMA pseudo Dice: 0.6704999804496765
2025-10-05 15:40:25.410398: 
2025-10-05 15:40:25.410747: Epoch 59
2025-10-05 15:40:25.411037: Current learning rate: 0.00638
2025-10-05 15:41:11.756417: Validation loss did not improve from -0.38022. Patience: 36/50
2025-10-05 15:41:11.756886: train_loss -0.7313
2025-10-05 15:41:11.757095: val_loss -0.3234
2025-10-05 15:41:11.757275: Pseudo dice [np.float32(0.67)]
2025-10-05 15:41:11.757492: Epoch time: 46.35 s
2025-10-05 15:41:12.861739: 
2025-10-05 15:41:12.862097: Epoch 60
2025-10-05 15:41:12.862314: Current learning rate: 0.00631
2025-10-05 15:41:59.206147: Validation loss did not improve from -0.38022. Patience: 37/50
2025-10-05 15:41:59.206862: train_loss -0.7371
2025-10-05 15:41:59.207000: val_loss -0.2995
2025-10-05 15:41:59.207190: Pseudo dice [np.float32(0.6811)]
2025-10-05 15:41:59.207401: Epoch time: 46.35 s
2025-10-05 15:41:59.207527: Yayy! New best EMA pseudo Dice: 0.6715999841690063
2025-10-05 15:42:00.309695: 
2025-10-05 15:42:00.310051: Epoch 61
2025-10-05 15:42:00.310264: Current learning rate: 0.00625
2025-10-05 15:42:46.689373: Validation loss did not improve from -0.38022. Patience: 38/50
2025-10-05 15:42:46.689808: train_loss -0.7377
2025-10-05 15:42:46.690001: val_loss -0.3258
2025-10-05 15:42:46.690183: Pseudo dice [np.float32(0.6838)]
2025-10-05 15:42:46.690336: Epoch time: 46.38 s
2025-10-05 15:42:46.690472: Yayy! New best EMA pseudo Dice: 0.6728000044822693
2025-10-05 15:42:47.792629: 
2025-10-05 15:42:47.792943: Epoch 62
2025-10-05 15:42:47.793124: Current learning rate: 0.00619
2025-10-05 15:43:34.125703: Validation loss did not improve from -0.38022. Patience: 39/50
2025-10-05 15:43:34.126498: train_loss -0.7458
2025-10-05 15:43:34.126642: val_loss -0.2628
2025-10-05 15:43:34.126760: Pseudo dice [np.float32(0.6543)]
2025-10-05 15:43:34.126907: Epoch time: 46.33 s
2025-10-05 15:43:34.775211: 
2025-10-05 15:43:34.775455: Epoch 63
2025-10-05 15:43:34.775620: Current learning rate: 0.00612
2025-10-05 15:44:21.120500: Validation loss did not improve from -0.38022. Patience: 40/50
2025-10-05 15:44:21.120915: train_loss -0.7471
2025-10-05 15:44:21.121060: val_loss -0.3226
2025-10-05 15:44:21.121219: Pseudo dice [np.float32(0.6859)]
2025-10-05 15:44:21.121349: Epoch time: 46.35 s
2025-10-05 15:44:21.774672: 
2025-10-05 15:44:21.774977: Epoch 64
2025-10-05 15:44:21.775170: Current learning rate: 0.00606
2025-10-05 15:45:08.188884: Validation loss did not improve from -0.38022. Patience: 41/50
2025-10-05 15:45:08.189643: train_loss -0.7467
2025-10-05 15:45:08.189821: val_loss -0.3321
2025-10-05 15:45:08.190019: Pseudo dice [np.float32(0.6839)]
2025-10-05 15:45:08.190203: Epoch time: 46.42 s
2025-10-05 15:45:08.653547: Yayy! New best EMA pseudo Dice: 0.6736000180244446
2025-10-05 15:45:09.759780: 
2025-10-05 15:45:09.760238: Epoch 65
2025-10-05 15:45:09.760610: Current learning rate: 0.006
2025-10-05 15:45:56.110643: Validation loss did not improve from -0.38022. Patience: 42/50
2025-10-05 15:45:56.111055: train_loss -0.7498
2025-10-05 15:45:56.111233: val_loss -0.3311
2025-10-05 15:45:56.111438: Pseudo dice [np.float32(0.6669)]
2025-10-05 15:45:56.111645: Epoch time: 46.35 s
2025-10-05 15:45:56.765175: 
2025-10-05 15:45:56.765531: Epoch 66
2025-10-05 15:45:56.765791: Current learning rate: 0.00593
2025-10-05 15:46:43.088932: Validation loss did not improve from -0.38022. Patience: 43/50
2025-10-05 15:46:43.089569: train_loss -0.7548
2025-10-05 15:46:43.089767: val_loss -0.2998
2025-10-05 15:46:43.089914: Pseudo dice [np.float32(0.6629)]
2025-10-05 15:46:43.090070: Epoch time: 46.33 s
2025-10-05 15:46:43.746976: 
2025-10-05 15:46:43.747251: Epoch 67
2025-10-05 15:46:43.747439: Current learning rate: 0.00587
2025-10-05 15:47:30.103038: Validation loss did not improve from -0.38022. Patience: 44/50
2025-10-05 15:47:30.103540: train_loss -0.7567
2025-10-05 15:47:30.103691: val_loss -0.3062
2025-10-05 15:47:30.103868: Pseudo dice [np.float32(0.6691)]
2025-10-05 15:47:30.104002: Epoch time: 46.36 s
2025-10-05 15:47:30.755703: 
2025-10-05 15:47:30.756027: Epoch 68
2025-10-05 15:47:30.756274: Current learning rate: 0.00581
2025-10-05 15:48:17.157247: Validation loss did not improve from -0.38022. Patience: 45/50
2025-10-05 15:48:17.158056: train_loss -0.7564
2025-10-05 15:48:17.158287: val_loss -0.2889
2025-10-05 15:48:17.158400: Pseudo dice [np.float32(0.6713)]
2025-10-05 15:48:17.158555: Epoch time: 46.4 s
2025-10-05 15:48:17.815922: 
2025-10-05 15:48:17.816208: Epoch 69
2025-10-05 15:48:17.816422: Current learning rate: 0.00574
2025-10-05 15:49:04.231994: Validation loss did not improve from -0.38022. Patience: 46/50
2025-10-05 15:49:04.232493: train_loss -0.7582
2025-10-05 15:49:04.232659: val_loss -0.3085
2025-10-05 15:49:04.232817: Pseudo dice [np.float32(0.6822)]
2025-10-05 15:49:04.232958: Epoch time: 46.42 s
2025-10-05 15:49:05.340022: 
2025-10-05 15:49:05.340304: Epoch 70
2025-10-05 15:49:05.340491: Current learning rate: 0.00568
2025-10-05 15:49:51.711784: Validation loss did not improve from -0.38022. Patience: 47/50
2025-10-05 15:49:51.712379: train_loss -0.758
2025-10-05 15:49:51.712520: val_loss -0.2968
2025-10-05 15:49:51.712629: Pseudo dice [np.float32(0.6746)]
2025-10-05 15:49:51.712758: Epoch time: 46.37 s
2025-10-05 15:49:52.354965: 
2025-10-05 15:49:52.355233: Epoch 71
2025-10-05 15:49:52.355421: Current learning rate: 0.00562
2025-10-05 15:50:38.712686: Validation loss did not improve from -0.38022. Patience: 48/50
2025-10-05 15:50:38.713071: train_loss -0.7686
2025-10-05 15:50:38.713219: val_loss -0.2889
2025-10-05 15:50:38.713339: Pseudo dice [np.float32(0.6767)]
2025-10-05 15:50:38.713491: Epoch time: 46.36 s
2025-10-05 15:50:39.367672: 
2025-10-05 15:50:39.368011: Epoch 72
2025-10-05 15:50:39.368208: Current learning rate: 0.00555
2025-10-05 15:51:25.703100: Validation loss did not improve from -0.38022. Patience: 49/50
2025-10-05 15:51:25.703878: train_loss -0.7647
2025-10-05 15:51:25.704124: val_loss -0.2823
2025-10-05 15:51:25.704315: Pseudo dice [np.float32(0.6614)]
2025-10-05 15:51:25.704543: Epoch time: 46.34 s
2025-10-05 15:51:26.861042: 
2025-10-05 15:51:26.861377: Epoch 73
2025-10-05 15:51:26.861610: Current learning rate: 0.00549
2025-10-05 15:52:13.244249: Validation loss did not improve from -0.38022. Patience: 50/50
2025-10-05 15:52:13.244686: train_loss -0.7746
2025-10-05 15:52:13.244904: val_loss -0.3242
2025-10-05 15:52:13.245092: Pseudo dice [np.float32(0.6892)]
2025-10-05 15:52:13.245319: Epoch time: 46.38 s
2025-10-05 15:52:13.245474: Yayy! New best EMA pseudo Dice: 0.673799991607666
2025-10-05 15:52:14.374930: 
2025-10-05 15:52:14.375282: Epoch 74
2025-10-05 15:52:14.375561: Current learning rate: 0.00542
2025-10-05 15:53:00.810453: Validation loss did not improve from -0.38022. Patience: 51/50
2025-10-05 15:53:00.811132: train_loss -0.7719
2025-10-05 15:53:00.811342: val_loss -0.2802
2025-10-05 15:53:00.811552: Pseudo dice [np.float32(0.6658)]
2025-10-05 15:53:00.811733: Epoch time: 46.44 s
2025-10-05 15:53:01.920283: 
2025-10-05 15:53:01.920596: Epoch 75
2025-10-05 15:53:01.920802: Current learning rate: 0.00536
2025-10-05 15:53:48.344740: Validation loss did not improve from -0.38022. Patience: 52/50
2025-10-05 15:53:48.345244: train_loss -0.7845
2025-10-05 15:53:48.345417: val_loss -0.2727
2025-10-05 15:53:48.345572: Pseudo dice [np.float32(0.6652)]
2025-10-05 15:53:48.345708: Epoch time: 46.43 s
2025-10-05 15:53:48.992292: 
2025-10-05 15:53:48.992591: Epoch 76
2025-10-05 15:53:48.992809: Current learning rate: 0.00529
2025-10-05 15:54:35.410015: Validation loss did not improve from -0.38022. Patience: 53/50
2025-10-05 15:54:35.410684: train_loss -0.7856
2025-10-05 15:54:35.410866: val_loss -0.2576
2025-10-05 15:54:35.411017: Pseudo dice [np.float32(0.6587)]
2025-10-05 15:54:35.411181: Epoch time: 46.42 s
2025-10-05 15:54:36.091623: 
2025-10-05 15:54:36.091997: Epoch 77
2025-10-05 15:54:36.092246: Current learning rate: 0.00523
2025-10-05 15:55:22.477917: Validation loss did not improve from -0.38022. Patience: 54/50
2025-10-05 15:55:22.478436: train_loss -0.7753
2025-10-05 15:55:22.478650: val_loss -0.3087
2025-10-05 15:55:22.478842: Pseudo dice [np.float32(0.6679)]
2025-10-05 15:55:22.479018: Epoch time: 46.39 s
2025-10-05 15:55:23.137030: 
2025-10-05 15:55:23.137290: Epoch 78
2025-10-05 15:55:23.137495: Current learning rate: 0.00517
2025-10-05 15:56:09.550794: Validation loss did not improve from -0.38022. Patience: 55/50
2025-10-05 15:56:09.551499: train_loss -0.7839
2025-10-05 15:56:09.551653: val_loss -0.3003
2025-10-05 15:56:09.551774: Pseudo dice [np.float32(0.6656)]
2025-10-05 15:56:09.551911: Epoch time: 46.42 s
2025-10-05 15:56:10.212977: 
2025-10-05 15:56:10.213310: Epoch 79
2025-10-05 15:56:10.213495: Current learning rate: 0.0051
2025-10-05 15:56:56.613485: Validation loss did not improve from -0.38022. Patience: 56/50
2025-10-05 15:56:56.613917: train_loss -0.788
2025-10-05 15:56:56.614106: val_loss -0.2958
2025-10-05 15:56:56.614239: Pseudo dice [np.float32(0.6739)]
2025-10-05 15:56:56.614384: Epoch time: 46.4 s
2025-10-05 15:56:57.731774: 
2025-10-05 15:56:57.732119: Epoch 80
2025-10-05 15:56:57.732311: Current learning rate: 0.00504
2025-10-05 15:57:44.108052: Validation loss did not improve from -0.38022. Patience: 57/50
2025-10-05 15:57:44.108884: train_loss -0.7936
2025-10-05 15:57:44.109117: val_loss -0.3044
2025-10-05 15:57:44.109306: Pseudo dice [np.float32(0.6842)]
2025-10-05 15:57:44.109520: Epoch time: 46.38 s
2025-10-05 15:57:44.773594: 
2025-10-05 15:57:44.773942: Epoch 81
2025-10-05 15:57:44.774129: Current learning rate: 0.00497
2025-10-05 15:58:31.116404: Validation loss did not improve from -0.38022. Patience: 58/50
2025-10-05 15:58:31.116872: train_loss -0.7898
2025-10-05 15:58:31.117131: val_loss -0.2645
2025-10-05 15:58:31.117275: Pseudo dice [np.float32(0.6699)]
2025-10-05 15:58:31.117426: Epoch time: 46.34 s
2025-10-05 15:58:31.770442: 
2025-10-05 15:58:31.770671: Epoch 82
2025-10-05 15:58:31.770854: Current learning rate: 0.00491
2025-10-05 15:59:18.176854: Validation loss did not improve from -0.38022. Patience: 59/50
2025-10-05 15:59:18.177476: train_loss -0.7887
2025-10-05 15:59:18.177632: val_loss -0.2699
2025-10-05 15:59:18.177780: Pseudo dice [np.float32(0.6633)]
2025-10-05 15:59:18.177949: Epoch time: 46.41 s
2025-10-05 15:59:18.814810: 
2025-10-05 15:59:18.815184: Epoch 83
2025-10-05 15:59:18.815415: Current learning rate: 0.00484
2025-10-05 16:00:05.243605: Validation loss did not improve from -0.38022. Patience: 60/50
2025-10-05 16:00:05.244192: train_loss -0.7974
2025-10-05 16:00:05.244559: val_loss -0.2967
2025-10-05 16:00:05.244935: Pseudo dice [np.float32(0.6769)]
2025-10-05 16:00:05.245458: Epoch time: 46.43 s
2025-10-05 16:00:05.888991: 
2025-10-05 16:00:05.889530: Epoch 84
2025-10-05 16:00:05.889948: Current learning rate: 0.00478
2025-10-05 16:00:52.342112: Validation loss did not improve from -0.38022. Patience: 61/50
2025-10-05 16:00:52.343091: train_loss -0.799
2025-10-05 16:00:52.343372: val_loss -0.279
2025-10-05 16:00:52.343664: Pseudo dice [np.float32(0.6722)]
2025-10-05 16:00:52.343982: Epoch time: 46.45 s
2025-10-05 16:00:53.666102: 
2025-10-05 16:00:53.666433: Epoch 85
2025-10-05 16:00:53.666700: Current learning rate: 0.00471
2025-10-05 16:01:40.023400: Validation loss did not improve from -0.38022. Patience: 62/50
2025-10-05 16:01:40.024037: train_loss -0.8004
2025-10-05 16:01:40.024668: val_loss -0.2777
2025-10-05 16:01:40.024880: Pseudo dice [np.float32(0.6727)]
2025-10-05 16:01:40.025112: Epoch time: 46.36 s
2025-10-05 16:01:40.658913: 
2025-10-05 16:01:40.659278: Epoch 86
2025-10-05 16:01:40.659479: Current learning rate: 0.00465
2025-10-05 16:02:26.976608: Validation loss did not improve from -0.38022. Patience: 63/50
2025-10-05 16:02:26.977674: train_loss -0.8012
2025-10-05 16:02:26.978119: val_loss -0.2716
2025-10-05 16:02:26.978503: Pseudo dice [np.float32(0.6622)]
2025-10-05 16:02:26.979024: Epoch time: 46.32 s
2025-10-05 16:02:27.625329: 
2025-10-05 16:02:27.625891: Epoch 87
2025-10-05 16:02:27.626367: Current learning rate: 0.00458
2025-10-05 16:03:13.983145: Validation loss did not improve from -0.38022. Patience: 64/50
2025-10-05 16:03:13.983787: train_loss -0.8003
2025-10-05 16:03:13.984087: val_loss -0.2532
2025-10-05 16:03:13.984260: Pseudo dice [np.float32(0.672)]
2025-10-05 16:03:13.984416: Epoch time: 46.36 s
2025-10-05 16:03:15.159182: 
2025-10-05 16:03:15.159641: Epoch 88
2025-10-05 16:03:15.159827: Current learning rate: 0.00452
2025-10-05 16:04:01.601791: Validation loss did not improve from -0.38022. Patience: 65/50
2025-10-05 16:04:01.602564: train_loss -0.8057
2025-10-05 16:04:01.602835: val_loss -0.2602
2025-10-05 16:04:01.603008: Pseudo dice [np.float32(0.6606)]
2025-10-05 16:04:01.603175: Epoch time: 46.44 s
2025-10-05 16:04:02.260827: 
2025-10-05 16:04:02.261131: Epoch 89
2025-10-05 16:04:02.261342: Current learning rate: 0.00445
2025-10-05 16:04:48.699980: Validation loss did not improve from -0.38022. Patience: 66/50
2025-10-05 16:04:48.700410: train_loss -0.8031
2025-10-05 16:04:48.700565: val_loss -0.2671
2025-10-05 16:04:48.700674: Pseudo dice [np.float32(0.6643)]
2025-10-05 16:04:48.700828: Epoch time: 46.44 s
2025-10-05 16:04:49.796355: 
2025-10-05 16:04:49.796673: Epoch 90
2025-10-05 16:04:49.796927: Current learning rate: 0.00438
2025-10-05 16:05:36.190536: Validation loss did not improve from -0.38022. Patience: 67/50
2025-10-05 16:05:36.191349: train_loss -0.8108
2025-10-05 16:05:36.191535: val_loss -0.2585
2025-10-05 16:05:36.191713: Pseudo dice [np.float32(0.6648)]
2025-10-05 16:05:36.191896: Epoch time: 46.4 s
2025-10-05 16:05:36.825557: 
2025-10-05 16:05:36.825915: Epoch 91
2025-10-05 16:05:36.826185: Current learning rate: 0.00432
2025-10-05 16:06:23.215942: Validation loss did not improve from -0.38022. Patience: 68/50
2025-10-05 16:06:23.216339: train_loss -0.8134
2025-10-05 16:06:23.216496: val_loss -0.2785
2025-10-05 16:06:23.216609: Pseudo dice [np.float32(0.6708)]
2025-10-05 16:06:23.216749: Epoch time: 46.39 s
2025-10-05 16:06:23.854766: 
2025-10-05 16:06:23.855095: Epoch 92
2025-10-05 16:06:23.855323: Current learning rate: 0.00425
2025-10-05 16:07:10.258025: Validation loss did not improve from -0.38022. Patience: 69/50
2025-10-05 16:07:10.258751: train_loss -0.8121
2025-10-05 16:07:10.258981: val_loss -0.2111
2025-10-05 16:07:10.259172: Pseudo dice [np.float32(0.6805)]
2025-10-05 16:07:10.259342: Epoch time: 46.4 s
2025-10-05 16:07:10.908295: 
2025-10-05 16:07:10.908605: Epoch 93
2025-10-05 16:07:10.908798: Current learning rate: 0.00419
2025-10-05 16:07:57.386584: Validation loss did not improve from -0.38022. Patience: 70/50
2025-10-05 16:07:57.387001: train_loss -0.8137
2025-10-05 16:07:57.387169: val_loss -0.2561
2025-10-05 16:07:57.387292: Pseudo dice [np.float32(0.6726)]
2025-10-05 16:07:57.387445: Epoch time: 46.48 s
2025-10-05 16:07:58.040988: 
2025-10-05 16:07:58.041232: Epoch 94
2025-10-05 16:07:58.041420: Current learning rate: 0.00412
2025-10-05 16:08:44.443841: Validation loss did not improve from -0.38022. Patience: 71/50
2025-10-05 16:08:44.444530: train_loss -0.8217
2025-10-05 16:08:44.444758: val_loss -0.2599
2025-10-05 16:08:44.444943: Pseudo dice [np.float32(0.6682)]
2025-10-05 16:08:44.445142: Epoch time: 46.4 s
2025-10-05 16:08:45.567466: 
2025-10-05 16:08:45.567831: Epoch 95
2025-10-05 16:08:45.568130: Current learning rate: 0.00405
2025-10-05 16:09:31.919585: Validation loss did not improve from -0.38022. Patience: 72/50
2025-10-05 16:09:31.920001: train_loss -0.8219
2025-10-05 16:09:31.920209: val_loss -0.2542
2025-10-05 16:09:31.920331: Pseudo dice [np.float32(0.6818)]
2025-10-05 16:09:31.920502: Epoch time: 46.35 s
2025-10-05 16:09:32.559999: 
2025-10-05 16:09:32.560281: Epoch 96
2025-10-05 16:09:32.560516: Current learning rate: 0.00399
2025-10-05 16:10:18.944192: Validation loss did not improve from -0.38022. Patience: 73/50
2025-10-05 16:10:18.945028: train_loss -0.8147
2025-10-05 16:10:18.945275: val_loss -0.2994
2025-10-05 16:10:18.945488: Pseudo dice [np.float32(0.6849)]
2025-10-05 16:10:18.945699: Epoch time: 46.39 s
2025-10-05 16:10:19.581543: 
2025-10-05 16:10:19.581907: Epoch 97
2025-10-05 16:10:19.582196: Current learning rate: 0.00392
2025-10-05 16:11:05.958713: Validation loss did not improve from -0.38022. Patience: 74/50
2025-10-05 16:11:05.959192: train_loss -0.8295
2025-10-05 16:11:05.959358: val_loss -0.2856
2025-10-05 16:11:05.959497: Pseudo dice [np.float32(0.6704)]
2025-10-05 16:11:05.959647: Epoch time: 46.38 s
2025-10-05 16:11:06.600662: 
2025-10-05 16:11:06.600970: Epoch 98
2025-10-05 16:11:06.601152: Current learning rate: 0.00385
2025-10-05 16:11:53.038553: Validation loss did not improve from -0.38022. Patience: 75/50
2025-10-05 16:11:53.039171: train_loss -0.8235
2025-10-05 16:11:53.039313: val_loss -0.2133
2025-10-05 16:11:53.039423: Pseudo dice [np.float32(0.6567)]
2025-10-05 16:11:53.039548: Epoch time: 46.44 s
2025-10-05 16:11:53.694743: 
2025-10-05 16:11:53.695088: Epoch 99
2025-10-05 16:11:53.695287: Current learning rate: 0.00379
2025-10-05 16:12:40.175160: Validation loss did not improve from -0.38022. Patience: 76/50
2025-10-05 16:12:40.175586: train_loss -0.8244
2025-10-05 16:12:40.175784: val_loss -0.2203
2025-10-05 16:12:40.175973: Pseudo dice [np.float32(0.665)]
2025-10-05 16:12:40.176162: Epoch time: 46.48 s
2025-10-05 16:12:41.292205: 
2025-10-05 16:12:41.292473: Epoch 100
2025-10-05 16:12:41.292662: Current learning rate: 0.00372
2025-10-05 16:13:27.685432: Validation loss did not improve from -0.38022. Patience: 77/50
2025-10-05 16:13:27.686058: train_loss -0.8295
2025-10-05 16:13:27.686226: val_loss -0.2578
2025-10-05 16:13:27.686363: Pseudo dice [np.float32(0.6742)]
2025-10-05 16:13:27.686491: Epoch time: 46.39 s
2025-10-05 16:13:28.330315: 
2025-10-05 16:13:28.330701: Epoch 101
2025-10-05 16:13:28.330935: Current learning rate: 0.00365
2025-10-05 16:14:14.666274: Validation loss did not improve from -0.38022. Patience: 78/50
2025-10-05 16:14:14.666716: train_loss -0.8314
2025-10-05 16:14:14.666898: val_loss -0.2464
2025-10-05 16:14:14.667080: Pseudo dice [np.float32(0.6657)]
2025-10-05 16:14:14.667269: Epoch time: 46.34 s
2025-10-05 16:14:15.311202: 
2025-10-05 16:14:15.311456: Epoch 102
2025-10-05 16:14:15.311651: Current learning rate: 0.00359
2025-10-05 16:15:01.639650: Validation loss did not improve from -0.38022. Patience: 79/50
2025-10-05 16:15:01.640352: train_loss -0.8273
2025-10-05 16:15:01.640498: val_loss -0.2943
2025-10-05 16:15:01.640629: Pseudo dice [np.float32(0.6814)]
2025-10-05 16:15:01.640795: Epoch time: 46.33 s
2025-10-05 16:15:02.295994: 
2025-10-05 16:15:02.296327: Epoch 103
2025-10-05 16:15:02.296535: Current learning rate: 0.00352
2025-10-05 16:15:48.716074: Validation loss did not improve from -0.38022. Patience: 80/50
2025-10-05 16:15:48.716532: train_loss -0.831
2025-10-05 16:15:48.716719: val_loss -0.2494
2025-10-05 16:15:48.716848: Pseudo dice [np.float32(0.6768)]
2025-10-05 16:15:48.717006: Epoch time: 46.42 s
2025-10-05 16:15:49.900151: 
2025-10-05 16:15:49.900564: Epoch 104
2025-10-05 16:15:49.900804: Current learning rate: 0.00345
2025-10-05 16:16:36.295837: Validation loss did not improve from -0.38022. Patience: 81/50
2025-10-05 16:16:36.296793: train_loss -0.8332
2025-10-05 16:16:36.297159: val_loss -0.2854
2025-10-05 16:16:36.297478: Pseudo dice [np.float32(0.6799)]
2025-10-05 16:16:36.297820: Epoch time: 46.4 s
2025-10-05 16:16:37.380939: 
2025-10-05 16:16:37.381300: Epoch 105
2025-10-05 16:16:37.381583: Current learning rate: 0.00338
2025-10-05 16:17:23.761242: Validation loss did not improve from -0.38022. Patience: 82/50
2025-10-05 16:17:23.761600: train_loss -0.8312
2025-10-05 16:17:23.761788: val_loss -0.2882
2025-10-05 16:17:23.761949: Pseudo dice [np.float32(0.6799)]
2025-10-05 16:17:23.762105: Epoch time: 46.38 s
2025-10-05 16:17:24.397199: 
2025-10-05 16:17:24.397554: Epoch 106
2025-10-05 16:17:24.397772: Current learning rate: 0.00332
2025-10-05 16:18:10.888153: Validation loss did not improve from -0.38022. Patience: 83/50
2025-10-05 16:18:10.888808: train_loss -0.8319
2025-10-05 16:18:10.889008: val_loss -0.2595
2025-10-05 16:18:10.889138: Pseudo dice [np.float32(0.663)]
2025-10-05 16:18:10.889538: Epoch time: 46.49 s
2025-10-05 16:18:11.546151: 
2025-10-05 16:18:11.546505: Epoch 107
2025-10-05 16:18:11.546695: Current learning rate: 0.00325
2025-10-05 16:18:58.145046: Validation loss did not improve from -0.38022. Patience: 84/50
2025-10-05 16:18:58.145559: train_loss -0.838
2025-10-05 16:18:58.145711: val_loss -0.2687
2025-10-05 16:18:58.146055: Pseudo dice [np.float32(0.6731)]
2025-10-05 16:18:58.146247: Epoch time: 46.6 s
2025-10-05 16:18:58.805332: 
2025-10-05 16:18:58.805628: Epoch 108
2025-10-05 16:18:58.805828: Current learning rate: 0.00318
2025-10-05 16:19:45.457398: Validation loss did not improve from -0.38022. Patience: 85/50
2025-10-05 16:19:45.457959: train_loss -0.84
2025-10-05 16:19:45.458109: val_loss -0.2484
2025-10-05 16:19:45.458217: Pseudo dice [np.float32(0.674)]
2025-10-05 16:19:45.458343: Epoch time: 46.65 s
2025-10-05 16:19:46.117062: 
2025-10-05 16:19:46.117439: Epoch 109
2025-10-05 16:19:46.117629: Current learning rate: 0.00311
2025-10-05 16:20:32.751500: Validation loss did not improve from -0.38022. Patience: 86/50
2025-10-05 16:20:32.751925: train_loss -0.8461
2025-10-05 16:20:32.752086: val_loss -0.2613
2025-10-05 16:20:32.752213: Pseudo dice [np.float32(0.6653)]
2025-10-05 16:20:32.752357: Epoch time: 46.64 s
2025-10-05 16:20:33.854782: 
2025-10-05 16:20:33.855104: Epoch 110
2025-10-05 16:20:33.855286: Current learning rate: 0.00304
2025-10-05 16:21:20.477585: Validation loss did not improve from -0.38022. Patience: 87/50
2025-10-05 16:21:20.478150: train_loss -0.838
2025-10-05 16:21:20.478355: val_loss -0.256
2025-10-05 16:21:20.478471: Pseudo dice [np.float32(0.6721)]
2025-10-05 16:21:20.478598: Epoch time: 46.62 s
2025-10-05 16:21:21.119802: 
2025-10-05 16:21:21.120139: Epoch 111
2025-10-05 16:21:21.120376: Current learning rate: 0.00297
2025-10-05 16:22:07.698241: Validation loss did not improve from -0.38022. Patience: 88/50
2025-10-05 16:22:07.698607: train_loss -0.8455
2025-10-05 16:22:07.698776: val_loss -0.2287
2025-10-05 16:22:07.698915: Pseudo dice [np.float32(0.6751)]
2025-10-05 16:22:07.699080: Epoch time: 46.58 s
2025-10-05 16:22:08.341433: 
2025-10-05 16:22:08.341676: Epoch 112
2025-10-05 16:22:08.341924: Current learning rate: 0.00291
2025-10-05 16:22:55.024409: Validation loss did not improve from -0.38022. Patience: 89/50
2025-10-05 16:22:55.025155: train_loss -0.8433
2025-10-05 16:22:55.025360: val_loss -0.223
2025-10-05 16:22:55.025533: Pseudo dice [np.float32(0.6692)]
2025-10-05 16:22:55.025769: Epoch time: 46.68 s
2025-10-05 16:22:55.674961: 
2025-10-05 16:22:55.675334: Epoch 113
2025-10-05 16:22:55.675526: Current learning rate: 0.00284
2025-10-05 16:23:42.324408: Validation loss did not improve from -0.38022. Patience: 90/50
2025-10-05 16:23:42.324933: train_loss -0.8432
2025-10-05 16:23:42.325097: val_loss -0.2388
2025-10-05 16:23:42.325242: Pseudo dice [np.float32(0.6619)]
2025-10-05 16:23:42.325427: Epoch time: 46.65 s
2025-10-05 16:23:42.974212: 
2025-10-05 16:23:42.974532: Epoch 114
2025-10-05 16:23:42.974780: Current learning rate: 0.00277
2025-10-05 16:24:29.512960: Validation loss did not improve from -0.38022. Patience: 91/50
2025-10-05 16:24:29.513782: train_loss -0.8466
2025-10-05 16:24:29.513999: val_loss -0.2405
2025-10-05 16:24:29.514191: Pseudo dice [np.float32(0.6718)]
2025-10-05 16:24:29.514364: Epoch time: 46.54 s
2025-10-05 16:24:30.605582: 
2025-10-05 16:24:30.605998: Epoch 115
2025-10-05 16:24:30.606261: Current learning rate: 0.0027
2025-10-05 16:25:17.162625: Validation loss did not improve from -0.38022. Patience: 92/50
2025-10-05 16:25:17.163083: train_loss -0.8474
2025-10-05 16:25:17.163283: val_loss -0.2539
2025-10-05 16:25:17.163438: Pseudo dice [np.float32(0.664)]
2025-10-05 16:25:17.163574: Epoch time: 46.56 s
2025-10-05 16:25:17.804834: 
2025-10-05 16:25:17.805147: Epoch 116
2025-10-05 16:25:17.805376: Current learning rate: 0.00263
2025-10-05 16:26:04.393368: Validation loss did not improve from -0.38022. Patience: 93/50
2025-10-05 16:26:04.394069: train_loss -0.8479
2025-10-05 16:26:04.394309: val_loss -0.258
2025-10-05 16:26:04.394453: Pseudo dice [np.float32(0.6805)]
2025-10-05 16:26:04.394614: Epoch time: 46.59 s
2025-10-05 16:26:05.036384: 
2025-10-05 16:26:05.036721: Epoch 117
2025-10-05 16:26:05.036945: Current learning rate: 0.00256
2025-10-05 16:26:51.694637: Validation loss did not improve from -0.38022. Patience: 94/50
2025-10-05 16:26:51.695083: train_loss -0.8487
2025-10-05 16:26:51.695252: val_loss -0.23
2025-10-05 16:26:51.695405: Pseudo dice [np.float32(0.6639)]
2025-10-05 16:26:51.695551: Epoch time: 46.66 s
2025-10-05 16:26:52.345515: 
2025-10-05 16:26:52.345897: Epoch 118
2025-10-05 16:26:52.346151: Current learning rate: 0.00249
2025-10-05 16:27:39.011652: Validation loss did not improve from -0.38022. Patience: 95/50
2025-10-05 16:27:39.012290: train_loss -0.8553
2025-10-05 16:27:39.012439: val_loss -0.2313
2025-10-05 16:27:39.012572: Pseudo dice [np.float32(0.671)]
2025-10-05 16:27:39.012759: Epoch time: 46.67 s
2025-10-05 16:27:39.664212: 
2025-10-05 16:27:39.664542: Epoch 119
2025-10-05 16:27:39.664730: Current learning rate: 0.00242
2025-10-05 16:28:26.213598: Validation loss did not improve from -0.38022. Patience: 96/50
2025-10-05 16:28:26.214023: train_loss -0.8498
2025-10-05 16:28:26.214216: val_loss -0.1983
2025-10-05 16:28:26.214365: Pseudo dice [np.float32(0.6693)]
2025-10-05 16:28:26.214553: Epoch time: 46.55 s
2025-10-05 16:28:27.822064: 
2025-10-05 16:28:27.822427: Epoch 120
2025-10-05 16:28:27.822634: Current learning rate: 0.00235
2025-10-05 16:29:14.388178: Validation loss did not improve from -0.38022. Patience: 97/50
2025-10-05 16:29:14.388782: train_loss -0.8513
2025-10-05 16:29:14.388945: val_loss -0.2538
2025-10-05 16:29:14.389097: Pseudo dice [np.float32(0.6795)]
2025-10-05 16:29:14.389253: Epoch time: 46.57 s
2025-10-05 16:29:15.041403: 
2025-10-05 16:29:15.041766: Epoch 121
2025-10-05 16:29:15.041964: Current learning rate: 0.00228
2025-10-05 16:30:01.663617: Validation loss did not improve from -0.38022. Patience: 98/50
2025-10-05 16:30:01.664022: train_loss -0.8529
2025-10-05 16:30:01.664251: val_loss -0.2614
2025-10-05 16:30:01.664416: Pseudo dice [np.float32(0.6702)]
2025-10-05 16:30:01.664597: Epoch time: 46.62 s
2025-10-05 16:30:02.321164: 
2025-10-05 16:30:02.321477: Epoch 122
2025-10-05 16:30:02.321663: Current learning rate: 0.00221
2025-10-05 16:30:48.923558: Validation loss did not improve from -0.38022. Patience: 99/50
2025-10-05 16:30:48.924303: train_loss -0.8568
2025-10-05 16:30:48.924442: val_loss -0.1978
2025-10-05 16:30:48.924562: Pseudo dice [np.float32(0.6704)]
2025-10-05 16:30:48.924699: Epoch time: 46.6 s
2025-10-05 16:30:49.585460: 
2025-10-05 16:30:49.585721: Epoch 123
2025-10-05 16:30:49.585907: Current learning rate: 0.00214
2025-10-05 16:31:36.220002: Validation loss did not improve from -0.38022. Patience: 100/50
2025-10-05 16:31:36.220493: train_loss -0.8531
2025-10-05 16:31:36.220682: val_loss -0.2374
2025-10-05 16:31:36.220850: Pseudo dice [np.float32(0.6714)]
2025-10-05 16:31:36.221030: Epoch time: 46.64 s
2025-10-05 16:31:36.868689: 
2025-10-05 16:31:36.869065: Epoch 124
2025-10-05 16:31:36.869249: Current learning rate: 0.00207
2025-10-05 16:32:23.447915: Validation loss did not improve from -0.38022. Patience: 101/50
2025-10-05 16:32:23.448617: train_loss -0.8557
2025-10-05 16:32:23.448778: val_loss -0.2663
2025-10-05 16:32:23.448916: Pseudo dice [np.float32(0.6685)]
2025-10-05 16:32:23.449058: Epoch time: 46.58 s
2025-10-05 16:32:24.550967: 
2025-10-05 16:32:24.551301: Epoch 125
2025-10-05 16:32:24.551510: Current learning rate: 0.00199
2025-10-05 16:33:11.099548: Validation loss did not improve from -0.38022. Patience: 102/50
2025-10-05 16:33:11.100072: train_loss -0.8627
2025-10-05 16:33:11.100238: val_loss -0.2505
2025-10-05 16:33:11.100378: Pseudo dice [np.float32(0.6663)]
2025-10-05 16:33:11.100518: Epoch time: 46.55 s
2025-10-05 16:33:11.754310: 
2025-10-05 16:33:11.754730: Epoch 126
2025-10-05 16:33:11.755004: Current learning rate: 0.00192
2025-10-05 16:33:58.405914: Validation loss did not improve from -0.38022. Patience: 103/50
2025-10-05 16:33:58.406460: train_loss -0.861
2025-10-05 16:33:58.406596: val_loss -0.2245
2025-10-05 16:33:58.406715: Pseudo dice [np.float32(0.6616)]
2025-10-05 16:33:58.406874: Epoch time: 46.65 s
2025-10-05 16:33:59.062310: 
2025-10-05 16:33:59.062616: Epoch 127
2025-10-05 16:33:59.062833: Current learning rate: 0.00185
2025-10-05 16:34:45.731975: Validation loss did not improve from -0.38022. Patience: 104/50
2025-10-05 16:34:45.732652: train_loss -0.8592
2025-10-05 16:34:45.732829: val_loss -0.2337
2025-10-05 16:34:45.732946: Pseudo dice [np.float32(0.6747)]
2025-10-05 16:34:45.733164: Epoch time: 46.67 s
2025-10-05 16:34:46.379479: 
2025-10-05 16:34:46.379713: Epoch 128
2025-10-05 16:34:46.379925: Current learning rate: 0.00178
2025-10-05 16:35:32.990832: Validation loss did not improve from -0.38022. Patience: 105/50
2025-10-05 16:35:32.991789: train_loss -0.8601
2025-10-05 16:35:32.992174: val_loss -0.2654
2025-10-05 16:35:32.992354: Pseudo dice [np.float32(0.6785)]
2025-10-05 16:35:32.992534: Epoch time: 46.61 s
2025-10-05 16:35:33.644105: 
2025-10-05 16:35:33.644446: Epoch 129
2025-10-05 16:35:33.644652: Current learning rate: 0.0017
2025-10-05 16:36:20.229944: Validation loss did not improve from -0.38022. Patience: 106/50
2025-10-05 16:36:20.230399: train_loss -0.8663
2025-10-05 16:36:20.230539: val_loss -0.2483
2025-10-05 16:36:20.230665: Pseudo dice [np.float32(0.6818)]
2025-10-05 16:36:20.230791: Epoch time: 46.59 s
2025-10-05 16:36:21.341596: 
2025-10-05 16:36:21.341950: Epoch 130
2025-10-05 16:36:21.342138: Current learning rate: 0.00163
2025-10-05 16:37:08.023501: Validation loss did not improve from -0.38022. Patience: 107/50
2025-10-05 16:37:08.024138: train_loss -0.8652
2025-10-05 16:37:08.024322: val_loss -0.174
2025-10-05 16:37:08.024477: Pseudo dice [np.float32(0.661)]
2025-10-05 16:37:08.024649: Epoch time: 46.68 s
2025-10-05 16:37:08.675579: 
2025-10-05 16:37:08.675862: Epoch 131
2025-10-05 16:37:08.676077: Current learning rate: 0.00156
2025-10-05 16:37:55.358797: Validation loss did not improve from -0.38022. Patience: 108/50
2025-10-05 16:37:55.359364: train_loss -0.8616
2025-10-05 16:37:55.359601: val_loss -0.2584
2025-10-05 16:37:55.359725: Pseudo dice [np.float32(0.6815)]
2025-10-05 16:37:55.359870: Epoch time: 46.68 s
2025-10-05 16:37:56.002807: 
2025-10-05 16:37:56.003205: Epoch 132
2025-10-05 16:37:56.003453: Current learning rate: 0.00148
2025-10-05 16:38:42.642648: Validation loss did not improve from -0.38022. Patience: 109/50
2025-10-05 16:38:42.643303: train_loss -0.8641
2025-10-05 16:38:42.643504: val_loss -0.2433
2025-10-05 16:38:42.643652: Pseudo dice [np.float32(0.6624)]
2025-10-05 16:38:42.643842: Epoch time: 46.64 s
2025-10-05 16:38:43.282992: 
2025-10-05 16:38:43.283267: Epoch 133
2025-10-05 16:38:43.283491: Current learning rate: 0.00141
2025-10-05 16:39:29.867205: Validation loss did not improve from -0.38022. Patience: 110/50
2025-10-05 16:39:29.867793: train_loss -0.8705
2025-10-05 16:39:29.868110: val_loss -0.1975
2025-10-05 16:39:29.868326: Pseudo dice [np.float32(0.6831)]
2025-10-05 16:39:29.868586: Epoch time: 46.59 s
2025-10-05 16:39:30.517202: 
2025-10-05 16:39:30.517556: Epoch 134
2025-10-05 16:39:30.517782: Current learning rate: 0.00133
2025-10-05 16:40:17.129551: Validation loss did not improve from -0.38022. Patience: 111/50
2025-10-05 16:40:17.130419: train_loss -0.8671
2025-10-05 16:40:17.130599: val_loss -0.2423
2025-10-05 16:40:17.130756: Pseudo dice [np.float32(0.6822)]
2025-10-05 16:40:17.130939: Epoch time: 46.61 s
2025-10-05 16:40:18.230596: 
2025-10-05 16:40:18.230966: Epoch 135
2025-10-05 16:40:18.231171: Current learning rate: 0.00126
2025-10-05 16:41:04.793560: Validation loss did not improve from -0.38022. Patience: 112/50
2025-10-05 16:41:04.794059: train_loss -0.8696
2025-10-05 16:41:04.794286: val_loss -0.2437
2025-10-05 16:41:04.794442: Pseudo dice [np.float32(0.6899)]
2025-10-05 16:41:04.794571: Epoch time: 46.56 s
2025-10-05 16:41:04.794730: Yayy! New best EMA pseudo Dice: 0.6748999953269958
2025-10-05 16:41:06.411482: 
2025-10-05 16:41:06.411730: Epoch 136
2025-10-05 16:41:06.411923: Current learning rate: 0.00118
2025-10-05 16:41:53.061830: Validation loss did not improve from -0.38022. Patience: 113/50
2025-10-05 16:41:53.062592: train_loss -0.8717
2025-10-05 16:41:53.062870: val_loss -0.2332
2025-10-05 16:41:53.063073: Pseudo dice [np.float32(0.6815)]
2025-10-05 16:41:53.063274: Epoch time: 46.65 s
2025-10-05 16:41:53.063510: Yayy! New best EMA pseudo Dice: 0.675599992275238
2025-10-05 16:41:54.195296: 
2025-10-05 16:41:54.195576: Epoch 137
2025-10-05 16:41:54.195814: Current learning rate: 0.00111
2025-10-05 16:42:40.803285: Validation loss did not improve from -0.38022. Patience: 114/50
2025-10-05 16:42:40.803780: train_loss -0.8697
2025-10-05 16:42:40.804032: val_loss -0.2572
2025-10-05 16:42:40.804193: Pseudo dice [np.float32(0.6901)]
2025-10-05 16:42:40.804358: Epoch time: 46.61 s
2025-10-05 16:42:40.804520: Yayy! New best EMA pseudo Dice: 0.6769999861717224
2025-10-05 16:42:41.890360: 
2025-10-05 16:42:41.890683: Epoch 138
2025-10-05 16:42:41.890934: Current learning rate: 0.00103
2025-10-05 16:43:28.489854: Validation loss did not improve from -0.38022. Patience: 115/50
2025-10-05 16:43:28.490416: train_loss -0.8726
2025-10-05 16:43:28.490569: val_loss -0.2654
2025-10-05 16:43:28.490679: Pseudo dice [np.float32(0.6904)]
2025-10-05 16:43:28.490845: Epoch time: 46.6 s
2025-10-05 16:43:28.491006: Yayy! New best EMA pseudo Dice: 0.6783999800682068
2025-10-05 16:43:29.588296: 
2025-10-05 16:43:29.588672: Epoch 139
2025-10-05 16:43:29.588866: Current learning rate: 0.00095
2025-10-05 16:44:16.181636: Validation loss did not improve from -0.38022. Patience: 116/50
2025-10-05 16:44:16.182084: train_loss -0.8711
2025-10-05 16:44:16.182283: val_loss -0.2088
2025-10-05 16:44:16.182403: Pseudo dice [np.float32(0.6798)]
2025-10-05 16:44:16.182534: Epoch time: 46.59 s
2025-10-05 16:44:16.646455: Yayy! New best EMA pseudo Dice: 0.6784999966621399
2025-10-05 16:44:17.716848: 
2025-10-05 16:44:17.717134: Epoch 140
2025-10-05 16:44:17.717350: Current learning rate: 0.00087
2025-10-05 16:45:04.354289: Validation loss did not improve from -0.38022. Patience: 117/50
2025-10-05 16:45:04.354909: train_loss -0.8756
2025-10-05 16:45:04.355095: val_loss -0.2328
2025-10-05 16:45:04.355214: Pseudo dice [np.float32(0.683)]
2025-10-05 16:45:04.355347: Epoch time: 46.64 s
2025-10-05 16:45:04.355510: Yayy! New best EMA pseudo Dice: 0.6789000034332275
2025-10-05 16:45:05.483372: 
2025-10-05 16:45:05.483698: Epoch 141
2025-10-05 16:45:05.483927: Current learning rate: 0.00079
2025-10-05 16:45:52.120193: Validation loss did not improve from -0.38022. Patience: 118/50
2025-10-05 16:45:52.120715: train_loss -0.8731
2025-10-05 16:45:52.120935: val_loss -0.1995
2025-10-05 16:45:52.121123: Pseudo dice [np.float32(0.6681)]
2025-10-05 16:45:52.121305: Epoch time: 46.64 s
2025-10-05 16:45:52.770618: 
2025-10-05 16:45:52.770982: Epoch 142
2025-10-05 16:45:52.771283: Current learning rate: 0.00071
2025-10-05 16:46:39.407289: Validation loss did not improve from -0.38022. Patience: 119/50
2025-10-05 16:46:39.408111: train_loss -0.8729
2025-10-05 16:46:39.408336: val_loss -0.2383
2025-10-05 16:46:39.408535: Pseudo dice [np.float32(0.6819)]
2025-10-05 16:46:39.408756: Epoch time: 46.64 s
2025-10-05 16:46:40.059397: 
2025-10-05 16:46:40.059713: Epoch 143
2025-10-05 16:46:40.059986: Current learning rate: 0.00063
2025-10-05 16:47:26.704473: Validation loss did not improve from -0.38022. Patience: 120/50
2025-10-05 16:47:26.705194: train_loss -0.8764
2025-10-05 16:47:26.705588: val_loss -0.2419
2025-10-05 16:47:26.705891: Pseudo dice [np.float32(0.6783)]
2025-10-05 16:47:26.706067: Epoch time: 46.65 s
2025-10-05 16:47:27.358037: 
2025-10-05 16:47:27.358351: Epoch 144
2025-10-05 16:47:27.358585: Current learning rate: 0.00055
2025-10-05 16:48:13.986173: Validation loss did not improve from -0.38022. Patience: 121/50
2025-10-05 16:48:13.986843: train_loss -0.8776
2025-10-05 16:48:13.986979: val_loss -0.202
2025-10-05 16:48:13.987087: Pseudo dice [np.float32(0.67)]
2025-10-05 16:48:13.987208: Epoch time: 46.63 s
2025-10-05 16:48:15.093446: 
2025-10-05 16:48:15.093805: Epoch 145
2025-10-05 16:48:15.093997: Current learning rate: 0.00047
2025-10-05 16:49:01.767442: Validation loss did not improve from -0.38022. Patience: 122/50
2025-10-05 16:49:01.767970: train_loss -0.8735
2025-10-05 16:49:01.768166: val_loss -0.2279
2025-10-05 16:49:01.768334: Pseudo dice [np.float32(0.6682)]
2025-10-05 16:49:01.768679: Epoch time: 46.68 s
2025-10-05 16:49:02.424500: 
2025-10-05 16:49:02.424843: Epoch 146
2025-10-05 16:49:02.425029: Current learning rate: 0.00038
2025-10-05 16:49:49.093561: Validation loss did not improve from -0.38022. Patience: 123/50
2025-10-05 16:49:49.094115: train_loss -0.8747
2025-10-05 16:49:49.094328: val_loss -0.2142
2025-10-05 16:49:49.094449: Pseudo dice [np.float32(0.6811)]
2025-10-05 16:49:49.094579: Epoch time: 46.67 s
2025-10-05 16:49:49.739955: 
2025-10-05 16:49:49.740246: Epoch 147
2025-10-05 16:49:49.740451: Current learning rate: 0.0003
2025-10-05 16:50:36.366169: Validation loss did not improve from -0.38022. Patience: 124/50
2025-10-05 16:50:36.366626: train_loss -0.877
2025-10-05 16:50:36.366793: val_loss -0.2101
2025-10-05 16:50:36.366910: Pseudo dice [np.float32(0.6801)]
2025-10-05 16:50:36.367098: Epoch time: 46.63 s
2025-10-05 16:50:37.029946: 
2025-10-05 16:50:37.030291: Epoch 148
2025-10-05 16:50:37.030534: Current learning rate: 0.00021
2025-10-05 16:51:23.644533: Validation loss did not improve from -0.38022. Patience: 125/50
2025-10-05 16:51:23.645144: train_loss -0.8763
2025-10-05 16:51:23.645344: val_loss -0.2363
2025-10-05 16:51:23.645518: Pseudo dice [np.float32(0.6803)]
2025-10-05 16:51:23.645675: Epoch time: 46.62 s
2025-10-05 16:51:24.291718: 
2025-10-05 16:51:24.292063: Epoch 149
2025-10-05 16:51:24.292290: Current learning rate: 0.00011
2025-10-05 16:52:10.928706: Validation loss did not improve from -0.38022. Patience: 126/50
2025-10-05 16:52:10.929186: train_loss -0.8769
2025-10-05 16:52:10.929339: val_loss -0.2569
2025-10-05 16:52:10.929464: Pseudo dice [np.float32(0.6938)]
2025-10-05 16:52:10.929609: Epoch time: 46.64 s
2025-10-05 16:52:10.929743: Yayy! New best EMA pseudo Dice: 0.6791999936103821
2025-10-05 16:52:12.506129: Training done.
2025-10-05 16:52:12.546994: Using splits from existing split file: /nfs/erelab001/shared/Computational_Group/Naravich/datasets/nnUNet_Datasets/nnUNet_preprocessed/Dataset310_nnInteractive_Calcium_OCT_CrossValidation/splits_final_80.json
2025-10-05 16:52:12.547692: The split file contains 5 splits.
2025-10-05 16:52:12.547851: Desired fold for training: 1
2025-10-05 16:52:12.547992: This split has 6 training and 4 validation cases.
2025-10-05 16:52:12.548237: predicting 106-002
2025-10-05 16:52:12.554857: 106-002, shape torch.Size([1, 539, 498, 498]), rank 0
2025-10-05 16:53:15.145141: predicting 401-004
2025-10-05 16:53:15.164575: 401-004, shape torch.Size([1, 375, 498, 498]), rank 0
2025-10-05 16:53:49.737366: predicting 704-003
2025-10-05 16:53:49.750413: 704-003, shape torch.Size([1, 375, 498, 498]), rank 0
2025-10-05 16:54:24.266904: predicting 706-005
2025-10-05 16:54:24.281981: 706-005, shape torch.Size([1, 375, 498, 498]), rank 0
2025-10-05 16:55:12.088605: Validation complete
2025-10-05 16:55:12.089109: Mean Validation Dice:  0.6504745855458465
Finished training fold 1 saving to /nfs/erelab001/shared/Computational_Group/Naravich/datasets/nnUNet_Datasets/nnUNet_results/Dataset310_nnInteractive_Calcium_OCT_CrossValidation/nnUNetTrainerScaleAnalysis80__nnUNetPlans__3d_32x160x128_b10/fold_1_No_Pretrained
