/nfs/erelab001/shared/Computational_Group/Naravich/nnUNet/.venv/bin/python
Starting training with CONFIG=3d_32x160x128_b10, DATASET_ID=307, TRAINER=nnUNetTrainerScaleAnalysis80
/nfs/erelab001/shared/Computational_Group/Naravich/nnUNet/nnunetv2/training/nnUNetTrainer/nnUNetTrainer.py:169: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  self.grad_scaler = GradScaler() if self.device.type == 'cuda' else None

############################
INFO: You are using the old nnU-Net default plans. We have updated our recommendations. Please consider using those instead! Read more here: https://github.com/MIC-DKFZ/nnUNet/blob/master/documentation/resenc_presets.md
############################

Using device: cuda:0

#######################################################################
Please cite the following paper when using nnU-Net:
Isensee, F., Jaeger, P. F., Kohl, S. A., Petersen, J., & Maier-Hein, K. H. (2021). nnU-Net: a self-configuring method for deep learning-based biomedical image segmentation. Nature methods, 18(2), 203-211.
#######################################################################

2025-10-15 09:31:28.522389: Using torch.compile...
################### Loading pretrained weights from file  /nfs/erelab001/shared/Computational_Group/Naravich/datasets/nnUNet_Datasets/../ModelGenesisOutputs/ModelGenesisNNUNetPretrainingV2_noNorm_correct_orientation/Converted_nnUNet_Genesis_OCT_Best.pt ###################
Below is the list of overlapping blocks in pretrained model and nnUNet architecture:
encoder.stages.0.0.convs.0.conv.weight shape torch.Size([32, 1, 3, 3, 3])
encoder.stages.0.0.convs.0.conv.bias shape torch.Size([32])
encoder.stages.0.0.convs.0.norm.weight shape torch.Size([32])
encoder.stages.0.0.convs.0.norm.bias shape torch.Size([32])
encoder.stages.0.0.convs.0.all_modules.0.weight shape torch.Size([32, 1, 3, 3, 3])
encoder.stages.0.0.convs.0.all_modules.0.bias shape torch.Size([32])
encoder.stages.0.0.convs.0.all_modules.1.weight shape torch.Size([32])
encoder.stages.0.0.convs.0.all_modules.1.bias shape torch.Size([32])
encoder.stages.0.0.convs.1.conv.weight shape torch.Size([32, 32, 3, 3, 3])
encoder.stages.0.0.convs.1.conv.bias shape torch.Size([32])
encoder.stages.0.0.convs.1.norm.weight shape torch.Size([32])
encoder.stages.0.0.convs.1.norm.bias shape torch.Size([32])
encoder.stages.0.0.convs.1.all_modules.0.weight shape torch.Size([32, 32, 3, 3, 3])
encoder.stages.0.0.convs.1.all_modules.0.bias shape torch.Size([32])
encoder.stages.0.0.convs.1.all_modules.1.weight shape torch.Size([32])
encoder.stages.0.0.convs.1.all_modules.1.bias shape torch.Size([32])
encoder.stages.1.0.convs.0.conv.weight shape torch.Size([64, 32, 3, 3, 3])
encoder.stages.1.0.convs.0.conv.bias shape torch.Size([64])
encoder.stages.1.0.convs.0.norm.weight shape torch.Size([64])
encoder.stages.1.0.convs.0.norm.bias shape torch.Size([64])
encoder.stages.1.0.convs.0.all_modules.0.weight shape torch.Size([64, 32, 3, 3, 3])
encoder.stages.1.0.convs.0.all_modules.0.bias shape torch.Size([64])
encoder.stages.1.0.convs.0.all_modules.1.weight shape torch.Size([64])
encoder.stages.1.0.convs.0.all_modules.1.bias shape torch.Size([64])
encoder.stages.1.0.convs.1.conv.weight shape torch.Size([64, 64, 3, 3, 3])
encoder.stages.1.0.convs.1.conv.bias shape torch.Size([64])
encoder.stages.1.0.convs.1.norm.weight shape torch.Size([64])
encoder.stages.1.0.convs.1.norm.bias shape torch.Size([64])
encoder.stages.1.0.convs.1.all_modules.0.weight shape torch.Size([64, 64, 3, 3, 3])
encoder.stages.1.0.convs.1.all_modules.0.bias shape torch.Size([64])
encoder.stages.1.0.convs.1.all_modules.1.weight shape torch.Size([64])
encoder.stages.1.0.convs.1.all_modules.1.bias shape torch.Size([64])
encoder.stages.2.0.convs.0.conv.weight shape torch.Size([128, 64, 3, 3, 3])
encoder.stages.2.0.convs.0.conv.bias shape torch.Size([128])
encoder.stages.2.0.convs.0.norm.weight shape torch.Size([128])
encoder.stages.2.0.convs.0.norm.bias shape torch.Size([128])
encoder.stages.2.0.convs.0.all_modules.0.weight shape torch.Size([128, 64, 3, 3, 3])
encoder.stages.2.0.convs.0.all_modules.0.bias shape torch.Size([128])
encoder.stages.2.0.convs.0.all_modules.1.weight shape torch.Size([128])
encoder.stages.2.0.convs.0.all_modules.1.bias shape torch.Size([128])
encoder.stages.2.0.convs.1.conv.weight shape torch.Size([128, 128, 3, 3, 3])
encoder.stages.2.0.convs.1.conv.bias shape torch.Size([128])
encoder.stages.2.0.convs.1.norm.weight shape torch.Size([128])
encoder.stages.2.0.convs.1.norm.bias shape torch.Size([128])
encoder.stages.2.0.convs.1.all_modules.0.weight shape torch.Size([128, 128, 3, 3, 3])
encoder.stages.2.0.convs.1.all_modules.0.bias shape torch.Size([128])
encoder.stages.2.0.convs.1.all_modules.1.weight shape torch.Size([128])
encoder.stages.2.0.convs.1.all_modules.1.bias shape torch.Size([128])
encoder.stages.3.0.convs.0.conv.weight shape torch.Size([256, 128, 3, 3, 3])
encoder.stages.3.0.convs.0.conv.bias shape torch.Size([256])
encoder.stages.3.0.convs.0.norm.weight shape torch.Size([256])
encoder.stages.3.0.convs.0.norm.bias shape torch.Size([256])
encoder.stages.3.0.convs.0.all_modules.0.weight shape torch.Size([256, 128, 3, 3, 3])
encoder.stages.3.0.convs.0.all_modules.0.bias shape torch.Size([256])
encoder.stages.3.0.convs.0.all_modules.1.weight shape torch.Size([256])
encoder.stages.3.0.convs.0.all_modules.1.bias shape torch.Size([256])
encoder.stages.3.0.convs.1.conv.weight shape torch.Size([256, 256, 3, 3, 3])
encoder.stages.3.0.convs.1.conv.bias shape torch.Size([256])
encoder.stages.3.0.convs.1.norm.weight shape torch.Size([256])
encoder.stages.3.0.convs.1.norm.bias shape torch.Size([256])
encoder.stages.3.0.convs.1.all_modules.0.weight shape torch.Size([256, 256, 3, 3, 3])
encoder.stages.3.0.convs.1.all_modules.0.bias shape torch.Size([256])
encoder.stages.3.0.convs.1.all_modules.1.weight shape torch.Size([256])
encoder.stages.3.0.convs.1.all_modules.1.bias shape torch.Size([256])
encoder.stages.4.0.convs.0.conv.weight shape torch.Size([320, 256, 3, 3, 3])
encoder.stages.4.0.convs.0.conv.bias shape torch.Size([320])
encoder.stages.4.0.convs.0.norm.weight shape torch.Size([320])
encoder.stages.4.0.convs.0.norm.bias shape torch.Size([320])
encoder.stages.4.0.convs.0.all_modules.0.weight shape torch.Size([320, 256, 3, 3, 3])
encoder.stages.4.0.convs.0.all_modules.0.bias shape torch.Size([320])
encoder.stages.4.0.convs.0.all_modules.1.weight shape torch.Size([320])
encoder.stages.4.0.convs.0.all_modules.1.bias shape torch.Size([320])
encoder.stages.4.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.4.0.convs.1.conv.bias shape torch.Size([320])
encoder.stages.4.0.convs.1.norm.weight shape torch.Size([320])
encoder.stages.4.0.convs.1.norm.bias shape torch.Size([320])
encoder.stages.4.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.4.0.convs.1.all_modules.0.bias shape torch.Size([320])
encoder.stages.4.0.convs.1.all_modules.1.weight shape torch.Size([320])
encoder.stages.4.0.convs.1.all_modules.1.bias shape torch.Size([320])
encoder.stages.5.0.convs.0.conv.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.5.0.convs.0.conv.bias shape torch.Size([320])
encoder.stages.5.0.convs.0.norm.weight shape torch.Size([320])
encoder.stages.5.0.convs.0.norm.bias shape torch.Size([320])
encoder.stages.5.0.convs.0.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.5.0.convs.0.all_modules.0.bias shape torch.Size([320])
encoder.stages.5.0.convs.0.all_modules.1.weight shape torch.Size([320])
encoder.stages.5.0.convs.0.all_modules.1.bias shape torch.Size([320])
encoder.stages.5.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.5.0.convs.1.conv.bias shape torch.Size([320])
encoder.stages.5.0.convs.1.norm.weight shape torch.Size([320])
encoder.stages.5.0.convs.1.norm.bias shape torch.Size([320])
encoder.stages.5.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.5.0.convs.1.all_modules.0.bias shape torch.Size([320])
encoder.stages.5.0.convs.1.all_modules.1.weight shape torch.Size([320])
encoder.stages.5.0.convs.1.all_modules.1.bias shape torch.Size([320])
decoder.encoder.stages.0.0.convs.0.conv.weight shape torch.Size([32, 1, 3, 3, 3])
decoder.encoder.stages.0.0.convs.0.conv.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.norm.weight shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.norm.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.all_modules.0.weight shape torch.Size([32, 1, 3, 3, 3])
decoder.encoder.stages.0.0.convs.0.all_modules.0.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.all_modules.1.weight shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.all_modules.1.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.conv.weight shape torch.Size([32, 32, 3, 3, 3])
decoder.encoder.stages.0.0.convs.1.conv.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.norm.weight shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.norm.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.all_modules.0.weight shape torch.Size([32, 32, 3, 3, 3])
decoder.encoder.stages.0.0.convs.1.all_modules.0.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.all_modules.1.weight shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.all_modules.1.bias shape torch.Size([32])
decoder.encoder.stages.1.0.convs.0.conv.weight shape torch.Size([64, 32, 3, 3, 3])
decoder.encoder.stages.1.0.convs.0.conv.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.norm.weight shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.norm.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.all_modules.0.weight shape torch.Size([64, 32, 3, 3, 3])
decoder.encoder.stages.1.0.convs.0.all_modules.0.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.all_modules.1.weight shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.all_modules.1.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.conv.weight shape torch.Size([64, 64, 3, 3, 3])
decoder.encoder.stages.1.0.convs.1.conv.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.norm.weight shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.norm.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.all_modules.0.weight shape torch.Size([64, 64, 3, 3, 3])
decoder.encoder.stages.1.0.convs.1.all_modules.0.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.all_modules.1.weight shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.all_modules.1.bias shape torch.Size([64])
decoder.encoder.stages.2.0.convs.0.conv.weight shape torch.Size([128, 64, 3, 3, 3])
decoder.encoder.stages.2.0.convs.0.conv.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.norm.weight shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.norm.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.all_modules.0.weight shape torch.Size([128, 64, 3, 3, 3])
decoder.encoder.stages.2.0.convs.0.all_modules.0.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.all_modules.1.weight shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.all_modules.1.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.conv.weight shape torch.Size([128, 128, 3, 3, 3])
decoder.encoder.stages.2.0.convs.1.conv.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.norm.weight shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.norm.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.all_modules.0.weight shape torch.Size([128, 128, 3, 3, 3])
decoder.encoder.stages.2.0.convs.1.all_modules.0.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.all_modules.1.weight shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.all_modules.1.bias shape torch.Size([128])
decoder.encoder.stages.3.0.convs.0.conv.weight shape torch.Size([256, 128, 3, 3, 3])
decoder.encoder.stages.3.0.convs.0.conv.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.norm.weight shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.norm.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.all_modules.0.weight shape torch.Size([256, 128, 3, 3, 3])
decoder.encoder.stages.3.0.convs.0.all_modules.0.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.all_modules.1.weight shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.all_modules.1.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.conv.weight shape torch.Size([256, 256, 3, 3, 3])
decoder.encoder.stages.3.0.convs.1.conv.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.norm.weight shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.norm.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.all_modules.0.weight shape torch.Size([256, 256, 3, 3, 3])
decoder.encoder.stages.3.0.convs.1.all_modules.0.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.all_modules.1.weight shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.all_modules.1.bias shape torch.Size([256])
decoder.encoder.stages.4.0.convs.0.conv.weight shape torch.Size([320, 256, 3, 3, 3])
decoder.encoder.stages.4.0.convs.0.conv.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.norm.weight shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.norm.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.all_modules.0.weight shape torch.Size([320, 256, 3, 3, 3])
decoder.encoder.stages.4.0.convs.0.all_modules.0.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.all_modules.1.weight shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.all_modules.1.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.4.0.convs.1.conv.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.norm.weight shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.norm.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.4.0.convs.1.all_modules.0.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.all_modules.1.weight shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.all_modules.1.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.conv.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.5.0.convs.0.conv.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.norm.weight shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.norm.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.5.0.convs.0.all_modules.0.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.all_modules.1.weight shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.all_modules.1.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.5.0.convs.1.conv.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.norm.weight shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.norm.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.5.0.convs.1.all_modules.0.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.all_modules.1.weight shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.all_modules.1.bias shape torch.Size([320])
decoder.stages.0.convs.0.conv.weight shape torch.Size([320, 640, 3, 3, 3])
decoder.stages.0.convs.0.conv.bias shape torch.Size([320])
decoder.stages.0.convs.0.norm.weight shape torch.Size([320])
decoder.stages.0.convs.0.norm.bias shape torch.Size([320])
decoder.stages.0.convs.0.all_modules.0.weight shape torch.Size([320, 640, 3, 3, 3])
decoder.stages.0.convs.0.all_modules.0.bias shape torch.Size([320])
decoder.stages.0.convs.0.all_modules.1.weight shape torch.Size([320])
decoder.stages.0.convs.0.all_modules.1.bias shape torch.Size([320])
decoder.stages.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.stages.0.convs.1.conv.bias shape torch.Size([320])
decoder.stages.0.convs.1.norm.weight shape torch.Size([320])
decoder.stages.0.convs.1.norm.bias shape torch.Size([320])
decoder.stages.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.stages.0.convs.1.all_modules.0.bias shape torch.Size([320])
decoder.stages.0.convs.1.all_modules.1.weight shape torch.Size([320])
decoder.stages.0.convs.1.all_modules.1.bias shape torch.Size([320])
decoder.stages.1.convs.0.conv.weight shape torch.Size([256, 512, 3, 3, 3])
decoder.stages.1.convs.0.conv.bias shape torch.Size([256])
decoder.stages.1.convs.0.norm.weight shape torch.Size([256])
decoder.stages.1.convs.0.norm.bias shape torch.Size([256])
decoder.stages.1.convs.0.all_modules.0.weight shape torch.Size([256, 512, 3, 3, 3])
decoder.stages.1.convs.0.all_modules.0.bias shape torch.Size([256])
decoder.stages.1.convs.0.all_modules.1.weight shape torch.Size([256])
decoder.stages.1.convs.0.all_modules.1.bias shape torch.Size([256])
decoder.stages.1.convs.1.conv.weight shape torch.Size([256, 256, 3, 3, 3])
decoder.stages.1.convs.1.conv.bias shape torch.Size([256])
decoder.stages.1.convs.1.norm.weight shape torch.Size([256])
decoder.stages.1.convs.1.norm.bias shape torch.Size([256])
decoder.stages.1.convs.1.all_modules.0.weight shape torch.Size([256, 256, 3, 3, 3])
decoder.stages.1.convs.1.all_modules.0.bias shape torch.Size([256])
decoder.stages.1.convs.1.all_modules.1.weight shape torch.Size([256])
decoder.stages.1.convs.1.all_modules.1.bias shape torch.Size([256])
decoder.stages.2.convs.0.conv.weight shape torch.Size([128, 256, 3, 3, 3])
decoder.stages.2.convs.0.conv.bias shape torch.Size([128])
decoder.stages.2.convs.0.norm.weight shape torch.Size([128])
decoder.stages.2.convs.0.norm.bias shape torch.Size([128])
decoder.stages.2.convs.0.all_modules.0.weight shape torch.Size([128, 256, 3, 3, 3])
decoder.stages.2.convs.0.all_modules.0.bias shape torch.Size([128])
decoder.stages.2.convs.0.all_modules.1.weight shape torch.Size([128])
decoder.stages.2.convs.0.all_modules.1.bias shape torch.Size([128])
decoder.stages.2.convs.1.conv.weight shape torch.Size([128, 128, 3, 3, 3])
decoder.stages.2.convs.1.conv.bias shape torch.Size([128])
decoder.stages.2.convs.1.norm.weight shape torch.Size([128])
decoder.stages.2.convs.1.norm.bias shape torch.Size([128])
decoder.stages.2.convs.1.all_modules.0.weight shape torch.Size([128, 128, 3, 3, 3])
decoder.stages.2.convs.1.all_modules.0.bias shape torch.Size([128])
decoder.stages.2.convs.1.all_modules.1.weight shape torch.Size([128])
decoder.stages.2.convs.1.all_modules.1.bias shape torch.Size([128])
decoder.stages.3.convs.0.conv.weight shape torch.Size([64, 128, 3, 3, 3])
decoder.stages.3.convs.0.conv.bias shape torch.Size([64])
decoder.stages.3.convs.0.norm.weight shape torch.Size([64])
decoder.stages.3.convs.0.norm.bias shape torch.Size([64])
decoder.stages.3.convs.0.all_modules.0.weight shape torch.Size([64, 128, 3, 3, 3])
decoder.stages.3.convs.0.all_modules.0.bias shape torch.Size([64])
decoder.stages.3.convs.0.all_modules.1.weight shape torch.Size([64])
decoder.stages.3.convs.0.all_modules.1.bias shape torch.Size([64])
decoder.stages.3.convs.1.conv.weight shape torch.Size([64, 64, 3, 3, 3])
decoder.stages.3.convs.1.conv.bias shape torch.Size([64])
decoder.stages.3.convs.1.norm.weight shape torch.Size([64])
decoder.stages.3.convs.1.norm.bias shape torch.Size([64])
decoder.stages.3.convs.1.all_modules.0.weight shape torch.Size([64, 64, 3, 3, 3])
decoder.stages.3.convs.1.all_modules.0.bias shape torch.Size([64])
decoder.stages.3.convs.1.all_modules.1.weight shape torch.Size([64])
decoder.stages.3.convs.1.all_modules.1.bias shape torch.Size([64])
decoder.stages.4.convs.0.conv.weight shape torch.Size([32, 64, 3, 3, 3])
decoder.stages.4.convs.0.conv.bias shape torch.Size([32])
decoder.stages.4.convs.0.norm.weight shape torch.Size([32])
decoder.stages.4.convs.0.norm.bias shape torch.Size([32])
decoder.stages.4.convs.0.all_modules.0.weight shape torch.Size([32, 64, 3, 3, 3])
decoder.stages.4.convs.0.all_modules.0.bias shape torch.Size([32])
decoder.stages.4.convs.0.all_modules.1.weight shape torch.Size([32])
decoder.stages.4.convs.0.all_modules.1.bias shape torch.Size([32])
decoder.stages.4.convs.1.conv.weight shape torch.Size([32, 32, 3, 3, 3])
decoder.stages.4.convs.1.conv.bias shape torch.Size([32])
decoder.stages.4.convs.1.norm.weight shape torch.Size([32])
decoder.stages.4.convs.1.norm.bias shape torch.Size([32])
decoder.stages.4.convs.1.all_modules.0.weight shape torch.Size([32, 32, 3, 3, 3])
decoder.stages.4.convs.1.all_modules.0.bias shape torch.Size([32])
decoder.stages.4.convs.1.all_modules.1.weight shape torch.Size([32])
decoder.stages.4.convs.1.all_modules.1.bias shape torch.Size([32])
decoder.transpconvs.0.weight shape torch.Size([320, 320, 1, 2, 2])
decoder.transpconvs.0.bias shape torch.Size([320])
decoder.transpconvs.1.weight shape torch.Size([320, 256, 2, 2, 2])
decoder.transpconvs.1.bias shape torch.Size([256])
decoder.transpconvs.2.weight shape torch.Size([256, 128, 2, 2, 2])
decoder.transpconvs.2.bias shape torch.Size([128])
decoder.transpconvs.3.weight shape torch.Size([128, 64, 2, 2, 2])
decoder.transpconvs.3.bias shape torch.Size([64])
decoder.transpconvs.4.weight shape torch.Size([64, 32, 2, 2, 2])
decoder.transpconvs.4.bias shape torch.Size([32])
################### Done ###################
2025-10-15 09:31:29.770537: do_dummy_2d_data_aug: True
2025-10-15 09:31:29.770983: Using splits from existing split file: /nfs/erelab001/shared/Computational_Group/Naravich/datasets/nnUNet_Datasets/nnUNet_preprocessed/Dataset307_Sohee_Calcium_OCT_CrossValidation/splits_final_80.json
2025-10-15 09:31:29.771185: The split file contains 5 splits.
2025-10-15 09:31:29.771342: Desired fold for training: 4
2025-10-15 09:31:29.771559: This split has 6 training and 5 validation cases.
using pin_memory on device 0
/nfs/erelab001/shared/Computational_Group/Naravich/nnUNet/.venv/lib/python3.12/site-packages/fft_conv_pytorch/fft_conv.py:139: UserWarning: Using a non-tuple sequence for multidimensional indexing is deprecated and will be changed in pytorch 2.9; use x[tuple(seq)] instead of x[seq]. In pytorch 2.9 this will be interpreted as tensor index, x[torch.tensor(seq)], which will result either in an error or a different result (Triggered internally at /pytorch/torch/csrc/autograd/python_variable_indexing.cpp:306.)
  output = output[crop_slices].contiguous()
/nfs/erelab001/shared/Computational_Group/Naravich/nnUNet/.venv/lib/python3.12/site-packages/fft_conv_pytorch/fft_conv.py:139: UserWarning: Using a non-tuple sequence for multidimensional indexing is deprecated and will be changed in pytorch 2.9; use x[tuple(seq)] instead of x[seq]. In pytorch 2.9 this will be interpreted as tensor index, x[torch.tensor(seq)], which will result either in an error or a different result (Triggered internally at /pytorch/torch/csrc/autograd/python_variable_indexing.cpp:306.)
  output = output[crop_slices].contiguous()
using pin_memory on device 0

This is the configuration used by this training:
Configuration name: 3d_32x160x128_b10
 {'data_identifier': 'nnUNetPlans_3d_fullres', 'preprocessor_name': 'DefaultPreprocessor', 'batch_size': 10, 'patch_size': [32, 160, 128], 'median_image_size_in_voxels': [375.0, 498.0, 498.0], 'spacing': [1.0, 1.0, 1.0], 'normalization_schemes': ['ZScoreNormalization'], 'use_mask_for_norm': [False], 'resampling_fn_data': 'resample_data_or_seg_to_shape', 'resampling_fn_seg': 'resample_data_or_seg_to_shape', 'resampling_fn_data_kwargs': {'is_seg': False, 'order': 3, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_seg_kwargs': {'is_seg': True, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_probabilities': 'resample_data_or_seg_to_shape', 'resampling_fn_probabilities_kwargs': {'is_seg': False, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'architecture': {'network_class_name': 'dynamic_network_architectures.architectures.unet.PlainConvUNet', 'arch_kwargs': {'n_stages': 6, 'features_per_stage': [32, 64, 128, 256, 320, 320], 'conv_op': 'torch.nn.modules.conv.Conv3d', 'kernel_sizes': [[3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3]], 'strides': [[1, 1, 1], [2, 2, 2], [2, 2, 2], [2, 2, 2], [2, 2, 2], [1, 2, 2]], 'n_conv_per_stage': [2, 2, 2, 2, 2, 2], 'n_conv_per_stage_decoder': [2, 2, 2, 2, 2], 'conv_bias': True, 'norm_op': 'torch.nn.modules.instancenorm.InstanceNorm3d', 'norm_op_kwargs': {'eps': 1e-05, 'affine': True}, 'dropout_op': None, 'dropout_op_kwargs': None, 'nonlin': 'torch.nn.LeakyReLU', 'nonlin_kwargs': {'inplace': True}, 'deep_supervision': True}, '_kw_requires_import': ['conv_op', 'norm_op', 'dropout_op', 'nonlin']}, 'batch_dice': True, 'inherits_from': '3d_fullres'} 

These are the global plan.json settings:
 {'dataset_name': 'Dataset307_Sohee_Calcium_OCT_CrossValidation', 'plans_name': 'nnUNetPlans', 'original_median_spacing_after_transp': [1.0, 1.0, 1.0], 'original_median_shape_after_transp': [375, 498, 498], 'image_reader_writer': 'SimpleITKIO', 'transpose_forward': [0, 1, 2], 'transpose_backward': [0, 1, 2], 'experiment_planner_used': 'ExperimentPlanner', 'label_manager': 'LabelManager', 'foreground_intensity_properties_per_channel': {'0': {'max': 0.4977976381778717, 'mean': 0.13507525622844696, 'median': 0.09599608182907104, 'min': 0.0, 'percentile_00_5': 0.014754901640117168, 'percentile_99_5': 0.4977976381778717, 'std': 0.12025152146816254}}} 

2025-10-15 09:31:31.919313: unpacking dataset...
/nfs/erelab001/shared/Computational_Group/Naravich/nnUNet/.venv/lib/python3.12/site-packages/fft_conv_pytorch/fft_conv.py:139: UserWarning: Using a non-tuple sequence for multidimensional indexing is deprecated and will be changed in pytorch 2.9; use x[tuple(seq)] instead of x[seq]. In pytorch 2.9 this will be interpreted as tensor index, x[torch.tensor(seq)], which will result either in an error or a different result (Triggered internally at /pytorch/torch/csrc/autograd/python_variable_indexing.cpp:306.)
  output = output[crop_slices].contiguous()
/nfs/erelab001/shared/Computational_Group/Naravich/nnUNet/.venv/lib/python3.12/site-packages/fft_conv_pytorch/fft_conv.py:139: UserWarning: Using a non-tuple sequence for multidimensional indexing is deprecated and will be changed in pytorch 2.9; use x[tuple(seq)] instead of x[seq]. In pytorch 2.9 this will be interpreted as tensor index, x[torch.tensor(seq)], which will result either in an error or a different result (Triggered internally at /pytorch/torch/csrc/autograd/python_variable_indexing.cpp:306.)
  output = output[crop_slices].contiguous()
/nfs/erelab001/shared/Computational_Group/Naravich/nnUNet/.venv/lib/python3.12/site-packages/fft_conv_pytorch/fft_conv.py:139: UserWarning: Using a non-tuple sequence for multidimensional indexing is deprecated and will be changed in pytorch 2.9; use x[tuple(seq)] instead of x[seq]. In pytorch 2.9 this will be interpreted as tensor index, x[torch.tensor(seq)], which will result either in an error or a different result (Triggered internally at /pytorch/torch/csrc/autograd/python_variable_indexing.cpp:306.)
  output = output[crop_slices].contiguous()
/nfs/erelab001/shared/Computational_Group/Naravich/nnUNet/.venv/lib/python3.12/site-packages/fft_conv_pytorch/fft_conv.py:139: UserWarning: Using a non-tuple sequence for multidimensional indexing is deprecated and will be changed in pytorch 2.9; use x[tuple(seq)] instead of x[seq]. In pytorch 2.9 this will be interpreted as tensor index, x[torch.tensor(seq)], which will result either in an error or a different result (Triggered internally at /pytorch/torch/csrc/autograd/python_variable_indexing.cpp:306.)
  output = output[crop_slices].contiguous()
/nfs/erelab001/shared/Computational_Group/Naravich/nnUNet/.venv/lib/python3.12/site-packages/fft_conv_pytorch/fft_conv.py:139: UserWarning: Using a non-tuple sequence for multidimensional indexing is deprecated and will be changed in pytorch 2.9; use x[tuple(seq)] instead of x[seq]. In pytorch 2.9 this will be interpreted as tensor index, x[torch.tensor(seq)], which will result either in an error or a different result (Triggered internally at /pytorch/torch/csrc/autograd/python_variable_indexing.cpp:306.)
  output = output[crop_slices].contiguous()
/nfs/erelab001/shared/Computational_Group/Naravich/nnUNet/.venv/lib/python3.12/site-packages/fft_conv_pytorch/fft_conv.py:139: UserWarning: Using a non-tuple sequence for multidimensional indexing is deprecated and will be changed in pytorch 2.9; use x[tuple(seq)] instead of x[seq]. In pytorch 2.9 this will be interpreted as tensor index, x[torch.tensor(seq)], which will result either in an error or a different result (Triggered internally at /pytorch/torch/csrc/autograd/python_variable_indexing.cpp:306.)
  output = output[crop_slices].contiguous()
/nfs/erelab001/shared/Computational_Group/Naravich/nnUNet/.venv/lib/python3.12/site-packages/fft_conv_pytorch/fft_conv.py:139: UserWarning: Using a non-tuple sequence for multidimensional indexing is deprecated and will be changed in pytorch 2.9; use x[tuple(seq)] instead of x[seq]. In pytorch 2.9 this will be interpreted as tensor index, x[torch.tensor(seq)], which will result either in an error or a different result (Triggered internally at /pytorch/torch/csrc/autograd/python_variable_indexing.cpp:306.)
  output = output[crop_slices].contiguous()
/nfs/erelab001/shared/Computational_Group/Naravich/nnUNet/.venv/lib/python3.12/site-packages/fft_conv_pytorch/fft_conv.py:139: UserWarning: Using a non-tuple sequence for multidimensional indexing is deprecated and will be changed in pytorch 2.9; use x[tuple(seq)] instead of x[seq]. In pytorch 2.9 this will be interpreted as tensor index, x[torch.tensor(seq)], which will result either in an error or a different result (Triggered internally at /pytorch/torch/csrc/autograd/python_variable_indexing.cpp:306.)
  output = output[crop_slices].contiguous()
/nfs/erelab001/shared/Computational_Group/Naravich/nnUNet/.venv/lib/python3.12/site-packages/fft_conv_pytorch/fft_conv.py:139: UserWarning: Using a non-tuple sequence for multidimensional indexing is deprecated and will be changed in pytorch 2.9; use x[tuple(seq)] instead of x[seq]. In pytorch 2.9 this will be interpreted as tensor index, x[torch.tensor(seq)], which will result either in an error or a different result (Triggered internally at /pytorch/torch/csrc/autograd/python_variable_indexing.cpp:306.)
  output = output[crop_slices].contiguous()
/nfs/erelab001/shared/Computational_Group/Naravich/nnUNet/.venv/lib/python3.12/site-packages/fft_conv_pytorch/fft_conv.py:139: UserWarning: Using a non-tuple sequence for multidimensional indexing is deprecated and will be changed in pytorch 2.9; use x[tuple(seq)] instead of x[seq]. In pytorch 2.9 this will be interpreted as tensor index, x[torch.tensor(seq)], which will result either in an error or a different result (Triggered internally at /pytorch/torch/csrc/autograd/python_variable_indexing.cpp:306.)
  output = output[crop_slices].contiguous()
/nfs/erelab001/shared/Computational_Group/Naravich/nnUNet/.venv/lib/python3.12/site-packages/torch/_inductor/lowering.py:7095: UserWarning: 
Online softmax is disabled on the fly since Inductor decides to
split the reduction. Cut an issue to PyTorch if this is an
important use case and you want to speed it up with online
softmax.

  warnings.warn(
/nfs/erelab001/shared/Computational_Group/Naravich/nnUNet/.venv/lib/python3.12/site-packages/torch/_inductor/lowering.py:7095: UserWarning: 
Online softmax is disabled on the fly since Inductor decides to
split the reduction. Cut an issue to PyTorch if this is an
important use case and you want to speed it up with online
softmax.

  warnings.warn(
/nfs/erelab001/shared/Computational_Group/Naravich/nnUNet/.venv/lib/python3.12/site-packages/torch/_inductor/lowering.py:7095: UserWarning: 
Online softmax is disabled on the fly since Inductor decides to
split the reduction. Cut an issue to PyTorch if this is an
important use case and you want to speed it up with online
softmax.

  warnings.warn(
/nfs/erelab001/shared/Computational_Group/Naravich/nnUNet/.venv/lib/python3.12/site-packages/torch/_inductor/lowering.py:7095: UserWarning: 
Online softmax is disabled on the fly since Inductor decides to
split the reduction. Cut an issue to PyTorch if this is an
important use case and you want to speed it up with online
softmax.

  warnings.warn(
2025-10-15 09:31:38.141523: unpacking done...
2025-10-15 09:31:38.143723: Unable to plot network architecture: nnUNet_compile is enabled!
2025-10-15 09:31:38.148741: 
2025-10-15 09:31:38.149006: Epoch 0
2025-10-15 09:31:38.149243: Current learning rate: 0.01
2025-10-15 09:32:59.691550: Validation loss improved from 1000.00000 to -0.23177! Patience: 0/50
2025-10-15 09:32:59.692250: train_loss -0.142
2025-10-15 09:32:59.692428: val_loss -0.2318
2025-10-15 09:32:59.692558: Pseudo dice [np.float32(0.5758)]
2025-10-15 09:32:59.692729: Epoch time: 81.54 s
2025-10-15 09:32:59.692847: Yayy! New best EMA pseudo Dice: 0.5758000016212463
2025-10-15 09:33:00.632042: 
2025-10-15 09:33:00.632415: Epoch 1
2025-10-15 09:33:00.632694: Current learning rate: 0.00994
2025-10-15 09:33:47.049175: Validation loss improved from -0.23177 to -0.25353! Patience: 0/50
2025-10-15 09:33:47.049644: train_loss -0.3254
2025-10-15 09:33:47.049778: val_loss -0.2535
2025-10-15 09:33:47.049915: Pseudo dice [np.float32(0.5943)]
2025-10-15 09:33:47.050044: Epoch time: 46.42 s
2025-10-15 09:33:47.050168: Yayy! New best EMA pseudo Dice: 0.5777000188827515
2025-10-15 09:33:48.123229: 
2025-10-15 09:33:48.123578: Epoch 2
2025-10-15 09:33:48.123795: Current learning rate: 0.00988
2025-10-15 09:34:34.636839: Validation loss improved from -0.25353 to -0.33797! Patience: 0/50
2025-10-15 09:34:34.637428: train_loss -0.3779
2025-10-15 09:34:34.637601: val_loss -0.338
2025-10-15 09:34:34.637735: Pseudo dice [np.float32(0.6256)]
2025-10-15 09:34:34.637872: Epoch time: 46.51 s
2025-10-15 09:34:34.637999: Yayy! New best EMA pseudo Dice: 0.5824999809265137
2025-10-15 09:34:35.686028: 
2025-10-15 09:34:35.686312: Epoch 3
2025-10-15 09:34:35.686507: Current learning rate: 0.00982
2025-10-15 09:35:22.111892: Validation loss improved from -0.33797 to -0.37070! Patience: 0/50
2025-10-15 09:35:22.112759: train_loss -0.4069
2025-10-15 09:35:22.112982: val_loss -0.3707
2025-10-15 09:35:22.113164: Pseudo dice [np.float32(0.6437)]
2025-10-15 09:35:22.113365: Epoch time: 46.43 s
2025-10-15 09:35:22.113552: Yayy! New best EMA pseudo Dice: 0.5885999798774719
2025-10-15 09:35:23.174606: 
2025-10-15 09:35:23.174961: Epoch 4
2025-10-15 09:35:23.175188: Current learning rate: 0.00976
2025-10-15 09:36:09.658705: Validation loss improved from -0.37070 to -0.40547! Patience: 0/50
2025-10-15 09:36:09.659304: train_loss -0.4499
2025-10-15 09:36:09.659457: val_loss -0.4055
2025-10-15 09:36:09.659575: Pseudo dice [np.float32(0.6743)]
2025-10-15 09:36:09.659734: Epoch time: 46.49 s
2025-10-15 09:36:10.071573: Yayy! New best EMA pseudo Dice: 0.5971999764442444
2025-10-15 09:36:11.158514: 
2025-10-15 09:36:11.158829: Epoch 5
2025-10-15 09:36:11.159079: Current learning rate: 0.0097
2025-10-15 09:36:57.738985: Validation loss improved from -0.40547 to -0.42910! Patience: 0/50
2025-10-15 09:36:57.739453: train_loss -0.4567
2025-10-15 09:36:57.739663: val_loss -0.4291
2025-10-15 09:36:57.739782: Pseudo dice [np.float32(0.676)]
2025-10-15 09:36:57.740036: Epoch time: 46.58 s
2025-10-15 09:36:57.740193: Yayy! New best EMA pseudo Dice: 0.6050999760627747
2025-10-15 09:36:58.803557: 
2025-10-15 09:36:58.803890: Epoch 6
2025-10-15 09:36:58.804113: Current learning rate: 0.00964
2025-10-15 09:37:45.304265: Validation loss did not improve from -0.42910. Patience: 1/50
2025-10-15 09:37:45.305052: train_loss -0.4829
2025-10-15 09:37:45.305223: val_loss -0.429
2025-10-15 09:37:45.305388: Pseudo dice [np.float32(0.6781)]
2025-10-15 09:37:45.305526: Epoch time: 46.5 s
2025-10-15 09:37:45.305685: Yayy! New best EMA pseudo Dice: 0.6123999953269958
2025-10-15 09:37:46.373507: 
2025-10-15 09:37:46.373824: Epoch 7
2025-10-15 09:37:46.374051: Current learning rate: 0.00958
2025-10-15 09:38:32.838688: Validation loss improved from -0.42910 to -0.46585! Patience: 1/50
2025-10-15 09:38:32.839130: train_loss -0.4904
2025-10-15 09:38:32.839312: val_loss -0.4658
2025-10-15 09:38:32.839436: Pseudo dice [np.float32(0.6926)]
2025-10-15 09:38:32.839607: Epoch time: 46.47 s
2025-10-15 09:38:32.839727: Yayy! New best EMA pseudo Dice: 0.6204000115394592
2025-10-15 09:38:33.921600: 
2025-10-15 09:38:33.921845: Epoch 8
2025-10-15 09:38:33.922033: Current learning rate: 0.00952
2025-10-15 09:39:20.405248: Validation loss did not improve from -0.46585. Patience: 1/50
2025-10-15 09:39:20.405864: train_loss -0.5232
2025-10-15 09:39:20.406027: val_loss -0.4109
2025-10-15 09:39:20.406198: Pseudo dice [np.float32(0.6718)]
2025-10-15 09:39:20.406340: Epoch time: 46.48 s
2025-10-15 09:39:20.406459: Yayy! New best EMA pseudo Dice: 0.6255000233650208
2025-10-15 09:39:21.469956: 
2025-10-15 09:39:21.470295: Epoch 9
2025-10-15 09:39:21.470525: Current learning rate: 0.00946
2025-10-15 09:40:07.934968: Validation loss did not improve from -0.46585. Patience: 2/50
2025-10-15 09:40:07.935421: train_loss -0.5292
2025-10-15 09:40:07.935595: val_loss -0.4054
2025-10-15 09:40:07.935703: Pseudo dice [np.float32(0.6591)]
2025-10-15 09:40:07.935823: Epoch time: 46.47 s
2025-10-15 09:40:08.383795: Yayy! New best EMA pseudo Dice: 0.6288999915122986
2025-10-15 09:40:09.418152: 
2025-10-15 09:40:09.418487: Epoch 10
2025-10-15 09:40:09.418674: Current learning rate: 0.0094
2025-10-15 09:40:55.909281: Validation loss did not improve from -0.46585. Patience: 3/50
2025-10-15 09:40:55.909990: train_loss -0.5308
2025-10-15 09:40:55.910125: val_loss -0.4316
2025-10-15 09:40:55.910256: Pseudo dice [np.float32(0.6849)]
2025-10-15 09:40:55.910387: Epoch time: 46.49 s
2025-10-15 09:40:55.910511: Yayy! New best EMA pseudo Dice: 0.6345000267028809
2025-10-15 09:40:56.989304: 
2025-10-15 09:40:56.989591: Epoch 11
2025-10-15 09:40:56.989786: Current learning rate: 0.00934
2025-10-15 09:41:43.601688: Validation loss improved from -0.46585 to -0.46669! Patience: 3/50
2025-10-15 09:41:43.602222: train_loss -0.5348
2025-10-15 09:41:43.602371: val_loss -0.4667
2025-10-15 09:41:43.602565: Pseudo dice [np.float32(0.6951)]
2025-10-15 09:41:43.602725: Epoch time: 46.61 s
2025-10-15 09:41:43.602852: Yayy! New best EMA pseudo Dice: 0.640500009059906
2025-10-15 09:41:45.238013: 
2025-10-15 09:41:45.238368: Epoch 12
2025-10-15 09:41:45.238612: Current learning rate: 0.00928
2025-10-15 09:42:31.745286: Validation loss did not improve from -0.46669. Patience: 1/50
2025-10-15 09:42:31.745957: train_loss -0.5555
2025-10-15 09:42:31.746137: val_loss -0.4634
2025-10-15 09:42:31.746274: Pseudo dice [np.float32(0.6976)]
2025-10-15 09:42:31.746427: Epoch time: 46.51 s
2025-10-15 09:42:31.746562: Yayy! New best EMA pseudo Dice: 0.6462000012397766
2025-10-15 09:42:32.836936: 
2025-10-15 09:42:32.837252: Epoch 13
2025-10-15 09:42:32.837459: Current learning rate: 0.00922
2025-10-15 09:43:19.415170: Validation loss did not improve from -0.46669. Patience: 2/50
2025-10-15 09:43:19.415665: train_loss -0.573
2025-10-15 09:43:19.415882: val_loss -0.4547
2025-10-15 09:43:19.416112: Pseudo dice [np.float32(0.6901)]
2025-10-15 09:43:19.416339: Epoch time: 46.58 s
2025-10-15 09:43:19.416528: Yayy! New best EMA pseudo Dice: 0.650600016117096
2025-10-15 09:43:20.491734: 
2025-10-15 09:43:20.492115: Epoch 14
2025-10-15 09:43:20.492394: Current learning rate: 0.00916
2025-10-15 09:44:06.971852: Validation loss improved from -0.46669 to -0.47571! Patience: 2/50
2025-10-15 09:44:06.972523: train_loss -0.5748
2025-10-15 09:44:06.972668: val_loss -0.4757
2025-10-15 09:44:06.972841: Pseudo dice [np.float32(0.7136)]
2025-10-15 09:44:06.972985: Epoch time: 46.48 s
2025-10-15 09:44:07.397382: Yayy! New best EMA pseudo Dice: 0.6568999886512756
2025-10-15 09:44:08.442170: 
2025-10-15 09:44:08.442449: Epoch 15
2025-10-15 09:44:08.442644: Current learning rate: 0.0091
2025-10-15 09:44:54.909611: Validation loss did not improve from -0.47571. Patience: 1/50
2025-10-15 09:44:54.910051: train_loss -0.5722
2025-10-15 09:44:54.910242: val_loss -0.4601
2025-10-15 09:44:54.910386: Pseudo dice [np.float32(0.6964)]
2025-10-15 09:44:54.910559: Epoch time: 46.47 s
2025-10-15 09:44:54.910706: Yayy! New best EMA pseudo Dice: 0.6608999967575073
2025-10-15 09:44:55.965245: 
2025-10-15 09:44:55.965539: Epoch 16
2025-10-15 09:44:55.965729: Current learning rate: 0.00903
2025-10-15 09:45:42.470092: Validation loss improved from -0.47571 to -0.49331! Patience: 1/50
2025-10-15 09:45:42.470721: train_loss -0.5889
2025-10-15 09:45:42.470910: val_loss -0.4933
2025-10-15 09:45:42.471059: Pseudo dice [np.float32(0.713)]
2025-10-15 09:45:42.471241: Epoch time: 46.51 s
2025-10-15 09:45:42.471393: Yayy! New best EMA pseudo Dice: 0.666100025177002
2025-10-15 09:45:43.525221: 
2025-10-15 09:45:43.525563: Epoch 17
2025-10-15 09:45:43.525751: Current learning rate: 0.00897
2025-10-15 09:46:30.063713: Validation loss did not improve from -0.49331. Patience: 1/50
2025-10-15 09:46:30.064281: train_loss -0.5825
2025-10-15 09:46:30.064480: val_loss -0.4262
2025-10-15 09:46:30.064618: Pseudo dice [np.float32(0.6799)]
2025-10-15 09:46:30.064745: Epoch time: 46.54 s
2025-10-15 09:46:30.064896: Yayy! New best EMA pseudo Dice: 0.6675000190734863
2025-10-15 09:46:31.158814: 
2025-10-15 09:46:31.159192: Epoch 18
2025-10-15 09:46:31.159401: Current learning rate: 0.00891
2025-10-15 09:47:17.747697: Validation loss did not improve from -0.49331. Patience: 2/50
2025-10-15 09:47:17.748258: train_loss -0.5936
2025-10-15 09:47:17.748412: val_loss -0.486
2025-10-15 09:47:17.748550: Pseudo dice [np.float32(0.7117)]
2025-10-15 09:47:17.748676: Epoch time: 46.59 s
2025-10-15 09:47:17.748788: Yayy! New best EMA pseudo Dice: 0.6718999743461609
2025-10-15 09:47:18.866294: 
2025-10-15 09:47:18.866629: Epoch 19
2025-10-15 09:47:18.866841: Current learning rate: 0.00885
2025-10-15 09:48:05.514884: Validation loss improved from -0.49331 to -0.50969! Patience: 2/50
2025-10-15 09:48:05.515428: train_loss -0.604
2025-10-15 09:48:05.515607: val_loss -0.5097
2025-10-15 09:48:05.515747: Pseudo dice [np.float32(0.7266)]
2025-10-15 09:48:05.515874: Epoch time: 46.65 s
2025-10-15 09:48:05.989203: Yayy! New best EMA pseudo Dice: 0.6773999929428101
2025-10-15 09:48:07.059332: 
2025-10-15 09:48:07.059650: Epoch 20
2025-10-15 09:48:07.059835: Current learning rate: 0.00879
2025-10-15 09:48:53.648081: Validation loss did not improve from -0.50969. Patience: 1/50
2025-10-15 09:48:53.648812: train_loss -0.6077
2025-10-15 09:48:53.648954: val_loss -0.4704
2025-10-15 09:48:53.649107: Pseudo dice [np.float32(0.6988)]
2025-10-15 09:48:53.649260: Epoch time: 46.59 s
2025-10-15 09:48:53.649395: Yayy! New best EMA pseudo Dice: 0.6794999837875366
2025-10-15 09:48:54.765967: 
2025-10-15 09:48:54.766409: Epoch 21
2025-10-15 09:48:54.766733: Current learning rate: 0.00873
2025-10-15 09:49:41.395270: Validation loss did not improve from -0.50969. Patience: 2/50
2025-10-15 09:49:41.395807: train_loss -0.6103
2025-10-15 09:49:41.395983: val_loss -0.4656
2025-10-15 09:49:41.396191: Pseudo dice [np.float32(0.6979)]
2025-10-15 09:49:41.396326: Epoch time: 46.63 s
2025-10-15 09:49:41.396465: Yayy! New best EMA pseudo Dice: 0.6812999844551086
2025-10-15 09:49:42.481662: 
2025-10-15 09:49:42.482041: Epoch 22
2025-10-15 09:49:42.482261: Current learning rate: 0.00867
2025-10-15 09:50:29.152183: Validation loss did not improve from -0.50969. Patience: 3/50
2025-10-15 09:50:29.153489: train_loss -0.621
2025-10-15 09:50:29.153852: val_loss -0.4807
2025-10-15 09:50:29.154155: Pseudo dice [np.float32(0.698)]
2025-10-15 09:50:29.154482: Epoch time: 46.67 s
2025-10-15 09:50:29.154807: Yayy! New best EMA pseudo Dice: 0.6830000281333923
2025-10-15 09:50:30.272638: 
2025-10-15 09:50:30.273103: Epoch 23
2025-10-15 09:50:30.273415: Current learning rate: 0.00861
2025-10-15 09:51:16.868796: Validation loss did not improve from -0.50969. Patience: 4/50
2025-10-15 09:51:16.869279: train_loss -0.6287
2025-10-15 09:51:16.869456: val_loss -0.5094
2025-10-15 09:51:16.869633: Pseudo dice [np.float32(0.7322)]
2025-10-15 09:51:16.869765: Epoch time: 46.6 s
2025-10-15 09:51:16.869906: Yayy! New best EMA pseudo Dice: 0.6879000067710876
2025-10-15 09:51:17.959252: 
2025-10-15 09:51:17.959702: Epoch 24
2025-10-15 09:51:17.959997: Current learning rate: 0.00855
2025-10-15 09:52:04.596019: Validation loss did not improve from -0.50969. Patience: 5/50
2025-10-15 09:52:04.596767: train_loss -0.626
2025-10-15 09:52:04.596971: val_loss -0.4539
2025-10-15 09:52:04.597124: Pseudo dice [np.float32(0.6931)]
2025-10-15 09:52:04.597253: Epoch time: 46.64 s
2025-10-15 09:52:05.046757: Yayy! New best EMA pseudo Dice: 0.6884999871253967
2025-10-15 09:52:06.135439: 
2025-10-15 09:52:06.135768: Epoch 25
2025-10-15 09:52:06.136030: Current learning rate: 0.00849
2025-10-15 09:52:52.641352: Validation loss did not improve from -0.50969. Patience: 6/50
2025-10-15 09:52:52.642000: train_loss -0.6371
2025-10-15 09:52:52.642320: val_loss -0.4959
2025-10-15 09:52:52.642500: Pseudo dice [np.float32(0.7196)]
2025-10-15 09:52:52.642630: Epoch time: 46.51 s
2025-10-15 09:52:52.642844: Yayy! New best EMA pseudo Dice: 0.6916000247001648
2025-10-15 09:52:53.718134: 
2025-10-15 09:52:53.718373: Epoch 26
2025-10-15 09:52:53.718590: Current learning rate: 0.00843
2025-10-15 09:53:40.313960: Validation loss did not improve from -0.50969. Patience: 7/50
2025-10-15 09:53:40.314754: train_loss -0.6354
2025-10-15 09:53:40.314887: val_loss -0.4689
2025-10-15 09:53:40.315031: Pseudo dice [np.float32(0.7091)]
2025-10-15 09:53:40.315174: Epoch time: 46.6 s
2025-10-15 09:53:40.315302: Yayy! New best EMA pseudo Dice: 0.6933000087738037
2025-10-15 09:53:41.931800: 
2025-10-15 09:53:41.932142: Epoch 27
2025-10-15 09:53:41.932356: Current learning rate: 0.00836
2025-10-15 09:54:28.595489: Validation loss did not improve from -0.50969. Patience: 8/50
2025-10-15 09:54:28.595931: train_loss -0.641
2025-10-15 09:54:28.596098: val_loss -0.4505
2025-10-15 09:54:28.596248: Pseudo dice [np.float32(0.6855)]
2025-10-15 09:54:28.596397: Epoch time: 46.66 s
2025-10-15 09:54:29.236613: 
2025-10-15 09:54:29.236953: Epoch 28
2025-10-15 09:54:29.237262: Current learning rate: 0.0083
2025-10-15 09:55:15.851103: Validation loss did not improve from -0.50969. Patience: 9/50
2025-10-15 09:55:15.851692: train_loss -0.6486
2025-10-15 09:55:15.851857: val_loss -0.4916
2025-10-15 09:55:15.851996: Pseudo dice [np.float32(0.7254)]
2025-10-15 09:55:15.852163: Epoch time: 46.62 s
2025-10-15 09:55:15.852312: Yayy! New best EMA pseudo Dice: 0.6958000063896179
2025-10-15 09:55:16.964911: 
2025-10-15 09:55:16.965239: Epoch 29
2025-10-15 09:55:16.965420: Current learning rate: 0.00824
2025-10-15 09:56:03.531410: Validation loss did not improve from -0.50969. Patience: 10/50
2025-10-15 09:56:03.531896: train_loss -0.6474
2025-10-15 09:56:03.532040: val_loss -0.4917
2025-10-15 09:56:03.532194: Pseudo dice [np.float32(0.7206)]
2025-10-15 09:56:03.532318: Epoch time: 46.57 s
2025-10-15 09:56:03.989329: Yayy! New best EMA pseudo Dice: 0.6983000040054321
2025-10-15 09:56:05.062730: 
2025-10-15 09:56:05.063010: Epoch 30
2025-10-15 09:56:05.063236: Current learning rate: 0.00818
2025-10-15 09:56:51.568983: Validation loss improved from -0.50969 to -0.51158! Patience: 10/50
2025-10-15 09:56:51.569841: train_loss -0.6546
2025-10-15 09:56:51.570062: val_loss -0.5116
2025-10-15 09:56:51.570260: Pseudo dice [np.float32(0.7222)]
2025-10-15 09:56:51.570472: Epoch time: 46.51 s
2025-10-15 09:56:51.570757: Yayy! New best EMA pseudo Dice: 0.7006999850273132
2025-10-15 09:56:52.649397: 
2025-10-15 09:56:52.649737: Epoch 31
2025-10-15 09:56:52.649979: Current learning rate: 0.00812
2025-10-15 09:57:39.272945: Validation loss did not improve from -0.51158. Patience: 1/50
2025-10-15 09:57:39.273458: train_loss -0.6572
2025-10-15 09:57:39.273633: val_loss -0.4945
2025-10-15 09:57:39.273753: Pseudo dice [np.float32(0.7118)]
2025-10-15 09:57:39.273900: Epoch time: 46.62 s
2025-10-15 09:57:39.274042: Yayy! New best EMA pseudo Dice: 0.7017999887466431
2025-10-15 09:57:40.386474: 
2025-10-15 09:57:40.386900: Epoch 32
2025-10-15 09:57:40.387178: Current learning rate: 0.00806
2025-10-15 09:58:27.098301: Validation loss did not improve from -0.51158. Patience: 2/50
2025-10-15 09:58:27.098902: train_loss -0.6529
2025-10-15 09:58:27.099105: val_loss -0.4711
2025-10-15 09:58:27.099269: Pseudo dice [np.float32(0.6982)]
2025-10-15 09:58:27.099434: Epoch time: 46.71 s
2025-10-15 09:58:27.757616: 
2025-10-15 09:58:27.757926: Epoch 33
2025-10-15 09:58:27.758343: Current learning rate: 0.008
2025-10-15 09:59:14.414622: Validation loss improved from -0.51158 to -0.53170! Patience: 2/50
2025-10-15 09:59:14.415233: train_loss -0.6626
2025-10-15 09:59:14.415436: val_loss -0.5317
2025-10-15 09:59:14.415639: Pseudo dice [np.float32(0.7362)]
2025-10-15 09:59:14.415835: Epoch time: 46.66 s
2025-10-15 09:59:14.415993: Yayy! New best EMA pseudo Dice: 0.7049000263214111
2025-10-15 09:59:15.526505: 
2025-10-15 09:59:15.526804: Epoch 34
2025-10-15 09:59:15.526990: Current learning rate: 0.00793
2025-10-15 10:00:02.143960: Validation loss did not improve from -0.53170. Patience: 1/50
2025-10-15 10:00:02.144573: train_loss -0.6749
2025-10-15 10:00:02.144711: val_loss -0.4886
2025-10-15 10:00:02.144831: Pseudo dice [np.float32(0.713)]
2025-10-15 10:00:02.145003: Epoch time: 46.62 s
2025-10-15 10:00:02.577063: Yayy! New best EMA pseudo Dice: 0.7056999802589417
2025-10-15 10:00:03.678718: 
2025-10-15 10:00:03.678980: Epoch 35
2025-10-15 10:00:03.679183: Current learning rate: 0.00787
2025-10-15 10:00:50.360833: Validation loss did not improve from -0.53170. Patience: 2/50
2025-10-15 10:00:50.361361: train_loss -0.6755
2025-10-15 10:00:50.361510: val_loss -0.4927
2025-10-15 10:00:50.361644: Pseudo dice [np.float32(0.7131)]
2025-10-15 10:00:50.361789: Epoch time: 46.68 s
2025-10-15 10:00:50.361957: Yayy! New best EMA pseudo Dice: 0.7064999938011169
2025-10-15 10:00:51.504661: 
2025-10-15 10:00:51.505037: Epoch 36
2025-10-15 10:00:51.505269: Current learning rate: 0.00781
2025-10-15 10:01:38.208495: Validation loss did not improve from -0.53170. Patience: 3/50
2025-10-15 10:01:38.209153: train_loss -0.6665
2025-10-15 10:01:38.209286: val_loss -0.478
2025-10-15 10:01:38.209409: Pseudo dice [np.float32(0.6978)]
2025-10-15 10:01:38.209527: Epoch time: 46.71 s
2025-10-15 10:01:38.849144: 
2025-10-15 10:01:38.849636: Epoch 37
2025-10-15 10:01:38.849993: Current learning rate: 0.00775
2025-10-15 10:02:25.546067: Validation loss did not improve from -0.53170. Patience: 4/50
2025-10-15 10:02:25.546552: train_loss -0.6702
2025-10-15 10:02:25.546726: val_loss -0.5096
2025-10-15 10:02:25.546889: Pseudo dice [np.float32(0.7202)]
2025-10-15 10:02:25.547052: Epoch time: 46.7 s
2025-10-15 10:02:25.547193: Yayy! New best EMA pseudo Dice: 0.707099974155426
2025-10-15 10:02:26.673496: 
2025-10-15 10:02:26.674189: Epoch 38
2025-10-15 10:02:26.674447: Current learning rate: 0.00769
2025-10-15 10:03:13.216406: Validation loss did not improve from -0.53170. Patience: 5/50
2025-10-15 10:03:13.217023: train_loss -0.6747
2025-10-15 10:03:13.217191: val_loss -0.494
2025-10-15 10:03:13.217318: Pseudo dice [np.float32(0.7068)]
2025-10-15 10:03:13.217487: Epoch time: 46.54 s
2025-10-15 10:03:13.851801: 
2025-10-15 10:03:13.852131: Epoch 39
2025-10-15 10:03:13.852343: Current learning rate: 0.00763
2025-10-15 10:04:00.405010: Validation loss did not improve from -0.53170. Patience: 6/50
2025-10-15 10:04:00.405406: train_loss -0.6794
2025-10-15 10:04:00.405574: val_loss -0.4684
2025-10-15 10:04:00.405750: Pseudo dice [np.float32(0.6983)]
2025-10-15 10:04:00.405906: Epoch time: 46.55 s
2025-10-15 10:04:01.503804: 
2025-10-15 10:04:01.504110: Epoch 40
2025-10-15 10:04:01.504321: Current learning rate: 0.00756
2025-10-15 10:04:48.040399: Validation loss did not improve from -0.53170. Patience: 7/50
2025-10-15 10:04:48.040975: train_loss -0.6806
2025-10-15 10:04:48.041116: val_loss -0.4693
2025-10-15 10:04:48.041274: Pseudo dice [np.float32(0.7004)]
2025-10-15 10:04:48.041443: Epoch time: 46.54 s
2025-10-15 10:04:48.688474: 
2025-10-15 10:04:48.688830: Epoch 41
2025-10-15 10:04:48.689075: Current learning rate: 0.0075
2025-10-15 10:05:35.254651: Validation loss did not improve from -0.53170. Patience: 8/50
2025-10-15 10:05:35.255075: train_loss -0.6804
2025-10-15 10:05:35.255254: val_loss -0.4628
2025-10-15 10:05:35.255428: Pseudo dice [np.float32(0.6975)]
2025-10-15 10:05:35.255619: Epoch time: 46.57 s
2025-10-15 10:05:35.880609: 
2025-10-15 10:05:35.880869: Epoch 42
2025-10-15 10:05:35.881148: Current learning rate: 0.00744
2025-10-15 10:06:22.489533: Validation loss did not improve from -0.53170. Patience: 9/50
2025-10-15 10:06:22.490298: train_loss -0.685
2025-10-15 10:06:22.490520: val_loss -0.4926
2025-10-15 10:06:22.490702: Pseudo dice [np.float32(0.7126)]
2025-10-15 10:06:22.490930: Epoch time: 46.61 s
2025-10-15 10:06:23.511220: 
2025-10-15 10:06:23.511503: Epoch 43
2025-10-15 10:06:23.511697: Current learning rate: 0.00738
2025-10-15 10:07:10.161635: Validation loss did not improve from -0.53170. Patience: 10/50
2025-10-15 10:07:10.162084: train_loss -0.6907
2025-10-15 10:07:10.162223: val_loss -0.5156
2025-10-15 10:07:10.162332: Pseudo dice [np.float32(0.7307)]
2025-10-15 10:07:10.162479: Epoch time: 46.65 s
2025-10-15 10:07:10.162588: Yayy! New best EMA pseudo Dice: 0.7081000208854675
2025-10-15 10:07:11.252081: 
2025-10-15 10:07:11.252423: Epoch 44
2025-10-15 10:07:11.252605: Current learning rate: 0.00732
2025-10-15 10:07:57.991827: Validation loss did not improve from -0.53170. Patience: 11/50
2025-10-15 10:07:57.992616: train_loss -0.6926
2025-10-15 10:07:57.992956: val_loss -0.5074
2025-10-15 10:07:57.993136: Pseudo dice [np.float32(0.7296)]
2025-10-15 10:07:57.993356: Epoch time: 46.74 s
2025-10-15 10:07:58.446283: Yayy! New best EMA pseudo Dice: 0.7102000117301941
2025-10-15 10:07:59.535975: 
2025-10-15 10:07:59.536330: Epoch 45
2025-10-15 10:07:59.536541: Current learning rate: 0.00725
2025-10-15 10:08:46.241477: Validation loss did not improve from -0.53170. Patience: 12/50
2025-10-15 10:08:46.241985: train_loss -0.6942
2025-10-15 10:08:46.242148: val_loss -0.4198
2025-10-15 10:08:46.242282: Pseudo dice [np.float32(0.68)]
2025-10-15 10:08:46.242487: Epoch time: 46.71 s
2025-10-15 10:08:46.881476: 
2025-10-15 10:08:46.881760: Epoch 46
2025-10-15 10:08:46.882015: Current learning rate: 0.00719
2025-10-15 10:09:33.399828: Validation loss did not improve from -0.53170. Patience: 13/50
2025-10-15 10:09:33.400304: train_loss -0.6974
2025-10-15 10:09:33.400425: val_loss -0.4673
2025-10-15 10:09:33.400540: Pseudo dice [np.float32(0.7045)]
2025-10-15 10:09:33.400665: Epoch time: 46.52 s
2025-10-15 10:09:34.024659: 
2025-10-15 10:09:34.025010: Epoch 47
2025-10-15 10:09:34.025230: Current learning rate: 0.00713
2025-10-15 10:10:20.637763: Validation loss did not improve from -0.53170. Patience: 14/50
2025-10-15 10:10:20.638443: train_loss -0.6973
2025-10-15 10:10:20.638732: val_loss -0.4789
2025-10-15 10:10:20.639055: Pseudo dice [np.float32(0.7094)]
2025-10-15 10:10:20.639403: Epoch time: 46.61 s
2025-10-15 10:10:21.279865: 
2025-10-15 10:10:21.280159: Epoch 48
2025-10-15 10:10:21.280352: Current learning rate: 0.00707
2025-10-15 10:11:07.870834: Validation loss did not improve from -0.53170. Patience: 15/50
2025-10-15 10:11:07.871481: train_loss -0.7051
2025-10-15 10:11:07.871657: val_loss -0.5302
2025-10-15 10:11:07.871802: Pseudo dice [np.float32(0.7425)]
2025-10-15 10:11:07.871995: Epoch time: 46.59 s
2025-10-15 10:11:07.872147: Yayy! New best EMA pseudo Dice: 0.7106999754905701
2025-10-15 10:11:08.962043: 
2025-10-15 10:11:08.962356: Epoch 49
2025-10-15 10:11:08.962639: Current learning rate: 0.007
2025-10-15 10:11:55.476484: Validation loss did not improve from -0.53170. Patience: 16/50
2025-10-15 10:11:55.477008: train_loss -0.7058
2025-10-15 10:11:55.477260: val_loss -0.4795
2025-10-15 10:11:55.477475: Pseudo dice [np.float32(0.7062)]
2025-10-15 10:11:55.477674: Epoch time: 46.52 s
2025-10-15 10:11:56.550268: 
2025-10-15 10:11:56.550604: Epoch 50
2025-10-15 10:11:56.550840: Current learning rate: 0.00694
2025-10-15 10:12:43.145943: Validation loss did not improve from -0.53170. Patience: 17/50
2025-10-15 10:12:43.146675: train_loss -0.708
2025-10-15 10:12:43.146963: val_loss -0.4743
2025-10-15 10:12:43.147178: Pseudo dice [np.float32(0.7033)]
2025-10-15 10:12:43.147403: Epoch time: 46.6 s
2025-10-15 10:12:43.787196: 
2025-10-15 10:12:43.787607: Epoch 51
2025-10-15 10:12:43.787870: Current learning rate: 0.00688
2025-10-15 10:13:30.372778: Validation loss did not improve from -0.53170. Patience: 18/50
2025-10-15 10:13:30.373393: train_loss -0.7102
2025-10-15 10:13:30.373590: val_loss -0.4992
2025-10-15 10:13:30.373761: Pseudo dice [np.float32(0.718)]
2025-10-15 10:13:30.373936: Epoch time: 46.59 s
2025-10-15 10:13:31.009732: 
2025-10-15 10:13:31.010112: Epoch 52
2025-10-15 10:13:31.010343: Current learning rate: 0.00682
2025-10-15 10:14:17.551311: Validation loss did not improve from -0.53170. Patience: 19/50
2025-10-15 10:14:17.552040: train_loss -0.712
2025-10-15 10:14:17.552193: val_loss -0.4551
2025-10-15 10:14:17.552322: Pseudo dice [np.float32(0.6937)]
2025-10-15 10:14:17.552445: Epoch time: 46.54 s
2025-10-15 10:14:18.187940: 
2025-10-15 10:14:18.188172: Epoch 53
2025-10-15 10:14:18.188360: Current learning rate: 0.00675
2025-10-15 10:15:04.772567: Validation loss did not improve from -0.53170. Patience: 20/50
2025-10-15 10:15:04.773091: train_loss -0.7131
2025-10-15 10:15:04.773227: val_loss -0.487
2025-10-15 10:15:04.773395: Pseudo dice [np.float32(0.7181)]
2025-10-15 10:15:04.773533: Epoch time: 46.59 s
2025-10-15 10:15:05.414820: 
2025-10-15 10:15:05.415064: Epoch 54
2025-10-15 10:15:05.415260: Current learning rate: 0.00669
2025-10-15 10:15:52.065902: Validation loss did not improve from -0.53170. Patience: 21/50
2025-10-15 10:15:52.066480: train_loss -0.7156
2025-10-15 10:15:52.066652: val_loss -0.4803
2025-10-15 10:15:52.066784: Pseudo dice [np.float32(0.7014)]
2025-10-15 10:15:52.066949: Epoch time: 46.65 s
2025-10-15 10:15:53.158610: 
2025-10-15 10:15:53.158874: Epoch 55
2025-10-15 10:15:53.159071: Current learning rate: 0.00663
2025-10-15 10:16:39.730186: Validation loss did not improve from -0.53170. Patience: 22/50
2025-10-15 10:16:39.730736: train_loss -0.7151
2025-10-15 10:16:39.730892: val_loss -0.4639
2025-10-15 10:16:39.731101: Pseudo dice [np.float32(0.6981)]
2025-10-15 10:16:39.731232: Epoch time: 46.57 s
2025-10-15 10:16:40.371761: 
2025-10-15 10:16:40.372113: Epoch 56
2025-10-15 10:16:40.372320: Current learning rate: 0.00657
2025-10-15 10:17:27.017902: Validation loss did not improve from -0.53170. Patience: 23/50
2025-10-15 10:17:27.018533: train_loss -0.7146
2025-10-15 10:17:27.018702: val_loss -0.4874
2025-10-15 10:17:27.018852: Pseudo dice [np.float32(0.7185)]
2025-10-15 10:17:27.019022: Epoch time: 46.65 s
2025-10-15 10:17:27.654377: 
2025-10-15 10:17:27.654692: Epoch 57
2025-10-15 10:17:27.654895: Current learning rate: 0.0065
2025-10-15 10:18:14.149220: Validation loss did not improve from -0.53170. Patience: 24/50
2025-10-15 10:18:14.149635: train_loss -0.7231
2025-10-15 10:18:14.149790: val_loss -0.4322
2025-10-15 10:18:14.149907: Pseudo dice [np.float32(0.6871)]
2025-10-15 10:18:14.150057: Epoch time: 46.5 s
2025-10-15 10:18:14.780622: 
2025-10-15 10:18:14.780848: Epoch 58
2025-10-15 10:18:14.781040: Current learning rate: 0.00644
2025-10-15 10:19:01.332827: Validation loss did not improve from -0.53170. Patience: 25/50
2025-10-15 10:19:01.333545: train_loss -0.7168
2025-10-15 10:19:01.333705: val_loss -0.5028
2025-10-15 10:19:01.333850: Pseudo dice [np.float32(0.7321)]
2025-10-15 10:19:01.334056: Epoch time: 46.55 s
2025-10-15 10:19:02.493635: 
2025-10-15 10:19:02.494036: Epoch 59
2025-10-15 10:19:02.494398: Current learning rate: 0.00638
2025-10-15 10:19:49.178741: Validation loss did not improve from -0.53170. Patience: 26/50
2025-10-15 10:19:49.179421: train_loss -0.7249
2025-10-15 10:19:49.179571: val_loss -0.479
2025-10-15 10:19:49.179738: Pseudo dice [np.float32(0.7195)]
2025-10-15 10:19:49.179908: Epoch time: 46.69 s
2025-10-15 10:19:50.298080: 
2025-10-15 10:19:50.298392: Epoch 60
2025-10-15 10:19:50.298621: Current learning rate: 0.00631
2025-10-15 10:20:36.897799: Validation loss did not improve from -0.53170. Patience: 27/50
2025-10-15 10:20:36.898481: train_loss -0.7265
2025-10-15 10:20:36.898660: val_loss -0.4451
2025-10-15 10:20:36.898820: Pseudo dice [np.float32(0.6942)]
2025-10-15 10:20:36.898992: Epoch time: 46.6 s
2025-10-15 10:20:37.551325: 
2025-10-15 10:20:37.551687: Epoch 61
2025-10-15 10:20:37.551906: Current learning rate: 0.00625
2025-10-15 10:21:24.105937: Validation loss did not improve from -0.53170. Patience: 28/50
2025-10-15 10:21:24.106401: train_loss -0.729
2025-10-15 10:21:24.106564: val_loss -0.5142
2025-10-15 10:21:24.106690: Pseudo dice [np.float32(0.7296)]
2025-10-15 10:21:24.106812: Epoch time: 46.56 s
2025-10-15 10:21:24.106949: Yayy! New best EMA pseudo Dice: 0.7106999754905701
2025-10-15 10:21:25.219274: 
2025-10-15 10:21:25.219573: Epoch 62
2025-10-15 10:21:25.219836: Current learning rate: 0.00619
2025-10-15 10:22:11.714579: Validation loss did not improve from -0.53170. Patience: 29/50
2025-10-15 10:22:11.715342: train_loss -0.7304
2025-10-15 10:22:11.715501: val_loss -0.504
2025-10-15 10:22:11.715630: Pseudo dice [np.float32(0.7237)]
2025-10-15 10:22:11.715774: Epoch time: 46.5 s
2025-10-15 10:22:11.715954: Yayy! New best EMA pseudo Dice: 0.7120000123977661
2025-10-15 10:22:12.819539: 
2025-10-15 10:22:12.819837: Epoch 63
2025-10-15 10:22:12.820021: Current learning rate: 0.00612
2025-10-15 10:22:59.408626: Validation loss did not improve from -0.53170. Patience: 30/50
2025-10-15 10:22:59.409112: train_loss -0.7326
2025-10-15 10:22:59.409278: val_loss -0.4916
2025-10-15 10:22:59.409442: Pseudo dice [np.float32(0.7178)]
2025-10-15 10:22:59.409585: Epoch time: 46.59 s
2025-10-15 10:22:59.409708: Yayy! New best EMA pseudo Dice: 0.7125999927520752
2025-10-15 10:23:00.501640: 
2025-10-15 10:23:00.501993: Epoch 64
2025-10-15 10:23:00.502213: Current learning rate: 0.00606
2025-10-15 10:23:47.145514: Validation loss did not improve from -0.53170. Patience: 31/50
2025-10-15 10:23:47.146167: train_loss -0.7274
2025-10-15 10:23:47.146330: val_loss -0.4928
2025-10-15 10:23:47.146464: Pseudo dice [np.float32(0.7195)]
2025-10-15 10:23:47.146602: Epoch time: 46.65 s
2025-10-15 10:23:47.588482: Yayy! New best EMA pseudo Dice: 0.7132999897003174
2025-10-15 10:23:48.699849: 
2025-10-15 10:23:48.700105: Epoch 65
2025-10-15 10:23:48.700285: Current learning rate: 0.006
2025-10-15 10:24:35.283773: Validation loss did not improve from -0.53170. Patience: 32/50
2025-10-15 10:24:35.284346: train_loss -0.7323
2025-10-15 10:24:35.284588: val_loss -0.4942
2025-10-15 10:24:35.284772: Pseudo dice [np.float32(0.7208)]
2025-10-15 10:24:35.284973: Epoch time: 46.59 s
2025-10-15 10:24:35.285135: Yayy! New best EMA pseudo Dice: 0.7139999866485596
2025-10-15 10:24:36.386472: 
2025-10-15 10:24:36.386824: Epoch 66
2025-10-15 10:24:36.387071: Current learning rate: 0.00593
2025-10-15 10:25:23.005311: Validation loss did not improve from -0.53170. Patience: 33/50
2025-10-15 10:25:23.005996: train_loss -0.7399
2025-10-15 10:25:23.006154: val_loss -0.4557
2025-10-15 10:25:23.006304: Pseudo dice [np.float32(0.6958)]
2025-10-15 10:25:23.006445: Epoch time: 46.62 s
2025-10-15 10:25:23.671287: 
2025-10-15 10:25:23.672080: Epoch 67
2025-10-15 10:25:23.672273: Current learning rate: 0.00587
2025-10-15 10:26:10.362062: Validation loss did not improve from -0.53170. Patience: 34/50
2025-10-15 10:26:10.362561: train_loss -0.7433
2025-10-15 10:26:10.362762: val_loss -0.4878
2025-10-15 10:26:10.362917: Pseudo dice [np.float32(0.7227)]
2025-10-15 10:26:10.363123: Epoch time: 46.69 s
2025-10-15 10:26:11.028100: 
2025-10-15 10:26:11.028393: Epoch 68
2025-10-15 10:26:11.028615: Current learning rate: 0.00581
2025-10-15 10:26:57.744407: Validation loss did not improve from -0.53170. Patience: 35/50
2025-10-15 10:26:57.745028: train_loss -0.7405
2025-10-15 10:26:57.745180: val_loss -0.4998
2025-10-15 10:26:57.745356: Pseudo dice [np.float32(0.7242)]
2025-10-15 10:26:57.745503: Epoch time: 46.72 s
2025-10-15 10:26:57.745641: Yayy! New best EMA pseudo Dice: 0.7143999934196472
2025-10-15 10:26:58.850610: 
2025-10-15 10:26:58.851135: Epoch 69
2025-10-15 10:26:58.851501: Current learning rate: 0.00574
2025-10-15 10:27:45.500404: Validation loss did not improve from -0.53170. Patience: 36/50
2025-10-15 10:27:45.500944: train_loss -0.7387
2025-10-15 10:27:45.501118: val_loss -0.4824
2025-10-15 10:27:45.501243: Pseudo dice [np.float32(0.6967)]
2025-10-15 10:27:45.501389: Epoch time: 46.65 s
2025-10-15 10:27:46.615237: 
2025-10-15 10:27:46.615721: Epoch 70
2025-10-15 10:27:46.616073: Current learning rate: 0.00568
2025-10-15 10:28:33.151572: Validation loss did not improve from -0.53170. Patience: 37/50
2025-10-15 10:28:33.152192: train_loss -0.7433
2025-10-15 10:28:33.152375: val_loss -0.4941
2025-10-15 10:28:33.152510: Pseudo dice [np.float32(0.7157)]
2025-10-15 10:28:33.152644: Epoch time: 46.54 s
2025-10-15 10:28:33.789790: 
2025-10-15 10:28:33.790138: Epoch 71
2025-10-15 10:28:33.790342: Current learning rate: 0.00562
2025-10-15 10:29:20.262312: Validation loss did not improve from -0.53170. Patience: 38/50
2025-10-15 10:29:20.262748: train_loss -0.7465
2025-10-15 10:29:20.262886: val_loss -0.498
2025-10-15 10:29:20.263071: Pseudo dice [np.float32(0.7244)]
2025-10-15 10:29:20.263281: Epoch time: 46.47 s
2025-10-15 10:29:20.901867: 
2025-10-15 10:29:20.902227: Epoch 72
2025-10-15 10:29:20.902480: Current learning rate: 0.00555
2025-10-15 10:30:07.457366: Validation loss did not improve from -0.53170. Patience: 39/50
2025-10-15 10:30:07.457936: train_loss -0.7441
2025-10-15 10:30:07.458124: val_loss -0.4687
2025-10-15 10:30:07.458243: Pseudo dice [np.float32(0.7124)]
2025-10-15 10:30:07.458431: Epoch time: 46.56 s
2025-10-15 10:30:08.102958: 
2025-10-15 10:30:08.103321: Epoch 73
2025-10-15 10:30:08.103530: Current learning rate: 0.00549
2025-10-15 10:30:54.679330: Validation loss did not improve from -0.53170. Patience: 40/50
2025-10-15 10:30:54.679846: train_loss -0.7464
2025-10-15 10:30:54.679994: val_loss -0.4504
2025-10-15 10:30:54.680196: Pseudo dice [np.float32(0.7003)]
2025-10-15 10:30:54.680383: Epoch time: 46.58 s
2025-10-15 10:30:55.874760: 
2025-10-15 10:30:55.875089: Epoch 74
2025-10-15 10:30:55.875281: Current learning rate: 0.00542
2025-10-15 10:31:42.525028: Validation loss did not improve from -0.53170. Patience: 41/50
2025-10-15 10:31:42.525703: train_loss -0.7455
2025-10-15 10:31:42.525877: val_loss -0.4699
2025-10-15 10:31:42.526032: Pseudo dice [np.float32(0.7096)]
2025-10-15 10:31:42.526220: Epoch time: 46.65 s
2025-10-15 10:31:43.639496: 
2025-10-15 10:31:43.639865: Epoch 75
2025-10-15 10:31:43.640160: Current learning rate: 0.00536
2025-10-15 10:32:30.221591: Validation loss did not improve from -0.53170. Patience: 42/50
2025-10-15 10:32:30.222003: train_loss -0.749
2025-10-15 10:32:30.222171: val_loss -0.4824
2025-10-15 10:32:30.222323: Pseudo dice [np.float32(0.7151)]
2025-10-15 10:32:30.222489: Epoch time: 46.58 s
2025-10-15 10:32:30.862033: 
2025-10-15 10:32:30.862286: Epoch 76
2025-10-15 10:32:30.862474: Current learning rate: 0.00529
2025-10-15 10:33:17.589648: Validation loss did not improve from -0.53170. Patience: 43/50
2025-10-15 10:33:17.590489: train_loss -0.7524
2025-10-15 10:33:17.590693: val_loss -0.4375
2025-10-15 10:33:17.590919: Pseudo dice [np.float32(0.6939)]
2025-10-15 10:33:17.591113: Epoch time: 46.73 s
2025-10-15 10:33:18.238281: 
2025-10-15 10:33:18.238677: Epoch 77
2025-10-15 10:33:18.238954: Current learning rate: 0.00523
2025-10-15 10:34:05.021128: Validation loss did not improve from -0.53170. Patience: 44/50
2025-10-15 10:34:05.021631: train_loss -0.7521
2025-10-15 10:34:05.021820: val_loss -0.4322
2025-10-15 10:34:05.021974: Pseudo dice [np.float32(0.6876)]
2025-10-15 10:34:05.022135: Epoch time: 46.78 s
2025-10-15 10:34:05.665622: 
2025-10-15 10:34:05.665905: Epoch 78
2025-10-15 10:34:05.666181: Current learning rate: 0.00517
2025-10-15 10:34:52.185760: Validation loss did not improve from -0.53170. Patience: 45/50
2025-10-15 10:34:52.186743: train_loss -0.7518
2025-10-15 10:34:52.187155: val_loss -0.4919
2025-10-15 10:34:52.187447: Pseudo dice [np.float32(0.7217)]
2025-10-15 10:34:52.187657: Epoch time: 46.52 s
2025-10-15 10:34:52.838683: 
2025-10-15 10:34:52.839029: Epoch 79
2025-10-15 10:34:52.839297: Current learning rate: 0.0051
2025-10-15 10:35:39.581315: Validation loss did not improve from -0.53170. Patience: 46/50
2025-10-15 10:35:39.581949: train_loss -0.7528
2025-10-15 10:35:39.582302: val_loss -0.4845
2025-10-15 10:35:39.582629: Pseudo dice [np.float32(0.7159)]
2025-10-15 10:35:39.582960: Epoch time: 46.74 s
2025-10-15 10:35:40.702427: 
2025-10-15 10:35:40.702823: Epoch 80
2025-10-15 10:35:40.703183: Current learning rate: 0.00504
2025-10-15 10:36:27.331812: Validation loss did not improve from -0.53170. Patience: 47/50
2025-10-15 10:36:27.332481: train_loss -0.7521
2025-10-15 10:36:27.332614: val_loss -0.4744
2025-10-15 10:36:27.332736: Pseudo dice [np.float32(0.7202)]
2025-10-15 10:36:27.332856: Epoch time: 46.63 s
2025-10-15 10:36:27.996614: 
2025-10-15 10:36:27.996978: Epoch 81
2025-10-15 10:36:27.997265: Current learning rate: 0.00497
2025-10-15 10:37:14.603430: Validation loss did not improve from -0.53170. Patience: 48/50
2025-10-15 10:37:14.603961: train_loss -0.751
2025-10-15 10:37:14.604182: val_loss -0.4606
2025-10-15 10:37:14.604354: Pseudo dice [np.float32(0.7066)]
2025-10-15 10:37:14.604530: Epoch time: 46.61 s
2025-10-15 10:37:15.247565: 
2025-10-15 10:37:15.247924: Epoch 82
2025-10-15 10:37:15.248183: Current learning rate: 0.00491
2025-10-15 10:38:01.853570: Validation loss did not improve from -0.53170. Patience: 49/50
2025-10-15 10:38:01.854215: train_loss -0.7575
2025-10-15 10:38:01.854362: val_loss -0.4604
2025-10-15 10:38:01.854509: Pseudo dice [np.float32(0.7048)]
2025-10-15 10:38:01.854634: Epoch time: 46.61 s
2025-10-15 10:38:02.479878: 
2025-10-15 10:38:02.480169: Epoch 83
2025-10-15 10:38:02.480371: Current learning rate: 0.00484
2025-10-15 10:38:49.136744: Validation loss did not improve from -0.53170. Patience: 50/50
2025-10-15 10:38:49.137247: train_loss -0.7575
2025-10-15 10:38:49.137452: val_loss -0.512
2025-10-15 10:38:49.137642: Pseudo dice [np.float32(0.7425)]
2025-10-15 10:38:49.137813: Epoch time: 46.66 s
2025-10-15 10:38:49.765562: 
2025-10-15 10:38:49.765791: Epoch 84
2025-10-15 10:38:49.765983: Current learning rate: 0.00478
2025-10-15 10:39:36.409271: Validation loss did not improve from -0.53170. Patience: 51/50
2025-10-15 10:39:36.410252: train_loss -0.7603
2025-10-15 10:39:36.410513: val_loss -0.4544
2025-10-15 10:39:36.410751: Pseudo dice [np.float32(0.6916)]
2025-10-15 10:39:36.411010: Epoch time: 46.65 s
2025-10-15 10:39:37.517656: 
2025-10-15 10:39:37.518033: Epoch 85
2025-10-15 10:39:37.518317: Current learning rate: 0.00471
2025-10-15 10:40:24.126621: Validation loss did not improve from -0.53170. Patience: 52/50
2025-10-15 10:40:24.127385: train_loss -0.7647
2025-10-15 10:40:24.127782: val_loss -0.4604
2025-10-15 10:40:24.128163: Pseudo dice [np.float32(0.7119)]
2025-10-15 10:40:24.128570: Epoch time: 46.61 s
2025-10-15 10:40:24.764988: 
2025-10-15 10:40:24.765353: Epoch 86
2025-10-15 10:40:24.765604: Current learning rate: 0.00465
2025-10-15 10:41:11.286892: Validation loss did not improve from -0.53170. Patience: 53/50
2025-10-15 10:41:11.287496: train_loss -0.7617
2025-10-15 10:41:11.287636: val_loss -0.4671
2025-10-15 10:41:11.287775: Pseudo dice [np.float32(0.7089)]
2025-10-15 10:41:11.287911: Epoch time: 46.52 s
2025-10-15 10:41:11.915194: 
2025-10-15 10:41:11.915583: Epoch 87
2025-10-15 10:41:11.915796: Current learning rate: 0.00458
2025-10-15 10:41:58.600541: Validation loss did not improve from -0.53170. Patience: 54/50
2025-10-15 10:41:58.601007: train_loss -0.759
2025-10-15 10:41:58.601176: val_loss -0.4667
2025-10-15 10:41:58.601334: Pseudo dice [np.float32(0.7119)]
2025-10-15 10:41:58.601510: Epoch time: 46.69 s
2025-10-15 10:41:59.227782: 
2025-10-15 10:41:59.228016: Epoch 88
2025-10-15 10:41:59.228216: Current learning rate: 0.00452
2025-10-15 10:42:45.794082: Validation loss did not improve from -0.53170. Patience: 55/50
2025-10-15 10:42:45.794857: train_loss -0.7586
2025-10-15 10:42:45.795071: val_loss -0.4853
2025-10-15 10:42:45.795258: Pseudo dice [np.float32(0.72)]
2025-10-15 10:42:45.795426: Epoch time: 46.57 s
2025-10-15 10:42:46.819143: 
2025-10-15 10:42:46.819623: Epoch 89
2025-10-15 10:42:46.819811: Current learning rate: 0.00445
2025-10-15 10:43:33.512228: Validation loss did not improve from -0.53170. Patience: 56/50
2025-10-15 10:43:33.512696: train_loss -0.7631
2025-10-15 10:43:33.512861: val_loss -0.4921
2025-10-15 10:43:33.513338: Pseudo dice [np.float32(0.7222)]
2025-10-15 10:43:33.513654: Epoch time: 46.69 s
2025-10-15 10:43:34.575703: 
2025-10-15 10:43:34.576115: Epoch 90
2025-10-15 10:43:34.576364: Current learning rate: 0.00438
2025-10-15 10:44:21.270452: Validation loss did not improve from -0.53170. Patience: 57/50
2025-10-15 10:44:21.271151: train_loss -0.7664
2025-10-15 10:44:21.271356: val_loss -0.4807
2025-10-15 10:44:21.271723: Pseudo dice [np.float32(0.7193)]
2025-10-15 10:44:21.271883: Epoch time: 46.7 s
2025-10-15 10:44:21.917508: 
2025-10-15 10:44:21.917888: Epoch 91
2025-10-15 10:44:21.918098: Current learning rate: 0.00432
2025-10-15 10:45:08.788498: Validation loss did not improve from -0.53170. Patience: 58/50
2025-10-15 10:45:08.789067: train_loss -0.7683
2025-10-15 10:45:08.789286: val_loss -0.4785
2025-10-15 10:45:08.789473: Pseudo dice [np.float32(0.7056)]
2025-10-15 10:45:08.789686: Epoch time: 46.87 s
2025-10-15 10:45:09.427769: 
2025-10-15 10:45:09.428108: Epoch 92
2025-10-15 10:45:09.428374: Current learning rate: 0.00425
2025-10-15 10:45:56.200275: Validation loss did not improve from -0.53170. Patience: 59/50
2025-10-15 10:45:56.201076: train_loss -0.7672
2025-10-15 10:45:56.201318: val_loss -0.4649
2025-10-15 10:45:56.201496: Pseudo dice [np.float32(0.7117)]
2025-10-15 10:45:56.201656: Epoch time: 46.77 s
2025-10-15 10:45:56.843282: 
2025-10-15 10:45:56.843608: Epoch 93
2025-10-15 10:45:56.843829: Current learning rate: 0.00419
2025-10-15 10:46:43.453691: Validation loss did not improve from -0.53170. Patience: 60/50
2025-10-15 10:46:43.454200: train_loss -0.7654
2025-10-15 10:46:43.454458: val_loss -0.4344
2025-10-15 10:46:43.454645: Pseudo dice [np.float32(0.6924)]
2025-10-15 10:46:43.454873: Epoch time: 46.61 s
2025-10-15 10:46:44.084904: 
2025-10-15 10:46:44.085246: Epoch 94
2025-10-15 10:46:44.085444: Current learning rate: 0.00412
2025-10-15 10:47:30.620495: Validation loss did not improve from -0.53170. Patience: 61/50
2025-10-15 10:47:30.621279: train_loss -0.7679
2025-10-15 10:47:30.621501: val_loss -0.4569
2025-10-15 10:47:30.621689: Pseudo dice [np.float32(0.6997)]
2025-10-15 10:47:30.621875: Epoch time: 46.54 s
2025-10-15 10:47:31.693498: 
2025-10-15 10:47:31.693942: Epoch 95
2025-10-15 10:47:31.694167: Current learning rate: 0.00405
2025-10-15 10:48:18.209294: Validation loss did not improve from -0.53170. Patience: 62/50
2025-10-15 10:48:18.209831: train_loss -0.7703
2025-10-15 10:48:18.210068: val_loss -0.4723
2025-10-15 10:48:18.210324: Pseudo dice [np.float32(0.7153)]
2025-10-15 10:48:18.210557: Epoch time: 46.52 s
2025-10-15 10:48:18.844002: 
2025-10-15 10:48:18.844246: Epoch 96
2025-10-15 10:48:18.844468: Current learning rate: 0.00399
2025-10-15 10:49:05.365527: Validation loss did not improve from -0.53170. Patience: 63/50
2025-10-15 10:49:05.366399: train_loss -0.7687
2025-10-15 10:49:05.366672: val_loss -0.4777
2025-10-15 10:49:05.366884: Pseudo dice [np.float32(0.7188)]
2025-10-15 10:49:05.367104: Epoch time: 46.52 s
2025-10-15 10:49:06.006531: 
2025-10-15 10:49:06.007025: Epoch 97
2025-10-15 10:49:06.007460: Current learning rate: 0.00392
2025-10-15 10:49:52.566995: Validation loss did not improve from -0.53170. Patience: 64/50
2025-10-15 10:49:52.567717: train_loss -0.7688
2025-10-15 10:49:52.567974: val_loss -0.497
2025-10-15 10:49:52.568283: Pseudo dice [np.float32(0.7276)]
2025-10-15 10:49:52.568571: Epoch time: 46.56 s
2025-10-15 10:49:53.221798: 
2025-10-15 10:49:53.222292: Epoch 98
2025-10-15 10:49:53.222655: Current learning rate: 0.00385
2025-10-15 10:50:39.936171: Validation loss did not improve from -0.53170. Patience: 65/50
2025-10-15 10:50:39.937791: train_loss -0.7749
2025-10-15 10:50:39.938163: val_loss -0.4591
2025-10-15 10:50:39.938497: Pseudo dice [np.float32(0.7077)]
2025-10-15 10:50:39.938865: Epoch time: 46.72 s
2025-10-15 10:50:40.594760: 
2025-10-15 10:50:40.595215: Epoch 99
2025-10-15 10:50:40.595595: Current learning rate: 0.00379
2025-10-15 10:51:27.247503: Validation loss did not improve from -0.53170. Patience: 66/50
2025-10-15 10:51:27.248159: train_loss -0.7717
2025-10-15 10:51:27.248388: val_loss -0.4867
2025-10-15 10:51:27.248595: Pseudo dice [np.float32(0.7131)]
2025-10-15 10:51:27.248763: Epoch time: 46.65 s
2025-10-15 10:51:28.424260: 
2025-10-15 10:51:28.424600: Epoch 100
2025-10-15 10:51:28.424815: Current learning rate: 0.00372
2025-10-15 10:52:15.015179: Validation loss did not improve from -0.53170. Patience: 67/50
2025-10-15 10:52:15.016169: train_loss -0.7749
2025-10-15 10:52:15.016353: val_loss -0.4967
2025-10-15 10:52:15.016477: Pseudo dice [np.float32(0.7255)]
2025-10-15 10:52:15.016660: Epoch time: 46.59 s
2025-10-15 10:52:15.673585: 
2025-10-15 10:52:15.673877: Epoch 101
2025-10-15 10:52:15.674068: Current learning rate: 0.00365
2025-10-15 10:53:02.308506: Validation loss did not improve from -0.53170. Patience: 68/50
2025-10-15 10:53:02.309026: train_loss -0.774
2025-10-15 10:53:02.309396: val_loss -0.4898
2025-10-15 10:53:02.309624: Pseudo dice [np.float32(0.7288)]
2025-10-15 10:53:02.309883: Epoch time: 46.64 s
2025-10-15 10:53:02.310113: Yayy! New best EMA pseudo Dice: 0.7150999903678894
2025-10-15 10:53:03.425817: 
2025-10-15 10:53:03.426309: Epoch 102
2025-10-15 10:53:03.426694: Current learning rate: 0.00359
2025-10-15 10:53:50.007962: Validation loss did not improve from -0.53170. Patience: 69/50
2025-10-15 10:53:50.009055: train_loss -0.7755
2025-10-15 10:53:50.009523: val_loss -0.4546
2025-10-15 10:53:50.009806: Pseudo dice [np.float32(0.7012)]
2025-10-15 10:53:50.010062: Epoch time: 46.58 s
2025-10-15 10:53:50.650735: 
2025-10-15 10:53:50.651090: Epoch 103
2025-10-15 10:53:50.651340: Current learning rate: 0.00352
2025-10-15 10:54:37.442958: Validation loss did not improve from -0.53170. Patience: 70/50
2025-10-15 10:54:37.443490: train_loss -0.7795
2025-10-15 10:54:37.443688: val_loss -0.4609
2025-10-15 10:54:37.443812: Pseudo dice [np.float32(0.7153)]
2025-10-15 10:54:37.443957: Epoch time: 46.79 s
2025-10-15 10:54:38.096045: 
2025-10-15 10:54:38.096429: Epoch 104
2025-10-15 10:54:38.096648: Current learning rate: 0.00345
2025-10-15 10:55:24.693397: Validation loss did not improve from -0.53170. Patience: 71/50
2025-10-15 10:55:24.693957: train_loss -0.7783
2025-10-15 10:55:24.694269: val_loss -0.5092
2025-10-15 10:55:24.694403: Pseudo dice [np.float32(0.7341)]
2025-10-15 10:55:24.694543: Epoch time: 46.6 s
2025-10-15 10:55:25.173308: Yayy! New best EMA pseudo Dice: 0.7159000039100647
2025-10-15 10:55:26.697853: 
2025-10-15 10:55:26.698563: Epoch 105
2025-10-15 10:55:26.698876: Current learning rate: 0.00338
2025-10-15 10:56:13.309103: Validation loss did not improve from -0.53170. Patience: 72/50
2025-10-15 10:56:13.309648: train_loss -0.778
2025-10-15 10:56:13.309862: val_loss -0.4448
2025-10-15 10:56:13.310089: Pseudo dice [np.float32(0.6979)]
2025-10-15 10:56:13.310308: Epoch time: 46.61 s
2025-10-15 10:56:13.960426: 
2025-10-15 10:56:13.960910: Epoch 106
2025-10-15 10:56:13.961261: Current learning rate: 0.00332
2025-10-15 10:57:00.470473: Validation loss did not improve from -0.53170. Patience: 73/50
2025-10-15 10:57:00.471724: train_loss -0.7749
2025-10-15 10:57:00.472110: val_loss -0.4303
2025-10-15 10:57:00.472468: Pseudo dice [np.float32(0.6875)]
2025-10-15 10:57:00.472842: Epoch time: 46.51 s
2025-10-15 10:57:01.120525: 
2025-10-15 10:57:01.120880: Epoch 107
2025-10-15 10:57:01.121097: Current learning rate: 0.00325
2025-10-15 10:57:47.620897: Validation loss did not improve from -0.53170. Patience: 74/50
2025-10-15 10:57:47.621413: train_loss -0.7785
2025-10-15 10:57:47.621578: val_loss -0.4934
2025-10-15 10:57:47.621792: Pseudo dice [np.float32(0.7221)]
2025-10-15 10:57:47.622009: Epoch time: 46.5 s
2025-10-15 10:57:48.262300: 
2025-10-15 10:57:48.262548: Epoch 108
2025-10-15 10:57:48.262758: Current learning rate: 0.00318
2025-10-15 10:58:34.783580: Validation loss did not improve from -0.53170. Patience: 75/50
2025-10-15 10:58:34.784600: train_loss -0.7799
2025-10-15 10:58:34.784907: val_loss -0.4637
2025-10-15 10:58:34.785187: Pseudo dice [np.float32(0.7054)]
2025-10-15 10:58:34.785429: Epoch time: 46.52 s
2025-10-15 10:58:35.427151: 
2025-10-15 10:58:35.427472: Epoch 109
2025-10-15 10:58:35.427683: Current learning rate: 0.00311
2025-10-15 10:59:21.905099: Validation loss did not improve from -0.53170. Patience: 76/50
2025-10-15 10:59:21.905734: train_loss -0.7809
2025-10-15 10:59:21.906186: val_loss -0.4932
2025-10-15 10:59:21.906515: Pseudo dice [np.float32(0.7172)]
2025-10-15 10:59:21.906792: Epoch time: 46.48 s
2025-10-15 10:59:23.012934: 
2025-10-15 10:59:23.013226: Epoch 110
2025-10-15 10:59:23.013512: Current learning rate: 0.00304
2025-10-15 11:00:09.526114: Validation loss did not improve from -0.53170. Patience: 77/50
2025-10-15 11:00:09.526977: train_loss -0.7818
2025-10-15 11:00:09.527195: val_loss -0.4852
2025-10-15 11:00:09.527381: Pseudo dice [np.float32(0.7117)]
2025-10-15 11:00:09.527665: Epoch time: 46.51 s
2025-10-15 11:00:10.176993: 
2025-10-15 11:00:10.177329: Epoch 111
2025-10-15 11:00:10.177533: Current learning rate: 0.00297
2025-10-15 11:00:56.743173: Validation loss did not improve from -0.53170. Patience: 78/50
2025-10-15 11:00:56.743742: train_loss -0.7809
2025-10-15 11:00:56.744007: val_loss -0.4615
2025-10-15 11:00:56.744308: Pseudo dice [np.float32(0.7158)]
2025-10-15 11:00:56.744608: Epoch time: 46.57 s
2025-10-15 11:00:57.392725: 
2025-10-15 11:00:57.393374: Epoch 112
2025-10-15 11:00:57.393635: Current learning rate: 0.00291
2025-10-15 11:01:44.035469: Validation loss did not improve from -0.53170. Patience: 79/50
2025-10-15 11:01:44.036046: train_loss -0.7841
2025-10-15 11:01:44.036198: val_loss -0.4426
2025-10-15 11:01:44.036353: Pseudo dice [np.float32(0.7082)]
2025-10-15 11:01:44.036497: Epoch time: 46.64 s
2025-10-15 11:01:44.695008: 
2025-10-15 11:01:44.695410: Epoch 113
2025-10-15 11:01:44.695738: Current learning rate: 0.00284
2025-10-15 11:02:31.224234: Validation loss did not improve from -0.53170. Patience: 80/50
2025-10-15 11:02:31.224812: train_loss -0.7838
2025-10-15 11:02:31.225145: val_loss -0.4688
2025-10-15 11:02:31.225434: Pseudo dice [np.float32(0.711)]
2025-10-15 11:02:31.225776: Epoch time: 46.53 s
2025-10-15 11:02:31.878804: 
2025-10-15 11:02:31.879270: Epoch 114
2025-10-15 11:02:31.879596: Current learning rate: 0.00277
2025-10-15 11:03:18.503683: Validation loss did not improve from -0.53170. Patience: 81/50
2025-10-15 11:03:18.504446: train_loss -0.7828
2025-10-15 11:03:18.504673: val_loss -0.502
2025-10-15 11:03:18.504842: Pseudo dice [np.float32(0.7317)]
2025-10-15 11:03:18.505023: Epoch time: 46.63 s
2025-10-15 11:03:19.608553: 
2025-10-15 11:03:19.609331: Epoch 115
2025-10-15 11:03:19.609719: Current learning rate: 0.0027
2025-10-15 11:04:06.139259: Validation loss did not improve from -0.53170. Patience: 82/50
2025-10-15 11:04:06.139703: train_loss -0.7845
2025-10-15 11:04:06.139850: val_loss -0.4565
2025-10-15 11:04:06.139997: Pseudo dice [np.float32(0.705)]
2025-10-15 11:04:06.140165: Epoch time: 46.53 s
2025-10-15 11:04:06.783905: 
2025-10-15 11:04:06.784181: Epoch 116
2025-10-15 11:04:06.784419: Current learning rate: 0.00263
2025-10-15 11:04:53.418652: Validation loss did not improve from -0.53170. Patience: 83/50
2025-10-15 11:04:53.419290: train_loss -0.7858
2025-10-15 11:04:53.419456: val_loss -0.4974
2025-10-15 11:04:53.419613: Pseudo dice [np.float32(0.7271)]
2025-10-15 11:04:53.419796: Epoch time: 46.64 s
2025-10-15 11:04:54.083662: 
2025-10-15 11:04:54.084198: Epoch 117
2025-10-15 11:04:54.084631: Current learning rate: 0.00256
2025-10-15 11:05:40.776883: Validation loss did not improve from -0.53170. Patience: 84/50
2025-10-15 11:05:40.777385: train_loss -0.7882
2025-10-15 11:05:40.777566: val_loss -0.4812
2025-10-15 11:05:40.777692: Pseudo dice [np.float32(0.7134)]
2025-10-15 11:05:40.777829: Epoch time: 46.69 s
2025-10-15 11:05:41.429361: 
2025-10-15 11:05:41.429725: Epoch 118
2025-10-15 11:05:41.429906: Current learning rate: 0.00249
2025-10-15 11:06:28.034210: Validation loss did not improve from -0.53170. Patience: 85/50
2025-10-15 11:06:28.034838: train_loss -0.7872
2025-10-15 11:06:28.035028: val_loss -0.4514
2025-10-15 11:06:28.035191: Pseudo dice [np.float32(0.7154)]
2025-10-15 11:06:28.035324: Epoch time: 46.61 s
2025-10-15 11:06:28.692321: 
2025-10-15 11:06:28.692675: Epoch 119
2025-10-15 11:06:28.692901: Current learning rate: 0.00242
2025-10-15 11:07:15.301555: Validation loss did not improve from -0.53170. Patience: 86/50
2025-10-15 11:07:15.301937: train_loss -0.7871
2025-10-15 11:07:15.302110: val_loss -0.4693
2025-10-15 11:07:15.302334: Pseudo dice [np.float32(0.7068)]
2025-10-15 11:07:15.302571: Epoch time: 46.61 s
2025-10-15 11:07:16.816673: 
2025-10-15 11:07:16.816925: Epoch 120
2025-10-15 11:07:16.817125: Current learning rate: 0.00235
2025-10-15 11:08:03.323821: Validation loss did not improve from -0.53170. Patience: 87/50
2025-10-15 11:08:03.324629: train_loss -0.7905
2025-10-15 11:08:03.324835: val_loss -0.4485
2025-10-15 11:08:03.325038: Pseudo dice [np.float32(0.7093)]
2025-10-15 11:08:03.325259: Epoch time: 46.51 s
2025-10-15 11:08:03.975686: 
2025-10-15 11:08:03.976096: Epoch 121
2025-10-15 11:08:03.976302: Current learning rate: 0.00228
2025-10-15 11:08:50.498063: Validation loss did not improve from -0.53170. Patience: 88/50
2025-10-15 11:08:50.498787: train_loss -0.7895
2025-10-15 11:08:50.499193: val_loss -0.4521
2025-10-15 11:08:50.499524: Pseudo dice [np.float32(0.7106)]
2025-10-15 11:08:50.499860: Epoch time: 46.52 s
2025-10-15 11:08:51.146962: 
2025-10-15 11:08:51.147312: Epoch 122
2025-10-15 11:08:51.147511: Current learning rate: 0.00221
2025-10-15 11:09:37.814242: Validation loss did not improve from -0.53170. Patience: 89/50
2025-10-15 11:09:37.815418: train_loss -0.7914
2025-10-15 11:09:37.815756: val_loss -0.4916
2025-10-15 11:09:37.816111: Pseudo dice [np.float32(0.7287)]
2025-10-15 11:09:37.816470: Epoch time: 46.67 s
2025-10-15 11:09:38.471918: 
2025-10-15 11:09:38.472354: Epoch 123
2025-10-15 11:09:38.472759: Current learning rate: 0.00214
2025-10-15 11:10:25.306807: Validation loss did not improve from -0.53170. Patience: 90/50
2025-10-15 11:10:25.307352: train_loss -0.7868
2025-10-15 11:10:25.307607: val_loss -0.4566
2025-10-15 11:10:25.307898: Pseudo dice [np.float32(0.7091)]
2025-10-15 11:10:25.308081: Epoch time: 46.84 s
2025-10-15 11:10:25.960508: 
2025-10-15 11:10:25.960855: Epoch 124
2025-10-15 11:10:25.961047: Current learning rate: 0.00207
2025-10-15 11:11:12.732179: Validation loss did not improve from -0.53170. Patience: 91/50
2025-10-15 11:11:12.732791: train_loss -0.7918
2025-10-15 11:11:12.733071: val_loss -0.4718
2025-10-15 11:11:12.733265: Pseudo dice [np.float32(0.719)]
2025-10-15 11:11:12.733439: Epoch time: 46.77 s
2025-10-15 11:11:13.843432: 
2025-10-15 11:11:13.843742: Epoch 125
2025-10-15 11:11:13.844050: Current learning rate: 0.00199
2025-10-15 11:12:00.562576: Validation loss did not improve from -0.53170. Patience: 92/50
2025-10-15 11:12:00.563002: train_loss -0.7885
2025-10-15 11:12:00.563209: val_loss -0.4619
2025-10-15 11:12:00.563348: Pseudo dice [np.float32(0.7087)]
2025-10-15 11:12:00.563537: Epoch time: 46.72 s
2025-10-15 11:12:01.213406: 
2025-10-15 11:12:01.213831: Epoch 126
2025-10-15 11:12:01.214219: Current learning rate: 0.00192
2025-10-15 11:12:47.738187: Validation loss did not improve from -0.53170. Patience: 93/50
2025-10-15 11:12:47.738883: train_loss -0.7898
2025-10-15 11:12:47.739024: val_loss -0.4698
2025-10-15 11:12:47.739206: Pseudo dice [np.float32(0.7163)]
2025-10-15 11:12:47.739357: Epoch time: 46.53 s
2025-10-15 11:12:48.384823: 
2025-10-15 11:12:48.385212: Epoch 127
2025-10-15 11:12:48.385546: Current learning rate: 0.00185
2025-10-15 11:13:34.975146: Validation loss did not improve from -0.53170. Patience: 94/50
2025-10-15 11:13:34.975550: train_loss -0.7921
2025-10-15 11:13:34.975762: val_loss -0.478
2025-10-15 11:13:34.975898: Pseudo dice [np.float32(0.7249)]
2025-10-15 11:13:34.976065: Epoch time: 46.59 s
2025-10-15 11:13:35.620256: 
2025-10-15 11:13:35.620664: Epoch 128
2025-10-15 11:13:35.621056: Current learning rate: 0.00178
2025-10-15 11:14:22.147180: Validation loss did not improve from -0.53170. Patience: 95/50
2025-10-15 11:14:22.148441: train_loss -0.7949
2025-10-15 11:14:22.149192: val_loss -0.4522
2025-10-15 11:14:22.149492: Pseudo dice [np.float32(0.7038)]
2025-10-15 11:14:22.149747: Epoch time: 46.53 s
2025-10-15 11:14:22.798659: 
2025-10-15 11:14:22.799028: Epoch 129
2025-10-15 11:14:22.799419: Current learning rate: 0.0017
2025-10-15 11:15:09.585271: Validation loss did not improve from -0.53170. Patience: 96/50
2025-10-15 11:15:09.585834: train_loss -0.7936
2025-10-15 11:15:09.586028: val_loss -0.4461
2025-10-15 11:15:09.586306: Pseudo dice [np.float32(0.7039)]
2025-10-15 11:15:09.586531: Epoch time: 46.79 s
2025-10-15 11:15:10.718065: 
2025-10-15 11:15:10.718443: Epoch 130
2025-10-15 11:15:10.718779: Current learning rate: 0.00163
2025-10-15 11:15:57.353294: Validation loss did not improve from -0.53170. Patience: 97/50
2025-10-15 11:15:57.353850: train_loss -0.7932
2025-10-15 11:15:57.354042: val_loss -0.4876
2025-10-15 11:15:57.354168: Pseudo dice [np.float32(0.7166)]
2025-10-15 11:15:57.354300: Epoch time: 46.64 s
2025-10-15 11:15:57.998310: 
2025-10-15 11:15:57.998657: Epoch 131
2025-10-15 11:15:57.998834: Current learning rate: 0.00156
2025-10-15 11:16:44.650039: Validation loss did not improve from -0.53170. Patience: 98/50
2025-10-15 11:16:44.650505: train_loss -0.7937
2025-10-15 11:16:44.650666: val_loss -0.4945
2025-10-15 11:16:44.650798: Pseudo dice [np.float32(0.7305)]
2025-10-15 11:16:44.650949: Epoch time: 46.65 s
2025-10-15 11:16:45.294608: 
2025-10-15 11:16:45.294909: Epoch 132
2025-10-15 11:16:45.295101: Current learning rate: 0.00148
2025-10-15 11:17:31.887218: Validation loss did not improve from -0.53170. Patience: 99/50
2025-10-15 11:17:31.887961: train_loss -0.7936
2025-10-15 11:17:31.888182: val_loss -0.437
2025-10-15 11:17:31.888305: Pseudo dice [np.float32(0.7041)]
2025-10-15 11:17:31.888475: Epoch time: 46.59 s
2025-10-15 11:17:32.544821: 
2025-10-15 11:17:32.545165: Epoch 133
2025-10-15 11:17:32.545380: Current learning rate: 0.00141
2025-10-15 11:18:19.182033: Validation loss did not improve from -0.53170. Patience: 100/50
2025-10-15 11:18:19.182592: train_loss -0.7965
2025-10-15 11:18:19.182845: val_loss -0.4253
2025-10-15 11:18:19.183059: Pseudo dice [np.float32(0.7036)]
2025-10-15 11:18:19.183264: Epoch time: 46.64 s
2025-10-15 11:18:19.844692: 
2025-10-15 11:18:19.844999: Epoch 134
2025-10-15 11:18:19.845198: Current learning rate: 0.00133
2025-10-15 11:19:06.430160: Validation loss did not improve from -0.53170. Patience: 101/50
2025-10-15 11:19:06.430820: train_loss -0.7965
2025-10-15 11:19:06.430968: val_loss -0.4825
2025-10-15 11:19:06.431120: Pseudo dice [np.float32(0.7167)]
2025-10-15 11:19:06.431254: Epoch time: 46.59 s
2025-10-15 11:19:08.073425: 
2025-10-15 11:19:08.073797: Epoch 135
2025-10-15 11:19:08.074015: Current learning rate: 0.00126
2025-10-15 11:19:54.620820: Validation loss did not improve from -0.53170. Patience: 102/50
2025-10-15 11:19:54.621298: train_loss -0.7977
2025-10-15 11:19:54.621460: val_loss -0.482
2025-10-15 11:19:54.621607: Pseudo dice [np.float32(0.7248)]
2025-10-15 11:19:54.621742: Epoch time: 46.55 s
2025-10-15 11:19:55.273239: 
2025-10-15 11:19:55.273538: Epoch 136
2025-10-15 11:19:55.273755: Current learning rate: 0.00118
2025-10-15 11:20:41.820174: Validation loss did not improve from -0.53170. Patience: 103/50
2025-10-15 11:20:41.820823: train_loss -0.7958
2025-10-15 11:20:41.820967: val_loss -0.4687
2025-10-15 11:20:41.821098: Pseudo dice [np.float32(0.7132)]
2025-10-15 11:20:41.821231: Epoch time: 46.55 s
2025-10-15 11:20:42.486784: 
2025-10-15 11:20:42.487128: Epoch 137
2025-10-15 11:20:42.487334: Current learning rate: 0.00111
2025-10-15 11:21:29.092901: Validation loss did not improve from -0.53170. Patience: 104/50
2025-10-15 11:21:29.093480: train_loss -0.7987
2025-10-15 11:21:29.093820: val_loss -0.4436
2025-10-15 11:21:29.094156: Pseudo dice [np.float32(0.7033)]
2025-10-15 11:21:29.094473: Epoch time: 46.61 s
2025-10-15 11:21:29.755437: 
2025-10-15 11:21:29.755867: Epoch 138
2025-10-15 11:21:29.756100: Current learning rate: 0.00103
2025-10-15 11:22:16.346167: Validation loss did not improve from -0.53170. Patience: 105/50
2025-10-15 11:22:16.347042: train_loss -0.7965
2025-10-15 11:22:16.347187: val_loss -0.4691
2025-10-15 11:22:16.347297: Pseudo dice [np.float32(0.7208)]
2025-10-15 11:22:16.347420: Epoch time: 46.59 s
2025-10-15 11:22:17.006782: 
2025-10-15 11:22:17.007135: Epoch 139
2025-10-15 11:22:17.007324: Current learning rate: 0.00095
2025-10-15 11:23:03.541454: Validation loss did not improve from -0.53170. Patience: 106/50
2025-10-15 11:23:03.541980: train_loss -0.7988
2025-10-15 11:23:03.542146: val_loss -0.48
2025-10-15 11:23:03.542257: Pseudo dice [np.float32(0.7165)]
2025-10-15 11:23:03.542411: Epoch time: 46.54 s
2025-10-15 11:23:04.632239: 
2025-10-15 11:23:04.632513: Epoch 140
2025-10-15 11:23:04.632707: Current learning rate: 0.00087
2025-10-15 11:23:51.180848: Validation loss did not improve from -0.53170. Patience: 107/50
2025-10-15 11:23:51.181578: train_loss -0.7997
2025-10-15 11:23:51.181715: val_loss -0.4481
2025-10-15 11:23:51.181823: Pseudo dice [np.float32(0.7017)]
2025-10-15 11:23:51.181948: Epoch time: 46.55 s
2025-10-15 11:23:51.833424: 
2025-10-15 11:23:51.833773: Epoch 141
2025-10-15 11:23:51.833959: Current learning rate: 0.00079
2025-10-15 11:24:38.387094: Validation loss did not improve from -0.53170. Patience: 108/50
2025-10-15 11:24:38.387633: train_loss -0.7972
2025-10-15 11:24:38.387778: val_loss -0.4477
2025-10-15 11:24:38.387905: Pseudo dice [np.float32(0.7051)]
2025-10-15 11:24:38.388123: Epoch time: 46.55 s
2025-10-15 11:24:39.046705: 
2025-10-15 11:24:39.047060: Epoch 142
2025-10-15 11:24:39.047294: Current learning rate: 0.00071
2025-10-15 11:25:25.628088: Validation loss did not improve from -0.53170. Patience: 109/50
2025-10-15 11:25:25.628732: train_loss -0.8006
2025-10-15 11:25:25.628930: val_loss -0.4476
2025-10-15 11:25:25.629115: Pseudo dice [np.float32(0.7061)]
2025-10-15 11:25:25.629271: Epoch time: 46.58 s
2025-10-15 11:25:26.300847: 
2025-10-15 11:25:26.301175: Epoch 143
2025-10-15 11:25:26.301397: Current learning rate: 0.00063
2025-10-15 11:26:12.901025: Validation loss did not improve from -0.53170. Patience: 110/50
2025-10-15 11:26:12.901492: train_loss -0.7995
2025-10-15 11:26:12.901660: val_loss -0.4768
2025-10-15 11:26:12.901787: Pseudo dice [np.float32(0.7193)]
2025-10-15 11:26:12.901956: Epoch time: 46.6 s
2025-10-15 11:26:13.550384: 
2025-10-15 11:26:13.550716: Epoch 144
2025-10-15 11:26:13.550941: Current learning rate: 0.00055
2025-10-15 11:27:00.080309: Validation loss did not improve from -0.53170. Patience: 111/50
2025-10-15 11:27:00.081012: train_loss -0.7995
2025-10-15 11:27:00.081260: val_loss -0.485
2025-10-15 11:27:00.081440: Pseudo dice [np.float32(0.7246)]
2025-10-15 11:27:00.081653: Epoch time: 46.53 s
2025-10-15 11:27:01.210842: 
2025-10-15 11:27:01.211214: Epoch 145
2025-10-15 11:27:01.211446: Current learning rate: 0.00047
2025-10-15 11:27:47.914616: Validation loss did not improve from -0.53170. Patience: 112/50
2025-10-15 11:27:47.915160: train_loss -0.7994
2025-10-15 11:27:47.915342: val_loss -0.4593
2025-10-15 11:27:47.915498: Pseudo dice [np.float32(0.6988)]
2025-10-15 11:27:47.915654: Epoch time: 46.7 s
2025-10-15 11:27:48.571985: 
2025-10-15 11:27:48.572314: Epoch 146
2025-10-15 11:27:48.572572: Current learning rate: 0.00038
2025-10-15 11:28:35.277204: Validation loss did not improve from -0.53170. Patience: 113/50
2025-10-15 11:28:35.277948: train_loss -0.7994
2025-10-15 11:28:35.278168: val_loss -0.4323
2025-10-15 11:28:35.278333: Pseudo dice [np.float32(0.7005)]
2025-10-15 11:28:35.278494: Epoch time: 46.71 s
2025-10-15 11:28:35.931075: 
2025-10-15 11:28:35.931350: Epoch 147
2025-10-15 11:28:35.931549: Current learning rate: 0.0003
2025-10-15 11:29:22.633019: Validation loss did not improve from -0.53170. Patience: 114/50
2025-10-15 11:29:22.633565: train_loss -0.802
2025-10-15 11:29:22.633812: val_loss -0.4343
2025-10-15 11:29:22.633979: Pseudo dice [np.float32(0.6839)]
2025-10-15 11:29:22.634210: Epoch time: 46.7 s
2025-10-15 11:29:23.288658: 
2025-10-15 11:29:23.289021: Epoch 148
2025-10-15 11:29:23.289245: Current learning rate: 0.00021
2025-10-15 11:30:09.939143: Validation loss did not improve from -0.53170. Patience: 115/50
2025-10-15 11:30:09.939893: train_loss -0.8044
2025-10-15 11:30:09.940094: val_loss -0.488
2025-10-15 11:30:09.940276: Pseudo dice [np.float32(0.7242)]
2025-10-15 11:30:09.940469: Epoch time: 46.65 s
2025-10-15 11:30:10.605051: 
2025-10-15 11:30:10.605375: Epoch 149
2025-10-15 11:30:10.605584: Current learning rate: 0.00011
2025-10-15 11:30:57.278972: Validation loss did not improve from -0.53170. Patience: 116/50
2025-10-15 11:30:57.279499: train_loss -0.8007
2025-10-15 11:30:57.279638: val_loss -0.4718
2025-10-15 11:30:57.279751: Pseudo dice [np.float32(0.7135)]
2025-10-15 11:30:57.279962: Epoch time: 46.68 s
2025-10-15 11:30:58.907708: Training done.
2025-10-15 11:30:58.998757: Using splits from existing split file: /nfs/erelab001/shared/Computational_Group/Naravich/datasets/nnUNet_Datasets/nnUNet_preprocessed/Dataset307_Sohee_Calcium_OCT_CrossValidation/splits_final_80.json
2025-10-15 11:30:58.999350: The split file contains 5 splits.
2025-10-15 11:30:58.999631: Desired fold for training: 4
2025-10-15 11:30:59.000257: This split has 6 training and 5 validation cases.
2025-10-15 11:30:59.000770: predicting 101-044
2025-10-15 11:30:59.004403: 101-044, shape torch.Size([1, 404, 498, 498]), rank 0
2025-10-15 11:31:49.867568: predicting 401-004
2025-10-15 11:31:49.879220: 401-004, shape torch.Size([1, 375, 498, 498]), rank 0
2025-10-15 11:32:24.379819: predicting 701-013
2025-10-15 11:32:24.388531: 701-013, shape torch.Size([1, 375, 498, 498]), rank 0
2025-10-15 11:32:58.778233: predicting 704-003
2025-10-15 11:32:58.786453: 704-003, shape torch.Size([1, 375, 498, 498]), rank 0
2025-10-15 11:33:33.191775: predicting 706-005
2025-10-15 11:33:33.204373: 706-005, shape torch.Size([1, 375, 498, 498]), rank 0
2025-10-15 11:34:20.806941: Validation complete
2025-10-15 11:34:20.807183: Mean Validation Dice:  0.7042743587576573
Finished training fold 4 saving to /nfs/erelab001/shared/Computational_Group/Naravich/datasets/nnUNet_Datasets/nnUNet_results/Dataset307_Sohee_Calcium_OCT_CrossValidation/nnUNetTrainerScaleAnalysis80__nnUNetPlans__3d_32x160x128_b10/fold_4_Genesis_Pretrained
