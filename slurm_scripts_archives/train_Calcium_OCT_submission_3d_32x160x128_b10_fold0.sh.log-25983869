/home/gridsan/nchutisilp/.conda/envs/nnunet/bin/python
wandb: Tracking run with wandb version 0.16.6
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.

############################
INFO: You are using the old nnU-Net default plans. We have updated our recommendations. Please consider using those instead! Read more here: https://github.com/MIC-DKFZ/nnUNet/blob/master/documentation/resenc_presets.md
############################

Using device: cuda:0

#######################################################################
Please cite the following paper when using nnU-Net:
Isensee, F., Jaeger, P. F., Kohl, S. A., Petersen, J., & Maier-Hein, K. H. (2021). nnU-Net: a self-configuring method for deep learning-based biomedical image segmentation. Nature methods, 18(2), 203-211.
#######################################################################

2024-05-07 14:00:14.433638: Using torch.compile...
/home/gridsan/nchutisilp/.local/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn("The verbose parameter is deprecated. Please use get_last_lr() "
################### Loading pretrained weights from file  /home/gridsan/nchutisilp/datasets/nnUNet_Datasets/nnUNet_results/Dataset300_Lumen_and_Wall_OCT/nnUNetTrainer__nnUNetPreprocessPlans__3d_fullres/fold_all/checkpoint_best.pth ###################
Below is the list of overlapping blocks in pretrained model and nnUNet architecture:
encoder.stages.0.0.convs.0.conv.weight shape torch.Size([32, 1, 3, 3, 3])
encoder.stages.0.0.convs.0.conv.bias shape torch.Size([32])
encoder.stages.0.0.convs.0.norm.weight shape torch.Size([32])
encoder.stages.0.0.convs.0.norm.bias shape torch.Size([32])
encoder.stages.0.0.convs.0.all_modules.0.weight shape torch.Size([32, 1, 3, 3, 3])
encoder.stages.0.0.convs.0.all_modules.0.bias shape torch.Size([32])
encoder.stages.0.0.convs.0.all_modules.1.weight shape torch.Size([32])
encoder.stages.0.0.convs.0.all_modules.1.bias shape torch.Size([32])
encoder.stages.0.0.convs.1.conv.weight shape torch.Size([32, 32, 3, 3, 3])
encoder.stages.0.0.convs.1.conv.bias shape torch.Size([32])
encoder.stages.0.0.convs.1.norm.weight shape torch.Size([32])
encoder.stages.0.0.convs.1.norm.bias shape torch.Size([32])
encoder.stages.0.0.convs.1.all_modules.0.weight shape torch.Size([32, 32, 3, 3, 3])
encoder.stages.0.0.convs.1.all_modules.0.bias shape torch.Size([32])
encoder.stages.0.0.convs.1.all_modules.1.weight shape torch.Size([32])
encoder.stages.0.0.convs.1.all_modules.1.bias shape torch.Size([32])
encoder.stages.1.0.convs.0.conv.weight shape torch.Size([64, 32, 3, 3, 3])
encoder.stages.1.0.convs.0.conv.bias shape torch.Size([64])
encoder.stages.1.0.convs.0.norm.weight shape torch.Size([64])
encoder.stages.1.0.convs.0.norm.bias shape torch.Size([64])
encoder.stages.1.0.convs.0.all_modules.0.weight shape torch.Size([64, 32, 3, 3, 3])
encoder.stages.1.0.convs.0.all_modules.0.bias shape torch.Size([64])
encoder.stages.1.0.convs.0.all_modules.1.weight shape torch.Size([64])
encoder.stages.1.0.convs.0.all_modules.1.bias shape torch.Size([64])
encoder.stages.1.0.convs.1.conv.weight shape torch.Size([64, 64, 3, 3, 3])
encoder.stages.1.0.convs.1.conv.bias shape torch.Size([64])
encoder.stages.1.0.convs.1.norm.weight shape torch.Size([64])
encoder.stages.1.0.convs.1.norm.bias shape torch.Size([64])
encoder.stages.1.0.convs.1.all_modules.0.weight shape torch.Size([64, 64, 3, 3, 3])
encoder.stages.1.0.convs.1.all_modules.0.bias shape torch.Size([64])
encoder.stages.1.0.convs.1.all_modules.1.weight shape torch.Size([64])
encoder.stages.1.0.convs.1.all_modules.1.bias shape torch.Size([64])
encoder.stages.2.0.convs.0.conv.weight shape torch.Size([128, 64, 3, 3, 3])
encoder.stages.2.0.convs.0.conv.bias shape torch.Size([128])
encoder.stages.2.0.convs.0.norm.weight shape torch.Size([128])
encoder.stages.2.0.convs.0.norm.bias shape torch.Size([128])
encoder.stages.2.0.convs.0.all_modules.0.weight shape torch.Size([128, 64, 3, 3, 3])
encoder.stages.2.0.convs.0.all_modules.0.bias shape torch.Size([128])
encoder.stages.2.0.convs.0.all_modules.1.weight shape torch.Size([128])
encoder.stages.2.0.convs.0.all_modules.1.bias shape torch.Size([128])
encoder.stages.2.0.convs.1.conv.weight shape torch.Size([128, 128, 3, 3, 3])
encoder.stages.2.0.convs.1.conv.bias shape torch.Size([128])
encoder.stages.2.0.convs.1.norm.weight shape torch.Size([128])
encoder.stages.2.0.convs.1.norm.bias shape torch.Size([128])
encoder.stages.2.0.convs.1.all_modules.0.weight shape torch.Size([128, 128, 3, 3, 3])
encoder.stages.2.0.convs.1.all_modules.0.bias shape torch.Size([128])
encoder.stages.2.0.convs.1.all_modules.1.weight shape torch.Size([128])
encoder.stages.2.0.convs.1.all_modules.1.bias shape torch.Size([128])
encoder.stages.3.0.convs.0.conv.weight shape torch.Size([256, 128, 3, 3, 3])
encoder.stages.3.0.convs.0.conv.bias shape torch.Size([256])
encoder.stages.3.0.convs.0.norm.weight shape torch.Size([256])
encoder.stages.3.0.convs.0.norm.bias shape torch.Size([256])
encoder.stages.3.0.convs.0.all_modules.0.weight shape torch.Size([256, 128, 3, 3, 3])
encoder.stages.3.0.convs.0.all_modules.0.bias shape torch.Size([256])
encoder.stages.3.0.convs.0.all_modules.1.weight shape torch.Size([256])
encoder.stages.3.0.convs.0.all_modules.1.bias shape torch.Size([256])
encoder.stages.3.0.convs.1.conv.weight shape torch.Size([256, 256, 3, 3, 3])
encoder.stages.3.0.convs.1.conv.bias shape torch.Size([256])
encoder.stages.3.0.convs.1.norm.weight shape torch.Size([256])
encoder.stages.3.0.convs.1.norm.bias shape torch.Size([256])
encoder.stages.3.0.convs.1.all_modules.0.weight shape torch.Size([256, 256, 3, 3, 3])
encoder.stages.3.0.convs.1.all_modules.0.bias shape torch.Size([256])
encoder.stages.3.0.convs.1.all_modules.1.weight shape torch.Size([256])
encoder.stages.3.0.convs.1.all_modules.1.bias shape torch.Size([256])
encoder.stages.4.0.convs.0.conv.weight shape torch.Size([320, 256, 3, 3, 3])
encoder.stages.4.0.convs.0.conv.bias shape torch.Size([320])
encoder.stages.4.0.convs.0.norm.weight shape torch.Size([320])
encoder.stages.4.0.convs.0.norm.bias shape torch.Size([320])
encoder.stages.4.0.convs.0.all_modules.0.weight shape torch.Size([320, 256, 3, 3, 3])
encoder.stages.4.0.convs.0.all_modules.0.bias shape torch.Size([320])
encoder.stages.4.0.convs.0.all_modules.1.weight shape torch.Size([320])
encoder.stages.4.0.convs.0.all_modules.1.bias shape torch.Size([320])
encoder.stages.4.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.4.0.convs.1.conv.bias shape torch.Size([320])
encoder.stages.4.0.convs.1.norm.weight shape torch.Size([320])
encoder.stages.4.0.convs.1.norm.bias shape torch.Size([320])
encoder.stages.4.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.4.0.convs.1.all_modules.0.bias shape torch.Size([320])
encoder.stages.4.0.convs.1.all_modules.1.weight shape torch.Size([320])
encoder.stages.4.0.convs.1.all_modules.1.bias shape torch.Size([320])
encoder.stages.5.0.convs.0.conv.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.5.0.convs.0.conv.bias shape torch.Size([320])
encoder.stages.5.0.convs.0.norm.weight shape torch.Size([320])
encoder.stages.5.0.convs.0.norm.bias shape torch.Size([320])
encoder.stages.5.0.convs.0.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.5.0.convs.0.all_modules.0.bias shape torch.Size([320])
encoder.stages.5.0.convs.0.all_modules.1.weight shape torch.Size([320])
encoder.stages.5.0.convs.0.all_modules.1.bias shape torch.Size([320])
encoder.stages.5.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.5.0.convs.1.conv.bias shape torch.Size([320])
encoder.stages.5.0.convs.1.norm.weight shape torch.Size([320])
encoder.stages.5.0.convs.1.norm.bias shape torch.Size([320])
encoder.stages.5.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.5.0.convs.1.all_modules.0.bias shape torch.Size([320])
encoder.stages.5.0.convs.1.all_modules.1.weight shape torch.Size([320])
encoder.stages.5.0.convs.1.all_modules.1.bias shape torch.Size([320])
decoder.encoder.stages.0.0.convs.0.conv.weight shape torch.Size([32, 1, 3, 3, 3])
decoder.encoder.stages.0.0.convs.0.conv.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.norm.weight shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.norm.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.all_modules.0.weight shape torch.Size([32, 1, 3, 3, 3])
decoder.encoder.stages.0.0.convs.0.all_modules.0.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.all_modules.1.weight shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.all_modules.1.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.conv.weight shape torch.Size([32, 32, 3, 3, 3])
decoder.encoder.stages.0.0.convs.1.conv.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.norm.weight shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.norm.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.all_modules.0.weight shape torch.Size([32, 32, 3, 3, 3])
decoder.encoder.stages.0.0.convs.1.all_modules.0.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.all_modules.1.weight shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.all_modules.1.bias shape torch.Size([32])
decoder.encoder.stages.1.0.convs.0.conv.weight shape torch.Size([64, 32, 3, 3, 3])
decoder.encoder.stages.1.0.convs.0.conv.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.norm.weight shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.norm.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.all_modules.0.weight shape torch.Size([64, 32, 3, 3, 3])
decoder.encoder.stages.1.0.convs.0.all_modules.0.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.all_modules.1.weight shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.all_modules.1.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.conv.weight shape torch.Size([64, 64, 3, 3, 3])
decoder.encoder.stages.1.0.convs.1.conv.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.norm.weight shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.norm.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.all_modules.0.weight shape torch.Size([64, 64, 3, 3, 3])
decoder.encoder.stages.1.0.convs.1.all_modules.0.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.all_modules.1.weight shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.all_modules.1.bias shape torch.Size([64])
decoder.encoder.stages.2.0.convs.0.conv.weight shape torch.Size([128, 64, 3, 3, 3])
decoder.encoder.stages.2.0.convs.0.conv.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.norm.weight shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.norm.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.all_modules.0.weight shape torch.Size([128, 64, 3, 3, 3])
decoder.encoder.stages.2.0.convs.0.all_modules.0.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.all_modules.1.weight shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.all_modules.1.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.conv.weight shape torch.Size([128, 128, 3, 3, 3])
decoder.encoder.stages.2.0.convs.1.conv.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.norm.weight shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.norm.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.all_modules.0.weight shape torch.Size([128, 128, 3, 3, 3])
decoder.encoder.stages.2.0.convs.1.all_modules.0.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.all_modules.1.weight shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.all_modules.1.bias shape torch.Size([128])
decoder.encoder.stages.3.0.convs.0.conv.weight shape torch.Size([256, 128, 3, 3, 3])
decoder.encoder.stages.3.0.convs.0.conv.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.norm.weight shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.norm.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.all_modules.0.weight shape torch.Size([256, 128, 3, 3, 3])
decoder.encoder.stages.3.0.convs.0.all_modules.0.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.all_modules.1.weight shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.all_modules.1.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.conv.weight shape torch.Size([256, 256, 3, 3, 3])
decoder.encoder.stages.3.0.convs.1.conv.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.norm.weight shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.norm.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.all_modules.0.weight shape torch.Size([256, 256, 3, 3, 3])
decoder.encoder.stages.3.0.convs.1.all_modules.0.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.all_modules.1.weight shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.all_modules.1.bias shape torch.Size([256])
decoder.encoder.stages.4.0.convs.0.conv.weight shape torch.Size([320, 256, 3, 3, 3])
decoder.encoder.stages.4.0.convs.0.conv.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.norm.weight shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.norm.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.all_modules.0.weight shape torch.Size([320, 256, 3, 3, 3])
decoder.encoder.stages.4.0.convs.0.all_modules.0.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.all_modules.1.weight shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.all_modules.1.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.4.0.convs.1.conv.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.norm.weight shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.norm.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.4.0.convs.1.all_modules.0.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.all_modules.1.weight shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.all_modules.1.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.conv.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.5.0.convs.0.conv.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.norm.weight shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.norm.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.5.0.convs.0.all_modules.0.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.all_modules.1.weight shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.all_modules.1.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.5.0.convs.1.conv.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.norm.weight shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.norm.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.5.0.convs.1.all_modules.0.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.all_modules.1.weight shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.all_modules.1.bias shape torch.Size([320])
decoder.stages.0.convs.0.conv.weight shape torch.Size([320, 640, 3, 3, 3])
decoder.stages.0.convs.0.conv.bias shape torch.Size([320])
decoder.stages.0.convs.0.norm.weight shape torch.Size([320])
decoder.stages.0.convs.0.norm.bias shape torch.Size([320])
decoder.stages.0.convs.0.all_modules.0.weight shape torch.Size([320, 640, 3, 3, 3])
decoder.stages.0.convs.0.all_modules.0.bias shape torch.Size([320])
decoder.stages.0.convs.0.all_modules.1.weight shape torch.Size([320])
decoder.stages.0.convs.0.all_modules.1.bias shape torch.Size([320])
decoder.stages.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.stages.0.convs.1.conv.bias shape torch.Size([320])
decoder.stages.0.convs.1.norm.weight shape torch.Size([320])
decoder.stages.0.convs.1.norm.bias shape torch.Size([320])
decoder.stages.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.stages.0.convs.1.all_modules.0.bias shape torch.Size([320])
decoder.stages.0.convs.1.all_modules.1.weight shape torch.Size([320])
decoder.stages.0.convs.1.all_modules.1.bias shape torch.Size([320])
decoder.stages.1.convs.0.conv.weight shape torch.Size([256, 512, 3, 3, 3])
decoder.stages.1.convs.0.conv.bias shape torch.Size([256])
decoder.stages.1.convs.0.norm.weight shape torch.Size([256])
decoder.stages.1.convs.0.norm.bias shape torch.Size([256])
decoder.stages.1.convs.0.all_modules.0.weight shape torch.Size([256, 512, 3, 3, 3])
decoder.stages.1.convs.0.all_modules.0.bias shape torch.Size([256])
decoder.stages.1.convs.0.all_modules.1.weight shape torch.Size([256])
decoder.stages.1.convs.0.all_modules.1.bias shape torch.Size([256])
decoder.stages.1.convs.1.conv.weight shape torch.Size([256, 256, 3, 3, 3])
decoder.stages.1.convs.1.conv.bias shape torch.Size([256])
decoder.stages.1.convs.1.norm.weight shape torch.Size([256])
decoder.stages.1.convs.1.norm.bias shape torch.Size([256])
decoder.stages.1.convs.1.all_modules.0.weight shape torch.Size([256, 256, 3, 3, 3])
decoder.stages.1.convs.1.all_modules.0.bias shape torch.Size([256])
decoder.stages.1.convs.1.all_modules.1.weight shape torch.Size([256])
decoder.stages.1.convs.1.all_modules.1.bias shape torch.Size([256])
decoder.stages.2.convs.0.conv.weight shape torch.Size([128, 256, 3, 3, 3])
decoder.stages.2.convs.0.conv.bias shape torch.Size([128])
decoder.stages.2.convs.0.norm.weight shape torch.Size([128])
decoder.stages.2.convs.0.norm.bias shape torch.Size([128])
decoder.stages.2.convs.0.all_modules.0.weight shape torch.Size([128, 256, 3, 3, 3])
decoder.stages.2.convs.0.all_modules.0.bias shape torch.Size([128])
decoder.stages.2.convs.0.all_modules.1.weight shape torch.Size([128])
decoder.stages.2.convs.0.all_modules.1.bias shape torch.Size([128])
decoder.stages.2.convs.1.conv.weight shape torch.Size([128, 128, 3, 3, 3])
decoder.stages.2.convs.1.conv.bias shape torch.Size([128])
decoder.stages.2.convs.1.norm.weight shape torch.Size([128])
decoder.stages.2.convs.1.norm.bias shape torch.Size([128])
decoder.stages.2.convs.1.all_modules.0.weight shape torch.Size([128, 128, 3, 3, 3])
decoder.stages.2.convs.1.all_modules.0.bias shape torch.Size([128])
decoder.stages.2.convs.1.all_modules.1.weight shape torch.Size([128])
decoder.stages.2.convs.1.all_modules.1.bias shape torch.Size([128])
decoder.stages.3.convs.0.conv.weight shape torch.Size([64, 128, 3, 3, 3])
decoder.stages.3.convs.0.conv.bias shape torch.Size([64])
decoder.stages.3.convs.0.norm.weight shape torch.Size([64])
decoder.stages.3.convs.0.norm.bias shape torch.Size([64])
decoder.stages.3.convs.0.all_modules.0.weight shape torch.Size([64, 128, 3, 3, 3])
decoder.stages.3.convs.0.all_modules.0.bias shape torch.Size([64])
decoder.stages.3.convs.0.all_modules.1.weight shape torch.Size([64])
decoder.stages.3.convs.0.all_modules.1.bias shape torch.Size([64])
decoder.stages.3.convs.1.conv.weight shape torch.Size([64, 64, 3, 3, 3])
decoder.stages.3.convs.1.conv.bias shape torch.Size([64])
decoder.stages.3.convs.1.norm.weight shape torch.Size([64])
decoder.stages.3.convs.1.norm.bias shape torch.Size([64])
decoder.stages.3.convs.1.all_modules.0.weight shape torch.Size([64, 64, 3, 3, 3])
decoder.stages.3.convs.1.all_modules.0.bias shape torch.Size([64])
decoder.stages.3.convs.1.all_modules.1.weight shape torch.Size([64])
decoder.stages.3.convs.1.all_modules.1.bias shape torch.Size([64])
decoder.stages.4.convs.0.conv.weight shape torch.Size([32, 64, 3, 3, 3])
decoder.stages.4.convs.0.conv.bias shape torch.Size([32])
decoder.stages.4.convs.0.norm.weight shape torch.Size([32])
decoder.stages.4.convs.0.norm.bias shape torch.Size([32])
decoder.stages.4.convs.0.all_modules.0.weight shape torch.Size([32, 64, 3, 3, 3])
decoder.stages.4.convs.0.all_modules.0.bias shape torch.Size([32])
decoder.stages.4.convs.0.all_modules.1.weight shape torch.Size([32])
decoder.stages.4.convs.0.all_modules.1.bias shape torch.Size([32])
decoder.stages.4.convs.1.conv.weight shape torch.Size([32, 32, 3, 3, 3])
decoder.stages.4.convs.1.conv.bias shape torch.Size([32])
decoder.stages.4.convs.1.norm.weight shape torch.Size([32])
decoder.stages.4.convs.1.norm.bias shape torch.Size([32])
decoder.stages.4.convs.1.all_modules.0.weight shape torch.Size([32, 32, 3, 3, 3])
decoder.stages.4.convs.1.all_modules.0.bias shape torch.Size([32])
decoder.stages.4.convs.1.all_modules.1.weight shape torch.Size([32])
decoder.stages.4.convs.1.all_modules.1.bias shape torch.Size([32])
decoder.transpconvs.0.weight shape torch.Size([320, 320, 1, 2, 2])
decoder.transpconvs.0.bias shape torch.Size([320])
decoder.transpconvs.1.weight shape torch.Size([320, 256, 2, 2, 2])
decoder.transpconvs.1.bias shape torch.Size([256])
decoder.transpconvs.2.weight shape torch.Size([256, 128, 2, 2, 2])
decoder.transpconvs.2.bias shape torch.Size([128])
decoder.transpconvs.3.weight shape torch.Size([128, 64, 2, 2, 2])
decoder.transpconvs.3.bias shape torch.Size([64])
decoder.transpconvs.4.weight shape torch.Size([64, 32, 2, 2, 2])
decoder.transpconvs.4.bias shape torch.Size([32])
################### Done ###################
2024-05-07 14:00:22.199903: do_dummy_2d_data_aug: True
2024-05-07 14:00:22.203841: Using splits from existing split file: /home/gridsan/nchutisilp/datasets/nnUNet_Datasets/nnUNet_preprocessed/Dataset302_Calcium_OCTv2/splits_final.json
2024-05-07 14:00:22.206811: The split file contains 3 splits.
2024-05-07 14:00:22.208674: Desired fold for training: 0
2024-05-07 14:00:22.210093: This split has 4 training and 2 validation cases.
using pin_memory on device 0
using pin_memory on device 0

This is the configuration used by this training:
Configuration name: 3d_32x160x128_b10
 {'data_identifier': 'nnUNetPlans_3d_fullres', 'preprocessor_name': 'DefaultPreprocessor', 'batch_size': 10, 'patch_size': [32, 160, 128], 'median_image_size_in_voxels': [375.0, 498.0, 498.0], 'spacing': [1.0, 1.0, 1.0], 'normalization_schemes': ['ZScoreNormalization'], 'use_mask_for_norm': [False], 'resampling_fn_data': 'resample_data_or_seg_to_shape', 'resampling_fn_seg': 'resample_data_or_seg_to_shape', 'resampling_fn_data_kwargs': {'is_seg': False, 'order': 3, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_seg_kwargs': {'is_seg': True, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_probabilities': 'resample_data_or_seg_to_shape', 'resampling_fn_probabilities_kwargs': {'is_seg': False, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'architecture': {'network_class_name': 'dynamic_network_architectures.architectures.unet.PlainConvUNet', 'arch_kwargs': {'n_stages': 6, 'features_per_stage': [32, 64, 128, 256, 320, 320], 'conv_op': 'torch.nn.modules.conv.Conv3d', 'kernel_sizes': [[3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3]], 'strides': [[1, 1, 1], [2, 2, 2], [2, 2, 2], [2, 2, 2], [2, 2, 2], [1, 2, 2]], 'n_conv_per_stage': [2, 2, 2, 2, 2, 2], 'n_conv_per_stage_decoder': [2, 2, 2, 2, 2], 'conv_bias': True, 'norm_op': 'torch.nn.modules.instancenorm.InstanceNorm3d', 'norm_op_kwargs': {'eps': 1e-05, 'affine': True}, 'dropout_op': None, 'dropout_op_kwargs': None, 'nonlin': 'torch.nn.LeakyReLU', 'nonlin_kwargs': {'inplace': True}, 'deep_supervision': True}, '_kw_requires_import': ['conv_op', 'norm_op', 'dropout_op', 'nonlin']}, 'batch_dice': True, 'inherits_from': '3d_fullres'} 

These are the global plan.json settings:
 {'dataset_name': 'Dataset302_Calcium_OCTv2', 'plans_name': 'nnUNetPlans', 'original_median_spacing_after_transp': [1.0, 1.0, 1.0], 'original_median_shape_after_transp': [375, 498, 498], 'image_reader_writer': 'SimpleITKIO', 'transpose_forward': [0, 1, 2], 'transpose_backward': [0, 1, 2], 'experiment_planner_used': 'ExperimentPlanner', 'label_manager': 'LabelManager', 'foreground_intensity_properties_per_channel': {'0': {'max': 0.4977976381778717, 'mean': 0.13332499563694, 'median': 0.08871842920780182, 'min': 0.0, 'percentile_00_5': 0.014754901640117168, 'percentile_99_5': 0.4977976381778717, 'std': 0.12022686749696732}}} 

2024-05-07 14:00:28.183119: unpacking dataset...
2024-05-07 14:00:34.241705: unpacking done...
2024-05-07 14:00:34.258434: Unable to plot network architecture: nnUNet_compile is enabled!
2024-05-07 14:00:34.357683: 
2024-05-07 14:00:34.359750: Epoch 0
2024-05-07 14:00:34.361502: Current learning rate: 0.01
2024-05-07 14:04:31.253808: Validation loss improved from 1000.00000 to -0.42477! Patience: 0/50
2024-05-07 14:04:31.314934: train_loss -0.3112
2024-05-07 14:04:31.318118: val_loss -0.4248
2024-05-07 14:04:31.319304: Pseudo dice [0.6761]
2024-05-07 14:04:31.320395: Epoch time: 236.9 s
2024-05-07 14:04:31.321481: Yayy! New best EMA pseudo Dice: 0.6761
2024-05-07 14:04:33.779461: 
2024-05-07 14:04:33.781978: Epoch 1
2024-05-07 14:04:33.783756: Current learning rate: 0.00999
2024-05-07 14:06:02.803315: Validation loss did not improve from -0.42477. Patience: 1/50
2024-05-07 14:06:02.806331: train_loss -0.4909
2024-05-07 14:06:02.809015: val_loss -0.4191
2024-05-07 14:06:02.810853: Pseudo dice [0.6699]
2024-05-07 14:06:02.812744: Epoch time: 89.03 s
2024-05-07 14:06:04.011607: 
2024-05-07 14:06:04.015656: Epoch 2
2024-05-07 14:06:04.018646: Current learning rate: 0.00998
2024-05-07 14:07:33.406437: Validation loss improved from -0.42477 to -0.45921! Patience: 1/50
2024-05-07 14:07:33.409753: train_loss -0.5186
2024-05-07 14:07:33.412000: val_loss -0.4592
2024-05-07 14:07:33.413395: Pseudo dice [0.7007]
2024-05-07 14:07:33.414666: Epoch time: 89.4 s
2024-05-07 14:07:33.416054: Yayy! New best EMA pseudo Dice: 0.678
2024-05-07 14:07:35.043305: 
2024-05-07 14:07:35.046447: Epoch 3
2024-05-07 14:07:35.047843: Current learning rate: 0.00997
2024-05-07 14:09:04.492459: Validation loss improved from -0.45921 to -0.47868! Patience: 0/50
2024-05-07 14:09:04.496092: train_loss -0.5472
2024-05-07 14:09:04.497863: val_loss -0.4787
2024-05-07 14:09:04.499087: Pseudo dice [0.7064]
2024-05-07 14:09:04.500380: Epoch time: 89.45 s
2024-05-07 14:09:04.501645: Yayy! New best EMA pseudo Dice: 0.6808
2024-05-07 14:09:06.085440: 
2024-05-07 14:09:06.090086: Epoch 4
2024-05-07 14:09:06.091907: Current learning rate: 0.00996
2024-05-07 14:10:35.595157: Validation loss did not improve from -0.47868. Patience: 1/50
2024-05-07 14:10:35.598572: train_loss -0.5698
2024-05-07 14:10:35.600552: val_loss -0.4578
2024-05-07 14:10:35.601908: Pseudo dice [0.696]
2024-05-07 14:10:35.603276: Epoch time: 89.51 s
2024-05-07 14:10:35.955212: Yayy! New best EMA pseudo Dice: 0.6823
2024-05-07 14:10:37.579018: 
2024-05-07 14:10:37.582563: Epoch 5
2024-05-07 14:10:37.584078: Current learning rate: 0.00995
2024-05-07 14:12:07.116669: Validation loss did not improve from -0.47868. Patience: 2/50
2024-05-07 14:12:07.119813: train_loss -0.5686
2024-05-07 14:12:07.121166: val_loss -0.4327
2024-05-07 14:12:07.122101: Pseudo dice [0.6805]
2024-05-07 14:12:07.123085: Epoch time: 89.54 s
2024-05-07 14:12:08.295260: 
2024-05-07 14:12:08.298957: Epoch 6
2024-05-07 14:12:08.300359: Current learning rate: 0.00995
2024-05-07 14:13:37.794810: Validation loss did not improve from -0.47868. Patience: 3/50
2024-05-07 14:13:37.797578: train_loss -0.5851
2024-05-07 14:13:37.799503: val_loss -0.4656
2024-05-07 14:13:37.800744: Pseudo dice [0.694]
2024-05-07 14:13:37.802508: Epoch time: 89.5 s
2024-05-07 14:13:37.803553: Yayy! New best EMA pseudo Dice: 0.6833
2024-05-07 14:13:39.416129: 
2024-05-07 14:13:39.419796: Epoch 7
2024-05-07 14:13:39.421759: Current learning rate: 0.00994
2024-05-07 14:15:08.877148: Validation loss improved from -0.47868 to -0.50022! Patience: 3/50
2024-05-07 14:15:08.879226: train_loss -0.6084
2024-05-07 14:15:08.880994: val_loss -0.5002
2024-05-07 14:15:08.882414: Pseudo dice [0.7116]
2024-05-07 14:15:08.883815: Epoch time: 89.46 s
2024-05-07 14:15:08.885053: Yayy! New best EMA pseudo Dice: 0.6862
2024-05-07 14:15:10.467983: 
2024-05-07 14:15:10.471288: Epoch 8
2024-05-07 14:15:10.472502: Current learning rate: 0.00993
2024-05-07 14:16:40.335792: Validation loss did not improve from -0.50022. Patience: 1/50
2024-05-07 14:16:40.339122: train_loss -0.6202
2024-05-07 14:16:40.340889: val_loss -0.4548
2024-05-07 14:16:40.342223: Pseudo dice [0.7026]
2024-05-07 14:16:40.343180: Epoch time: 89.87 s
2024-05-07 14:16:40.344127: Yayy! New best EMA pseudo Dice: 0.6878
2024-05-07 14:16:42.384430: 
2024-05-07 14:16:42.387759: Epoch 9
2024-05-07 14:16:42.389558: Current learning rate: 0.00992
2024-05-07 14:18:12.435211: Validation loss did not improve from -0.50022. Patience: 2/50
2024-05-07 14:18:12.438096: train_loss -0.6166
2024-05-07 14:18:12.439981: val_loss -0.4794
2024-05-07 14:18:12.441260: Pseudo dice [0.7043]
2024-05-07 14:18:12.442295: Epoch time: 90.05 s
2024-05-07 14:18:12.796478: Yayy! New best EMA pseudo Dice: 0.6895
2024-05-07 14:18:14.311451: 
2024-05-07 14:18:14.314905: Epoch 10
2024-05-07 14:18:14.316420: Current learning rate: 0.00991
2024-05-07 14:19:44.238301: Validation loss did not improve from -0.50022. Patience: 3/50
2024-05-07 14:19:44.241229: train_loss -0.6235
2024-05-07 14:19:44.243083: val_loss -0.4683
2024-05-07 14:19:44.244236: Pseudo dice [0.6911]
2024-05-07 14:19:44.245270: Epoch time: 89.93 s
2024-05-07 14:19:44.246195: Yayy! New best EMA pseudo Dice: 0.6896
2024-05-07 14:19:45.800846: 
2024-05-07 14:19:45.804046: Epoch 11
2024-05-07 14:19:45.805234: Current learning rate: 0.0099
2024-05-07 14:21:15.800304: Validation loss improved from -0.50022 to -0.50245! Patience: 3/50
2024-05-07 14:21:15.803308: train_loss -0.6382
2024-05-07 14:21:15.805140: val_loss -0.5024
2024-05-07 14:21:15.806433: Pseudo dice [0.7179]
2024-05-07 14:21:15.807439: Epoch time: 90.0 s
2024-05-07 14:21:15.808353: Yayy! New best EMA pseudo Dice: 0.6925
2024-05-07 14:21:17.365460: 
2024-05-07 14:21:17.368063: Epoch 12
2024-05-07 14:21:17.369080: Current learning rate: 0.00989
2024-05-07 14:22:47.298133: Validation loss improved from -0.50245 to -0.50947! Patience: 0/50
2024-05-07 14:22:47.300524: train_loss -0.6409
2024-05-07 14:22:47.301721: val_loss -0.5095
2024-05-07 14:22:47.302804: Pseudo dice [0.7189]
2024-05-07 14:22:47.303924: Epoch time: 89.94 s
2024-05-07 14:22:47.304833: Yayy! New best EMA pseudo Dice: 0.6951
2024-05-07 14:22:48.960876: 
2024-05-07 14:22:48.964587: Epoch 13
2024-05-07 14:22:48.966089: Current learning rate: 0.00988
2024-05-07 14:24:18.836138: Validation loss did not improve from -0.50947. Patience: 1/50
2024-05-07 14:24:18.839790: train_loss -0.6447
2024-05-07 14:24:18.841672: val_loss -0.4907
2024-05-07 14:24:18.842975: Pseudo dice [0.7174]
2024-05-07 14:24:18.844338: Epoch time: 89.88 s
2024-05-07 14:24:18.845670: Yayy! New best EMA pseudo Dice: 0.6973
2024-05-07 14:24:20.476452: 
2024-05-07 14:24:20.479541: Epoch 14
2024-05-07 14:24:20.481534: Current learning rate: 0.00987
2024-05-07 14:25:50.351181: Validation loss improved from -0.50947 to -0.52531! Patience: 1/50
2024-05-07 14:25:50.353677: train_loss -0.6562
2024-05-07 14:25:50.354874: val_loss -0.5253
2024-05-07 14:25:50.355918: Pseudo dice [0.7402]
2024-05-07 14:25:50.357087: Epoch time: 89.88 s
2024-05-07 14:25:50.841628: Yayy! New best EMA pseudo Dice: 0.7016
2024-05-07 14:25:52.739229: 
2024-05-07 14:25:52.741948: Epoch 15
2024-05-07 14:25:52.743386: Current learning rate: 0.00986
2024-05-07 14:27:22.795205: Validation loss improved from -0.52531 to -0.52718! Patience: 0/50
2024-05-07 14:27:22.798168: train_loss -0.655
2024-05-07 14:27:22.799711: val_loss -0.5272
2024-05-07 14:27:22.801052: Pseudo dice [0.7277]
2024-05-07 14:27:22.802738: Epoch time: 90.06 s
2024-05-07 14:27:22.804774: Yayy! New best EMA pseudo Dice: 0.7042
2024-05-07 14:27:24.421505: 
2024-05-07 14:27:24.425189: Epoch 16
2024-05-07 14:27:24.427011: Current learning rate: 0.00986
2024-05-07 14:28:54.938625: Validation loss did not improve from -0.52718. Patience: 1/50
2024-05-07 14:28:54.940776: train_loss -0.6566
2024-05-07 14:28:54.942155: val_loss -0.5126
2024-05-07 14:28:54.943402: Pseudo dice [0.7321]
2024-05-07 14:28:54.944578: Epoch time: 90.52 s
2024-05-07 14:28:54.945733: Yayy! New best EMA pseudo Dice: 0.707
2024-05-07 14:28:56.584145: 
2024-05-07 14:28:56.588281: Epoch 17
2024-05-07 14:28:56.590391: Current learning rate: 0.00985
2024-05-07 14:30:27.232470: Validation loss improved from -0.52718 to -0.52752! Patience: 1/50
2024-05-07 14:30:27.235597: train_loss -0.6608
2024-05-07 14:30:27.237185: val_loss -0.5275
2024-05-07 14:30:27.238652: Pseudo dice [0.7362]
2024-05-07 14:30:27.239773: Epoch time: 90.65 s
2024-05-07 14:30:27.240806: Yayy! New best EMA pseudo Dice: 0.7099
2024-05-07 14:30:28.999225: 
2024-05-07 14:30:29.002836: Epoch 18
2024-05-07 14:30:29.003847: Current learning rate: 0.00984
2024-05-07 14:31:59.711911: Validation loss did not improve from -0.52752. Patience: 1/50
2024-05-07 14:31:59.715098: train_loss -0.668
2024-05-07 14:31:59.716537: val_loss -0.4969
2024-05-07 14:31:59.717796: Pseudo dice [0.7142]
2024-05-07 14:31:59.718927: Epoch time: 90.72 s
2024-05-07 14:31:59.719906: Yayy! New best EMA pseudo Dice: 0.7104
2024-05-07 14:32:01.386746: 
2024-05-07 14:32:01.390247: Epoch 19
2024-05-07 14:32:01.391967: Current learning rate: 0.00983
2024-05-07 14:33:31.890757: Validation loss did not improve from -0.52752. Patience: 2/50
2024-05-07 14:33:31.893439: train_loss -0.6698
2024-05-07 14:33:31.894873: val_loss -0.5274
2024-05-07 14:33:31.895906: Pseudo dice [0.7244]
2024-05-07 14:33:31.897096: Epoch time: 90.51 s
2024-05-07 14:33:32.262201: Yayy! New best EMA pseudo Dice: 0.7118
2024-05-07 14:33:34.268963: 
2024-05-07 14:33:34.272404: Epoch 20
2024-05-07 14:33:34.273831: Current learning rate: 0.00982
2024-05-07 14:35:04.629267: Validation loss improved from -0.52752 to -0.53749! Patience: 2/50
2024-05-07 14:35:04.632097: train_loss -0.6827
2024-05-07 14:35:04.633394: val_loss -0.5375
2024-05-07 14:35:04.634351: Pseudo dice [0.7399]
2024-05-07 14:35:04.635352: Epoch time: 90.36 s
2024-05-07 14:35:04.636228: Yayy! New best EMA pseudo Dice: 0.7146
2024-05-07 14:35:06.306873: 
2024-05-07 14:35:06.310529: Epoch 21
2024-05-07 14:35:06.311976: Current learning rate: 0.00981
2024-05-07 14:36:36.618614: Validation loss did not improve from -0.53749. Patience: 1/50
2024-05-07 14:36:36.621984: train_loss -0.6828
2024-05-07 14:36:36.624030: val_loss -0.5197
2024-05-07 14:36:36.625140: Pseudo dice [0.7327]
2024-05-07 14:36:36.626330: Epoch time: 90.32 s
2024-05-07 14:36:36.627359: Yayy! New best EMA pseudo Dice: 0.7164
2024-05-07 14:36:38.227087: 
2024-05-07 14:36:38.230675: Epoch 22
2024-05-07 14:36:38.232443: Current learning rate: 0.0098
2024-05-07 14:38:08.601953: Validation loss did not improve from -0.53749. Patience: 2/50
2024-05-07 14:38:08.604733: train_loss -0.6807
2024-05-07 14:38:08.606158: val_loss -0.5063
2024-05-07 14:38:08.607228: Pseudo dice [0.7255]
2024-05-07 14:38:08.608363: Epoch time: 90.38 s
2024-05-07 14:38:08.609522: Yayy! New best EMA pseudo Dice: 0.7173
2024-05-07 14:38:10.261590: 
2024-05-07 14:38:10.267112: Epoch 23
2024-05-07 14:38:10.268410: Current learning rate: 0.00979
2024-05-07 14:39:48.447416: Validation loss did not improve from -0.53749. Patience: 3/50
2024-05-07 14:39:48.450497: train_loss -0.6773
2024-05-07 14:39:48.452395: val_loss -0.5236
2024-05-07 14:39:48.453535: Pseudo dice [0.726]
2024-05-07 14:39:48.454738: Epoch time: 98.19 s
2024-05-07 14:39:48.455957: Yayy! New best EMA pseudo Dice: 0.7182
2024-05-07 14:39:50.072325: 
2024-05-07 14:39:50.075403: Epoch 24
2024-05-07 14:39:50.076562: Current learning rate: 0.00978
2024-05-07 14:41:20.421347: Validation loss did not improve from -0.53749. Patience: 4/50
2024-05-07 14:41:20.424563: train_loss -0.6848
2024-05-07 14:41:20.426193: val_loss -0.5293
2024-05-07 14:41:20.427538: Pseudo dice [0.7342]
2024-05-07 14:41:20.428999: Epoch time: 90.35 s
2024-05-07 14:41:20.804877: Yayy! New best EMA pseudo Dice: 0.7198
2024-05-07 14:41:22.434669: 
2024-05-07 14:41:22.437733: Epoch 25
2024-05-07 14:41:22.439711: Current learning rate: 0.00977
2024-05-07 14:42:53.284486: Validation loss did not improve from -0.53749. Patience: 5/50
2024-05-07 14:42:53.287299: train_loss -0.6881
2024-05-07 14:42:53.288528: val_loss -0.5284
2024-05-07 14:42:53.289558: Pseudo dice [0.7328]
2024-05-07 14:42:53.290469: Epoch time: 90.85 s
2024-05-07 14:42:53.291286: Yayy! New best EMA pseudo Dice: 0.7211
2024-05-07 14:42:54.957230: 
2024-05-07 14:42:54.960435: Epoch 26
2024-05-07 14:42:54.962162: Current learning rate: 0.00977
2024-05-07 14:44:25.688454: Validation loss did not improve from -0.53749. Patience: 6/50
2024-05-07 14:44:25.691742: train_loss -0.7033
2024-05-07 14:44:25.693444: val_loss -0.5242
2024-05-07 14:44:25.694714: Pseudo dice [0.7332]
2024-05-07 14:44:25.696103: Epoch time: 90.74 s
2024-05-07 14:44:25.697580: Yayy! New best EMA pseudo Dice: 0.7223
2024-05-07 14:44:27.310364: 
2024-05-07 14:44:27.313981: Epoch 27
2024-05-07 14:44:27.315923: Current learning rate: 0.00976
2024-05-07 14:45:58.124640: Validation loss did not improve from -0.53749. Patience: 7/50
2024-05-07 14:45:58.127266: train_loss -0.7041
2024-05-07 14:45:58.128511: val_loss -0.5352
2024-05-07 14:45:58.129624: Pseudo dice [0.7435]
2024-05-07 14:45:58.130531: Epoch time: 90.82 s
2024-05-07 14:45:58.131358: Yayy! New best EMA pseudo Dice: 0.7244
2024-05-07 14:45:59.817858: 
2024-05-07 14:45:59.821611: Epoch 28
2024-05-07 14:45:59.823100: Current learning rate: 0.00975
2024-05-07 14:47:30.878981: Validation loss did not improve from -0.53749. Patience: 8/50
2024-05-07 14:47:30.881405: train_loss -0.6953
2024-05-07 14:47:30.882542: val_loss -0.5176
2024-05-07 14:47:30.883704: Pseudo dice [0.7288]
2024-05-07 14:47:30.884916: Epoch time: 91.07 s
2024-05-07 14:47:30.885889: Yayy! New best EMA pseudo Dice: 0.7248
2024-05-07 14:47:32.524832: 
2024-05-07 14:47:32.528280: Epoch 29
2024-05-07 14:47:32.529339: Current learning rate: 0.00974
2024-05-07 14:49:03.375088: Validation loss did not improve from -0.53749. Patience: 9/50
2024-05-07 14:49:03.378071: train_loss -0.7044
2024-05-07 14:49:03.379166: val_loss -0.5252
2024-05-07 14:49:03.380057: Pseudo dice [0.7363]
2024-05-07 14:49:03.381264: Epoch time: 90.85 s
2024-05-07 14:49:03.764680: Yayy! New best EMA pseudo Dice: 0.726
2024-05-07 14:49:05.462090: 
2024-05-07 14:49:05.465527: Epoch 30
2024-05-07 14:49:05.467788: Current learning rate: 0.00973
2024-05-07 14:50:36.526199: Validation loss did not improve from -0.53749. Patience: 10/50
2024-05-07 14:50:36.528294: train_loss -0.7026
2024-05-07 14:50:36.529625: val_loss -0.5196
2024-05-07 14:50:36.530886: Pseudo dice [0.7399]
2024-05-07 14:50:36.532037: Epoch time: 91.07 s
2024-05-07 14:50:36.533514: Yayy! New best EMA pseudo Dice: 0.7274
2024-05-07 14:50:38.638357: 
2024-05-07 14:50:38.641532: Epoch 31
2024-05-07 14:50:38.643496: Current learning rate: 0.00972
2024-05-07 14:52:09.674314: Validation loss improved from -0.53749 to -0.55774! Patience: 10/50
2024-05-07 14:52:09.676505: train_loss -0.7067
2024-05-07 14:52:09.678101: val_loss -0.5577
2024-05-07 14:52:09.679697: Pseudo dice [0.7509]
2024-05-07 14:52:09.680967: Epoch time: 91.04 s
2024-05-07 14:52:09.682040: Yayy! New best EMA pseudo Dice: 0.7297
2024-05-07 14:52:11.382535: 
2024-05-07 14:52:11.386882: Epoch 32
2024-05-07 14:52:11.388816: Current learning rate: 0.00971
2024-05-07 14:53:42.376277: Validation loss did not improve from -0.55774. Patience: 1/50
2024-05-07 14:53:42.379143: train_loss -0.7128
2024-05-07 14:53:42.380449: val_loss -0.5152
2024-05-07 14:53:42.381430: Pseudo dice [0.732]
2024-05-07 14:53:42.382459: Epoch time: 91.0 s
2024-05-07 14:53:42.383508: Yayy! New best EMA pseudo Dice: 0.73
2024-05-07 14:53:44.055489: 
2024-05-07 14:53:44.058813: Epoch 33
2024-05-07 14:53:44.095680: Current learning rate: 0.0097
2024-05-07 14:55:15.657523: Validation loss did not improve from -0.55774. Patience: 2/50
2024-05-07 14:55:15.659821: train_loss -0.7146
2024-05-07 14:55:15.661497: val_loss -0.5221
2024-05-07 14:55:15.662444: Pseudo dice [0.7342]
2024-05-07 14:55:15.663645: Epoch time: 91.61 s
2024-05-07 14:55:15.664810: Yayy! New best EMA pseudo Dice: 0.7304
2024-05-07 14:55:17.407886: 
2024-05-07 14:55:17.410884: Epoch 34
2024-05-07 14:55:17.411927: Current learning rate: 0.00969
2024-05-07 14:56:48.325173: Validation loss did not improve from -0.55774. Patience: 3/50
2024-05-07 14:56:48.327333: train_loss -0.7079
2024-05-07 14:56:48.328637: val_loss -0.5523
2024-05-07 14:56:48.329624: Pseudo dice [0.746]
2024-05-07 14:56:48.330869: Epoch time: 90.92 s
2024-05-07 14:56:48.720009: Yayy! New best EMA pseudo Dice: 0.7319
2024-05-07 14:56:50.409726: 
2024-05-07 14:56:50.412310: Epoch 35
2024-05-07 14:56:50.413821: Current learning rate: 0.00968
2024-05-07 14:58:21.758638: Validation loss did not improve from -0.55774. Patience: 4/50
2024-05-07 14:58:21.761064: train_loss -0.721
2024-05-07 14:58:21.762943: val_loss -0.5566
2024-05-07 14:58:21.764281: Pseudo dice [0.747]
2024-05-07 14:58:21.765647: Epoch time: 91.35 s
2024-05-07 14:58:21.766799: Yayy! New best EMA pseudo Dice: 0.7334
2024-05-07 14:58:23.428876: 
2024-05-07 14:58:23.433230: Epoch 36
2024-05-07 14:58:23.435259: Current learning rate: 0.00968
2024-05-07 14:59:54.214854: Validation loss did not improve from -0.55774. Patience: 5/50
2024-05-07 14:59:54.217505: train_loss -0.723
2024-05-07 14:59:54.219130: val_loss -0.5086
2024-05-07 14:59:54.220312: Pseudo dice [0.7266]
2024-05-07 14:59:54.221418: Epoch time: 90.79 s
2024-05-07 14:59:55.537823: 
2024-05-07 14:59:55.541696: Epoch 37
2024-05-07 14:59:55.543614: Current learning rate: 0.00967
2024-05-07 15:01:26.384683: Validation loss did not improve from -0.55774. Patience: 6/50
2024-05-07 15:01:26.387223: train_loss -0.718
2024-05-07 15:01:26.388698: val_loss -0.5135
2024-05-07 15:01:26.390057: Pseudo dice [0.7285]
2024-05-07 15:01:26.391585: Epoch time: 90.85 s
2024-05-07 15:01:27.767208: 
2024-05-07 15:01:27.770021: Epoch 38
2024-05-07 15:01:27.771131: Current learning rate: 0.00966
2024-05-07 15:03:01.046690: Validation loss did not improve from -0.55774. Patience: 7/50
2024-05-07 15:03:01.049495: train_loss -0.7231
2024-05-07 15:03:01.051876: val_loss -0.5112
2024-05-07 15:03:01.053588: Pseudo dice [0.7241]
2024-05-07 15:03:01.054618: Epoch time: 93.28 s
2024-05-07 15:03:02.379322: 
2024-05-07 15:03:02.382926: Epoch 39
2024-05-07 15:03:02.384709: Current learning rate: 0.00965
2024-05-07 15:04:33.306247: Validation loss did not improve from -0.55774. Patience: 8/50
2024-05-07 15:04:33.308992: train_loss -0.7203
2024-05-07 15:04:33.310887: val_loss -0.5006
2024-05-07 15:04:33.311871: Pseudo dice [0.7224]
2024-05-07 15:04:33.313195: Epoch time: 90.93 s
2024-05-07 15:04:35.069105: 
2024-05-07 15:04:35.072318: Epoch 40
2024-05-07 15:04:35.074019: Current learning rate: 0.00964
2024-05-07 15:06:07.868300: Validation loss improved from -0.55774 to -0.57094! Patience: 8/50
2024-05-07 15:06:07.871148: train_loss -0.7173
2024-05-07 15:06:07.872610: val_loss -0.5709
2024-05-07 15:06:07.873760: Pseudo dice [0.763]
2024-05-07 15:06:07.874913: Epoch time: 92.8 s
2024-05-07 15:06:07.875944: Yayy! New best EMA pseudo Dice: 0.7338
2024-05-07 15:06:09.636609: 
2024-05-07 15:06:09.640746: Epoch 41
2024-05-07 15:06:09.642142: Current learning rate: 0.00963
2024-05-07 15:07:43.639345: Validation loss did not improve from -0.57094. Patience: 1/50
2024-05-07 15:07:43.658274: train_loss -0.7228
2024-05-07 15:07:43.771632: val_loss -0.5062
2024-05-07 15:07:43.772753: Pseudo dice [0.7347]
2024-05-07 15:07:43.782795: Epoch time: 94.01 s
2024-05-07 15:07:43.784510: Yayy! New best EMA pseudo Dice: 0.7339
2024-05-07 15:07:48.783010: 
2024-05-07 15:07:48.785783: Epoch 42
2024-05-07 15:07:48.787065: Current learning rate: 0.00962
2024-05-07 15:09:19.688109: Validation loss did not improve from -0.57094. Patience: 2/50
2024-05-07 15:09:19.690174: train_loss -0.7211
2024-05-07 15:09:19.691711: val_loss -0.5528
2024-05-07 15:09:19.692835: Pseudo dice [0.7474]
2024-05-07 15:09:19.694310: Epoch time: 90.91 s
2024-05-07 15:09:19.695245: Yayy! New best EMA pseudo Dice: 0.7353
2024-05-07 15:09:21.912365: 
2024-05-07 15:09:21.915810: Epoch 43
2024-05-07 15:09:21.917252: Current learning rate: 0.00961
2024-05-07 15:10:53.761365: Validation loss did not improve from -0.57094. Patience: 3/50
2024-05-07 15:10:53.776687: train_loss -0.7283
2024-05-07 15:10:53.779709: val_loss -0.5418
2024-05-07 15:10:53.781274: Pseudo dice [0.7499]
2024-05-07 15:10:53.782721: Epoch time: 91.85 s
2024-05-07 15:10:53.783966: Yayy! New best EMA pseudo Dice: 0.7367
2024-05-07 15:10:56.850524: 
2024-05-07 15:10:56.853819: Epoch 44
2024-05-07 15:10:56.855675: Current learning rate: 0.0096
2024-05-07 15:12:26.951820: Validation loss did not improve from -0.57094. Patience: 4/50
2024-05-07 15:12:26.954547: train_loss -0.7236
2024-05-07 15:12:26.955927: val_loss -0.4902
2024-05-07 15:12:26.956949: Pseudo dice [0.7161]
2024-05-07 15:12:26.958018: Epoch time: 90.11 s
2024-05-07 15:12:28.564766: 
2024-05-07 15:12:28.568002: Epoch 45
2024-05-07 15:12:28.569587: Current learning rate: 0.00959
2024-05-07 15:13:58.583263: Validation loss did not improve from -0.57094. Patience: 5/50
2024-05-07 15:13:58.585672: train_loss -0.7304
2024-05-07 15:13:58.587107: val_loss -0.557
2024-05-07 15:13:58.588428: Pseudo dice [0.759]
2024-05-07 15:13:58.589609: Epoch time: 90.02 s
2024-05-07 15:13:58.590784: Yayy! New best EMA pseudo Dice: 0.7371
2024-05-07 15:14:00.174900: 
2024-05-07 15:14:00.178519: Epoch 46
2024-05-07 15:14:00.180071: Current learning rate: 0.00959
2024-05-07 15:15:29.157912: Validation loss did not improve from -0.57094. Patience: 6/50
2024-05-07 15:15:29.160831: train_loss -0.7363
2024-05-07 15:15:29.162526: val_loss -0.5479
2024-05-07 15:15:29.163665: Pseudo dice [0.7452]
2024-05-07 15:15:29.164749: Epoch time: 88.99 s
2024-05-07 15:15:29.165812: Yayy! New best EMA pseudo Dice: 0.7379
2024-05-07 15:15:30.774760: 
2024-05-07 15:15:30.778702: Epoch 47
2024-05-07 15:15:30.780479: Current learning rate: 0.00958
2024-05-07 15:16:59.742646: Validation loss did not improve from -0.57094. Patience: 7/50
2024-05-07 15:16:59.744759: train_loss -0.732
2024-05-07 15:16:59.746228: val_loss -0.5248
2024-05-07 15:16:59.747504: Pseudo dice [0.7345]
2024-05-07 15:16:59.748652: Epoch time: 88.97 s
2024-05-07 15:17:01.010529: 
2024-05-07 15:17:01.013341: Epoch 48
2024-05-07 15:17:01.014559: Current learning rate: 0.00957
2024-05-07 15:18:30.874646: Validation loss did not improve from -0.57094. Patience: 8/50
2024-05-07 15:18:30.878349: train_loss -0.7323
2024-05-07 15:18:30.879981: val_loss -0.5289
2024-05-07 15:18:30.881068: Pseudo dice [0.7402]
2024-05-07 15:18:30.882216: Epoch time: 89.87 s
2024-05-07 15:18:32.119744: 
2024-05-07 15:18:32.122803: Epoch 49
2024-05-07 15:18:32.124253: Current learning rate: 0.00956
2024-05-07 15:20:01.298325: Validation loss did not improve from -0.57094. Patience: 9/50
2024-05-07 15:20:01.300760: train_loss -0.7349
2024-05-07 15:20:01.302386: val_loss -0.5296
2024-05-07 15:20:01.303447: Pseudo dice [0.7311]
2024-05-07 15:20:01.304502: Epoch time: 89.18 s
2024-05-07 15:20:02.932665: 
2024-05-07 15:20:02.936857: Epoch 50
2024-05-07 15:20:02.938479: Current learning rate: 0.00955
2024-05-07 15:21:32.307964: Validation loss did not improve from -0.57094. Patience: 10/50
2024-05-07 15:21:32.311015: train_loss -0.7381
2024-05-07 15:21:32.312490: val_loss -0.5269
2024-05-07 15:21:32.313661: Pseudo dice [0.7361]
2024-05-07 15:21:32.314926: Epoch time: 89.38 s
2024-05-07 15:21:33.896676: 
2024-05-07 15:21:33.899843: Epoch 51
2024-05-07 15:21:33.901376: Current learning rate: 0.00954
2024-05-07 15:23:04.581152: Validation loss did not improve from -0.57094. Patience: 11/50
2024-05-07 15:23:04.583273: train_loss -0.7364
2024-05-07 15:23:04.584588: val_loss -0.5518
2024-05-07 15:23:04.585574: Pseudo dice [0.7565]
2024-05-07 15:23:04.586556: Epoch time: 90.69 s
2024-05-07 15:23:04.587521: Yayy! New best EMA pseudo Dice: 0.739
2024-05-07 15:23:06.264795: 
2024-05-07 15:23:06.267881: Epoch 52
2024-05-07 15:23:06.269147: Current learning rate: 0.00953
2024-05-07 15:24:35.867497: Validation loss did not improve from -0.57094. Patience: 12/50
2024-05-07 15:24:35.870022: train_loss -0.733
2024-05-07 15:24:35.871443: val_loss -0.5368
2024-05-07 15:24:35.872393: Pseudo dice [0.7455]
2024-05-07 15:24:35.873372: Epoch time: 89.61 s
2024-05-07 15:24:35.874367: Yayy! New best EMA pseudo Dice: 0.7396
2024-05-07 15:24:37.535616: 
2024-05-07 15:24:37.539194: Epoch 53
2024-05-07 15:24:37.541165: Current learning rate: 0.00952
2024-05-07 15:26:08.323927: Validation loss did not improve from -0.57094. Patience: 13/50
2024-05-07 15:26:08.326757: train_loss -0.7353
2024-05-07 15:26:08.328155: val_loss -0.5166
2024-05-07 15:26:08.329353: Pseudo dice [0.73]
2024-05-07 15:26:08.330568: Epoch time: 90.79 s
2024-05-07 15:26:09.570407: 
2024-05-07 15:26:09.574108: Epoch 54
2024-05-07 15:26:09.575620: Current learning rate: 0.00951
2024-05-07 15:27:39.040501: Validation loss did not improve from -0.57094. Patience: 14/50
2024-05-07 15:27:39.043050: train_loss -0.7408
2024-05-07 15:27:39.044456: val_loss -0.5555
2024-05-07 15:27:39.045679: Pseudo dice [0.7502]
2024-05-07 15:27:39.046748: Epoch time: 89.47 s
2024-05-07 15:27:39.425417: Yayy! New best EMA pseudo Dice: 0.7398
2024-05-07 15:27:41.380474: 
2024-05-07 15:27:41.383569: Epoch 55
2024-05-07 15:27:41.384818: Current learning rate: 0.0095
2024-05-07 15:29:10.872858: Validation loss improved from -0.57094 to -0.57856! Patience: 14/50
2024-05-07 15:29:10.875565: train_loss -0.7404
2024-05-07 15:29:10.877042: val_loss -0.5786
2024-05-07 15:29:10.878028: Pseudo dice [0.7582]
2024-05-07 15:29:10.879254: Epoch time: 89.5 s
2024-05-07 15:29:10.880296: Yayy! New best EMA pseudo Dice: 0.7417
2024-05-07 15:29:12.528438: 
2024-05-07 15:29:12.531362: Epoch 56
2024-05-07 15:29:12.532391: Current learning rate: 0.00949
2024-05-07 15:30:43.124459: Validation loss did not improve from -0.57856. Patience: 1/50
2024-05-07 15:30:43.127371: train_loss -0.7459
2024-05-07 15:30:43.128755: val_loss -0.5067
2024-05-07 15:30:43.129917: Pseudo dice [0.7175]
2024-05-07 15:30:43.131053: Epoch time: 90.6 s
2024-05-07 15:30:44.420829: 
2024-05-07 15:30:44.424046: Epoch 57
2024-05-07 15:30:44.426147: Current learning rate: 0.00949
2024-05-07 15:32:13.922617: Validation loss did not improve from -0.57856. Patience: 2/50
2024-05-07 15:32:13.924674: train_loss -0.7462
2024-05-07 15:32:13.925953: val_loss -0.5616
2024-05-07 15:32:13.927032: Pseudo dice [0.7544]
2024-05-07 15:32:13.928167: Epoch time: 89.51 s
2024-05-07 15:32:15.184286: 
2024-05-07 15:32:15.186657: Epoch 58
2024-05-07 15:32:15.187838: Current learning rate: 0.00948
2024-05-07 15:33:45.857722: Validation loss did not improve from -0.57856. Patience: 3/50
2024-05-07 15:33:45.860513: train_loss -0.7441
2024-05-07 15:33:45.861862: val_loss -0.5225
2024-05-07 15:33:45.862823: Pseudo dice [0.7424]
2024-05-07 15:33:45.863950: Epoch time: 90.68 s
2024-05-07 15:33:47.159298: 
2024-05-07 15:33:47.162831: Epoch 59
2024-05-07 15:33:47.164189: Current learning rate: 0.00947
2024-05-07 15:35:16.744456: Validation loss did not improve from -0.57856. Patience: 4/50
2024-05-07 15:35:16.747211: train_loss -0.74
2024-05-07 15:35:16.749053: val_loss -0.5296
2024-05-07 15:35:16.750700: Pseudo dice [0.743]
2024-05-07 15:35:16.752330: Epoch time: 89.59 s
2024-05-07 15:35:18.496132: 
2024-05-07 15:35:18.498758: Epoch 60
2024-05-07 15:35:18.499943: Current learning rate: 0.00946
2024-05-07 15:36:48.072084: Validation loss did not improve from -0.57856. Patience: 5/50
2024-05-07 15:36:48.074676: train_loss -0.7443
2024-05-07 15:36:48.076308: val_loss -0.5573
2024-05-07 15:36:48.077948: Pseudo dice [0.7565]
2024-05-07 15:36:48.079211: Epoch time: 89.58 s
2024-05-07 15:36:48.080530: Yayy! New best EMA pseudo Dice: 0.7427
2024-05-07 15:36:49.766870: 
2024-05-07 15:36:49.770353: Epoch 61
2024-05-07 15:36:49.772053: Current learning rate: 0.00945
2024-05-07 15:38:20.369338: Validation loss did not improve from -0.57856. Patience: 6/50
2024-05-07 15:38:20.372590: train_loss -0.7414
2024-05-07 15:38:20.374267: val_loss -0.5374
2024-05-07 15:38:20.375329: Pseudo dice [0.738]
2024-05-07 15:38:20.376375: Epoch time: 90.61 s
2024-05-07 15:38:21.710520: 
2024-05-07 15:38:21.713770: Epoch 62
2024-05-07 15:38:21.715205: Current learning rate: 0.00944
2024-05-07 15:39:51.464527: Validation loss did not improve from -0.57856. Patience: 7/50
2024-05-07 15:39:51.467808: train_loss -0.7554
2024-05-07 15:39:51.469363: val_loss -0.5185
2024-05-07 15:39:51.470932: Pseudo dice [0.7302]
2024-05-07 15:39:51.472375: Epoch time: 89.76 s
2024-05-07 15:39:52.804826: 
2024-05-07 15:39:52.807490: Epoch 63
2024-05-07 15:39:52.808835: Current learning rate: 0.00943
2024-05-07 15:41:25.948148: Validation loss did not improve from -0.57856. Patience: 8/50
2024-05-07 15:41:25.950501: train_loss -0.7602
2024-05-07 15:41:25.951943: val_loss -0.5454
2024-05-07 15:41:25.953042: Pseudo dice [0.7446]
2024-05-07 15:41:25.954262: Epoch time: 93.15 s
2024-05-07 15:41:27.303471: 
2024-05-07 15:41:27.306602: Epoch 64
2024-05-07 15:41:27.308264: Current learning rate: 0.00942
2024-05-07 15:42:56.954206: Validation loss did not improve from -0.57856. Patience: 9/50
2024-05-07 15:42:56.956692: train_loss -0.7589
2024-05-07 15:42:56.958291: val_loss -0.5132
2024-05-07 15:42:56.959727: Pseudo dice [0.7253]
2024-05-07 15:42:56.960844: Epoch time: 89.65 s
2024-05-07 15:42:58.629934: 
2024-05-07 15:42:58.633426: Epoch 65
2024-05-07 15:42:58.634926: Current learning rate: 0.00941
2024-05-07 15:44:28.273245: Validation loss did not improve from -0.57856. Patience: 10/50
2024-05-07 15:44:28.275647: train_loss -0.7499
2024-05-07 15:44:28.277320: val_loss -0.538
2024-05-07 15:44:28.278740: Pseudo dice [0.7478]
2024-05-07 15:44:28.280099: Epoch time: 89.65 s
2024-05-07 15:44:29.960819: 
2024-05-07 15:44:29.963726: Epoch 66
2024-05-07 15:44:29.965055: Current learning rate: 0.0094
2024-05-07 15:46:00.724192: Validation loss did not improve from -0.57856. Patience: 11/50
2024-05-07 15:46:00.726538: train_loss -0.7588
2024-05-07 15:46:00.727835: val_loss -0.5374
2024-05-07 15:46:00.729208: Pseudo dice [0.7495]
2024-05-07 15:46:00.730436: Epoch time: 90.77 s
2024-05-07 15:46:02.024671: 
2024-05-07 15:46:02.178465: Epoch 67
2024-05-07 15:46:02.269738: Current learning rate: 0.00939
2024-05-07 15:47:31.891719: Validation loss did not improve from -0.57856. Patience: 12/50
2024-05-07 15:47:31.894284: train_loss -0.755
2024-05-07 15:47:31.895974: val_loss -0.5416
2024-05-07 15:47:31.897443: Pseudo dice [0.7421]
2024-05-07 15:47:31.898835: Epoch time: 89.87 s
2024-05-07 15:47:33.225146: 
2024-05-07 15:47:33.228437: Epoch 68
2024-05-07 15:47:33.230037: Current learning rate: 0.00939
2024-05-07 15:49:02.814748: Validation loss did not improve from -0.57856. Patience: 13/50
2024-05-07 15:49:02.817104: train_loss -0.76
2024-05-07 15:49:02.818388: val_loss -0.5631
2024-05-07 15:49:02.819544: Pseudo dice [0.7509]
2024-05-07 15:49:02.820482: Epoch time: 89.59 s
2024-05-07 15:49:04.176225: 
2024-05-07 15:49:04.179007: Epoch 69
2024-05-07 15:49:04.180642: Current learning rate: 0.00938
2024-05-07 15:50:35.011830: Validation loss did not improve from -0.57856. Patience: 14/50
2024-05-07 15:50:35.014020: train_loss -0.7618
2024-05-07 15:50:35.015324: val_loss -0.5415
2024-05-07 15:50:35.016324: Pseudo dice [0.7451]
2024-05-07 15:50:35.017323: Epoch time: 90.84 s
2024-05-07 15:50:35.405321: Yayy! New best EMA pseudo Dice: 0.7427
2024-05-07 15:50:37.149656: 
2024-05-07 15:50:37.152671: Epoch 70
2024-05-07 15:50:37.153974: Current learning rate: 0.00937
2024-05-07 15:52:06.679298: Validation loss did not improve from -0.57856. Patience: 15/50
2024-05-07 15:52:06.682511: train_loss -0.7603
2024-05-07 15:52:06.683985: val_loss -0.5367
2024-05-07 15:52:06.684870: Pseudo dice [0.7429]
2024-05-07 15:52:06.685714: Epoch time: 89.53 s
2024-05-07 15:52:06.686547: Yayy! New best EMA pseudo Dice: 0.7427
2024-05-07 15:52:08.392333: 
2024-05-07 15:52:08.396399: Epoch 71
2024-05-07 15:52:08.397672: Current learning rate: 0.00936
2024-05-07 15:53:39.688464: Validation loss did not improve from -0.57856. Patience: 16/50
2024-05-07 15:53:39.690928: train_loss -0.762
2024-05-07 15:53:39.692590: val_loss -0.5561
2024-05-07 15:53:39.693649: Pseudo dice [0.7495]
2024-05-07 15:53:39.694808: Epoch time: 91.3 s
2024-05-07 15:53:39.695832: Yayy! New best EMA pseudo Dice: 0.7434
2024-05-07 15:53:41.335885: 
2024-05-07 15:53:41.339340: Epoch 72
2024-05-07 15:53:41.340937: Current learning rate: 0.00935
2024-05-07 15:55:10.895169: Validation loss did not improve from -0.57856. Patience: 17/50
2024-05-07 15:55:10.897986: train_loss -0.7635
2024-05-07 15:55:10.899387: val_loss -0.5336
2024-05-07 15:55:10.900627: Pseudo dice [0.745]
2024-05-07 15:55:10.901803: Epoch time: 89.56 s
2024-05-07 15:55:10.902888: Yayy! New best EMA pseudo Dice: 0.7436
2024-05-07 15:55:12.645814: 
2024-05-07 15:55:12.648858: Epoch 73
2024-05-07 15:55:12.650262: Current learning rate: 0.00934
2024-05-07 15:56:42.329796: Validation loss did not improve from -0.57856. Patience: 18/50
2024-05-07 15:56:42.331868: train_loss -0.7627
2024-05-07 15:56:42.333572: val_loss -0.5245
2024-05-07 15:56:42.334636: Pseudo dice [0.7348]
2024-05-07 15:56:42.335673: Epoch time: 89.69 s
2024-05-07 15:56:43.646024: 
2024-05-07 15:56:43.649488: Epoch 74
2024-05-07 15:56:43.651352: Current learning rate: 0.00933
2024-05-07 15:58:15.096001: Validation loss did not improve from -0.57856. Patience: 19/50
2024-05-07 15:58:15.098321: train_loss -0.758
2024-05-07 15:58:15.099740: val_loss -0.5548
2024-05-07 15:58:15.100762: Pseudo dice [0.7546]
2024-05-07 15:58:15.101779: Epoch time: 91.45 s
2024-05-07 15:58:15.480598: Yayy! New best EMA pseudo Dice: 0.7439
2024-05-07 15:58:17.153958: 
2024-05-07 15:58:17.157796: Epoch 75
2024-05-07 15:58:17.159099: Current learning rate: 0.00932
2024-05-07 15:59:46.622983: Validation loss did not improve from -0.57856. Patience: 20/50
2024-05-07 15:59:46.625451: train_loss -0.7625
2024-05-07 15:59:46.626815: val_loss -0.5358
2024-05-07 15:59:46.627753: Pseudo dice [0.749]
2024-05-07 15:59:46.628667: Epoch time: 89.47 s
2024-05-07 15:59:46.629669: Yayy! New best EMA pseudo Dice: 0.7444
2024-05-07 15:59:48.315764: 
2024-05-07 15:59:48.318944: Epoch 76
2024-05-07 15:59:48.320044: Current learning rate: 0.00931
2024-05-07 16:01:19.099972: Validation loss did not improve from -0.57856. Patience: 21/50
2024-05-07 16:01:19.102411: train_loss -0.764
2024-05-07 16:01:19.103855: val_loss -0.5357
2024-05-07 16:01:19.105105: Pseudo dice [0.744]
2024-05-07 16:01:19.106371: Epoch time: 90.79 s
2024-05-07 16:01:20.451390: 
2024-05-07 16:01:20.454849: Epoch 77
2024-05-07 16:01:20.456703: Current learning rate: 0.0093
2024-05-07 16:02:49.900521: Validation loss did not improve from -0.57856. Patience: 22/50
2024-05-07 16:02:49.903712: train_loss -0.7645
2024-05-07 16:02:49.905120: val_loss -0.545
2024-05-07 16:02:49.906415: Pseudo dice [0.7501]
2024-05-07 16:02:49.907410: Epoch time: 89.45 s
2024-05-07 16:02:49.908299: Yayy! New best EMA pseudo Dice: 0.7449
2024-05-07 16:02:52.052303: 
2024-05-07 16:02:52.055584: Epoch 78
2024-05-07 16:02:52.057306: Current learning rate: 0.0093
2024-05-07 16:04:21.538690: Validation loss did not improve from -0.57856. Patience: 23/50
2024-05-07 16:04:21.540724: train_loss -0.7658
2024-05-07 16:04:21.541986: val_loss -0.5116
2024-05-07 16:04:21.543110: Pseudo dice [0.7256]
2024-05-07 16:04:21.544172: Epoch time: 89.49 s
2024-05-07 16:04:22.900576: 
2024-05-07 16:04:22.904116: Epoch 79
2024-05-07 16:04:22.906124: Current learning rate: 0.00929
2024-05-07 16:05:53.889862: Validation loss did not improve from -0.57856. Patience: 24/50
2024-05-07 16:05:53.892514: train_loss -0.7672
2024-05-07 16:05:53.893708: val_loss -0.5441
2024-05-07 16:05:53.894895: Pseudo dice [0.7465]
2024-05-07 16:05:53.895981: Epoch time: 90.99 s
2024-05-07 16:05:55.673744: 
2024-05-07 16:05:55.676404: Epoch 80
2024-05-07 16:05:55.677589: Current learning rate: 0.00928
2024-05-07 16:07:25.073818: Validation loss did not improve from -0.57856. Patience: 25/50
2024-05-07 16:07:25.076038: train_loss -0.7643
2024-05-07 16:07:25.077296: val_loss -0.543
2024-05-07 16:07:25.078258: Pseudo dice [0.7448]
2024-05-07 16:07:25.079276: Epoch time: 89.4 s
2024-05-07 16:07:26.422848: 
2024-05-07 16:07:26.425570: Epoch 81
2024-05-07 16:07:26.426706: Current learning rate: 0.00927
2024-05-07 16:09:02.885114: Validation loss did not improve from -0.57856. Patience: 26/50
2024-05-07 16:09:02.887417: train_loss -0.7631
2024-05-07 16:09:02.888782: val_loss -0.5288
2024-05-07 16:09:02.889770: Pseudo dice [0.7451]
2024-05-07 16:09:02.890738: Epoch time: 96.47 s
2024-05-07 16:09:04.379391: 
2024-05-07 16:09:04.382336: Epoch 82
2024-05-07 16:09:04.383588: Current learning rate: 0.00926
2024-05-07 16:10:36.083965: Validation loss did not improve from -0.57856. Patience: 27/50
2024-05-07 16:10:36.086168: train_loss -0.7558
2024-05-07 16:10:36.087609: val_loss -0.4956
2024-05-07 16:10:36.088703: Pseudo dice [0.7258]
2024-05-07 16:10:36.089934: Epoch time: 91.71 s
2024-05-07 16:10:37.542465: 
2024-05-07 16:10:37.546386: Epoch 83
2024-05-07 16:10:37.588751: Current learning rate: 0.00925
2024-05-07 16:12:08.285380: Validation loss did not improve from -0.57856. Patience: 28/50
2024-05-07 16:12:08.306499: train_loss -0.7584
2024-05-07 16:12:08.307938: val_loss -0.5322
2024-05-07 16:12:08.309120: Pseudo dice [0.7344]
2024-05-07 16:12:08.310130: Epoch time: 90.77 s
2024-05-07 16:12:10.075395: 
2024-05-07 16:12:10.077810: Epoch 84
2024-05-07 16:12:10.078949: Current learning rate: 0.00924
2024-05-07 16:13:42.340292: Validation loss did not improve from -0.57856. Patience: 29/50
2024-05-07 16:13:42.347205: train_loss -0.7552
2024-05-07 16:13:42.348935: val_loss -0.5252
2024-05-07 16:13:42.350246: Pseudo dice [0.7412]
2024-05-07 16:13:42.351386: Epoch time: 92.27 s
2024-05-07 16:13:44.678962: 
2024-05-07 16:13:44.682313: Epoch 85
2024-05-07 16:13:44.683899: Current learning rate: 0.00923
2024-05-07 16:15:14.287072: Validation loss did not improve from -0.57856. Patience: 30/50
2024-05-07 16:15:14.288991: train_loss -0.7635
2024-05-07 16:15:14.322361: val_loss -0.5288
2024-05-07 16:15:14.323415: Pseudo dice [0.7497]
2024-05-07 16:15:14.328512: Epoch time: 89.61 s
2024-05-07 16:15:15.876360: 
2024-05-07 16:15:15.879519: Epoch 86
2024-05-07 16:15:15.880515: Current learning rate: 0.00922
2024-05-07 16:16:46.931519: Validation loss did not improve from -0.57856. Patience: 31/50
2024-05-07 16:16:46.934240: train_loss -0.7628
2024-05-07 16:16:46.935754: val_loss -0.5309
2024-05-07 16:16:46.937075: Pseudo dice [0.7459]
2024-05-07 16:16:46.938380: Epoch time: 91.06 s
2024-05-07 16:16:48.380461: 
2024-05-07 16:16:48.383211: Epoch 87
2024-05-07 16:16:48.384369: Current learning rate: 0.00921
2024-05-07 16:18:19.895849: Validation loss did not improve from -0.57856. Patience: 32/50
2024-05-07 16:18:19.898710: train_loss -0.7663
2024-05-07 16:18:19.900270: val_loss -0.5267
2024-05-07 16:18:19.901554: Pseudo dice [0.7465]
2024-05-07 16:18:19.902906: Epoch time: 91.52 s
2024-05-07 16:18:21.162731: 
2024-05-07 16:18:21.166157: Epoch 88
2024-05-07 16:18:21.167891: Current learning rate: 0.0092
2024-05-07 16:19:51.905979: Validation loss did not improve from -0.57856. Patience: 33/50
2024-05-07 16:19:51.908043: train_loss -0.7628
2024-05-07 16:19:51.909470: val_loss -0.5409
2024-05-07 16:19:51.910432: Pseudo dice [0.7457]
2024-05-07 16:19:51.911332: Epoch time: 90.75 s
2024-05-07 16:19:53.293169: 
2024-05-07 16:19:53.296267: Epoch 89
2024-05-07 16:19:53.297955: Current learning rate: 0.0092
2024-05-07 16:21:28.494742: Validation loss did not improve from -0.57856. Patience: 34/50
2024-05-07 16:21:28.497558: train_loss -0.7674
2024-05-07 16:21:28.499142: val_loss -0.5571
2024-05-07 16:21:28.500331: Pseudo dice [0.7582]
2024-05-07 16:21:28.501517: Epoch time: 95.21 s
2024-05-07 16:21:30.164478: 
2024-05-07 16:21:30.167118: Epoch 90
2024-05-07 16:21:30.168245: Current learning rate: 0.00919
2024-05-07 16:22:59.730181: Validation loss did not improve from -0.57856. Patience: 35/50
2024-05-07 16:22:59.733009: train_loss -0.7693
2024-05-07 16:22:59.734610: val_loss -0.5493
2024-05-07 16:22:59.735869: Pseudo dice [0.7451]
2024-05-07 16:22:59.736987: Epoch time: 89.57 s
2024-05-07 16:23:01.067295: 
2024-05-07 16:23:01.070880: Epoch 91
2024-05-07 16:23:01.072692: Current learning rate: 0.00918
2024-05-07 16:24:30.996892: Validation loss improved from -0.57856 to -0.58100! Patience: 35/50
2024-05-07 16:24:31.000010: train_loss -0.7766
2024-05-07 16:24:31.001457: val_loss -0.581
2024-05-07 16:24:31.002690: Pseudo dice [0.7655]
2024-05-07 16:24:31.003690: Epoch time: 89.93 s
2024-05-07 16:24:31.004817: Yayy! New best EMA pseudo Dice: 0.7467
2024-05-07 16:24:32.632260: 
2024-05-07 16:24:32.635470: Epoch 92
2024-05-07 16:24:32.637146: Current learning rate: 0.00917
2024-05-07 16:26:04.780337: Validation loss did not improve from -0.58100. Patience: 1/50
2024-05-07 16:26:04.782686: train_loss -0.7791
2024-05-07 16:26:04.783989: val_loss -0.5648
2024-05-07 16:26:04.785053: Pseudo dice [0.7611]
2024-05-07 16:26:04.786083: Epoch time: 92.15 s
2024-05-07 16:26:04.787130: Yayy! New best EMA pseudo Dice: 0.7482
2024-05-07 16:26:06.435894: 
2024-05-07 16:26:06.438921: Epoch 93
2024-05-07 16:26:06.440064: Current learning rate: 0.00916
2024-05-07 16:27:36.181669: Validation loss did not improve from -0.58100. Patience: 2/50
2024-05-07 16:27:36.184838: train_loss -0.776
2024-05-07 16:27:36.186594: val_loss -0.5613
2024-05-07 16:27:36.187759: Pseudo dice [0.7552]
2024-05-07 16:27:36.188743: Epoch time: 89.75 s
2024-05-07 16:27:36.189724: Yayy! New best EMA pseudo Dice: 0.7489
2024-05-07 16:27:37.858581: 
2024-05-07 16:27:37.861132: Epoch 94
2024-05-07 16:27:37.862533: Current learning rate: 0.00915
2024-05-07 16:29:10.163479: Validation loss did not improve from -0.58100. Patience: 3/50
2024-05-07 16:29:10.166197: train_loss -0.7757
2024-05-07 16:29:10.167819: val_loss -0.5448
2024-05-07 16:29:10.168816: Pseudo dice [0.7503]
2024-05-07 16:29:10.169919: Epoch time: 92.31 s
2024-05-07 16:29:10.557592: Yayy! New best EMA pseudo Dice: 0.749
2024-05-07 16:29:12.194697: 
2024-05-07 16:29:12.197896: Epoch 95
2024-05-07 16:29:12.198985: Current learning rate: 0.00914
2024-05-07 16:30:41.899478: Validation loss did not improve from -0.58100. Patience: 4/50
2024-05-07 16:30:41.902589: train_loss -0.7795
2024-05-07 16:30:41.904417: val_loss -0.4962
2024-05-07 16:30:41.905598: Pseudo dice [0.7337]
2024-05-07 16:30:41.906565: Epoch time: 89.71 s
2024-05-07 16:30:43.191612: 
2024-05-07 16:30:43.195035: Epoch 96
2024-05-07 16:30:43.196440: Current learning rate: 0.00913
2024-05-07 16:32:13.026735: Validation loss did not improve from -0.58100. Patience: 5/50
2024-05-07 16:32:13.028782: train_loss -0.7722
2024-05-07 16:32:13.030110: val_loss -0.5257
2024-05-07 16:32:13.031143: Pseudo dice [0.749]
2024-05-07 16:32:13.032250: Epoch time: 89.84 s
2024-05-07 16:32:14.351161: 
2024-05-07 16:32:14.353796: Epoch 97
2024-05-07 16:32:14.355179: Current learning rate: 0.00912
2024-05-07 16:33:46.031502: Validation loss did not improve from -0.58100. Patience: 6/50
2024-05-07 16:33:46.033978: train_loss -0.7668
2024-05-07 16:33:46.035325: val_loss -0.5376
2024-05-07 16:33:46.036372: Pseudo dice [0.7491]
2024-05-07 16:33:46.037538: Epoch time: 91.68 s
2024-05-07 16:33:47.347855: 
2024-05-07 16:33:47.352006: Epoch 98
2024-05-07 16:33:47.353984: Current learning rate: 0.00911
2024-05-07 16:35:16.918442: Validation loss did not improve from -0.58100. Patience: 7/50
2024-05-07 16:35:16.921684: train_loss -0.7707
2024-05-07 16:35:16.923464: val_loss -0.5446
2024-05-07 16:35:16.924685: Pseudo dice [0.7537]
2024-05-07 16:35:16.926149: Epoch time: 89.58 s
2024-05-07 16:35:18.235693: 
2024-05-07 16:35:18.239515: Epoch 99
2024-05-07 16:35:18.241309: Current learning rate: 0.0091
2024-05-07 16:36:51.183579: Validation loss did not improve from -0.58100. Patience: 8/50
2024-05-07 16:36:51.185795: train_loss -0.775
2024-05-07 16:36:51.187131: val_loss -0.5455
2024-05-07 16:36:51.188335: Pseudo dice [0.7515]
2024-05-07 16:36:51.189392: Epoch time: 92.95 s
2024-05-07 16:36:52.921032: 
2024-05-07 16:36:52.924235: Epoch 100
2024-05-07 16:36:52.925480: Current learning rate: 0.0091
2024-05-07 16:38:22.384293: Validation loss did not improve from -0.58100. Patience: 9/50
2024-05-07 16:38:22.386334: train_loss -0.7796
2024-05-07 16:38:22.387662: val_loss -0.5408
2024-05-07 16:38:22.388835: Pseudo dice [0.7519]
2024-05-07 16:38:22.389990: Epoch time: 89.47 s
2024-05-07 16:38:22.390949: Yayy! New best EMA pseudo Dice: 0.749
2024-05-07 16:38:24.828765: 
2024-05-07 16:38:24.831404: Epoch 101
2024-05-07 16:38:24.832788: Current learning rate: 0.00909
2024-05-07 16:39:54.340071: Validation loss did not improve from -0.58100. Patience: 10/50
2024-05-07 16:39:54.342529: train_loss -0.7822
2024-05-07 16:39:54.343938: val_loss -0.5542
2024-05-07 16:39:54.344899: Pseudo dice [0.7592]
2024-05-07 16:39:54.345860: Epoch time: 89.51 s
2024-05-07 16:39:54.346767: Yayy! New best EMA pseudo Dice: 0.75
2024-05-07 16:39:56.021303: 
2024-05-07 16:39:56.023894: Epoch 102
2024-05-07 16:39:56.025641: Current learning rate: 0.00908
2024-05-07 16:41:27.934417: Validation loss did not improve from -0.58100. Patience: 11/50
2024-05-07 16:41:27.936743: train_loss -0.7774
2024-05-07 16:41:27.938172: val_loss -0.5513
2024-05-07 16:41:27.939263: Pseudo dice [0.748]
2024-05-07 16:41:27.940298: Epoch time: 91.92 s
2024-05-07 16:41:29.280114: 
2024-05-07 16:41:29.282523: Epoch 103
2024-05-07 16:41:29.284112: Current learning rate: 0.00907
2024-05-07 16:42:58.821661: Validation loss did not improve from -0.58100. Patience: 12/50
2024-05-07 16:42:58.823821: train_loss -0.7764
2024-05-07 16:42:58.824903: val_loss -0.5617
2024-05-07 16:42:58.826092: Pseudo dice [0.7583]
2024-05-07 16:42:58.827071: Epoch time: 89.55 s
2024-05-07 16:42:58.828162: Yayy! New best EMA pseudo Dice: 0.7507
2024-05-07 16:43:00.516438: 
2024-05-07 16:43:00.519164: Epoch 104
2024-05-07 16:43:00.520400: Current learning rate: 0.00906
2024-05-07 16:44:31.650565: Validation loss did not improve from -0.58100. Patience: 13/50
2024-05-07 16:44:31.653027: train_loss -0.7778
2024-05-07 16:44:31.654582: val_loss -0.5481
2024-05-07 16:44:31.655767: Pseudo dice [0.7562]
2024-05-07 16:44:31.656887: Epoch time: 91.14 s
2024-05-07 16:44:32.107412: Yayy! New best EMA pseudo Dice: 0.7512
2024-05-07 16:44:33.961336: 
2024-05-07 16:44:33.963607: Epoch 105
2024-05-07 16:44:33.965004: Current learning rate: 0.00905
2024-05-07 16:46:06.401976: Validation loss did not improve from -0.58100. Patience: 14/50
2024-05-07 16:46:06.404344: train_loss -0.777
2024-05-07 16:46:06.405761: val_loss -0.5077
2024-05-07 16:46:06.407037: Pseudo dice [0.7352]
2024-05-07 16:46:06.408519: Epoch time: 92.44 s
2024-05-07 16:46:07.733016: 
2024-05-07 16:46:07.736408: Epoch 106
2024-05-07 16:46:07.738482: Current learning rate: 0.00904
2024-05-07 16:47:37.559089: Validation loss did not improve from -0.58100. Patience: 15/50
2024-05-07 16:47:37.561387: train_loss -0.7769
2024-05-07 16:47:37.562766: val_loss -0.5604
2024-05-07 16:47:37.563948: Pseudo dice [0.7544]
2024-05-07 16:47:37.565130: Epoch time: 89.83 s
2024-05-07 16:47:38.873489: 
2024-05-07 16:47:38.877006: Epoch 107
2024-05-07 16:47:38.878958: Current learning rate: 0.00903
2024-05-07 16:49:11.800670: Validation loss did not improve from -0.58100. Patience: 16/50
2024-05-07 16:49:11.802909: train_loss -0.7773
2024-05-07 16:49:11.804211: val_loss -0.5373
2024-05-07 16:49:11.805161: Pseudo dice [0.7435]
2024-05-07 16:49:11.806145: Epoch time: 92.93 s
2024-05-07 16:49:13.104456: 
2024-05-07 16:49:13.106813: Epoch 108
2024-05-07 16:49:13.108069: Current learning rate: 0.00902
2024-05-07 16:50:42.955524: Validation loss did not improve from -0.58100. Patience: 17/50
2024-05-07 16:50:42.957845: train_loss -0.7827
2024-05-07 16:50:42.959117: val_loss -0.5412
2024-05-07 16:50:42.960361: Pseudo dice [0.7473]
2024-05-07 16:50:42.961483: Epoch time: 89.85 s
2024-05-07 16:50:44.270424: 
2024-05-07 16:50:44.273435: Epoch 109
2024-05-07 16:50:44.274590: Current learning rate: 0.00901
2024-05-07 16:52:14.175743: Validation loss did not improve from -0.58100. Patience: 18/50
2024-05-07 16:52:14.178507: train_loss -0.7777
2024-05-07 16:52:14.179781: val_loss -0.5503
2024-05-07 16:52:14.180897: Pseudo dice [0.7455]
2024-05-07 16:52:14.181831: Epoch time: 89.91 s
2024-05-07 16:52:15.911900: 
2024-05-07 16:52:15.915125: Epoch 110
2024-05-07 16:52:15.917115: Current learning rate: 0.009
2024-05-07 16:53:52.391488: Validation loss did not improve from -0.58100. Patience: 19/50
2024-05-07 16:53:52.394144: train_loss -0.7788
2024-05-07 16:53:52.395631: val_loss -0.5413
2024-05-07 16:53:52.396933: Pseudo dice [0.7524]
2024-05-07 16:53:52.398270: Epoch time: 96.48 s
2024-05-07 16:53:53.709641: 
2024-05-07 16:53:53.712734: Epoch 111
2024-05-07 16:53:53.714419: Current learning rate: 0.009
2024-05-07 16:55:23.416056: Validation loss did not improve from -0.58100. Patience: 20/50
2024-05-07 16:55:23.418540: train_loss -0.7811
2024-05-07 16:55:23.420021: val_loss -0.5514
2024-05-07 16:55:23.421185: Pseudo dice [0.7553]
2024-05-07 16:55:23.422515: Epoch time: 89.71 s
2024-05-07 16:55:24.736718: 
2024-05-07 16:55:24.739760: Epoch 112
2024-05-07 16:55:24.741398: Current learning rate: 0.00899
2024-05-07 16:56:57.342346: Validation loss did not improve from -0.58100. Patience: 21/50
2024-05-07 16:56:57.344862: train_loss -0.7774
2024-05-07 16:56:57.346337: val_loss -0.5452
2024-05-07 16:56:57.347433: Pseudo dice [0.749]
2024-05-07 16:56:57.348710: Epoch time: 92.61 s
2024-05-07 16:56:59.041594: 
2024-05-07 16:56:59.044721: Epoch 113
2024-05-07 16:56:59.046458: Current learning rate: 0.00898
2024-05-07 16:58:28.874935: Validation loss did not improve from -0.58100. Patience: 22/50
2024-05-07 16:58:28.877112: train_loss -0.7787
2024-05-07 16:58:28.878386: val_loss -0.5245
2024-05-07 16:58:28.879775: Pseudo dice [0.7456]
2024-05-07 16:58:28.880928: Epoch time: 89.84 s
2024-05-07 16:58:30.283362: 
2024-05-07 16:58:30.286738: Epoch 114
2024-05-07 16:58:30.288080: Current learning rate: 0.00897
2024-05-07 17:00:00.131583: Validation loss did not improve from -0.58100. Patience: 23/50
2024-05-07 17:00:00.133882: train_loss -0.7813
2024-05-07 17:00:00.135192: val_loss -0.5449
2024-05-07 17:00:00.136343: Pseudo dice [0.7518]
2024-05-07 17:00:00.137322: Epoch time: 89.85 s
2024-05-07 17:00:01.945391: 
2024-05-07 17:00:01.952132: Epoch 115
2024-05-07 17:00:01.953649: Current learning rate: 0.00896
2024-05-07 17:01:34.876945: Validation loss did not improve from -0.58100. Patience: 24/50
2024-05-07 17:01:34.879492: train_loss -0.7659
2024-05-07 17:01:34.880906: val_loss -0.5397
2024-05-07 17:01:34.882219: Pseudo dice [0.7384]
2024-05-07 17:01:34.883475: Epoch time: 92.94 s
2024-05-07 17:01:36.242194: 
2024-05-07 17:01:36.244864: Epoch 116
2024-05-07 17:01:36.246371: Current learning rate: 0.00895
2024-05-07 17:03:05.510601: Validation loss did not improve from -0.58100. Patience: 25/50
2024-05-07 17:03:05.513297: train_loss -0.7687
2024-05-07 17:03:05.514974: val_loss -0.5508
2024-05-07 17:03:05.516300: Pseudo dice [0.7597]
2024-05-07 17:03:05.517551: Epoch time: 89.27 s
2024-05-07 17:03:06.938093: 
2024-05-07 17:03:06.941132: Epoch 117
2024-05-07 17:03:06.942528: Current learning rate: 0.00894
2024-05-07 17:04:39.561748: Validation loss did not improve from -0.58100. Patience: 26/50
2024-05-07 17:04:39.564072: train_loss -0.7757
2024-05-07 17:04:39.565497: val_loss -0.5438
2024-05-07 17:04:39.566538: Pseudo dice [0.746]
2024-05-07 17:04:39.567730: Epoch time: 92.63 s
2024-05-07 17:04:40.922461: 
2024-05-07 17:04:40.926356: Epoch 118
2024-05-07 17:04:40.928439: Current learning rate: 0.00893
2024-05-07 17:06:10.068538: Validation loss did not improve from -0.58100. Patience: 27/50
2024-05-07 17:06:10.071434: train_loss -0.7795
2024-05-07 17:06:10.072818: val_loss -0.5308
2024-05-07 17:06:10.073877: Pseudo dice [0.752]
2024-05-07 17:06:10.074921: Epoch time: 89.15 s
2024-05-07 17:06:11.445037: 
2024-05-07 17:06:11.448218: Epoch 119
2024-05-07 17:06:11.450214: Current learning rate: 0.00892
2024-05-07 17:07:40.714208: Validation loss did not improve from -0.58100. Patience: 28/50
2024-05-07 17:07:40.716681: train_loss -0.7766
2024-05-07 17:07:40.718019: val_loss -0.5458
2024-05-07 17:07:40.719061: Pseudo dice [0.7567]
2024-05-07 17:07:40.720004: Epoch time: 89.27 s
2024-05-07 17:07:42.518449: 
2024-05-07 17:07:42.521049: Epoch 120
2024-05-07 17:07:42.522250: Current learning rate: 0.00891
2024-05-07 17:09:14.856780: Validation loss did not improve from -0.58100. Patience: 29/50
2024-05-07 17:09:14.859298: train_loss -0.78
2024-05-07 17:09:14.860945: val_loss -0.5358
2024-05-07 17:09:14.862209: Pseudo dice [0.749]
2024-05-07 17:09:14.863367: Epoch time: 92.34 s
2024-05-07 17:09:16.257256: 
2024-05-07 17:09:16.260294: Epoch 121
2024-05-07 17:09:16.262058: Current learning rate: 0.0089
2024-05-07 17:10:51.698809: Validation loss did not improve from -0.58100. Patience: 30/50
2024-05-07 17:10:51.701020: train_loss -0.7888
2024-05-07 17:10:51.702368: val_loss -0.5649
2024-05-07 17:10:51.703465: Pseudo dice [0.7612]
2024-05-07 17:10:51.704559: Epoch time: 95.45 s
2024-05-07 17:10:53.062997: 
2024-05-07 17:10:53.066233: Epoch 122
2024-05-07 17:10:53.067485: Current learning rate: 0.00889
2024-05-07 17:12:25.095747: Validation loss did not improve from -0.58100. Patience: 31/50
2024-05-07 17:12:25.098504: train_loss -0.7861
2024-05-07 17:12:25.100244: val_loss -0.5584
2024-05-07 17:12:25.101320: Pseudo dice [0.7638]
2024-05-07 17:12:25.102402: Epoch time: 92.04 s
2024-05-07 17:12:25.103453: Yayy! New best EMA pseudo Dice: 0.7525
2024-05-07 17:12:26.910256: 
2024-05-07 17:12:26.913436: Epoch 123
2024-05-07 17:12:26.914930: Current learning rate: 0.00889
2024-05-07 17:14:06.552227: Validation loss did not improve from -0.58100. Patience: 32/50
2024-05-07 17:14:06.554494: train_loss -0.7855
2024-05-07 17:14:06.555936: val_loss -0.5171
2024-05-07 17:14:06.557214: Pseudo dice [0.74]
2024-05-07 17:14:06.558365: Epoch time: 99.65 s
2024-05-07 17:14:07.958937: 
2024-05-07 17:14:07.961437: Epoch 124
2024-05-07 17:14:07.962723: Current learning rate: 0.00888
2024-05-07 17:16:14.208089: Validation loss did not improve from -0.58100. Patience: 33/50
2024-05-07 17:16:14.210765: train_loss -0.7875
2024-05-07 17:16:14.212369: val_loss -0.549
2024-05-07 17:16:14.213371: Pseudo dice [0.7519]
2024-05-07 17:16:14.214433: Epoch time: 126.25 s
2024-05-07 17:16:16.797263: 
2024-05-07 17:16:16.799812: Epoch 125
2024-05-07 17:16:16.801074: Current learning rate: 0.00887
2024-05-07 17:17:58.585819: Validation loss did not improve from -0.58100. Patience: 34/50
2024-05-07 17:17:58.588735: train_loss -0.7859
2024-05-07 17:17:58.590204: val_loss -0.5618
2024-05-07 17:17:58.591377: Pseudo dice [0.7565]
2024-05-07 17:17:58.592586: Epoch time: 101.79 s
2024-05-07 17:18:00.292773: 
2024-05-07 17:18:00.295407: Epoch 126
2024-05-07 17:18:00.297110: Current learning rate: 0.00886
2024-05-07 17:19:57.366796: Validation loss did not improve from -0.58100. Patience: 35/50
2024-05-07 17:19:57.369233: train_loss -0.7871
2024-05-07 17:19:57.371244: val_loss -0.5619
2024-05-07 17:19:57.372271: Pseudo dice [0.7595]
2024-05-07 17:19:57.373505: Epoch time: 117.08 s
2024-05-07 17:19:57.374554: Yayy! New best EMA pseudo Dice: 0.7526
2024-05-07 17:19:59.175110: 
2024-05-07 17:19:59.176483: Epoch 127
2024-05-07 17:19:59.177464: Current learning rate: 0.00885
2024-05-07 17:21:43.232836: Validation loss did not improve from -0.58100. Patience: 36/50
2024-05-07 17:21:43.234372: train_loss -0.7866
2024-05-07 17:21:43.235722: val_loss -0.5389
2024-05-07 17:21:43.236989: Pseudo dice [0.7503]
2024-05-07 17:21:43.238435: Epoch time: 104.06 s
2024-05-07 17:21:44.655790: 
2024-05-07 17:21:44.657470: Epoch 128
2024-05-07 17:21:44.658627: Current learning rate: 0.00884
2024-05-07 17:23:43.734886: Validation loss did not improve from -0.58100. Patience: 37/50
2024-05-07 17:23:43.736433: train_loss -0.7859
2024-05-07 17:23:43.737760: val_loss -0.5331
2024-05-07 17:23:43.738799: Pseudo dice [0.7439]
2024-05-07 17:23:43.740072: Epoch time: 119.08 s
2024-05-07 17:23:45.197441: 
2024-05-07 17:23:45.199022: Epoch 129
2024-05-07 17:23:45.200287: Current learning rate: 0.00883
2024-05-07 17:25:33.690889: Validation loss did not improve from -0.58100. Patience: 38/50
2024-05-07 17:25:33.692402: train_loss -0.7795
2024-05-07 17:25:33.693640: val_loss -0.5296
2024-05-07 17:25:33.694690: Pseudo dice [0.7459]
2024-05-07 17:25:33.696097: Epoch time: 108.5 s
2024-05-07 17:25:35.575194: 
2024-05-07 17:25:35.576833: Epoch 130
2024-05-07 17:25:35.578036: Current learning rate: 0.00882
2024-05-07 17:27:33.635484: Validation loss did not improve from -0.58100. Patience: 39/50
2024-05-07 17:27:33.637077: train_loss -0.7784
2024-05-07 17:27:33.638279: val_loss -0.5422
2024-05-07 17:27:33.639261: Pseudo dice [0.7448]
2024-05-07 17:27:33.640347: Epoch time: 118.06 s
2024-05-07 17:27:35.129536: 
2024-05-07 17:27:35.131231: Epoch 131
2024-05-07 17:27:35.132396: Current learning rate: 0.00881
2024-05-07 17:29:22.657896: Validation loss did not improve from -0.58100. Patience: 40/50
2024-05-07 17:29:22.659835: train_loss -0.7782
2024-05-07 17:29:22.661109: val_loss -0.5601
2024-05-07 17:29:22.662231: Pseudo dice [0.7611]
2024-05-07 17:29:22.663262: Epoch time: 107.53 s
2024-05-07 17:29:24.084364: 
2024-05-07 17:29:24.086305: Epoch 132
2024-05-07 17:29:24.087465: Current learning rate: 0.0088
2024-05-07 17:31:23.922076: Validation loss did not improve from -0.58100. Patience: 41/50
2024-05-07 17:31:23.923572: train_loss -0.783
2024-05-07 17:31:23.925192: val_loss -0.5535
2024-05-07 17:31:23.926823: Pseudo dice [0.7538]
2024-05-07 17:31:23.928177: Epoch time: 119.84 s
2024-05-07 17:31:25.395118: 
2024-05-07 17:31:25.396363: Epoch 133
2024-05-07 17:31:25.397443: Current learning rate: 0.00879
2024-05-07 17:33:16.604920: Validation loss did not improve from -0.58100. Patience: 42/50
2024-05-07 17:33:16.606414: train_loss -0.7918
2024-05-07 17:33:16.607703: val_loss -0.5394
2024-05-07 17:33:16.608845: Pseudo dice [0.7439]
2024-05-07 17:33:16.610972: Epoch time: 111.21 s
2024-05-07 17:33:18.030045: 
2024-05-07 17:33:18.031637: Epoch 134
2024-05-07 17:33:18.032797: Current learning rate: 0.00879
2024-05-07 17:35:17.436875: Validation loss did not improve from -0.58100. Patience: 43/50
2024-05-07 17:35:17.438219: train_loss -0.7885
2024-05-07 17:35:17.439806: val_loss -0.5339
2024-05-07 17:35:17.441415: Pseudo dice [0.7445]
2024-05-07 17:35:17.442831: Epoch time: 119.41 s
2024-05-07 17:35:19.338574: 
2024-05-07 17:35:19.340217: Epoch 135
2024-05-07 17:35:19.341432: Current learning rate: 0.00878
2024-05-07 17:37:07.525961: Validation loss did not improve from -0.58100. Patience: 44/50
2024-05-07 17:37:07.527561: train_loss -0.788
2024-05-07 17:37:07.528909: val_loss -0.5479
2024-05-07 17:37:07.529982: Pseudo dice [0.7549]
2024-05-07 17:37:07.531021: Epoch time: 108.19 s
2024-05-07 17:37:08.959080: 
2024-05-07 17:37:08.961017: Epoch 136
2024-05-07 17:37:08.962076: Current learning rate: 0.00877
2024-05-07 17:39:10.473082: Validation loss did not improve from -0.58100. Patience: 45/50
2024-05-07 17:39:10.474515: train_loss -0.784
2024-05-07 17:39:10.475757: val_loss -0.5485
2024-05-07 17:39:10.477134: Pseudo dice [0.754]
2024-05-07 17:39:10.478302: Epoch time: 121.52 s
2024-05-07 17:39:12.838425: 
2024-05-07 17:39:12.839850: Epoch 137
2024-05-07 17:39:12.840970: Current learning rate: 0.00876
2024-05-07 17:41:03.556097: Validation loss did not improve from -0.58100. Patience: 46/50
2024-05-07 17:41:03.557638: train_loss -0.7894
2024-05-07 17:41:03.558941: val_loss -0.5468
2024-05-07 17:41:03.560179: Pseudo dice [0.7545]
2024-05-07 17:41:03.561549: Epoch time: 110.72 s
2024-05-07 17:41:04.988392: 
2024-05-07 17:41:04.989801: Epoch 138
2024-05-07 17:41:04.990833: Current learning rate: 0.00875
2024-05-07 17:43:06.190043: Validation loss did not improve from -0.58100. Patience: 47/50
2024-05-07 17:43:06.191422: train_loss -0.7901
2024-05-07 17:43:06.192700: val_loss -0.5342
2024-05-07 17:43:06.193920: Pseudo dice [0.7525]
2024-05-07 17:43:06.195048: Epoch time: 121.2 s
2024-05-07 17:43:07.659272: 
2024-05-07 17:43:07.660883: Epoch 139
2024-05-07 17:43:07.662017: Current learning rate: 0.00874
2024-05-07 17:45:01.096242: Validation loss did not improve from -0.58100. Patience: 48/50
2024-05-07 17:45:01.097764: train_loss -0.7943
2024-05-07 17:45:01.099211: val_loss -0.5296
2024-05-07 17:45:01.100399: Pseudo dice [0.7509]
2024-05-07 17:45:01.101577: Epoch time: 113.44 s
2024-05-07 17:45:03.075344: 
2024-05-07 17:45:03.077800: Epoch 140
2024-05-07 17:45:03.079224: Current learning rate: 0.00873
2024-05-07 17:47:06.119216: Validation loss did not improve from -0.58100. Patience: 49/50
2024-05-07 17:47:06.120787: train_loss -0.7935
2024-05-07 17:47:06.122256: val_loss -0.5254
2024-05-07 17:47:06.123771: Pseudo dice [0.7448]
2024-05-07 17:47:06.125089: Epoch time: 123.05 s
2024-05-07 17:47:07.597274: 
2024-05-07 17:47:07.599166: Epoch 141
2024-05-07 17:47:07.600760: Current learning rate: 0.00872
2024-05-07 17:49:00.573528: Validation loss did not improve from -0.58100. Patience: 50/50
2024-05-07 17:49:00.575040: train_loss -0.7972
2024-05-07 17:49:00.576366: val_loss -0.5547
2024-05-07 17:49:00.577531: Pseudo dice [0.7602]
2024-05-07 17:49:00.578616: Epoch time: 112.98 s
2024-05-07 17:49:02.099019: Patience reached. Stopping training.
2024-05-07 17:49:02.543824: Training done.
2024-05-07 17:49:03.268274: Using splits from existing split file: /home/gridsan/nchutisilp/datasets/nnUNet_Datasets/nnUNet_preprocessed/Dataset302_Calcium_OCTv2/splits_final.json
2024-05-07 17:49:03.287034: The split file contains 3 splits.
2024-05-07 17:49:03.288369: Desired fold for training: 0
2024-05-07 17:49:03.289296: This split has 4 training and 2 validation cases.
2024-05-07 17:49:03.290415: predicting 101-019
2024-05-07 17:49:03.385823: 101-019, shape torch.Size([1, 375, 498, 498]), rank 0
2024-05-07 17:51:25.690477: predicting 704-003
2024-05-07 17:51:25.746303: 704-003, shape torch.Size([1, 375, 498, 498]), rank 0
2024-05-07 17:54:05.091263: Validation complete
2024-05-07 17:54:05.093315: Mean Validation Dice:  0.7562027643331921
wandb: 
wandb: Run history:
wandb:            ema_fg_dice ▁▁▂▃▄▄▅▅▆▆▆▆▇▇▇▇▇▇▇▇▇▇▇▇▇▇██████████████
wandb:   epoch_end_timestamps ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇██
wandb: epoch_start_timestamps ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇██
wandb:                    lrs ████▇▇▇▇▇▆▆▆▆▆▆▅▅▅▅▅▅▄▄▄▄▄▃▃▃▃▃▃▂▂▂▂▂▁▁▁
wandb:           mean_fg_dice ▁▄▄▅▅▆▆▆▆▆▇▅▇▆█▆█▆▇▇▇▇▇▆▇██▇█▆▇█▆█▆█▇▇██
wandb:           train_losses █▇▅▅▄▄▄▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▁▂▁▁▁▁▂▁▁▁▁▁▁▁
wandb:             val_losses █▅▄▄▃▃▃▃▃▃▁▄▂▃▁▃▁▂▂▂▁▂▂▂▃▁▁▂▁▄▂▂▂▂▃▁▃▂▂▁
wandb: 
wandb: Run summary:
wandb:            ema_fg_dice 0.7517
wandb:   epoch_end_timestamps 1715118540.57487
wandb: epoch_start_timestamps 1715118427.59571
wandb:                    lrs 0.00872
wandb:           mean_fg_dice 0.76017
wandb:           train_losses -0.79719
wandb:             val_losses -0.55472
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/gridsan/nchutisilp/datasets/nnUNet_Datasets/nnUNet_results/Dataset302_Calcium_OCTv2/nnUNetTrainer__nnUNetPlans__3d_32x160x128_b10/fold_0/wandb/offline-run-20240507_140010-0asy64hh
wandb: Find logs at: /home/gridsan/nchutisilp/datasets/nnUNet_Datasets/nnUNet_results/Dataset302_Calcium_OCTv2/nnUNetTrainer__nnUNetPlans__3d_32x160x128_b10/fold_0/wandb/offline-run-20240507_140010-0asy64hh/logs
WARNING:urllib3.connectionpool:Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x7fd95ec7bca0>: Failed to establish a new connection: [Errno 101] Network is unreachable')': /api/4504800232407040/envelope/
WARNING:urllib3.connectionpool:Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x7fd95ec6b4c0>: Failed to establish a new connection: [Errno 101] Network is unreachable')': /api/4504800232407040/envelope/
WARNING:urllib3.connectionpool:Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x7fd929ead460>: Failed to establish a new connection: [Errno 101] Network is unreachable')': /api/4504800232407040/envelope/
WARNING:urllib3.connectionpool:Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x7fd8933f9f10>: Failed to establish a new connection: [Errno 101] Network is unreachable')': /api/4504800232407040/envelope/
WARNING:urllib3.connectionpool:Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x7fd8a0d5a4c0>: Failed to establish a new connection: [Errno 101] Network is unreachable')': /api/4504800232407040/envelope/
WARNING:urllib3.connectionpool:Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x7fd8933fd5e0>: Failed to establish a new connection: [Errno 101] Network is unreachable')': /api/4504800232407040/envelope/
FOLD 0 CONFIG 3d_32x160x128_b10 TRAINER nnUNetTrainer
usage: nnUNetv2_predict [-h] -i I -o O -d D [-p P] [-tr TR] -c C
                        [-f F [F ...]] [-step_size STEP_SIZE] [--disable_tta]
                        [--verbose] [--save_probabilities]
                        [--continue_prediction] [-chk CHK] [-npp NPP]
                        [-nps NPS]
                        [-prev_stage_predictions PREV_STAGE_PREDICTIONS]
                        [-num_parts NUM_PARTS] [-part_id PART_ID]
                        [-device DEVICE] [--disable_progress_bar]
nnUNetv2_predict: error: argument -d: expected one argument

#######################################################################
Please cite the following paper when using nnU-Net:
Isensee, F., Jaeger, P. F., Kohl, S. A., Petersen, J., & Maier-Hein, K. H. (2021). nnU-Net: a self-configuring method for deep learning-based biomedical image segmentation. Nature methods, 18(2), 203-211.
#######################################################################

Traceback (most recent call last):
  File "/home/gridsan/nchutisilp/.local/bin/nnUNetv2_evaluate_simple", line 8, in <module>
    sys.exit(evaluate_simple_entry_point())
  File "/home/gridsan/nchutisilp/projects/nnUNet/nnunetv2/evaluation/evaluate_predictions.py", line 251, in evaluate_simple_entry_point
    compute_metrics_on_folder_simple(args.gt_folder, args.pred_folder, args.l, args.o, args.np, args.il, chill=args.chill)
  File "/home/gridsan/nchutisilp/projects/nnUNet/nnunetv2/evaluation/evaluate_predictions.py", line 213, in compute_metrics_on_folder_simple
    compute_metrics_on_folder(folder_ref, folder_pred, output_file, rw, file_ending,
  File "/home/gridsan/nchutisilp/projects/nnUNet/nnunetv2/evaluation/evaluate_predictions.py", line 139, in compute_metrics_on_folder
    assert all(present), "Not all files in folder_ref exist in folder_pred"
AssertionError: Not all files in folder_ref exist in folder_pred
Completed FOLD 0 CONFIG 3d_32x160x128_b10 TRAINER nnUNetTrainer
