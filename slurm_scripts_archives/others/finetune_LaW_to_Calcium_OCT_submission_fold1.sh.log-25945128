/home/gridsan/nchutisilp/.conda/envs/nnunet/bin/python
wandb: Tracking run with wandb version 0.16.6
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.

############################
INFO: You are using the old nnU-Net default plans. We have updated our recommendations. Please consider using those instead! Read more here: https://github.com/MIC-DKFZ/nnUNet/blob/master/documentation/resenc_presets.md
############################

Using device: cuda:0

#######################################################################
Please cite the following paper when using nnU-Net:
Isensee, F., Jaeger, P. F., Kohl, S. A., Petersen, J., & Maier-Hein, K. H. (2021). nnU-Net: a self-configuring method for deep learning-based biomedical image segmentation. Nature methods, 18(2), 203-211.
#######################################################################

2024-05-03 18:24:49.702501: Using torch.compile...
/home/gridsan/nchutisilp/.local/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn("The verbose parameter is deprecated. Please use get_last_lr() "
################### Loading pretrained weights from file  /home/gridsan/nchutisilp/datasets/nnUNet_Datasets/nnUNet_results/Dataset300_Lumen_and_Wall_OCT/nnUNetTrainer__nnUNetPreprocessPlans__3d_fullres/fold_all/checkpoint_best.pth ###################
Below is the list of overlapping blocks in pretrained model and nnUNet architecture:
encoder.stages.0.0.convs.0.conv.weight shape torch.Size([32, 1, 3, 3, 3])
encoder.stages.0.0.convs.0.conv.bias shape torch.Size([32])
encoder.stages.0.0.convs.0.norm.weight shape torch.Size([32])
encoder.stages.0.0.convs.0.norm.bias shape torch.Size([32])
encoder.stages.0.0.convs.0.all_modules.0.weight shape torch.Size([32, 1, 3, 3, 3])
encoder.stages.0.0.convs.0.all_modules.0.bias shape torch.Size([32])
encoder.stages.0.0.convs.0.all_modules.1.weight shape torch.Size([32])
encoder.stages.0.0.convs.0.all_modules.1.bias shape torch.Size([32])
encoder.stages.0.0.convs.1.conv.weight shape torch.Size([32, 32, 3, 3, 3])
encoder.stages.0.0.convs.1.conv.bias shape torch.Size([32])
encoder.stages.0.0.convs.1.norm.weight shape torch.Size([32])
encoder.stages.0.0.convs.1.norm.bias shape torch.Size([32])
encoder.stages.0.0.convs.1.all_modules.0.weight shape torch.Size([32, 32, 3, 3, 3])
encoder.stages.0.0.convs.1.all_modules.0.bias shape torch.Size([32])
encoder.stages.0.0.convs.1.all_modules.1.weight shape torch.Size([32])
encoder.stages.0.0.convs.1.all_modules.1.bias shape torch.Size([32])
encoder.stages.1.0.convs.0.conv.weight shape torch.Size([64, 32, 3, 3, 3])
encoder.stages.1.0.convs.0.conv.bias shape torch.Size([64])
encoder.stages.1.0.convs.0.norm.weight shape torch.Size([64])
encoder.stages.1.0.convs.0.norm.bias shape torch.Size([64])
encoder.stages.1.0.convs.0.all_modules.0.weight shape torch.Size([64, 32, 3, 3, 3])
encoder.stages.1.0.convs.0.all_modules.0.bias shape torch.Size([64])
encoder.stages.1.0.convs.0.all_modules.1.weight shape torch.Size([64])
encoder.stages.1.0.convs.0.all_modules.1.bias shape torch.Size([64])
encoder.stages.1.0.convs.1.conv.weight shape torch.Size([64, 64, 3, 3, 3])
encoder.stages.1.0.convs.1.conv.bias shape torch.Size([64])
encoder.stages.1.0.convs.1.norm.weight shape torch.Size([64])
encoder.stages.1.0.convs.1.norm.bias shape torch.Size([64])
encoder.stages.1.0.convs.1.all_modules.0.weight shape torch.Size([64, 64, 3, 3, 3])
encoder.stages.1.0.convs.1.all_modules.0.bias shape torch.Size([64])
encoder.stages.1.0.convs.1.all_modules.1.weight shape torch.Size([64])
encoder.stages.1.0.convs.1.all_modules.1.bias shape torch.Size([64])
encoder.stages.2.0.convs.0.conv.weight shape torch.Size([128, 64, 3, 3, 3])
encoder.stages.2.0.convs.0.conv.bias shape torch.Size([128])
encoder.stages.2.0.convs.0.norm.weight shape torch.Size([128])
encoder.stages.2.0.convs.0.norm.bias shape torch.Size([128])
encoder.stages.2.0.convs.0.all_modules.0.weight shape torch.Size([128, 64, 3, 3, 3])
encoder.stages.2.0.convs.0.all_modules.0.bias shape torch.Size([128])
encoder.stages.2.0.convs.0.all_modules.1.weight shape torch.Size([128])
encoder.stages.2.0.convs.0.all_modules.1.bias shape torch.Size([128])
encoder.stages.2.0.convs.1.conv.weight shape torch.Size([128, 128, 3, 3, 3])
encoder.stages.2.0.convs.1.conv.bias shape torch.Size([128])
encoder.stages.2.0.convs.1.norm.weight shape torch.Size([128])
encoder.stages.2.0.convs.1.norm.bias shape torch.Size([128])
encoder.stages.2.0.convs.1.all_modules.0.weight shape torch.Size([128, 128, 3, 3, 3])
encoder.stages.2.0.convs.1.all_modules.0.bias shape torch.Size([128])
encoder.stages.2.0.convs.1.all_modules.1.weight shape torch.Size([128])
encoder.stages.2.0.convs.1.all_modules.1.bias shape torch.Size([128])
encoder.stages.3.0.convs.0.conv.weight shape torch.Size([256, 128, 3, 3, 3])
encoder.stages.3.0.convs.0.conv.bias shape torch.Size([256])
encoder.stages.3.0.convs.0.norm.weight shape torch.Size([256])
encoder.stages.3.0.convs.0.norm.bias shape torch.Size([256])
encoder.stages.3.0.convs.0.all_modules.0.weight shape torch.Size([256, 128, 3, 3, 3])
encoder.stages.3.0.convs.0.all_modules.0.bias shape torch.Size([256])
encoder.stages.3.0.convs.0.all_modules.1.weight shape torch.Size([256])
encoder.stages.3.0.convs.0.all_modules.1.bias shape torch.Size([256])
encoder.stages.3.0.convs.1.conv.weight shape torch.Size([256, 256, 3, 3, 3])
encoder.stages.3.0.convs.1.conv.bias shape torch.Size([256])
encoder.stages.3.0.convs.1.norm.weight shape torch.Size([256])
encoder.stages.3.0.convs.1.norm.bias shape torch.Size([256])
encoder.stages.3.0.convs.1.all_modules.0.weight shape torch.Size([256, 256, 3, 3, 3])
encoder.stages.3.0.convs.1.all_modules.0.bias shape torch.Size([256])
encoder.stages.3.0.convs.1.all_modules.1.weight shape torch.Size([256])
encoder.stages.3.0.convs.1.all_modules.1.bias shape torch.Size([256])
encoder.stages.4.0.convs.0.conv.weight shape torch.Size([320, 256, 3, 3, 3])
encoder.stages.4.0.convs.0.conv.bias shape torch.Size([320])
encoder.stages.4.0.convs.0.norm.weight shape torch.Size([320])
encoder.stages.4.0.convs.0.norm.bias shape torch.Size([320])
encoder.stages.4.0.convs.0.all_modules.0.weight shape torch.Size([320, 256, 3, 3, 3])
encoder.stages.4.0.convs.0.all_modules.0.bias shape torch.Size([320])
encoder.stages.4.0.convs.0.all_modules.1.weight shape torch.Size([320])
encoder.stages.4.0.convs.0.all_modules.1.bias shape torch.Size([320])
encoder.stages.4.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.4.0.convs.1.conv.bias shape torch.Size([320])
encoder.stages.4.0.convs.1.norm.weight shape torch.Size([320])
encoder.stages.4.0.convs.1.norm.bias shape torch.Size([320])
encoder.stages.4.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.4.0.convs.1.all_modules.0.bias shape torch.Size([320])
encoder.stages.4.0.convs.1.all_modules.1.weight shape torch.Size([320])
encoder.stages.4.0.convs.1.all_modules.1.bias shape torch.Size([320])
encoder.stages.5.0.convs.0.conv.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.5.0.convs.0.conv.bias shape torch.Size([320])
encoder.stages.5.0.convs.0.norm.weight shape torch.Size([320])
encoder.stages.5.0.convs.0.norm.bias shape torch.Size([320])
encoder.stages.5.0.convs.0.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.5.0.convs.0.all_modules.0.bias shape torch.Size([320])
encoder.stages.5.0.convs.0.all_modules.1.weight shape torch.Size([320])
encoder.stages.5.0.convs.0.all_modules.1.bias shape torch.Size([320])
encoder.stages.5.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.5.0.convs.1.conv.bias shape torch.Size([320])
encoder.stages.5.0.convs.1.norm.weight shape torch.Size([320])
encoder.stages.5.0.convs.1.norm.bias shape torch.Size([320])
encoder.stages.5.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.5.0.convs.1.all_modules.0.bias shape torch.Size([320])
encoder.stages.5.0.convs.1.all_modules.1.weight shape torch.Size([320])
encoder.stages.5.0.convs.1.all_modules.1.bias shape torch.Size([320])
decoder.encoder.stages.0.0.convs.0.conv.weight shape torch.Size([32, 1, 3, 3, 3])
decoder.encoder.stages.0.0.convs.0.conv.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.norm.weight shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.norm.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.all_modules.0.weight shape torch.Size([32, 1, 3, 3, 3])
decoder.encoder.stages.0.0.convs.0.all_modules.0.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.all_modules.1.weight shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.all_modules.1.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.conv.weight shape torch.Size([32, 32, 3, 3, 3])
decoder.encoder.stages.0.0.convs.1.conv.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.norm.weight shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.norm.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.all_modules.0.weight shape torch.Size([32, 32, 3, 3, 3])
decoder.encoder.stages.0.0.convs.1.all_modules.0.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.all_modules.1.weight shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.all_modules.1.bias shape torch.Size([32])
decoder.encoder.stages.1.0.convs.0.conv.weight shape torch.Size([64, 32, 3, 3, 3])
decoder.encoder.stages.1.0.convs.0.conv.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.norm.weight shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.norm.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.all_modules.0.weight shape torch.Size([64, 32, 3, 3, 3])
decoder.encoder.stages.1.0.convs.0.all_modules.0.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.all_modules.1.weight shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.all_modules.1.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.conv.weight shape torch.Size([64, 64, 3, 3, 3])
decoder.encoder.stages.1.0.convs.1.conv.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.norm.weight shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.norm.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.all_modules.0.weight shape torch.Size([64, 64, 3, 3, 3])
decoder.encoder.stages.1.0.convs.1.all_modules.0.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.all_modules.1.weight shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.all_modules.1.bias shape torch.Size([64])
decoder.encoder.stages.2.0.convs.0.conv.weight shape torch.Size([128, 64, 3, 3, 3])
decoder.encoder.stages.2.0.convs.0.conv.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.norm.weight shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.norm.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.all_modules.0.weight shape torch.Size([128, 64, 3, 3, 3])
decoder.encoder.stages.2.0.convs.0.all_modules.0.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.all_modules.1.weight shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.all_modules.1.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.conv.weight shape torch.Size([128, 128, 3, 3, 3])
decoder.encoder.stages.2.0.convs.1.conv.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.norm.weight shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.norm.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.all_modules.0.weight shape torch.Size([128, 128, 3, 3, 3])
decoder.encoder.stages.2.0.convs.1.all_modules.0.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.all_modules.1.weight shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.all_modules.1.bias shape torch.Size([128])
decoder.encoder.stages.3.0.convs.0.conv.weight shape torch.Size([256, 128, 3, 3, 3])
decoder.encoder.stages.3.0.convs.0.conv.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.norm.weight shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.norm.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.all_modules.0.weight shape torch.Size([256, 128, 3, 3, 3])
decoder.encoder.stages.3.0.convs.0.all_modules.0.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.all_modules.1.weight shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.all_modules.1.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.conv.weight shape torch.Size([256, 256, 3, 3, 3])
decoder.encoder.stages.3.0.convs.1.conv.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.norm.weight shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.norm.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.all_modules.0.weight shape torch.Size([256, 256, 3, 3, 3])
decoder.encoder.stages.3.0.convs.1.all_modules.0.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.all_modules.1.weight shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.all_modules.1.bias shape torch.Size([256])
decoder.encoder.stages.4.0.convs.0.conv.weight shape torch.Size([320, 256, 3, 3, 3])
decoder.encoder.stages.4.0.convs.0.conv.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.norm.weight shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.norm.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.all_modules.0.weight shape torch.Size([320, 256, 3, 3, 3])
decoder.encoder.stages.4.0.convs.0.all_modules.0.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.all_modules.1.weight shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.all_modules.1.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.4.0.convs.1.conv.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.norm.weight shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.norm.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.4.0.convs.1.all_modules.0.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.all_modules.1.weight shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.all_modules.1.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.conv.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.5.0.convs.0.conv.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.norm.weight shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.norm.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.5.0.convs.0.all_modules.0.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.all_modules.1.weight shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.all_modules.1.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.5.0.convs.1.conv.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.norm.weight shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.norm.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.5.0.convs.1.all_modules.0.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.all_modules.1.weight shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.all_modules.1.bias shape torch.Size([320])
decoder.stages.0.convs.0.conv.weight shape torch.Size([320, 640, 3, 3, 3])
decoder.stages.0.convs.0.conv.bias shape torch.Size([320])
decoder.stages.0.convs.0.norm.weight shape torch.Size([320])
decoder.stages.0.convs.0.norm.bias shape torch.Size([320])
decoder.stages.0.convs.0.all_modules.0.weight shape torch.Size([320, 640, 3, 3, 3])
decoder.stages.0.convs.0.all_modules.0.bias shape torch.Size([320])
decoder.stages.0.convs.0.all_modules.1.weight shape torch.Size([320])
decoder.stages.0.convs.0.all_modules.1.bias shape torch.Size([320])
decoder.stages.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.stages.0.convs.1.conv.bias shape torch.Size([320])
decoder.stages.0.convs.1.norm.weight shape torch.Size([320])
decoder.stages.0.convs.1.norm.bias shape torch.Size([320])
decoder.stages.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.stages.0.convs.1.all_modules.0.bias shape torch.Size([320])
decoder.stages.0.convs.1.all_modules.1.weight shape torch.Size([320])
decoder.stages.0.convs.1.all_modules.1.bias shape torch.Size([320])
decoder.stages.1.convs.0.conv.weight shape torch.Size([256, 512, 3, 3, 3])
decoder.stages.1.convs.0.conv.bias shape torch.Size([256])
decoder.stages.1.convs.0.norm.weight shape torch.Size([256])
decoder.stages.1.convs.0.norm.bias shape torch.Size([256])
decoder.stages.1.convs.0.all_modules.0.weight shape torch.Size([256, 512, 3, 3, 3])
decoder.stages.1.convs.0.all_modules.0.bias shape torch.Size([256])
decoder.stages.1.convs.0.all_modules.1.weight shape torch.Size([256])
decoder.stages.1.convs.0.all_modules.1.bias shape torch.Size([256])
decoder.stages.1.convs.1.conv.weight shape torch.Size([256, 256, 3, 3, 3])
decoder.stages.1.convs.1.conv.bias shape torch.Size([256])
decoder.stages.1.convs.1.norm.weight shape torch.Size([256])
decoder.stages.1.convs.1.norm.bias shape torch.Size([256])
decoder.stages.1.convs.1.all_modules.0.weight shape torch.Size([256, 256, 3, 3, 3])
decoder.stages.1.convs.1.all_modules.0.bias shape torch.Size([256])
decoder.stages.1.convs.1.all_modules.1.weight shape torch.Size([256])
decoder.stages.1.convs.1.all_modules.1.bias shape torch.Size([256])
decoder.stages.2.convs.0.conv.weight shape torch.Size([128, 256, 3, 3, 3])
decoder.stages.2.convs.0.conv.bias shape torch.Size([128])
decoder.stages.2.convs.0.norm.weight shape torch.Size([128])
decoder.stages.2.convs.0.norm.bias shape torch.Size([128])
decoder.stages.2.convs.0.all_modules.0.weight shape torch.Size([128, 256, 3, 3, 3])
decoder.stages.2.convs.0.all_modules.0.bias shape torch.Size([128])
decoder.stages.2.convs.0.all_modules.1.weight shape torch.Size([128])
decoder.stages.2.convs.0.all_modules.1.bias shape torch.Size([128])
decoder.stages.2.convs.1.conv.weight shape torch.Size([128, 128, 3, 3, 3])
decoder.stages.2.convs.1.conv.bias shape torch.Size([128])
decoder.stages.2.convs.1.norm.weight shape torch.Size([128])
decoder.stages.2.convs.1.norm.bias shape torch.Size([128])
decoder.stages.2.convs.1.all_modules.0.weight shape torch.Size([128, 128, 3, 3, 3])
decoder.stages.2.convs.1.all_modules.0.bias shape torch.Size([128])
decoder.stages.2.convs.1.all_modules.1.weight shape torch.Size([128])
decoder.stages.2.convs.1.all_modules.1.bias shape torch.Size([128])
decoder.stages.3.convs.0.conv.weight shape torch.Size([64, 128, 3, 3, 3])
decoder.stages.3.convs.0.conv.bias shape torch.Size([64])
decoder.stages.3.convs.0.norm.weight shape torch.Size([64])
decoder.stages.3.convs.0.norm.bias shape torch.Size([64])
decoder.stages.3.convs.0.all_modules.0.weight shape torch.Size([64, 128, 3, 3, 3])
decoder.stages.3.convs.0.all_modules.0.bias shape torch.Size([64])
decoder.stages.3.convs.0.all_modules.1.weight shape torch.Size([64])
decoder.stages.3.convs.0.all_modules.1.bias shape torch.Size([64])
decoder.stages.3.convs.1.conv.weight shape torch.Size([64, 64, 3, 3, 3])
decoder.stages.3.convs.1.conv.bias shape torch.Size([64])
decoder.stages.3.convs.1.norm.weight shape torch.Size([64])
decoder.stages.3.convs.1.norm.bias shape torch.Size([64])
decoder.stages.3.convs.1.all_modules.0.weight shape torch.Size([64, 64, 3, 3, 3])
decoder.stages.3.convs.1.all_modules.0.bias shape torch.Size([64])
decoder.stages.3.convs.1.all_modules.1.weight shape torch.Size([64])
decoder.stages.3.convs.1.all_modules.1.bias shape torch.Size([64])
decoder.stages.4.convs.0.conv.weight shape torch.Size([32, 64, 3, 3, 3])
decoder.stages.4.convs.0.conv.bias shape torch.Size([32])
decoder.stages.4.convs.0.norm.weight shape torch.Size([32])
decoder.stages.4.convs.0.norm.bias shape torch.Size([32])
decoder.stages.4.convs.0.all_modules.0.weight shape torch.Size([32, 64, 3, 3, 3])
decoder.stages.4.convs.0.all_modules.0.bias shape torch.Size([32])
decoder.stages.4.convs.0.all_modules.1.weight shape torch.Size([32])
decoder.stages.4.convs.0.all_modules.1.bias shape torch.Size([32])
decoder.stages.4.convs.1.conv.weight shape torch.Size([32, 32, 3, 3, 3])
decoder.stages.4.convs.1.conv.bias shape torch.Size([32])
decoder.stages.4.convs.1.norm.weight shape torch.Size([32])
decoder.stages.4.convs.1.norm.bias shape torch.Size([32])
decoder.stages.4.convs.1.all_modules.0.weight shape torch.Size([32, 32, 3, 3, 3])
decoder.stages.4.convs.1.all_modules.0.bias shape torch.Size([32])
decoder.stages.4.convs.1.all_modules.1.weight shape torch.Size([32])
decoder.stages.4.convs.1.all_modules.1.bias shape torch.Size([32])
decoder.transpconvs.0.weight shape torch.Size([320, 320, 1, 2, 2])
decoder.transpconvs.0.bias shape torch.Size([320])
decoder.transpconvs.1.weight shape torch.Size([320, 256, 2, 2, 2])
decoder.transpconvs.1.bias shape torch.Size([256])
decoder.transpconvs.2.weight shape torch.Size([256, 128, 2, 2, 2])
decoder.transpconvs.2.bias shape torch.Size([128])
decoder.transpconvs.3.weight shape torch.Size([128, 64, 2, 2, 2])
decoder.transpconvs.3.bias shape torch.Size([64])
decoder.transpconvs.4.weight shape torch.Size([64, 32, 2, 2, 2])
decoder.transpconvs.4.bias shape torch.Size([32])
################### Done ###################
2024-05-03 18:25:05.746499: do_dummy_2d_data_aug: False
2024-05-03 18:25:05.776065: Using splits from existing split file: /home/gridsan/nchutisilp/datasets/nnUNet_Datasets/nnUNet_preprocessed/Dataset302_Calcium_OCTv2/splits_final.json
2024-05-03 18:25:05.793463: The split file contains 3 splits.
2024-05-03 18:25:05.796044: Desired fold for training: 1
2024-05-03 18:25:05.797863: This split has 4 training and 2 validation cases.
using pin_memory on device 0
using pin_memory on device 0

This is the configuration used by this training:
Configuration name: 3d_fullres
 {'data_identifier': 'nnUNetPlans_3d_fullres', 'preprocessor_name': 'DefaultPreprocessor', 'batch_size': 2, 'patch_size': [112, 160, 128], 'median_image_size_in_voxels': [375.0, 498.0, 498.0], 'spacing': [1.0, 1.0, 1.0], 'normalization_schemes': ['ZScoreNormalization'], 'use_mask_for_norm': [False], 'resampling_fn_data': 'resample_data_or_seg_to_shape', 'resampling_fn_seg': 'resample_data_or_seg_to_shape', 'resampling_fn_data_kwargs': {'is_seg': False, 'order': 3, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_seg_kwargs': {'is_seg': True, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_probabilities': 'resample_data_or_seg_to_shape', 'resampling_fn_probabilities_kwargs': {'is_seg': False, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'architecture': {'network_class_name': 'dynamic_network_architectures.architectures.unet.PlainConvUNet', 'arch_kwargs': {'n_stages': 6, 'features_per_stage': [32, 64, 128, 256, 320, 320], 'conv_op': 'torch.nn.modules.conv.Conv3d', 'kernel_sizes': [[3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3]], 'strides': [[1, 1, 1], [2, 2, 2], [2, 2, 2], [2, 2, 2], [2, 2, 2], [1, 2, 2]], 'n_conv_per_stage': [2, 2, 2, 2, 2, 2], 'n_conv_per_stage_decoder': [2, 2, 2, 2, 2], 'conv_bias': True, 'norm_op': 'torch.nn.modules.instancenorm.InstanceNorm3d', 'norm_op_kwargs': {'eps': 1e-05, 'affine': True}, 'dropout_op': None, 'dropout_op_kwargs': None, 'nonlin': 'torch.nn.LeakyReLU', 'nonlin_kwargs': {'inplace': True}, 'deep_supervision': True}, '_kw_requires_import': ['conv_op', 'norm_op', 'dropout_op', 'nonlin']}, 'batch_dice': True} 

These are the global plan.json settings:
 {'dataset_name': 'Dataset302_Calcium_OCTv2', 'plans_name': 'nnUNetPlans', 'original_median_spacing_after_transp': [1.0, 1.0, 1.0], 'original_median_shape_after_transp': [375, 498, 498], 'image_reader_writer': 'SimpleITKIO', 'transpose_forward': [0, 1, 2], 'transpose_backward': [0, 1, 2], 'experiment_planner_used': 'ExperimentPlanner', 'label_manager': 'LabelManager', 'foreground_intensity_properties_per_channel': {'0': {'max': 0.4977976381778717, 'mean': 0.13332499563694, 'median': 0.08871842920780182, 'min': 0.0, 'percentile_00_5': 0.014754901640117168, 'percentile_99_5': 0.4977976381778717, 'std': 0.12022686749696732}}} 

2024-05-03 18:25:15.242699: unpacking dataset...
2024-05-03 18:25:21.770905: unpacking done...
2024-05-03 18:25:21.797829: Unable to plot network architecture: nnUNet_compile is enabled!
2024-05-03 18:25:22.025973: 
2024-05-03 18:25:22.028131: Epoch 0
2024-05-03 18:25:22.030487: Current learning rate: 0.01
2024-05-03 18:28:50.207201: Validation loss improved from 1000.00000 to -0.33504! Patience: 0/50
2024-05-03 18:28:50.242810: train_loss -0.2765
2024-05-03 18:28:50.255122: val_loss -0.335
2024-05-03 18:28:50.256483: Pseudo dice [0.6423]
2024-05-03 18:28:50.257566: Epoch time: 208.18 s
2024-05-03 18:28:50.258422: Yayy! New best EMA pseudo Dice: 0.6423
2024-05-03 18:28:53.087407: 
2024-05-03 18:28:53.089529: Epoch 1
2024-05-03 18:28:53.091239: Current learning rate: 0.00999
2024-05-03 18:29:56.542843: Validation loss improved from -0.33504 to -0.33901! Patience: 0/50
2024-05-03 18:29:56.544291: train_loss -0.4244
2024-05-03 18:29:56.545633: val_loss -0.339
2024-05-03 18:29:56.546688: Pseudo dice [0.6822]
2024-05-03 18:29:56.547681: Epoch time: 63.46 s
2024-05-03 18:29:56.548669: Yayy! New best EMA pseudo Dice: 0.6463
2024-05-03 18:29:58.078808: 
2024-05-03 18:29:58.080699: Epoch 2
2024-05-03 18:29:58.081732: Current learning rate: 0.00998
2024-05-03 18:31:01.858432: Validation loss improved from -0.33901 to -0.37003! Patience: 0/50
2024-05-03 18:31:01.859836: train_loss -0.4536
2024-05-03 18:31:01.860931: val_loss -0.37
2024-05-03 18:31:01.861957: Pseudo dice [0.6783]
2024-05-03 18:31:01.862905: Epoch time: 63.78 s
2024-05-03 18:31:01.863867: Yayy! New best EMA pseudo Dice: 0.6495
2024-05-03 18:31:03.472697: 
2024-05-03 18:31:03.474450: Epoch 3
2024-05-03 18:31:03.475484: Current learning rate: 0.00997
2024-05-03 18:32:07.385802: Validation loss improved from -0.37003 to -0.40098! Patience: 0/50
2024-05-03 18:32:07.387144: train_loss -0.4808
2024-05-03 18:32:07.388517: val_loss -0.401
2024-05-03 18:32:07.389735: Pseudo dice [0.7138]
2024-05-03 18:32:07.390785: Epoch time: 63.92 s
2024-05-03 18:32:07.391843: Yayy! New best EMA pseudo Dice: 0.6559
2024-05-03 18:32:08.978303: 
2024-05-03 18:32:08.980341: Epoch 4
2024-05-03 18:32:08.982337: Current learning rate: 0.00996
2024-05-03 18:33:13.035873: Validation loss did not improve from -0.40098. Patience: 1/50
2024-05-03 18:33:13.037464: train_loss -0.5092
2024-05-03 18:33:13.038844: val_loss -0.3653
2024-05-03 18:33:13.039993: Pseudo dice [0.7031]
2024-05-03 18:33:13.041047: Epoch time: 64.06 s
2024-05-03 18:33:13.331003: Yayy! New best EMA pseudo Dice: 0.6606
2024-05-03 18:33:14.922215: 
2024-05-03 18:33:14.923916: Epoch 5
2024-05-03 18:33:14.924972: Current learning rate: 0.00995
2024-05-03 18:34:18.921913: Validation loss did not improve from -0.40098. Patience: 2/50
2024-05-03 18:34:18.923232: train_loss -0.5149
2024-05-03 18:34:18.924426: val_loss -0.3458
2024-05-03 18:34:18.925459: Pseudo dice [0.6685]
2024-05-03 18:34:18.926354: Epoch time: 64.0 s
2024-05-03 18:34:18.927315: Yayy! New best EMA pseudo Dice: 0.6614
2024-05-03 18:34:20.461564: 
2024-05-03 18:34:20.463468: Epoch 6
2024-05-03 18:34:20.465037: Current learning rate: 0.00995
2024-05-03 18:35:24.498154: Validation loss improved from -0.40098 to -0.44940! Patience: 2/50
2024-05-03 18:35:24.500017: train_loss -0.5264
2024-05-03 18:35:24.501542: val_loss -0.4494
2024-05-03 18:35:24.502764: Pseudo dice [0.7211]
2024-05-03 18:35:24.503875: Epoch time: 64.04 s
2024-05-03 18:35:24.505102: Yayy! New best EMA pseudo Dice: 0.6674
2024-05-03 18:35:26.476313: 
2024-05-03 18:35:26.478633: Epoch 7
2024-05-03 18:35:26.479769: Current learning rate: 0.00994
2024-05-03 18:36:30.477615: Validation loss did not improve from -0.44940. Patience: 1/50
2024-05-03 18:36:30.479182: train_loss -0.5417
2024-05-03 18:36:30.480888: val_loss -0.3567
2024-05-03 18:36:30.481972: Pseudo dice [0.6773]
2024-05-03 18:36:30.483080: Epoch time: 64.0 s
2024-05-03 18:36:30.483994: Yayy! New best EMA pseudo Dice: 0.6684
2024-05-03 18:36:32.070390: 
2024-05-03 18:36:32.072475: Epoch 8
2024-05-03 18:36:32.074063: Current learning rate: 0.00993
2024-05-03 18:37:36.249226: Validation loss did not improve from -0.44940. Patience: 2/50
2024-05-03 18:37:36.250663: train_loss -0.5609
2024-05-03 18:37:36.251983: val_loss -0.4094
2024-05-03 18:37:36.253027: Pseudo dice [0.7043]
2024-05-03 18:37:36.254113: Epoch time: 64.18 s
2024-05-03 18:37:36.255058: Yayy! New best EMA pseudo Dice: 0.672
2024-05-03 18:37:37.883186: 
2024-05-03 18:37:37.885483: Epoch 9
2024-05-03 18:37:37.886984: Current learning rate: 0.00992
2024-05-03 18:38:42.047046: Validation loss improved from -0.44940 to -0.45288! Patience: 2/50
2024-05-03 18:38:42.048450: train_loss -0.5782
2024-05-03 18:38:42.049743: val_loss -0.4529
2024-05-03 18:38:42.050743: Pseudo dice [0.7547]
2024-05-03 18:38:42.051762: Epoch time: 64.17 s
2024-05-03 18:38:42.381950: Yayy! New best EMA pseudo Dice: 0.6802
2024-05-03 18:38:43.918606: 
2024-05-03 18:38:43.920585: Epoch 10
2024-05-03 18:38:43.921706: Current learning rate: 0.00991
2024-05-03 18:39:48.080065: Validation loss did not improve from -0.45288. Patience: 1/50
2024-05-03 18:39:48.081525: train_loss -0.5522
2024-05-03 18:39:48.083423: val_loss -0.3611
2024-05-03 18:39:48.084518: Pseudo dice [0.6819]
2024-05-03 18:39:48.085724: Epoch time: 64.16 s
2024-05-03 18:39:48.086739: Yayy! New best EMA pseudo Dice: 0.6804
2024-05-03 18:39:49.633714: 
2024-05-03 18:39:49.635337: Epoch 11
2024-05-03 18:39:49.636326: Current learning rate: 0.0099
2024-05-03 18:40:53.830914: Validation loss did not improve from -0.45288. Patience: 2/50
2024-05-03 18:40:53.835228: train_loss -0.5572
2024-05-03 18:40:53.836580: val_loss -0.3993
2024-05-03 18:40:53.837611: Pseudo dice [0.6889]
2024-05-03 18:40:53.838754: Epoch time: 64.2 s
2024-05-03 18:40:53.839662: Yayy! New best EMA pseudo Dice: 0.6813
2024-05-03 18:40:55.383690: 
2024-05-03 18:40:55.385710: Epoch 12
2024-05-03 18:40:55.386889: Current learning rate: 0.00989
2024-05-03 18:41:59.581263: Validation loss did not improve from -0.45288. Patience: 3/50
2024-05-03 18:41:59.582505: train_loss -0.5794
2024-05-03 18:41:59.583937: val_loss -0.4081
2024-05-03 18:41:59.585171: Pseudo dice [0.707]
2024-05-03 18:41:59.586262: Epoch time: 64.2 s
2024-05-03 18:41:59.587191: Yayy! New best EMA pseudo Dice: 0.6838
2024-05-03 18:42:01.155267: 
2024-05-03 18:42:01.157346: Epoch 13
2024-05-03 18:42:01.158659: Current learning rate: 0.00988
2024-05-03 18:43:05.313148: Validation loss improved from -0.45288 to -0.46334! Patience: 3/50
2024-05-03 18:43:05.314656: train_loss -0.5914
2024-05-03 18:43:05.315893: val_loss -0.4633
2024-05-03 18:43:05.317103: Pseudo dice [0.7444]
2024-05-03 18:43:05.318222: Epoch time: 64.16 s
2024-05-03 18:43:05.319280: Yayy! New best EMA pseudo Dice: 0.6899
2024-05-03 18:43:06.921786: 
2024-05-03 18:43:06.924101: Epoch 14
2024-05-03 18:43:06.925265: Current learning rate: 0.00987
2024-05-03 18:44:11.107472: Validation loss did not improve from -0.46334. Patience: 1/50
2024-05-03 18:44:11.108804: train_loss -0.5941
2024-05-03 18:44:11.110056: val_loss -0.3727
2024-05-03 18:44:11.111074: Pseudo dice [0.7191]
2024-05-03 18:44:11.112041: Epoch time: 64.19 s
2024-05-03 18:44:11.448263: Yayy! New best EMA pseudo Dice: 0.6928
2024-05-03 18:44:13.053785: 
2024-05-03 18:44:13.055336: Epoch 15
2024-05-03 18:44:13.056499: Current learning rate: 0.00986
2024-05-03 18:45:17.152489: Validation loss did not improve from -0.46334. Patience: 2/50
2024-05-03 18:45:17.154361: train_loss -0.6134
2024-05-03 18:45:17.155789: val_loss -0.4626
2024-05-03 18:45:17.157001: Pseudo dice [0.728]
2024-05-03 18:45:17.158151: Epoch time: 64.1 s
2024-05-03 18:45:17.159263: Yayy! New best EMA pseudo Dice: 0.6963
2024-05-03 18:45:18.798642: 
2024-05-03 18:45:18.800679: Epoch 16
2024-05-03 18:45:18.801704: Current learning rate: 0.00986
2024-05-03 18:46:23.076514: Validation loss did not improve from -0.46334. Patience: 3/50
2024-05-03 18:46:23.078569: train_loss -0.6214
2024-05-03 18:46:23.080070: val_loss -0.4331
2024-05-03 18:46:23.081231: Pseudo dice [0.7166]
2024-05-03 18:46:23.082383: Epoch time: 64.28 s
2024-05-03 18:46:23.083445: Yayy! New best EMA pseudo Dice: 0.6983
2024-05-03 18:46:24.710697: 
2024-05-03 18:46:24.712839: Epoch 17
2024-05-03 18:46:24.714780: Current learning rate: 0.00985
2024-05-03 18:47:28.975945: Validation loss did not improve from -0.46334. Patience: 4/50
2024-05-03 18:47:28.977635: train_loss -0.5999
2024-05-03 18:47:28.978807: val_loss -0.3673
2024-05-03 18:47:28.979761: Pseudo dice [0.7048]
2024-05-03 18:47:28.980958: Epoch time: 64.27 s
2024-05-03 18:47:28.982049: Yayy! New best EMA pseudo Dice: 0.699
2024-05-03 18:47:31.013885: 
2024-05-03 18:47:31.016526: Epoch 18
2024-05-03 18:47:31.017708: Current learning rate: 0.00984
2024-05-03 18:48:35.282183: Validation loss did not improve from -0.46334. Patience: 5/50
2024-05-03 18:48:35.283675: train_loss -0.6272
2024-05-03 18:48:35.284829: val_loss -0.4209
2024-05-03 18:48:35.285827: Pseudo dice [0.7082]
2024-05-03 18:48:35.286891: Epoch time: 64.27 s
2024-05-03 18:48:35.287867: Yayy! New best EMA pseudo Dice: 0.6999
2024-05-03 18:48:36.909740: 
2024-05-03 18:48:36.911340: Epoch 19
2024-05-03 18:48:36.912486: Current learning rate: 0.00983
2024-05-03 18:49:41.153719: Validation loss did not improve from -0.46334. Patience: 6/50
2024-05-03 18:49:41.155301: train_loss -0.6205
2024-05-03 18:49:41.156595: val_loss -0.4244
2024-05-03 18:49:41.157611: Pseudo dice [0.7445]
2024-05-03 18:49:41.158782: Epoch time: 64.25 s
2024-05-03 18:49:41.502641: Yayy! New best EMA pseudo Dice: 0.7044
2024-05-03 18:49:43.118275: 
2024-05-03 18:49:43.120266: Epoch 20
2024-05-03 18:49:43.121588: Current learning rate: 0.00982
2024-05-03 18:50:47.379597: Validation loss did not improve from -0.46334. Patience: 7/50
2024-05-03 18:50:47.381197: train_loss -0.6152
2024-05-03 18:50:47.382472: val_loss -0.4276
2024-05-03 18:50:47.383688: Pseudo dice [0.7229]
2024-05-03 18:50:47.384754: Epoch time: 64.26 s
2024-05-03 18:50:47.385789: Yayy! New best EMA pseudo Dice: 0.7062
2024-05-03 18:50:49.044977: 
2024-05-03 18:50:49.047085: Epoch 21
2024-05-03 18:50:49.048802: Current learning rate: 0.00981
2024-05-03 18:51:53.274913: Validation loss did not improve from -0.46334. Patience: 8/50
2024-05-03 18:51:53.276496: train_loss -0.6458
2024-05-03 18:51:53.278388: val_loss -0.456
2024-05-03 18:51:53.279633: Pseudo dice [0.7368]
2024-05-03 18:51:53.280744: Epoch time: 64.23 s
2024-05-03 18:51:53.281759: Yayy! New best EMA pseudo Dice: 0.7093
2024-05-03 18:51:54.835105: 
2024-05-03 18:51:54.836999: Epoch 22
2024-05-03 18:51:54.838074: Current learning rate: 0.0098
2024-05-03 18:52:59.060421: Validation loss did not improve from -0.46334. Patience: 9/50
2024-05-03 18:52:59.061755: train_loss -0.6398
2024-05-03 18:52:59.063055: val_loss -0.4212
2024-05-03 18:52:59.064436: Pseudo dice [0.7223]
2024-05-03 18:52:59.065465: Epoch time: 64.23 s
2024-05-03 18:52:59.068027: Yayy! New best EMA pseudo Dice: 0.7106
2024-05-03 18:53:00.678490: 
2024-05-03 18:53:00.680466: Epoch 23
2024-05-03 18:53:00.681794: Current learning rate: 0.00979
2024-05-03 18:54:04.910087: Validation loss did not improve from -0.46334. Patience: 10/50
2024-05-03 18:54:04.911278: train_loss -0.6507
2024-05-03 18:54:04.912602: val_loss -0.4314
2024-05-03 18:54:04.913856: Pseudo dice [0.702]
2024-05-03 18:54:04.915025: Epoch time: 64.23 s
2024-05-03 18:54:06.141671: 
2024-05-03 18:54:06.143458: Epoch 24
2024-05-03 18:54:06.145004: Current learning rate: 0.00978
2024-05-03 18:55:10.448224: Validation loss did not improve from -0.46334. Patience: 11/50
2024-05-03 18:55:10.449658: train_loss -0.6372
2024-05-03 18:55:10.451533: val_loss -0.4169
2024-05-03 18:55:10.452990: Pseudo dice [0.7251]
2024-05-03 18:55:10.454351: Epoch time: 64.31 s
2024-05-03 18:55:10.807037: Yayy! New best EMA pseudo Dice: 0.7113
2024-05-03 18:55:12.382860: 
2024-05-03 18:55:12.385298: Epoch 25
2024-05-03 18:55:12.386545: Current learning rate: 0.00977
2024-05-03 18:56:16.700093: Validation loss did not improve from -0.46334. Patience: 12/50
2024-05-03 18:56:16.701428: train_loss -0.6601
2024-05-03 18:56:16.702708: val_loss -0.4264
2024-05-03 18:56:16.703938: Pseudo dice [0.7125]
2024-05-03 18:56:16.705039: Epoch time: 64.32 s
2024-05-03 18:56:16.705925: Yayy! New best EMA pseudo Dice: 0.7114
2024-05-03 18:56:18.293634: 
2024-05-03 18:56:18.295469: Epoch 26
2024-05-03 18:56:18.296512: Current learning rate: 0.00977
2024-05-03 18:57:22.606781: Validation loss improved from -0.46334 to -0.49572! Patience: 12/50
2024-05-03 18:57:22.608169: train_loss -0.6627
2024-05-03 18:57:22.609749: val_loss -0.4957
2024-05-03 18:57:22.610871: Pseudo dice [0.7452]
2024-05-03 18:57:22.612113: Epoch time: 64.32 s
2024-05-03 18:57:22.613248: Yayy! New best EMA pseudo Dice: 0.7148
2024-05-03 18:57:24.185680: 
2024-05-03 18:57:24.187763: Epoch 27
2024-05-03 18:57:24.189086: Current learning rate: 0.00976
2024-05-03 18:58:28.461382: Validation loss did not improve from -0.49572. Patience: 1/50
2024-05-03 18:58:28.463142: train_loss -0.6665
2024-05-03 18:58:28.464732: val_loss -0.4176
2024-05-03 18:58:28.465799: Pseudo dice [0.7056]
2024-05-03 18:58:28.466806: Epoch time: 64.28 s
2024-05-03 18:58:29.693583: 
2024-05-03 18:58:29.695930: Epoch 28
2024-05-03 18:58:29.697447: Current learning rate: 0.00975
2024-05-03 18:59:34.030188: Validation loss did not improve from -0.49572. Patience: 2/50
2024-05-03 18:59:34.031405: train_loss -0.6608
2024-05-03 18:59:34.032538: val_loss -0.3632
2024-05-03 18:59:34.033621: Pseudo dice [0.7061]
2024-05-03 18:59:34.034706: Epoch time: 64.34 s
2024-05-03 18:59:35.585771: 
2024-05-03 18:59:35.587795: Epoch 29
2024-05-03 18:59:35.589327: Current learning rate: 0.00974
2024-05-03 19:00:39.862864: Validation loss did not improve from -0.49572. Patience: 3/50
2024-05-03 19:00:39.864276: train_loss -0.6627
2024-05-03 19:00:39.865698: val_loss -0.473
2024-05-03 19:00:39.866774: Pseudo dice [0.7333]
2024-05-03 19:00:39.867901: Epoch time: 64.28 s
2024-05-03 19:00:40.219522: Yayy! New best EMA pseudo Dice: 0.7151
2024-05-03 19:00:42.275733: 
2024-05-03 19:00:42.277645: Epoch 30
2024-05-03 19:00:42.279018: Current learning rate: 0.00973
2024-05-03 19:01:46.568457: Validation loss did not improve from -0.49572. Patience: 4/50
2024-05-03 19:01:46.570284: train_loss -0.6661
2024-05-03 19:01:46.571565: val_loss -0.3958
2024-05-03 19:01:46.572602: Pseudo dice [0.698]
2024-05-03 19:01:46.573757: Epoch time: 64.3 s
2024-05-03 19:01:47.869639: 
2024-05-03 19:01:47.871691: Epoch 31
2024-05-03 19:01:47.873062: Current learning rate: 0.00972
2024-05-03 19:02:52.221342: Validation loss did not improve from -0.49572. Patience: 5/50
2024-05-03 19:02:52.222969: train_loss -0.6687
2024-05-03 19:02:52.224582: val_loss -0.4373
2024-05-03 19:02:52.225951: Pseudo dice [0.7247]
2024-05-03 19:02:52.226915: Epoch time: 64.36 s
2024-05-03 19:02:53.476493: 
2024-05-03 19:02:53.478774: Epoch 32
2024-05-03 19:02:53.480169: Current learning rate: 0.00971
2024-05-03 19:03:57.846946: Validation loss did not improve from -0.49572. Patience: 6/50
2024-05-03 19:03:57.848467: train_loss -0.6594
2024-05-03 19:03:57.849742: val_loss -0.4141
2024-05-03 19:03:57.850951: Pseudo dice [0.7128]
2024-05-03 19:03:57.852216: Epoch time: 64.37 s
2024-05-03 19:03:59.110600: 
2024-05-03 19:03:59.112147: Epoch 33
2024-05-03 19:03:59.113285: Current learning rate: 0.0097
2024-05-03 19:05:03.407170: Validation loss did not improve from -0.49572. Patience: 7/50
2024-05-03 19:05:03.408990: train_loss -0.6717
2024-05-03 19:05:03.410468: val_loss -0.4166
2024-05-03 19:05:03.412193: Pseudo dice [0.7171]
2024-05-03 19:05:03.414120: Epoch time: 64.3 s
2024-05-03 19:05:04.791758: 
2024-05-03 19:05:04.794035: Epoch 34
2024-05-03 19:05:04.795258: Current learning rate: 0.00969
2024-05-03 19:06:09.005521: Validation loss did not improve from -0.49572. Patience: 8/50
2024-05-03 19:06:09.007072: train_loss -0.6674
2024-05-03 19:06:09.008739: val_loss -0.4581
2024-05-03 19:06:09.009919: Pseudo dice [0.7376]
2024-05-03 19:06:09.011092: Epoch time: 64.22 s
2024-05-03 19:06:09.359025: Yayy! New best EMA pseudo Dice: 0.7169
2024-05-03 19:06:10.985422: 
2024-05-03 19:06:10.987977: Epoch 35
2024-05-03 19:06:10.989621: Current learning rate: 0.00968
2024-05-03 19:07:15.146079: Validation loss did not improve from -0.49572. Patience: 9/50
2024-05-03 19:07:15.147972: train_loss -0.6761
2024-05-03 19:07:15.149999: val_loss -0.3914
2024-05-03 19:07:15.151366: Pseudo dice [0.6961]
2024-05-03 19:07:15.152695: Epoch time: 64.16 s
2024-05-03 19:07:16.494264: 
2024-05-03 19:07:16.498371: Epoch 36
2024-05-03 19:07:16.500421: Current learning rate: 0.00968
2024-05-03 19:08:20.765990: Validation loss did not improve from -0.49572. Patience: 10/50
2024-05-03 19:08:20.767594: train_loss -0.669
2024-05-03 19:08:20.769132: val_loss -0.4521
2024-05-03 19:08:20.770384: Pseudo dice [0.7118]
2024-05-03 19:08:20.771674: Epoch time: 64.28 s
2024-05-03 19:08:22.063008: 
2024-05-03 19:08:22.065279: Epoch 37
2024-05-03 19:08:22.067218: Current learning rate: 0.00967
2024-05-03 19:09:26.335854: Validation loss did not improve from -0.49572. Patience: 11/50
2024-05-03 19:09:26.337760: train_loss -0.6753
2024-05-03 19:09:26.339466: val_loss -0.4842
2024-05-03 19:09:26.340860: Pseudo dice [0.7468]
2024-05-03 19:09:26.342573: Epoch time: 64.28 s
2024-05-03 19:09:26.344035: Yayy! New best EMA pseudo Dice: 0.7178
2024-05-03 19:09:28.017805: 
2024-05-03 19:09:28.021889: Epoch 38
2024-05-03 19:09:28.023960: Current learning rate: 0.00966
2024-05-03 19:10:32.278168: Validation loss did not improve from -0.49572. Patience: 12/50
2024-05-03 19:10:32.280392: train_loss -0.6938
2024-05-03 19:10:32.282412: val_loss -0.4714
2024-05-03 19:10:32.283567: Pseudo dice [0.7316]
2024-05-03 19:10:32.284824: Epoch time: 64.26 s
2024-05-03 19:10:32.285901: Yayy! New best EMA pseudo Dice: 0.7192
2024-05-03 19:10:33.917362: 
2024-05-03 19:10:33.920133: Epoch 39
2024-05-03 19:10:33.921802: Current learning rate: 0.00965
2024-05-03 19:11:38.119143: Validation loss did not improve from -0.49572. Patience: 13/50
2024-05-03 19:11:38.121306: train_loss -0.698
2024-05-03 19:11:38.123817: val_loss -0.4547
2024-05-03 19:11:38.125220: Pseudo dice [0.7274]
2024-05-03 19:11:38.126305: Epoch time: 64.21 s
2024-05-03 19:11:38.473513: Yayy! New best EMA pseudo Dice: 0.72
2024-05-03 19:11:40.619819: 
2024-05-03 19:11:40.621538: Epoch 40
2024-05-03 19:11:40.622731: Current learning rate: 0.00964
2024-05-03 19:12:44.814933: Validation loss did not improve from -0.49572. Patience: 14/50
2024-05-03 19:12:44.816742: train_loss -0.6801
2024-05-03 19:12:44.818295: val_loss -0.4338
2024-05-03 19:12:44.820158: Pseudo dice [0.736]
2024-05-03 19:12:44.821882: Epoch time: 64.2 s
2024-05-03 19:12:44.823525: Yayy! New best EMA pseudo Dice: 0.7216
2024-05-03 19:12:46.548368: 
2024-05-03 19:12:46.550556: Epoch 41
2024-05-03 19:12:46.551597: Current learning rate: 0.00963
2024-05-03 19:13:50.929544: Validation loss did not improve from -0.49572. Patience: 15/50
2024-05-03 19:13:50.931065: train_loss -0.685
2024-05-03 19:13:50.932658: val_loss -0.4462
2024-05-03 19:13:50.934164: Pseudo dice [0.7222]
2024-05-03 19:13:50.935534: Epoch time: 64.38 s
2024-05-03 19:13:50.936465: Yayy! New best EMA pseudo Dice: 0.7216
2024-05-03 19:13:52.503803: 
2024-05-03 19:13:52.505944: Epoch 42
2024-05-03 19:13:52.507215: Current learning rate: 0.00962
2024-05-03 19:14:56.873222: Validation loss did not improve from -0.49572. Patience: 16/50
2024-05-03 19:14:56.875251: train_loss -0.6844
2024-05-03 19:14:56.877019: val_loss -0.4494
2024-05-03 19:14:56.878115: Pseudo dice [0.7296]
2024-05-03 19:14:56.879061: Epoch time: 64.37 s
2024-05-03 19:14:56.880085: Yayy! New best EMA pseudo Dice: 0.7224
2024-05-03 19:14:58.439976: 
2024-05-03 19:14:58.441295: Epoch 43
2024-05-03 19:14:58.442490: Current learning rate: 0.00961
2024-05-03 19:16:02.823795: Validation loss did not improve from -0.49572. Patience: 17/50
2024-05-03 19:16:02.826111: train_loss -0.6903
2024-05-03 19:16:02.827760: val_loss -0.4807
2024-05-03 19:16:02.828879: Pseudo dice [0.7521]
2024-05-03 19:16:02.829916: Epoch time: 64.39 s
2024-05-03 19:16:02.831145: Yayy! New best EMA pseudo Dice: 0.7254
2024-05-03 19:16:04.404710: 
2024-05-03 19:16:04.406456: Epoch 44
2024-05-03 19:16:04.407468: Current learning rate: 0.0096
2024-05-03 19:17:08.752247: Validation loss did not improve from -0.49572. Patience: 18/50
2024-05-03 19:17:08.754146: train_loss -0.707
2024-05-03 19:17:08.755703: val_loss -0.4855
2024-05-03 19:17:08.756694: Pseudo dice [0.7515]
2024-05-03 19:17:08.757581: Epoch time: 64.35 s
2024-05-03 19:17:09.110480: Yayy! New best EMA pseudo Dice: 0.728
2024-05-03 19:17:10.905583: 
2024-05-03 19:17:10.907448: Epoch 45
2024-05-03 19:17:10.908813: Current learning rate: 0.00959
2024-05-03 19:18:15.280810: Validation loss did not improve from -0.49572. Patience: 19/50
2024-05-03 19:18:15.282190: train_loss -0.6895
2024-05-03 19:18:15.283358: val_loss -0.4515
2024-05-03 19:18:15.284540: Pseudo dice [0.753]
2024-05-03 19:18:15.285621: Epoch time: 64.38 s
2024-05-03 19:18:15.286623: Yayy! New best EMA pseudo Dice: 0.7305
2024-05-03 19:18:16.856355: 
2024-05-03 19:18:16.858481: Epoch 46
2024-05-03 19:18:16.860289: Current learning rate: 0.00959
2024-05-03 19:19:21.258834: Validation loss did not improve from -0.49572. Patience: 20/50
2024-05-03 19:19:21.260344: train_loss -0.7024
2024-05-03 19:19:21.261569: val_loss -0.4797
2024-05-03 19:19:21.262511: Pseudo dice [0.7588]
2024-05-03 19:19:21.263402: Epoch time: 64.41 s
2024-05-03 19:19:21.264496: Yayy! New best EMA pseudo Dice: 0.7333
2024-05-03 19:19:22.804746: 
2024-05-03 19:19:22.806740: Epoch 47
2024-05-03 19:19:22.807915: Current learning rate: 0.00958
2024-05-03 19:20:27.157876: Validation loss did not improve from -0.49572. Patience: 21/50
2024-05-03 19:20:27.159197: train_loss -0.7039
2024-05-03 19:20:27.160334: val_loss -0.4012
2024-05-03 19:20:27.161533: Pseudo dice [0.6995]
2024-05-03 19:20:27.162531: Epoch time: 64.36 s
2024-05-03 19:20:28.363001: 
2024-05-03 19:20:28.364863: Epoch 48
2024-05-03 19:20:28.366137: Current learning rate: 0.00957
2024-05-03 19:21:32.755975: Validation loss did not improve from -0.49572. Patience: 22/50
2024-05-03 19:21:32.760930: train_loss -0.7123
2024-05-03 19:21:32.762523: val_loss -0.4313
2024-05-03 19:21:32.763576: Pseudo dice [0.7314]
2024-05-03 19:21:32.764533: Epoch time: 64.4 s
2024-05-03 19:21:33.979929: 
2024-05-03 19:21:33.981667: Epoch 49
2024-05-03 19:21:33.982611: Current learning rate: 0.00956
2024-05-03 19:22:38.367283: Validation loss did not improve from -0.49572. Patience: 23/50
2024-05-03 19:22:38.368752: train_loss -0.7132
2024-05-03 19:22:38.370476: val_loss -0.3998
2024-05-03 19:22:38.371825: Pseudo dice [0.708]
2024-05-03 19:22:38.373055: Epoch time: 64.39 s
2024-05-03 19:22:39.940553: 
2024-05-03 19:22:39.942287: Epoch 50
2024-05-03 19:22:39.943524: Current learning rate: 0.00955
2024-05-03 19:23:44.330311: Validation loss did not improve from -0.49572. Patience: 24/50
2024-05-03 19:23:44.331622: train_loss -0.693
2024-05-03 19:23:44.333196: val_loss -0.4406
2024-05-03 19:23:44.334314: Pseudo dice [0.7257]
2024-05-03 19:23:44.335609: Epoch time: 64.39 s
2024-05-03 19:23:45.554599: 
2024-05-03 19:23:45.556497: Epoch 51
2024-05-03 19:23:45.558179: Current learning rate: 0.00954
2024-05-03 19:24:50.218233: Validation loss did not improve from -0.49572. Patience: 25/50
2024-05-03 19:24:50.219682: train_loss -0.6934
2024-05-03 19:24:50.221173: val_loss -0.4514
2024-05-03 19:24:50.222564: Pseudo dice [0.706]
2024-05-03 19:24:50.223788: Epoch time: 64.67 s
2024-05-03 19:24:51.823106: 
2024-05-03 19:24:51.825613: Epoch 52
2024-05-03 19:24:51.827510: Current learning rate: 0.00953
2024-05-03 19:25:57.383394: Validation loss did not improve from -0.49572. Patience: 26/50
2024-05-03 19:25:57.384890: train_loss -0.6976
2024-05-03 19:25:57.395602: val_loss -0.4274
2024-05-03 19:25:57.397082: Pseudo dice [0.7082]
2024-05-03 19:25:57.399501: Epoch time: 65.56 s
2024-05-03 19:25:58.711846: 
2024-05-03 19:25:58.713858: Epoch 53
2024-05-03 19:25:58.715515: Current learning rate: 0.00952
2024-05-03 19:27:03.106604: Validation loss did not improve from -0.49572. Patience: 27/50
2024-05-03 19:27:03.108196: train_loss -0.7009
2024-05-03 19:27:03.109537: val_loss -0.4234
2024-05-03 19:27:03.110793: Pseudo dice [0.7224]
2024-05-03 19:27:03.111957: Epoch time: 64.4 s
2024-05-03 19:27:04.342529: 
2024-05-03 19:27:04.344508: Epoch 54
2024-05-03 19:27:04.345854: Current learning rate: 0.00951
2024-05-03 19:28:08.705034: Validation loss did not improve from -0.49572. Patience: 28/50
2024-05-03 19:28:08.706451: train_loss -0.7181
2024-05-03 19:28:08.707906: val_loss -0.4791
2024-05-03 19:28:08.709009: Pseudo dice [0.7388]
2024-05-03 19:28:08.710241: Epoch time: 64.37 s
2024-05-03 19:28:10.469305: 
2024-05-03 19:28:10.471265: Epoch 55
2024-05-03 19:28:10.472423: Current learning rate: 0.0095
2024-05-03 19:29:14.841041: Validation loss did not improve from -0.49572. Patience: 29/50
2024-05-03 19:29:14.842329: train_loss -0.708
2024-05-03 19:29:14.843929: val_loss -0.4922
2024-05-03 19:29:14.845464: Pseudo dice [0.7385]
2024-05-03 19:29:14.846915: Epoch time: 64.37 s
2024-05-03 19:29:16.060815: 
2024-05-03 19:29:16.063136: Epoch 56
2024-05-03 19:29:16.064715: Current learning rate: 0.00949
2024-05-03 19:30:20.440899: Validation loss did not improve from -0.49572. Patience: 30/50
2024-05-03 19:30:20.443322: train_loss -0.7162
2024-05-03 19:30:20.470926: val_loss -0.3527
2024-05-03 19:30:20.472977: Pseudo dice [0.696]
2024-05-03 19:30:20.474065: Epoch time: 64.38 s
2024-05-03 19:30:21.692200: 
2024-05-03 19:30:21.696783: Epoch 57
2024-05-03 19:30:21.698016: Current learning rate: 0.00949
2024-05-03 19:31:26.767085: Validation loss did not improve from -0.49572. Patience: 31/50
2024-05-03 19:31:26.768503: train_loss -0.7222
2024-05-03 19:31:26.769671: val_loss -0.393
2024-05-03 19:31:26.770894: Pseudo dice [0.7178]
2024-05-03 19:31:26.771928: Epoch time: 65.08 s
2024-05-03 19:31:27.984842: 
2024-05-03 19:31:27.987059: Epoch 58
2024-05-03 19:31:27.988395: Current learning rate: 0.00948
2024-05-03 19:32:33.160949: Validation loss did not improve from -0.49572. Patience: 32/50
2024-05-03 19:32:33.163515: train_loss -0.7127
2024-05-03 19:32:33.165396: val_loss -0.3984
2024-05-03 19:32:33.166433: Pseudo dice [0.7191]
2024-05-03 19:32:33.167639: Epoch time: 65.18 s
2024-05-03 19:32:34.432543: 
2024-05-03 19:32:34.434890: Epoch 59
2024-05-03 19:32:34.436145: Current learning rate: 0.00947
2024-05-03 19:33:38.980376: Validation loss did not improve from -0.49572. Patience: 33/50
2024-05-03 19:33:38.981766: train_loss -0.7189
2024-05-03 19:33:38.982990: val_loss -0.4512
2024-05-03 19:33:38.985332: Pseudo dice [0.744]
2024-05-03 19:33:38.986888: Epoch time: 64.55 s
2024-05-03 19:33:40.761749: 
2024-05-03 19:33:40.764490: Epoch 60
2024-05-03 19:33:40.765980: Current learning rate: 0.00946
2024-05-03 19:34:45.098871: Validation loss did not improve from -0.49572. Patience: 34/50
2024-05-03 19:34:45.105856: train_loss -0.7214
2024-05-03 19:34:45.107316: val_loss -0.4675
2024-05-03 19:34:45.108448: Pseudo dice [0.7444]
2024-05-03 19:34:45.109685: Epoch time: 64.34 s
2024-05-03 19:34:46.406640: 
2024-05-03 19:34:46.408920: Epoch 61
2024-05-03 19:34:46.410835: Current learning rate: 0.00945
2024-05-03 19:35:50.694852: Validation loss did not improve from -0.49572. Patience: 35/50
2024-05-03 19:35:50.696155: train_loss -0.7327
2024-05-03 19:35:50.697309: val_loss -0.4586
2024-05-03 19:35:50.698337: Pseudo dice [0.7402]
2024-05-03 19:35:50.699236: Epoch time: 64.29 s
2024-05-03 19:35:51.923705: 
2024-05-03 19:35:51.925500: Epoch 62
2024-05-03 19:35:51.926614: Current learning rate: 0.00944
2024-05-03 19:36:56.205174: Validation loss did not improve from -0.49572. Patience: 36/50
2024-05-03 19:36:56.206505: train_loss -0.7147
2024-05-03 19:36:56.207543: val_loss -0.4662
2024-05-03 19:36:56.208584: Pseudo dice [0.7339]
2024-05-03 19:36:56.209580: Epoch time: 64.28 s
2024-05-03 19:36:57.439490: 
2024-05-03 19:36:57.441527: Epoch 63
2024-05-03 19:36:57.442947: Current learning rate: 0.00943
2024-05-03 19:38:01.759692: Validation loss did not improve from -0.49572. Patience: 37/50
2024-05-03 19:38:01.761127: train_loss -0.7194
2024-05-03 19:38:01.762571: val_loss -0.4177
2024-05-03 19:38:01.763760: Pseudo dice [0.7424]
2024-05-03 19:38:01.764894: Epoch time: 64.32 s
2024-05-03 19:38:05.718588: 
2024-05-03 19:38:05.720851: Epoch 64
2024-05-03 19:38:05.721864: Current learning rate: 0.00942
2024-05-03 19:39:10.008908: Validation loss did not improve from -0.49572. Patience: 38/50
2024-05-03 19:39:10.010284: train_loss -0.7199
2024-05-03 19:39:10.011279: val_loss -0.4363
2024-05-03 19:39:10.012164: Pseudo dice [0.719]
2024-05-03 19:39:10.013119: Epoch time: 64.29 s
2024-05-03 19:39:11.618991: 
2024-05-03 19:39:11.621095: Epoch 65
2024-05-03 19:39:11.622234: Current learning rate: 0.00941
2024-05-03 19:40:15.927335: Validation loss did not improve from -0.49572. Patience: 39/50
2024-05-03 19:40:15.928569: train_loss -0.7161
2024-05-03 19:40:15.929909: val_loss -0.4497
2024-05-03 19:40:15.931025: Pseudo dice [0.7242]
2024-05-03 19:40:15.932200: Epoch time: 64.31 s
2024-05-03 19:40:17.172130: 
2024-05-03 19:40:17.174639: Epoch 66
2024-05-03 19:40:17.176013: Current learning rate: 0.0094
2024-05-03 19:41:21.518473: Validation loss did not improve from -0.49572. Patience: 40/50
2024-05-03 19:41:21.519772: train_loss -0.726
2024-05-03 19:41:21.521849: val_loss -0.4124
2024-05-03 19:41:21.523054: Pseudo dice [0.726]
2024-05-03 19:41:21.524197: Epoch time: 64.35 s
2024-05-03 19:41:22.760673: 
2024-05-03 19:41:22.762215: Epoch 67
2024-05-03 19:41:22.763618: Current learning rate: 0.00939
2024-05-03 19:42:27.038273: Validation loss did not improve from -0.49572. Patience: 41/50
2024-05-03 19:42:27.039751: train_loss -0.7331
2024-05-03 19:42:27.040871: val_loss -0.4369
2024-05-03 19:42:27.041915: Pseudo dice [0.7293]
2024-05-03 19:42:27.042867: Epoch time: 64.28 s
2024-05-03 19:42:28.312267: 
2024-05-03 19:42:28.313779: Epoch 68
2024-05-03 19:42:28.314758: Current learning rate: 0.00939
2024-05-03 19:43:32.573247: Validation loss did not improve from -0.49572. Patience: 42/50
2024-05-03 19:43:32.574585: train_loss -0.7328
2024-05-03 19:43:32.575693: val_loss -0.4297
2024-05-03 19:43:32.576791: Pseudo dice [0.7212]
2024-05-03 19:43:32.577877: Epoch time: 64.26 s
2024-05-03 19:43:33.847712: 
2024-05-03 19:43:33.849234: Epoch 69
2024-05-03 19:43:33.850499: Current learning rate: 0.00938
2024-05-03 19:44:38.075881: Validation loss did not improve from -0.49572. Patience: 43/50
2024-05-03 19:44:38.077205: train_loss -0.7398
2024-05-03 19:44:38.078441: val_loss -0.4738
2024-05-03 19:44:38.079353: Pseudo dice [0.746]
2024-05-03 19:44:38.080392: Epoch time: 64.23 s
2024-05-03 19:44:39.675622: 
2024-05-03 19:44:39.677710: Epoch 70
2024-05-03 19:44:39.678790: Current learning rate: 0.00937
2024-05-03 19:45:43.911405: Validation loss did not improve from -0.49572. Patience: 44/50
2024-05-03 19:45:43.912919: train_loss -0.7388
2024-05-03 19:45:43.914286: val_loss -0.4115
2024-05-03 19:45:43.960655: Pseudo dice [0.6945]
2024-05-03 19:45:43.961938: Epoch time: 64.24 s
2024-05-03 19:45:45.229536: 
2024-05-03 19:45:45.231535: Epoch 71
2024-05-03 19:45:45.232520: Current learning rate: 0.00936
2024-05-03 19:46:49.428134: Validation loss did not improve from -0.49572. Patience: 45/50
2024-05-03 19:46:49.429620: train_loss -0.739
2024-05-03 19:46:49.431106: val_loss -0.4457
2024-05-03 19:46:49.432157: Pseudo dice [0.726]
2024-05-03 19:46:49.433724: Epoch time: 64.2 s
2024-05-03 19:46:50.777143: 
2024-05-03 19:46:50.779886: Epoch 72
2024-05-03 19:46:50.781644: Current learning rate: 0.00935
2024-05-03 19:47:55.029124: Validation loss did not improve from -0.49572. Patience: 46/50
2024-05-03 19:47:55.030536: train_loss -0.7352
2024-05-03 19:47:55.031761: val_loss -0.3761
2024-05-03 19:47:55.032626: Pseudo dice [0.7193]
2024-05-03 19:47:55.033653: Epoch time: 64.26 s
2024-05-03 19:47:56.290622: 
2024-05-03 19:47:56.292253: Epoch 73
2024-05-03 19:47:56.293957: Current learning rate: 0.00934
2024-05-03 19:49:00.467276: Validation loss did not improve from -0.49572. Patience: 47/50
2024-05-03 19:49:00.469510: train_loss -0.7402
2024-05-03 19:49:00.470924: val_loss -0.438
2024-05-03 19:49:00.472155: Pseudo dice [0.742]
2024-05-03 19:49:00.473210: Epoch time: 64.18 s
2024-05-03 19:49:01.753371: 
2024-05-03 19:49:01.755317: Epoch 74
2024-05-03 19:49:01.756303: Current learning rate: 0.00933
2024-05-03 19:50:07.734241: Validation loss did not improve from -0.49572. Patience: 48/50
2024-05-03 19:50:07.735920: train_loss -0.7363
2024-05-03 19:50:07.737441: val_loss -0.4461
2024-05-03 19:50:07.738597: Pseudo dice [0.7423]
2024-05-03 19:50:07.739619: Epoch time: 65.98 s
2024-05-03 19:50:09.556746: 
2024-05-03 19:50:09.559074: Epoch 75
2024-05-03 19:50:09.561043: Current learning rate: 0.00932
2024-05-03 19:51:13.897879: Validation loss did not improve from -0.49572. Patience: 49/50
2024-05-03 19:51:13.899405: train_loss -0.7181
2024-05-03 19:51:13.900509: val_loss -0.4248
2024-05-03 19:51:13.901559: Pseudo dice [0.7279]
2024-05-03 19:51:13.902618: Epoch time: 64.34 s
2024-05-03 19:51:15.653414: 
2024-05-03 19:51:15.654892: Epoch 76
2024-05-03 19:51:15.655980: Current learning rate: 0.00931
2024-05-03 19:52:19.879390: Validation loss did not improve from -0.49572. Patience: 50/50
2024-05-03 19:52:19.880807: train_loss -0.7132
2024-05-03 19:52:19.881940: val_loss -0.4649
2024-05-03 19:52:19.883133: Pseudo dice [0.7521]
2024-05-03 19:52:19.884181: Epoch time: 64.23 s
2024-05-03 19:52:21.155008: Patience reached. Stopping training.
2024-05-03 19:52:21.536402: Training done.
2024-05-03 19:52:22.008428: Using splits from existing split file: /home/gridsan/nchutisilp/datasets/nnUNet_Datasets/nnUNet_preprocessed/Dataset302_Calcium_OCTv2/splits_final.json
2024-05-03 19:52:22.024577: The split file contains 3 splits.
2024-05-03 19:52:22.025832: Desired fold for training: 1
2024-05-03 19:52:22.026803: This split has 4 training and 2 validation cases.
2024-05-03 19:52:22.027937: predicting 101-044
2024-05-03 19:52:22.120624: 101-044, shape torch.Size([1, 404, 498, 498]), rank 0
2024-05-03 19:54:27.367182: predicting 106-002
2024-05-03 19:54:27.386508: 106-002, shape torch.Size([1, 539, 498, 498]), rank 0
2024-05-03 19:56:47.978996: Validation complete
2024-05-03 19:56:47.980488: Mean Validation Dice:  0.7205301678810374
wandb: 
wandb: Run history:
wandb:            ema_fg_dice ▁▁▂▂▃▄▄▅▅▅▆▆▆▆▇▇▇▇▇▇▇▇▇████▇▇▇▇▇████▇▇██
wandb:   epoch_end_timestamps ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███
wandb: epoch_start_timestamps ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███
wandb:                    lrs ████▇▇▇▇▇▆▆▆▆▆▆▅▅▅▅▅▅▄▄▄▄▄▃▃▃▃▃▂▂▂▂▂▂▁▁▁
wandb:           mean_fg_dice ▁▃▅▃▃█▄▇▆▅▇▇▅▅▅▆▆▅▄▇▆▇▆██▆▆▅▇▄▆▇▇▆▆▆▄▆▇█
wandb:           train_losses █▆▅▄▄▃▄▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▁▂▁▂▂▁▁▁▁▁▁▁▁▁▁▁▁
wandb:             val_losses ██▅█▇▃▅▂▂▇▄▂▄▄▄▂▃▄▅▁▂▃▃▁▁▄▃▄▁▇▅▂▂▃▄▄▄▆▃▂
wandb: 
wandb: Run summary:
wandb:            ema_fg_dice 0.73078
wandb:   epoch_end_timestamps 1714780339.88066
wandb: epoch_start_timestamps 1714780275.65223
wandb:                    lrs 0.00931
wandb:           mean_fg_dice 0.75205
wandb:           train_losses -0.7132
wandb:             val_losses -0.46493
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/gridsan/nchutisilp/datasets/nnUNet_Datasets/nnUNet_results/Dataset302_Calcium_OCTv2/nnUNetTrainer__nnUNetPlans__3d_fullres/fold_1/wandb/offline-run-20240503_182440-p1qaa0p0
wandb: Find logs at: /home/gridsan/nchutisilp/datasets/nnUNet_Datasets/nnUNet_results/Dataset302_Calcium_OCTv2/nnUNetTrainer__nnUNetPlans__3d_fullres/fold_1/wandb/offline-run-20240503_182440-p1qaa0p0/logs
WARNING:urllib3.connectionpool:Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x7fd33b496250>: Failed to establish a new connection: [Errno 101] Network is unreachable')': /api/4504800232407040/envelope/
WARNING:urllib3.connectionpool:Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x7fd272573a30>: Failed to establish a new connection: [Errno 101] Network is unreachable')': /api/4504800232407040/envelope/
WARNING:urllib3.connectionpool:Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x7fd2723de670>: Failed to establish a new connection: [Errno 101] Network is unreachable')': /api/4504800232407040/envelope/
WARNING:urllib3.connectionpool:Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x7fd2f05d07f0>: Failed to establish a new connection: [Errno 101] Network is unreachable')': /api/4504800232407040/envelope/
WARNING:urllib3.connectionpool:Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x7fd34a5a84f0>: Failed to establish a new connection: [Errno 101] Network is unreachable')': /api/4504800232407040/envelope/
WARNING:urllib3.connectionpool:Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x7fd34a5a8310>: Failed to establish a new connection: [Errno 101] Network is unreachable')': /api/4504800232407040/envelope/
FOLD 1 CONFIG 3d_fullres TRAINER nnUNetTrainer
usage: nnUNetv2_predict [-h] -i I -o O -d D [-p P] [-tr TR] -c C
                        [-f F [F ...]] [-step_size STEP_SIZE] [--disable_tta]
                        [--verbose] [--save_probabilities]
                        [--continue_prediction] [-chk CHK] [-npp NPP]
                        [-nps NPS]
                        [-prev_stage_predictions PREV_STAGE_PREDICTIONS]
                        [-num_parts NUM_PARTS] [-part_id PART_ID]
                        [-device DEVICE] [--disable_progress_bar]
nnUNetv2_predict: error: argument -d: expected one argument

#######################################################################
Please cite the following paper when using nnU-Net:
Isensee, F., Jaeger, P. F., Kohl, S. A., Petersen, J., & Maier-Hein, K. H. (2021). nnU-Net: a self-configuring method for deep learning-based biomedical image segmentation. Nature methods, 18(2), 203-211.
#######################################################################

Traceback (most recent call last):
  File "/home/gridsan/nchutisilp/.local/bin/nnUNetv2_evaluate_simple", line 8, in <module>
    sys.exit(evaluate_simple_entry_point())
  File "/home/gridsan/nchutisilp/projects/nnUNet/nnunetv2/evaluation/evaluate_predictions.py", line 251, in evaluate_simple_entry_point
    compute_metrics_on_folder_simple(args.gt_folder, args.pred_folder, args.l, args.o, args.np, args.il, chill=args.chill)
  File "/home/gridsan/nchutisilp/projects/nnUNet/nnunetv2/evaluation/evaluate_predictions.py", line 213, in compute_metrics_on_folder_simple
    compute_metrics_on_folder(folder_ref, folder_pred, output_file, rw, file_ending,
  File "/home/gridsan/nchutisilp/projects/nnUNet/nnunetv2/evaluation/evaluate_predictions.py", line 135, in compute_metrics_on_folder
    files_pred = subfiles(folder_pred, suffix=file_ending, join=False)
  File "/home/gridsan/nchutisilp/.local/lib/python3.9/site-packages/batchgenerators/utilities/file_and_folder_operations.py", line 40, in subfiles
    res = [l(folder, i) for i in os.listdir(folder) if os.path.isfile(os.path.join(folder, i))
FileNotFoundError: [Errno 2] No such file or directory: '/home/gridsan/nchutisilp/datasets/nnUNet_Datasets/nnUNet_results/Dataset302_Calcium_OCTv2/nnUNetTrainer__nnUNetPlans__3d_fullres/fold_1_finetuned_with_LaW_test'
Completed FOLD 1 CONFIG 3d_fullres TRAINER nnUNetTrainer
