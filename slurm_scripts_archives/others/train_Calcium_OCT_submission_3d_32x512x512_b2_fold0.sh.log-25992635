/home/gridsan/nchutisilp/.conda/envs/nnunet/bin/python
wandb: Tracking run with wandb version 0.16.6
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.

############################
INFO: You are using the old nnU-Net default plans. We have updated our recommendations. Please consider using those instead! Read more here: https://github.com/MIC-DKFZ/nnUNet/blob/master/documentation/resenc_presets.md
############################

Using device: cuda:0

#######################################################################
Please cite the following paper when using nnU-Net:
Isensee, F., Jaeger, P. F., Kohl, S. A., Petersen, J., & Maier-Hein, K. H. (2021). nnU-Net: a self-configuring method for deep learning-based biomedical image segmentation. Nature methods, 18(2), 203-211.
#######################################################################

2024-05-08 19:32:23.420133: Using torch.compile...
/home/gridsan/nchutisilp/.local/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn("The verbose parameter is deprecated. Please use get_last_lr() "
################### Loading pretrained weights from file  /home/gridsan/nchutisilp/datasets/nnUNet_Datasets/nnUNet_results/Dataset300_Lumen_and_Wall_OCT/nnUNetTrainer__nnUNetPreprocessPlans__3d_fullres/fold_all/checkpoint_best.pth ###################
Below is the list of overlapping blocks in pretrained model and nnUNet architecture:
encoder.stages.0.0.convs.0.conv.weight shape torch.Size([32, 1, 3, 3, 3])
encoder.stages.0.0.convs.0.conv.bias shape torch.Size([32])
encoder.stages.0.0.convs.0.norm.weight shape torch.Size([32])
encoder.stages.0.0.convs.0.norm.bias shape torch.Size([32])
encoder.stages.0.0.convs.0.all_modules.0.weight shape torch.Size([32, 1, 3, 3, 3])
encoder.stages.0.0.convs.0.all_modules.0.bias shape torch.Size([32])
encoder.stages.0.0.convs.0.all_modules.1.weight shape torch.Size([32])
encoder.stages.0.0.convs.0.all_modules.1.bias shape torch.Size([32])
encoder.stages.0.0.convs.1.conv.weight shape torch.Size([32, 32, 3, 3, 3])
encoder.stages.0.0.convs.1.conv.bias shape torch.Size([32])
encoder.stages.0.0.convs.1.norm.weight shape torch.Size([32])
encoder.stages.0.0.convs.1.norm.bias shape torch.Size([32])
encoder.stages.0.0.convs.1.all_modules.0.weight shape torch.Size([32, 32, 3, 3, 3])
encoder.stages.0.0.convs.1.all_modules.0.bias shape torch.Size([32])
encoder.stages.0.0.convs.1.all_modules.1.weight shape torch.Size([32])
encoder.stages.0.0.convs.1.all_modules.1.bias shape torch.Size([32])
encoder.stages.1.0.convs.0.conv.weight shape torch.Size([64, 32, 3, 3, 3])
encoder.stages.1.0.convs.0.conv.bias shape torch.Size([64])
encoder.stages.1.0.convs.0.norm.weight shape torch.Size([64])
encoder.stages.1.0.convs.0.norm.bias shape torch.Size([64])
encoder.stages.1.0.convs.0.all_modules.0.weight shape torch.Size([64, 32, 3, 3, 3])
encoder.stages.1.0.convs.0.all_modules.0.bias shape torch.Size([64])
encoder.stages.1.0.convs.0.all_modules.1.weight shape torch.Size([64])
encoder.stages.1.0.convs.0.all_modules.1.bias shape torch.Size([64])
encoder.stages.1.0.convs.1.conv.weight shape torch.Size([64, 64, 3, 3, 3])
encoder.stages.1.0.convs.1.conv.bias shape torch.Size([64])
encoder.stages.1.0.convs.1.norm.weight shape torch.Size([64])
encoder.stages.1.0.convs.1.norm.bias shape torch.Size([64])
encoder.stages.1.0.convs.1.all_modules.0.weight shape torch.Size([64, 64, 3, 3, 3])
encoder.stages.1.0.convs.1.all_modules.0.bias shape torch.Size([64])
encoder.stages.1.0.convs.1.all_modules.1.weight shape torch.Size([64])
encoder.stages.1.0.convs.1.all_modules.1.bias shape torch.Size([64])
encoder.stages.2.0.convs.0.conv.weight shape torch.Size([128, 64, 3, 3, 3])
encoder.stages.2.0.convs.0.conv.bias shape torch.Size([128])
encoder.stages.2.0.convs.0.norm.weight shape torch.Size([128])
encoder.stages.2.0.convs.0.norm.bias shape torch.Size([128])
encoder.stages.2.0.convs.0.all_modules.0.weight shape torch.Size([128, 64, 3, 3, 3])
encoder.stages.2.0.convs.0.all_modules.0.bias shape torch.Size([128])
encoder.stages.2.0.convs.0.all_modules.1.weight shape torch.Size([128])
encoder.stages.2.0.convs.0.all_modules.1.bias shape torch.Size([128])
encoder.stages.2.0.convs.1.conv.weight shape torch.Size([128, 128, 3, 3, 3])
encoder.stages.2.0.convs.1.conv.bias shape torch.Size([128])
encoder.stages.2.0.convs.1.norm.weight shape torch.Size([128])
encoder.stages.2.0.convs.1.norm.bias shape torch.Size([128])
encoder.stages.2.0.convs.1.all_modules.0.weight shape torch.Size([128, 128, 3, 3, 3])
encoder.stages.2.0.convs.1.all_modules.0.bias shape torch.Size([128])
encoder.stages.2.0.convs.1.all_modules.1.weight shape torch.Size([128])
encoder.stages.2.0.convs.1.all_modules.1.bias shape torch.Size([128])
encoder.stages.3.0.convs.0.conv.weight shape torch.Size([256, 128, 3, 3, 3])
encoder.stages.3.0.convs.0.conv.bias shape torch.Size([256])
encoder.stages.3.0.convs.0.norm.weight shape torch.Size([256])
encoder.stages.3.0.convs.0.norm.bias shape torch.Size([256])
encoder.stages.3.0.convs.0.all_modules.0.weight shape torch.Size([256, 128, 3, 3, 3])
encoder.stages.3.0.convs.0.all_modules.0.bias shape torch.Size([256])
encoder.stages.3.0.convs.0.all_modules.1.weight shape torch.Size([256])
encoder.stages.3.0.convs.0.all_modules.1.bias shape torch.Size([256])
encoder.stages.3.0.convs.1.conv.weight shape torch.Size([256, 256, 3, 3, 3])
encoder.stages.3.0.convs.1.conv.bias shape torch.Size([256])
encoder.stages.3.0.convs.1.norm.weight shape torch.Size([256])
encoder.stages.3.0.convs.1.norm.bias shape torch.Size([256])
encoder.stages.3.0.convs.1.all_modules.0.weight shape torch.Size([256, 256, 3, 3, 3])
encoder.stages.3.0.convs.1.all_modules.0.bias shape torch.Size([256])
encoder.stages.3.0.convs.1.all_modules.1.weight shape torch.Size([256])
encoder.stages.3.0.convs.1.all_modules.1.bias shape torch.Size([256])
encoder.stages.4.0.convs.0.conv.weight shape torch.Size([320, 256, 3, 3, 3])
encoder.stages.4.0.convs.0.conv.bias shape torch.Size([320])
encoder.stages.4.0.convs.0.norm.weight shape torch.Size([320])
encoder.stages.4.0.convs.0.norm.bias shape torch.Size([320])
encoder.stages.4.0.convs.0.all_modules.0.weight shape torch.Size([320, 256, 3, 3, 3])
encoder.stages.4.0.convs.0.all_modules.0.bias shape torch.Size([320])
encoder.stages.4.0.convs.0.all_modules.1.weight shape torch.Size([320])
encoder.stages.4.0.convs.0.all_modules.1.bias shape torch.Size([320])
encoder.stages.4.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.4.0.convs.1.conv.bias shape torch.Size([320])
encoder.stages.4.0.convs.1.norm.weight shape torch.Size([320])
encoder.stages.4.0.convs.1.norm.bias shape torch.Size([320])
encoder.stages.4.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.4.0.convs.1.all_modules.0.bias shape torch.Size([320])
encoder.stages.4.0.convs.1.all_modules.1.weight shape torch.Size([320])
encoder.stages.4.0.convs.1.all_modules.1.bias shape torch.Size([320])
encoder.stages.5.0.convs.0.conv.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.5.0.convs.0.conv.bias shape torch.Size([320])
encoder.stages.5.0.convs.0.norm.weight shape torch.Size([320])
encoder.stages.5.0.convs.0.norm.bias shape torch.Size([320])
encoder.stages.5.0.convs.0.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.5.0.convs.0.all_modules.0.bias shape torch.Size([320])
encoder.stages.5.0.convs.0.all_modules.1.weight shape torch.Size([320])
encoder.stages.5.0.convs.0.all_modules.1.bias shape torch.Size([320])
encoder.stages.5.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.5.0.convs.1.conv.bias shape torch.Size([320])
encoder.stages.5.0.convs.1.norm.weight shape torch.Size([320])
encoder.stages.5.0.convs.1.norm.bias shape torch.Size([320])
encoder.stages.5.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.5.0.convs.1.all_modules.0.bias shape torch.Size([320])
encoder.stages.5.0.convs.1.all_modules.1.weight shape torch.Size([320])
encoder.stages.5.0.convs.1.all_modules.1.bias shape torch.Size([320])
decoder.encoder.stages.0.0.convs.0.conv.weight shape torch.Size([32, 1, 3, 3, 3])
decoder.encoder.stages.0.0.convs.0.conv.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.norm.weight shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.norm.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.all_modules.0.weight shape torch.Size([32, 1, 3, 3, 3])
decoder.encoder.stages.0.0.convs.0.all_modules.0.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.all_modules.1.weight shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.all_modules.1.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.conv.weight shape torch.Size([32, 32, 3, 3, 3])
decoder.encoder.stages.0.0.convs.1.conv.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.norm.weight shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.norm.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.all_modules.0.weight shape torch.Size([32, 32, 3, 3, 3])
decoder.encoder.stages.0.0.convs.1.all_modules.0.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.all_modules.1.weight shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.all_modules.1.bias shape torch.Size([32])
decoder.encoder.stages.1.0.convs.0.conv.weight shape torch.Size([64, 32, 3, 3, 3])
decoder.encoder.stages.1.0.convs.0.conv.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.norm.weight shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.norm.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.all_modules.0.weight shape torch.Size([64, 32, 3, 3, 3])
decoder.encoder.stages.1.0.convs.0.all_modules.0.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.all_modules.1.weight shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.all_modules.1.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.conv.weight shape torch.Size([64, 64, 3, 3, 3])
decoder.encoder.stages.1.0.convs.1.conv.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.norm.weight shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.norm.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.all_modules.0.weight shape torch.Size([64, 64, 3, 3, 3])
decoder.encoder.stages.1.0.convs.1.all_modules.0.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.all_modules.1.weight shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.all_modules.1.bias shape torch.Size([64])
decoder.encoder.stages.2.0.convs.0.conv.weight shape torch.Size([128, 64, 3, 3, 3])
decoder.encoder.stages.2.0.convs.0.conv.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.norm.weight shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.norm.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.all_modules.0.weight shape torch.Size([128, 64, 3, 3, 3])
decoder.encoder.stages.2.0.convs.0.all_modules.0.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.all_modules.1.weight shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.all_modules.1.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.conv.weight shape torch.Size([128, 128, 3, 3, 3])
decoder.encoder.stages.2.0.convs.1.conv.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.norm.weight shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.norm.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.all_modules.0.weight shape torch.Size([128, 128, 3, 3, 3])
decoder.encoder.stages.2.0.convs.1.all_modules.0.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.all_modules.1.weight shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.all_modules.1.bias shape torch.Size([128])
decoder.encoder.stages.3.0.convs.0.conv.weight shape torch.Size([256, 128, 3, 3, 3])
decoder.encoder.stages.3.0.convs.0.conv.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.norm.weight shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.norm.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.all_modules.0.weight shape torch.Size([256, 128, 3, 3, 3])
decoder.encoder.stages.3.0.convs.0.all_modules.0.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.all_modules.1.weight shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.all_modules.1.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.conv.weight shape torch.Size([256, 256, 3, 3, 3])
decoder.encoder.stages.3.0.convs.1.conv.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.norm.weight shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.norm.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.all_modules.0.weight shape torch.Size([256, 256, 3, 3, 3])
decoder.encoder.stages.3.0.convs.1.all_modules.0.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.all_modules.1.weight shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.all_modules.1.bias shape torch.Size([256])
decoder.encoder.stages.4.0.convs.0.conv.weight shape torch.Size([320, 256, 3, 3, 3])
decoder.encoder.stages.4.0.convs.0.conv.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.norm.weight shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.norm.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.all_modules.0.weight shape torch.Size([320, 256, 3, 3, 3])
decoder.encoder.stages.4.0.convs.0.all_modules.0.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.all_modules.1.weight shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.all_modules.1.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.4.0.convs.1.conv.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.norm.weight shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.norm.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.4.0.convs.1.all_modules.0.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.all_modules.1.weight shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.all_modules.1.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.conv.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.5.0.convs.0.conv.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.norm.weight shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.norm.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.5.0.convs.0.all_modules.0.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.all_modules.1.weight shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.all_modules.1.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.5.0.convs.1.conv.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.norm.weight shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.norm.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.5.0.convs.1.all_modules.0.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.all_modules.1.weight shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.all_modules.1.bias shape torch.Size([320])
decoder.stages.0.convs.0.conv.weight shape torch.Size([320, 640, 3, 3, 3])
decoder.stages.0.convs.0.conv.bias shape torch.Size([320])
decoder.stages.0.convs.0.norm.weight shape torch.Size([320])
decoder.stages.0.convs.0.norm.bias shape torch.Size([320])
decoder.stages.0.convs.0.all_modules.0.weight shape torch.Size([320, 640, 3, 3, 3])
decoder.stages.0.convs.0.all_modules.0.bias shape torch.Size([320])
decoder.stages.0.convs.0.all_modules.1.weight shape torch.Size([320])
decoder.stages.0.convs.0.all_modules.1.bias shape torch.Size([320])
decoder.stages.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.stages.0.convs.1.conv.bias shape torch.Size([320])
decoder.stages.0.convs.1.norm.weight shape torch.Size([320])
decoder.stages.0.convs.1.norm.bias shape torch.Size([320])
decoder.stages.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.stages.0.convs.1.all_modules.0.bias shape torch.Size([320])
decoder.stages.0.convs.1.all_modules.1.weight shape torch.Size([320])
decoder.stages.0.convs.1.all_modules.1.bias shape torch.Size([320])
decoder.stages.1.convs.0.conv.weight shape torch.Size([256, 512, 3, 3, 3])
decoder.stages.1.convs.0.conv.bias shape torch.Size([256])
decoder.stages.1.convs.0.norm.weight shape torch.Size([256])
decoder.stages.1.convs.0.norm.bias shape torch.Size([256])
decoder.stages.1.convs.0.all_modules.0.weight shape torch.Size([256, 512, 3, 3, 3])
decoder.stages.1.convs.0.all_modules.0.bias shape torch.Size([256])
decoder.stages.1.convs.0.all_modules.1.weight shape torch.Size([256])
decoder.stages.1.convs.0.all_modules.1.bias shape torch.Size([256])
decoder.stages.1.convs.1.conv.weight shape torch.Size([256, 256, 3, 3, 3])
decoder.stages.1.convs.1.conv.bias shape torch.Size([256])
decoder.stages.1.convs.1.norm.weight shape torch.Size([256])
decoder.stages.1.convs.1.norm.bias shape torch.Size([256])
decoder.stages.1.convs.1.all_modules.0.weight shape torch.Size([256, 256, 3, 3, 3])
decoder.stages.1.convs.1.all_modules.0.bias shape torch.Size([256])
decoder.stages.1.convs.1.all_modules.1.weight shape torch.Size([256])
decoder.stages.1.convs.1.all_modules.1.bias shape torch.Size([256])
decoder.stages.2.convs.0.conv.weight shape torch.Size([128, 256, 3, 3, 3])
decoder.stages.2.convs.0.conv.bias shape torch.Size([128])
decoder.stages.2.convs.0.norm.weight shape torch.Size([128])
decoder.stages.2.convs.0.norm.bias shape torch.Size([128])
decoder.stages.2.convs.0.all_modules.0.weight shape torch.Size([128, 256, 3, 3, 3])
decoder.stages.2.convs.0.all_modules.0.bias shape torch.Size([128])
decoder.stages.2.convs.0.all_modules.1.weight shape torch.Size([128])
decoder.stages.2.convs.0.all_modules.1.bias shape torch.Size([128])
decoder.stages.2.convs.1.conv.weight shape torch.Size([128, 128, 3, 3, 3])
decoder.stages.2.convs.1.conv.bias shape torch.Size([128])
decoder.stages.2.convs.1.norm.weight shape torch.Size([128])
decoder.stages.2.convs.1.norm.bias shape torch.Size([128])
decoder.stages.2.convs.1.all_modules.0.weight shape torch.Size([128, 128, 3, 3, 3])
decoder.stages.2.convs.1.all_modules.0.bias shape torch.Size([128])
decoder.stages.2.convs.1.all_modules.1.weight shape torch.Size([128])
decoder.stages.2.convs.1.all_modules.1.bias shape torch.Size([128])
decoder.stages.3.convs.0.conv.weight shape torch.Size([64, 128, 3, 3, 3])
decoder.stages.3.convs.0.conv.bias shape torch.Size([64])
decoder.stages.3.convs.0.norm.weight shape torch.Size([64])
decoder.stages.3.convs.0.norm.bias shape torch.Size([64])
decoder.stages.3.convs.0.all_modules.0.weight shape torch.Size([64, 128, 3, 3, 3])
decoder.stages.3.convs.0.all_modules.0.bias shape torch.Size([64])
decoder.stages.3.convs.0.all_modules.1.weight shape torch.Size([64])
decoder.stages.3.convs.0.all_modules.1.bias shape torch.Size([64])
decoder.stages.3.convs.1.conv.weight shape torch.Size([64, 64, 3, 3, 3])
decoder.stages.3.convs.1.conv.bias shape torch.Size([64])
decoder.stages.3.convs.1.norm.weight shape torch.Size([64])
decoder.stages.3.convs.1.norm.bias shape torch.Size([64])
decoder.stages.3.convs.1.all_modules.0.weight shape torch.Size([64, 64, 3, 3, 3])
decoder.stages.3.convs.1.all_modules.0.bias shape torch.Size([64])
decoder.stages.3.convs.1.all_modules.1.weight shape torch.Size([64])
decoder.stages.3.convs.1.all_modules.1.bias shape torch.Size([64])
decoder.stages.4.convs.0.conv.weight shape torch.Size([32, 64, 3, 3, 3])
decoder.stages.4.convs.0.conv.bias shape torch.Size([32])
decoder.stages.4.convs.0.norm.weight shape torch.Size([32])
decoder.stages.4.convs.0.norm.bias shape torch.Size([32])
decoder.stages.4.convs.0.all_modules.0.weight shape torch.Size([32, 64, 3, 3, 3])
decoder.stages.4.convs.0.all_modules.0.bias shape torch.Size([32])
decoder.stages.4.convs.0.all_modules.1.weight shape torch.Size([32])
decoder.stages.4.convs.0.all_modules.1.bias shape torch.Size([32])
decoder.stages.4.convs.1.conv.weight shape torch.Size([32, 32, 3, 3, 3])
decoder.stages.4.convs.1.conv.bias shape torch.Size([32])
decoder.stages.4.convs.1.norm.weight shape torch.Size([32])
decoder.stages.4.convs.1.norm.bias shape torch.Size([32])
decoder.stages.4.convs.1.all_modules.0.weight shape torch.Size([32, 32, 3, 3, 3])
decoder.stages.4.convs.1.all_modules.0.bias shape torch.Size([32])
decoder.stages.4.convs.1.all_modules.1.weight shape torch.Size([32])
decoder.stages.4.convs.1.all_modules.1.bias shape torch.Size([32])
decoder.transpconvs.0.weight shape torch.Size([320, 320, 1, 2, 2])
decoder.transpconvs.0.bias shape torch.Size([320])
decoder.transpconvs.1.weight shape torch.Size([320, 256, 2, 2, 2])
decoder.transpconvs.1.bias shape torch.Size([256])
decoder.transpconvs.2.weight shape torch.Size([256, 128, 2, 2, 2])
decoder.transpconvs.2.bias shape torch.Size([128])
decoder.transpconvs.3.weight shape torch.Size([128, 64, 2, 2, 2])
decoder.transpconvs.3.bias shape torch.Size([64])
decoder.transpconvs.4.weight shape torch.Size([64, 32, 2, 2, 2])
decoder.transpconvs.4.bias shape torch.Size([32])
################### Done ###################
2024-05-08 19:32:47.215218: do_dummy_2d_data_aug: True
2024-05-08 19:32:47.218701: Using splits from existing split file: /home/gridsan/nchutisilp/datasets/nnUNet_Datasets/nnUNet_preprocessed/Dataset302_Calcium_OCTv2/splits_final.json
2024-05-08 19:32:47.221583: The split file contains 3 splits.
2024-05-08 19:32:47.222646: Desired fold for training: 1
2024-05-08 19:32:47.223993: This split has 4 training and 2 validation cases.
using pin_memory on device 0
using pin_memory on device 0

This is the configuration used by this training:
Configuration name: 3d_32x512x512_b2
 {'data_identifier': 'nnUNetPlans_3d_fullres', 'preprocessor_name': 'DefaultPreprocessor', 'batch_size': 2, 'patch_size': [32, 512, 512], 'median_image_size_in_voxels': [375.0, 498.0, 498.0], 'spacing': [1.0, 1.0, 1.0], 'normalization_schemes': ['ZScoreNormalization'], 'use_mask_for_norm': [False], 'resampling_fn_data': 'resample_data_or_seg_to_shape', 'resampling_fn_seg': 'resample_data_or_seg_to_shape', 'resampling_fn_data_kwargs': {'is_seg': False, 'order': 3, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_seg_kwargs': {'is_seg': True, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_probabilities': 'resample_data_or_seg_to_shape', 'resampling_fn_probabilities_kwargs': {'is_seg': False, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'architecture': {'network_class_name': 'dynamic_network_architectures.architectures.unet.PlainConvUNet', 'arch_kwargs': {'n_stages': 6, 'features_per_stage': [32, 64, 128, 256, 320, 320], 'conv_op': 'torch.nn.modules.conv.Conv3d', 'kernel_sizes': [[3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3]], 'strides': [[1, 1, 1], [2, 2, 2], [2, 2, 2], [2, 2, 2], [2, 2, 2], [1, 2, 2]], 'n_conv_per_stage': [2, 2, 2, 2, 2, 2], 'n_conv_per_stage_decoder': [2, 2, 2, 2, 2], 'conv_bias': True, 'norm_op': 'torch.nn.modules.instancenorm.InstanceNorm3d', 'norm_op_kwargs': {'eps': 1e-05, 'affine': True}, 'dropout_op': None, 'dropout_op_kwargs': None, 'nonlin': 'torch.nn.LeakyReLU', 'nonlin_kwargs': {'inplace': True}, 'deep_supervision': True}, '_kw_requires_import': ['conv_op', 'norm_op', 'dropout_op', 'nonlin']}, 'batch_dice': True, 'inherits_from': '3d_fullres'} 

These are the global plan.json settings:
 {'dataset_name': 'Dataset302_Calcium_OCTv2', 'plans_name': 'nnUNetPlans', 'original_median_spacing_after_transp': [1.0, 1.0, 1.0], 'original_median_shape_after_transp': [375, 498, 498], 'image_reader_writer': 'SimpleITKIO', 'transpose_forward': [0, 1, 2], 'transpose_backward': [0, 1, 2], 'experiment_planner_used': 'ExperimentPlanner', 'label_manager': 'LabelManager', 'foreground_intensity_properties_per_channel': {'0': {'max': 0.4977976381778717, 'mean': 0.13332499563694, 'median': 0.08871842920780182, 'min': 0.0, 'percentile_00_5': 0.014754901640117168, 'percentile_99_5': 0.4977976381778717, 'std': 0.12022686749696732}}} 

2024-05-08 19:33:12.142802: unpacking dataset...
2024-05-08 19:33:17.363236: unpacking done...
2024-05-08 19:33:17.428336: Unable to plot network architecture: nnUNet_compile is enabled!
2024-05-08 19:33:17.547693: 
2024-05-08 19:33:17.549274: Epoch 0
2024-05-08 19:33:17.550553: Current learning rate: 0.01
2024-05-08 19:40:15.466650: Validation loss improved from 1000.00000 to -0.48869! Patience: 0/50
2024-05-08 19:40:15.468147: train_loss -0.4165
2024-05-08 19:40:15.469470: val_loss -0.4887
2024-05-08 19:40:15.470564: Pseudo dice [0.6578]
2024-05-08 19:40:15.471665: Epoch time: 417.92 s
2024-05-08 19:40:15.472883: Yayy! New best EMA pseudo Dice: 0.6578
2024-05-08 19:40:19.925755: 
2024-05-08 19:40:19.927530: Epoch 1
2024-05-08 19:40:19.928627: Current learning rate: 0.00999
2024-05-08 19:44:34.480515: Validation loss improved from -0.48869 to -0.56579! Patience: 0/50
2024-05-08 19:44:34.483517: train_loss -0.5788
2024-05-08 19:44:34.485141: val_loss -0.5658
2024-05-08 19:44:34.486432: Pseudo dice [0.7151]
2024-05-08 19:44:34.487646: Epoch time: 254.56 s
2024-05-08 19:44:34.488772: Yayy! New best EMA pseudo Dice: 0.6635
2024-05-08 19:44:36.323805: 
2024-05-08 19:44:36.326332: Epoch 2
2024-05-08 19:44:36.327660: Current learning rate: 0.00998
2024-05-08 19:48:42.241680: Validation loss improved from -0.56579 to -0.57102! Patience: 0/50
2024-05-08 19:48:42.244790: train_loss -0.622
2024-05-08 19:48:42.246335: val_loss -0.571
2024-05-08 19:48:42.247501: Pseudo dice [0.7209]
2024-05-08 19:48:42.248764: Epoch time: 245.92 s
2024-05-08 19:48:42.249810: Yayy! New best EMA pseudo Dice: 0.6692
2024-05-08 19:48:44.072744: 
2024-05-08 19:48:44.074219: Epoch 3
2024-05-08 19:48:44.075699: Current learning rate: 0.00997
2024-05-08 19:52:49.214263: Validation loss improved from -0.57102 to -0.59115! Patience: 0/50
2024-05-08 19:52:49.216331: train_loss -0.6673
2024-05-08 19:52:49.218254: val_loss -0.5912
2024-05-08 19:52:49.219422: Pseudo dice [0.7344]
2024-05-08 19:52:49.220430: Epoch time: 245.15 s
2024-05-08 19:52:49.221463: Yayy! New best EMA pseudo Dice: 0.6758
2024-05-08 19:52:50.921448: 
2024-05-08 19:52:50.923343: Epoch 4
2024-05-08 19:52:50.924496: Current learning rate: 0.00996
2024-05-08 19:56:59.548076: Validation loss did not improve from -0.59115. Patience: 1/50
2024-05-08 19:56:59.550337: train_loss -0.6615
2024-05-08 19:56:59.551619: val_loss -0.576
2024-05-08 19:56:59.552617: Pseudo dice [0.7218]
2024-05-08 19:56:59.553711: Epoch time: 248.63 s
2024-05-08 19:56:59.949703: Yayy! New best EMA pseudo Dice: 0.6804
2024-05-08 19:57:01.808882: 
2024-05-08 19:57:01.810651: Epoch 5
2024-05-08 19:57:01.811605: Current learning rate: 0.00995
2024-05-08 20:00:58.037855: Validation loss did not improve from -0.59115. Patience: 2/50
2024-05-08 20:00:58.039435: train_loss -0.6688
2024-05-08 20:00:58.040571: val_loss -0.5837
2024-05-08 20:00:58.041550: Pseudo dice [0.7316]
2024-05-08 20:00:58.042683: Epoch time: 236.23 s
2024-05-08 20:00:58.043633: Yayy! New best EMA pseudo Dice: 0.6855
2024-05-08 20:00:59.678399: 
2024-05-08 20:00:59.681149: Epoch 6
2024-05-08 20:00:59.683024: Current learning rate: 0.00995
2024-05-08 20:05:11.432598: Validation loss did not improve from -0.59115. Patience: 3/50
2024-05-08 20:05:11.434111: train_loss -0.6898
2024-05-08 20:05:11.435555: val_loss -0.554
2024-05-08 20:05:11.436737: Pseudo dice [0.7121]
2024-05-08 20:05:11.438014: Epoch time: 251.76 s
2024-05-08 20:05:11.439222: Yayy! New best EMA pseudo Dice: 0.6881
2024-05-08 20:05:13.196090: 
2024-05-08 20:05:13.197680: Epoch 7
2024-05-08 20:05:13.198786: Current learning rate: 0.00994
2024-05-08 20:09:14.133070: Validation loss did not improve from -0.59115. Patience: 4/50
2024-05-08 20:09:14.134380: train_loss -0.7165
2024-05-08 20:09:14.136148: val_loss -0.586
2024-05-08 20:09:14.137694: Pseudo dice [0.7407]
2024-05-08 20:09:14.139124: Epoch time: 240.94 s
2024-05-08 20:09:14.140515: Yayy! New best EMA pseudo Dice: 0.6934
2024-05-08 20:09:16.309525: 
2024-05-08 20:09:16.313087: Epoch 8
2024-05-08 20:09:16.315065: Current learning rate: 0.00993
2024-05-08 20:13:17.111214: Validation loss improved from -0.59115 to -0.60162! Patience: 4/50
2024-05-08 20:13:17.113053: train_loss -0.7203
2024-05-08 20:13:17.114555: val_loss -0.6016
2024-05-08 20:13:17.115742: Pseudo dice [0.7359]
2024-05-08 20:13:17.116951: Epoch time: 240.8 s
2024-05-08 20:13:17.117969: Yayy! New best EMA pseudo Dice: 0.6977
2024-05-08 20:13:18.770237: 
2024-05-08 20:13:18.772811: Epoch 9
2024-05-08 20:13:18.774538: Current learning rate: 0.00992
2024-05-08 20:17:16.067983: Validation loss improved from -0.60162 to -0.61567! Patience: 0/50
2024-05-08 20:17:16.070168: train_loss -0.7227
2024-05-08 20:17:16.072001: val_loss -0.6157
2024-05-08 20:17:16.073331: Pseudo dice [0.731]
2024-05-08 20:17:16.074550: Epoch time: 237.3 s
2024-05-08 20:17:16.444241: Yayy! New best EMA pseudo Dice: 0.701
2024-05-08 20:17:18.035019: 
2024-05-08 20:17:18.037961: Epoch 10
2024-05-08 20:17:18.039276: Current learning rate: 0.00991
2024-05-08 20:21:16.148360: Validation loss did not improve from -0.61567. Patience: 1/50
2024-05-08 20:21:16.149929: train_loss -0.73
2024-05-08 20:21:16.151129: val_loss -0.5849
2024-05-08 20:21:16.152122: Pseudo dice [0.7295]
2024-05-08 20:21:16.153165: Epoch time: 238.12 s
2024-05-08 20:21:16.154254: Yayy! New best EMA pseudo Dice: 0.7038
2024-05-08 20:21:17.696405: 
2024-05-08 20:21:17.698551: Epoch 11
2024-05-08 20:21:17.699771: Current learning rate: 0.0099
2024-05-08 20:25:16.622869: Validation loss improved from -0.61567 to -0.61733! Patience: 1/50
2024-05-08 20:25:16.624624: train_loss -0.7218
2024-05-08 20:25:16.625752: val_loss -0.6173
2024-05-08 20:25:16.626688: Pseudo dice [0.7547]
2024-05-08 20:25:16.627786: Epoch time: 238.93 s
2024-05-08 20:25:16.628739: Yayy! New best EMA pseudo Dice: 0.7089
2024-05-08 20:25:18.383856: 
2024-05-08 20:25:18.385233: Epoch 12
2024-05-08 20:25:18.386240: Current learning rate: 0.00989
2024-05-08 20:29:14.863753: Validation loss did not improve from -0.61733. Patience: 1/50
2024-05-08 20:29:14.865696: train_loss -0.7396
2024-05-08 20:29:14.867165: val_loss -0.5529
2024-05-08 20:29:14.868190: Pseudo dice [0.7111]
2024-05-08 20:29:14.869189: Epoch time: 236.48 s
2024-05-08 20:29:14.870353: Yayy! New best EMA pseudo Dice: 0.7091
2024-05-08 20:29:16.474463: 
2024-05-08 20:29:16.476511: Epoch 13
2024-05-08 20:29:16.477955: Current learning rate: 0.00988
2024-05-08 20:33:27.166199: Validation loss did not improve from -0.61733. Patience: 2/50
2024-05-08 20:33:27.167711: train_loss -0.7504
2024-05-08 20:33:27.169224: val_loss -0.606
2024-05-08 20:33:27.170464: Pseudo dice [0.7535]
2024-05-08 20:33:27.171752: Epoch time: 250.69 s
2024-05-08 20:33:27.172893: Yayy! New best EMA pseudo Dice: 0.7136
2024-05-08 20:33:29.033137: 
2024-05-08 20:33:29.034927: Epoch 14
2024-05-08 20:33:29.036181: Current learning rate: 0.00987
2024-05-08 20:37:26.072971: Validation loss improved from -0.61733 to -0.62282! Patience: 2/50
2024-05-08 20:37:26.075175: train_loss -0.758
2024-05-08 20:37:26.077318: val_loss -0.6228
2024-05-08 20:37:26.078647: Pseudo dice [0.7535]
2024-05-08 20:37:26.079808: Epoch time: 237.04 s
2024-05-08 20:37:26.425242: Yayy! New best EMA pseudo Dice: 0.7176
2024-05-08 20:37:28.144479: 
2024-05-08 20:37:28.145988: Epoch 15
2024-05-08 20:37:28.147114: Current learning rate: 0.00986
2024-05-08 20:41:32.776744: Validation loss did not improve from -0.62282. Patience: 1/50
2024-05-08 20:41:32.779138: train_loss -0.7564
2024-05-08 20:41:32.781676: val_loss -0.5533
2024-05-08 20:41:32.783092: Pseudo dice [0.7139]
2024-05-08 20:41:32.784368: Epoch time: 244.64 s
2024-05-08 20:41:34.283615: 
2024-05-08 20:41:34.286151: Epoch 16
2024-05-08 20:41:34.287586: Current learning rate: 0.00986
2024-05-08 20:45:32.365296: Validation loss did not improve from -0.62282. Patience: 2/50
2024-05-08 20:45:32.366900: train_loss -0.7406
2024-05-08 20:45:32.368512: val_loss -0.6077
2024-05-08 20:45:32.370450: Pseudo dice [0.7477]
2024-05-08 20:45:32.372291: Epoch time: 238.08 s
2024-05-08 20:45:32.373465: Yayy! New best EMA pseudo Dice: 0.7202
2024-05-08 20:45:34.136347: 
2024-05-08 20:45:34.139232: Epoch 17
2024-05-08 20:45:34.140735: Current learning rate: 0.00985
2024-05-08 20:49:32.091102: Validation loss did not improve from -0.62282. Patience: 3/50
2024-05-08 20:49:32.092494: train_loss -0.754
2024-05-08 20:49:32.093692: val_loss -0.6138
2024-05-08 20:49:32.094711: Pseudo dice [0.753]
2024-05-08 20:49:32.095802: Epoch time: 237.96 s
2024-05-08 20:49:32.096743: Yayy! New best EMA pseudo Dice: 0.7235
2024-05-08 20:49:33.776311: 
2024-05-08 20:49:33.778691: Epoch 18
2024-05-08 20:49:33.779835: Current learning rate: 0.00984
2024-05-08 20:53:31.041956: Validation loss did not improve from -0.62282. Patience: 4/50
2024-05-08 20:53:31.043554: train_loss -0.7566
2024-05-08 20:53:31.044918: val_loss -0.5722
2024-05-08 20:53:31.046119: Pseudo dice [0.7264]
2024-05-08 20:53:31.047253: Epoch time: 237.27 s
2024-05-08 20:53:31.048377: Yayy! New best EMA pseudo Dice: 0.7238
2024-05-08 20:53:39.127776: 
2024-05-08 20:53:39.130750: Epoch 19
2024-05-08 20:53:39.132424: Current learning rate: 0.00983
2024-05-08 20:57:38.902035: Validation loss did not improve from -0.62282. Patience: 5/50
2024-05-08 20:57:38.903694: train_loss -0.7491
2024-05-08 20:57:38.904799: val_loss -0.5766
2024-05-08 20:57:38.905806: Pseudo dice [0.7174]
2024-05-08 20:57:38.906802: Epoch time: 239.78 s
2024-05-08 20:57:40.678999: 
2024-05-08 20:57:40.680101: Epoch 20
2024-05-08 20:57:40.681408: Current learning rate: 0.00982
2024-05-08 21:01:38.228951: Validation loss did not improve from -0.62282. Patience: 6/50
2024-05-08 21:01:38.230330: train_loss -0.7385
2024-05-08 21:01:38.231632: val_loss -0.5374
2024-05-08 21:01:38.232651: Pseudo dice [0.7122]
2024-05-08 21:01:38.233737: Epoch time: 237.55 s
2024-05-08 21:01:39.605235: 
2024-05-08 21:01:39.607316: Epoch 21
2024-05-08 21:01:39.608720: Current learning rate: 0.00981
2024-05-08 21:05:37.031699: Validation loss did not improve from -0.62282. Patience: 7/50
2024-05-08 21:05:37.034798: train_loss -0.755
2024-05-08 21:05:37.036588: val_loss -0.5497
2024-05-08 21:05:37.037848: Pseudo dice [0.7136]
2024-05-08 21:05:37.039031: Epoch time: 237.43 s
2024-05-08 21:05:38.294396: 
2024-05-08 21:05:38.296971: Epoch 22
2024-05-08 21:05:38.298299: Current learning rate: 0.0098
2024-05-08 21:09:37.265979: Validation loss did not improve from -0.62282. Patience: 8/50
2024-05-08 21:09:37.268225: train_loss -0.7313
2024-05-08 21:09:37.270127: val_loss -0.5866
2024-05-08 21:09:37.271578: Pseudo dice [0.7396]
2024-05-08 21:09:37.272654: Epoch time: 238.98 s
2024-05-08 21:09:38.517085: 
2024-05-08 21:09:38.518973: Epoch 23
2024-05-08 21:09:38.520257: Current learning rate: 0.00979
2024-05-08 21:13:38.958451: Validation loss did not improve from -0.62282. Patience: 9/50
2024-05-08 21:13:38.961112: train_loss -0.7523
2024-05-08 21:13:38.962660: val_loss -0.6161
2024-05-08 21:13:38.963822: Pseudo dice [0.7472]
2024-05-08 21:13:38.964965: Epoch time: 240.45 s
2024-05-08 21:13:38.965847: Yayy! New best EMA pseudo Dice: 0.7255
2024-05-08 21:13:40.615475: 
2024-05-08 21:13:40.618018: Epoch 24
2024-05-08 21:13:40.619542: Current learning rate: 0.00978
2024-05-08 21:17:40.580949: Validation loss did not improve from -0.62282. Patience: 10/50
2024-05-08 21:17:40.582936: train_loss -0.761
2024-05-08 21:17:40.584902: val_loss -0.5536
2024-05-08 21:17:40.586309: Pseudo dice [0.7162]
2024-05-08 21:17:40.587556: Epoch time: 239.97 s
2024-05-08 21:17:42.335730: 
2024-05-08 21:17:42.338406: Epoch 25
2024-05-08 21:17:42.340183: Current learning rate: 0.00977
2024-05-08 21:21:39.558680: Validation loss did not improve from -0.62282. Patience: 11/50
2024-05-08 21:21:39.560078: train_loss -0.7733
2024-05-08 21:21:39.561357: val_loss -0.5831
2024-05-08 21:21:39.562537: Pseudo dice [0.7365]
2024-05-08 21:21:39.563623: Epoch time: 237.23 s
2024-05-08 21:21:39.564473: Yayy! New best EMA pseudo Dice: 0.7257
2024-05-08 21:21:41.144063: 
2024-05-08 21:21:41.146075: Epoch 26
2024-05-08 21:21:41.147409: Current learning rate: 0.00977
2024-05-08 21:25:38.489722: Validation loss did not improve from -0.62282. Patience: 12/50
2024-05-08 21:25:38.491518: train_loss -0.7778
2024-05-08 21:25:38.492891: val_loss -0.5839
2024-05-08 21:25:38.494108: Pseudo dice [0.7338]
2024-05-08 21:25:38.495281: Epoch time: 237.35 s
2024-05-08 21:25:38.496531: Yayy! New best EMA pseudo Dice: 0.7265
2024-05-08 21:25:44.031205: 
2024-05-08 21:25:44.033751: Epoch 27
2024-05-08 21:25:44.035265: Current learning rate: 0.00976
2024-05-08 21:29:41.441541: Validation loss did not improve from -0.62282. Patience: 13/50
2024-05-08 21:29:41.443674: train_loss -0.7744
2024-05-08 21:29:41.445303: val_loss -0.5552
2024-05-08 21:29:41.446799: Pseudo dice [0.7051]
2024-05-08 21:29:41.448155: Epoch time: 237.41 s
2024-05-08 21:29:42.678764: 
2024-05-08 21:29:42.681295: Epoch 28
2024-05-08 21:29:42.682599: Current learning rate: 0.00975
2024-05-08 21:33:39.654756: Validation loss did not improve from -0.62282. Patience: 14/50
2024-05-08 21:33:39.656301: train_loss -0.7782
2024-05-08 21:33:39.657636: val_loss -0.5799
2024-05-08 21:33:39.658787: Pseudo dice [0.7256]
2024-05-08 21:33:39.659790: Epoch time: 236.98 s
2024-05-08 21:33:40.948355: 
2024-05-08 21:33:40.950014: Epoch 29
2024-05-08 21:33:40.951523: Current learning rate: 0.00974
2024-05-08 21:37:47.903406: Validation loss did not improve from -0.62282. Patience: 15/50
2024-05-08 21:37:47.959805: train_loss -0.7805
2024-05-08 21:37:47.962567: val_loss -0.5668
2024-05-08 21:37:47.964224: Pseudo dice [0.7256]
2024-05-08 21:37:47.965707: Epoch time: 247.01 s
2024-05-08 21:37:54.869008: 
2024-05-08 21:37:54.871091: Epoch 30
2024-05-08 21:37:54.872327: Current learning rate: 0.00973
2024-05-08 21:41:52.300796: Validation loss improved from -0.62282 to -0.62319! Patience: 15/50
2024-05-08 21:41:52.302724: train_loss -0.7839
2024-05-08 21:41:52.304467: val_loss -0.6232
2024-05-08 21:41:52.306042: Pseudo dice [0.7615]
2024-05-08 21:41:52.307308: Epoch time: 237.44 s
2024-05-08 21:41:52.308581: Yayy! New best EMA pseudo Dice: 0.7283
2024-05-08 21:41:53.884326: 
2024-05-08 21:41:53.886940: Epoch 31
2024-05-08 21:41:53.888459: Current learning rate: 0.00972
2024-05-08 21:45:51.370802: Validation loss did not improve from -0.62319. Patience: 1/50
2024-05-08 21:45:51.374444: train_loss -0.7824
2024-05-08 21:45:51.376024: val_loss -0.6124
2024-05-08 21:45:51.377164: Pseudo dice [0.7476]
2024-05-08 21:45:51.378377: Epoch time: 237.49 s
2024-05-08 21:45:51.379498: Yayy! New best EMA pseudo Dice: 0.7302
2024-05-08 21:45:53.059568: 
2024-05-08 21:45:53.061973: Epoch 32
2024-05-08 21:45:53.063422: Current learning rate: 0.00971
2024-05-08 21:49:49.328637: Validation loss did not improve from -0.62319. Patience: 2/50
2024-05-08 21:49:49.330676: train_loss -0.781
2024-05-08 21:49:49.332408: val_loss -0.5669
2024-05-08 21:49:49.333817: Pseudo dice [0.7263]
2024-05-08 21:49:49.335424: Epoch time: 236.27 s
2024-05-08 21:49:50.645016: 
2024-05-08 21:49:50.647421: Epoch 33
2024-05-08 21:49:50.648912: Current learning rate: 0.0097
2024-05-08 21:53:49.660198: Validation loss did not improve from -0.62319. Patience: 3/50
2024-05-08 21:53:49.661688: train_loss -0.7689
2024-05-08 21:53:49.662912: val_loss -0.4535
2024-05-08 21:53:49.663937: Pseudo dice [0.6749]
2024-05-08 21:53:49.664984: Epoch time: 239.02 s
2024-05-08 21:53:50.973682: 
2024-05-08 21:53:50.975522: Epoch 34
2024-05-08 21:53:50.976711: Current learning rate: 0.00969
2024-05-08 21:57:48.125059: Validation loss did not improve from -0.62319. Patience: 4/50
2024-05-08 21:57:48.126620: train_loss -0.7686
2024-05-08 21:57:48.127943: val_loss -0.5724
2024-05-08 21:57:48.129110: Pseudo dice [0.7284]
2024-05-08 21:57:48.130183: Epoch time: 237.15 s
2024-05-08 21:57:50.282434: 
2024-05-08 21:57:50.284612: Epoch 35
2024-05-08 21:57:50.286175: Current learning rate: 0.00968
2024-05-08 22:01:46.596780: Validation loss did not improve from -0.62319. Patience: 5/50
2024-05-08 22:01:46.598393: train_loss -0.7857
2024-05-08 22:01:46.599679: val_loss -0.611
2024-05-08 22:01:46.600784: Pseudo dice [0.7449]
2024-05-08 22:01:46.601719: Epoch time: 236.32 s
2024-05-08 22:01:47.905348: 
2024-05-08 22:01:47.907279: Epoch 36
2024-05-08 22:01:47.908525: Current learning rate: 0.00968
2024-05-08 22:05:45.546428: Validation loss did not improve from -0.62319. Patience: 6/50
2024-05-08 22:05:45.548462: train_loss -0.784
2024-05-08 22:05:45.549787: val_loss -0.5928
2024-05-08 22:05:45.550706: Pseudo dice [0.7493]
2024-05-08 22:05:45.551669: Epoch time: 237.64 s
2024-05-08 22:05:46.893380: 
2024-05-08 22:05:46.896442: Epoch 37
2024-05-08 22:05:46.897710: Current learning rate: 0.00967
2024-05-08 22:09:44.357905: Validation loss did not improve from -0.62319. Patience: 7/50
2024-05-08 22:09:44.360010: train_loss -0.7961
2024-05-08 22:09:44.362241: val_loss -0.6015
2024-05-08 22:09:44.363789: Pseudo dice [0.7461]
2024-05-08 22:09:44.365117: Epoch time: 237.47 s
2024-05-08 22:09:44.366111: Yayy! New best EMA pseudo Dice: 0.7307
2024-05-08 22:09:46.071140: 
2024-05-08 22:09:46.073261: Epoch 38
2024-05-08 22:09:46.074671: Current learning rate: 0.00966
2024-05-08 22:13:42.886620: Validation loss did not improve from -0.62319. Patience: 8/50
2024-05-08 22:13:42.888104: train_loss -0.8006
2024-05-08 22:13:42.889268: val_loss -0.5675
2024-05-08 22:13:42.890303: Pseudo dice [0.7242]
2024-05-08 22:13:42.891324: Epoch time: 236.82 s
2024-05-08 22:13:44.248284: 
2024-05-08 22:13:44.250435: Epoch 39
2024-05-08 22:13:44.251646: Current learning rate: 0.00965
2024-05-08 22:17:41.683263: Validation loss did not improve from -0.62319. Patience: 9/50
2024-05-08 22:17:41.685433: train_loss -0.796
2024-05-08 22:17:41.686551: val_loss -0.5825
2024-05-08 22:17:41.687614: Pseudo dice [0.7404]
2024-05-08 22:17:41.688568: Epoch time: 237.44 s
2024-05-08 22:17:42.107262: Yayy! New best EMA pseudo Dice: 0.7311
2024-05-08 22:17:43.888690: 
2024-05-08 22:17:43.890524: Epoch 40
2024-05-08 22:17:43.891648: Current learning rate: 0.00964
2024-05-08 22:21:39.168125: Validation loss improved from -0.62319 to -0.65368! Patience: 9/50
2024-05-08 22:21:39.169645: train_loss -0.7984
2024-05-08 22:21:39.170971: val_loss -0.6537
2024-05-08 22:21:39.172136: Pseudo dice [0.7705]
2024-05-08 22:21:39.173145: Epoch time: 235.28 s
2024-05-08 22:21:39.174141: Yayy! New best EMA pseudo Dice: 0.735
2024-05-08 22:21:42.175355: 
2024-05-08 22:21:42.226357: Epoch 41
2024-05-08 22:21:42.228021: Current learning rate: 0.00963
2024-05-08 22:25:38.410686: Validation loss did not improve from -0.65368. Patience: 1/50
2024-05-08 22:25:38.412308: train_loss -0.7994
2024-05-08 22:25:38.415133: val_loss -0.55
2024-05-08 22:25:38.416805: Pseudo dice [0.7221]
2024-05-08 22:25:38.417906: Epoch time: 236.24 s
2024-05-08 22:25:39.706549: 
2024-05-08 22:25:39.708506: Epoch 42
2024-05-08 22:25:39.709647: Current learning rate: 0.00962
2024-05-08 22:29:35.294467: Validation loss did not improve from -0.65368. Patience: 2/50
2024-05-08 22:29:35.295837: train_loss -0.7969
2024-05-08 22:29:35.296985: val_loss -0.6109
2024-05-08 22:29:35.298066: Pseudo dice [0.7482]
2024-05-08 22:29:35.298991: Epoch time: 235.59 s
2024-05-08 22:29:35.299825: Yayy! New best EMA pseudo Dice: 0.7352
2024-05-08 22:29:37.253214: 
2024-05-08 22:29:37.256068: Epoch 43
2024-05-08 22:29:37.257584: Current learning rate: 0.00961
2024-05-08 22:33:33.951844: Validation loss did not improve from -0.65368. Patience: 3/50
2024-05-08 22:33:33.953671: train_loss -0.8062
2024-05-08 22:33:33.955437: val_loss -0.6186
2024-05-08 22:33:33.956522: Pseudo dice [0.7552]
2024-05-08 22:33:33.957499: Epoch time: 236.7 s
2024-05-08 22:33:33.958469: Yayy! New best EMA pseudo Dice: 0.7372
2024-05-08 22:33:35.536147: 
2024-05-08 22:33:35.538539: Epoch 44
2024-05-08 22:33:35.540192: Current learning rate: 0.0096
2024-05-08 22:37:33.430891: Validation loss did not improve from -0.65368. Patience: 4/50
2024-05-08 22:37:33.432440: train_loss -0.8095
2024-05-08 22:37:33.433824: val_loss -0.6372
2024-05-08 22:37:33.435065: Pseudo dice [0.7664]
2024-05-08 22:37:33.436235: Epoch time: 237.9 s
2024-05-08 22:37:33.810498: Yayy! New best EMA pseudo Dice: 0.7401
2024-05-08 22:37:35.390142: 
2024-05-08 22:37:35.392712: Epoch 45
2024-05-08 22:37:35.394311: Current learning rate: 0.00959
2024-05-08 22:41:38.252764: Validation loss did not improve from -0.65368. Patience: 5/50
2024-05-08 22:41:38.272817: train_loss -0.8087
2024-05-08 22:41:38.327775: val_loss -0.6258
2024-05-08 22:41:38.329412: Pseudo dice [0.7578]
2024-05-08 22:41:38.331011: Epoch time: 242.88 s
2024-05-08 22:41:38.332122: Yayy! New best EMA pseudo Dice: 0.7419
2024-05-08 22:41:41.361520: 
2024-05-08 22:41:41.363883: Epoch 46
2024-05-08 22:41:41.365311: Current learning rate: 0.00959
2024-05-08 22:45:39.160680: Validation loss did not improve from -0.65368. Patience: 6/50
2024-05-08 22:45:39.162456: train_loss -0.8106
2024-05-08 22:45:39.163740: val_loss -0.6286
2024-05-08 22:45:39.164814: Pseudo dice [0.7589]
2024-05-08 22:45:39.165793: Epoch time: 237.8 s
2024-05-08 22:45:39.166759: Yayy! New best EMA pseudo Dice: 0.7436
2024-05-08 22:45:40.886865: 
2024-05-08 22:45:40.889247: Epoch 47
2024-05-08 22:45:40.890660: Current learning rate: 0.00958
2024-05-08 22:49:41.028120: Validation loss did not improve from -0.65368. Patience: 7/50
2024-05-08 22:49:41.030977: train_loss -0.8088
2024-05-08 22:49:41.032621: val_loss -0.5921
2024-05-08 22:49:41.034010: Pseudo dice [0.7387]
2024-05-08 22:49:41.035193: Epoch time: 240.15 s
2024-05-08 22:49:42.244580: 
2024-05-08 22:49:42.247487: Epoch 48
2024-05-08 22:49:42.248623: Current learning rate: 0.00957
2024-05-08 22:53:40.481473: Validation loss did not improve from -0.65368. Patience: 8/50
2024-05-08 22:53:40.483144: train_loss -0.8132
2024-05-08 22:53:40.484917: val_loss -0.6259
2024-05-08 22:53:40.486020: Pseudo dice [0.7609]
2024-05-08 22:53:40.486962: Epoch time: 238.24 s
2024-05-08 22:53:40.487957: Yayy! New best EMA pseudo Dice: 0.7449
2024-05-08 22:53:42.182226: 
2024-05-08 22:53:42.185730: Epoch 49
2024-05-08 22:53:42.187208: Current learning rate: 0.00956
2024-05-08 22:57:43.080869: Validation loss did not improve from -0.65368. Patience: 9/50
2024-05-08 22:57:43.082911: train_loss -0.8115
2024-05-08 22:57:43.084215: val_loss -0.6223
2024-05-08 22:57:43.085320: Pseudo dice [0.7656]
2024-05-08 22:57:43.086447: Epoch time: 240.9 s
2024-05-08 22:57:43.495025: Yayy! New best EMA pseudo Dice: 0.747
2024-05-08 22:57:45.449223: 
2024-05-08 22:57:45.452113: Epoch 50
2024-05-08 22:57:45.453620: Current learning rate: 0.00955
2024-05-08 23:01:44.145864: Validation loss did not improve from -0.65368. Patience: 10/50
2024-05-08 23:01:44.147366: train_loss -0.8153
2024-05-08 23:01:44.148544: val_loss -0.6342
2024-05-08 23:01:44.149544: Pseudo dice [0.7557]
2024-05-08 23:01:44.150513: Epoch time: 238.7 s
2024-05-08 23:01:44.151469: Yayy! New best EMA pseudo Dice: 0.7478
2024-05-08 23:01:45.865093: 
2024-05-08 23:01:45.867361: Epoch 51
2024-05-08 23:01:45.868948: Current learning rate: 0.00954
2024-05-08 23:05:46.405195: Validation loss did not improve from -0.65368. Patience: 11/50
2024-05-08 23:05:46.406713: train_loss -0.8163
2024-05-08 23:05:46.408023: val_loss -0.6187
2024-05-08 23:05:46.409053: Pseudo dice [0.7578]
2024-05-08 23:05:46.410272: Epoch time: 240.54 s
2024-05-08 23:05:46.411212: Yayy! New best EMA pseudo Dice: 0.7488
2024-05-08 23:05:50.196623: 
2024-05-08 23:05:50.198389: Epoch 52
2024-05-08 23:05:50.199610: Current learning rate: 0.00953
2024-05-08 23:09:49.187193: Validation loss did not improve from -0.65368. Patience: 12/50
2024-05-08 23:09:49.188717: train_loss -0.8157
2024-05-08 23:09:49.189967: val_loss -0.5789
2024-05-08 23:09:49.190977: Pseudo dice [0.7331]
2024-05-08 23:09:49.192061: Epoch time: 238.99 s
2024-05-08 23:09:50.653462: 
2024-05-08 23:09:50.655415: Epoch 53
2024-05-08 23:09:50.656888: Current learning rate: 0.00952
2024-05-08 23:13:50.534263: Validation loss did not improve from -0.65368. Patience: 13/50
2024-05-08 23:13:50.535983: train_loss -0.8099
2024-05-08 23:13:50.537802: val_loss -0.5623
2024-05-08 23:13:50.539767: Pseudo dice [0.7302]
2024-05-08 23:13:50.540818: Epoch time: 239.88 s
2024-05-08 23:13:51.803834: 
2024-05-08 23:13:51.805716: Epoch 54
2024-05-08 23:13:51.806805: Current learning rate: 0.00951
2024-05-08 23:17:51.194624: Validation loss did not improve from -0.65368. Patience: 14/50
2024-05-08 23:17:51.196203: train_loss -0.8064
2024-05-08 23:17:51.197426: val_loss -0.5493
2024-05-08 23:17:51.198449: Pseudo dice [0.7119]
2024-05-08 23:17:51.199562: Epoch time: 239.39 s
2024-05-08 23:17:52.880216: 
2024-05-08 23:17:52.881642: Epoch 55
2024-05-08 23:17:52.882793: Current learning rate: 0.0095
2024-05-08 23:21:52.685544: Validation loss did not improve from -0.65368. Patience: 15/50
2024-05-08 23:21:52.686797: train_loss -0.7962
2024-05-08 23:21:52.688012: val_loss -0.595
2024-05-08 23:21:52.689011: Pseudo dice [0.7367]
2024-05-08 23:21:52.689979: Epoch time: 239.81 s
2024-05-08 23:21:53.955757: 
2024-05-08 23:21:53.957394: Epoch 56
2024-05-08 23:21:53.958869: Current learning rate: 0.00949
2024-05-08 23:25:54.117102: Validation loss did not improve from -0.65368. Patience: 16/50
2024-05-08 23:25:54.119646: train_loss -0.7941
2024-05-08 23:25:54.120933: val_loss -0.5076
2024-05-08 23:25:54.121902: Pseudo dice [0.6947]
2024-05-08 23:25:54.122938: Epoch time: 240.17 s
2024-05-08 23:25:55.465984: 
2024-05-08 23:25:55.468164: Epoch 57
2024-05-08 23:25:55.469387: Current learning rate: 0.00949
2024-05-08 23:29:53.600036: Validation loss did not improve from -0.65368. Patience: 17/50
2024-05-08 23:29:53.601696: train_loss -0.8028
2024-05-08 23:29:53.602842: val_loss -0.5567
2024-05-08 23:29:53.603967: Pseudo dice [0.7347]
2024-05-08 23:29:53.605203: Epoch time: 238.14 s
2024-05-08 23:29:54.892869: 
2024-05-08 23:29:54.894892: Epoch 58
2024-05-08 23:29:54.896610: Current learning rate: 0.00948
2024-05-08 23:33:52.386333: Validation loss did not improve from -0.65368. Patience: 18/50
2024-05-08 23:33:52.387702: train_loss -0.81
2024-05-08 23:33:52.388804: val_loss -0.5851
2024-05-08 23:33:52.389746: Pseudo dice [0.7337]
2024-05-08 23:33:52.390697: Epoch time: 237.5 s
2024-05-08 23:33:53.748257: 
2024-05-08 23:33:53.749793: Epoch 59
2024-05-08 23:33:53.750786: Current learning rate: 0.00947
2024-05-08 23:37:52.657269: Validation loss did not improve from -0.65368. Patience: 19/50
2024-05-08 23:37:52.659024: train_loss -0.8034
2024-05-08 23:37:52.660419: val_loss -0.5675
2024-05-08 23:37:52.661475: Pseudo dice [0.7184]
2024-05-08 23:37:52.662726: Epoch time: 238.91 s
2024-05-08 23:37:54.414029: 
2024-05-08 23:37:54.415884: Epoch 60
2024-05-08 23:37:54.417513: Current learning rate: 0.00946
2024-05-08 23:41:54.344844: Validation loss did not improve from -0.65368. Patience: 20/50
2024-05-08 23:41:54.346256: train_loss -0.8047
2024-05-08 23:41:54.347365: val_loss -0.61
2024-05-08 23:41:54.348782: Pseudo dice [0.7621]
2024-05-08 23:41:54.350053: Epoch time: 239.93 s
2024-05-08 23:41:55.690990: 
2024-05-08 23:41:55.692853: Epoch 61
2024-05-08 23:41:55.693820: Current learning rate: 0.00945
2024-05-08 23:45:57.582126: Validation loss did not improve from -0.65368. Patience: 21/50
2024-05-08 23:45:57.612119: train_loss -0.8189
2024-05-08 23:45:57.634688: val_loss -0.5428
2024-05-08 23:45:57.636508: Pseudo dice [0.7168]
2024-05-08 23:45:57.637824: Epoch time: 241.91 s
2024-05-08 23:45:59.634408: 
2024-05-08 23:45:59.637202: Epoch 62
2024-05-08 23:45:59.639295: Current learning rate: 0.00944
2024-05-08 23:49:57.665325: Validation loss did not improve from -0.65368. Patience: 22/50
2024-05-08 23:49:57.667346: train_loss -0.8122
2024-05-08 23:49:57.669070: val_loss -0.5942
2024-05-08 23:49:57.670388: Pseudo dice [0.7596]
2024-05-08 23:49:57.671791: Epoch time: 238.03 s
2024-05-08 23:49:59.014101: 
2024-05-08 23:49:59.016200: Epoch 63
2024-05-08 23:49:59.017560: Current learning rate: 0.00943
2024-05-08 23:53:57.314981: Validation loss did not improve from -0.65368. Patience: 23/50
2024-05-08 23:53:57.335697: train_loss -0.8171
2024-05-08 23:53:57.337259: val_loss -0.6053
2024-05-08 23:53:57.338404: Pseudo dice [0.7443]
2024-05-08 23:53:57.339523: Epoch time: 238.3 s
2024-05-08 23:53:59.457213: 
2024-05-08 23:53:59.459323: Epoch 64
2024-05-08 23:53:59.460665: Current learning rate: 0.00942
2024-05-08 23:57:57.854824: Validation loss did not improve from -0.65368. Patience: 24/50
2024-05-08 23:57:57.856391: train_loss -0.8247
2024-05-08 23:57:57.857691: val_loss -0.6096
2024-05-08 23:57:57.858741: Pseudo dice [0.7546]
2024-05-08 23:57:57.859782: Epoch time: 238.4 s
2024-05-08 23:58:00.500372: 
2024-05-08 23:58:00.502559: Epoch 65
2024-05-08 23:58:00.503872: Current learning rate: 0.00941
2024-05-09 00:01:58.069945: Validation loss did not improve from -0.65368. Patience: 25/50
2024-05-09 00:01:58.072140: train_loss -0.8267
2024-05-09 00:01:58.073490: val_loss -0.6185
2024-05-09 00:01:58.074582: Pseudo dice [0.748]
2024-05-09 00:01:58.075621: Epoch time: 237.57 s
2024-05-09 00:01:59.405216: 
2024-05-09 00:01:59.407752: Epoch 66
2024-05-09 00:01:59.409303: Current learning rate: 0.0094
2024-05-09 00:05:56.250640: Validation loss did not improve from -0.65368. Patience: 26/50
2024-05-09 00:05:56.252964: train_loss -0.8243
2024-05-09 00:05:56.254298: val_loss -0.6107
2024-05-09 00:05:56.255430: Pseudo dice [0.7392]
2024-05-09 00:05:56.256566: Epoch time: 236.85 s
2024-05-09 00:05:57.617048: 
2024-05-09 00:05:57.618876: Epoch 67
2024-05-09 00:05:57.620279: Current learning rate: 0.00939
2024-05-09 00:09:58.050445: Validation loss did not improve from -0.65368. Patience: 27/50
2024-05-09 00:09:58.052176: train_loss -0.8235
2024-05-09 00:09:58.053439: val_loss -0.575
2024-05-09 00:09:58.054521: Pseudo dice [0.7341]
2024-05-09 00:09:58.055526: Epoch time: 240.44 s
2024-05-09 00:09:59.375158: 
2024-05-09 00:09:59.376471: Epoch 68
2024-05-09 00:09:59.377406: Current learning rate: 0.00939
2024-05-09 00:14:02.146343: Validation loss did not improve from -0.65368. Patience: 28/50
2024-05-09 00:14:02.147736: train_loss -0.8284
2024-05-09 00:14:02.148948: val_loss -0.5876
2024-05-09 00:14:02.150134: Pseudo dice [0.7564]
2024-05-09 00:14:02.151229: Epoch time: 242.77 s
2024-05-09 00:14:03.504570: 
2024-05-09 00:14:03.506870: Epoch 69
2024-05-09 00:14:03.508495: Current learning rate: 0.00938
2024-05-09 00:18:02.441187: Validation loss did not improve from -0.65368. Patience: 29/50
2024-05-09 00:18:02.442872: train_loss -0.8303
2024-05-09 00:18:02.444000: val_loss -0.6012
2024-05-09 00:18:02.445201: Pseudo dice [0.7478]
2024-05-09 00:18:02.446267: Epoch time: 238.94 s
2024-05-09 00:18:05.476830: 
2024-05-09 00:18:05.479586: Epoch 70
2024-05-09 00:18:05.481461: Current learning rate: 0.00937
2024-05-09 00:22:05.087507: Validation loss did not improve from -0.65368. Patience: 30/50
2024-05-09 00:22:05.089091: train_loss -0.8335
2024-05-09 00:22:05.090383: val_loss -0.6087
2024-05-09 00:22:05.091538: Pseudo dice [0.7587]
2024-05-09 00:22:05.092726: Epoch time: 239.61 s
2024-05-09 00:22:06.367739: 
2024-05-09 00:22:06.370442: Epoch 71
2024-05-09 00:22:06.371941: Current learning rate: 0.00936
2024-05-09 00:26:08.962522: Validation loss did not improve from -0.65368. Patience: 31/50
2024-05-09 00:26:08.964124: train_loss -0.8326
2024-05-09 00:26:08.965655: val_loss -0.6073
2024-05-09 00:26:08.966995: Pseudo dice [0.7424]
2024-05-09 00:26:08.968194: Epoch time: 242.6 s
2024-05-09 00:26:10.321851: 
2024-05-09 00:26:10.324367: Epoch 72
2024-05-09 00:26:10.325888: Current learning rate: 0.00935
2024-05-09 00:30:13.014684: Validation loss did not improve from -0.65368. Patience: 32/50
2024-05-09 00:30:13.016071: train_loss -0.8286
2024-05-09 00:30:13.017779: val_loss -0.6069
2024-05-09 00:30:13.019057: Pseudo dice [0.749]
2024-05-09 00:30:13.020052: Epoch time: 242.7 s
2024-05-09 00:30:14.354951: 
2024-05-09 00:30:14.357866: Epoch 73
2024-05-09 00:30:14.359502: Current learning rate: 0.00934
2024-05-09 00:34:15.652719: Validation loss did not improve from -0.65368. Patience: 33/50
2024-05-09 00:34:15.655038: train_loss -0.8319
2024-05-09 00:34:15.656961: val_loss -0.5381
2024-05-09 00:34:15.658380: Pseudo dice [0.7167]
2024-05-09 00:34:15.659348: Epoch time: 241.3 s
2024-05-09 00:34:16.924677: 
2024-05-09 00:34:16.926821: Epoch 74
2024-05-09 00:34:16.928464: Current learning rate: 0.00933
2024-05-09 00:38:17.238441: Validation loss did not improve from -0.65368. Patience: 34/50
2024-05-09 00:38:17.240160: train_loss -0.8236
2024-05-09 00:38:17.241924: val_loss -0.5578
2024-05-09 00:38:17.243268: Pseudo dice [0.7218]
2024-05-09 00:38:17.244458: Epoch time: 240.32 s
2024-05-09 00:38:19.578530: 
2024-05-09 00:38:19.580582: Epoch 75
2024-05-09 00:38:19.581897: Current learning rate: 0.00932
2024-05-09 00:42:19.335967: Validation loss did not improve from -0.65368. Patience: 35/50
2024-05-09 00:42:19.337691: train_loss -0.8282
2024-05-09 00:42:19.339297: val_loss -0.5871
2024-05-09 00:42:19.340761: Pseudo dice [0.7379]
2024-05-09 00:42:19.342110: Epoch time: 239.76 s
2024-05-09 00:42:20.621254: 
2024-05-09 00:42:20.623858: Epoch 76
2024-05-09 00:42:20.625989: Current learning rate: 0.00931
2024-05-09 00:46:19.766607: Validation loss did not improve from -0.65368. Patience: 36/50
2024-05-09 00:46:19.767748: train_loss -0.8239
2024-05-09 00:46:19.769010: val_loss -0.6063
2024-05-09 00:46:19.770079: Pseudo dice [0.7426]
2024-05-09 00:46:19.771261: Epoch time: 239.15 s
2024-05-09 00:46:21.088586: 
2024-05-09 00:46:21.090245: Epoch 77
2024-05-09 00:46:21.091353: Current learning rate: 0.0093
2024-05-09 00:50:20.931337: Validation loss did not improve from -0.65368. Patience: 37/50
2024-05-09 00:50:20.932829: train_loss -0.8256
2024-05-09 00:50:20.934272: val_loss -0.6134
2024-05-09 00:50:20.935399: Pseudo dice [0.7485]
2024-05-09 00:50:20.936816: Epoch time: 239.85 s
2024-05-09 00:50:22.310333: 
2024-05-09 00:50:22.313304: Epoch 78
2024-05-09 00:50:22.315013: Current learning rate: 0.0093
2024-05-09 00:54:24.720495: Validation loss did not improve from -0.65368. Patience: 38/50
2024-05-09 00:54:24.845250: train_loss -0.826
2024-05-09 00:54:24.849225: val_loss -0.5829
2024-05-09 00:54:24.851067: Pseudo dice [0.7299]
2024-05-09 00:54:24.853293: Epoch time: 242.52 s
2024-05-09 00:54:26.603861: 
2024-05-09 00:54:26.606189: Epoch 79
2024-05-09 00:54:26.607632: Current learning rate: 0.00929
2024-05-09 00:58:28.648118: Validation loss did not improve from -0.65368. Patience: 39/50
2024-05-09 00:58:28.689131: train_loss -0.83
2024-05-09 00:58:28.690751: val_loss -0.6487
2024-05-09 00:58:28.691963: Pseudo dice [0.7713]
2024-05-09 00:58:28.693051: Epoch time: 242.09 s
2024-05-09 00:58:31.321791: 
2024-05-09 00:58:31.324272: Epoch 80
2024-05-09 00:58:31.325256: Current learning rate: 0.00928
2024-05-09 01:02:32.404170: Validation loss did not improve from -0.65368. Patience: 40/50
2024-05-09 01:02:32.435577: train_loss -0.8322
2024-05-09 01:02:32.437592: val_loss -0.6189
2024-05-09 01:02:32.438910: Pseudo dice [0.7547]
2024-05-09 01:02:32.440135: Epoch time: 241.09 s
2024-05-09 01:02:33.840445: 
2024-05-09 01:02:33.842284: Epoch 81
2024-05-09 01:02:33.843274: Current learning rate: 0.00927
2024-05-09 01:06:34.695350: Validation loss did not improve from -0.65368. Patience: 41/50
2024-05-09 01:06:34.697219: train_loss -0.8299
2024-05-09 01:06:34.698743: val_loss -0.6004
2024-05-09 01:06:34.699968: Pseudo dice [0.7539]
2024-05-09 01:06:34.701048: Epoch time: 240.86 s
2024-05-09 01:06:36.154556: 
2024-05-09 01:06:36.156120: Epoch 82
2024-05-09 01:06:36.157349: Current learning rate: 0.00926
2024-05-09 01:10:34.810674: Validation loss did not improve from -0.65368. Patience: 42/50
2024-05-09 01:10:34.812265: train_loss -0.8303
2024-05-09 01:10:34.813532: val_loss -0.6225
2024-05-09 01:10:34.814458: Pseudo dice [0.7676]
2024-05-09 01:10:34.815372: Epoch time: 238.66 s
2024-05-09 01:10:36.190096: 
2024-05-09 01:10:36.191882: Epoch 83
2024-05-09 01:10:36.192959: Current learning rate: 0.00925
2024-05-09 01:14:35.493331: Validation loss did not improve from -0.65368. Patience: 43/50
2024-05-09 01:14:35.494927: train_loss -0.8323
2024-05-09 01:14:35.496175: val_loss -0.6088
2024-05-09 01:14:35.497150: Pseudo dice [0.7407]
2024-05-09 01:14:35.498173: Epoch time: 239.31 s
2024-05-09 01:14:36.873567: 
2024-05-09 01:14:36.875137: Epoch 84
2024-05-09 01:14:36.876573: Current learning rate: 0.00924
2024-05-09 01:18:36.880156: Validation loss did not improve from -0.65368. Patience: 44/50
2024-05-09 01:18:36.881568: train_loss -0.8278
2024-05-09 01:18:36.882571: val_loss -0.57
2024-05-09 01:18:36.883518: Pseudo dice [0.7332]
2024-05-09 01:18:36.884428: Epoch time: 240.01 s
2024-05-09 01:18:40.045869: 
2024-05-09 01:18:40.047898: Epoch 85
2024-05-09 01:18:40.048915: Current learning rate: 0.00923
2024-05-09 01:22:42.350253: Validation loss did not improve from -0.65368. Patience: 45/50
2024-05-09 01:22:42.351804: train_loss -0.8342
2024-05-09 01:22:42.352957: val_loss -0.6299
2024-05-09 01:22:42.354029: Pseudo dice [0.7597]
2024-05-09 01:22:42.354990: Epoch time: 242.31 s
2024-05-09 01:22:43.723518: 
2024-05-09 01:22:43.725444: Epoch 86
2024-05-09 01:22:43.726582: Current learning rate: 0.00922
2024-05-09 01:26:43.093473: Validation loss did not improve from -0.65368. Patience: 46/50
2024-05-09 01:26:43.095010: train_loss -0.833
2024-05-09 01:26:43.096122: val_loss -0.65
2024-05-09 01:26:43.097218: Pseudo dice [0.776]
2024-05-09 01:26:43.098212: Epoch time: 239.37 s
2024-05-09 01:26:43.099104: Yayy! New best EMA pseudo Dice: 0.7496
2024-05-09 01:26:46.881043: 
2024-05-09 01:26:46.885189: Epoch 87
2024-05-09 01:26:46.887426: Current learning rate: 0.00921
2024-05-09 01:30:41.880467: Validation loss did not improve from -0.65368. Patience: 47/50
2024-05-09 01:30:41.884367: train_loss -0.8391
2024-05-09 01:30:41.886357: val_loss -0.5601
2024-05-09 01:30:41.887612: Pseudo dice [0.7206]
2024-05-09 01:30:41.888837: Epoch time: 235.0 s
2024-05-09 01:30:43.101531: 
2024-05-09 01:30:43.111547: Epoch 88
2024-05-09 01:30:43.113573: Current learning rate: 0.0092
2024-05-09 01:34:38.104183: Validation loss did not improve from -0.65368. Patience: 48/50
2024-05-09 01:34:38.106851: train_loss -0.8436
2024-05-09 01:34:38.108721: val_loss -0.6215
2024-05-09 01:34:38.109892: Pseudo dice [0.75]
2024-05-09 01:34:38.110946: Epoch time: 235.01 s
2024-05-09 01:34:39.319793: 
2024-05-09 01:34:39.322722: Epoch 89
2024-05-09 01:34:39.324190: Current learning rate: 0.0092
2024-05-09 01:38:34.030403: Validation loss did not improve from -0.65368. Patience: 49/50
2024-05-09 01:38:34.032821: train_loss -0.8427
2024-05-09 01:38:34.034875: val_loss -0.6034
2024-05-09 01:38:34.036505: Pseudo dice [0.7408]
2024-05-09 01:38:34.037449: Epoch time: 234.71 s
2024-05-09 01:38:35.548670: 
2024-05-09 01:38:35.551301: Epoch 90
2024-05-09 01:38:35.552883: Current learning rate: 0.00919
2024-05-09 01:42:30.532590: Validation loss did not improve from -0.65368. Patience: 50/50
2024-05-09 01:42:30.534791: train_loss -0.8355
2024-05-09 01:42:30.536832: val_loss -0.6151
2024-05-09 01:42:30.538593: Pseudo dice [0.7566]
2024-05-09 01:42:30.540452: Epoch time: 234.99 s
2024-05-09 01:42:31.761814: Patience reached. Stopping training.
2024-05-09 01:42:32.232229: Training done.
2024-05-09 01:42:32.993688: Using splits from existing split file: /home/gridsan/nchutisilp/datasets/nnUNet_Datasets/nnUNet_preprocessed/Dataset302_Calcium_OCTv2/splits_final.json
2024-05-09 01:42:32.996468: The split file contains 3 splits.
2024-05-09 01:42:32.997789: Desired fold for training: 1
2024-05-09 01:42:32.998817: This split has 4 training and 2 validation cases.
2024-05-09 01:42:32.999955: predicting 101-044
2024-05-09 01:42:33.099917: 101-044, shape torch.Size([1, 404, 498, 498]), rank 0
2024-05-09 01:43:35.567758: predicting 106-002
2024-05-09 01:43:35.673078: 106-002, shape torch.Size([1, 539, 498, 498]), rank 0
2024-05-09 01:45:52.478923: Validation complete
2024-05-09 01:45:52.480587: Mean Validation Dice:  0.7189169796828612
wandb: 
wandb: Run history:
wandb:            ema_fg_dice ▁▂▃▃▄▅▅▆▆▆▆▆▆▆▇▆▇▇▇▇█████▇▇▇▇▇███▇▇█████
wandb:   epoch_end_timestamps ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███
wandb: epoch_start_timestamps ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇████
wandb:                    lrs ████▇▇▇▇▇▆▆▆▆▆▅▅▅▅▅▅▄▄▄▄▄▄▃▃▃▃▃▂▂▂▂▂▁▁▁▁
wandb:           mean_fg_dice ▁▅▅▅▆█▇▇▆▅▇▆▄▆▆▆▇▇▅████▆▆▆▅██▇▇▇▅▇▆█▇█▅▇
wandb:           train_losses █▅▄▄▃▃▃▃▂▃▂▂▂▂▂▂▂▂▂▂▂▁▁▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁
wandb:             val_losses █▄▄▅▂▂▂▂▄▆▂▃▅▄▄▄▃▃▅▂▁▁▁▄▃▅▄▃▂▂▃▂▆▂▃▂▂▁▅▂
wandb: 
wandb: Run summary:
wandb:            ema_fg_dice 0.74742
wandb:   epoch_end_timestamps 1715233350.53465
wandb: epoch_start_timestamps 1715233115.54741
wandb:                    lrs 0.00919
wandb:           mean_fg_dice 0.75665
wandb:           train_losses -0.83551
wandb:             val_losses -0.61512
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/gridsan/nchutisilp/datasets/nnUNet_Datasets/nnUNet_results/Dataset302_Calcium_OCTv2/nnUNetTrainer__nnUNetPlans__3d_32x512x512_b2/fold_1/wandb/offline-run-20240508_193212-jwb9zcjr
wandb: Find logs at: /home/gridsan/nchutisilp/datasets/nnUNet_Datasets/nnUNet_results/Dataset302_Calcium_OCTv2/nnUNetTrainer__nnUNetPlans__3d_32x512x512_b2/fold_1/wandb/offline-run-20240508_193212-jwb9zcjr/logs
WARNING:urllib3.connectionpool:Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x7f0ab68ea0a0>: Failed to establish a new connection: [Errno 101] Network is unreachable')': /api/4504800232407040/envelope/
WARNING:urllib3.connectionpool:Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x7f0a8f827310>: Failed to establish a new connection: [Errno 101] Network is unreachable')': /api/4504800232407040/envelope/
WARNING:urllib3.connectionpool:Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x7f0a667966a0>: Failed to establish a new connection: [Errno 101] Network is unreachable')': /api/4504800232407040/envelope/
WARNING:urllib3.connectionpool:Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x7f0a9feb75e0>: Failed to establish a new connection: [Errno 101] Network is unreachable')': /api/4504800232407040/envelope/
WARNING:urllib3.connectionpool:Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x7f0ac56834f0>: Failed to establish a new connection: [Errno 101] Network is unreachable')': /api/4504800232407040/envelope/
WARNING:urllib3.connectionpool:Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x7f0ac5683220>: Failed to establish a new connection: [Errno 101] Network is unreachable')': /api/4504800232407040/envelope/
FOLD 1 CONFIG 3d_32x512x512_b2 TRAINER nnUNetTrainer

#######################################################################
Please cite the following paper when using nnU-Net:
Isensee, F., Jaeger, P. F., Kohl, S. A., Petersen, J., & Maier-Hein, K. H. (2021). nnU-Net: a self-configuring method for deep learning-based biomedical image segmentation. Nature methods, 18(2), 203-211.
#######################################################################

There are 2 cases in the source folder
I am process 0 out of 1 (max process ID is 0, we start counting with 0!)
There are 2 cases that I would like to predict

Predicting 101-045:
perform_everything_on_device: True
  0%|          | 0/23 [00:00<?, ?it/s]  4%|▍         | 1/23 [00:29<10:42, 29.22s/it]  9%|▊         | 2/23 [00:30<04:30, 12.90s/it] 13%|█▎        | 3/23 [00:32<02:33,  7.68s/it] 17%|█▋        | 4/23 [00:33<01:39,  5.23s/it] 22%|██▏       | 5/23 [00:35<01:09,  3.88s/it] 26%|██▌       | 6/23 [00:36<00:52,  3.06s/it] 30%|███       | 7/23 [00:38<00:40,  2.55s/it] 35%|███▍      | 8/23 [00:39<00:33,  2.21s/it] 39%|███▉      | 9/23 [00:41<00:27,  1.98s/it] 43%|████▎     | 10/23 [00:42<00:23,  1.83s/it] 48%|████▊     | 11/23 [00:44<00:20,  1.72s/it] 52%|█████▏    | 12/23 [00:45<00:18,  1.65s/it] 57%|█████▋    | 13/23 [00:46<00:15,  1.60s/it] 61%|██████    | 14/23 [00:48<00:14,  1.56s/it] 65%|██████▌   | 15/23 [00:49<00:12,  1.54s/it] 70%|██████▉   | 16/23 [00:51<00:10,  1.52s/it] 74%|███████▍  | 17/23 [00:52<00:09,  1.51s/it] 78%|███████▊  | 18/23 [00:54<00:07,  1.50s/it] 83%|████████▎ | 19/23 [00:55<00:05,  1.49s/it] 87%|████████▋ | 20/23 [00:57<00:04,  1.49s/it] 91%|█████████▏| 21/23 [00:58<00:02,  1.49s/it] 96%|█████████▌| 22/23 [01:00<00:01,  1.49s/it]100%|██████████| 23/23 [01:01<00:00,  1.49s/it]100%|██████████| 23/23 [01:01<00:00,  2.69s/it]
sending off prediction to background worker for resampling and export
done with 101-045

Predicting 706-005:
perform_everything_on_device: True
  0%|          | 0/23 [00:00<?, ?it/s]  4%|▍         | 1/23 [00:00<00:17,  1.25it/s]  9%|▊         | 2/23 [00:02<00:25,  1.20s/it] 13%|█▎        | 3/23 [00:03<00:26,  1.33s/it] 17%|█▋        | 4/23 [00:05<00:26,  1.39s/it] 22%|██▏       | 5/23 [00:06<00:25,  1.43s/it] 26%|██▌       | 6/23 [00:08<00:24,  1.45s/it] 30%|███       | 7/23 [00:09<00:23,  1.46s/it] 35%|███▍      | 8/23 [00:11<00:22,  1.47s/it] 39%|███▉      | 9/23 [00:12<00:20,  1.48s/it] 43%|████▎     | 10/23 [00:14<00:19,  1.48s/it] 48%|████▊     | 11/23 [00:15<00:17,  1.48s/it] 52%|█████▏    | 12/23 [00:17<00:16,  1.48s/it] 57%|█████▋    | 13/23 [00:18<00:14,  1.48s/it] 61%|██████    | 14/23 [00:20<00:13,  1.48s/it] 65%|██████▌   | 15/23 [00:21<00:11,  1.48s/it] 70%|██████▉   | 16/23 [00:23<00:10,  1.49s/it] 74%|███████▍  | 17/23 [00:24<00:08,  1.49s/it] 78%|███████▊  | 18/23 [00:26<00:07,  1.49s/it] 83%|████████▎ | 19/23 [00:27<00:05,  1.49s/it] 87%|████████▋ | 20/23 [00:29<00:04,  1.49s/it] 91%|█████████▏| 21/23 [00:30<00:02,  1.49s/it] 96%|█████████▌| 22/23 [00:32<00:01,  1.49s/it]100%|██████████| 23/23 [00:33<00:00,  1.49s/it]100%|██████████| 23/23 [00:33<00:00,  1.46s/it]
sending off prediction to background worker for resampling and export
done with 706-005
Completed FOLD 1 CONFIG 3d_32x512x512_b2 TRAINER nnUNetTrainer
