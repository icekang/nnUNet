/home/gridsan/nchutisilp/.conda/envs/nnunet/bin/python
wandb: Tracking run with wandb version 0.16.6
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.

############################
INFO: You are using the old nnU-Net default plans. We have updated our recommendations. Please consider using those instead! Read more here: https://github.com/MIC-DKFZ/nnUNet/blob/master/documentation/resenc_presets.md
############################

Using device: cuda:0

#######################################################################
Please cite the following paper when using nnU-Net:
Isensee, F., Jaeger, P. F., Kohl, S. A., Petersen, J., & Maier-Hein, K. H. (2021). nnU-Net: a self-configuring method for deep learning-based biomedical image segmentation. Nature methods, 18(2), 203-211.
#######################################################################

2024-09-16 07:16:24.874789: Using torch.compile...
/home/gridsan/nchutisilp/.local/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn("The verbose parameter is deprecated. Please use get_last_lr() "
################### Loading pretrained weights from file  /home/gridsan/nchutisilp/projects/OpenAI-CLIP/clip_preivl_poststent_pretrained_nnUNet.pt ###################
Below is the list of overlapping blocks in pretrained model and nnUNet architecture:
encoder.stages.0.0.convs.0.conv.weight shape torch.Size([32, 1, 3, 3, 3])
encoder.stages.0.0.convs.0.conv.bias shape torch.Size([32])
encoder.stages.0.0.convs.0.norm.weight shape torch.Size([32])
encoder.stages.0.0.convs.0.norm.bias shape torch.Size([32])
encoder.stages.0.0.convs.0.all_modules.0.weight shape torch.Size([32, 1, 3, 3, 3])
encoder.stages.0.0.convs.0.all_modules.0.bias shape torch.Size([32])
encoder.stages.0.0.convs.0.all_modules.1.weight shape torch.Size([32])
encoder.stages.0.0.convs.0.all_modules.1.bias shape torch.Size([32])
encoder.stages.0.0.convs.1.conv.weight shape torch.Size([32, 32, 3, 3, 3])
encoder.stages.0.0.convs.1.conv.bias shape torch.Size([32])
encoder.stages.0.0.convs.1.norm.weight shape torch.Size([32])
encoder.stages.0.0.convs.1.norm.bias shape torch.Size([32])
encoder.stages.0.0.convs.1.all_modules.0.weight shape torch.Size([32, 32, 3, 3, 3])
encoder.stages.0.0.convs.1.all_modules.0.bias shape torch.Size([32])
encoder.stages.0.0.convs.1.all_modules.1.weight shape torch.Size([32])
encoder.stages.0.0.convs.1.all_modules.1.bias shape torch.Size([32])
encoder.stages.1.0.convs.0.conv.weight shape torch.Size([64, 32, 3, 3, 3])
encoder.stages.1.0.convs.0.conv.bias shape torch.Size([64])
encoder.stages.1.0.convs.0.norm.weight shape torch.Size([64])
encoder.stages.1.0.convs.0.norm.bias shape torch.Size([64])
encoder.stages.1.0.convs.0.all_modules.0.weight shape torch.Size([64, 32, 3, 3, 3])
encoder.stages.1.0.convs.0.all_modules.0.bias shape torch.Size([64])
encoder.stages.1.0.convs.0.all_modules.1.weight shape torch.Size([64])
encoder.stages.1.0.convs.0.all_modules.1.bias shape torch.Size([64])
encoder.stages.1.0.convs.1.conv.weight shape torch.Size([64, 64, 3, 3, 3])
encoder.stages.1.0.convs.1.conv.bias shape torch.Size([64])
encoder.stages.1.0.convs.1.norm.weight shape torch.Size([64])
encoder.stages.1.0.convs.1.norm.bias shape torch.Size([64])
encoder.stages.1.0.convs.1.all_modules.0.weight shape torch.Size([64, 64, 3, 3, 3])
encoder.stages.1.0.convs.1.all_modules.0.bias shape torch.Size([64])
encoder.stages.1.0.convs.1.all_modules.1.weight shape torch.Size([64])
encoder.stages.1.0.convs.1.all_modules.1.bias shape torch.Size([64])
encoder.stages.2.0.convs.0.conv.weight shape torch.Size([128, 64, 3, 3, 3])
encoder.stages.2.0.convs.0.conv.bias shape torch.Size([128])
encoder.stages.2.0.convs.0.norm.weight shape torch.Size([128])
encoder.stages.2.0.convs.0.norm.bias shape torch.Size([128])
encoder.stages.2.0.convs.0.all_modules.0.weight shape torch.Size([128, 64, 3, 3, 3])
encoder.stages.2.0.convs.0.all_modules.0.bias shape torch.Size([128])
encoder.stages.2.0.convs.0.all_modules.1.weight shape torch.Size([128])
encoder.stages.2.0.convs.0.all_modules.1.bias shape torch.Size([128])
encoder.stages.2.0.convs.1.conv.weight shape torch.Size([128, 128, 3, 3, 3])
encoder.stages.2.0.convs.1.conv.bias shape torch.Size([128])
encoder.stages.2.0.convs.1.norm.weight shape torch.Size([128])
encoder.stages.2.0.convs.1.norm.bias shape torch.Size([128])
encoder.stages.2.0.convs.1.all_modules.0.weight shape torch.Size([128, 128, 3, 3, 3])
encoder.stages.2.0.convs.1.all_modules.0.bias shape torch.Size([128])
encoder.stages.2.0.convs.1.all_modules.1.weight shape torch.Size([128])
encoder.stages.2.0.convs.1.all_modules.1.bias shape torch.Size([128])
encoder.stages.3.0.convs.0.conv.weight shape torch.Size([256, 128, 3, 3, 3])
encoder.stages.3.0.convs.0.conv.bias shape torch.Size([256])
encoder.stages.3.0.convs.0.norm.weight shape torch.Size([256])
encoder.stages.3.0.convs.0.norm.bias shape torch.Size([256])
encoder.stages.3.0.convs.0.all_modules.0.weight shape torch.Size([256, 128, 3, 3, 3])
encoder.stages.3.0.convs.0.all_modules.0.bias shape torch.Size([256])
encoder.stages.3.0.convs.0.all_modules.1.weight shape torch.Size([256])
encoder.stages.3.0.convs.0.all_modules.1.bias shape torch.Size([256])
encoder.stages.3.0.convs.1.conv.weight shape torch.Size([256, 256, 3, 3, 3])
encoder.stages.3.0.convs.1.conv.bias shape torch.Size([256])
encoder.stages.3.0.convs.1.norm.weight shape torch.Size([256])
encoder.stages.3.0.convs.1.norm.bias shape torch.Size([256])
encoder.stages.3.0.convs.1.all_modules.0.weight shape torch.Size([256, 256, 3, 3, 3])
encoder.stages.3.0.convs.1.all_modules.0.bias shape torch.Size([256])
encoder.stages.3.0.convs.1.all_modules.1.weight shape torch.Size([256])
encoder.stages.3.0.convs.1.all_modules.1.bias shape torch.Size([256])
encoder.stages.4.0.convs.0.conv.weight shape torch.Size([320, 256, 3, 3, 3])
encoder.stages.4.0.convs.0.conv.bias shape torch.Size([320])
encoder.stages.4.0.convs.0.norm.weight shape torch.Size([320])
encoder.stages.4.0.convs.0.norm.bias shape torch.Size([320])
encoder.stages.4.0.convs.0.all_modules.0.weight shape torch.Size([320, 256, 3, 3, 3])
encoder.stages.4.0.convs.0.all_modules.0.bias shape torch.Size([320])
encoder.stages.4.0.convs.0.all_modules.1.weight shape torch.Size([320])
encoder.stages.4.0.convs.0.all_modules.1.bias shape torch.Size([320])
encoder.stages.4.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.4.0.convs.1.conv.bias shape torch.Size([320])
encoder.stages.4.0.convs.1.norm.weight shape torch.Size([320])
encoder.stages.4.0.convs.1.norm.bias shape torch.Size([320])
encoder.stages.4.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.4.0.convs.1.all_modules.0.bias shape torch.Size([320])
encoder.stages.4.0.convs.1.all_modules.1.weight shape torch.Size([320])
encoder.stages.4.0.convs.1.all_modules.1.bias shape torch.Size([320])
encoder.stages.5.0.convs.0.conv.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.5.0.convs.0.conv.bias shape torch.Size([320])
encoder.stages.5.0.convs.0.norm.weight shape torch.Size([320])
encoder.stages.5.0.convs.0.norm.bias shape torch.Size([320])
encoder.stages.5.0.convs.0.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.5.0.convs.0.all_modules.0.bias shape torch.Size([320])
encoder.stages.5.0.convs.0.all_modules.1.weight shape torch.Size([320])
encoder.stages.5.0.convs.0.all_modules.1.bias shape torch.Size([320])
encoder.stages.5.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.5.0.convs.1.conv.bias shape torch.Size([320])
encoder.stages.5.0.convs.1.norm.weight shape torch.Size([320])
encoder.stages.5.0.convs.1.norm.bias shape torch.Size([320])
encoder.stages.5.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.5.0.convs.1.all_modules.0.bias shape torch.Size([320])
encoder.stages.5.0.convs.1.all_modules.1.weight shape torch.Size([320])
encoder.stages.5.0.convs.1.all_modules.1.bias shape torch.Size([320])
decoder.encoder.stages.0.0.convs.0.conv.weight shape torch.Size([32, 1, 3, 3, 3])
decoder.encoder.stages.0.0.convs.0.conv.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.norm.weight shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.norm.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.all_modules.0.weight shape torch.Size([32, 1, 3, 3, 3])
decoder.encoder.stages.0.0.convs.0.all_modules.0.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.all_modules.1.weight shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.all_modules.1.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.conv.weight shape torch.Size([32, 32, 3, 3, 3])
decoder.encoder.stages.0.0.convs.1.conv.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.norm.weight shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.norm.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.all_modules.0.weight shape torch.Size([32, 32, 3, 3, 3])
decoder.encoder.stages.0.0.convs.1.all_modules.0.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.all_modules.1.weight shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.all_modules.1.bias shape torch.Size([32])
decoder.encoder.stages.1.0.convs.0.conv.weight shape torch.Size([64, 32, 3, 3, 3])
decoder.encoder.stages.1.0.convs.0.conv.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.norm.weight shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.norm.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.all_modules.0.weight shape torch.Size([64, 32, 3, 3, 3])
decoder.encoder.stages.1.0.convs.0.all_modules.0.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.all_modules.1.weight shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.all_modules.1.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.conv.weight shape torch.Size([64, 64, 3, 3, 3])
decoder.encoder.stages.1.0.convs.1.conv.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.norm.weight shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.norm.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.all_modules.0.weight shape torch.Size([64, 64, 3, 3, 3])
decoder.encoder.stages.1.0.convs.1.all_modules.0.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.all_modules.1.weight shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.all_modules.1.bias shape torch.Size([64])
decoder.encoder.stages.2.0.convs.0.conv.weight shape torch.Size([128, 64, 3, 3, 3])
decoder.encoder.stages.2.0.convs.0.conv.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.norm.weight shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.norm.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.all_modules.0.weight shape torch.Size([128, 64, 3, 3, 3])
decoder.encoder.stages.2.0.convs.0.all_modules.0.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.all_modules.1.weight shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.all_modules.1.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.conv.weight shape torch.Size([128, 128, 3, 3, 3])
decoder.encoder.stages.2.0.convs.1.conv.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.norm.weight shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.norm.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.all_modules.0.weight shape torch.Size([128, 128, 3, 3, 3])
decoder.encoder.stages.2.0.convs.1.all_modules.0.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.all_modules.1.weight shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.all_modules.1.bias shape torch.Size([128])
decoder.encoder.stages.3.0.convs.0.conv.weight shape torch.Size([256, 128, 3, 3, 3])
decoder.encoder.stages.3.0.convs.0.conv.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.norm.weight shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.norm.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.all_modules.0.weight shape torch.Size([256, 128, 3, 3, 3])
decoder.encoder.stages.3.0.convs.0.all_modules.0.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.all_modules.1.weight shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.all_modules.1.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.conv.weight shape torch.Size([256, 256, 3, 3, 3])
decoder.encoder.stages.3.0.convs.1.conv.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.norm.weight shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.norm.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.all_modules.0.weight shape torch.Size([256, 256, 3, 3, 3])
decoder.encoder.stages.3.0.convs.1.all_modules.0.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.all_modules.1.weight shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.all_modules.1.bias shape torch.Size([256])
decoder.encoder.stages.4.0.convs.0.conv.weight shape torch.Size([320, 256, 3, 3, 3])
decoder.encoder.stages.4.0.convs.0.conv.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.norm.weight shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.norm.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.all_modules.0.weight shape torch.Size([320, 256, 3, 3, 3])
decoder.encoder.stages.4.0.convs.0.all_modules.0.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.all_modules.1.weight shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.all_modules.1.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.4.0.convs.1.conv.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.norm.weight shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.norm.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.4.0.convs.1.all_modules.0.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.all_modules.1.weight shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.all_modules.1.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.conv.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.5.0.convs.0.conv.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.norm.weight shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.norm.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.5.0.convs.0.all_modules.0.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.all_modules.1.weight shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.all_modules.1.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.5.0.convs.1.conv.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.norm.weight shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.norm.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.5.0.convs.1.all_modules.0.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.all_modules.1.weight shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.all_modules.1.bias shape torch.Size([320])
decoder.stages.0.convs.0.conv.weight shape torch.Size([320, 640, 3, 3, 3])
decoder.stages.0.convs.0.conv.bias shape torch.Size([320])
decoder.stages.0.convs.0.norm.weight shape torch.Size([320])
decoder.stages.0.convs.0.norm.bias shape torch.Size([320])
decoder.stages.0.convs.0.all_modules.0.weight shape torch.Size([320, 640, 3, 3, 3])
decoder.stages.0.convs.0.all_modules.0.bias shape torch.Size([320])
decoder.stages.0.convs.0.all_modules.1.weight shape torch.Size([320])
decoder.stages.0.convs.0.all_modules.1.bias shape torch.Size([320])
decoder.stages.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.stages.0.convs.1.conv.bias shape torch.Size([320])
decoder.stages.0.convs.1.norm.weight shape torch.Size([320])
decoder.stages.0.convs.1.norm.bias shape torch.Size([320])
decoder.stages.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.stages.0.convs.1.all_modules.0.bias shape torch.Size([320])
decoder.stages.0.convs.1.all_modules.1.weight shape torch.Size([320])
decoder.stages.0.convs.1.all_modules.1.bias shape torch.Size([320])
decoder.stages.1.convs.0.conv.weight shape torch.Size([256, 512, 3, 3, 3])
decoder.stages.1.convs.0.conv.bias shape torch.Size([256])
decoder.stages.1.convs.0.norm.weight shape torch.Size([256])
decoder.stages.1.convs.0.norm.bias shape torch.Size([256])
decoder.stages.1.convs.0.all_modules.0.weight shape torch.Size([256, 512, 3, 3, 3])
decoder.stages.1.convs.0.all_modules.0.bias shape torch.Size([256])
decoder.stages.1.convs.0.all_modules.1.weight shape torch.Size([256])
decoder.stages.1.convs.0.all_modules.1.bias shape torch.Size([256])
decoder.stages.1.convs.1.conv.weight shape torch.Size([256, 256, 3, 3, 3])
decoder.stages.1.convs.1.conv.bias shape torch.Size([256])
decoder.stages.1.convs.1.norm.weight shape torch.Size([256])
decoder.stages.1.convs.1.norm.bias shape torch.Size([256])
decoder.stages.1.convs.1.all_modules.0.weight shape torch.Size([256, 256, 3, 3, 3])
decoder.stages.1.convs.1.all_modules.0.bias shape torch.Size([256])
decoder.stages.1.convs.1.all_modules.1.weight shape torch.Size([256])
decoder.stages.1.convs.1.all_modules.1.bias shape torch.Size([256])
decoder.stages.2.convs.0.conv.weight shape torch.Size([128, 256, 3, 3, 3])
decoder.stages.2.convs.0.conv.bias shape torch.Size([128])
decoder.stages.2.convs.0.norm.weight shape torch.Size([128])
decoder.stages.2.convs.0.norm.bias shape torch.Size([128])
decoder.stages.2.convs.0.all_modules.0.weight shape torch.Size([128, 256, 3, 3, 3])
decoder.stages.2.convs.0.all_modules.0.bias shape torch.Size([128])
decoder.stages.2.convs.0.all_modules.1.weight shape torch.Size([128])
decoder.stages.2.convs.0.all_modules.1.bias shape torch.Size([128])
decoder.stages.2.convs.1.conv.weight shape torch.Size([128, 128, 3, 3, 3])
decoder.stages.2.convs.1.conv.bias shape torch.Size([128])
decoder.stages.2.convs.1.norm.weight shape torch.Size([128])
decoder.stages.2.convs.1.norm.bias shape torch.Size([128])
decoder.stages.2.convs.1.all_modules.0.weight shape torch.Size([128, 128, 3, 3, 3])
decoder.stages.2.convs.1.all_modules.0.bias shape torch.Size([128])
decoder.stages.2.convs.1.all_modules.1.weight shape torch.Size([128])
decoder.stages.2.convs.1.all_modules.1.bias shape torch.Size([128])
decoder.stages.3.convs.0.conv.weight shape torch.Size([64, 128, 3, 3, 3])
decoder.stages.3.convs.0.conv.bias shape torch.Size([64])
decoder.stages.3.convs.0.norm.weight shape torch.Size([64])
decoder.stages.3.convs.0.norm.bias shape torch.Size([64])
decoder.stages.3.convs.0.all_modules.0.weight shape torch.Size([64, 128, 3, 3, 3])
decoder.stages.3.convs.0.all_modules.0.bias shape torch.Size([64])
decoder.stages.3.convs.0.all_modules.1.weight shape torch.Size([64])
decoder.stages.3.convs.0.all_modules.1.bias shape torch.Size([64])
decoder.stages.3.convs.1.conv.weight shape torch.Size([64, 64, 3, 3, 3])
decoder.stages.3.convs.1.conv.bias shape torch.Size([64])
decoder.stages.3.convs.1.norm.weight shape torch.Size([64])
decoder.stages.3.convs.1.norm.bias shape torch.Size([64])
decoder.stages.3.convs.1.all_modules.0.weight shape torch.Size([64, 64, 3, 3, 3])
decoder.stages.3.convs.1.all_modules.0.bias shape torch.Size([64])
decoder.stages.3.convs.1.all_modules.1.weight shape torch.Size([64])
decoder.stages.3.convs.1.all_modules.1.bias shape torch.Size([64])
decoder.stages.4.convs.0.conv.weight shape torch.Size([32, 64, 3, 3, 3])
decoder.stages.4.convs.0.conv.bias shape torch.Size([32])
decoder.stages.4.convs.0.norm.weight shape torch.Size([32])
decoder.stages.4.convs.0.norm.bias shape torch.Size([32])
decoder.stages.4.convs.0.all_modules.0.weight shape torch.Size([32, 64, 3, 3, 3])
decoder.stages.4.convs.0.all_modules.0.bias shape torch.Size([32])
decoder.stages.4.convs.0.all_modules.1.weight shape torch.Size([32])
decoder.stages.4.convs.0.all_modules.1.bias shape torch.Size([32])
decoder.stages.4.convs.1.conv.weight shape torch.Size([32, 32, 3, 3, 3])
decoder.stages.4.convs.1.conv.bias shape torch.Size([32])
decoder.stages.4.convs.1.norm.weight shape torch.Size([32])
decoder.stages.4.convs.1.norm.bias shape torch.Size([32])
decoder.stages.4.convs.1.all_modules.0.weight shape torch.Size([32, 32, 3, 3, 3])
decoder.stages.4.convs.1.all_modules.0.bias shape torch.Size([32])
decoder.stages.4.convs.1.all_modules.1.weight shape torch.Size([32])
decoder.stages.4.convs.1.all_modules.1.bias shape torch.Size([32])
decoder.transpconvs.0.weight shape torch.Size([320, 320, 1, 2, 2])
decoder.transpconvs.0.bias shape torch.Size([320])
decoder.transpconvs.1.weight shape torch.Size([320, 256, 2, 2, 2])
decoder.transpconvs.1.bias shape torch.Size([256])
decoder.transpconvs.2.weight shape torch.Size([256, 128, 2, 2, 2])
decoder.transpconvs.2.bias shape torch.Size([128])
decoder.transpconvs.3.weight shape torch.Size([128, 64, 2, 2, 2])
decoder.transpconvs.3.bias shape torch.Size([64])
decoder.transpconvs.4.weight shape torch.Size([64, 32, 2, 2, 2])
decoder.transpconvs.4.bias shape torch.Size([32])
################### Done ###################
2024-09-16 07:16:25.823965: do_dummy_2d_data_aug: True
2024-09-16 07:16:25.825622: Using splits from existing split file: /home/gridsan/nchutisilp/datasets/nnUNet_Datasets/nnUNet_preprocessed/Dataset302_Calcium_OCTv2/splits_final.json
2024-09-16 07:16:25.826971: The split file contains 3 splits.
2024-09-16 07:16:25.827732: Desired fold for training: 1
2024-09-16 07:16:25.828528: This split has 4 training and 2 validation cases.
using pin_memory on device 0
using pin_memory on device 0

This is the configuration used by this training:
Configuration name: 3d_32x160x128_b10
 {'data_identifier': 'nnUNetPlans_3d_fullres', 'preprocessor_name': 'DefaultPreprocessor', 'batch_size': 10, 'patch_size': [32, 160, 128], 'median_image_size_in_voxels': [375.0, 498.0, 498.0], 'spacing': [1.0, 1.0, 1.0], 'normalization_schemes': ['ZScoreNormalization'], 'use_mask_for_norm': [False], 'resampling_fn_data': 'resample_data_or_seg_to_shape', 'resampling_fn_seg': 'resample_data_or_seg_to_shape', 'resampling_fn_data_kwargs': {'is_seg': False, 'order': 3, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_seg_kwargs': {'is_seg': True, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_probabilities': 'resample_data_or_seg_to_shape', 'resampling_fn_probabilities_kwargs': {'is_seg': False, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'architecture': {'network_class_name': 'dynamic_network_architectures.architectures.unet.PlainConvUNet', 'arch_kwargs': {'n_stages': 6, 'features_per_stage': [32, 64, 128, 256, 320, 320], 'conv_op': 'torch.nn.modules.conv.Conv3d', 'kernel_sizes': [[3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3]], 'strides': [[1, 1, 1], [2, 2, 2], [2, 2, 2], [2, 2, 2], [2, 2, 2], [1, 2, 2]], 'n_conv_per_stage': [2, 2, 2, 2, 2, 2], 'n_conv_per_stage_decoder': [2, 2, 2, 2, 2], 'conv_bias': True, 'norm_op': 'torch.nn.modules.instancenorm.InstanceNorm3d', 'norm_op_kwargs': {'eps': 1e-05, 'affine': True}, 'dropout_op': None, 'dropout_op_kwargs': None, 'nonlin': 'torch.nn.LeakyReLU', 'nonlin_kwargs': {'inplace': True}, 'deep_supervision': True}, '_kw_requires_import': ['conv_op', 'norm_op', 'dropout_op', 'nonlin']}, 'batch_dice': True, 'inherits_from': '3d_fullres'} 

These are the global plan.json settings:
 {'dataset_name': 'Dataset302_Calcium_OCTv2', 'plans_name': 'nnUNetPlans', 'original_median_spacing_after_transp': [1.0, 1.0, 1.0], 'original_median_shape_after_transp': [375, 498, 498], 'image_reader_writer': 'SimpleITKIO', 'transpose_forward': [0, 1, 2], 'transpose_backward': [0, 1, 2], 'experiment_planner_used': 'ExperimentPlanner', 'label_manager': 'LabelManager', 'foreground_intensity_properties_per_channel': {'0': {'max': 0.4977976381778717, 'mean': 0.13332499563694, 'median': 0.08871842920780182, 'min': 0.0, 'percentile_00_5': 0.014754901640117168, 'percentile_99_5': 0.4977976381778717, 'std': 0.12022686749696732}}} 

2024-09-16 07:16:29.441803: unpacking dataset...
2024-09-16 07:16:34.317884: unpacking done...
2024-09-16 07:16:34.330147: Unable to plot network architecture: nnUNet_compile is enabled!
2024-09-16 07:16:34.452003: 
2024-09-16 07:16:34.453401: Epoch 0
2024-09-16 07:16:34.454507: Current learning rate: 0.01
2024-09-16 07:19:21.005180: Validation loss improved from 1000.00000 to -0.20226! Patience: 0/50
2024-09-16 07:19:21.006605: train_loss -0.0872
2024-09-16 07:19:21.007797: val_loss -0.2023
2024-09-16 07:19:21.008765: Pseudo dice [0.5466]
2024-09-16 07:19:21.009794: Epoch time: 166.56 s
2024-09-16 07:19:21.010858: Yayy! New best EMA pseudo Dice: 0.5466
2024-09-16 07:19:22.621549: 
2024-09-16 07:19:22.622748: Epoch 1
2024-09-16 07:19:22.623732: Current learning rate: 0.00999
2024-09-16 07:21:05.540196: Validation loss improved from -0.20226 to -0.26039! Patience: 0/50
2024-09-16 07:21:05.541558: train_loss -0.2368
2024-09-16 07:21:05.543256: val_loss -0.2604
2024-09-16 07:21:05.544286: Pseudo dice [0.5829]
2024-09-16 07:21:05.545556: Epoch time: 102.92 s
2024-09-16 07:21:05.546539: Yayy! New best EMA pseudo Dice: 0.5502
2024-09-16 07:21:07.340273: 
2024-09-16 07:21:07.341928: Epoch 2
2024-09-16 07:21:07.342915: Current learning rate: 0.00998
2024-09-16 07:22:53.625112: Validation loss improved from -0.26039 to -0.31550! Patience: 0/50
2024-09-16 07:22:53.626665: train_loss -0.3098
2024-09-16 07:22:53.627835: val_loss -0.3155
2024-09-16 07:22:53.629037: Pseudo dice [0.6324]
2024-09-16 07:22:53.630103: Epoch time: 106.29 s
2024-09-16 07:22:53.631065: Yayy! New best EMA pseudo Dice: 0.5585
2024-09-16 07:22:55.497751: 
2024-09-16 07:22:55.499263: Epoch 3
2024-09-16 07:22:55.500330: Current learning rate: 0.00997
2024-09-16 07:24:35.792156: Validation loss did not improve from -0.31550. Patience: 1/50
2024-09-16 07:24:35.793593: train_loss -0.3636
2024-09-16 07:24:35.794763: val_loss -0.2831
2024-09-16 07:24:35.795693: Pseudo dice [0.6218]
2024-09-16 07:24:35.796564: Epoch time: 100.3 s
2024-09-16 07:24:35.797328: Yayy! New best EMA pseudo Dice: 0.5648
2024-09-16 07:24:37.649843: 
2024-09-16 07:24:37.651573: Epoch 4
2024-09-16 07:24:37.652762: Current learning rate: 0.00996
2024-09-16 07:26:27.919728: Validation loss improved from -0.31550 to -0.36449! Patience: 1/50
2024-09-16 07:26:27.920988: train_loss -0.3918
2024-09-16 07:26:27.922053: val_loss -0.3645
2024-09-16 07:26:27.922991: Pseudo dice [0.6747]
2024-09-16 07:26:27.923880: Epoch time: 110.27 s
2024-09-16 07:26:28.268314: Yayy! New best EMA pseudo Dice: 0.5758
2024-09-16 07:26:30.153189: 
2024-09-16 07:26:30.154865: Epoch 5
2024-09-16 07:26:30.155870: Current learning rate: 0.00995
2024-09-16 07:28:06.960153: Validation loss did not improve from -0.36449. Patience: 1/50
2024-09-16 07:28:06.961904: train_loss -0.4225
2024-09-16 07:28:06.963237: val_loss -0.3381
2024-09-16 07:28:06.964183: Pseudo dice [0.6447]
2024-09-16 07:28:06.965301: Epoch time: 96.81 s
2024-09-16 07:28:06.966305: Yayy! New best EMA pseudo Dice: 0.5827
2024-09-16 07:28:08.870834: 
2024-09-16 07:28:08.872488: Epoch 6
2024-09-16 07:28:08.873665: Current learning rate: 0.00995
2024-09-16 07:29:59.065423: Validation loss did not improve from -0.36449. Patience: 2/50
2024-09-16 07:29:59.066822: train_loss -0.4332
2024-09-16 07:29:59.068155: val_loss -0.352
2024-09-16 07:29:59.069243: Pseudo dice [0.6488]
2024-09-16 07:29:59.070279: Epoch time: 110.2 s
2024-09-16 07:29:59.071206: Yayy! New best EMA pseudo Dice: 0.5893
2024-09-16 07:30:00.932132: 
2024-09-16 07:30:00.933903: Epoch 7
2024-09-16 07:30:00.935058: Current learning rate: 0.00994
2024-09-16 07:31:47.690961: Validation loss did not improve from -0.36449. Patience: 3/50
2024-09-16 07:31:47.692318: train_loss -0.4566
2024-09-16 07:31:47.693597: val_loss -0.3384
2024-09-16 07:31:47.694754: Pseudo dice [0.6362]
2024-09-16 07:31:47.695840: Epoch time: 106.76 s
2024-09-16 07:31:47.696853: Yayy! New best EMA pseudo Dice: 0.594
2024-09-16 07:31:49.532772: 
2024-09-16 07:31:49.534228: Epoch 8
2024-09-16 07:31:49.535233: Current learning rate: 0.00993
2024-09-16 07:33:39.074756: Validation loss improved from -0.36449 to -0.38483! Patience: 3/50
2024-09-16 07:33:39.076139: train_loss -0.4718
2024-09-16 07:33:39.077281: val_loss -0.3848
2024-09-16 07:33:39.078248: Pseudo dice [0.6743]
2024-09-16 07:33:39.079224: Epoch time: 109.55 s
2024-09-16 07:33:39.080034: Yayy! New best EMA pseudo Dice: 0.602
2024-09-16 07:33:41.491976: 
2024-09-16 07:33:41.493464: Epoch 9
2024-09-16 07:33:41.494537: Current learning rate: 0.00992
2024-09-16 07:35:27.046535: Validation loss improved from -0.38483 to -0.39618! Patience: 0/50
2024-09-16 07:35:27.048241: train_loss -0.4922
2024-09-16 07:35:27.049496: val_loss -0.3962
2024-09-16 07:35:27.050707: Pseudo dice [0.6724]
2024-09-16 07:35:27.051766: Epoch time: 105.56 s
2024-09-16 07:35:27.516866: Yayy! New best EMA pseudo Dice: 0.6091
2024-09-16 07:35:29.353734: 
2024-09-16 07:35:29.355469: Epoch 10
2024-09-16 07:35:29.356492: Current learning rate: 0.00991
2024-09-16 07:37:20.718998: Validation loss improved from -0.39618 to -0.40897! Patience: 0/50
2024-09-16 07:37:20.720288: train_loss -0.5061
2024-09-16 07:37:20.721373: val_loss -0.409
2024-09-16 07:37:20.722342: Pseudo dice [0.6784]
2024-09-16 07:37:20.723378: Epoch time: 111.37 s
2024-09-16 07:37:20.724252: Yayy! New best EMA pseudo Dice: 0.616
2024-09-16 07:37:22.476473: 
2024-09-16 07:37:22.478026: Epoch 11
2024-09-16 07:37:22.479079: Current learning rate: 0.0099
2024-09-16 07:38:58.146383: Validation loss improved from -0.40897 to -0.41925! Patience: 0/50
2024-09-16 07:38:58.147736: train_loss -0.5193
2024-09-16 07:38:58.149023: val_loss -0.4192
2024-09-16 07:38:58.150115: Pseudo dice [0.6755]
2024-09-16 07:38:58.151253: Epoch time: 95.67 s
2024-09-16 07:38:58.152321: Yayy! New best EMA pseudo Dice: 0.6219
2024-09-16 07:38:59.942390: 
2024-09-16 07:38:59.944157: Epoch 12
2024-09-16 07:38:59.945603: Current learning rate: 0.00989
2024-09-16 07:40:55.501060: Validation loss improved from -0.41925 to -0.41980! Patience: 0/50
2024-09-16 07:40:55.502492: train_loss -0.5286
2024-09-16 07:40:55.503836: val_loss -0.4198
2024-09-16 07:40:55.504734: Pseudo dice [0.6857]
2024-09-16 07:40:55.505626: Epoch time: 115.56 s
2024-09-16 07:40:55.506557: Yayy! New best EMA pseudo Dice: 0.6283
2024-09-16 07:40:57.331471: 
2024-09-16 07:40:57.332872: Epoch 13
2024-09-16 07:40:57.333956: Current learning rate: 0.00988
2024-09-16 07:42:36.679987: Validation loss improved from -0.41980 to -0.45418! Patience: 0/50
2024-09-16 07:42:36.681273: train_loss -0.5116
2024-09-16 07:42:36.682389: val_loss -0.4542
2024-09-16 07:42:36.683380: Pseudo dice [0.6942]
2024-09-16 07:42:36.684295: Epoch time: 99.35 s
2024-09-16 07:42:36.685191: Yayy! New best EMA pseudo Dice: 0.6349
2024-09-16 07:42:38.536865: 
2024-09-16 07:42:38.538566: Epoch 14
2024-09-16 07:42:38.539929: Current learning rate: 0.00987
2024-09-16 07:44:36.294189: Validation loss improved from -0.45418 to -0.46084! Patience: 0/50
2024-09-16 07:44:36.295444: train_loss -0.5293
2024-09-16 07:44:36.296554: val_loss -0.4608
2024-09-16 07:44:36.297470: Pseudo dice [0.7181]
2024-09-16 07:44:36.298388: Epoch time: 117.76 s
2024-09-16 07:44:36.714235: Yayy! New best EMA pseudo Dice: 0.6432
2024-09-16 07:44:38.608130: 
2024-09-16 07:44:38.609975: Epoch 15
2024-09-16 07:44:38.611158: Current learning rate: 0.00986
2024-09-16 07:46:30.341814: Validation loss did not improve from -0.46084. Patience: 1/50
2024-09-16 07:46:30.343314: train_loss -0.5508
2024-09-16 07:46:30.344549: val_loss -0.3741
2024-09-16 07:46:30.345462: Pseudo dice [0.6669]
2024-09-16 07:46:30.346427: Epoch time: 111.74 s
2024-09-16 07:46:30.347323: Yayy! New best EMA pseudo Dice: 0.6456
2024-09-16 07:46:32.227178: 
2024-09-16 07:46:32.240180: Epoch 16
2024-09-16 07:46:32.241200: Current learning rate: 0.00986
2024-09-16 07:48:34.446856: Validation loss did not improve from -0.46084. Patience: 2/50
2024-09-16 07:48:34.448302: train_loss -0.5633
2024-09-16 07:48:34.449512: val_loss -0.4466
2024-09-16 07:48:34.450384: Pseudo dice [0.7075]
2024-09-16 07:48:34.451367: Epoch time: 122.22 s
2024-09-16 07:48:34.452249: Yayy! New best EMA pseudo Dice: 0.6518
2024-09-16 07:48:36.401595: 
2024-09-16 07:48:36.403124: Epoch 17
2024-09-16 07:48:36.404384: Current learning rate: 0.00985
2024-09-16 07:50:31.002245: Validation loss improved from -0.46084 to -0.49788! Patience: 2/50
2024-09-16 07:50:31.003339: train_loss -0.5558
2024-09-16 07:50:31.004448: val_loss -0.4979
2024-09-16 07:50:31.005432: Pseudo dice [0.7327]
2024-09-16 07:50:31.006528: Epoch time: 114.6 s
2024-09-16 07:50:31.007344: Yayy! New best EMA pseudo Dice: 0.6599
2024-09-16 07:50:33.172337: 
2024-09-16 07:50:33.173993: Epoch 18
2024-09-16 07:50:33.174982: Current learning rate: 0.00984
2024-09-16 07:52:30.393184: Validation loss did not improve from -0.49788. Patience: 1/50
2024-09-16 07:52:30.394536: train_loss -0.5583
2024-09-16 07:52:30.395619: val_loss -0.3993
2024-09-16 07:52:30.396604: Pseudo dice [0.6764]
2024-09-16 07:52:30.397660: Epoch time: 117.22 s
2024-09-16 07:52:30.398525: Yayy! New best EMA pseudo Dice: 0.6615
2024-09-16 07:52:32.334428: 
2024-09-16 07:52:32.336336: Epoch 19
2024-09-16 07:52:32.337692: Current learning rate: 0.00983
2024-09-16 07:54:30.738524: Validation loss did not improve from -0.49788. Patience: 2/50
2024-09-16 07:54:30.740067: train_loss -0.5573
2024-09-16 07:54:30.741462: val_loss -0.4737
2024-09-16 07:54:30.742446: Pseudo dice [0.7326]
2024-09-16 07:54:30.743552: Epoch time: 118.41 s
2024-09-16 07:54:31.165555: Yayy! New best EMA pseudo Dice: 0.6686
2024-09-16 07:54:33.479107: 
2024-09-16 07:54:33.481136: Epoch 20
2024-09-16 07:54:33.482268: Current learning rate: 0.00982
2024-09-16 07:56:27.888134: Validation loss did not improve from -0.49788. Patience: 3/50
2024-09-16 07:56:27.889417: train_loss -0.5715
2024-09-16 07:56:27.890521: val_loss -0.4496
2024-09-16 07:56:27.891490: Pseudo dice [0.7054]
2024-09-16 07:56:27.892399: Epoch time: 114.41 s
2024-09-16 07:56:27.893254: Yayy! New best EMA pseudo Dice: 0.6723
2024-09-16 07:56:29.817805: 
2024-09-16 07:56:29.819345: Epoch 21
2024-09-16 07:56:29.820338: Current learning rate: 0.00981
2024-09-16 07:58:24.760199: Validation loss did not improve from -0.49788. Patience: 4/50
2024-09-16 07:58:24.761549: train_loss -0.5827
2024-09-16 07:58:24.762908: val_loss -0.424
2024-09-16 07:58:24.764029: Pseudo dice [0.6888]
2024-09-16 07:58:24.764992: Epoch time: 114.95 s
2024-09-16 07:58:24.765987: Yayy! New best EMA pseudo Dice: 0.674
2024-09-16 07:58:26.629959: 
2024-09-16 07:58:26.631735: Epoch 22
2024-09-16 07:58:26.633037: Current learning rate: 0.0098
2024-09-16 08:00:07.660724: Validation loss did not improve from -0.49788. Patience: 5/50
2024-09-16 08:00:07.662176: train_loss -0.5845
2024-09-16 08:00:07.663365: val_loss -0.4465
2024-09-16 08:00:07.664553: Pseudo dice [0.6966]
2024-09-16 08:00:07.665513: Epoch time: 101.03 s
2024-09-16 08:00:07.669393: Yayy! New best EMA pseudo Dice: 0.6762
2024-09-16 08:00:09.474557: 
2024-09-16 08:00:09.476235: Epoch 23
2024-09-16 08:00:09.477467: Current learning rate: 0.00979
2024-09-16 08:02:03.081332: Validation loss did not improve from -0.49788. Patience: 6/50
2024-09-16 08:02:03.082783: train_loss -0.584
2024-09-16 08:02:03.083833: val_loss -0.4461
2024-09-16 08:02:03.084901: Pseudo dice [0.7089]
2024-09-16 08:02:03.085867: Epoch time: 113.61 s
2024-09-16 08:02:03.086762: Yayy! New best EMA pseudo Dice: 0.6795
2024-09-16 08:02:04.869006: 
2024-09-16 08:02:04.870455: Epoch 24
2024-09-16 08:02:04.871723: Current learning rate: 0.00978
2024-09-16 08:03:55.108283: Validation loss did not improve from -0.49788. Patience: 7/50
2024-09-16 08:03:55.110043: train_loss -0.5984
2024-09-16 08:03:55.111376: val_loss -0.444
2024-09-16 08:03:55.112433: Pseudo dice [0.7201]
2024-09-16 08:03:55.113524: Epoch time: 110.24 s
2024-09-16 08:03:55.542625: Yayy! New best EMA pseudo Dice: 0.6835
2024-09-16 08:03:57.403515: 
2024-09-16 08:03:57.405629: Epoch 25
2024-09-16 08:03:57.406984: Current learning rate: 0.00977
2024-09-16 08:05:52.070706: Validation loss improved from -0.49788 to -0.50550! Patience: 7/50
2024-09-16 08:05:52.072108: train_loss -0.6031
2024-09-16 08:05:52.073610: val_loss -0.5055
2024-09-16 08:05:52.074794: Pseudo dice [0.7443]
2024-09-16 08:05:52.076104: Epoch time: 114.67 s
2024-09-16 08:05:52.077295: Yayy! New best EMA pseudo Dice: 0.6896
2024-09-16 08:05:53.907091: 
2024-09-16 08:05:53.908814: Epoch 26
2024-09-16 08:05:53.910028: Current learning rate: 0.00977
2024-09-16 08:07:51.528483: Validation loss did not improve from -0.50550. Patience: 1/50
2024-09-16 08:07:51.529778: train_loss -0.6086
2024-09-16 08:07:51.530854: val_loss -0.4185
2024-09-16 08:07:51.531797: Pseudo dice [0.6985]
2024-09-16 08:07:51.533021: Epoch time: 117.63 s
2024-09-16 08:07:51.533966: Yayy! New best EMA pseudo Dice: 0.6905
2024-09-16 08:07:53.419718: 
2024-09-16 08:07:53.421305: Epoch 27
2024-09-16 08:07:53.422623: Current learning rate: 0.00976
2024-09-16 08:09:56.182327: Validation loss did not improve from -0.50550. Patience: 2/50
2024-09-16 08:09:56.183785: train_loss -0.6154
2024-09-16 08:09:56.185096: val_loss -0.4517
2024-09-16 08:09:56.186149: Pseudo dice [0.705]
2024-09-16 08:09:56.187255: Epoch time: 122.77 s
2024-09-16 08:09:56.188288: Yayy! New best EMA pseudo Dice: 0.692
2024-09-16 08:09:58.068502: 
2024-09-16 08:09:58.070189: Epoch 28
2024-09-16 08:09:58.071288: Current learning rate: 0.00975
2024-09-16 08:11:52.835808: Validation loss did not improve from -0.50550. Patience: 3/50
2024-09-16 08:11:52.837161: train_loss -0.6195
2024-09-16 08:11:52.838571: val_loss -0.4934
2024-09-16 08:11:52.839886: Pseudo dice [0.7354]
2024-09-16 08:11:52.841275: Epoch time: 114.77 s
2024-09-16 08:11:52.842475: Yayy! New best EMA pseudo Dice: 0.6963
2024-09-16 08:11:54.691368: 
2024-09-16 08:11:54.693355: Epoch 29
2024-09-16 08:11:54.694864: Current learning rate: 0.00974
2024-09-16 08:13:37.016551: Validation loss did not improve from -0.50550. Patience: 4/50
2024-09-16 08:13:37.017932: train_loss -0.6137
2024-09-16 08:13:37.019258: val_loss -0.3934
2024-09-16 08:13:37.020104: Pseudo dice [0.6833]
2024-09-16 08:13:37.021079: Epoch time: 102.33 s
2024-09-16 08:13:38.897716: 
2024-09-16 08:13:38.899137: Epoch 30
2024-09-16 08:13:38.900213: Current learning rate: 0.00973
2024-09-16 08:15:40.109917: Validation loss did not improve from -0.50550. Patience: 5/50
2024-09-16 08:15:40.111200: train_loss -0.6237
2024-09-16 08:15:40.112416: val_loss -0.2507
2024-09-16 08:15:40.113455: Pseudo dice [0.5827]
2024-09-16 08:15:40.114425: Epoch time: 121.22 s
2024-09-16 08:15:41.997977: 
2024-09-16 08:15:41.999695: Epoch 31
2024-09-16 08:15:42.000728: Current learning rate: 0.00972
2024-09-16 08:17:38.579307: Validation loss did not improve from -0.50550. Patience: 6/50
2024-09-16 08:17:38.580581: train_loss -0.6289
2024-09-16 08:17:38.581580: val_loss -0.4736
2024-09-16 08:17:38.582597: Pseudo dice [0.724]
2024-09-16 08:17:38.583569: Epoch time: 116.58 s
2024-09-16 08:17:40.066498: 
2024-09-16 08:17:40.067905: Epoch 32
2024-09-16 08:17:40.068990: Current learning rate: 0.00971
2024-09-16 08:19:39.419430: Validation loss did not improve from -0.50550. Patience: 7/50
2024-09-16 08:19:39.420956: train_loss -0.6352
2024-09-16 08:19:39.422202: val_loss -0.4822
2024-09-16 08:19:39.423047: Pseudo dice [0.7248]
2024-09-16 08:19:39.423912: Epoch time: 119.36 s
2024-09-16 08:19:40.912273: 
2024-09-16 08:19:40.913835: Epoch 33
2024-09-16 08:19:40.915032: Current learning rate: 0.0097
2024-09-16 08:21:37.505352: Validation loss did not improve from -0.50550. Patience: 8/50
2024-09-16 08:21:37.508149: train_loss -0.6387
2024-09-16 08:21:37.509408: val_loss -0.4063
2024-09-16 08:21:37.510488: Pseudo dice [0.707]
2024-09-16 08:21:37.511815: Epoch time: 116.6 s
2024-09-16 08:21:38.962424: 
2024-09-16 08:21:38.964011: Epoch 34
2024-09-16 08:21:38.965155: Current learning rate: 0.00969
2024-09-16 08:23:22.933892: Validation loss did not improve from -0.50550. Patience: 9/50
2024-09-16 08:23:22.935317: train_loss -0.6367
2024-09-16 08:23:22.936657: val_loss -0.505
2024-09-16 08:23:22.937676: Pseudo dice [0.7347]
2024-09-16 08:23:22.938546: Epoch time: 103.97 s
2024-09-16 08:23:23.469925: Yayy! New best EMA pseudo Dice: 0.6972
2024-09-16 08:23:25.344934: 
2024-09-16 08:23:25.346662: Epoch 35
2024-09-16 08:23:25.347774: Current learning rate: 0.00968
2024-09-16 08:25:21.558046: Validation loss did not improve from -0.50550. Patience: 10/50
2024-09-16 08:25:21.561087: train_loss -0.6392
2024-09-16 08:25:21.562938: val_loss -0.4734
2024-09-16 08:25:21.564342: Pseudo dice [0.7295]
2024-09-16 08:25:21.565537: Epoch time: 116.22 s
2024-09-16 08:25:21.566701: Yayy! New best EMA pseudo Dice: 0.7004
2024-09-16 08:25:23.505078: 
2024-09-16 08:25:23.507001: Epoch 36
2024-09-16 08:25:23.508066: Current learning rate: 0.00968
2024-09-16 08:27:22.774895: Validation loss did not improve from -0.50550. Patience: 11/50
2024-09-16 08:27:22.776352: train_loss -0.645
2024-09-16 08:27:22.777331: val_loss -0.4546
2024-09-16 08:27:22.778411: Pseudo dice [0.7261]
2024-09-16 08:27:22.779314: Epoch time: 119.27 s
2024-09-16 08:27:22.780238: Yayy! New best EMA pseudo Dice: 0.703
2024-09-16 08:27:24.694139: 
2024-09-16 08:27:24.695300: Epoch 37
2024-09-16 08:27:24.696236: Current learning rate: 0.00967
2024-09-16 08:29:26.753546: Validation loss did not improve from -0.50550. Patience: 12/50
2024-09-16 08:29:26.754891: train_loss -0.6404
2024-09-16 08:29:26.756032: val_loss -0.4528
2024-09-16 08:29:26.757038: Pseudo dice [0.7226]
2024-09-16 08:29:26.758145: Epoch time: 122.06 s
2024-09-16 08:29:26.759235: Yayy! New best EMA pseudo Dice: 0.705
2024-09-16 08:29:28.657562: 
2024-09-16 08:29:28.659067: Epoch 38
2024-09-16 08:29:28.660146: Current learning rate: 0.00966
2024-09-16 08:31:29.938580: Validation loss did not improve from -0.50550. Patience: 13/50
2024-09-16 08:31:29.940038: train_loss -0.6451
2024-09-16 08:31:29.942565: val_loss -0.4635
2024-09-16 08:31:29.943854: Pseudo dice [0.7118]
2024-09-16 08:31:29.944910: Epoch time: 121.28 s
2024-09-16 08:31:29.945823: Yayy! New best EMA pseudo Dice: 0.7056
2024-09-16 08:31:31.887877: 
2024-09-16 08:31:31.889841: Epoch 39
2024-09-16 08:31:31.891286: Current learning rate: 0.00965
2024-09-16 08:33:17.313648: Validation loss did not improve from -0.50550. Patience: 14/50
2024-09-16 08:33:17.315219: train_loss -0.651
2024-09-16 08:33:17.316612: val_loss -0.4542
2024-09-16 08:33:17.317660: Pseudo dice [0.7161]
2024-09-16 08:33:17.318806: Epoch time: 105.43 s
2024-09-16 08:33:17.724819: Yayy! New best EMA pseudo Dice: 0.7067
2024-09-16 08:33:19.639026: 
2024-09-16 08:33:19.640730: Epoch 40
2024-09-16 08:33:19.641964: Current learning rate: 0.00964
2024-09-16 08:35:21.388062: Validation loss did not improve from -0.50550. Patience: 15/50
2024-09-16 08:35:21.389505: train_loss -0.6591
2024-09-16 08:35:21.390686: val_loss -0.418
2024-09-16 08:35:21.391644: Pseudo dice [0.7094]
2024-09-16 08:35:21.392743: Epoch time: 121.75 s
2024-09-16 08:35:21.393778: Yayy! New best EMA pseudo Dice: 0.707
2024-09-16 08:35:23.409319: 
2024-09-16 08:35:23.410898: Epoch 41
2024-09-16 08:35:23.411905: Current learning rate: 0.00963
2024-09-16 08:37:22.197661: Validation loss did not improve from -0.50550. Patience: 16/50
2024-09-16 08:37:22.199115: train_loss -0.6588
2024-09-16 08:37:22.200616: val_loss -0.3389
2024-09-16 08:37:22.201776: Pseudo dice [0.6539]
2024-09-16 08:37:22.202933: Epoch time: 118.79 s
2024-09-16 08:37:25.015973: 
2024-09-16 08:37:25.017719: Epoch 42
2024-09-16 08:37:25.018852: Current learning rate: 0.00962
2024-09-16 08:39:29.029882: Validation loss improved from -0.50550 to -0.50596! Patience: 16/50
2024-09-16 08:39:29.031304: train_loss -0.6579
2024-09-16 08:39:29.032419: val_loss -0.506
2024-09-16 08:39:29.033278: Pseudo dice [0.7407]
2024-09-16 08:39:29.034187: Epoch time: 124.02 s
2024-09-16 08:39:30.460866: 
2024-09-16 08:39:30.462413: Epoch 43
2024-09-16 08:39:30.463499: Current learning rate: 0.00961
2024-09-16 08:41:29.324437: Validation loss did not improve from -0.50596. Patience: 1/50
2024-09-16 08:41:29.325782: train_loss -0.6691
2024-09-16 08:41:29.326959: val_loss -0.4397
2024-09-16 08:41:29.327972: Pseudo dice [0.7097]
2024-09-16 08:41:29.329073: Epoch time: 118.87 s
2024-09-16 08:41:30.732347: 
2024-09-16 08:41:30.734106: Epoch 44
2024-09-16 08:41:30.735298: Current learning rate: 0.0096
2024-09-16 08:43:15.414957: Validation loss did not improve from -0.50596. Patience: 2/50
2024-09-16 08:43:15.416390: train_loss -0.6675
2024-09-16 08:43:15.417433: val_loss -0.433
2024-09-16 08:43:15.418326: Pseudo dice [0.7059]
2024-09-16 08:43:15.419259: Epoch time: 104.69 s
2024-09-16 08:43:17.384244: 
2024-09-16 08:43:17.385956: Epoch 45
2024-09-16 08:43:17.386876: Current learning rate: 0.00959
2024-09-16 08:45:18.555767: Validation loss did not improve from -0.50596. Patience: 3/50
2024-09-16 08:45:18.556910: train_loss -0.6773
2024-09-16 08:45:18.558030: val_loss -0.4118
2024-09-16 08:45:18.558980: Pseudo dice [0.7037]
2024-09-16 08:45:18.559943: Epoch time: 121.17 s
2024-09-16 08:45:19.994248: 
2024-09-16 08:45:19.995872: Epoch 46
2024-09-16 08:45:19.997054: Current learning rate: 0.00959
2024-09-16 08:47:19.440508: Validation loss did not improve from -0.50596. Patience: 4/50
2024-09-16 08:47:19.441780: train_loss -0.6734
2024-09-16 08:47:19.442851: val_loss -0.4616
2024-09-16 08:47:19.443904: Pseudo dice [0.7298]
2024-09-16 08:47:19.444865: Epoch time: 119.45 s
2024-09-16 08:47:19.445734: Yayy! New best EMA pseudo Dice: 0.7082
2024-09-16 08:47:21.289131: 
2024-09-16 08:47:21.290820: Epoch 47
2024-09-16 08:47:21.292076: Current learning rate: 0.00958
2024-09-16 08:49:25.025992: Validation loss did not improve from -0.50596. Patience: 5/50
2024-09-16 08:49:25.027422: train_loss -0.6841
2024-09-16 08:49:25.028768: val_loss -0.3901
2024-09-16 08:49:25.030036: Pseudo dice [0.6824]
2024-09-16 08:49:25.031107: Epoch time: 123.74 s
2024-09-16 08:49:26.439815: 
2024-09-16 08:49:26.441442: Epoch 48
2024-09-16 08:49:26.442497: Current learning rate: 0.00957
2024-09-16 08:51:26.574639: Validation loss did not improve from -0.50596. Patience: 6/50
2024-09-16 08:51:26.575730: train_loss -0.682
2024-09-16 08:51:26.576774: val_loss -0.4087
2024-09-16 08:51:26.577690: Pseudo dice [0.6996]
2024-09-16 08:51:26.578613: Epoch time: 120.14 s
2024-09-16 08:51:28.007160: 
2024-09-16 08:51:28.008669: Epoch 49
2024-09-16 08:51:28.009552: Current learning rate: 0.00956
2024-09-16 08:53:14.670228: Validation loss did not improve from -0.50596. Patience: 7/50
2024-09-16 08:53:14.671762: train_loss -0.6911
2024-09-16 08:53:14.673144: val_loss -0.4222
2024-09-16 08:53:14.674192: Pseudo dice [0.7141]
2024-09-16 08:53:14.675259: Epoch time: 106.67 s
2024-09-16 08:53:16.526479: 
2024-09-16 08:53:16.528219: Epoch 50
2024-09-16 08:53:16.529307: Current learning rate: 0.00955
2024-09-16 08:55:18.366620: Validation loss improved from -0.50596 to -0.50774! Patience: 7/50
2024-09-16 08:55:18.368140: train_loss -0.6837
2024-09-16 08:55:18.369186: val_loss -0.5077
2024-09-16 08:55:18.370187: Pseudo dice [0.7453]
2024-09-16 08:55:18.371064: Epoch time: 121.84 s
2024-09-16 08:55:18.371942: Yayy! New best EMA pseudo Dice: 0.7098
2024-09-16 08:55:20.234934: 
2024-09-16 08:55:20.236603: Epoch 51
2024-09-16 08:55:20.237634: Current learning rate: 0.00954
2024-09-16 08:57:27.263746: Validation loss did not improve from -0.50774. Patience: 1/50
2024-09-16 08:57:27.265159: train_loss -0.6877
2024-09-16 08:57:27.266149: val_loss -0.4503
2024-09-16 08:57:27.267095: Pseudo dice [0.7187]
2024-09-16 08:57:27.268002: Epoch time: 127.03 s
2024-09-16 08:57:27.268899: Yayy! New best EMA pseudo Dice: 0.7107
2024-09-16 08:57:29.055948: 
2024-09-16 08:57:29.057388: Epoch 52
2024-09-16 08:57:29.058837: Current learning rate: 0.00953
2024-09-16 08:59:31.932491: Validation loss did not improve from -0.50774. Patience: 2/50
2024-09-16 08:59:31.933812: train_loss -0.6742
2024-09-16 08:59:31.935218: val_loss -0.4234
2024-09-16 08:59:31.962559: Pseudo dice [0.713]
2024-09-16 08:59:31.963686: Epoch time: 122.88 s
2024-09-16 08:59:31.964580: Yayy! New best EMA pseudo Dice: 0.711
2024-09-16 08:59:34.322500: 
2024-09-16 08:59:34.324325: Epoch 53
2024-09-16 08:59:34.325381: Current learning rate: 0.00952
2024-09-16 09:01:32.605196: Validation loss did not improve from -0.50774. Patience: 3/50
2024-09-16 09:01:32.607564: train_loss -0.6812
2024-09-16 09:01:32.608903: val_loss -0.405
2024-09-16 09:01:32.610204: Pseudo dice [0.6861]
2024-09-16 09:01:32.611848: Epoch time: 118.29 s
2024-09-16 09:01:34.083408: 
2024-09-16 09:01:34.085111: Epoch 54
2024-09-16 09:01:34.086123: Current learning rate: 0.00951
2024-09-16 09:03:20.639765: Validation loss did not improve from -0.50774. Patience: 4/50
2024-09-16 09:03:20.641224: train_loss -0.6976
2024-09-16 09:03:20.642392: val_loss -0.4729
2024-09-16 09:03:20.643689: Pseudo dice [0.7356]
2024-09-16 09:03:20.644675: Epoch time: 106.56 s
2024-09-16 09:03:21.075510: Yayy! New best EMA pseudo Dice: 0.7112
2024-09-16 09:03:22.979383: 
2024-09-16 09:03:22.981195: Epoch 55
2024-09-16 09:03:22.982494: Current learning rate: 0.0095
2024-09-16 09:05:23.015033: Validation loss improved from -0.50774 to -0.51731! Patience: 4/50
2024-09-16 09:05:23.016413: train_loss -0.6923
2024-09-16 09:05:23.017461: val_loss -0.5173
2024-09-16 09:05:23.018358: Pseudo dice [0.7598]
2024-09-16 09:05:23.019174: Epoch time: 120.04 s
2024-09-16 09:05:23.020073: Yayy! New best EMA pseudo Dice: 0.7161
2024-09-16 09:05:24.843120: 
2024-09-16 09:05:24.844630: Epoch 56
2024-09-16 09:05:24.845864: Current learning rate: 0.00949
2024-09-16 09:07:24.055516: Validation loss did not improve from -0.51731. Patience: 1/50
2024-09-16 09:07:24.056920: train_loss -0.6974
2024-09-16 09:07:24.057977: val_loss -0.3553
2024-09-16 09:07:24.058997: Pseudo dice [0.6811]
2024-09-16 09:07:24.059980: Epoch time: 119.22 s
2024-09-16 09:07:25.504749: 
2024-09-16 09:07:25.519655: Epoch 57
2024-09-16 09:07:25.520980: Current learning rate: 0.00949
2024-09-16 09:09:30.653664: Validation loss did not improve from -0.51731. Patience: 2/50
2024-09-16 09:09:30.655057: train_loss -0.6993
2024-09-16 09:09:30.656258: val_loss -0.4807
2024-09-16 09:09:30.657332: Pseudo dice [0.7425]
2024-09-16 09:09:30.658433: Epoch time: 125.15 s
2024-09-16 09:09:32.127718: 
2024-09-16 09:09:32.129211: Epoch 58
2024-09-16 09:09:32.130482: Current learning rate: 0.00948
2024-09-16 09:11:34.694349: Validation loss did not improve from -0.51731. Patience: 3/50
2024-09-16 09:11:34.695821: train_loss -0.7073
2024-09-16 09:11:34.696975: val_loss -0.3732
2024-09-16 09:11:34.698014: Pseudo dice [0.6819]
2024-09-16 09:11:34.698941: Epoch time: 122.57 s
2024-09-16 09:11:36.234748: 
2024-09-16 09:11:36.236541: Epoch 59
2024-09-16 09:11:36.237546: Current learning rate: 0.00947
2024-09-16 09:13:26.830735: Validation loss did not improve from -0.51731. Patience: 4/50
2024-09-16 09:13:26.832787: train_loss -0.7123
2024-09-16 09:13:26.834114: val_loss -0.3605
2024-09-16 09:13:26.835074: Pseudo dice [0.6801]
2024-09-16 09:13:26.836144: Epoch time: 110.6 s
2024-09-16 09:13:28.849587: 
2024-09-16 09:13:28.851230: Epoch 60
2024-09-16 09:13:28.852185: Current learning rate: 0.00946
2024-09-16 09:15:34.332236: Validation loss did not improve from -0.51731. Patience: 5/50
2024-09-16 09:15:34.333691: train_loss -0.7058
2024-09-16 09:15:34.336239: val_loss -0.464
2024-09-16 09:15:34.337375: Pseudo dice [0.7322]
2024-09-16 09:15:34.338738: Epoch time: 125.49 s
2024-09-16 09:15:35.879769: 
2024-09-16 09:15:35.881409: Epoch 61
2024-09-16 09:15:35.882348: Current learning rate: 0.00945
2024-09-16 09:17:35.024929: Validation loss did not improve from -0.51731. Patience: 6/50
2024-09-16 09:17:35.026345: train_loss -0.7084
2024-09-16 09:17:35.027718: val_loss -0.4913
2024-09-16 09:17:35.029030: Pseudo dice [0.7495]
2024-09-16 09:17:35.030330: Epoch time: 119.15 s
2024-09-16 09:17:36.448351: 
2024-09-16 09:17:36.450034: Epoch 62
2024-09-16 09:17:36.451319: Current learning rate: 0.00944
2024-09-16 09:19:37.651816: Validation loss did not improve from -0.51731. Patience: 7/50
2024-09-16 09:19:37.653097: train_loss -0.7085
2024-09-16 09:19:37.654097: val_loss -0.4445
2024-09-16 09:19:37.654992: Pseudo dice [0.7082]
2024-09-16 09:19:37.656001: Epoch time: 121.21 s
2024-09-16 09:19:39.084206: 
2024-09-16 09:19:39.085804: Epoch 63
2024-09-16 09:19:39.086873: Current learning rate: 0.00943
2024-09-16 09:21:41.863770: Validation loss did not improve from -0.51731. Patience: 8/50
2024-09-16 09:21:41.865403: train_loss -0.7129
2024-09-16 09:21:41.866964: val_loss -0.5057
2024-09-16 09:21:41.868073: Pseudo dice [0.7512]
2024-09-16 09:21:41.869183: Epoch time: 122.78 s
2024-09-16 09:21:41.870198: Yayy! New best EMA pseudo Dice: 0.7181
2024-09-16 09:21:44.340152: 
2024-09-16 09:21:44.342126: Epoch 64
2024-09-16 09:21:44.343510: Current learning rate: 0.00942
2024-09-16 09:23:37.775176: Validation loss did not improve from -0.51731. Patience: 9/50
2024-09-16 09:23:37.776593: train_loss -0.7184
2024-09-16 09:23:37.777984: val_loss -0.3464
2024-09-16 09:23:37.779053: Pseudo dice [0.6757]
2024-09-16 09:23:37.780151: Epoch time: 113.44 s
2024-09-16 09:23:39.650135: 
2024-09-16 09:23:39.652728: Epoch 65
2024-09-16 09:23:39.654120: Current learning rate: 0.00941
2024-09-16 09:25:42.201774: Validation loss did not improve from -0.51731. Patience: 10/50
2024-09-16 09:25:42.203244: train_loss -0.72
2024-09-16 09:25:42.204426: val_loss -0.419
2024-09-16 09:25:42.205324: Pseudo dice [0.6954]
2024-09-16 09:25:42.206337: Epoch time: 122.56 s
2024-09-16 09:25:43.794077: 
2024-09-16 09:25:43.795805: Epoch 66
2024-09-16 09:25:43.798169: Current learning rate: 0.0094
2024-09-16 09:27:47.230041: Validation loss did not improve from -0.51731. Patience: 11/50
2024-09-16 09:27:47.271983: train_loss -0.7175
2024-09-16 09:27:47.273330: val_loss -0.3908
2024-09-16 09:27:47.274398: Pseudo dice [0.6991]
2024-09-16 09:27:47.275508: Epoch time: 123.48 s
2024-09-16 09:27:48.907314: 
2024-09-16 09:27:48.909325: Epoch 67
2024-09-16 09:27:48.910535: Current learning rate: 0.00939
2024-09-16 09:29:36.596905: Validation loss did not improve from -0.51731. Patience: 12/50
2024-09-16 09:29:36.613919: train_loss -0.7169
2024-09-16 09:29:36.615290: val_loss -0.4577
2024-09-16 09:29:36.616322: Pseudo dice [0.7229]
2024-09-16 09:29:36.617190: Epoch time: 107.71 s
2024-09-16 09:29:38.152484: 
2024-09-16 09:29:38.153786: Epoch 68
2024-09-16 09:29:38.154697: Current learning rate: 0.00939
2024-09-16 09:31:43.894037: Validation loss did not improve from -0.51731. Patience: 13/50
2024-09-16 09:31:43.895470: train_loss -0.7279
2024-09-16 09:31:43.896544: val_loss -0.491
2024-09-16 09:31:43.897527: Pseudo dice [0.741]
2024-09-16 09:31:43.898512: Epoch time: 125.74 s
2024-09-16 09:31:45.458812: 
2024-09-16 09:31:45.460530: Epoch 69
2024-09-16 09:31:45.461699: Current learning rate: 0.00938
2024-09-16 09:33:49.350255: Validation loss did not improve from -0.51731. Patience: 14/50
2024-09-16 09:33:49.352686: train_loss -0.7294
2024-09-16 09:33:49.354287: val_loss -0.4062
2024-09-16 09:33:49.355458: Pseudo dice [0.7167]
2024-09-16 09:33:49.356508: Epoch time: 123.9 s
2024-09-16 09:33:51.216326: 
2024-09-16 09:33:51.218315: Epoch 70
2024-09-16 09:33:51.219663: Current learning rate: 0.00937
2024-09-16 09:35:52.097489: Validation loss did not improve from -0.51731. Patience: 15/50
2024-09-16 09:35:52.099086: train_loss -0.7142
2024-09-16 09:35:52.100367: val_loss -0.4332
2024-09-16 09:35:52.101543: Pseudo dice [0.7186]
2024-09-16 09:35:52.102772: Epoch time: 120.89 s
2024-09-16 09:35:53.666389: 
2024-09-16 09:35:53.668192: Epoch 71
2024-09-16 09:35:53.669404: Current learning rate: 0.00936
2024-09-16 09:37:58.944659: Validation loss did not improve from -0.51731. Patience: 16/50
2024-09-16 09:37:58.947721: train_loss -0.7199
2024-09-16 09:37:58.949077: val_loss -0.4504
2024-09-16 09:37:58.950034: Pseudo dice [0.7179]
2024-09-16 09:37:58.950980: Epoch time: 125.28 s
2024-09-16 09:38:00.456440: 
2024-09-16 09:38:00.458174: Epoch 72
2024-09-16 09:38:00.459201: Current learning rate: 0.00935
2024-09-16 09:39:51.034145: Validation loss did not improve from -0.51731. Patience: 17/50
2024-09-16 09:39:51.035716: train_loss -0.7279
2024-09-16 09:39:51.037082: val_loss -0.485
2024-09-16 09:39:51.038207: Pseudo dice [0.7387]
2024-09-16 09:39:51.039417: Epoch time: 110.58 s
2024-09-16 09:39:52.519562: 
2024-09-16 09:39:52.521589: Epoch 73
2024-09-16 09:39:52.523178: Current learning rate: 0.00934
2024-09-16 09:41:58.689533: Validation loss did not improve from -0.51731. Patience: 18/50
2024-09-16 09:41:58.690944: train_loss -0.7317
2024-09-16 09:41:58.692143: val_loss -0.3909
2024-09-16 09:41:58.693070: Pseudo dice [0.7082]
2024-09-16 09:41:58.694041: Epoch time: 126.17 s
2024-09-16 09:42:00.190325: 
2024-09-16 09:42:00.191642: Epoch 74
2024-09-16 09:42:00.192664: Current learning rate: 0.00933
2024-09-16 09:44:04.734449: Validation loss did not improve from -0.51731. Patience: 19/50
2024-09-16 09:44:04.735913: train_loss -0.7331
2024-09-16 09:44:04.737254: val_loss -0.4126
2024-09-16 09:44:04.738339: Pseudo dice [0.7073]
2024-09-16 09:44:04.739433: Epoch time: 124.55 s
2024-09-16 09:44:07.150606: 
2024-09-16 09:44:07.152492: Epoch 75
2024-09-16 09:44:07.153741: Current learning rate: 0.00932
2024-09-16 09:46:01.558738: Validation loss did not improve from -0.51731. Patience: 20/50
2024-09-16 09:46:01.560087: train_loss -0.7287
2024-09-16 09:46:01.561200: val_loss -0.4294
2024-09-16 09:46:01.562288: Pseudo dice [0.7212]
2024-09-16 09:46:01.563350: Epoch time: 114.41 s
2024-09-16 09:46:03.850643: 
2024-09-16 09:46:03.852441: Epoch 76
2024-09-16 09:46:03.853623: Current learning rate: 0.00931
2024-09-16 09:48:08.074572: Validation loss did not improve from -0.51731. Patience: 21/50
2024-09-16 09:48:08.076069: train_loss -0.7287
2024-09-16 09:48:08.077390: val_loss -0.5007
2024-09-16 09:48:08.078421: Pseudo dice [0.7537]
2024-09-16 09:48:08.079511: Epoch time: 124.23 s
2024-09-16 09:48:08.080465: Yayy! New best EMA pseudo Dice: 0.7202
2024-09-16 09:48:09.971653: 
2024-09-16 09:48:09.973274: Epoch 77
2024-09-16 09:48:09.974308: Current learning rate: 0.0093
2024-09-16 09:50:07.284252: Validation loss did not improve from -0.51731. Patience: 22/50
2024-09-16 09:50:07.285691: train_loss -0.7285
2024-09-16 09:50:07.286652: val_loss -0.4198
2024-09-16 09:50:07.287637: Pseudo dice [0.7088]
2024-09-16 09:50:07.288655: Epoch time: 117.32 s
2024-09-16 09:50:08.778335: 
2024-09-16 09:50:08.779887: Epoch 78
2024-09-16 09:50:08.780997: Current learning rate: 0.0093
2024-09-16 09:52:15.228497: Validation loss did not improve from -0.51731. Patience: 23/50
2024-09-16 09:52:15.231045: train_loss -0.7359
2024-09-16 09:52:15.234312: val_loss -0.4491
2024-09-16 09:52:15.235882: Pseudo dice [0.7268]
2024-09-16 09:52:15.237247: Epoch time: 126.45 s
2024-09-16 09:52:16.884566: 
2024-09-16 09:52:16.886321: Epoch 79
2024-09-16 09:52:16.887646: Current learning rate: 0.00929
2024-09-16 09:54:22.579846: Validation loss did not improve from -0.51731. Patience: 24/50
2024-09-16 09:54:22.581863: train_loss -0.737
2024-09-16 09:54:22.583411: val_loss -0.4797
2024-09-16 09:54:22.584593: Pseudo dice [0.745]
2024-09-16 09:54:22.585710: Epoch time: 125.7 s
2024-09-16 09:54:23.032486: Yayy! New best EMA pseudo Dice: 0.7224
2024-09-16 09:54:25.178891: 
2024-09-16 09:54:25.180852: Epoch 80
2024-09-16 09:54:25.182325: Current learning rate: 0.00928
2024-09-16 09:56:23.017686: Validation loss did not improve from -0.51731. Patience: 25/50
2024-09-16 09:56:23.020746: train_loss -0.7385
2024-09-16 09:56:23.022150: val_loss -0.4512
2024-09-16 09:56:23.023174: Pseudo dice [0.7156]
2024-09-16 09:56:23.024108: Epoch time: 117.84 s
2024-09-16 09:56:24.626904: 
2024-09-16 09:56:24.629091: Epoch 81
2024-09-16 09:56:24.630692: Current learning rate: 0.00927
2024-09-16 09:58:35.119705: Validation loss did not improve from -0.51731. Patience: 26/50
2024-09-16 09:58:35.121103: train_loss -0.7415
2024-09-16 09:58:35.122342: val_loss -0.4331
2024-09-16 09:58:35.123484: Pseudo dice [0.7176]
2024-09-16 09:58:35.124490: Epoch time: 130.5 s
2024-09-16 09:58:36.667474: 
2024-09-16 09:58:36.668971: Epoch 82
2024-09-16 09:58:36.669964: Current learning rate: 0.00926
2024-09-16 10:00:41.565429: Validation loss did not improve from -0.51731. Patience: 27/50
2024-09-16 10:00:41.566780: train_loss -0.7463
2024-09-16 10:00:41.567950: val_loss -0.482
2024-09-16 10:00:41.568785: Pseudo dice [0.7394]
2024-09-16 10:00:41.569676: Epoch time: 124.9 s
2024-09-16 10:00:41.570487: Yayy! New best EMA pseudo Dice: 0.7231
2024-09-16 10:00:43.430165: 
2024-09-16 10:00:43.431770: Epoch 83
2024-09-16 10:00:43.432787: Current learning rate: 0.00925
2024-09-16 10:02:54.798116: Validation loss did not improve from -0.51731. Patience: 28/50
2024-09-16 10:02:54.799589: train_loss -0.7427
2024-09-16 10:02:54.800807: val_loss -0.4547
2024-09-16 10:02:54.801830: Pseudo dice [0.7279]
2024-09-16 10:02:54.802791: Epoch time: 131.37 s
2024-09-16 10:02:54.803662: Yayy! New best EMA pseudo Dice: 0.7236
2024-09-16 10:02:56.660328: 
2024-09-16 10:02:56.661896: Epoch 84
2024-09-16 10:02:56.662997: Current learning rate: 0.00924
2024-09-16 10:05:03.422618: Validation loss did not improve from -0.51731. Patience: 29/50
2024-09-16 10:05:03.423882: train_loss -0.7484
2024-09-16 10:05:03.424865: val_loss -0.3879
2024-09-16 10:05:03.425787: Pseudo dice [0.6972]
2024-09-16 10:05:03.426637: Epoch time: 126.77 s
2024-09-16 10:05:05.299895: 
2024-09-16 10:05:05.301502: Epoch 85
2024-09-16 10:05:05.302515: Current learning rate: 0.00923
2024-09-16 10:06:57.244201: Validation loss did not improve from -0.51731. Patience: 30/50
2024-09-16 10:06:57.245562: train_loss -0.7465
2024-09-16 10:06:57.246590: val_loss -0.4075
2024-09-16 10:06:57.247534: Pseudo dice [0.7081]
2024-09-16 10:06:57.248475: Epoch time: 111.95 s
2024-09-16 10:06:58.640403: 
2024-09-16 10:06:58.642493: Epoch 86
2024-09-16 10:06:58.643773: Current learning rate: 0.00922
2024-09-16 10:09:07.349078: Validation loss did not improve from -0.51731. Patience: 31/50
2024-09-16 10:09:07.350291: train_loss -0.7474
2024-09-16 10:09:07.351384: val_loss -0.4294
2024-09-16 10:09:07.352335: Pseudo dice [0.7154]
2024-09-16 10:09:07.353191: Epoch time: 128.71 s
2024-09-16 10:09:09.305213: 
2024-09-16 10:09:09.306995: Epoch 87
2024-09-16 10:09:09.308128: Current learning rate: 0.00921
2024-09-16 10:11:15.083107: Validation loss did not improve from -0.51731. Patience: 32/50
2024-09-16 10:11:15.084491: train_loss -0.7488
2024-09-16 10:11:15.085649: val_loss -0.4577
2024-09-16 10:11:15.086540: Pseudo dice [0.733]
2024-09-16 10:11:15.087579: Epoch time: 125.78 s
2024-09-16 10:11:16.524063: 
2024-09-16 10:11:16.525479: Epoch 88
2024-09-16 10:11:16.526459: Current learning rate: 0.0092
2024-09-16 10:13:52.060513: Validation loss did not improve from -0.51731. Patience: 33/50
2024-09-16 10:13:52.061748: train_loss -0.7551
2024-09-16 10:13:52.062994: val_loss -0.4489
2024-09-16 10:13:52.064048: Pseudo dice [0.7224]
2024-09-16 10:13:52.065243: Epoch time: 155.54 s
2024-09-16 10:13:53.429698: 
2024-09-16 10:13:53.431524: Epoch 89
2024-09-16 10:13:53.432585: Current learning rate: 0.0092
2024-09-16 10:16:24.516387: Validation loss did not improve from -0.51731. Patience: 34/50
2024-09-16 10:16:24.517401: train_loss -0.7533
2024-09-16 10:16:24.518424: val_loss -0.4763
2024-09-16 10:16:24.519525: Pseudo dice [0.749]
2024-09-16 10:16:24.520627: Epoch time: 151.09 s
2024-09-16 10:16:24.946417: Yayy! New best EMA pseudo Dice: 0.7236
2024-09-16 10:16:26.726083: 
2024-09-16 10:16:26.727507: Epoch 90
2024-09-16 10:16:26.728430: Current learning rate: 0.00919
2024-09-16 10:19:14.754493: Validation loss did not improve from -0.51731. Patience: 35/50
2024-09-16 10:19:14.755872: train_loss -0.7572
2024-09-16 10:19:14.757241: val_loss -0.4725
2024-09-16 10:19:14.758378: Pseudo dice [0.7455]
2024-09-16 10:19:14.759497: Epoch time: 168.03 s
2024-09-16 10:19:14.760653: Yayy! New best EMA pseudo Dice: 0.7258
2024-09-16 10:19:16.652755: 
2024-09-16 10:19:16.654514: Epoch 91
2024-09-16 10:19:16.655791: Current learning rate: 0.00918
2024-09-16 10:21:57.828738: Validation loss did not improve from -0.51731. Patience: 36/50
2024-09-16 10:21:57.829993: train_loss -0.7541
2024-09-16 10:21:57.831052: val_loss -0.4842
2024-09-16 10:21:57.832042: Pseudo dice [0.7435]
2024-09-16 10:21:57.832982: Epoch time: 161.18 s
2024-09-16 10:21:57.833907: Yayy! New best EMA pseudo Dice: 0.7276
2024-09-16 10:21:59.634340: 
2024-09-16 10:21:59.636000: Epoch 92
2024-09-16 10:21:59.637207: Current learning rate: 0.00917
2024-09-16 10:24:40.778368: Validation loss did not improve from -0.51731. Patience: 37/50
2024-09-16 10:24:40.779731: train_loss -0.7562
2024-09-16 10:24:40.781133: val_loss -0.3681
2024-09-16 10:24:40.782299: Pseudo dice [0.6888]
2024-09-16 10:24:40.783484: Epoch time: 161.15 s
2024-09-16 10:24:42.130167: 
2024-09-16 10:24:42.132160: Epoch 93
2024-09-16 10:24:42.133515: Current learning rate: 0.00916
2024-09-16 10:27:08.795762: Validation loss did not improve from -0.51731. Patience: 38/50
2024-09-16 10:27:08.797045: train_loss -0.7532
2024-09-16 10:27:08.798393: val_loss -0.3849
2024-09-16 10:27:08.799415: Pseudo dice [0.6901]
2024-09-16 10:27:08.800430: Epoch time: 146.67 s
2024-09-16 10:27:10.188243: 
2024-09-16 10:27:10.189950: Epoch 94
2024-09-16 10:27:10.191077: Current learning rate: 0.00915
2024-09-16 10:29:31.993522: Validation loss did not improve from -0.51731. Patience: 39/50
2024-09-16 10:29:31.994797: train_loss -0.759
2024-09-16 10:29:31.996093: val_loss -0.4117
2024-09-16 10:29:31.997888: Pseudo dice [0.7069]
2024-09-16 10:29:31.999211: Epoch time: 141.81 s
2024-09-16 10:29:33.813031: 
2024-09-16 10:29:33.814799: Epoch 95
2024-09-16 10:29:33.816030: Current learning rate: 0.00914
2024-09-16 10:32:02.651991: Validation loss did not improve from -0.51731. Patience: 40/50
2024-09-16 10:32:02.653347: train_loss -0.7591
2024-09-16 10:32:02.654614: val_loss -0.4148
2024-09-16 10:32:02.655636: Pseudo dice [0.7053]
2024-09-16 10:32:02.656528: Epoch time: 148.84 s
2024-09-16 10:32:04.021756: 
2024-09-16 10:32:04.023300: Epoch 96
2024-09-16 10:32:04.024278: Current learning rate: 0.00913
2024-09-16 10:34:29.826141: Validation loss did not improve from -0.51731. Patience: 41/50
2024-09-16 10:34:29.827513: train_loss -0.7634
2024-09-16 10:34:29.828571: val_loss -0.4583
2024-09-16 10:34:29.829549: Pseudo dice [0.7409]
2024-09-16 10:34:29.831826: Epoch time: 145.81 s
2024-09-16 10:34:31.235294: 
2024-09-16 10:34:31.236782: Epoch 97
2024-09-16 10:34:31.237834: Current learning rate: 0.00912
2024-09-16 10:36:46.393877: Validation loss did not improve from -0.51731. Patience: 42/50
2024-09-16 10:36:46.395557: train_loss -0.7625
2024-09-16 10:36:46.396962: val_loss -0.4689
2024-09-16 10:36:46.398111: Pseudo dice [0.7404]
2024-09-16 10:36:46.399218: Epoch time: 135.16 s
2024-09-16 10:36:47.778129: 
2024-09-16 10:36:47.779595: Epoch 98
2024-09-16 10:36:47.780615: Current learning rate: 0.00911
2024-09-16 10:39:15.212226: Validation loss did not improve from -0.51731. Patience: 43/50
2024-09-16 10:39:15.213603: train_loss -0.7637
2024-09-16 10:39:15.214655: val_loss -0.4373
2024-09-16 10:39:15.215573: Pseudo dice [0.719]
2024-09-16 10:39:15.216377: Epoch time: 147.44 s
2024-09-16 10:39:17.005693: 
2024-09-16 10:39:17.007128: Epoch 99
2024-09-16 10:39:17.008024: Current learning rate: 0.0091
2024-09-16 10:41:50.218553: Validation loss did not improve from -0.51731. Patience: 44/50
2024-09-16 10:41:50.219718: train_loss -0.7651
2024-09-16 10:41:50.220873: val_loss -0.3599
2024-09-16 10:41:50.222079: Pseudo dice [0.6886]
2024-09-16 10:41:50.223177: Epoch time: 153.22 s
2024-09-16 10:41:51.979091: 
2024-09-16 10:41:51.980780: Epoch 100
2024-09-16 10:41:51.982060: Current learning rate: 0.0091
2024-09-16 10:44:11.979940: Validation loss did not improve from -0.51731. Patience: 45/50
2024-09-16 10:44:11.981435: train_loss -0.7645
2024-09-16 10:44:11.982999: val_loss -0.4542
2024-09-16 10:44:11.984377: Pseudo dice [0.7449]
2024-09-16 10:44:11.985646: Epoch time: 140.0 s
2024-09-16 10:44:13.398391: 
2024-09-16 10:44:13.400320: Epoch 101
2024-09-16 10:44:13.401586: Current learning rate: 0.00909
2024-09-16 10:46:39.372554: Validation loss did not improve from -0.51731. Patience: 46/50
2024-09-16 10:46:39.373528: train_loss -0.7618
2024-09-16 10:46:39.374587: val_loss -0.4934
2024-09-16 10:46:39.375487: Pseudo dice [0.7571]
2024-09-16 10:46:39.376415: Epoch time: 145.98 s
2024-09-16 10:46:40.825090: 
2024-09-16 10:46:40.826651: Epoch 102
2024-09-16 10:46:40.827723: Current learning rate: 0.00908
2024-09-16 10:49:06.304308: Validation loss did not improve from -0.51731. Patience: 47/50
2024-09-16 10:49:06.305684: train_loss -0.7715
2024-09-16 10:49:06.306794: val_loss -0.4149
2024-09-16 10:49:06.307735: Pseudo dice [0.718]
2024-09-16 10:49:06.308655: Epoch time: 145.48 s
2024-09-16 10:49:07.697838: 
2024-09-16 10:49:07.699891: Epoch 103
2024-09-16 10:49:07.701166: Current learning rate: 0.00907
2024-09-16 10:51:34.140674: Validation loss did not improve from -0.51731. Patience: 48/50
2024-09-16 10:51:34.141952: train_loss -0.7653
2024-09-16 10:51:34.143179: val_loss -0.3931
2024-09-16 10:51:34.144027: Pseudo dice [0.7007]
2024-09-16 10:51:34.144941: Epoch time: 146.45 s
2024-09-16 10:51:35.516963: 
2024-09-16 10:51:35.518409: Epoch 104
2024-09-16 10:51:35.519441: Current learning rate: 0.00906
2024-09-16 10:54:03.038840: Validation loss did not improve from -0.51731. Patience: 49/50
2024-09-16 10:54:03.040097: train_loss -0.7662
2024-09-16 10:54:03.041186: val_loss -0.4775
2024-09-16 10:54:03.042120: Pseudo dice [0.7477]
2024-09-16 10:54:03.042958: Epoch time: 147.52 s
2024-09-16 10:54:04.867474: 
2024-09-16 10:54:04.868901: Epoch 105
2024-09-16 10:54:04.869909: Current learning rate: 0.00905
2024-09-16 10:56:33.000208: Validation loss did not improve from -0.51731. Patience: 50/50
2024-09-16 10:56:33.002136: train_loss -0.7583
2024-09-16 10:56:33.003280: val_loss -0.4651
2024-09-16 10:56:33.004132: Pseudo dice [0.7338]
2024-09-16 10:56:33.004954: Epoch time: 148.14 s
2024-09-16 10:56:34.375479: Patience reached. Stopping training.
2024-09-16 10:56:34.777818: Training done.
2024-09-16 10:56:34.923771: Using splits from existing split file: /home/gridsan/nchutisilp/datasets/nnUNet_Datasets/nnUNet_preprocessed/Dataset302_Calcium_OCTv2/splits_final.json
2024-09-16 10:56:34.925944: The split file contains 3 splits.
2024-09-16 10:56:34.927012: Desired fold for training: 1
2024-09-16 10:56:34.927883: This split has 4 training and 2 validation cases.
2024-09-16 10:56:34.928919: predicting 101-044
2024-09-16 10:56:34.967259: 101-044, shape torch.Size([1, 404, 498, 498]), rank 0
2024-09-16 10:59:02.147128: predicting 106-002
2024-09-16 10:59:02.165278: 106-002, shape torch.Size([1, 539, 498, 498]), rank 0
2024-09-16 11:02:31.851879: Validation complete
2024-09-16 11:02:31.854462: Mean Validation Dice:  0.7186096719360056
wandb: 
wandb: Run history:
wandb:            ema_fg_dice ▁▁▂▃▄▄▅▅▆▆▇▇▇▇▇▇▇▇▇▇▇▇▇█▇▇██████████████
wandb:   epoch_end_timestamps ▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇███
wandb: epoch_start_timestamps ▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇███
wandb:                    lrs ███▇▇▇▇▇▇▆▆▆▆▆▆▅▅▅▅▅▄▄▄▄▄▄▃▃▃▃▃▂▂▂▂▂▂▁▁▁
wandb:           mean_fg_dice ▁▄▄▅▆▆▇▅▆▇▆▆▇▇▇▇▇▆▆▇▆▆▆█▅▇▇█▇▇▇▇▇▇█▇█▆▇▇
wandb:           train_losses █▆▅▄▄▄▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:             val_losses █▅▅▄▃▂▂▃▃▂▃▃▁▁▂▃▂▃▃▂▃▄▄▁▅▂▂▁▂▂▂▂▃▂▁▃▂▄▃▂
wandb: 
wandb: Run summary:
wandb:            ema_fg_dice 0.7252
wandb:   epoch_end_timestamps 1726498593.00196
wandb: epoch_start_timestamps 1726498444.86567
wandb:                    lrs 0.00905
wandb:           mean_fg_dice 0.73376
wandb:           train_losses -0.75829
wandb:             val_losses -0.46513
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/gridsan/nchutisilp/datasets/nnUNet_Datasets/nnUNet_results/Dataset302_Calcium_OCTv2/nnUNetTrainer__nnUNetPlans__3d_32x160x128_b10/fold_1/wandb/offline-run-20240916_071622-9jkb4sk0
wandb: Find logs at: /home/gridsan/nchutisilp/datasets/nnUNet_Datasets/nnUNet_results/Dataset302_Calcium_OCTv2/nnUNetTrainer__nnUNetPlans__3d_32x160x128_b10/fold_1/wandb/offline-run-20240916_071622-9jkb4sk0/logs
WARNING:urllib3.connectionpool:Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x7f2cba20acd0>: Failed to establish a new connection: [Errno 101] Network is unreachable')': /api/4504800232407040/envelope/
WARNING:urllib3.connectionpool:Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x7f2ccaf2a220>: Failed to establish a new connection: [Errno 101] Network is unreachable')': /api/4504800232407040/envelope/
WARNING:urllib3.connectionpool:Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x7f2c843d7a30>: Failed to establish a new connection: [Errno 101] Network is unreachable')': /api/4504800232407040/envelope/
WARNING:urllib3.connectionpool:Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x7f2c84307820>: Failed to establish a new connection: [Errno 101] Network is unreachable')': /api/4504800232407040/envelope/
WARNING:urllib3.connectionpool:Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x7f2cb024d700>: Failed to establish a new connection: [Errno 101] Network is unreachable')': /api/4504800232407040/envelope/
WARNING:urllib3.connectionpool:Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x7f2cf778a550>: Failed to establish a new connection: [Errno 101] Network is unreachable')': /api/4504800232407040/envelope/
FOLD 1 CONFIG 3d_32x160x128_b10 TRAINER nnUNetTrainer

#######################################################################
Please cite the following paper when using nnU-Net:
Isensee, F., Jaeger, P. F., Kohl, S. A., Petersen, J., & Maier-Hein, K. H. (2021). nnU-Net: a self-configuring method for deep learning-based biomedical image segmentation. Nature methods, 18(2), 203-211.
#######################################################################

There are 2 cases in the source folder
I am process 0 out of 1 (max process ID is 0, we start counting with 0!)
There are 2 cases that I would like to predict
Traceback (most recent call last):
  File "/home/gridsan/nchutisilp/.conda/envs/nnunet/bin/nnUNetv2_predict", line 8, in <module>
    sys.exit(predict_entry_point())
  File "/home/gridsan/nchutisilp/projects/nnUNet/nnunetv2/inference/predict_from_raw_data.py", line 866, in predict_entry_point
    predictor.predict_from_files(args.i, args.o, save_probabilities=args.save_probabilities,
  File "/home/gridsan/nchutisilp/projects/nnUNet/nnunetv2/inference/predict_from_raw_data.py", line 258, in predict_from_files
    return self.predict_from_data_iterator(data_iterator, save_probabilities, num_processes_segmentation_export)
  File "/home/gridsan/nchutisilp/projects/nnUNet/nnunetv2/inference/predict_from_raw_data.py", line 351, in predict_from_data_iterator
    for preprocessed in data_iterator:
  File "/home/gridsan/nchutisilp/projects/nnUNet/nnunetv2/inference/data_iterators.py", line 117, in preprocessing_iterator_fromfiles
    [i.pin_memory() for i in item.values() if isinstance(i, torch.Tensor)]
  File "/home/gridsan/nchutisilp/projects/nnUNet/nnunetv2/inference/data_iterators.py", line 117, in <listcomp>
    [i.pin_memory() for i in item.values() if isinstance(i, torch.Tensor)]
RuntimeError: CUDA error: no CUDA-capable device is detected
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Device-side assertions were explicitly omitted for this error check; the error probably arose while initializing the DSA handlers.
Traceback (most recent call last):
  File "/home/gridsan/nchutisilp/.conda/envs/nnunet/bin/nnUNetv2_evaluate_simple", line 8, in <module>
    sys.exit(evaluate_simple_entry_point())
  File "/home/gridsan/nchutisilp/projects/nnUNet/nnunetv2/evaluation/evaluate_predictions.py", line 250, in evaluate_simple_entry_point
    compute_metrics_on_folder_simple(args.gt_folder, args.pred_folder, args.l, args.o, args.np, args.il, chill=args.chill)
  File "/home/gridsan/nchutisilp/projects/nnUNet/nnunetv2/evaluation/evaluate_predictions.py", line 212, in compute_metrics_on_folder_simple
    compute_metrics_on_folder(folder_ref, folder_pred, output_file, rw, file_ending,
  File "/home/gridsan/nchutisilp/projects/nnUNet/nnunetv2/evaluation/evaluate_predictions.py", line 138, in compute_metrics_on_folder
    assert all(present), "Not all files in folder_ref exist in folder_pred"
AssertionError: Not all files in folder_ref exist in folder_pred
