/home/gridsan/nchutisilp/.conda/envs/nnunet/bin/python
wandb: Tracking run with wandb version 0.16.6
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.

############################
INFO: You are using the old nnU-Net default plans. We have updated our recommendations. Please consider using those instead! Read more here: https://github.com/MIC-DKFZ/nnUNet/blob/master/documentation/resenc_presets.md
############################

Using device: cuda:0

#######################################################################
Please cite the following paper when using nnU-Net:
Isensee, F., Jaeger, P. F., Kohl, S. A., Petersen, J., & Maier-Hein, K. H. (2021). nnU-Net: a self-configuring method for deep learning-based biomedical image segmentation. Nature methods, 18(2), 203-211.
#######################################################################

2024-05-07 17:08:52.762310: Using torch.compile...
/home/gridsan/nchutisilp/.local/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn("The verbose parameter is deprecated. Please use get_last_lr() "
################### Loading pretrained weights from file  /home/gridsan/nchutisilp/datasets/nnUNet_Datasets/nnUNet_results/Dataset300_Lumen_and_Wall_OCT/nnUNetTrainer__nnUNetPreprocessPlans__3d_fullres/fold_all/checkpoint_best.pth ###################
Below is the list of overlapping blocks in pretrained model and nnUNet architecture:
encoder.stages.0.0.convs.0.conv.weight shape torch.Size([32, 1, 3, 3, 3])
encoder.stages.0.0.convs.0.conv.bias shape torch.Size([32])
encoder.stages.0.0.convs.0.norm.weight shape torch.Size([32])
encoder.stages.0.0.convs.0.norm.bias shape torch.Size([32])
encoder.stages.0.0.convs.0.all_modules.0.weight shape torch.Size([32, 1, 3, 3, 3])
encoder.stages.0.0.convs.0.all_modules.0.bias shape torch.Size([32])
encoder.stages.0.0.convs.0.all_modules.1.weight shape torch.Size([32])
encoder.stages.0.0.convs.0.all_modules.1.bias shape torch.Size([32])
encoder.stages.0.0.convs.1.conv.weight shape torch.Size([32, 32, 3, 3, 3])
encoder.stages.0.0.convs.1.conv.bias shape torch.Size([32])
encoder.stages.0.0.convs.1.norm.weight shape torch.Size([32])
encoder.stages.0.0.convs.1.norm.bias shape torch.Size([32])
encoder.stages.0.0.convs.1.all_modules.0.weight shape torch.Size([32, 32, 3, 3, 3])
encoder.stages.0.0.convs.1.all_modules.0.bias shape torch.Size([32])
encoder.stages.0.0.convs.1.all_modules.1.weight shape torch.Size([32])
encoder.stages.0.0.convs.1.all_modules.1.bias shape torch.Size([32])
encoder.stages.1.0.convs.0.conv.weight shape torch.Size([64, 32, 3, 3, 3])
encoder.stages.1.0.convs.0.conv.bias shape torch.Size([64])
encoder.stages.1.0.convs.0.norm.weight shape torch.Size([64])
encoder.stages.1.0.convs.0.norm.bias shape torch.Size([64])
encoder.stages.1.0.convs.0.all_modules.0.weight shape torch.Size([64, 32, 3, 3, 3])
encoder.stages.1.0.convs.0.all_modules.0.bias shape torch.Size([64])
encoder.stages.1.0.convs.0.all_modules.1.weight shape torch.Size([64])
encoder.stages.1.0.convs.0.all_modules.1.bias shape torch.Size([64])
encoder.stages.1.0.convs.1.conv.weight shape torch.Size([64, 64, 3, 3, 3])
encoder.stages.1.0.convs.1.conv.bias shape torch.Size([64])
encoder.stages.1.0.convs.1.norm.weight shape torch.Size([64])
encoder.stages.1.0.convs.1.norm.bias shape torch.Size([64])
encoder.stages.1.0.convs.1.all_modules.0.weight shape torch.Size([64, 64, 3, 3, 3])
encoder.stages.1.0.convs.1.all_modules.0.bias shape torch.Size([64])
encoder.stages.1.0.convs.1.all_modules.1.weight shape torch.Size([64])
encoder.stages.1.0.convs.1.all_modules.1.bias shape torch.Size([64])
encoder.stages.2.0.convs.0.conv.weight shape torch.Size([128, 64, 3, 3, 3])
encoder.stages.2.0.convs.0.conv.bias shape torch.Size([128])
encoder.stages.2.0.convs.0.norm.weight shape torch.Size([128])
encoder.stages.2.0.convs.0.norm.bias shape torch.Size([128])
encoder.stages.2.0.convs.0.all_modules.0.weight shape torch.Size([128, 64, 3, 3, 3])
encoder.stages.2.0.convs.0.all_modules.0.bias shape torch.Size([128])
encoder.stages.2.0.convs.0.all_modules.1.weight shape torch.Size([128])
encoder.stages.2.0.convs.0.all_modules.1.bias shape torch.Size([128])
encoder.stages.2.0.convs.1.conv.weight shape torch.Size([128, 128, 3, 3, 3])
encoder.stages.2.0.convs.1.conv.bias shape torch.Size([128])
encoder.stages.2.0.convs.1.norm.weight shape torch.Size([128])
encoder.stages.2.0.convs.1.norm.bias shape torch.Size([128])
encoder.stages.2.0.convs.1.all_modules.0.weight shape torch.Size([128, 128, 3, 3, 3])
encoder.stages.2.0.convs.1.all_modules.0.bias shape torch.Size([128])
encoder.stages.2.0.convs.1.all_modules.1.weight shape torch.Size([128])
encoder.stages.2.0.convs.1.all_modules.1.bias shape torch.Size([128])
encoder.stages.3.0.convs.0.conv.weight shape torch.Size([256, 128, 3, 3, 3])
encoder.stages.3.0.convs.0.conv.bias shape torch.Size([256])
encoder.stages.3.0.convs.0.norm.weight shape torch.Size([256])
encoder.stages.3.0.convs.0.norm.bias shape torch.Size([256])
encoder.stages.3.0.convs.0.all_modules.0.weight shape torch.Size([256, 128, 3, 3, 3])
encoder.stages.3.0.convs.0.all_modules.0.bias shape torch.Size([256])
encoder.stages.3.0.convs.0.all_modules.1.weight shape torch.Size([256])
encoder.stages.3.0.convs.0.all_modules.1.bias shape torch.Size([256])
encoder.stages.3.0.convs.1.conv.weight shape torch.Size([256, 256, 3, 3, 3])
encoder.stages.3.0.convs.1.conv.bias shape torch.Size([256])
encoder.stages.3.0.convs.1.norm.weight shape torch.Size([256])
encoder.stages.3.0.convs.1.norm.bias shape torch.Size([256])
encoder.stages.3.0.convs.1.all_modules.0.weight shape torch.Size([256, 256, 3, 3, 3])
encoder.stages.3.0.convs.1.all_modules.0.bias shape torch.Size([256])
encoder.stages.3.0.convs.1.all_modules.1.weight shape torch.Size([256])
encoder.stages.3.0.convs.1.all_modules.1.bias shape torch.Size([256])
encoder.stages.4.0.convs.0.conv.weight shape torch.Size([320, 256, 3, 3, 3])
encoder.stages.4.0.convs.0.conv.bias shape torch.Size([320])
encoder.stages.4.0.convs.0.norm.weight shape torch.Size([320])
encoder.stages.4.0.convs.0.norm.bias shape torch.Size([320])
encoder.stages.4.0.convs.0.all_modules.0.weight shape torch.Size([320, 256, 3, 3, 3])
encoder.stages.4.0.convs.0.all_modules.0.bias shape torch.Size([320])
encoder.stages.4.0.convs.0.all_modules.1.weight shape torch.Size([320])
encoder.stages.4.0.convs.0.all_modules.1.bias shape torch.Size([320])
encoder.stages.4.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.4.0.convs.1.conv.bias shape torch.Size([320])
encoder.stages.4.0.convs.1.norm.weight shape torch.Size([320])
encoder.stages.4.0.convs.1.norm.bias shape torch.Size([320])
encoder.stages.4.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.4.0.convs.1.all_modules.0.bias shape torch.Size([320])
encoder.stages.4.0.convs.1.all_modules.1.weight shape torch.Size([320])
encoder.stages.4.0.convs.1.all_modules.1.bias shape torch.Size([320])
encoder.stages.5.0.convs.0.conv.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.5.0.convs.0.conv.bias shape torch.Size([320])
encoder.stages.5.0.convs.0.norm.weight shape torch.Size([320])
encoder.stages.5.0.convs.0.norm.bias shape torch.Size([320])
encoder.stages.5.0.convs.0.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.5.0.convs.0.all_modules.0.bias shape torch.Size([320])
encoder.stages.5.0.convs.0.all_modules.1.weight shape torch.Size([320])
encoder.stages.5.0.convs.0.all_modules.1.bias shape torch.Size([320])
encoder.stages.5.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.5.0.convs.1.conv.bias shape torch.Size([320])
encoder.stages.5.0.convs.1.norm.weight shape torch.Size([320])
encoder.stages.5.0.convs.1.norm.bias shape torch.Size([320])
encoder.stages.5.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.5.0.convs.1.all_modules.0.bias shape torch.Size([320])
encoder.stages.5.0.convs.1.all_modules.1.weight shape torch.Size([320])
encoder.stages.5.0.convs.1.all_modules.1.bias shape torch.Size([320])
decoder.encoder.stages.0.0.convs.0.conv.weight shape torch.Size([32, 1, 3, 3, 3])
decoder.encoder.stages.0.0.convs.0.conv.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.norm.weight shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.norm.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.all_modules.0.weight shape torch.Size([32, 1, 3, 3, 3])
decoder.encoder.stages.0.0.convs.0.all_modules.0.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.all_modules.1.weight shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.all_modules.1.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.conv.weight shape torch.Size([32, 32, 3, 3, 3])
decoder.encoder.stages.0.0.convs.1.conv.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.norm.weight shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.norm.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.all_modules.0.weight shape torch.Size([32, 32, 3, 3, 3])
decoder.encoder.stages.0.0.convs.1.all_modules.0.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.all_modules.1.weight shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.all_modules.1.bias shape torch.Size([32])
decoder.encoder.stages.1.0.convs.0.conv.weight shape torch.Size([64, 32, 3, 3, 3])
decoder.encoder.stages.1.0.convs.0.conv.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.norm.weight shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.norm.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.all_modules.0.weight shape torch.Size([64, 32, 3, 3, 3])
decoder.encoder.stages.1.0.convs.0.all_modules.0.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.all_modules.1.weight shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.all_modules.1.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.conv.weight shape torch.Size([64, 64, 3, 3, 3])
decoder.encoder.stages.1.0.convs.1.conv.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.norm.weight shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.norm.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.all_modules.0.weight shape torch.Size([64, 64, 3, 3, 3])
decoder.encoder.stages.1.0.convs.1.all_modules.0.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.all_modules.1.weight shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.all_modules.1.bias shape torch.Size([64])
decoder.encoder.stages.2.0.convs.0.conv.weight shape torch.Size([128, 64, 3, 3, 3])
decoder.encoder.stages.2.0.convs.0.conv.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.norm.weight shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.norm.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.all_modules.0.weight shape torch.Size([128, 64, 3, 3, 3])
decoder.encoder.stages.2.0.convs.0.all_modules.0.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.all_modules.1.weight shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.all_modules.1.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.conv.weight shape torch.Size([128, 128, 3, 3, 3])
decoder.encoder.stages.2.0.convs.1.conv.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.norm.weight shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.norm.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.all_modules.0.weight shape torch.Size([128, 128, 3, 3, 3])
decoder.encoder.stages.2.0.convs.1.all_modules.0.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.all_modules.1.weight shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.all_modules.1.bias shape torch.Size([128])
decoder.encoder.stages.3.0.convs.0.conv.weight shape torch.Size([256, 128, 3, 3, 3])
decoder.encoder.stages.3.0.convs.0.conv.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.norm.weight shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.norm.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.all_modules.0.weight shape torch.Size([256, 128, 3, 3, 3])
decoder.encoder.stages.3.0.convs.0.all_modules.0.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.all_modules.1.weight shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.all_modules.1.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.conv.weight shape torch.Size([256, 256, 3, 3, 3])
decoder.encoder.stages.3.0.convs.1.conv.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.norm.weight shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.norm.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.all_modules.0.weight shape torch.Size([256, 256, 3, 3, 3])
decoder.encoder.stages.3.0.convs.1.all_modules.0.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.all_modules.1.weight shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.all_modules.1.bias shape torch.Size([256])
decoder.encoder.stages.4.0.convs.0.conv.weight shape torch.Size([320, 256, 3, 3, 3])
decoder.encoder.stages.4.0.convs.0.conv.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.norm.weight shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.norm.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.all_modules.0.weight shape torch.Size([320, 256, 3, 3, 3])
decoder.encoder.stages.4.0.convs.0.all_modules.0.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.all_modules.1.weight shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.all_modules.1.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.4.0.convs.1.conv.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.norm.weight shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.norm.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.4.0.convs.1.all_modules.0.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.all_modules.1.weight shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.all_modules.1.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.conv.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.5.0.convs.0.conv.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.norm.weight shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.norm.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.5.0.convs.0.all_modules.0.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.all_modules.1.weight shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.all_modules.1.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.5.0.convs.1.conv.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.norm.weight shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.norm.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.5.0.convs.1.all_modules.0.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.all_modules.1.weight shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.all_modules.1.bias shape torch.Size([320])
decoder.stages.0.convs.0.conv.weight shape torch.Size([320, 640, 3, 3, 3])
decoder.stages.0.convs.0.conv.bias shape torch.Size([320])
decoder.stages.0.convs.0.norm.weight shape torch.Size([320])
decoder.stages.0.convs.0.norm.bias shape torch.Size([320])
decoder.stages.0.convs.0.all_modules.0.weight shape torch.Size([320, 640, 3, 3, 3])
decoder.stages.0.convs.0.all_modules.0.bias shape torch.Size([320])
decoder.stages.0.convs.0.all_modules.1.weight shape torch.Size([320])
decoder.stages.0.convs.0.all_modules.1.bias shape torch.Size([320])
decoder.stages.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.stages.0.convs.1.conv.bias shape torch.Size([320])
decoder.stages.0.convs.1.norm.weight shape torch.Size([320])
decoder.stages.0.convs.1.norm.bias shape torch.Size([320])
decoder.stages.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.stages.0.convs.1.all_modules.0.bias shape torch.Size([320])
decoder.stages.0.convs.1.all_modules.1.weight shape torch.Size([320])
decoder.stages.0.convs.1.all_modules.1.bias shape torch.Size([320])
decoder.stages.1.convs.0.conv.weight shape torch.Size([256, 512, 3, 3, 3])
decoder.stages.1.convs.0.conv.bias shape torch.Size([256])
decoder.stages.1.convs.0.norm.weight shape torch.Size([256])
decoder.stages.1.convs.0.norm.bias shape torch.Size([256])
decoder.stages.1.convs.0.all_modules.0.weight shape torch.Size([256, 512, 3, 3, 3])
decoder.stages.1.convs.0.all_modules.0.bias shape torch.Size([256])
decoder.stages.1.convs.0.all_modules.1.weight shape torch.Size([256])
decoder.stages.1.convs.0.all_modules.1.bias shape torch.Size([256])
decoder.stages.1.convs.1.conv.weight shape torch.Size([256, 256, 3, 3, 3])
decoder.stages.1.convs.1.conv.bias shape torch.Size([256])
decoder.stages.1.convs.1.norm.weight shape torch.Size([256])
decoder.stages.1.convs.1.norm.bias shape torch.Size([256])
decoder.stages.1.convs.1.all_modules.0.weight shape torch.Size([256, 256, 3, 3, 3])
decoder.stages.1.convs.1.all_modules.0.bias shape torch.Size([256])
decoder.stages.1.convs.1.all_modules.1.weight shape torch.Size([256])
decoder.stages.1.convs.1.all_modules.1.bias shape torch.Size([256])
decoder.stages.2.convs.0.conv.weight shape torch.Size([128, 256, 3, 3, 3])
decoder.stages.2.convs.0.conv.bias shape torch.Size([128])
decoder.stages.2.convs.0.norm.weight shape torch.Size([128])
decoder.stages.2.convs.0.norm.bias shape torch.Size([128])
decoder.stages.2.convs.0.all_modules.0.weight shape torch.Size([128, 256, 3, 3, 3])
decoder.stages.2.convs.0.all_modules.0.bias shape torch.Size([128])
decoder.stages.2.convs.0.all_modules.1.weight shape torch.Size([128])
decoder.stages.2.convs.0.all_modules.1.bias shape torch.Size([128])
decoder.stages.2.convs.1.conv.weight shape torch.Size([128, 128, 3, 3, 3])
decoder.stages.2.convs.1.conv.bias shape torch.Size([128])
decoder.stages.2.convs.1.norm.weight shape torch.Size([128])
decoder.stages.2.convs.1.norm.bias shape torch.Size([128])
decoder.stages.2.convs.1.all_modules.0.weight shape torch.Size([128, 128, 3, 3, 3])
decoder.stages.2.convs.1.all_modules.0.bias shape torch.Size([128])
decoder.stages.2.convs.1.all_modules.1.weight shape torch.Size([128])
decoder.stages.2.convs.1.all_modules.1.bias shape torch.Size([128])
decoder.stages.3.convs.0.conv.weight shape torch.Size([64, 128, 3, 3, 3])
decoder.stages.3.convs.0.conv.bias shape torch.Size([64])
decoder.stages.3.convs.0.norm.weight shape torch.Size([64])
decoder.stages.3.convs.0.norm.bias shape torch.Size([64])
decoder.stages.3.convs.0.all_modules.0.weight shape torch.Size([64, 128, 3, 3, 3])
decoder.stages.3.convs.0.all_modules.0.bias shape torch.Size([64])
decoder.stages.3.convs.0.all_modules.1.weight shape torch.Size([64])
decoder.stages.3.convs.0.all_modules.1.bias shape torch.Size([64])
decoder.stages.3.convs.1.conv.weight shape torch.Size([64, 64, 3, 3, 3])
decoder.stages.3.convs.1.conv.bias shape torch.Size([64])
decoder.stages.3.convs.1.norm.weight shape torch.Size([64])
decoder.stages.3.convs.1.norm.bias shape torch.Size([64])
decoder.stages.3.convs.1.all_modules.0.weight shape torch.Size([64, 64, 3, 3, 3])
decoder.stages.3.convs.1.all_modules.0.bias shape torch.Size([64])
decoder.stages.3.convs.1.all_modules.1.weight shape torch.Size([64])
decoder.stages.3.convs.1.all_modules.1.bias shape torch.Size([64])
decoder.stages.4.convs.0.conv.weight shape torch.Size([32, 64, 3, 3, 3])
decoder.stages.4.convs.0.conv.bias shape torch.Size([32])
decoder.stages.4.convs.0.norm.weight shape torch.Size([32])
decoder.stages.4.convs.0.norm.bias shape torch.Size([32])
decoder.stages.4.convs.0.all_modules.0.weight shape torch.Size([32, 64, 3, 3, 3])
decoder.stages.4.convs.0.all_modules.0.bias shape torch.Size([32])
decoder.stages.4.convs.0.all_modules.1.weight shape torch.Size([32])
decoder.stages.4.convs.0.all_modules.1.bias shape torch.Size([32])
decoder.stages.4.convs.1.conv.weight shape torch.Size([32, 32, 3, 3, 3])
decoder.stages.4.convs.1.conv.bias shape torch.Size([32])
decoder.stages.4.convs.1.norm.weight shape torch.Size([32])
decoder.stages.4.convs.1.norm.bias shape torch.Size([32])
decoder.stages.4.convs.1.all_modules.0.weight shape torch.Size([32, 32, 3, 3, 3])
decoder.stages.4.convs.1.all_modules.0.bias shape torch.Size([32])
decoder.stages.4.convs.1.all_modules.1.weight shape torch.Size([32])
decoder.stages.4.convs.1.all_modules.1.bias shape torch.Size([32])
decoder.transpconvs.0.weight shape torch.Size([320, 320, 1, 2, 2])
decoder.transpconvs.0.bias shape torch.Size([320])
decoder.transpconvs.1.weight shape torch.Size([320, 256, 2, 2, 2])
decoder.transpconvs.1.bias shape torch.Size([256])
decoder.transpconvs.2.weight shape torch.Size([256, 128, 2, 2, 2])
decoder.transpconvs.2.bias shape torch.Size([128])
decoder.transpconvs.3.weight shape torch.Size([128, 64, 2, 2, 2])
decoder.transpconvs.3.bias shape torch.Size([64])
decoder.transpconvs.4.weight shape torch.Size([64, 32, 2, 2, 2])
decoder.transpconvs.4.bias shape torch.Size([32])
################### Done ###################
2024-05-07 17:09:13.993020: do_dummy_2d_data_aug: True
2024-05-07 17:09:13.996129: Using splits from existing split file: /home/gridsan/nchutisilp/datasets/nnUNet_Datasets/nnUNet_preprocessed/Dataset302_Calcium_OCTv2/splits_final.json
2024-05-07 17:09:14.014206: The split file contains 3 splits.
2024-05-07 17:09:14.016837: Desired fold for training: 0
2024-05-07 17:09:14.018036: This split has 4 training and 2 validation cases.
using pin_memory on device 0
using pin_memory on device 0

This is the configuration used by this training:
Configuration name: 3d_128x512x512_b10
 {'data_identifier': 'nnUNetPlans_3d_fullres', 'preprocessor_name': 'DefaultPreprocessor', 'batch_size': 2, 'patch_size': [32, 512, 512], 'median_image_size_in_voxels': [375.0, 498.0, 498.0], 'spacing': [1.0, 1.0, 1.0], 'normalization_schemes': ['ZScoreNormalization'], 'use_mask_for_norm': [False], 'resampling_fn_data': 'resample_data_or_seg_to_shape', 'resampling_fn_seg': 'resample_data_or_seg_to_shape', 'resampling_fn_data_kwargs': {'is_seg': False, 'order': 3, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_seg_kwargs': {'is_seg': True, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_probabilities': 'resample_data_or_seg_to_shape', 'resampling_fn_probabilities_kwargs': {'is_seg': False, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'architecture': {'network_class_name': 'dynamic_network_architectures.architectures.unet.PlainConvUNet', 'arch_kwargs': {'n_stages': 6, 'features_per_stage': [32, 64, 128, 256, 320, 320], 'conv_op': 'torch.nn.modules.conv.Conv3d', 'kernel_sizes': [[3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3]], 'strides': [[1, 1, 1], [2, 2, 2], [2, 2, 2], [2, 2, 2], [2, 2, 2], [1, 2, 2]], 'n_conv_per_stage': [2, 2, 2, 2, 2, 2], 'n_conv_per_stage_decoder': [2, 2, 2, 2, 2], 'conv_bias': True, 'norm_op': 'torch.nn.modules.instancenorm.InstanceNorm3d', 'norm_op_kwargs': {'eps': 1e-05, 'affine': True}, 'dropout_op': None, 'dropout_op_kwargs': None, 'nonlin': 'torch.nn.LeakyReLU', 'nonlin_kwargs': {'inplace': True}, 'deep_supervision': True}, '_kw_requires_import': ['conv_op', 'norm_op', 'dropout_op', 'nonlin']}, 'batch_dice': True, 'inherits_from': '3d_fullres'} 

These are the global plan.json settings:
 {'dataset_name': 'Dataset302_Calcium_OCTv2', 'plans_name': 'nnUNetPlans', 'original_median_spacing_after_transp': [1.0, 1.0, 1.0], 'original_median_shape_after_transp': [375, 498, 498], 'image_reader_writer': 'SimpleITKIO', 'transpose_forward': [0, 1, 2], 'transpose_backward': [0, 1, 2], 'experiment_planner_used': 'ExperimentPlanner', 'label_manager': 'LabelManager', 'foreground_intensity_properties_per_channel': {'0': {'max': 0.4977976381778717, 'mean': 0.13332499563694, 'median': 0.08871842920780182, 'min': 0.0, 'percentile_00_5': 0.014754901640117168, 'percentile_99_5': 0.4977976381778717, 'std': 0.12022686749696732}}} 

2024-05-07 17:09:30.877746: unpacking dataset...
2024-05-07 17:09:37.265848: unpacking done...
2024-05-07 17:09:37.783862: Unable to plot network architecture: nnUNet_compile is enabled!
2024-05-07 17:09:38.116857: 
2024-05-07 17:09:38.118454: Epoch 0
2024-05-07 17:09:38.119734: Current learning rate: 0.01
2024-05-07 17:17:06.283807: Validation loss improved from 1000.00000 to -0.45262! Patience: 0/50
2024-05-07 17:17:06.297641: train_loss -0.4361
2024-05-07 17:17:06.306369: val_loss -0.4526
2024-05-07 17:17:06.307771: Pseudo dice [0.6163]
2024-05-07 17:17:06.309168: Epoch time: 448.17 s
2024-05-07 17:17:06.310453: Yayy! New best EMA pseudo Dice: 0.6163
2024-05-07 17:17:08.779032: 
2024-05-07 17:17:08.780771: Epoch 1
2024-05-07 17:17:08.781979: Current learning rate: 0.00999
2024-05-07 17:21:03.569762: Validation loss improved from -0.45262 to -0.48828! Patience: 0/50
2024-05-07 17:21:03.571309: train_loss -0.5782
2024-05-07 17:21:03.572480: val_loss -0.4883
2024-05-07 17:21:03.573526: Pseudo dice [0.6436]
2024-05-07 17:21:03.574634: Epoch time: 234.79 s
2024-05-07 17:21:03.575705: Yayy! New best EMA pseudo Dice: 0.619
2024-05-07 17:21:05.147255: 
2024-05-07 17:21:05.148854: Epoch 2
2024-05-07 17:21:05.150014: Current learning rate: 0.00998
2024-05-07 17:25:00.108373: Validation loss improved from -0.48828 to -0.53332! Patience: 0/50
2024-05-07 17:25:00.109775: train_loss -0.6044
2024-05-07 17:25:00.111038: val_loss -0.5333
2024-05-07 17:25:00.112236: Pseudo dice [0.6725]
2024-05-07 17:25:00.113240: Epoch time: 234.96 s
2024-05-07 17:25:00.114385: Yayy! New best EMA pseudo Dice: 0.6244
2024-05-07 17:25:01.741870: 
2024-05-07 17:25:01.743744: Epoch 3
2024-05-07 17:25:01.744770: Current learning rate: 0.00997
2024-05-07 17:28:56.700154: Validation loss improved from -0.53332 to -0.54091! Patience: 0/50
2024-05-07 17:28:56.702068: train_loss -0.6293
2024-05-07 17:28:56.703768: val_loss -0.5409
2024-05-07 17:28:56.705242: Pseudo dice [0.6822]
2024-05-07 17:28:56.706652: Epoch time: 234.96 s
2024-05-07 17:28:56.707987: Yayy! New best EMA pseudo Dice: 0.6302
2024-05-07 17:28:58.256560: 
2024-05-07 17:28:58.258355: Epoch 4
2024-05-07 17:28:58.259496: Current learning rate: 0.00996
2024-05-07 17:32:53.276817: Validation loss improved from -0.54091 to -0.56851! Patience: 0/50
2024-05-07 17:32:53.278097: train_loss -0.6344
2024-05-07 17:32:53.279235: val_loss -0.5685
2024-05-07 17:32:53.280262: Pseudo dice [0.696]
2024-05-07 17:32:53.281330: Epoch time: 235.02 s
2024-05-07 17:32:53.662694: Yayy! New best EMA pseudo Dice: 0.6368
2024-05-07 17:32:55.311684: 
2024-05-07 17:32:55.313660: Epoch 5
2024-05-07 17:32:55.314971: Current learning rate: 0.00995
2024-05-07 17:36:50.471564: Validation loss did not improve from -0.56851. Patience: 1/50
2024-05-07 17:36:50.472716: train_loss -0.6357
2024-05-07 17:36:50.473757: val_loss -0.5461
2024-05-07 17:36:50.474738: Pseudo dice [0.6765]
2024-05-07 17:36:50.475688: Epoch time: 235.16 s
2024-05-07 17:36:50.476538: Yayy! New best EMA pseudo Dice: 0.6407
2024-05-07 17:36:52.076200: 
2024-05-07 17:36:52.078209: Epoch 6
2024-05-07 17:36:52.079775: Current learning rate: 0.00995
2024-05-07 17:40:47.537430: Validation loss did not improve from -0.56851. Patience: 2/50
2024-05-07 17:40:47.539241: train_loss -0.6697
2024-05-07 17:40:47.540653: val_loss -0.5578
2024-05-07 17:40:47.541828: Pseudo dice [0.6897]
2024-05-07 17:40:47.542869: Epoch time: 235.46 s
2024-05-07 17:40:47.543826: Yayy! New best EMA pseudo Dice: 0.6456
2024-05-07 17:40:49.119537: 
2024-05-07 17:40:49.122182: Epoch 7
2024-05-07 17:40:49.123419: Current learning rate: 0.00994
2024-05-07 17:44:44.395466: Validation loss improved from -0.56851 to -0.59079! Patience: 2/50
2024-05-07 17:44:44.396766: train_loss -0.6541
2024-05-07 17:44:44.398123: val_loss -0.5908
2024-05-07 17:44:44.399384: Pseudo dice [0.7115]
2024-05-07 17:44:44.400589: Epoch time: 235.28 s
2024-05-07 17:44:44.401895: Yayy! New best EMA pseudo Dice: 0.6522
2024-05-07 17:44:46.410158: 
2024-05-07 17:44:46.411749: Epoch 8
2024-05-07 17:44:46.413078: Current learning rate: 0.00993
2024-05-07 17:48:42.457036: Validation loss did not improve from -0.59079. Patience: 1/50
2024-05-07 17:48:42.458314: train_loss -0.6874
2024-05-07 17:48:42.459689: val_loss -0.5844
2024-05-07 17:48:42.460854: Pseudo dice [0.7092]
2024-05-07 17:48:42.462017: Epoch time: 236.05 s
2024-05-07 17:48:42.463095: Yayy! New best EMA pseudo Dice: 0.6579
2024-05-07 17:48:44.072209: 
2024-05-07 17:48:44.074008: Epoch 9
2024-05-07 17:48:44.075871: Current learning rate: 0.00992
2024-05-07 17:52:39.053926: Validation loss did not improve from -0.59079. Patience: 2/50
2024-05-07 17:52:39.054992: train_loss -0.6918
2024-05-07 17:52:39.056294: val_loss -0.5466
2024-05-07 17:52:39.057567: Pseudo dice [0.6926]
2024-05-07 17:52:39.058957: Epoch time: 234.98 s
2024-05-07 17:52:39.449913: Yayy! New best EMA pseudo Dice: 0.6614
2024-05-07 17:52:41.051511: 
2024-05-07 17:52:41.053324: Epoch 10
2024-05-07 17:52:41.054435: Current learning rate: 0.00991
2024-05-07 17:56:35.367298: Validation loss improved from -0.59079 to -0.61891! Patience: 2/50
2024-05-07 17:56:35.368402: train_loss -0.7097
2024-05-07 17:56:35.369644: val_loss -0.6189
2024-05-07 17:56:35.370824: Pseudo dice [0.7316]
2024-05-07 17:56:35.371898: Epoch time: 234.32 s
2024-05-07 17:56:35.372875: Yayy! New best EMA pseudo Dice: 0.6684
2024-05-07 17:56:36.997428: 
2024-05-07 17:56:36.998916: Epoch 11
2024-05-07 17:56:37.000109: Current learning rate: 0.0099
2024-05-07 18:00:30.585768: Validation loss did not improve from -0.61891. Patience: 1/50
2024-05-07 18:00:30.586859: train_loss -0.7155
2024-05-07 18:00:30.587822: val_loss -0.5667
2024-05-07 18:00:30.588829: Pseudo dice [0.7083]
2024-05-07 18:00:30.589789: Epoch time: 233.59 s
2024-05-07 18:00:30.590781: Yayy! New best EMA pseudo Dice: 0.6724
2024-05-07 18:00:32.286690: 
2024-05-07 18:00:32.287929: Epoch 12
2024-05-07 18:00:32.289054: Current learning rate: 0.00989
2024-05-07 18:04:25.870619: Validation loss did not improve from -0.61891. Patience: 2/50
2024-05-07 18:04:25.871817: train_loss -0.7205
2024-05-07 18:04:25.873171: val_loss -0.5978
2024-05-07 18:04:25.874331: Pseudo dice [0.7109]
2024-05-07 18:04:25.875573: Epoch time: 233.59 s
2024-05-07 18:04:25.876756: Yayy! New best EMA pseudo Dice: 0.6762
2024-05-07 18:04:27.614809: 
2024-05-07 18:04:27.616924: Epoch 13
2024-05-07 18:04:27.618237: Current learning rate: 0.00988
2024-05-07 18:08:23.747756: Validation loss did not improve from -0.61891. Patience: 3/50
2024-05-07 18:08:23.748997: train_loss -0.7195
2024-05-07 18:08:23.793020: val_loss -0.6093
2024-05-07 18:08:23.794245: Pseudo dice [0.734]
2024-05-07 18:08:23.797438: Epoch time: 236.14 s
2024-05-07 18:08:23.798545: Yayy! New best EMA pseudo Dice: 0.682
2024-05-07 18:08:25.526262: 
2024-05-07 18:08:25.527939: Epoch 14
2024-05-07 18:08:25.528981: Current learning rate: 0.00987
2024-05-07 18:12:19.797098: Validation loss did not improve from -0.61891. Patience: 4/50
2024-05-07 18:12:19.798527: train_loss -0.7349
2024-05-07 18:12:19.799987: val_loss -0.597
2024-05-07 18:12:19.801134: Pseudo dice [0.724]
2024-05-07 18:12:19.802076: Epoch time: 234.27 s
2024-05-07 18:12:20.203871: Yayy! New best EMA pseudo Dice: 0.6862
2024-05-07 18:12:21.899845: 
2024-05-07 18:12:21.901174: Epoch 15
2024-05-07 18:12:21.902644: Current learning rate: 0.00986
2024-05-07 18:16:16.215086: Validation loss did not improve from -0.61891. Patience: 5/50
2024-05-07 18:16:16.216675: train_loss -0.7435
2024-05-07 18:16:16.218178: val_loss -0.6128
2024-05-07 18:16:16.219383: Pseudo dice [0.7384]
2024-05-07 18:16:16.220507: Epoch time: 234.32 s
2024-05-07 18:16:16.221713: Yayy! New best EMA pseudo Dice: 0.6914
2024-05-07 18:16:18.077456: 
2024-05-07 18:16:18.079515: Epoch 16
2024-05-07 18:16:18.080759: Current learning rate: 0.00986
2024-05-07 18:20:14.081933: Validation loss did not improve from -0.61891. Patience: 6/50
2024-05-07 18:20:14.104615: train_loss -0.7511
2024-05-07 18:20:14.106311: val_loss -0.5872
2024-05-07 18:20:14.107399: Pseudo dice [0.7165]
2024-05-07 18:20:14.108690: Epoch time: 236.01 s
2024-05-07 18:20:14.109697: Yayy! New best EMA pseudo Dice: 0.6939
2024-05-07 18:20:16.665191: 
2024-05-07 18:20:16.668009: Epoch 17
2024-05-07 18:20:16.669891: Current learning rate: 0.00985
2024-05-07 18:24:11.288657: Validation loss improved from -0.61891 to -0.63635! Patience: 6/50
2024-05-07 18:24:11.315759: train_loss -0.7486
2024-05-07 18:24:11.317133: val_loss -0.6363
2024-05-07 18:24:11.318257: Pseudo dice [0.7485]
2024-05-07 18:24:11.319312: Epoch time: 234.65 s
2024-05-07 18:24:11.320370: Yayy! New best EMA pseudo Dice: 0.6994
2024-05-07 18:24:14.071846: 
2024-05-07 18:24:14.073679: Epoch 18
2024-05-07 18:24:14.075294: Current learning rate: 0.00984
2024-05-07 18:28:07.843123: Validation loss did not improve from -0.63635. Patience: 1/50
2024-05-07 18:28:07.844383: train_loss -0.7529
2024-05-07 18:28:07.845600: val_loss -0.5817
2024-05-07 18:28:07.846883: Pseudo dice [0.7105]
2024-05-07 18:28:07.847928: Epoch time: 233.77 s
2024-05-07 18:28:07.848844: Yayy! New best EMA pseudo Dice: 0.7005
2024-05-07 18:28:12.608068: 
2024-05-07 18:28:12.609144: Epoch 19
2024-05-07 18:28:12.610771: Current learning rate: 0.00983
2024-05-07 18:32:06.988072: Validation loss did not improve from -0.63635. Patience: 2/50
2024-05-07 18:32:06.989225: train_loss -0.765
2024-05-07 18:32:06.990305: val_loss -0.6302
2024-05-07 18:32:06.991230: Pseudo dice [0.7372]
2024-05-07 18:32:06.992122: Epoch time: 234.38 s
2024-05-07 18:32:07.354920: Yayy! New best EMA pseudo Dice: 0.7042
2024-05-07 18:32:09.008462: 
2024-05-07 18:32:09.009953: Epoch 20
2024-05-07 18:32:09.010911: Current learning rate: 0.00982
2024-05-07 18:36:01.323235: Validation loss did not improve from -0.63635. Patience: 3/50
2024-05-07 18:36:01.324414: train_loss -0.7689
2024-05-07 18:36:01.325550: val_loss -0.5899
2024-05-07 18:36:01.326747: Pseudo dice [0.7157]
2024-05-07 18:36:01.327824: Epoch time: 232.32 s
2024-05-07 18:36:01.328693: Yayy! New best EMA pseudo Dice: 0.7053
2024-05-07 18:36:03.009410: 
2024-05-07 18:36:03.011319: Epoch 21
2024-05-07 18:36:03.012897: Current learning rate: 0.00981
2024-05-07 18:39:55.555917: Validation loss did not improve from -0.63635. Patience: 4/50
2024-05-07 18:39:55.557407: train_loss -0.7715
2024-05-07 18:39:55.558563: val_loss -0.6047
2024-05-07 18:39:55.559552: Pseudo dice [0.7193]
2024-05-07 18:39:55.560647: Epoch time: 232.55 s
2024-05-07 18:39:55.561775: Yayy! New best EMA pseudo Dice: 0.7067
2024-05-07 18:39:57.145623: 
2024-05-07 18:39:57.147468: Epoch 22
2024-05-07 18:39:57.148680: Current learning rate: 0.0098
2024-05-07 18:43:50.932134: Validation loss did not improve from -0.63635. Patience: 5/50
2024-05-07 18:43:50.933341: train_loss -0.7747
2024-05-07 18:43:50.934571: val_loss -0.6243
2024-05-07 18:43:50.935664: Pseudo dice [0.7336]
2024-05-07 18:43:50.936633: Epoch time: 233.79 s
2024-05-07 18:43:50.938708: Yayy! New best EMA pseudo Dice: 0.7094
2024-05-07 18:43:52.457115: 
2024-05-07 18:43:52.459557: Epoch 23
2024-05-07 18:43:52.460525: Current learning rate: 0.00979
2024-05-07 18:47:45.270180: Validation loss improved from -0.63635 to -0.63970! Patience: 5/50
2024-05-07 18:47:45.271321: train_loss -0.7788
2024-05-07 18:47:45.272450: val_loss -0.6397
2024-05-07 18:47:45.273522: Pseudo dice [0.7349]
2024-05-07 18:47:45.274564: Epoch time: 232.82 s
2024-05-07 18:47:45.275476: Yayy! New best EMA pseudo Dice: 0.712
2024-05-07 18:47:46.998771: 
2024-05-07 18:47:47.000410: Epoch 24
2024-05-07 18:47:47.001913: Current learning rate: 0.00978
2024-05-07 18:51:41.271252: Validation loss did not improve from -0.63970. Patience: 1/50
2024-05-07 18:51:41.272611: train_loss -0.7793
2024-05-07 18:51:41.273935: val_loss -0.5977
2024-05-07 18:51:41.275117: Pseudo dice [0.7234]
2024-05-07 18:51:41.276244: Epoch time: 234.28 s
2024-05-07 18:51:41.657376: Yayy! New best EMA pseudo Dice: 0.7131
2024-05-07 18:51:43.334520: 
2024-05-07 18:51:43.335959: Epoch 25
2024-05-07 18:51:43.337293: Current learning rate: 0.00977
2024-05-07 18:55:37.322695: Validation loss did not improve from -0.63970. Patience: 2/50
2024-05-07 18:55:37.323878: train_loss -0.7652
2024-05-07 18:55:37.325403: val_loss -0.5759
2024-05-07 18:55:37.337166: Pseudo dice [0.6984]
2024-05-07 18:55:37.338703: Epoch time: 233.99 s
2024-05-07 18:55:38.617574: 
2024-05-07 18:55:38.620202: Epoch 26
2024-05-07 18:55:38.621686: Current learning rate: 0.00977
2024-05-07 18:59:32.408128: Validation loss did not improve from -0.63970. Patience: 3/50
2024-05-07 18:59:32.409863: train_loss -0.7718
2024-05-07 18:59:32.411067: val_loss -0.5728
2024-05-07 18:59:32.412293: Pseudo dice [0.6911]
2024-05-07 18:59:32.413348: Epoch time: 233.79 s
2024-05-07 18:59:33.688342: 
2024-05-07 18:59:33.690121: Epoch 27
2024-05-07 18:59:33.691523: Current learning rate: 0.00976
2024-05-07 19:03:27.483777: Validation loss did not improve from -0.63970. Patience: 4/50
2024-05-07 19:03:27.484978: train_loss -0.7728
2024-05-07 19:03:27.486141: val_loss -0.6121
2024-05-07 19:03:27.487040: Pseudo dice [0.7332]
2024-05-07 19:03:27.488159: Epoch time: 233.8 s
2024-05-07 19:03:28.741038: 
2024-05-07 19:03:28.742321: Epoch 28
2024-05-07 19:03:28.743273: Current learning rate: 0.00975
2024-05-07 19:07:24.225147: Validation loss improved from -0.63970 to -0.64216! Patience: 4/50
2024-05-07 19:07:24.226357: train_loss -0.7561
2024-05-07 19:07:24.227572: val_loss -0.6422
2024-05-07 19:07:24.228662: Pseudo dice [0.7427]
2024-05-07 19:07:24.230066: Epoch time: 235.49 s
2024-05-07 19:07:24.231075: Yayy! New best EMA pseudo Dice: 0.715
2024-05-07 19:07:25.857170: 
2024-05-07 19:07:25.858983: Epoch 29
2024-05-07 19:07:25.860637: Current learning rate: 0.00974
2024-05-07 19:11:22.188085: Validation loss did not improve from -0.64216. Patience: 1/50
2024-05-07 19:11:22.189231: train_loss -0.7693
2024-05-07 19:11:22.190443: val_loss -0.61
2024-05-07 19:11:22.191641: Pseudo dice [0.727]
2024-05-07 19:11:22.192787: Epoch time: 236.33 s
2024-05-07 19:11:22.581381: Yayy! New best EMA pseudo Dice: 0.7162
2024-05-07 19:11:24.587622: 
2024-05-07 19:11:24.590162: Epoch 30
2024-05-07 19:11:24.591441: Current learning rate: 0.00973
2024-05-07 19:15:21.733466: Validation loss did not improve from -0.64216. Patience: 2/50
2024-05-07 19:15:21.734826: train_loss -0.7725
2024-05-07 19:15:21.737240: val_loss -0.6125
2024-05-07 19:15:21.738326: Pseudo dice [0.7292]
2024-05-07 19:15:21.739729: Epoch time: 237.15 s
2024-05-07 19:15:21.740932: Yayy! New best EMA pseudo Dice: 0.7175
2024-05-07 19:15:23.831523: 
2024-05-07 19:15:23.833711: Epoch 31
2024-05-07 19:15:23.835063: Current learning rate: 0.00972
2024-05-07 19:19:20.089818: Validation loss did not improve from -0.64216. Patience: 3/50
2024-05-07 19:19:20.090954: train_loss -0.7874
2024-05-07 19:19:20.091925: val_loss -0.6192
2024-05-07 19:19:20.092898: Pseudo dice [0.7374]
2024-05-07 19:19:20.093829: Epoch time: 236.26 s
2024-05-07 19:19:20.094702: Yayy! New best EMA pseudo Dice: 0.7195
2024-05-07 19:19:21.742828: 
2024-05-07 19:19:21.744815: Epoch 32
2024-05-07 19:19:21.745959: Current learning rate: 0.00971
2024-05-07 19:23:19.768365: Validation loss did not improve from -0.64216. Patience: 4/50
2024-05-07 19:23:19.785553: train_loss -0.7901
2024-05-07 19:23:19.786756: val_loss -0.6137
2024-05-07 19:23:19.787894: Pseudo dice [0.7317]
2024-05-07 19:23:19.789182: Epoch time: 238.04 s
2024-05-07 19:23:19.790353: Yayy! New best EMA pseudo Dice: 0.7207
2024-05-07 19:23:21.588924: 
2024-05-07 19:23:21.591028: Epoch 33
2024-05-07 19:23:21.592313: Current learning rate: 0.0097
2024-05-07 19:27:20.585308: Validation loss did not improve from -0.64216. Patience: 5/50
2024-05-07 19:27:20.623215: train_loss -0.7978
2024-05-07 19:27:20.624664: val_loss -0.6236
2024-05-07 19:27:20.625775: Pseudo dice [0.74]
2024-05-07 19:27:20.626909: Epoch time: 239.01 s
2024-05-07 19:27:20.628131: Yayy! New best EMA pseudo Dice: 0.7226
2024-05-07 19:27:23.059209: 
2024-05-07 19:27:23.060622: Epoch 34
2024-05-07 19:27:23.061873: Current learning rate: 0.00969
2024-05-07 19:31:20.565536: Validation loss did not improve from -0.64216. Patience: 6/50
2024-05-07 19:31:20.566681: train_loss -0.7995
2024-05-07 19:31:20.567755: val_loss -0.6348
2024-05-07 19:31:20.568851: Pseudo dice [0.751]
2024-05-07 19:31:20.569983: Epoch time: 237.51 s
2024-05-07 19:31:20.938859: Yayy! New best EMA pseudo Dice: 0.7255
2024-05-07 19:31:22.559517: 
2024-05-07 19:31:22.561747: Epoch 35
2024-05-07 19:31:22.562835: Current learning rate: 0.00968
2024-05-07 19:35:19.506575: Validation loss did not improve from -0.64216. Patience: 7/50
2024-05-07 19:35:19.509749: train_loss -0.7914
2024-05-07 19:35:19.511055: val_loss -0.6035
2024-05-07 19:35:19.512244: Pseudo dice [0.7307]
2024-05-07 19:35:19.513438: Epoch time: 236.95 s
2024-05-07 19:35:19.514452: Yayy! New best EMA pseudo Dice: 0.726
2024-05-07 19:35:21.222881: 
2024-05-07 19:35:21.224802: Epoch 36
2024-05-07 19:35:21.225911: Current learning rate: 0.00968
2024-05-07 19:39:18.289775: Validation loss did not improve from -0.64216. Patience: 8/50
2024-05-07 19:39:18.291463: train_loss -0.7856
2024-05-07 19:39:18.293239: val_loss -0.6265
2024-05-07 19:39:18.294620: Pseudo dice [0.7413]
2024-05-07 19:39:18.295663: Epoch time: 237.07 s
2024-05-07 19:39:18.296632: Yayy! New best EMA pseudo Dice: 0.7275
2024-05-07 19:39:19.959636: 
2024-05-07 19:39:19.961672: Epoch 37
2024-05-07 19:39:19.962794: Current learning rate: 0.00967
2024-05-07 19:43:15.241439: Validation loss did not improve from -0.64216. Patience: 9/50
2024-05-07 19:43:15.242755: train_loss -0.7953
2024-05-07 19:43:15.243943: val_loss -0.6255
2024-05-07 19:43:15.245283: Pseudo dice [0.7332]
2024-05-07 19:43:15.246329: Epoch time: 235.28 s
2024-05-07 19:43:15.247321: Yayy! New best EMA pseudo Dice: 0.7281
2024-05-07 19:43:16.943112: 
2024-05-07 19:43:16.945366: Epoch 38
2024-05-07 19:43:16.946820: Current learning rate: 0.00966
2024-05-07 19:47:11.262797: Validation loss did not improve from -0.64216. Patience: 10/50
2024-05-07 19:47:11.264392: train_loss -0.7972
2024-05-07 19:47:11.265587: val_loss -0.634
2024-05-07 19:47:11.266780: Pseudo dice [0.751]
2024-05-07 19:47:11.267934: Epoch time: 234.32 s
2024-05-07 19:47:11.269065: Yayy! New best EMA pseudo Dice: 0.7304
2024-05-07 19:47:12.987416: 
2024-05-07 19:47:12.989211: Epoch 39
2024-05-07 19:47:12.990410: Current learning rate: 0.00965
2024-05-07 19:51:07.348189: Validation loss did not improve from -0.64216. Patience: 11/50
2024-05-07 19:51:07.349383: train_loss -0.7966
2024-05-07 19:51:07.350490: val_loss -0.6237
2024-05-07 19:51:07.351546: Pseudo dice [0.7421]
2024-05-07 19:51:07.352471: Epoch time: 234.36 s
2024-05-07 19:51:07.727601: Yayy! New best EMA pseudo Dice: 0.7316
2024-05-07 19:51:09.433952: 
2024-05-07 19:51:09.435098: Epoch 40
2024-05-07 19:51:09.436170: Current learning rate: 0.00964
2024-05-07 19:55:02.817533: Validation loss did not improve from -0.64216. Patience: 12/50
2024-05-07 19:55:02.818692: train_loss -0.7964
2024-05-07 19:55:02.819640: val_loss -0.5844
2024-05-07 19:55:02.820617: Pseudo dice [0.7239]
2024-05-07 19:55:02.821881: Epoch time: 233.39 s
2024-05-07 19:55:05.184406: 
2024-05-07 19:55:05.186343: Epoch 41
2024-05-07 19:55:05.188268: Current learning rate: 0.00963
2024-05-07 19:58:58.930779: Validation loss did not improve from -0.64216. Patience: 13/50
2024-05-07 19:58:58.931965: train_loss -0.7968
2024-05-07 19:58:58.933353: val_loss -0.6046
2024-05-07 19:58:58.934563: Pseudo dice [0.731]
2024-05-07 19:58:58.935685: Epoch time: 233.75 s
2024-05-07 19:59:00.156775: 
2024-05-07 19:59:00.158835: Epoch 42
2024-05-07 19:59:00.160186: Current learning rate: 0.00962
2024-05-07 20:02:53.625428: Validation loss did not improve from -0.64216. Patience: 14/50
2024-05-07 20:02:53.626548: train_loss -0.8026
2024-05-07 20:02:53.627664: val_loss -0.6327
2024-05-07 20:02:53.628730: Pseudo dice [0.7342]
2024-05-07 20:02:53.629622: Epoch time: 233.47 s
2024-05-07 20:02:54.857501: 
2024-05-07 20:02:54.859487: Epoch 43
2024-05-07 20:02:54.861234: Current learning rate: 0.00961
2024-05-07 20:06:51.903351: Validation loss did not improve from -0.64216. Patience: 15/50
2024-05-07 20:06:51.904685: train_loss -0.8104
2024-05-07 20:06:51.906037: val_loss -0.6344
2024-05-07 20:06:51.907150: Pseudo dice [0.7454]
2024-05-07 20:06:51.908193: Epoch time: 237.05 s
2024-05-07 20:06:51.909364: Yayy! New best EMA pseudo Dice: 0.7326
2024-05-07 20:06:53.511350: 
2024-05-07 20:06:53.513172: Epoch 44
2024-05-07 20:06:53.514352: Current learning rate: 0.0096
2024-05-07 20:10:50.746522: Validation loss did not improve from -0.64216. Patience: 16/50
2024-05-07 20:10:50.747662: train_loss -0.8068
2024-05-07 20:10:50.748931: val_loss -0.6134
2024-05-07 20:10:50.749941: Pseudo dice [0.7229]
2024-05-07 20:10:50.751188: Epoch time: 237.24 s
2024-05-07 20:10:52.357308: 
2024-05-07 20:10:52.358885: Epoch 45
2024-05-07 20:10:52.360137: Current learning rate: 0.00959
2024-05-07 20:14:49.114300: Validation loss did not improve from -0.64216. Patience: 17/50
2024-05-07 20:14:49.115374: train_loss -0.8047
2024-05-07 20:14:49.116432: val_loss -0.6148
2024-05-07 20:14:49.117422: Pseudo dice [0.7318]
2024-05-07 20:14:49.118415: Epoch time: 236.76 s
2024-05-07 20:14:50.359143: 
2024-05-07 20:14:50.361256: Epoch 46
2024-05-07 20:14:50.362775: Current learning rate: 0.00959
2024-05-07 20:18:46.041885: Validation loss did not improve from -0.64216. Patience: 18/50
2024-05-07 20:18:46.043313: train_loss -0.811
2024-05-07 20:18:46.056488: val_loss -0.6337
2024-05-07 20:18:46.057797: Pseudo dice [0.7419]
2024-05-07 20:18:46.059909: Epoch time: 235.69 s
2024-05-07 20:18:46.060969: Yayy! New best EMA pseudo Dice: 0.7327
2024-05-07 20:18:47.790750: 
2024-05-07 20:18:47.793373: Epoch 47
2024-05-07 20:18:47.795124: Current learning rate: 0.00958
2024-05-07 20:22:44.437809: Validation loss did not improve from -0.64216. Patience: 19/50
2024-05-07 20:22:44.439008: train_loss -0.8142
2024-05-07 20:22:44.440238: val_loss -0.6127
2024-05-07 20:22:44.441437: Pseudo dice [0.7232]
2024-05-07 20:22:44.442407: Epoch time: 236.65 s
2024-05-07 20:22:45.790412: 
2024-05-07 20:22:45.792462: Epoch 48
2024-05-07 20:22:45.793504: Current learning rate: 0.00957
2024-05-07 20:26:42.943687: Validation loss did not improve from -0.64216. Patience: 20/50
2024-05-07 20:26:42.966492: train_loss -0.814
2024-05-07 20:26:42.968341: val_loss -0.6143
2024-05-07 20:26:42.969455: Pseudo dice [0.7307]
2024-05-07 20:26:42.970544: Epoch time: 237.18 s
2024-05-07 20:26:44.581079: 
2024-05-07 20:26:44.582938: Epoch 49
2024-05-07 20:26:44.584079: Current learning rate: 0.00956
2024-05-07 20:30:40.904013: Validation loss did not improve from -0.64216. Patience: 21/50
2024-05-07 20:30:40.905477: train_loss -0.8134
2024-05-07 20:30:40.906663: val_loss -0.6316
2024-05-07 20:30:40.907746: Pseudo dice [0.7436]
2024-05-07 20:30:40.908822: Epoch time: 236.33 s
2024-05-07 20:30:41.287502: Yayy! New best EMA pseudo Dice: 0.7328
2024-05-07 20:30:42.953107: 
2024-05-07 20:30:42.955871: Epoch 50
2024-05-07 20:30:42.957189: Current learning rate: 0.00955
2024-05-07 20:34:40.663527: Validation loss did not improve from -0.64216. Patience: 22/50
2024-05-07 20:34:40.690016: train_loss -0.814
2024-05-07 20:34:40.691498: val_loss -0.61
2024-05-07 20:34:40.692611: Pseudo dice [0.7382]
2024-05-07 20:34:40.693747: Epoch time: 237.74 s
2024-05-07 20:34:40.694704: Yayy! New best EMA pseudo Dice: 0.7334
2024-05-07 20:34:42.772424: 
2024-05-07 20:34:42.774233: Epoch 51
2024-05-07 20:34:42.775632: Current learning rate: 0.00954
2024-05-07 20:38:39.023748: Validation loss did not improve from -0.64216. Patience: 23/50
2024-05-07 20:38:39.024951: train_loss -0.8196
2024-05-07 20:38:39.026209: val_loss -0.635
2024-05-07 20:38:39.027309: Pseudo dice [0.7483]
2024-05-07 20:38:39.028432: Epoch time: 236.25 s
2024-05-07 20:38:39.029541: Yayy! New best EMA pseudo Dice: 0.7348
2024-05-07 20:38:41.299349: 
2024-05-07 20:38:41.301376: Epoch 52
2024-05-07 20:38:41.303007: Current learning rate: 0.00953
2024-05-07 20:42:37.221211: Validation loss did not improve from -0.64216. Patience: 24/50
2024-05-07 20:42:37.240386: train_loss -0.8199
2024-05-07 20:42:37.241605: val_loss -0.6368
2024-05-07 20:42:37.242920: Pseudo dice [0.7434]
2024-05-07 20:42:37.244159: Epoch time: 235.92 s
2024-05-07 20:42:37.245312: Yayy! New best EMA pseudo Dice: 0.7357
2024-05-07 20:42:40.758913: 
2024-05-07 20:42:40.761420: Epoch 53
2024-05-07 20:42:40.762738: Current learning rate: 0.00952
2024-05-07 20:46:37.470608: Validation loss did not improve from -0.64216. Patience: 25/50
2024-05-07 20:46:37.472126: train_loss -0.8202
2024-05-07 20:46:37.473451: val_loss -0.6053
2024-05-07 20:46:37.474700: Pseudo dice [0.7237]
2024-05-07 20:46:37.475798: Epoch time: 236.71 s
2024-05-07 20:46:38.732894: 
2024-05-07 20:46:38.735060: Epoch 54
2024-05-07 20:46:38.737369: Current learning rate: 0.00951
2024-05-07 20:50:34.330280: Validation loss did not improve from -0.64216. Patience: 26/50
2024-05-07 20:50:34.331567: train_loss -0.8186
2024-05-07 20:50:34.332797: val_loss -0.621
2024-05-07 20:50:34.333878: Pseudo dice [0.7406]
2024-05-07 20:50:34.335402: Epoch time: 235.6 s
2024-05-07 20:50:35.972622: 
2024-05-07 20:50:35.974189: Epoch 55
2024-05-07 20:50:35.975327: Current learning rate: 0.0095
2024-05-07 20:54:31.226386: Validation loss did not improve from -0.64216. Patience: 27/50
2024-05-07 20:54:31.227544: train_loss -0.819
2024-05-07 20:54:31.228559: val_loss -0.5848
2024-05-07 20:54:31.229529: Pseudo dice [0.7165]
2024-05-07 20:54:31.230498: Epoch time: 235.26 s
2024-05-07 20:54:32.554724: 
2024-05-07 20:54:32.555843: Epoch 56
2024-05-07 20:54:32.557114: Current learning rate: 0.00949
2024-05-07 20:58:26.136627: Validation loss did not improve from -0.64216. Patience: 28/50
2024-05-07 20:58:26.137944: train_loss -0.8234
2024-05-07 20:58:26.139144: val_loss -0.6381
2024-05-07 20:58:26.140204: Pseudo dice [0.7508]
2024-05-07 20:58:26.141237: Epoch time: 233.58 s
2024-05-07 20:58:27.431708: 
2024-05-07 20:58:27.433805: Epoch 57
2024-05-07 20:58:27.435029: Current learning rate: 0.00949
2024-05-07 21:02:22.214022: Validation loss did not improve from -0.64216. Patience: 29/50
2024-05-07 21:02:22.215690: train_loss -0.8235
2024-05-07 21:02:22.216660: val_loss -0.5639
2024-05-07 21:02:22.217708: Pseudo dice [0.7039]
2024-05-07 21:02:22.218666: Epoch time: 234.78 s
2024-05-07 21:02:23.469332: 
2024-05-07 21:02:23.470857: Epoch 58
2024-05-07 21:02:23.471947: Current learning rate: 0.00948
2024-05-07 21:06:17.896532: Validation loss did not improve from -0.64216. Patience: 30/50
2024-05-07 21:06:17.897952: train_loss -0.8175
2024-05-07 21:06:17.899006: val_loss -0.6372
2024-05-07 21:06:17.900006: Pseudo dice [0.7532]
2024-05-07 21:06:17.900977: Epoch time: 234.43 s
2024-05-07 21:06:19.179228: 
2024-05-07 21:06:19.180556: Epoch 59
2024-05-07 21:06:19.181738: Current learning rate: 0.00947
2024-05-07 21:10:14.092415: Validation loss improved from -0.64216 to -0.65492! Patience: 30/50
2024-05-07 21:10:14.093581: train_loss -0.821
2024-05-07 21:10:14.094679: val_loss -0.6549
2024-05-07 21:10:14.095664: Pseudo dice [0.7594]
2024-05-07 21:10:14.096573: Epoch time: 234.92 s
2024-05-07 21:10:14.482484: Yayy! New best EMA pseudo Dice: 0.7366
2024-05-07 21:10:16.162165: 
2024-05-07 21:10:16.164597: Epoch 60
2024-05-07 21:10:16.165762: Current learning rate: 0.00946
2024-05-07 21:14:12.365755: Validation loss did not improve from -0.65492. Patience: 1/50
2024-05-07 21:14:12.366890: train_loss -0.818
2024-05-07 21:14:12.368177: val_loss -0.6033
2024-05-07 21:14:12.369510: Pseudo dice [0.7268]
2024-05-07 21:14:12.370779: Epoch time: 236.21 s
2024-05-07 21:14:13.644448: 
2024-05-07 21:14:13.646969: Epoch 61
2024-05-07 21:14:13.648496: Current learning rate: 0.00945
2024-05-07 21:18:08.543592: Validation loss did not improve from -0.65492. Patience: 2/50
2024-05-07 21:18:08.544822: train_loss -0.8146
2024-05-07 21:18:08.545818: val_loss -0.6207
2024-05-07 21:18:08.546769: Pseudo dice [0.7411]
2024-05-07 21:18:08.547801: Epoch time: 234.9 s
2024-05-07 21:18:09.813357: 
2024-05-07 21:18:09.815451: Epoch 62
2024-05-07 21:18:09.816563: Current learning rate: 0.00944
2024-05-07 21:22:05.014044: Validation loss did not improve from -0.65492. Patience: 3/50
2024-05-07 21:22:05.015218: train_loss -0.8161
2024-05-07 21:22:05.016497: val_loss -0.599
2024-05-07 21:22:05.017511: Pseudo dice [0.7295]
2024-05-07 21:22:05.018566: Epoch time: 235.2 s
2024-05-07 21:22:06.337197: 
2024-05-07 21:22:06.338658: Epoch 63
2024-05-07 21:22:06.339993: Current learning rate: 0.00943
2024-05-07 21:26:03.114797: Validation loss did not improve from -0.65492. Patience: 4/50
2024-05-07 21:26:03.117270: train_loss -0.8214
2024-05-07 21:26:03.163788: val_loss -0.6301
2024-05-07 21:26:03.165234: Pseudo dice [0.7304]
2024-05-07 21:26:03.172582: Epoch time: 236.78 s
2024-05-07 21:26:04.881169: 
2024-05-07 21:26:04.883038: Epoch 64
2024-05-07 21:26:04.884653: Current learning rate: 0.00942
2024-05-07 21:29:59.498906: Validation loss did not improve from -0.65492. Patience: 5/50
2024-05-07 21:29:59.500142: train_loss -0.8234
2024-05-07 21:29:59.501128: val_loss -0.6233
2024-05-07 21:29:59.502146: Pseudo dice [0.7311]
2024-05-07 21:29:59.503146: Epoch time: 234.62 s
2024-05-07 21:30:02.133008: 
2024-05-07 21:30:02.134918: Epoch 65
2024-05-07 21:30:02.136649: Current learning rate: 0.00941
2024-05-07 21:33:59.355560: Validation loss did not improve from -0.65492. Patience: 6/50
2024-05-07 21:33:59.370288: train_loss -0.8231
2024-05-07 21:33:59.371839: val_loss -0.5743
2024-05-07 21:33:59.372898: Pseudo dice [0.704]
2024-05-07 21:33:59.373788: Epoch time: 237.24 s
2024-05-07 21:34:00.946239: 
2024-05-07 21:34:00.948336: Epoch 66
2024-05-07 21:34:00.949403: Current learning rate: 0.0094
2024-05-07 21:37:59.546723: Validation loss did not improve from -0.65492. Patience: 7/50
2024-05-07 21:37:59.596615: train_loss -0.8264
2024-05-07 21:37:59.598048: val_loss -0.6156
2024-05-07 21:37:59.599273: Pseudo dice [0.7365]
2024-05-07 21:37:59.600259: Epoch time: 238.64 s
2024-05-07 21:38:01.831631: 
2024-05-07 21:38:01.833685: Epoch 67
2024-05-07 21:38:01.835284: Current learning rate: 0.00939
2024-05-07 21:41:57.116047: Validation loss did not improve from -0.65492. Patience: 8/50
2024-05-07 21:41:57.117429: train_loss -0.8325
2024-05-07 21:41:57.118597: val_loss -0.6018
2024-05-07 21:41:57.119883: Pseudo dice [0.7206]
2024-05-07 21:41:57.121004: Epoch time: 235.29 s
2024-05-07 21:41:58.453175: 
2024-05-07 21:41:58.455117: Epoch 68
2024-05-07 21:41:58.456625: Current learning rate: 0.00939
2024-05-07 21:45:54.403015: Validation loss did not improve from -0.65492. Patience: 9/50
2024-05-07 21:45:54.404215: train_loss -0.8304
2024-05-07 21:45:54.405427: val_loss -0.6382
2024-05-07 21:45:54.406450: Pseudo dice [0.7474]
2024-05-07 21:45:54.407478: Epoch time: 235.95 s
2024-05-07 21:45:55.894858: 
2024-05-07 21:45:55.897040: Epoch 69
2024-05-07 21:45:55.898964: Current learning rate: 0.00938
2024-05-07 21:49:50.833437: Validation loss did not improve from -0.65492. Patience: 10/50
2024-05-07 21:49:50.834842: train_loss -0.8331
2024-05-07 21:49:50.835964: val_loss -0.6193
2024-05-07 21:49:50.836962: Pseudo dice [0.7344]
2024-05-07 21:49:50.837904: Epoch time: 234.94 s
2024-05-07 21:49:53.255103: 
2024-05-07 21:49:53.256639: Epoch 70
2024-05-07 21:49:53.257784: Current learning rate: 0.00937
2024-05-07 21:53:48.636283: Validation loss did not improve from -0.65492. Patience: 11/50
2024-05-07 21:53:48.637744: train_loss -0.8282
2024-05-07 21:53:48.639069: val_loss -0.6347
2024-05-07 21:53:48.640261: Pseudo dice [0.7436]
2024-05-07 21:53:48.641202: Epoch time: 235.38 s
2024-05-07 21:53:49.983015: 
2024-05-07 21:53:49.984770: Epoch 71
2024-05-07 21:53:49.986305: Current learning rate: 0.00936
2024-05-07 21:57:45.533436: Validation loss did not improve from -0.65492. Patience: 12/50
2024-05-07 21:57:45.534841: train_loss -0.8314
2024-05-07 21:57:45.536072: val_loss -0.6031
2024-05-07 21:57:45.537142: Pseudo dice [0.7384]
2024-05-07 21:57:45.538256: Epoch time: 235.55 s
2024-05-07 21:57:46.878361: 
2024-05-07 21:57:46.880414: Epoch 72
2024-05-07 21:57:46.881686: Current learning rate: 0.00935
2024-05-07 22:01:42.404345: Validation loss did not improve from -0.65492. Patience: 13/50
2024-05-07 22:01:42.405491: train_loss -0.8323
2024-05-07 22:01:42.406983: val_loss -0.6025
2024-05-07 22:01:42.408514: Pseudo dice [0.7281]
2024-05-07 22:01:42.409636: Epoch time: 235.53 s
2024-05-07 22:01:43.841493: 
2024-05-07 22:01:43.843210: Epoch 73
2024-05-07 22:01:43.844433: Current learning rate: 0.00934
2024-05-07 22:05:40.861156: Validation loss did not improve from -0.65492. Patience: 14/50
2024-05-07 22:05:40.862480: train_loss -0.8316
2024-05-07 22:05:40.863547: val_loss -0.6245
2024-05-07 22:05:40.865257: Pseudo dice [0.7364]
2024-05-07 22:05:40.866217: Epoch time: 237.02 s
2024-05-07 22:05:42.202945: 
2024-05-07 22:05:42.204502: Epoch 74
2024-05-07 22:05:42.205632: Current learning rate: 0.00933
2024-05-07 22:09:38.130121: Validation loss did not improve from -0.65492. Patience: 15/50
2024-05-07 22:09:38.140311: train_loss -0.8368
2024-05-07 22:09:38.141452: val_loss -0.5854
2024-05-07 22:09:38.142389: Pseudo dice [0.7262]
2024-05-07 22:09:38.143315: Epoch time: 235.93 s
2024-05-07 22:09:40.048792: 
2024-05-07 22:09:40.050312: Epoch 75
2024-05-07 22:09:40.051542: Current learning rate: 0.00932
2024-05-07 22:13:35.918025: Validation loss did not improve from -0.65492. Patience: 16/50
2024-05-07 22:13:35.919254: train_loss -0.8343
2024-05-07 22:13:35.920269: val_loss -0.6282
2024-05-07 22:13:35.921238: Pseudo dice [0.7379]
2024-05-07 22:13:35.922282: Epoch time: 235.87 s
2024-05-07 22:13:38.233870: 
2024-05-07 22:13:38.235901: Epoch 76
2024-05-07 22:13:38.237239: Current learning rate: 0.00931
2024-05-07 22:17:35.026528: Validation loss did not improve from -0.65492. Patience: 17/50
2024-05-07 22:17:35.027735: train_loss -0.8314
2024-05-07 22:17:35.029082: val_loss -0.6469
2024-05-07 22:17:35.030125: Pseudo dice [0.7557]
2024-05-07 22:17:35.031224: Epoch time: 236.79 s
2024-05-07 22:17:36.365052: 
2024-05-07 22:17:36.366638: Epoch 77
2024-05-07 22:17:36.367620: Current learning rate: 0.0093
2024-05-07 22:21:32.781507: Validation loss did not improve from -0.65492. Patience: 18/50
2024-05-07 22:21:32.782700: train_loss -0.8315
2024-05-07 22:21:32.783815: val_loss -0.6317
2024-05-07 22:21:32.784873: Pseudo dice [0.744]
2024-05-07 22:21:32.785880: Epoch time: 236.42 s
2024-05-07 22:21:32.786738: Yayy! New best EMA pseudo Dice: 0.7367
2024-05-07 22:21:34.541806: 
2024-05-07 22:21:34.543241: Epoch 78
2024-05-07 22:21:34.544306: Current learning rate: 0.0093
2024-05-07 22:25:31.450151: Validation loss did not improve from -0.65492. Patience: 19/50
2024-05-07 22:25:31.451294: train_loss -0.8291
2024-05-07 22:25:31.452676: val_loss -0.6126
2024-05-07 22:25:31.453830: Pseudo dice [0.7399]
2024-05-07 22:25:31.454981: Epoch time: 236.91 s
2024-05-07 22:25:31.456115: Yayy! New best EMA pseudo Dice: 0.737
2024-05-07 22:25:33.195822: 
2024-05-07 22:25:33.198463: Epoch 79
2024-05-07 22:25:33.200299: Current learning rate: 0.00929
2024-05-07 22:29:30.205568: Validation loss did not improve from -0.65492. Patience: 20/50
2024-05-07 22:29:30.206792: train_loss -0.8331
2024-05-07 22:29:30.209016: val_loss -0.6419
2024-05-07 22:29:30.209944: Pseudo dice [0.7559]
2024-05-07 22:29:30.211487: Epoch time: 237.01 s
2024-05-07 22:29:30.601983: Yayy! New best EMA pseudo Dice: 0.7389
2024-05-07 22:29:32.364113: 
2024-05-07 22:29:32.365943: Epoch 80
2024-05-07 22:29:32.367444: Current learning rate: 0.00928
2024-05-07 22:33:30.575212: Validation loss did not improve from -0.65492. Patience: 21/50
2024-05-07 22:33:30.576698: train_loss -0.8352
2024-05-07 22:33:30.577884: val_loss -0.6192
2024-05-07 22:33:30.578875: Pseudo dice [0.7402]
2024-05-07 22:33:30.579915: Epoch time: 238.21 s
2024-05-07 22:33:30.580819: Yayy! New best EMA pseudo Dice: 0.739
2024-05-07 22:33:32.451454: 
2024-05-07 22:33:32.452831: Epoch 81
2024-05-07 22:33:32.454078: Current learning rate: 0.00927
2024-05-07 22:37:32.175211: Validation loss did not improve from -0.65492. Patience: 22/50
2024-05-07 22:37:32.183479: train_loss -0.8339
2024-05-07 22:37:32.185855: val_loss -0.5908
2024-05-07 22:37:32.187488: Pseudo dice [0.7283]
2024-05-07 22:37:32.188667: Epoch time: 239.73 s
2024-05-07 22:37:33.658615: 
2024-05-07 22:37:33.660217: Epoch 82
2024-05-07 22:37:33.661381: Current learning rate: 0.00926
2024-05-07 22:41:31.049902: Validation loss did not improve from -0.65492. Patience: 23/50
2024-05-07 22:41:31.051319: train_loss -0.8303
2024-05-07 22:41:31.052655: val_loss -0.5962
2024-05-07 22:41:31.053999: Pseudo dice [0.7336]
2024-05-07 22:41:31.055065: Epoch time: 237.39 s
2024-05-07 22:41:32.461285: 
2024-05-07 22:41:32.462948: Epoch 83
2024-05-07 22:41:32.464139: Current learning rate: 0.00925
2024-05-07 22:45:29.841294: Validation loss did not improve from -0.65492. Patience: 24/50
2024-05-07 22:45:29.885273: train_loss -0.8314
2024-05-07 22:45:29.887117: val_loss -0.6215
2024-05-07 22:45:29.888532: Pseudo dice [0.7326]
2024-05-07 22:45:29.890072: Epoch time: 237.42 s
2024-05-07 22:45:31.710335: 
2024-05-07 22:45:31.712706: Epoch 84
2024-05-07 22:45:31.714037: Current learning rate: 0.00924
2024-05-07 22:49:28.328221: Validation loss did not improve from -0.65492. Patience: 25/50
2024-05-07 22:49:28.329511: train_loss -0.8386
2024-05-07 22:49:28.330643: val_loss -0.6071
2024-05-07 22:49:28.332156: Pseudo dice [0.7241]
2024-05-07 22:49:28.333457: Epoch time: 236.62 s
2024-05-07 22:49:30.199423: 
2024-05-07 22:49:30.201489: Epoch 85
2024-05-07 22:49:30.202782: Current learning rate: 0.00923
2024-05-07 22:53:27.234035: Validation loss did not improve from -0.65492. Patience: 26/50
2024-05-07 22:53:27.235698: train_loss -0.8358
2024-05-07 22:53:27.237451: val_loss -0.6055
2024-05-07 22:53:27.238474: Pseudo dice [0.7251]
2024-05-07 22:53:27.239361: Epoch time: 237.04 s
2024-05-07 22:53:28.640417: 
2024-05-07 22:53:28.642066: Epoch 86
2024-05-07 22:53:28.643209: Current learning rate: 0.00922
2024-05-07 22:57:25.475678: Validation loss did not improve from -0.65492. Patience: 27/50
2024-05-07 22:57:25.477044: train_loss -0.8282
2024-05-07 22:57:25.478392: val_loss -0.6096
2024-05-07 22:57:25.479580: Pseudo dice [0.7323]
2024-05-07 22:57:25.480706: Epoch time: 236.84 s
2024-05-07 22:57:26.751368: 
2024-05-07 22:57:26.753193: Epoch 87
2024-05-07 22:57:26.754476: Current learning rate: 0.00921
2024-05-07 23:01:24.263316: Validation loss did not improve from -0.65492. Patience: 28/50
2024-05-07 23:01:24.264562: train_loss -0.8249
2024-05-07 23:01:24.265759: val_loss -0.651
2024-05-07 23:01:24.266899: Pseudo dice [0.7564]
2024-05-07 23:01:24.268030: Epoch time: 237.51 s
2024-05-07 23:01:26.531343: 
2024-05-07 23:01:26.532791: Epoch 88
2024-05-07 23:01:26.533948: Current learning rate: 0.0092
2024-05-07 23:05:23.746460: Validation loss did not improve from -0.65492. Patience: 29/50
2024-05-07 23:05:23.747664: train_loss -0.8321
2024-05-07 23:05:23.748819: val_loss -0.6438
2024-05-07 23:05:23.749938: Pseudo dice [0.7516]
2024-05-07 23:05:23.751078: Epoch time: 237.22 s
2024-05-07 23:05:25.076824: 
2024-05-07 23:05:25.078885: Epoch 89
2024-05-07 23:05:25.080263: Current learning rate: 0.0092
2024-05-07 23:09:21.371567: Validation loss did not improve from -0.65492. Patience: 30/50
2024-05-07 23:09:21.373504: train_loss -0.8389
2024-05-07 23:09:21.375026: val_loss -0.6261
2024-05-07 23:09:21.376658: Pseudo dice [0.7408]
2024-05-07 23:09:21.377918: Epoch time: 236.3 s
2024-05-07 23:09:23.080529: 
2024-05-07 23:09:23.081696: Epoch 90
2024-05-07 23:09:23.082613: Current learning rate: 0.00919
2024-05-07 23:13:19.903302: Validation loss did not improve from -0.65492. Patience: 31/50
2024-05-07 23:13:19.904667: train_loss -0.8373
2024-05-07 23:13:19.905967: val_loss -0.6252
2024-05-07 23:13:19.907075: Pseudo dice [0.7336]
2024-05-07 23:13:19.907977: Epoch time: 236.83 s
2024-05-07 23:13:21.182411: 
2024-05-07 23:13:21.185037: Epoch 91
2024-05-07 23:13:21.186291: Current learning rate: 0.00918
2024-05-07 23:17:18.320256: Validation loss did not improve from -0.65492. Patience: 32/50
2024-05-07 23:17:18.340181: train_loss -0.8419
2024-05-07 23:17:18.341557: val_loss -0.6172
2024-05-07 23:17:18.342582: Pseudo dice [0.7354]
2024-05-07 23:17:18.343653: Epoch time: 237.14 s
2024-05-07 23:17:19.790691: 
2024-05-07 23:17:19.792139: Epoch 92
2024-05-07 23:17:19.793663: Current learning rate: 0.00917
2024-05-07 23:21:17.461488: Validation loss did not improve from -0.65492. Patience: 33/50
2024-05-07 23:21:17.463161: train_loss -0.8423
2024-05-07 23:21:17.464631: val_loss -0.6323
2024-05-07 23:21:17.465843: Pseudo dice [0.7457]
2024-05-07 23:21:17.466998: Epoch time: 237.67 s
2024-05-07 23:21:18.749014: 
2024-05-07 23:21:18.750932: Epoch 93
2024-05-07 23:21:18.752195: Current learning rate: 0.00916
2024-05-07 23:25:17.330622: Validation loss did not improve from -0.65492. Patience: 34/50
2024-05-07 23:25:17.331826: train_loss -0.8441
2024-05-07 23:25:17.333010: val_loss -0.6195
2024-05-07 23:25:17.334662: Pseudo dice [0.7375]
2024-05-07 23:25:17.336450: Epoch time: 238.58 s
2024-05-07 23:25:18.585317: 
2024-05-07 23:25:18.587261: Epoch 94
2024-05-07 23:25:18.588673: Current learning rate: 0.00915
2024-05-07 23:29:15.590906: Validation loss did not improve from -0.65492. Patience: 35/50
2024-05-07 23:29:15.592353: train_loss -0.8376
2024-05-07 23:29:15.593709: val_loss -0.6124
2024-05-07 23:29:15.594971: Pseudo dice [0.7347]
2024-05-07 23:29:15.596233: Epoch time: 237.01 s
2024-05-07 23:29:17.259475: 
2024-05-07 23:29:17.261400: Epoch 95
2024-05-07 23:29:17.262817: Current learning rate: 0.00914
2024-05-07 23:33:14.198967: Validation loss did not improve from -0.65492. Patience: 36/50
2024-05-07 23:33:14.200187: train_loss -0.8419
2024-05-07 23:33:14.201383: val_loss -0.626
2024-05-07 23:33:14.202631: Pseudo dice [0.7507]
2024-05-07 23:33:14.203690: Epoch time: 236.94 s
2024-05-07 23:33:14.204661: Yayy! New best EMA pseudo Dice: 0.7393
2024-05-07 23:33:15.761592: 
2024-05-07 23:33:15.763944: Epoch 96
2024-05-07 23:33:15.765829: Current learning rate: 0.00913
2024-05-07 23:37:13.308882: Validation loss did not improve from -0.65492. Patience: 37/50
2024-05-07 23:37:13.310234: train_loss -0.8436
2024-05-07 23:37:13.312379: val_loss -0.6134
2024-05-07 23:37:13.372547: Pseudo dice [0.7237]
2024-05-07 23:37:13.374542: Epoch time: 237.55 s
2024-05-07 23:37:14.977272: 
2024-05-07 23:37:14.978799: Epoch 97
2024-05-07 23:37:14.980015: Current learning rate: 0.00912
2024-05-07 23:41:13.992198: Validation loss did not improve from -0.65492. Patience: 38/50
2024-05-07 23:41:13.996941: train_loss -0.8421
2024-05-07 23:41:13.998647: val_loss -0.6045
2024-05-07 23:41:13.999690: Pseudo dice [0.7267]
2024-05-07 23:41:14.000788: Epoch time: 239.02 s
2024-05-07 23:41:15.402359: 
2024-05-07 23:41:15.403813: Epoch 98
2024-05-07 23:41:15.405359: Current learning rate: 0.00911
2024-05-07 23:45:12.952720: Validation loss did not improve from -0.65492. Patience: 39/50
2024-05-07 23:45:12.968074: train_loss -0.8471
2024-05-07 23:45:12.969174: val_loss -0.6441
2024-05-07 23:45:12.970103: Pseudo dice [0.753]
2024-05-07 23:45:12.971070: Epoch time: 237.57 s
2024-05-07 23:45:14.425542: 
2024-05-07 23:45:14.427074: Epoch 99
2024-05-07 23:45:14.428248: Current learning rate: 0.0091
2024-05-07 23:49:11.683711: Validation loss did not improve from -0.65492. Patience: 40/50
2024-05-07 23:49:11.763798: train_loss -0.844
2024-05-07 23:49:11.765545: val_loss -0.5904
2024-05-07 23:49:11.766797: Pseudo dice [0.7245]
2024-05-07 23:49:11.767733: Epoch time: 237.34 s
2024-05-07 23:49:16.190122: 
2024-05-07 23:49:16.192501: Epoch 100
2024-05-07 23:49:16.193741: Current learning rate: 0.0091
2024-05-07 23:53:12.843724: Validation loss did not improve from -0.65492. Patience: 41/50
2024-05-07 23:53:12.845167: train_loss -0.8407
2024-05-07 23:53:12.846328: val_loss -0.6465
2024-05-07 23:53:12.847296: Pseudo dice [0.7559]
2024-05-07 23:53:12.848334: Epoch time: 236.66 s
2024-05-07 23:53:14.160482: 
2024-05-07 23:53:14.390122: Epoch 101
2024-05-07 23:53:14.392819: Current learning rate: 0.00909
2024-05-07 23:57:11.894252: Validation loss did not improve from -0.65492. Patience: 42/50
2024-05-07 23:57:11.895415: train_loss -0.8459
2024-05-07 23:57:11.896628: val_loss -0.6341
2024-05-07 23:57:11.897756: Pseudo dice [0.7472]
2024-05-07 23:57:11.898869: Epoch time: 237.74 s
2024-05-07 23:57:11.899896: Yayy! New best EMA pseudo Dice: 0.7396
2024-05-07 23:57:13.509845: 
2024-05-07 23:57:13.511975: Epoch 102
2024-05-07 23:57:13.513314: Current learning rate: 0.00908
2024-05-08 00:01:09.804527: Validation loss did not improve from -0.65492. Patience: 43/50
2024-05-08 00:01:09.805814: train_loss -0.8438
2024-05-08 00:01:09.807260: val_loss -0.6469
2024-05-08 00:01:09.808572: Pseudo dice [0.7484]
2024-05-08 00:01:09.810356: Epoch time: 236.3 s
2024-05-08 00:01:09.812132: Yayy! New best EMA pseudo Dice: 0.7405
2024-05-08 00:01:11.416944: 
2024-05-08 00:01:11.418317: Epoch 103
2024-05-08 00:01:11.419500: Current learning rate: 0.00907
2024-05-08 00:05:07.708385: Validation loss did not improve from -0.65492. Patience: 44/50
2024-05-08 00:05:07.709565: train_loss -0.8464
2024-05-08 00:05:07.710627: val_loss -0.6424
2024-05-08 00:05:07.711867: Pseudo dice [0.7462]
2024-05-08 00:05:07.713009: Epoch time: 236.29 s
2024-05-08 00:05:07.714047: Yayy! New best EMA pseudo Dice: 0.7411
2024-05-08 00:05:09.384910: 
2024-05-08 00:05:09.386507: Epoch 104
2024-05-08 00:05:09.387808: Current learning rate: 0.00906
2024-05-08 00:09:06.829146: Validation loss did not improve from -0.65492. Patience: 45/50
2024-05-08 00:09:06.830400: train_loss -0.8464
2024-05-08 00:09:06.831466: val_loss -0.5924
2024-05-08 00:09:06.832454: Pseudo dice [0.727]
2024-05-08 00:09:06.833351: Epoch time: 237.45 s
2024-05-08 00:09:08.514573: 
2024-05-08 00:09:08.515789: Epoch 105
2024-05-08 00:09:08.516992: Current learning rate: 0.00905
2024-05-08 00:13:04.684166: Validation loss did not improve from -0.65492. Patience: 46/50
2024-05-08 00:13:04.685750: train_loss -0.8465
2024-05-08 00:13:04.686873: val_loss -0.612
2024-05-08 00:13:04.687801: Pseudo dice [0.7267]
2024-05-08 00:13:04.688840: Epoch time: 236.17 s
2024-05-08 00:13:05.958052: 
2024-05-08 00:13:05.959160: Epoch 106
2024-05-08 00:13:05.960796: Current learning rate: 0.00904
2024-05-08 00:17:03.106763: Validation loss did not improve from -0.65492. Patience: 47/50
2024-05-08 00:17:03.108136: train_loss -0.8465
2024-05-08 00:17:03.109257: val_loss -0.6154
2024-05-08 00:17:03.110251: Pseudo dice [0.7328]
2024-05-08 00:17:03.111417: Epoch time: 237.15 s
2024-05-08 00:17:04.400774: 
2024-05-08 00:17:04.402063: Epoch 107
2024-05-08 00:17:04.403217: Current learning rate: 0.00903
2024-05-08 00:21:00.117481: Validation loss did not improve from -0.65492. Patience: 48/50
2024-05-08 00:21:00.118715: train_loss -0.8479
2024-05-08 00:21:00.120005: val_loss -0.6109
2024-05-08 00:21:00.121080: Pseudo dice [0.7303]
2024-05-08 00:21:00.122142: Epoch time: 235.72 s
2024-05-08 00:21:01.406976: 
2024-05-08 00:21:01.408719: Epoch 108
2024-05-08 00:21:01.409998: Current learning rate: 0.00902
2024-05-08 00:24:57.627104: Validation loss did not improve from -0.65492. Patience: 49/50
2024-05-08 00:24:57.655282: train_loss -0.8492
2024-05-08 00:24:57.657325: val_loss -0.6475
2024-05-08 00:24:57.658839: Pseudo dice [0.7574]
2024-05-08 00:24:57.659988: Epoch time: 236.22 s
2024-05-08 00:24:59.007730: 
2024-05-08 00:24:59.010011: Epoch 109
2024-05-08 00:24:59.011622: Current learning rate: 0.00901
2024-05-08 00:28:53.789067: Validation loss did not improve from -0.65492. Patience: 50/50
2024-05-08 00:28:53.790930: train_loss -0.8504
2024-05-08 00:28:53.792427: val_loss -0.5962
2024-05-08 00:28:53.793991: Pseudo dice [0.7308]
2024-05-08 00:28:53.796038: Epoch time: 234.78 s
2024-05-08 00:28:55.307760: Patience reached. Stopping training.
2024-05-08 00:28:55.744139: Training done.
2024-05-08 00:28:56.195730: Using splits from existing split file: /home/gridsan/nchutisilp/datasets/nnUNet_Datasets/nnUNet_preprocessed/Dataset302_Calcium_OCTv2/splits_final.json
2024-05-08 00:28:56.198519: The split file contains 3 splits.
2024-05-08 00:28:56.199604: Desired fold for training: 0
2024-05-08 00:28:56.200775: This split has 4 training and 2 validation cases.
2024-05-08 00:28:56.202005: predicting 101-019
2024-05-08 00:28:56.257772: 101-019, shape torch.Size([1, 375, 498, 498]), rank 0
2024-05-08 00:29:42.337703: predicting 704-003
2024-05-08 00:29:42.423746: 704-003, shape torch.Size([1, 375, 498, 498]), rank 0
2024-05-08 00:30:33.605284: Validation complete
2024-05-08 00:30:33.610549: Mean Validation Dice:  0.7119528736211344
wandb: 
wandb: Run history:
wandb:            ema_fg_dice ▁▁▂▃▄▅▅▆▆▆▆▇▇▇▇▇▇▇██████▇███████████████
wandb:   epoch_end_timestamps ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███
wandb: epoch_start_timestamps ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███
wandb:                    lrs ███▇▇▇▇▇▇▆▆▆▆▆▆▅▅▅▅▅▄▄▄▄▄▄▃▃▃▃▃▂▂▂▂▂▂▁▁▁
wandb:           mean_fg_dice ▁▄▄▆▆▇▆▇▇▅▇▇▇▇▇▇▆▆▇▆▆█▇▇▆▇▇▇▇▇▇▇▇▇█▇██▇▇
wandb:           train_losses █▅▅▄▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:             val_losses █▅▅▃▄▂▃▂▂▄▂▂▂▂▂▃▂▂▂▂▃▁▂▂▃▂▃▂▂▃▂▂▂▂▂▃▁▁▂▃
wandb: 
wandb: Run summary:
wandb:            ema_fg_dice 0.73827
wandb:   epoch_end_timestamps 1715142533.79078
wandb: epoch_start_timestamps 1715142299.00644
wandb:                    lrs 0.00901
wandb:           mean_fg_dice 0.73081
wandb:           train_losses -0.85044
wandb:             val_losses -0.59624
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/gridsan/nchutisilp/datasets/nnUNet_Datasets/nnUNet_results/Dataset302_Calcium_OCTv2/nnUNetTrainer__nnUNetPlans__3d_128x512x512_b10/fold_0/wandb/offline-run-20240507_170844-jzkiitzw
wandb: Find logs at: /home/gridsan/nchutisilp/datasets/nnUNet_Datasets/nnUNet_results/Dataset302_Calcium_OCTv2/nnUNetTrainer__nnUNetPlans__3d_128x512x512_b10/fold_0/wandb/offline-run-20240507_170844-jzkiitzw/logs
WARNING:urllib3.connectionpool:Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x7f9c37afd520>: Failed to establish a new connection: [Errno 101] Network is unreachable')': /api/4504800232407040/envelope/
WARNING:urllib3.connectionpool:Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x7f9c67adc100>: Failed to establish a new connection: [Errno 101] Network is unreachable')': /api/4504800232407040/envelope/
WARNING:urllib3.connectionpool:Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x7f9c37a2f070>: Failed to establish a new connection: [Errno 101] Network is unreachable')': /api/4504800232407040/envelope/
WARNING:urllib3.connectionpool:Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x7f9c323b3070>: Failed to establish a new connection: [Errno 101] Network is unreachable')': /api/4504800232407040/envelope/
WARNING:urllib3.connectionpool:Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x7f9c34aa8130>: Failed to establish a new connection: [Errno 101] Network is unreachable')': /api/4504800232407040/envelope/
WARNING:urllib3.connectionpool:Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x7f9c62b2ce20>: Failed to establish a new connection: [Errno 101] Network is unreachable')': /api/4504800232407040/envelope/
FOLD 0 CONFIG 3d_128x512x512_b10 TRAINER nnUNetTrainer
usage: nnUNetv2_predict [-h] -i I -o O -d D [-p P] [-tr TR] -c C
                        [-f F [F ...]] [-step_size STEP_SIZE] [--disable_tta]
                        [--verbose] [--save_probabilities]
                        [--continue_prediction] [-chk CHK] [-npp NPP]
                        [-nps NPS]
                        [-prev_stage_predictions PREV_STAGE_PREDICTIONS]
                        [-num_parts NUM_PARTS] [-part_id PART_ID]
                        [-device DEVICE] [--disable_progress_bar]
nnUNetv2_predict: error: argument -d: expected one argument

#######################################################################
Please cite the following paper when using nnU-Net:
Isensee, F., Jaeger, P. F., Kohl, S. A., Petersen, J., & Maier-Hein, K. H. (2021). nnU-Net: a self-configuring method for deep learning-based biomedical image segmentation. Nature methods, 18(2), 203-211.
#######################################################################

Traceback (most recent call last):
  File "/home/gridsan/nchutisilp/.local/bin/nnUNetv2_evaluate_simple", line 8, in <module>
    sys.exit(evaluate_simple_entry_point())
  File "/home/gridsan/nchutisilp/projects/nnUNet/nnunetv2/evaluation/evaluate_predictions.py", line 251, in evaluate_simple_entry_point
    compute_metrics_on_folder_simple(args.gt_folder, args.pred_folder, args.l, args.o, args.np, args.il, chill=args.chill)
  File "/home/gridsan/nchutisilp/projects/nnUNet/nnunetv2/evaluation/evaluate_predictions.py", line 213, in compute_metrics_on_folder_simple
    compute_metrics_on_folder(folder_ref, folder_pred, output_file, rw, file_ending,
  File "/home/gridsan/nchutisilp/projects/nnUNet/nnunetv2/evaluation/evaluate_predictions.py", line 139, in compute_metrics_on_folder
    assert all(present), "Not all files in folder_ref exist in folder_pred"
AssertionError: Not all files in folder_ref exist in folder_pred
Completed FOLD 0 CONFIG 3d_128x512x512_b10 TRAINER nnUNetTrainer
