/home/gridsan/nchutisilp/.conda/envs/nnunet/bin/python
wandb: Tracking run with wandb version 0.16.6
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.

############################
INFO: You are using the old nnU-Net default plans. We have updated our recommendations. Please consider using those instead! Read more here: https://github.com/MIC-DKFZ/nnUNet/blob/master/documentation/resenc_presets.md
############################

Using device: cuda:0

#######################################################################
Please cite the following paper when using nnU-Net:
Isensee, F., Jaeger, P. F., Kohl, S. A., Petersen, J., & Maier-Hein, K. H. (2021). nnU-Net: a self-configuring method for deep learning-based biomedical image segmentation. Nature methods, 18(2), 203-211.
#######################################################################

2024-05-08 13:59:23.322526: Using torch.compile...
/home/gridsan/nchutisilp/.local/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn("The verbose parameter is deprecated. Please use get_last_lr() "
################### Loading pretrained weights from file  /home/gridsan/nchutisilp/datasets/nnUNet_Datasets/nnUNet_results/Dataset300_Lumen_and_Wall_OCT/nnUNetTrainer__nnUNetPreprocessPlans__3d_fullres/fold_all/checkpoint_best.pth ###################
Below is the list of overlapping blocks in pretrained model and nnUNet architecture:
encoder.stages.0.0.convs.0.conv.weight shape torch.Size([32, 1, 3, 3, 3])
encoder.stages.0.0.convs.0.conv.bias shape torch.Size([32])
encoder.stages.0.0.convs.0.norm.weight shape torch.Size([32])
encoder.stages.0.0.convs.0.norm.bias shape torch.Size([32])
encoder.stages.0.0.convs.0.all_modules.0.weight shape torch.Size([32, 1, 3, 3, 3])
encoder.stages.0.0.convs.0.all_modules.0.bias shape torch.Size([32])
encoder.stages.0.0.convs.0.all_modules.1.weight shape torch.Size([32])
encoder.stages.0.0.convs.0.all_modules.1.bias shape torch.Size([32])
encoder.stages.0.0.convs.1.conv.weight shape torch.Size([32, 32, 3, 3, 3])
encoder.stages.0.0.convs.1.conv.bias shape torch.Size([32])
encoder.stages.0.0.convs.1.norm.weight shape torch.Size([32])
encoder.stages.0.0.convs.1.norm.bias shape torch.Size([32])
encoder.stages.0.0.convs.1.all_modules.0.weight shape torch.Size([32, 32, 3, 3, 3])
encoder.stages.0.0.convs.1.all_modules.0.bias shape torch.Size([32])
encoder.stages.0.0.convs.1.all_modules.1.weight shape torch.Size([32])
encoder.stages.0.0.convs.1.all_modules.1.bias shape torch.Size([32])
encoder.stages.1.0.convs.0.conv.weight shape torch.Size([64, 32, 3, 3, 3])
encoder.stages.1.0.convs.0.conv.bias shape torch.Size([64])
encoder.stages.1.0.convs.0.norm.weight shape torch.Size([64])
encoder.stages.1.0.convs.0.norm.bias shape torch.Size([64])
encoder.stages.1.0.convs.0.all_modules.0.weight shape torch.Size([64, 32, 3, 3, 3])
encoder.stages.1.0.convs.0.all_modules.0.bias shape torch.Size([64])
encoder.stages.1.0.convs.0.all_modules.1.weight shape torch.Size([64])
encoder.stages.1.0.convs.0.all_modules.1.bias shape torch.Size([64])
encoder.stages.1.0.convs.1.conv.weight shape torch.Size([64, 64, 3, 3, 3])
encoder.stages.1.0.convs.1.conv.bias shape torch.Size([64])
encoder.stages.1.0.convs.1.norm.weight shape torch.Size([64])
encoder.stages.1.0.convs.1.norm.bias shape torch.Size([64])
encoder.stages.1.0.convs.1.all_modules.0.weight shape torch.Size([64, 64, 3, 3, 3])
encoder.stages.1.0.convs.1.all_modules.0.bias shape torch.Size([64])
encoder.stages.1.0.convs.1.all_modules.1.weight shape torch.Size([64])
encoder.stages.1.0.convs.1.all_modules.1.bias shape torch.Size([64])
encoder.stages.2.0.convs.0.conv.weight shape torch.Size([128, 64, 3, 3, 3])
encoder.stages.2.0.convs.0.conv.bias shape torch.Size([128])
encoder.stages.2.0.convs.0.norm.weight shape torch.Size([128])
encoder.stages.2.0.convs.0.norm.bias shape torch.Size([128])
encoder.stages.2.0.convs.0.all_modules.0.weight shape torch.Size([128, 64, 3, 3, 3])
encoder.stages.2.0.convs.0.all_modules.0.bias shape torch.Size([128])
encoder.stages.2.0.convs.0.all_modules.1.weight shape torch.Size([128])
encoder.stages.2.0.convs.0.all_modules.1.bias shape torch.Size([128])
encoder.stages.2.0.convs.1.conv.weight shape torch.Size([128, 128, 3, 3, 3])
encoder.stages.2.0.convs.1.conv.bias shape torch.Size([128])
encoder.stages.2.0.convs.1.norm.weight shape torch.Size([128])
encoder.stages.2.0.convs.1.norm.bias shape torch.Size([128])
encoder.stages.2.0.convs.1.all_modules.0.weight shape torch.Size([128, 128, 3, 3, 3])
encoder.stages.2.0.convs.1.all_modules.0.bias shape torch.Size([128])
encoder.stages.2.0.convs.1.all_modules.1.weight shape torch.Size([128])
encoder.stages.2.0.convs.1.all_modules.1.bias shape torch.Size([128])
encoder.stages.3.0.convs.0.conv.weight shape torch.Size([256, 128, 3, 3, 3])
encoder.stages.3.0.convs.0.conv.bias shape torch.Size([256])
encoder.stages.3.0.convs.0.norm.weight shape torch.Size([256])
encoder.stages.3.0.convs.0.norm.bias shape torch.Size([256])
encoder.stages.3.0.convs.0.all_modules.0.weight shape torch.Size([256, 128, 3, 3, 3])
encoder.stages.3.0.convs.0.all_modules.0.bias shape torch.Size([256])
encoder.stages.3.0.convs.0.all_modules.1.weight shape torch.Size([256])
encoder.stages.3.0.convs.0.all_modules.1.bias shape torch.Size([256])
encoder.stages.3.0.convs.1.conv.weight shape torch.Size([256, 256, 3, 3, 3])
encoder.stages.3.0.convs.1.conv.bias shape torch.Size([256])
encoder.stages.3.0.convs.1.norm.weight shape torch.Size([256])
encoder.stages.3.0.convs.1.norm.bias shape torch.Size([256])
encoder.stages.3.0.convs.1.all_modules.0.weight shape torch.Size([256, 256, 3, 3, 3])
encoder.stages.3.0.convs.1.all_modules.0.bias shape torch.Size([256])
encoder.stages.3.0.convs.1.all_modules.1.weight shape torch.Size([256])
encoder.stages.3.0.convs.1.all_modules.1.bias shape torch.Size([256])
encoder.stages.4.0.convs.0.conv.weight shape torch.Size([320, 256, 3, 3, 3])
encoder.stages.4.0.convs.0.conv.bias shape torch.Size([320])
encoder.stages.4.0.convs.0.norm.weight shape torch.Size([320])
encoder.stages.4.0.convs.0.norm.bias shape torch.Size([320])
encoder.stages.4.0.convs.0.all_modules.0.weight shape torch.Size([320, 256, 3, 3, 3])
encoder.stages.4.0.convs.0.all_modules.0.bias shape torch.Size([320])
encoder.stages.4.0.convs.0.all_modules.1.weight shape torch.Size([320])
encoder.stages.4.0.convs.0.all_modules.1.bias shape torch.Size([320])
encoder.stages.4.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.4.0.convs.1.conv.bias shape torch.Size([320])
encoder.stages.4.0.convs.1.norm.weight shape torch.Size([320])
encoder.stages.4.0.convs.1.norm.bias shape torch.Size([320])
encoder.stages.4.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.4.0.convs.1.all_modules.0.bias shape torch.Size([320])
encoder.stages.4.0.convs.1.all_modules.1.weight shape torch.Size([320])
encoder.stages.4.0.convs.1.all_modules.1.bias shape torch.Size([320])
encoder.stages.5.0.convs.0.conv.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.5.0.convs.0.conv.bias shape torch.Size([320])
encoder.stages.5.0.convs.0.norm.weight shape torch.Size([320])
encoder.stages.5.0.convs.0.norm.bias shape torch.Size([320])
encoder.stages.5.0.convs.0.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.5.0.convs.0.all_modules.0.bias shape torch.Size([320])
encoder.stages.5.0.convs.0.all_modules.1.weight shape torch.Size([320])
encoder.stages.5.0.convs.0.all_modules.1.bias shape torch.Size([320])
encoder.stages.5.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.5.0.convs.1.conv.bias shape torch.Size([320])
encoder.stages.5.0.convs.1.norm.weight shape torch.Size([320])
encoder.stages.5.0.convs.1.norm.bias shape torch.Size([320])
encoder.stages.5.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.5.0.convs.1.all_modules.0.bias shape torch.Size([320])
encoder.stages.5.0.convs.1.all_modules.1.weight shape torch.Size([320])
encoder.stages.5.0.convs.1.all_modules.1.bias shape torch.Size([320])
decoder.encoder.stages.0.0.convs.0.conv.weight shape torch.Size([32, 1, 3, 3, 3])
decoder.encoder.stages.0.0.convs.0.conv.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.norm.weight shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.norm.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.all_modules.0.weight shape torch.Size([32, 1, 3, 3, 3])
decoder.encoder.stages.0.0.convs.0.all_modules.0.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.all_modules.1.weight shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.all_modules.1.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.conv.weight shape torch.Size([32, 32, 3, 3, 3])
decoder.encoder.stages.0.0.convs.1.conv.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.norm.weight shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.norm.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.all_modules.0.weight shape torch.Size([32, 32, 3, 3, 3])
decoder.encoder.stages.0.0.convs.1.all_modules.0.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.all_modules.1.weight shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.all_modules.1.bias shape torch.Size([32])
decoder.encoder.stages.1.0.convs.0.conv.weight shape torch.Size([64, 32, 3, 3, 3])
decoder.encoder.stages.1.0.convs.0.conv.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.norm.weight shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.norm.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.all_modules.0.weight shape torch.Size([64, 32, 3, 3, 3])
decoder.encoder.stages.1.0.convs.0.all_modules.0.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.all_modules.1.weight shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.all_modules.1.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.conv.weight shape torch.Size([64, 64, 3, 3, 3])
decoder.encoder.stages.1.0.convs.1.conv.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.norm.weight shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.norm.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.all_modules.0.weight shape torch.Size([64, 64, 3, 3, 3])
decoder.encoder.stages.1.0.convs.1.all_modules.0.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.all_modules.1.weight shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.all_modules.1.bias shape torch.Size([64])
decoder.encoder.stages.2.0.convs.0.conv.weight shape torch.Size([128, 64, 3, 3, 3])
decoder.encoder.stages.2.0.convs.0.conv.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.norm.weight shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.norm.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.all_modules.0.weight shape torch.Size([128, 64, 3, 3, 3])
decoder.encoder.stages.2.0.convs.0.all_modules.0.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.all_modules.1.weight shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.all_modules.1.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.conv.weight shape torch.Size([128, 128, 3, 3, 3])
decoder.encoder.stages.2.0.convs.1.conv.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.norm.weight shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.norm.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.all_modules.0.weight shape torch.Size([128, 128, 3, 3, 3])
decoder.encoder.stages.2.0.convs.1.all_modules.0.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.all_modules.1.weight shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.all_modules.1.bias shape torch.Size([128])
decoder.encoder.stages.3.0.convs.0.conv.weight shape torch.Size([256, 128, 3, 3, 3])
decoder.encoder.stages.3.0.convs.0.conv.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.norm.weight shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.norm.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.all_modules.0.weight shape torch.Size([256, 128, 3, 3, 3])
decoder.encoder.stages.3.0.convs.0.all_modules.0.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.all_modules.1.weight shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.all_modules.1.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.conv.weight shape torch.Size([256, 256, 3, 3, 3])
decoder.encoder.stages.3.0.convs.1.conv.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.norm.weight shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.norm.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.all_modules.0.weight shape torch.Size([256, 256, 3, 3, 3])
decoder.encoder.stages.3.0.convs.1.all_modules.0.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.all_modules.1.weight shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.all_modules.1.bias shape torch.Size([256])
decoder.encoder.stages.4.0.convs.0.conv.weight shape torch.Size([320, 256, 3, 3, 3])
decoder.encoder.stages.4.0.convs.0.conv.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.norm.weight shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.norm.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.all_modules.0.weight shape torch.Size([320, 256, 3, 3, 3])
decoder.encoder.stages.4.0.convs.0.all_modules.0.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.all_modules.1.weight shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.all_modules.1.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.4.0.convs.1.conv.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.norm.weight shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.norm.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.4.0.convs.1.all_modules.0.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.all_modules.1.weight shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.all_modules.1.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.conv.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.5.0.convs.0.conv.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.norm.weight shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.norm.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.5.0.convs.0.all_modules.0.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.all_modules.1.weight shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.all_modules.1.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.5.0.convs.1.conv.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.norm.weight shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.norm.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.5.0.convs.1.all_modules.0.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.all_modules.1.weight shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.all_modules.1.bias shape torch.Size([320])
decoder.stages.0.convs.0.conv.weight shape torch.Size([320, 640, 3, 3, 3])
decoder.stages.0.convs.0.conv.bias shape torch.Size([320])
decoder.stages.0.convs.0.norm.weight shape torch.Size([320])
decoder.stages.0.convs.0.norm.bias shape torch.Size([320])
decoder.stages.0.convs.0.all_modules.0.weight shape torch.Size([320, 640, 3, 3, 3])
decoder.stages.0.convs.0.all_modules.0.bias shape torch.Size([320])
decoder.stages.0.convs.0.all_modules.1.weight shape torch.Size([320])
decoder.stages.0.convs.0.all_modules.1.bias shape torch.Size([320])
decoder.stages.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.stages.0.convs.1.conv.bias shape torch.Size([320])
decoder.stages.0.convs.1.norm.weight shape torch.Size([320])
decoder.stages.0.convs.1.norm.bias shape torch.Size([320])
decoder.stages.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.stages.0.convs.1.all_modules.0.bias shape torch.Size([320])
decoder.stages.0.convs.1.all_modules.1.weight shape torch.Size([320])
decoder.stages.0.convs.1.all_modules.1.bias shape torch.Size([320])
decoder.stages.1.convs.0.conv.weight shape torch.Size([256, 512, 3, 3, 3])
decoder.stages.1.convs.0.conv.bias shape torch.Size([256])
decoder.stages.1.convs.0.norm.weight shape torch.Size([256])
decoder.stages.1.convs.0.norm.bias shape torch.Size([256])
decoder.stages.1.convs.0.all_modules.0.weight shape torch.Size([256, 512, 3, 3, 3])
decoder.stages.1.convs.0.all_modules.0.bias shape torch.Size([256])
decoder.stages.1.convs.0.all_modules.1.weight shape torch.Size([256])
decoder.stages.1.convs.0.all_modules.1.bias shape torch.Size([256])
decoder.stages.1.convs.1.conv.weight shape torch.Size([256, 256, 3, 3, 3])
decoder.stages.1.convs.1.conv.bias shape torch.Size([256])
decoder.stages.1.convs.1.norm.weight shape torch.Size([256])
decoder.stages.1.convs.1.norm.bias shape torch.Size([256])
decoder.stages.1.convs.1.all_modules.0.weight shape torch.Size([256, 256, 3, 3, 3])
decoder.stages.1.convs.1.all_modules.0.bias shape torch.Size([256])
decoder.stages.1.convs.1.all_modules.1.weight shape torch.Size([256])
decoder.stages.1.convs.1.all_modules.1.bias shape torch.Size([256])
decoder.stages.2.convs.0.conv.weight shape torch.Size([128, 256, 3, 3, 3])
decoder.stages.2.convs.0.conv.bias shape torch.Size([128])
decoder.stages.2.convs.0.norm.weight shape torch.Size([128])
decoder.stages.2.convs.0.norm.bias shape torch.Size([128])
decoder.stages.2.convs.0.all_modules.0.weight shape torch.Size([128, 256, 3, 3, 3])
decoder.stages.2.convs.0.all_modules.0.bias shape torch.Size([128])
decoder.stages.2.convs.0.all_modules.1.weight shape torch.Size([128])
decoder.stages.2.convs.0.all_modules.1.bias shape torch.Size([128])
decoder.stages.2.convs.1.conv.weight shape torch.Size([128, 128, 3, 3, 3])
decoder.stages.2.convs.1.conv.bias shape torch.Size([128])
decoder.stages.2.convs.1.norm.weight shape torch.Size([128])
decoder.stages.2.convs.1.norm.bias shape torch.Size([128])
decoder.stages.2.convs.1.all_modules.0.weight shape torch.Size([128, 128, 3, 3, 3])
decoder.stages.2.convs.1.all_modules.0.bias shape torch.Size([128])
decoder.stages.2.convs.1.all_modules.1.weight shape torch.Size([128])
decoder.stages.2.convs.1.all_modules.1.bias shape torch.Size([128])
decoder.stages.3.convs.0.conv.weight shape torch.Size([64, 128, 3, 3, 3])
decoder.stages.3.convs.0.conv.bias shape torch.Size([64])
decoder.stages.3.convs.0.norm.weight shape torch.Size([64])
decoder.stages.3.convs.0.norm.bias shape torch.Size([64])
decoder.stages.3.convs.0.all_modules.0.weight shape torch.Size([64, 128, 3, 3, 3])
decoder.stages.3.convs.0.all_modules.0.bias shape torch.Size([64])
decoder.stages.3.convs.0.all_modules.1.weight shape torch.Size([64])
decoder.stages.3.convs.0.all_modules.1.bias shape torch.Size([64])
decoder.stages.3.convs.1.conv.weight shape torch.Size([64, 64, 3, 3, 3])
decoder.stages.3.convs.1.conv.bias shape torch.Size([64])
decoder.stages.3.convs.1.norm.weight shape torch.Size([64])
decoder.stages.3.convs.1.norm.bias shape torch.Size([64])
decoder.stages.3.convs.1.all_modules.0.weight shape torch.Size([64, 64, 3, 3, 3])
decoder.stages.3.convs.1.all_modules.0.bias shape torch.Size([64])
decoder.stages.3.convs.1.all_modules.1.weight shape torch.Size([64])
decoder.stages.3.convs.1.all_modules.1.bias shape torch.Size([64])
decoder.stages.4.convs.0.conv.weight shape torch.Size([32, 64, 3, 3, 3])
decoder.stages.4.convs.0.conv.bias shape torch.Size([32])
decoder.stages.4.convs.0.norm.weight shape torch.Size([32])
decoder.stages.4.convs.0.norm.bias shape torch.Size([32])
decoder.stages.4.convs.0.all_modules.0.weight shape torch.Size([32, 64, 3, 3, 3])
decoder.stages.4.convs.0.all_modules.0.bias shape torch.Size([32])
decoder.stages.4.convs.0.all_modules.1.weight shape torch.Size([32])
decoder.stages.4.convs.0.all_modules.1.bias shape torch.Size([32])
decoder.stages.4.convs.1.conv.weight shape torch.Size([32, 32, 3, 3, 3])
decoder.stages.4.convs.1.conv.bias shape torch.Size([32])
decoder.stages.4.convs.1.norm.weight shape torch.Size([32])
decoder.stages.4.convs.1.norm.bias shape torch.Size([32])
decoder.stages.4.convs.1.all_modules.0.weight shape torch.Size([32, 32, 3, 3, 3])
decoder.stages.4.convs.1.all_modules.0.bias shape torch.Size([32])
decoder.stages.4.convs.1.all_modules.1.weight shape torch.Size([32])
decoder.stages.4.convs.1.all_modules.1.bias shape torch.Size([32])
decoder.transpconvs.0.weight shape torch.Size([320, 320, 1, 2, 2])
decoder.transpconvs.0.bias shape torch.Size([320])
decoder.transpconvs.1.weight shape torch.Size([320, 256, 2, 2, 2])
decoder.transpconvs.1.bias shape torch.Size([256])
decoder.transpconvs.2.weight shape torch.Size([256, 128, 2, 2, 2])
decoder.transpconvs.2.bias shape torch.Size([128])
decoder.transpconvs.3.weight shape torch.Size([128, 64, 2, 2, 2])
decoder.transpconvs.3.bias shape torch.Size([64])
decoder.transpconvs.4.weight shape torch.Size([64, 32, 2, 2, 2])
decoder.transpconvs.4.bias shape torch.Size([32])
################### Done ###################
2024-05-08 13:59:48.649406: do_dummy_2d_data_aug: True
2024-05-08 13:59:48.674296: Using splits from existing split file: /home/gridsan/nchutisilp/datasets/nnUNet_Datasets/nnUNet_preprocessed/Dataset302_Calcium_OCTv2/splits_final.json
2024-05-08 13:59:48.692173: The split file contains 3 splits.
2024-05-08 13:59:48.694657: Desired fold for training: 2
2024-05-08 13:59:48.696577: This split has 4 training and 2 validation cases.
using pin_memory on device 0
using pin_memory on device 0

This is the configuration used by this training:
Configuration name: 3d_32x512x512_b2
 {'data_identifier': 'nnUNetPlans_3d_fullres', 'preprocessor_name': 'DefaultPreprocessor', 'batch_size': 2, 'patch_size': [32, 512, 512], 'median_image_size_in_voxels': [375.0, 498.0, 498.0], 'spacing': [1.0, 1.0, 1.0], 'normalization_schemes': ['ZScoreNormalization'], 'use_mask_for_norm': [False], 'resampling_fn_data': 'resample_data_or_seg_to_shape', 'resampling_fn_seg': 'resample_data_or_seg_to_shape', 'resampling_fn_data_kwargs': {'is_seg': False, 'order': 3, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_seg_kwargs': {'is_seg': True, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_probabilities': 'resample_data_or_seg_to_shape', 'resampling_fn_probabilities_kwargs': {'is_seg': False, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'architecture': {'network_class_name': 'dynamic_network_architectures.architectures.unet.PlainConvUNet', 'arch_kwargs': {'n_stages': 6, 'features_per_stage': [32, 64, 128, 256, 320, 320], 'conv_op': 'torch.nn.modules.conv.Conv3d', 'kernel_sizes': [[3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3]], 'strides': [[1, 1, 1], [2, 2, 2], [2, 2, 2], [2, 2, 2], [2, 2, 2], [1, 2, 2]], 'n_conv_per_stage': [2, 2, 2, 2, 2, 2], 'n_conv_per_stage_decoder': [2, 2, 2, 2, 2], 'conv_bias': True, 'norm_op': 'torch.nn.modules.instancenorm.InstanceNorm3d', 'norm_op_kwargs': {'eps': 1e-05, 'affine': True}, 'dropout_op': None, 'dropout_op_kwargs': None, 'nonlin': 'torch.nn.LeakyReLU', 'nonlin_kwargs': {'inplace': True}, 'deep_supervision': True}, '_kw_requires_import': ['conv_op', 'norm_op', 'dropout_op', 'nonlin']}, 'batch_dice': True, 'inherits_from': '3d_fullres'} 

These are the global plan.json settings:
 {'dataset_name': 'Dataset302_Calcium_OCTv2', 'plans_name': 'nnUNetPlans', 'original_median_spacing_after_transp': [1.0, 1.0, 1.0], 'original_median_shape_after_transp': [375, 498, 498], 'image_reader_writer': 'SimpleITKIO', 'transpose_forward': [0, 1, 2], 'transpose_backward': [0, 1, 2], 'experiment_planner_used': 'ExperimentPlanner', 'label_manager': 'LabelManager', 'foreground_intensity_properties_per_channel': {'0': {'max': 0.4977976381778717, 'mean': 0.13332499563694, 'median': 0.08871842920780182, 'min': 0.0, 'percentile_00_5': 0.014754901640117168, 'percentile_99_5': 0.4977976381778717, 'std': 0.12022686749696732}}} 

2024-05-08 14:00:08.412748: unpacking dataset...
2024-05-08 14:00:14.619538: unpacking done...
2024-05-08 14:00:14.652123: Unable to plot network architecture: nnUNet_compile is enabled!
2024-05-08 14:00:14.748574: 
2024-05-08 14:00:14.750696: Epoch 0
2024-05-08 14:00:14.752509: Current learning rate: 0.01
2024-05-08 14:06:25.277378: Validation loss improved from 1000.00000 to -0.51325! Patience: 0/50
2024-05-08 14:06:25.280045: train_loss -0.4384
2024-05-08 14:06:25.282778: val_loss -0.5133
2024-05-08 14:06:25.284357: Pseudo dice [0.6575]
2024-05-08 14:06:25.285416: Epoch time: 370.53 s
2024-05-08 14:06:25.286322: Yayy! New best EMA pseudo Dice: 0.6575
2024-05-08 14:06:26.697616: 
2024-05-08 14:06:26.700560: Epoch 1
2024-05-08 14:06:26.702374: Current learning rate: 0.00999
2024-05-08 14:10:19.934103: Validation loss did not improve from -0.51325. Patience: 1/50
2024-05-08 14:10:19.935773: train_loss -0.6022
2024-05-08 14:10:19.937704: val_loss -0.4432
2024-05-08 14:10:19.938827: Pseudo dice [0.6131]
2024-05-08 14:10:19.940194: Epoch time: 233.24 s
2024-05-08 14:10:21.114910: 
2024-05-08 14:10:21.117656: Epoch 2
2024-05-08 14:10:21.119168: Current learning rate: 0.00998
2024-05-08 14:14:13.963386: Validation loss improved from -0.51325 to -0.54535! Patience: 1/50
2024-05-08 14:14:13.965505: train_loss -0.6266
2024-05-08 14:14:13.967219: val_loss -0.5453
2024-05-08 14:14:13.968575: Pseudo dice [0.6799]
2024-05-08 14:14:13.969819: Epoch time: 232.85 s
2024-05-08 14:14:15.270130: 
2024-05-08 14:14:15.272583: Epoch 3
2024-05-08 14:14:15.274106: Current learning rate: 0.00997
2024-05-08 14:18:07.564029: Validation loss improved from -0.54535 to -0.55799! Patience: 0/50
2024-05-08 14:18:07.565793: train_loss -0.6372
2024-05-08 14:18:07.567430: val_loss -0.558
2024-05-08 14:18:07.568687: Pseudo dice [0.691]
2024-05-08 14:18:07.569887: Epoch time: 232.3 s
2024-05-08 14:18:07.570940: Yayy! New best EMA pseudo Dice: 0.6593
2024-05-08 14:18:09.157672: 
2024-05-08 14:18:09.159810: Epoch 4
2024-05-08 14:18:09.161073: Current learning rate: 0.00996
2024-05-08 14:22:01.003462: Validation loss improved from -0.55799 to -0.56547! Patience: 0/50
2024-05-08 14:22:01.005281: train_loss -0.6397
2024-05-08 14:22:01.007454: val_loss -0.5655
2024-05-08 14:22:01.009368: Pseudo dice [0.6861]
2024-05-08 14:22:01.010492: Epoch time: 231.85 s
2024-05-08 14:22:01.493497: Yayy! New best EMA pseudo Dice: 0.662
2024-05-08 14:22:03.087038: 
2024-05-08 14:22:03.089933: Epoch 5
2024-05-08 14:22:03.091851: Current learning rate: 0.00995
2024-05-08 14:25:55.871883: Validation loss did not improve from -0.56547. Patience: 1/50
2024-05-08 14:25:55.873732: train_loss -0.6553
2024-05-08 14:25:55.875285: val_loss -0.5275
2024-05-08 14:25:55.876523: Pseudo dice [0.6686]
2024-05-08 14:25:55.877831: Epoch time: 232.79 s
2024-05-08 14:25:55.879318: Yayy! New best EMA pseudo Dice: 0.6626
2024-05-08 14:25:57.403404: 
2024-05-08 14:25:57.406184: Epoch 6
2024-05-08 14:25:57.407623: Current learning rate: 0.00995
2024-05-08 14:29:50.170839: Validation loss did not improve from -0.56547. Patience: 2/50
2024-05-08 14:29:50.173105: train_loss -0.6826
2024-05-08 14:29:50.175399: val_loss -0.5284
2024-05-08 14:29:50.176765: Pseudo dice [0.672]
2024-05-08 14:29:50.177934: Epoch time: 232.77 s
2024-05-08 14:29:50.178990: Yayy! New best EMA pseudo Dice: 0.6636
2024-05-08 14:29:51.740721: 
2024-05-08 14:29:51.743336: Epoch 7
2024-05-08 14:29:51.745119: Current learning rate: 0.00994
2024-05-08 14:33:45.431718: Validation loss improved from -0.56547 to -0.56880! Patience: 2/50
2024-05-08 14:33:45.433241: train_loss -0.6845
2024-05-08 14:33:45.434508: val_loss -0.5688
2024-05-08 14:33:45.436141: Pseudo dice [0.6998]
2024-05-08 14:33:45.437246: Epoch time: 233.69 s
2024-05-08 14:33:45.438368: Yayy! New best EMA pseudo Dice: 0.6672
2024-05-08 14:33:47.414523: 
2024-05-08 14:33:47.417180: Epoch 8
2024-05-08 14:33:47.419533: Current learning rate: 0.00993
2024-05-08 14:37:41.426841: Validation loss improved from -0.56880 to -0.57949! Patience: 0/50
2024-05-08 14:37:41.428589: train_loss -0.7037
2024-05-08 14:37:41.430371: val_loss -0.5795
2024-05-08 14:37:41.431975: Pseudo dice [0.6971]
2024-05-08 14:37:41.433310: Epoch time: 234.02 s
2024-05-08 14:37:41.434471: Yayy! New best EMA pseudo Dice: 0.6702
2024-05-08 14:37:43.031747: 
2024-05-08 14:37:43.034875: Epoch 9
2024-05-08 14:37:43.036753: Current learning rate: 0.00992
2024-05-08 14:41:37.295055: Validation loss did not improve from -0.57949. Patience: 1/50
2024-05-08 14:41:37.296555: train_loss -0.6973
2024-05-08 14:41:37.297915: val_loss -0.5766
2024-05-08 14:41:37.299224: Pseudo dice [0.6978]
2024-05-08 14:41:37.300329: Epoch time: 234.27 s
2024-05-08 14:41:37.647379: Yayy! New best EMA pseudo Dice: 0.6729
2024-05-08 14:41:39.156534: 
2024-05-08 14:41:39.159377: Epoch 10
2024-05-08 14:41:39.160888: Current learning rate: 0.00991
2024-05-08 14:45:33.243446: Validation loss improved from -0.57949 to -0.60958! Patience: 1/50
2024-05-08 14:45:33.245016: train_loss -0.7103
2024-05-08 14:45:33.246831: val_loss -0.6096
2024-05-08 14:45:33.247850: Pseudo dice [0.7287]
2024-05-08 14:45:33.248831: Epoch time: 234.09 s
2024-05-08 14:45:33.249713: Yayy! New best EMA pseudo Dice: 0.6785
2024-05-08 14:45:34.767589: 
2024-05-08 14:45:34.769871: Epoch 11
2024-05-08 14:45:34.770913: Current learning rate: 0.0099
2024-05-08 14:49:29.348956: Validation loss did not improve from -0.60958. Patience: 1/50
2024-05-08 14:49:29.351595: train_loss -0.7046
2024-05-08 14:49:29.353428: val_loss -0.5797
2024-05-08 14:49:29.354737: Pseudo dice [0.7005]
2024-05-08 14:49:29.356018: Epoch time: 234.59 s
2024-05-08 14:49:29.357238: Yayy! New best EMA pseudo Dice: 0.6807
2024-05-08 14:49:30.894595: 
2024-05-08 14:49:30.897275: Epoch 12
2024-05-08 14:49:30.898671: Current learning rate: 0.00989
2024-05-08 14:53:25.352723: Validation loss did not improve from -0.60958. Patience: 2/50
2024-05-08 14:53:25.355242: train_loss -0.7157
2024-05-08 14:53:25.357048: val_loss -0.5885
2024-05-08 14:53:25.358209: Pseudo dice [0.7072]
2024-05-08 14:53:25.359654: Epoch time: 234.46 s
2024-05-08 14:53:25.360838: Yayy! New best EMA pseudo Dice: 0.6834
2024-05-08 14:53:26.903743: 
2024-05-08 14:53:26.906770: Epoch 13
2024-05-08 14:53:26.908718: Current learning rate: 0.00988
2024-05-08 14:57:21.411854: Validation loss did not improve from -0.60958. Patience: 3/50
2024-05-08 14:57:21.413485: train_loss -0.7238
2024-05-08 14:57:21.414966: val_loss -0.5714
2024-05-08 14:57:21.416168: Pseudo dice [0.701]
2024-05-08 14:57:21.417364: Epoch time: 234.51 s
2024-05-08 14:57:21.418521: Yayy! New best EMA pseudo Dice: 0.6851
2024-05-08 14:57:22.975897: 
2024-05-08 14:57:22.978929: Epoch 14
2024-05-08 14:57:22.980726: Current learning rate: 0.00987
2024-05-08 15:01:21.525865: Validation loss improved from -0.60958 to -0.61323! Patience: 3/50
2024-05-08 15:01:21.528075: train_loss -0.7338
2024-05-08 15:01:21.530035: val_loss -0.6132
2024-05-08 15:01:21.531384: Pseudo dice [0.7241]
2024-05-08 15:01:21.532830: Epoch time: 238.55 s
2024-05-08 15:01:21.895213: Yayy! New best EMA pseudo Dice: 0.689
2024-05-08 15:01:23.406924: 
2024-05-08 15:01:23.409761: Epoch 15
2024-05-08 15:01:23.411260: Current learning rate: 0.00986
2024-05-08 15:05:15.771186: Validation loss did not improve from -0.61323. Patience: 1/50
2024-05-08 15:05:15.773370: train_loss -0.7552
2024-05-08 15:05:15.776012: val_loss -0.5998
2024-05-08 15:05:15.777955: Pseudo dice [0.7237]
2024-05-08 15:05:15.779104: Epoch time: 232.37 s
2024-05-08 15:05:15.780622: Yayy! New best EMA pseudo Dice: 0.6925
2024-05-08 15:05:17.340885: 
2024-05-08 15:05:17.344492: Epoch 16
2024-05-08 15:05:17.346030: Current learning rate: 0.00986
2024-05-08 15:09:13.847145: Validation loss did not improve from -0.61323. Patience: 2/50
2024-05-08 15:09:13.849034: train_loss -0.752
2024-05-08 15:09:13.850911: val_loss -0.5547
2024-05-08 15:09:13.852149: Pseudo dice [0.6907]
2024-05-08 15:09:13.853344: Epoch time: 236.51 s
2024-05-08 15:09:15.114255: 
2024-05-08 15:09:15.116618: Epoch 17
2024-05-08 15:09:15.117817: Current learning rate: 0.00985
2024-05-08 15:13:11.290242: Validation loss did not improve from -0.61323. Patience: 3/50
2024-05-08 15:13:11.294711: train_loss -0.7308
2024-05-08 15:13:11.297421: val_loss -0.5453
2024-05-08 15:13:11.299091: Pseudo dice [0.684]
2024-05-08 15:13:11.300654: Epoch time: 236.18 s
2024-05-08 15:13:12.612861: 
2024-05-08 15:13:12.614977: Epoch 18
2024-05-08 15:13:12.616232: Current learning rate: 0.00984
2024-05-08 15:17:08.384796: Validation loss did not improve from -0.61323. Patience: 4/50
2024-05-08 15:17:08.387182: train_loss -0.7164
2024-05-08 15:17:08.389725: val_loss -0.577
2024-05-08 15:17:08.391824: Pseudo dice [0.7103]
2024-05-08 15:17:08.393334: Epoch time: 235.78 s
2024-05-08 15:17:08.394628: Yayy! New best EMA pseudo Dice: 0.6934
2024-05-08 15:17:11.024020: 
2024-05-08 15:17:11.026954: Epoch 19
2024-05-08 15:17:11.028745: Current learning rate: 0.00983
2024-05-08 15:21:06.624213: Validation loss did not improve from -0.61323. Patience: 5/50
2024-05-08 15:21:06.626220: train_loss -0.7277
2024-05-08 15:21:06.627933: val_loss -0.5341
2024-05-08 15:21:06.628972: Pseudo dice [0.6655]
2024-05-08 15:21:06.629889: Epoch time: 235.6 s
2024-05-08 15:21:08.167410: 
2024-05-08 15:21:08.169738: Epoch 20
2024-05-08 15:21:08.170965: Current learning rate: 0.00982
2024-05-08 15:25:03.777740: Validation loss did not improve from -0.61323. Patience: 6/50
2024-05-08 15:25:03.779337: train_loss -0.7464
2024-05-08 15:25:03.780356: val_loss -0.591
2024-05-08 15:25:03.781480: Pseudo dice [0.709]
2024-05-08 15:25:03.782633: Epoch time: 235.61 s
2024-05-08 15:25:05.045809: 
2024-05-08 15:25:05.047815: Epoch 21
2024-05-08 15:25:05.049247: Current learning rate: 0.00981
2024-05-08 15:29:00.074752: Validation loss did not improve from -0.61323. Patience: 7/50
2024-05-08 15:29:00.076402: train_loss -0.7452
2024-05-08 15:29:00.077781: val_loss -0.5978
2024-05-08 15:29:00.079475: Pseudo dice [0.7185]
2024-05-08 15:29:00.080983: Epoch time: 235.03 s
2024-05-08 15:29:00.082421: Yayy! New best EMA pseudo Dice: 0.695
2024-05-08 15:29:01.637135: 
2024-05-08 15:29:01.640285: Epoch 22
2024-05-08 15:29:01.642002: Current learning rate: 0.0098
2024-05-08 15:32:55.992145: Validation loss did not improve from -0.61323. Patience: 8/50
2024-05-08 15:32:55.993738: train_loss -0.7461
2024-05-08 15:32:55.995107: val_loss -0.5743
2024-05-08 15:32:55.996070: Pseudo dice [0.7031]
2024-05-08 15:32:55.997257: Epoch time: 234.36 s
2024-05-08 15:32:55.998406: Yayy! New best EMA pseudo Dice: 0.6958
2024-05-08 15:32:57.526635: 
2024-05-08 15:32:57.529369: Epoch 23
2024-05-08 15:32:57.531191: Current learning rate: 0.00979
2024-05-08 15:36:51.108510: Validation loss improved from -0.61323 to -0.62456! Patience: 8/50
2024-05-08 15:36:51.110286: train_loss -0.757
2024-05-08 15:36:51.112152: val_loss -0.6246
2024-05-08 15:36:51.113703: Pseudo dice [0.7418]
2024-05-08 15:36:51.115129: Epoch time: 233.58 s
2024-05-08 15:36:51.116225: Yayy! New best EMA pseudo Dice: 0.7004
2024-05-08 15:36:52.599878: 
2024-05-08 15:36:52.603175: Epoch 24
2024-05-08 15:36:52.605064: Current learning rate: 0.00978
2024-05-08 15:40:46.690520: Validation loss did not improve from -0.62456. Patience: 1/50
2024-05-08 15:40:46.692119: train_loss -0.7676
2024-05-08 15:40:46.693304: val_loss -0.5931
2024-05-08 15:40:46.694275: Pseudo dice [0.7153]
2024-05-08 15:40:46.695343: Epoch time: 234.09 s
2024-05-08 15:40:47.091292: Yayy! New best EMA pseudo Dice: 0.7019
2024-05-08 15:40:48.717015: 
2024-05-08 15:40:48.719481: Epoch 25
2024-05-08 15:40:48.720969: Current learning rate: 0.00977
2024-05-08 15:44:43.246798: Validation loss did not improve from -0.62456. Patience: 2/50
2024-05-08 15:44:43.248265: train_loss -0.7711
2024-05-08 15:44:43.249763: val_loss -0.6175
2024-05-08 15:44:43.250930: Pseudo dice [0.7385]
2024-05-08 15:44:43.251997: Epoch time: 234.53 s
2024-05-08 15:44:43.253071: Yayy! New best EMA pseudo Dice: 0.7056
2024-05-08 15:44:44.922896: 
2024-05-08 15:44:44.924888: Epoch 26
2024-05-08 15:44:44.925997: Current learning rate: 0.00977
2024-05-08 15:48:39.890921: Validation loss improved from -0.62456 to -0.67299! Patience: 2/50
2024-05-08 15:48:39.892334: train_loss -0.7729
2024-05-08 15:48:39.893509: val_loss -0.673
2024-05-08 15:48:39.894447: Pseudo dice [0.7745]
2024-05-08 15:48:39.895572: Epoch time: 234.97 s
2024-05-08 15:48:39.896527: Yayy! New best EMA pseudo Dice: 0.7125
2024-05-08 15:48:41.655863: 
2024-05-08 15:48:41.657776: Epoch 27
2024-05-08 15:48:41.658988: Current learning rate: 0.00976
2024-05-08 15:52:37.683744: Validation loss did not improve from -0.67299. Patience: 1/50
2024-05-08 15:52:37.685271: train_loss -0.7737
2024-05-08 15:52:37.686384: val_loss -0.5789
2024-05-08 15:52:37.687620: Pseudo dice [0.7099]
2024-05-08 15:52:37.688913: Epoch time: 236.03 s
2024-05-08 15:52:39.061002: 
2024-05-08 15:52:39.062719: Epoch 28
2024-05-08 15:52:39.064149: Current learning rate: 0.00975
2024-05-08 15:56:42.834535: Validation loss did not improve from -0.67299. Patience: 2/50
2024-05-08 15:56:42.836005: train_loss -0.7644
2024-05-08 15:56:42.837211: val_loss -0.6109
2024-05-08 15:56:42.838161: Pseudo dice [0.7206]
2024-05-08 15:56:42.839315: Epoch time: 243.78 s
2024-05-08 15:56:42.840557: Yayy! New best EMA pseudo Dice: 0.7131
2024-05-08 15:56:44.572891: 
2024-05-08 15:56:44.574985: Epoch 29
2024-05-08 15:56:44.576051: Current learning rate: 0.00974
2024-05-08 16:00:46.116586: Validation loss did not improve from -0.67299. Patience: 3/50
2024-05-08 16:00:46.117854: train_loss -0.7706
2024-05-08 16:00:46.118950: val_loss -0.6505
2024-05-08 16:00:46.119998: Pseudo dice [0.7587]
2024-05-08 16:00:46.121070: Epoch time: 241.55 s
2024-05-08 16:00:46.516633: Yayy! New best EMA pseudo Dice: 0.7176
2024-05-08 16:00:48.652982: 
2024-05-08 16:00:48.655176: Epoch 30
2024-05-08 16:00:48.656388: Current learning rate: 0.00973
2024-05-08 16:04:51.025940: Validation loss did not improve from -0.67299. Patience: 4/50
2024-05-08 16:04:51.027555: train_loss -0.7621
2024-05-08 16:04:51.028653: val_loss -0.5646
2024-05-08 16:04:51.029850: Pseudo dice [0.7056]
2024-05-08 16:04:51.031035: Epoch time: 242.38 s
2024-05-08 16:04:52.631639: 
2024-05-08 16:04:52.633127: Epoch 31
2024-05-08 16:04:52.634250: Current learning rate: 0.00972
2024-05-08 16:09:01.104968: Validation loss did not improve from -0.67299. Patience: 5/50
2024-05-08 16:09:01.106199: train_loss -0.7571
2024-05-08 16:09:01.107328: val_loss -0.5267
2024-05-08 16:09:01.108300: Pseudo dice [0.6827]
2024-05-08 16:09:01.109314: Epoch time: 248.48 s
2024-05-08 16:09:02.490760: 
2024-05-08 16:09:02.492295: Epoch 32
2024-05-08 16:09:02.493376: Current learning rate: 0.00971
2024-05-08 16:13:11.637650: Validation loss did not improve from -0.67299. Patience: 6/50
2024-05-08 16:13:11.640068: train_loss -0.7688
2024-05-08 16:13:11.642164: val_loss -0.5835
2024-05-08 16:13:11.643332: Pseudo dice [0.7095]
2024-05-08 16:13:11.644487: Epoch time: 249.15 s
2024-05-08 16:13:13.234227: 
2024-05-08 16:13:13.235952: Epoch 33
2024-05-08 16:13:13.237309: Current learning rate: 0.0097
2024-05-08 16:17:19.961792: Validation loss did not improve from -0.67299. Patience: 7/50
2024-05-08 16:17:20.033692: train_loss -0.7793
2024-05-08 16:17:20.035434: val_loss -0.6135
2024-05-08 16:17:20.036733: Pseudo dice [0.7304]
2024-05-08 16:17:20.037838: Epoch time: 246.8 s
2024-05-08 16:17:22.161566: 
2024-05-08 16:17:22.162997: Epoch 34
2024-05-08 16:17:22.163931: Current learning rate: 0.00969
2024-05-08 16:21:30.717155: Validation loss did not improve from -0.67299. Patience: 8/50
2024-05-08 16:21:30.718740: train_loss -0.7827
2024-05-08 16:21:30.719853: val_loss -0.6225
2024-05-08 16:21:30.720970: Pseudo dice [0.7478]
2024-05-08 16:21:30.722115: Epoch time: 248.56 s
2024-05-08 16:21:31.126116: Yayy! New best EMA pseudo Dice: 0.7178
2024-05-08 16:21:32.915166: 
2024-05-08 16:21:32.916861: Epoch 35
2024-05-08 16:21:32.917968: Current learning rate: 0.00968
2024-05-08 16:25:34.705776: Validation loss did not improve from -0.67299. Patience: 9/50
2024-05-08 16:25:34.707282: train_loss -0.7822
2024-05-08 16:25:34.708425: val_loss -0.6495
2024-05-08 16:25:34.709354: Pseudo dice [0.749]
2024-05-08 16:25:34.710360: Epoch time: 241.79 s
2024-05-08 16:25:34.711437: Yayy! New best EMA pseudo Dice: 0.7209
2024-05-08 16:25:36.530609: 
2024-05-08 16:25:36.532229: Epoch 36
2024-05-08 16:25:36.533428: Current learning rate: 0.00968
2024-05-08 16:29:39.700486: Validation loss did not improve from -0.67299. Patience: 10/50
2024-05-08 16:29:39.701859: train_loss -0.7899
2024-05-08 16:29:39.702880: val_loss -0.6236
2024-05-08 16:29:39.703783: Pseudo dice [0.7298]
2024-05-08 16:29:39.704693: Epoch time: 243.17 s
2024-05-08 16:29:39.705597: Yayy! New best EMA pseudo Dice: 0.7218
2024-05-08 16:29:41.519934: 
2024-05-08 16:29:41.521756: Epoch 37
2024-05-08 16:29:41.522773: Current learning rate: 0.00967
2024-05-08 16:33:40.345551: Validation loss did not improve from -0.67299. Patience: 11/50
2024-05-08 16:33:40.347224: train_loss -0.7876
2024-05-08 16:33:40.348348: val_loss -0.6404
2024-05-08 16:33:40.349559: Pseudo dice [0.7516]
2024-05-08 16:33:40.350719: Epoch time: 238.83 s
2024-05-08 16:33:40.351757: Yayy! New best EMA pseudo Dice: 0.7248
2024-05-08 16:33:42.668262: 
2024-05-08 16:33:42.670767: Epoch 38
2024-05-08 16:33:42.672339: Current learning rate: 0.00966
2024-05-08 16:37:54.175282: Validation loss did not improve from -0.67299. Patience: 12/50
2024-05-08 16:37:54.177104: train_loss -0.7835
2024-05-08 16:37:54.178558: val_loss -0.6032
2024-05-08 16:37:54.179769: Pseudo dice [0.7364]
2024-05-08 16:37:54.181090: Epoch time: 251.51 s
2024-05-08 16:37:54.182167: Yayy! New best EMA pseudo Dice: 0.726
2024-05-08 16:37:55.997812: 
2024-05-08 16:37:55.999628: Epoch 39
2024-05-08 16:37:56.000708: Current learning rate: 0.00965
2024-05-08 16:42:03.556902: Validation loss did not improve from -0.67299. Patience: 13/50
2024-05-08 16:42:03.558205: train_loss -0.7877
2024-05-08 16:42:03.559324: val_loss -0.6082
2024-05-08 16:42:03.560830: Pseudo dice [0.7307]
2024-05-08 16:42:03.562034: Epoch time: 247.56 s
2024-05-08 16:42:03.961701: Yayy! New best EMA pseudo Dice: 0.7264
2024-05-08 16:42:05.818232: 
2024-05-08 16:42:05.819584: Epoch 40
2024-05-08 16:42:05.820654: Current learning rate: 0.00964
2024-05-08 16:46:09.590318: Validation loss did not improve from -0.67299. Patience: 14/50
2024-05-08 16:46:09.592248: train_loss -0.796
2024-05-08 16:46:09.593467: val_loss -0.629
2024-05-08 16:46:09.594439: Pseudo dice [0.7446]
2024-05-08 16:46:09.595389: Epoch time: 243.78 s
2024-05-08 16:46:09.596284: Yayy! New best EMA pseudo Dice: 0.7283
2024-05-08 16:46:11.907683: 
2024-05-08 16:46:11.909104: Epoch 41
2024-05-08 16:46:11.910172: Current learning rate: 0.00963
2024-05-08 16:50:19.115771: Validation loss did not improve from -0.67299. Patience: 15/50
2024-05-08 16:50:19.117133: train_loss -0.7986
2024-05-08 16:50:19.118274: val_loss -0.6215
2024-05-08 16:50:19.119331: Pseudo dice [0.746]
2024-05-08 16:50:19.120439: Epoch time: 247.21 s
2024-05-08 16:50:19.121641: Yayy! New best EMA pseudo Dice: 0.73
2024-05-08 16:50:20.881674: 
2024-05-08 16:50:20.883308: Epoch 42
2024-05-08 16:50:20.884478: Current learning rate: 0.00962
2024-05-08 16:54:34.879385: Validation loss did not improve from -0.67299. Patience: 16/50
2024-05-08 16:54:34.880893: train_loss -0.7873
2024-05-08 16:54:34.882306: val_loss -0.5514
2024-05-08 16:54:34.883550: Pseudo dice [0.6899]
2024-05-08 16:54:34.884795: Epoch time: 254.0 s
2024-05-08 16:54:36.252993: 
2024-05-08 16:54:36.254749: Epoch 43
2024-05-08 16:54:36.255955: Current learning rate: 0.00961
2024-05-08 16:58:48.319039: Validation loss did not improve from -0.67299. Patience: 17/50
2024-05-08 16:58:48.320722: train_loss -0.7958
2024-05-08 16:58:48.322219: val_loss -0.6532
2024-05-08 16:58:48.323593: Pseudo dice [0.7623]
2024-05-08 16:58:48.324953: Epoch time: 252.07 s
2024-05-08 16:58:49.693156: 
2024-05-08 16:58:49.694597: Epoch 44
2024-05-08 16:58:49.695694: Current learning rate: 0.0096
2024-05-08 17:03:01.945109: Validation loss did not improve from -0.67299. Patience: 18/50
2024-05-08 17:03:01.946724: train_loss -0.8006
2024-05-08 17:03:01.948188: val_loss -0.5539
2024-05-08 17:03:01.949455: Pseudo dice [0.6922]
2024-05-08 17:03:01.950637: Epoch time: 252.25 s
2024-05-08 17:03:03.711250: 
2024-05-08 17:03:03.712826: Epoch 45
2024-05-08 17:03:03.713970: Current learning rate: 0.00959
2024-05-08 17:07:05.715430: Validation loss did not improve from -0.67299. Patience: 19/50
2024-05-08 17:07:05.717266: train_loss -0.8073
2024-05-08 17:07:05.718712: val_loss -0.6576
2024-05-08 17:07:05.719758: Pseudo dice [0.7604]
2024-05-08 17:07:05.720861: Epoch time: 242.01 s
2024-05-08 17:07:07.124351: 
2024-05-08 17:07:07.125821: Epoch 46
2024-05-08 17:07:07.126923: Current learning rate: 0.00959
2024-05-08 17:11:14.646020: Validation loss did not improve from -0.67299. Patience: 20/50
2024-05-08 17:11:14.647431: train_loss -0.8078
2024-05-08 17:11:14.648567: val_loss -0.5813
2024-05-08 17:11:14.649683: Pseudo dice [0.7123]
2024-05-08 17:11:14.650690: Epoch time: 247.52 s
2024-05-08 17:11:15.992960: 
2024-05-08 17:11:15.994497: Epoch 47
2024-05-08 17:11:15.995675: Current learning rate: 0.00958
2024-05-08 17:15:22.535222: Validation loss did not improve from -0.67299. Patience: 21/50
2024-05-08 17:15:22.550172: train_loss -0.8017
2024-05-08 17:15:22.551831: val_loss -0.583
2024-05-08 17:15:22.552711: Pseudo dice [0.7175]
2024-05-08 17:15:22.553929: Epoch time: 246.56 s
2024-05-08 17:15:23.915077: 
2024-05-08 17:15:23.917020: Epoch 48
2024-05-08 17:15:23.918232: Current learning rate: 0.00957
2024-05-08 17:19:36.618070: Validation loss did not improve from -0.67299. Patience: 22/50
2024-05-08 17:19:36.619795: train_loss -0.8112
2024-05-08 17:19:36.621083: val_loss -0.6644
2024-05-08 17:19:36.622272: Pseudo dice [0.7691]
2024-05-08 17:19:36.623407: Epoch time: 252.71 s
2024-05-08 17:19:36.624677: Yayy! New best EMA pseudo Dice: 0.7309
2024-05-08 17:19:38.425854: 
2024-05-08 17:19:38.427493: Epoch 49
2024-05-08 17:19:38.428661: Current learning rate: 0.00956
2024-05-08 17:23:56.358225: Validation loss did not improve from -0.67299. Patience: 23/50
2024-05-08 17:23:56.375080: train_loss -0.8088
2024-05-08 17:23:56.376774: val_loss -0.6284
2024-05-08 17:23:56.377844: Pseudo dice [0.7371]
2024-05-08 17:23:56.378872: Epoch time: 257.94 s
2024-05-08 17:23:56.742116: Yayy! New best EMA pseudo Dice: 0.7315
2024-05-08 17:23:59.887338: 
2024-05-08 17:23:59.888849: Epoch 50
2024-05-08 17:23:59.889926: Current learning rate: 0.00955
2024-05-08 17:28:16.584723: Validation loss did not improve from -0.67299. Patience: 24/50
2024-05-08 17:28:16.586133: train_loss -0.8106
2024-05-08 17:28:16.587134: val_loss -0.5879
2024-05-08 17:28:16.588070: Pseudo dice [0.7287]
2024-05-08 17:28:16.589086: Epoch time: 256.7 s
2024-05-08 17:28:17.968704: 
2024-05-08 17:28:17.970269: Epoch 51
2024-05-08 17:28:17.971458: Current learning rate: 0.00954
2024-05-08 17:32:34.032628: Validation loss did not improve from -0.67299. Patience: 25/50
2024-05-08 17:32:34.034106: train_loss -0.8157
2024-05-08 17:32:34.035209: val_loss -0.5951
2024-05-08 17:32:34.036218: Pseudo dice [0.7207]
2024-05-08 17:32:34.037208: Epoch time: 256.07 s
2024-05-08 17:32:35.912565: 
2024-05-08 17:32:35.914053: Epoch 52
2024-05-08 17:32:35.915254: Current learning rate: 0.00953
2024-05-08 17:36:45.295287: Validation loss did not improve from -0.67299. Patience: 26/50
2024-05-08 17:36:45.296681: train_loss -0.8104
2024-05-08 17:36:45.297865: val_loss -0.6091
2024-05-08 17:36:45.299035: Pseudo dice [0.7348]
2024-05-08 17:36:45.300215: Epoch time: 249.39 s
2024-05-08 17:36:46.681888: 
2024-05-08 17:36:46.683942: Epoch 53
2024-05-08 17:36:46.685287: Current learning rate: 0.00952
2024-05-08 17:40:56.301147: Validation loss did not improve from -0.67299. Patience: 27/50
2024-05-08 17:40:56.302495: train_loss -0.8126
2024-05-08 17:40:56.303756: val_loss -0.6308
2024-05-08 17:40:56.304892: Pseudo dice [0.737]
2024-05-08 17:40:56.306063: Epoch time: 249.62 s
2024-05-08 17:40:57.734856: 
2024-05-08 17:40:57.736718: Epoch 54
2024-05-08 17:40:57.737808: Current learning rate: 0.00951
2024-05-08 17:45:10.139737: Validation loss did not improve from -0.67299. Patience: 28/50
2024-05-08 17:45:10.141226: train_loss -0.8118
2024-05-08 17:45:10.142630: val_loss -0.5888
2024-05-08 17:45:10.143923: Pseudo dice [0.7254]
2024-05-08 17:45:10.145245: Epoch time: 252.41 s
2024-05-08 17:45:11.895888: 
2024-05-08 17:45:11.897903: Epoch 55
2024-05-08 17:45:11.899257: Current learning rate: 0.0095
2024-05-08 17:49:28.978803: Validation loss did not improve from -0.67299. Patience: 29/50
2024-05-08 17:49:28.980090: train_loss -0.8162
2024-05-08 17:49:28.981243: val_loss -0.6075
2024-05-08 17:49:28.982400: Pseudo dice [0.7293]
2024-05-08 17:49:28.983481: Epoch time: 257.09 s
2024-05-08 17:49:30.361423: 
2024-05-08 17:49:30.363406: Epoch 56
2024-05-08 17:49:30.364564: Current learning rate: 0.00949
2024-05-08 17:53:45.414549: Validation loss did not improve from -0.67299. Patience: 30/50
2024-05-08 17:53:45.415771: train_loss -0.8043
2024-05-08 17:53:45.416819: val_loss -0.5902
2024-05-08 17:53:45.417733: Pseudo dice [0.7197]
2024-05-08 17:53:45.418655: Epoch time: 255.06 s
2024-05-08 17:53:46.792297: 
2024-05-08 17:53:46.793733: Epoch 57
2024-05-08 17:53:46.794676: Current learning rate: 0.00949
2024-05-08 17:58:06.257713: Validation loss did not improve from -0.67299. Patience: 31/50
2024-05-08 17:58:06.259168: train_loss -0.8141
2024-05-08 17:58:06.260403: val_loss -0.6252
2024-05-08 17:58:06.261689: Pseudo dice [0.7418]
2024-05-08 17:58:06.262952: Epoch time: 259.47 s
2024-05-08 17:58:07.638030: 
2024-05-08 17:58:07.640155: Epoch 58
2024-05-08 17:58:07.641574: Current learning rate: 0.00948
2024-05-08 18:02:21.480524: Validation loss did not improve from -0.67299. Patience: 32/50
2024-05-08 18:02:21.482008: train_loss -0.813
2024-05-08 18:02:21.483154: val_loss -0.5979
2024-05-08 18:02:21.484137: Pseudo dice [0.7397]
2024-05-08 18:02:21.485245: Epoch time: 253.85 s
2024-05-08 18:02:21.486236: Yayy! New best EMA pseudo Dice: 0.7316
2024-05-08 18:02:23.342496: 
2024-05-08 18:02:23.344210: Epoch 59
2024-05-08 18:02:23.345344: Current learning rate: 0.00947
2024-05-08 18:06:29.967602: Validation loss did not improve from -0.67299. Patience: 33/50
2024-05-08 18:06:29.968941: train_loss -0.8168
2024-05-08 18:06:29.970159: val_loss -0.6032
2024-05-08 18:06:29.971125: Pseudo dice [0.7317]
2024-05-08 18:06:29.972092: Epoch time: 246.63 s
2024-05-08 18:06:30.351597: Yayy! New best EMA pseudo Dice: 0.7316
2024-05-08 18:06:32.203521: 
2024-05-08 18:06:32.205051: Epoch 60
2024-05-08 18:06:32.205997: Current learning rate: 0.00946
2024-05-08 18:10:41.220522: Validation loss did not improve from -0.67299. Patience: 34/50
2024-05-08 18:10:41.221758: train_loss -0.8123
2024-05-08 18:10:41.222889: val_loss -0.6269
2024-05-08 18:10:41.224211: Pseudo dice [0.7402]
2024-05-08 18:10:41.225246: Epoch time: 249.02 s
2024-05-08 18:10:41.226167: Yayy! New best EMA pseudo Dice: 0.7325
2024-05-08 18:10:43.015932: 
2024-05-08 18:10:43.017445: Epoch 61
2024-05-08 18:10:43.018518: Current learning rate: 0.00945
2024-05-08 18:14:51.103084: Validation loss did not improve from -0.67299. Patience: 35/50
2024-05-08 18:14:51.104671: train_loss -0.8228
2024-05-08 18:14:51.105981: val_loss -0.6236
2024-05-08 18:14:51.107139: Pseudo dice [0.7489]
2024-05-08 18:14:51.108163: Epoch time: 248.09 s
2024-05-08 18:14:51.109056: Yayy! New best EMA pseudo Dice: 0.7341
2024-05-08 18:14:52.913323: 
2024-05-08 18:14:52.914920: Epoch 62
2024-05-08 18:14:52.915964: Current learning rate: 0.00944
2024-05-08 18:19:02.881131: Validation loss did not improve from -0.67299. Patience: 36/50
2024-05-08 18:19:02.882520: train_loss -0.8185
2024-05-08 18:19:02.883846: val_loss -0.5611
2024-05-08 18:19:02.884990: Pseudo dice [0.6952]
2024-05-08 18:19:02.886119: Epoch time: 249.97 s
2024-05-08 18:19:04.277144: 
2024-05-08 18:19:04.279141: Epoch 63
2024-05-08 18:19:04.280306: Current learning rate: 0.00943
2024-05-08 18:23:20.527355: Validation loss did not improve from -0.67299. Patience: 37/50
2024-05-08 18:23:20.540625: train_loss -0.7734
2024-05-08 18:23:20.542939: val_loss -0.5766
2024-05-08 18:23:20.544377: Pseudo dice [0.7038]
2024-05-08 18:23:20.545824: Epoch time: 256.26 s
2024-05-08 18:23:24.581617: 
2024-05-08 18:23:24.583464: Epoch 64
2024-05-08 18:23:24.584596: Current learning rate: 0.00942
2024-05-08 18:27:39.067907: Validation loss did not improve from -0.67299. Patience: 38/50
2024-05-08 18:27:39.079225: train_loss -0.7745
2024-05-08 18:27:39.080767: val_loss -0.6039
2024-05-08 18:27:39.081908: Pseudo dice [0.7233]
2024-05-08 18:27:39.082953: Epoch time: 254.49 s
2024-05-08 18:27:41.508377: 
2024-05-08 18:27:41.510228: Epoch 65
2024-05-08 18:27:41.511284: Current learning rate: 0.00941
2024-05-08 18:31:56.186878: Validation loss did not improve from -0.67299. Patience: 39/50
2024-05-08 18:31:56.188615: train_loss -0.787
2024-05-08 18:31:56.189872: val_loss -0.5587
2024-05-08 18:31:56.190950: Pseudo dice [0.6955]
2024-05-08 18:31:56.192021: Epoch time: 254.68 s
2024-05-08 18:31:57.637900: 
2024-05-08 18:31:57.639481: Epoch 66
2024-05-08 18:31:57.640620: Current learning rate: 0.0094
2024-05-08 18:36:12.969830: Validation loss did not improve from -0.67299. Patience: 40/50
2024-05-08 18:36:12.971245: train_loss -0.7981
2024-05-08 18:36:12.972362: val_loss -0.5994
2024-05-08 18:36:12.973428: Pseudo dice [0.7179]
2024-05-08 18:36:12.974463: Epoch time: 255.33 s
2024-05-08 18:36:14.420449: 
2024-05-08 18:36:14.421818: Epoch 67
2024-05-08 18:36:14.422867: Current learning rate: 0.00939
2024-05-08 18:40:30.149166: Validation loss did not improve from -0.67299. Patience: 41/50
2024-05-08 18:40:30.151235: train_loss -0.8056
2024-05-08 18:40:30.152458: val_loss -0.5935
2024-05-08 18:40:30.153469: Pseudo dice [0.7189]
2024-05-08 18:40:30.154493: Epoch time: 255.73 s
2024-05-08 18:40:31.614472: 
2024-05-08 18:40:31.616085: Epoch 68
2024-05-08 18:40:31.617165: Current learning rate: 0.00939
2024-05-08 18:44:48.043212: Validation loss did not improve from -0.67299. Patience: 42/50
2024-05-08 18:44:48.044627: train_loss -0.8138
2024-05-08 18:44:48.045920: val_loss -0.5763
2024-05-08 18:44:48.047154: Pseudo dice [0.7035]
2024-05-08 18:44:48.048410: Epoch time: 256.43 s
2024-05-08 18:44:49.479098: 
2024-05-08 18:44:49.480639: Epoch 69
2024-05-08 18:44:49.481785: Current learning rate: 0.00938
2024-05-08 18:48:59.720674: Validation loss did not improve from -0.67299. Patience: 43/50
2024-05-08 18:48:59.722192: train_loss -0.8238
2024-05-08 18:48:59.723497: val_loss -0.6439
2024-05-08 18:48:59.724723: Pseudo dice [0.7549]
2024-05-08 18:48:59.725834: Epoch time: 250.24 s
2024-05-08 18:49:01.607262: 
2024-05-08 18:49:01.608789: Epoch 70
2024-05-08 18:49:01.610097: Current learning rate: 0.00937
2024-05-08 18:53:10.633681: Validation loss did not improve from -0.67299. Patience: 44/50
2024-05-08 18:53:10.635404: train_loss -0.8181
2024-05-08 18:53:10.637096: val_loss -0.613
2024-05-08 18:53:10.638604: Pseudo dice [0.7382]
2024-05-08 18:53:10.640125: Epoch time: 249.03 s
2024-05-08 18:53:12.075077: 
2024-05-08 18:53:12.077090: Epoch 71
2024-05-08 18:53:12.078443: Current learning rate: 0.00936
2024-05-08 18:57:20.148218: Validation loss did not improve from -0.67299. Patience: 45/50
2024-05-08 18:57:20.150061: train_loss -0.821
2024-05-08 18:57:20.151439: val_loss -0.6195
2024-05-08 18:57:20.152582: Pseudo dice [0.7408]
2024-05-08 18:57:20.153839: Epoch time: 248.08 s
2024-05-08 18:57:21.644739: 
2024-05-08 18:57:21.646449: Epoch 72
2024-05-08 18:57:21.647518: Current learning rate: 0.00935
2024-05-08 19:01:29.851938: Validation loss did not improve from -0.67299. Patience: 46/50
2024-05-08 19:01:29.853455: train_loss -0.8217
2024-05-08 19:01:29.854571: val_loss -0.6207
2024-05-08 19:01:29.855539: Pseudo dice [0.7409]
2024-05-08 19:01:29.856701: Epoch time: 248.21 s
2024-05-08 19:01:31.286744: 
2024-05-08 19:01:31.288453: Epoch 73
2024-05-08 19:01:31.289837: Current learning rate: 0.00934
2024-05-08 19:05:46.750165: Validation loss did not improve from -0.67299. Patience: 47/50
2024-05-08 19:05:46.751585: train_loss -0.8246
2024-05-08 19:05:46.753229: val_loss -0.5655
2024-05-08 19:05:46.754379: Pseudo dice [0.6916]
2024-05-08 19:05:46.755590: Epoch time: 255.47 s
2024-05-08 19:05:48.195071: 
2024-05-08 19:05:48.196517: Epoch 74
2024-05-08 19:05:48.197602: Current learning rate: 0.00933
2024-05-08 19:10:05.679995: Validation loss did not improve from -0.67299. Patience: 48/50
2024-05-08 19:10:05.681584: train_loss -0.8184
2024-05-08 19:10:05.682794: val_loss -0.6178
2024-05-08 19:10:05.683851: Pseudo dice [0.7314]
2024-05-08 19:10:05.685031: Epoch time: 257.49 s
2024-05-08 19:10:08.140333: 
2024-05-08 19:10:08.142306: Epoch 75
2024-05-08 19:10:08.143651: Current learning rate: 0.00932
2024-05-08 19:14:27.636575: Validation loss did not improve from -0.67299. Patience: 49/50
2024-05-08 19:14:27.637801: train_loss -0.8221
2024-05-08 19:14:27.638875: val_loss -0.5949
2024-05-08 19:14:27.639864: Pseudo dice [0.738]
2024-05-08 19:14:27.640800: Epoch time: 259.5 s
2024-05-08 19:14:29.074698: 
2024-05-08 19:14:29.076384: Epoch 76
2024-05-08 19:14:29.077510: Current learning rate: 0.00931
2024-05-08 19:18:50.915705: Validation loss did not improve from -0.67299. Patience: 50/50
2024-05-08 19:18:50.917049: train_loss -0.8222
2024-05-08 19:18:50.918265: val_loss -0.5786
2024-05-08 19:18:50.919393: Pseudo dice [0.7185]
2024-05-08 19:18:50.920473: Epoch time: 261.84 s
2024-05-08 19:18:52.444773: Patience reached. Stopping training.
2024-05-08 19:18:52.933212: Training done.
2024-05-08 19:18:54.041496: Using splits from existing split file: /home/gridsan/nchutisilp/datasets/nnUNet_Datasets/nnUNet_preprocessed/Dataset302_Calcium_OCTv2/splits_final.json
2024-05-08 19:18:54.061069: The split file contains 3 splits.
2024-05-08 19:18:54.062846: Desired fold for training: 2
2024-05-08 19:18:54.063950: This split has 4 training and 2 validation cases.
2024-05-08 19:18:54.065135: predicting 401-004
2024-05-08 19:18:54.159369: 401-004, shape torch.Size([1, 375, 498, 498]), rank 0
2024-05-08 19:21:26.904430: predicting 701-013
2024-05-08 19:21:26.920487: 701-013, shape torch.Size([1, 375, 498, 498]), rank 0
2024-05-08 19:25:58.411801: Validation complete
2024-05-08 19:25:58.413688: Mean Validation Dice:  0.7211531258083574
wandb: 
wandb: Run history:
wandb:            ema_fg_dice ▁▁▂▂▂▃▃▄▄▄▄▅▅▆▆▇▆▆▇▇▇█▇▇██████████▇▇▇█▇▇
wandb:   epoch_end_timestamps ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███
wandb: epoch_start_timestamps ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███
wandb:                    lrs ████▇▇▇▇▇▆▆▆▆▆▆▅▅▅▅▅▅▄▄▄▄▄▃▃▃▃▃▂▂▂▂▂▂▁▁▁
wandb:           mean_fg_dice ▃▁▄▃▅▅▅▅▆▄▃▆▇▇▅█▄▆▇▇▇▇▄▅▅█▆▆▆▆▇▇▅▆▆▅▇▇▆▆
wandb:           train_losses █▅▄▄▄▃▃▃▂▃▃▂▂▂▂▂▂▂▂▂▂▁▂▁▁▁▁▁▁▁▁▁▁▂▁▁▁▁▁▁
wandb:             val_losses ▆█▄▅▄▄▄▄▃▅▅▃▂▂▄▁▅▃▁▂▃▂▅▄▄▁▃▃▃▃▃▂▄▃▃▄▃▂▂▄
wandb: 
wandb: Run summary:
wandb:            ema_fg_dice 0.72598
wandb:   epoch_end_timestamps 1715210330.91689
wandb: epoch_start_timestamps 1715210069.07312
wandb:                    lrs 0.00931
wandb:           mean_fg_dice 0.71854
wandb:           train_losses -0.82216
wandb:             val_losses -0.57858
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/gridsan/nchutisilp/datasets/nnUNet_Datasets/nnUNet_results/Dataset302_Calcium_OCTv2/nnUNetTrainer__nnUNetPlans__3d_32x512x512_b2/fold_2/wandb/offline-run-20240508_135913-tnsrkrf0
wandb: Find logs at: /home/gridsan/nchutisilp/datasets/nnUNet_Datasets/nnUNet_results/Dataset302_Calcium_OCTv2/nnUNetTrainer__nnUNetPlans__3d_32x512x512_b2/fold_2/wandb/offline-run-20240508_135913-tnsrkrf0/logs
WARNING:urllib3.connectionpool:Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x7f9aac83f160>: Failed to establish a new connection: [Errno 101] Network is unreachable')': /api/4504800232407040/envelope/
WARNING:urllib3.connectionpool:Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x7f9aac946fa0>: Failed to establish a new connection: [Errno 101] Network is unreachable')': /api/4504800232407040/envelope/
WARNING:urllib3.connectionpool:Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x7f9a021ef6d0>: Failed to establish a new connection: [Errno 101] Network is unreachable')': /api/4504800232407040/envelope/
WARNING:urllib3.connectionpool:Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x7f9aaa089610>: Failed to establish a new connection: [Errno 101] Network is unreachable')': /api/4504800232407040/envelope/
WARNING:urllib3.connectionpool:Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x7f9adeb9c4c0>: Failed to establish a new connection: [Errno 101] Network is unreachable')': /api/4504800232407040/envelope/
WARNING:urllib3.connectionpool:Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x7f9adeb9c1f0>: Failed to establish a new connection: [Errno 101] Network is unreachable')': /api/4504800232407040/envelope/
FOLD 2 CONFIG 3d_32x512x512_b2 TRAINER nnUNetTrainer

#######################################################################
Please cite the following paper when using nnU-Net:
Isensee, F., Jaeger, P. F., Kohl, S. A., Petersen, J., & Maier-Hein, K. H. (2021). nnU-Net: a self-configuring method for deep learning-based biomedical image segmentation. Nature methods, 18(2), 203-211.
#######################################################################

There are 2 cases in the source folder
I am process 0 out of 1 (max process ID is 0, we start counting with 0!)
There are 2 cases that I would like to predict

Predicting 101-045:
perform_everything_on_device: True
  0%|          | 0/23 [00:00<?, ?it/s]  4%|▍         | 1/23 [01:40<36:57, 100.78s/it]  9%|▊         | 2/23 [01:42<14:49, 42.37s/it]  13%|█▎        | 3/23 [01:43<07:53, 23.70s/it] 17%|█▋        | 4/23 [01:45<04:43, 14.93s/it] 22%|██▏       | 5/23 [01:46<03:01, 10.08s/it] 26%|██▌       | 6/23 [01:48<02:01,  7.15s/it] 30%|███       | 7/23 [01:49<01:24,  5.30s/it] 35%|███▍      | 8/23 [01:51<01:01,  4.08s/it] 39%|███▉      | 9/23 [01:52<00:45,  3.27s/it] 43%|████▎     | 10/23 [01:54<00:35,  2.72s/it] 48%|████▊     | 11/23 [01:55<00:28,  2.34s/it] 52%|█████▏    | 12/23 [01:57<00:22,  2.08s/it] 57%|█████▋    | 13/23 [01:58<00:18,  1.90s/it] 61%|██████    | 14/23 [02:00<00:15,  1.77s/it] 65%|██████▌   | 15/23 [02:01<00:13,  1.68s/it] 70%|██████▉   | 16/23 [02:02<00:11,  1.62s/it] 74%|███████▍  | 17/23 [02:04<00:09,  1.58s/it] 78%|███████▊  | 18/23 [02:05<00:07,  1.55s/it] 83%|████████▎ | 19/23 [02:07<00:06,  1.53s/it] 87%|████████▋ | 20/23 [02:08<00:04,  1.51s/it] 91%|█████████▏| 21/23 [02:10<00:03,  1.50s/it] 96%|█████████▌| 22/23 [02:11<00:01,  1.50s/it]100%|██████████| 23/23 [02:13<00:00,  1.49s/it]100%|██████████| 23/23 [02:13<00:00,  5.80s/it]
sending off prediction to background worker for resampling and export
done with 101-045

Predicting 706-005:
perform_everything_on_device: True
  0%|          | 0/23 [00:00<?, ?it/s]  4%|▍         | 1/23 [00:00<00:17,  1.26it/s]  9%|▊         | 2/23 [00:02<00:25,  1.20s/it] 13%|█▎        | 3/23 [00:03<00:26,  1.33s/it] 17%|█▋        | 4/23 [00:05<00:26,  1.39s/it] 22%|██▏       | 5/23 [00:06<00:25,  1.42s/it] 26%|██▌       | 6/23 [00:08<00:24,  1.44s/it] 30%|███       | 7/23 [00:09<00:23,  1.46s/it] 35%|███▍      | 8/23 [00:11<00:21,  1.46s/it] 39%|███▉      | 9/23 [00:12<00:20,  1.47s/it] 43%|████▎     | 10/23 [00:14<00:19,  1.47s/it] 48%|████▊     | 11/23 [00:15<00:17,  1.48s/it] 52%|█████▏    | 12/23 [00:17<00:16,  1.48s/it] 57%|█████▋    | 13/23 [00:18<00:14,  1.48s/it] 61%|██████    | 14/23 [00:20<00:13,  1.48s/it] 65%|██████▌   | 15/23 [00:21<00:11,  1.48s/it] 70%|██████▉   | 16/23 [00:23<00:10,  1.48s/it] 74%|███████▍  | 17/23 [00:24<00:08,  1.48s/it] 78%|███████▊  | 18/23 [00:25<00:07,  1.48s/it] 83%|████████▎ | 19/23 [00:27<00:05,  1.48s/it] 87%|████████▋ | 20/23 [00:28<00:04,  1.48s/it] 91%|█████████▏| 21/23 [00:30<00:02,  1.48s/it] 96%|█████████▌| 22/23 [00:31<00:01,  1.48s/it]100%|██████████| 23/23 [00:33<00:00,  1.48s/it]100%|██████████| 23/23 [00:33<00:00,  1.45s/it]
sending off prediction to background worker for resampling and export
done with 706-005
Completed FOLD 2 CONFIG 3d_32x512x512_b2 TRAINER nnUNetTrainer
