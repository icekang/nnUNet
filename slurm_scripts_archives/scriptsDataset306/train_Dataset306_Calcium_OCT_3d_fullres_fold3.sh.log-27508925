/home/gridsan/nchutisilp/.conda/envs/nnunet/bin/python

############################
INFO: You are using the old nnU-Net default plans. We have updated our recommendations. Please consider using those instead! Read more here: https://github.com/MIC-DKFZ/nnUNet/blob/master/documentation/resenc_presets.md
############################

Using device: cuda:0

#######################################################################
Please cite the following paper when using nnU-Net:
Isensee, F., Jaeger, P. F., Kohl, S. A., Petersen, J., & Maier-Hein, K. H. (2021). nnU-Net: a self-configuring method for deep learning-based biomedical image segmentation. Nature methods, 18(2), 203-211.
#######################################################################

2024-11-27 18:23:04.818284: Using torch.compile...
/home/gridsan/nchutisilp/.local/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn("The verbose parameter is deprecated. Please use get_last_lr() "
2024-11-27 18:23:22.847110: do_dummy_2d_data_aug: False
2024-11-27 18:23:22.882396: Using splits from existing split file: /home/gridsan/nchutisilp/datasets/nnUNet_Datasets/nnUNet_preprocessed/Dataset306_Sohee_Ajay_Calcium_OCT/splits_final.json
2024-11-27 18:23:22.906229: The split file contains 5 splits.
2024-11-27 18:23:22.908412: Desired fold for training: 3
2024-11-27 18:23:22.909562: This split has 11 training and 2 validation cases.
using pin_memory on device 0
using pin_memory on device 0

This is the configuration used by this training:
Configuration name: 3d_fullres
 {'data_identifier': 'nnUNetPlans_3d_fullres', 'preprocessor_name': 'DefaultPreprocessor', 'batch_size': 2, 'patch_size': [112, 160, 128], 'median_image_size_in_voxels': [375.0, 498.0, 498.0], 'spacing': [1.0, 1.0, 1.0], 'normalization_schemes': ['ZScoreNormalization'], 'use_mask_for_norm': [False], 'resampling_fn_data': 'resample_data_or_seg_to_shape', 'resampling_fn_seg': 'resample_data_or_seg_to_shape', 'resampling_fn_data_kwargs': {'is_seg': False, 'order': 3, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_seg_kwargs': {'is_seg': True, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_probabilities': 'resample_data_or_seg_to_shape', 'resampling_fn_probabilities_kwargs': {'is_seg': False, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'architecture': {'network_class_name': 'dynamic_network_architectures.architectures.unet.PlainConvUNet', 'arch_kwargs': {'n_stages': 6, 'features_per_stage': [32, 64, 128, 256, 320, 320], 'conv_op': 'torch.nn.modules.conv.Conv3d', 'kernel_sizes': [[3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3]], 'strides': [[1, 1, 1], [2, 2, 2], [2, 2, 2], [2, 2, 2], [2, 2, 2], [1, 2, 2]], 'n_conv_per_stage': [2, 2, 2, 2, 2, 2], 'n_conv_per_stage_decoder': [2, 2, 2, 2, 2], 'conv_bias': True, 'norm_op': 'torch.nn.modules.instancenorm.InstanceNorm3d', 'norm_op_kwargs': {'eps': 1e-05, 'affine': True}, 'dropout_op': None, 'dropout_op_kwargs': None, 'nonlin': 'torch.nn.LeakyReLU', 'nonlin_kwargs': {'inplace': True}, 'deep_supervision': True}, '_kw_requires_import': ['conv_op', 'norm_op', 'dropout_op', 'nonlin']}, 'batch_dice': True} 

These are the global plan.json settings:
 {'dataset_name': 'Dataset306_Sohee_Ajay_Calcium_OCT', 'plans_name': 'nnUNetPlans', 'original_median_spacing_after_transp': [1.0, 1.0, 1.0], 'original_median_shape_after_transp': [375, 498, 498], 'image_reader_writer': 'SimpleITKIO', 'transpose_forward': [0, 1, 2], 'transpose_backward': [0, 1, 2], 'experiment_planner_used': 'ExperimentPlanner', 'label_manager': 'LabelManager', 'foreground_intensity_properties_per_channel': {'0': {'max': 0.49803921580314636, 'mean': 0.14296366274356842, 'median': 0.10980392247438431, 'min': 0.0, 'percentile_00_5': 0.01558823511004448, 'percentile_99_5': 0.49803921580314636, 'std': 0.11562220752239227}}} 

2024-11-27 18:23:32.548970: unpacking dataset...
2024-11-27 18:23:37.994221: unpacking done...
2024-11-27 18:23:38.072758: Unable to plot network architecture: nnUNet_compile is enabled!
2024-11-27 18:23:38.232440: 
2024-11-27 18:23:38.233822: Epoch 800
2024-11-27 18:23:38.234838: Current learning rate: 0.00235
2024-11-27 18:26:46.420879: train_loss -0.8007
2024-11-27 18:26:46.422977: val_loss -0.5369
2024-11-27 18:26:46.424128: Pseudo dice [0.7368]
2024-11-27 18:26:46.424961: Epoch time: 188.19 s
2024-11-27 18:26:48.340947: 
2024-11-27 18:26:48.342660: Epoch 801
2024-11-27 18:26:48.343723: Current learning rate: 0.00234
2024-11-27 18:28:26.508513: train_loss -0.8007
2024-11-27 18:28:26.509426: val_loss -0.502
2024-11-27 18:28:26.510190: Pseudo dice [0.7312]
2024-11-27 18:28:26.510908: Epoch time: 98.17 s
2024-11-27 18:28:28.203114: 
2024-11-27 18:28:28.204231: Epoch 802
2024-11-27 18:28:28.204943: Current learning rate: 0.00233
2024-11-27 18:31:04.183457: train_loss -0.8028
2024-11-27 18:31:04.184410: val_loss -0.5569
2024-11-27 18:31:04.185423: Pseudo dice [0.7552]
2024-11-27 18:31:04.186817: Epoch time: 155.98 s
2024-11-27 18:31:05.887514: 
2024-11-27 18:31:05.889043: Epoch 803
2024-11-27 18:31:05.890491: Current learning rate: 0.00232
2024-11-27 18:34:24.719709: train_loss -0.8035
2024-11-27 18:34:24.720793: val_loss -0.5344
2024-11-27 18:34:24.721606: Pseudo dice [0.7413]
2024-11-27 18:34:24.722360: Epoch time: 198.83 s
2024-11-27 18:34:26.410227: 
2024-11-27 18:34:26.411395: Epoch 804
2024-11-27 18:34:26.412301: Current learning rate: 0.00231
2024-11-27 18:37:57.697262: train_loss -0.8071
2024-11-27 18:37:57.698259: val_loss -0.4917
2024-11-27 18:37:57.699153: Pseudo dice [0.7307]
2024-11-27 18:37:57.699838: Epoch time: 211.29 s
2024-11-27 18:38:00.021607: 
2024-11-27 18:38:00.023273: Epoch 805
2024-11-27 18:38:00.024109: Current learning rate: 0.0023
2024-11-27 18:42:02.385379: train_loss -0.8056
2024-11-27 18:42:02.386495: val_loss -0.5375
2024-11-27 18:42:02.387537: Pseudo dice [0.7457]
2024-11-27 18:42:02.388645: Epoch time: 242.37 s
2024-11-27 18:42:04.069702: 
2024-11-27 18:42:04.071249: Epoch 806
2024-11-27 18:42:04.072107: Current learning rate: 0.00229
2024-11-27 18:46:13.781887: train_loss -0.8033
2024-11-27 18:46:13.782793: val_loss -0.5091
2024-11-27 18:46:13.783637: Pseudo dice [0.7324]
2024-11-27 18:46:13.784347: Epoch time: 249.71 s
2024-11-27 18:46:15.547700: 
2024-11-27 18:46:15.549181: Epoch 807
2024-11-27 18:46:15.550152: Current learning rate: 0.00228
2024-11-27 18:50:17.298565: train_loss -0.8035
2024-11-27 18:50:17.299673: val_loss -0.5613
2024-11-27 18:50:17.300498: Pseudo dice [0.7593]
2024-11-27 18:50:17.301194: Epoch time: 241.75 s
2024-11-27 18:50:19.123522: 
2024-11-27 18:50:19.124982: Epoch 808
2024-11-27 18:50:19.125818: Current learning rate: 0.00226
2024-11-27 18:54:22.409509: train_loss -0.8055
2024-11-27 18:54:22.410562: val_loss -0.4915
2024-11-27 18:54:22.411306: Pseudo dice [0.7314]
2024-11-27 18:54:22.411998: Epoch time: 243.29 s
2024-11-27 18:54:24.092710: 
2024-11-27 18:54:24.094283: Epoch 809
2024-11-27 18:54:24.095156: Current learning rate: 0.00225
2024-11-27 18:59:01.445027: train_loss -0.8031
2024-11-27 18:59:01.446088: val_loss -0.5647
2024-11-27 18:59:01.446916: Pseudo dice [0.7503]
2024-11-27 18:59:01.447700: Epoch time: 277.35 s
2024-11-27 18:59:03.135870: 
2024-11-27 18:59:03.137705: Epoch 810
2024-11-27 18:59:03.138921: Current learning rate: 0.00224
2024-11-27 19:03:25.946370: train_loss -0.7995
2024-11-27 19:03:25.947228: val_loss -0.4983
2024-11-27 19:03:25.948193: Pseudo dice [0.7287]
2024-11-27 19:03:25.948938: Epoch time: 262.81 s
2024-11-27 19:03:27.750331: 
2024-11-27 19:03:27.751466: Epoch 811
2024-11-27 19:03:27.752420: Current learning rate: 0.00223
2024-11-27 19:07:53.376181: train_loss -0.7965
2024-11-27 19:07:53.377320: val_loss -0.5817
2024-11-27 19:07:53.378322: Pseudo dice [0.7521]
2024-11-27 19:07:53.379124: Epoch time: 265.63 s
2024-11-27 19:07:55.051178: 
2024-11-27 19:07:55.052729: Epoch 812
2024-11-27 19:07:55.053587: Current learning rate: 0.00222
2024-11-27 19:12:29.062221: train_loss -0.7962
2024-11-27 19:12:29.063345: val_loss -0.5047
2024-11-27 19:12:29.064370: Pseudo dice [0.7126]
2024-11-27 19:12:29.065313: Epoch time: 274.01 s
2024-11-27 19:12:30.749035: 
2024-11-27 19:12:30.750770: Epoch 813
2024-11-27 19:12:30.751854: Current learning rate: 0.00221
2024-11-27 19:17:05.707769: train_loss -0.8001
2024-11-27 19:17:05.708700: val_loss -0.4756
2024-11-27 19:17:05.709531: Pseudo dice [0.7101]
2024-11-27 19:17:05.710373: Epoch time: 274.96 s
2024-11-27 19:17:07.498282: 
2024-11-27 19:17:07.499665: Epoch 814
2024-11-27 19:17:07.500727: Current learning rate: 0.0022
2024-11-27 19:21:33.933441: train_loss -0.7944
2024-11-27 19:21:33.934467: val_loss -0.4183
2024-11-27 19:21:33.935291: Pseudo dice [0.6707]
2024-11-27 19:21:33.936115: Epoch time: 266.44 s
2024-11-27 19:21:35.748738: 
2024-11-27 19:21:35.750301: Epoch 815
2024-11-27 19:21:35.751270: Current learning rate: 0.00219
2024-11-27 19:26:23.138227: train_loss -0.7953
2024-11-27 19:26:23.139019: val_loss -0.4154
2024-11-27 19:26:23.140006: Pseudo dice [0.6711]
2024-11-27 19:26:23.140845: Epoch time: 287.39 s
2024-11-27 19:26:25.248787: 
2024-11-27 19:26:25.249876: Epoch 816
2024-11-27 19:26:25.250784: Current learning rate: 0.00218
2024-11-27 19:31:03.121958: train_loss -0.802
2024-11-27 19:31:03.124794: val_loss -0.4196
2024-11-27 19:31:03.125853: Pseudo dice [0.6957]
2024-11-27 19:31:03.127337: Epoch time: 277.87 s
2024-11-27 19:31:04.932405: 
2024-11-27 19:31:04.933874: Epoch 817
2024-11-27 19:31:04.934815: Current learning rate: 0.00217
2024-11-27 19:35:59.679344: train_loss -0.7972
2024-11-27 19:35:59.681155: val_loss -0.5478
2024-11-27 19:35:59.682180: Pseudo dice [0.7334]
2024-11-27 19:35:59.683223: Epoch time: 294.75 s
2024-11-27 19:36:01.365766: 
2024-11-27 19:36:01.367280: Epoch 818
2024-11-27 19:36:01.368316: Current learning rate: 0.00216
2024-11-27 19:40:40.942698: train_loss -0.795
2024-11-27 19:40:40.943611: val_loss -0.4751
2024-11-27 19:40:40.944553: Pseudo dice [0.7071]
2024-11-27 19:40:40.945546: Epoch time: 279.58 s
2024-11-27 19:40:42.617875: 
2024-11-27 19:40:42.619384: Epoch 819
2024-11-27 19:40:42.620256: Current learning rate: 0.00215
2024-11-27 19:45:38.530651: train_loss -0.801
2024-11-27 19:45:38.531653: val_loss -0.5003
2024-11-27 19:45:38.532624: Pseudo dice [0.7123]
2024-11-27 19:45:38.533489: Epoch time: 295.91 s
2024-11-27 19:45:40.114673: 
2024-11-27 19:45:40.116220: Epoch 820
2024-11-27 19:45:40.117345: Current learning rate: 0.00214
2024-11-27 19:50:40.018703: train_loss -0.8024
2024-11-27 19:50:40.019710: val_loss -0.4265
2024-11-27 19:50:40.020435: Pseudo dice [0.675]
2024-11-27 19:50:40.021121: Epoch time: 299.91 s
2024-11-27 19:50:41.579279: 
2024-11-27 19:50:41.580848: Epoch 821
2024-11-27 19:50:41.581753: Current learning rate: 0.00213
2024-11-27 19:55:40.275773: train_loss -0.8042
2024-11-27 19:55:40.276681: val_loss -0.5243
2024-11-27 19:55:40.277556: Pseudo dice [0.7261]
2024-11-27 19:55:40.278277: Epoch time: 298.7 s
2024-11-27 19:55:41.928660: 
2024-11-27 19:55:41.929976: Epoch 822
2024-11-27 19:55:41.930785: Current learning rate: 0.00212
2024-11-27 20:00:19.314529: train_loss -0.8047
2024-11-27 20:00:19.315560: val_loss -0.4105
2024-11-27 20:00:19.316511: Pseudo dice [0.6713]
2024-11-27 20:00:19.317332: Epoch time: 277.39 s
2024-11-27 20:00:20.980432: 
2024-11-27 20:00:20.981882: Epoch 823
2024-11-27 20:00:20.982736: Current learning rate: 0.0021
2024-11-27 20:05:12.876380: train_loss -0.8033
2024-11-27 20:05:12.877418: val_loss -0.4707
2024-11-27 20:05:12.878219: Pseudo dice [0.7076]
2024-11-27 20:05:12.878909: Epoch time: 291.9 s
2024-11-27 20:05:14.430640: 
2024-11-27 20:05:14.431968: Epoch 824
2024-11-27 20:05:14.432671: Current learning rate: 0.00209
2024-11-27 20:10:16.323229: train_loss -0.809
2024-11-27 20:10:16.323900: val_loss -0.5483
2024-11-27 20:10:16.324654: Pseudo dice [0.7502]
2024-11-27 20:10:16.325426: Epoch time: 301.89 s
2024-11-27 20:10:17.865789: 
2024-11-27 20:10:17.867126: Epoch 825
2024-11-27 20:10:17.867832: Current learning rate: 0.00208
2024-11-27 20:15:25.943434: train_loss -0.8018
2024-11-27 20:15:25.944746: val_loss -0.4727
2024-11-27 20:15:25.945852: Pseudo dice [0.6892]
2024-11-27 20:15:25.946873: Epoch time: 308.08 s
2024-11-27 20:15:27.481349: 
2024-11-27 20:15:27.483147: Epoch 826
2024-11-27 20:15:27.484147: Current learning rate: 0.00207
2024-11-27 20:20:00.468983: train_loss -0.8053
2024-11-27 20:20:00.470016: val_loss -0.5824
2024-11-27 20:20:00.470957: Pseudo dice [0.7663]
2024-11-27 20:20:00.471694: Epoch time: 272.99 s
2024-11-27 20:20:03.097337: 
2024-11-27 20:20:03.098897: Epoch 827
2024-11-27 20:20:03.099839: Current learning rate: 0.00206
2024-11-27 20:24:53.050439: train_loss -0.8097
2024-11-27 20:24:53.051452: val_loss -0.4685
2024-11-27 20:24:53.052263: Pseudo dice [0.7102]
2024-11-27 20:24:53.052955: Epoch time: 289.95 s
2024-11-27 20:24:54.759610: 
2024-11-27 20:24:54.761007: Epoch 828
2024-11-27 20:24:54.761836: Current learning rate: 0.00205
2024-11-27 20:29:40.556735: train_loss -0.807
2024-11-27 20:29:40.557847: val_loss -0.471
2024-11-27 20:29:40.558617: Pseudo dice [0.7187]
2024-11-27 20:29:40.559300: Epoch time: 285.8 s
2024-11-27 20:29:42.215607: 
2024-11-27 20:29:42.216854: Epoch 829
2024-11-27 20:29:42.217641: Current learning rate: 0.00204
2024-11-27 20:34:31.849836: train_loss -0.8058
2024-11-27 20:34:31.851878: val_loss -0.5326
2024-11-27 20:34:31.852771: Pseudo dice [0.7336]
2024-11-27 20:34:31.853749: Epoch time: 289.64 s
2024-11-27 20:34:33.520324: 
2024-11-27 20:34:33.521646: Epoch 830
2024-11-27 20:34:33.522517: Current learning rate: 0.00203
2024-11-27 20:39:31.612574: train_loss -0.8113
2024-11-27 20:39:31.615018: val_loss -0.4848
2024-11-27 20:39:31.616140: Pseudo dice [0.7016]
2024-11-27 20:39:31.617971: Epoch time: 298.09 s
2024-11-27 20:39:33.226074: 
2024-11-27 20:39:33.227736: Epoch 831
2024-11-27 20:39:33.228657: Current learning rate: 0.00202
2024-11-27 20:44:19.672289: train_loss -0.8059
2024-11-27 20:44:19.673553: val_loss -0.4821
2024-11-27 20:44:19.674425: Pseudo dice [0.7121]
2024-11-27 20:44:19.675268: Epoch time: 286.45 s
2024-11-27 20:44:21.338700: 
2024-11-27 20:44:21.340056: Epoch 832
2024-11-27 20:44:21.340892: Current learning rate: 0.00201
2024-11-27 20:49:19.019745: train_loss -0.8037
2024-11-27 20:49:19.020708: val_loss -0.5813
2024-11-27 20:49:19.021510: Pseudo dice [0.7762]
2024-11-27 20:49:19.022369: Epoch time: 297.68 s
2024-11-27 20:49:20.770373: 
2024-11-27 20:49:20.771845: Epoch 833
2024-11-27 20:49:20.772783: Current learning rate: 0.002
2024-11-27 20:54:27.736113: train_loss -0.8111
2024-11-27 20:54:27.737190: val_loss -0.463
2024-11-27 20:54:27.738328: Pseudo dice [0.7078]
2024-11-27 20:54:27.739392: Epoch time: 306.97 s
2024-11-27 20:54:29.346195: 
2024-11-27 20:54:29.347909: Epoch 834
2024-11-27 20:54:29.349061: Current learning rate: 0.00199
2024-11-27 20:59:41.381106: train_loss -0.808
2024-11-27 20:59:41.382180: val_loss -0.4265
2024-11-27 20:59:41.382965: Pseudo dice [0.6863]
2024-11-27 20:59:41.383790: Epoch time: 312.04 s
2024-11-27 20:59:43.068069: 
2024-11-27 20:59:43.069404: Epoch 835
2024-11-27 20:59:43.070189: Current learning rate: 0.00198
2024-11-27 21:04:26.692246: train_loss -0.8096
2024-11-27 21:04:26.693325: val_loss -0.5577
2024-11-27 21:04:26.694106: Pseudo dice [0.7399]
2024-11-27 21:04:26.695005: Epoch time: 283.63 s
2024-11-27 21:04:28.363782: 
2024-11-27 21:04:28.365170: Epoch 836
2024-11-27 21:04:28.366065: Current learning rate: 0.00196
2024-11-27 21:10:03.891596: train_loss -0.8064
2024-11-27 21:10:03.892702: val_loss -0.5155
2024-11-27 21:10:03.893629: Pseudo dice [0.7253]
2024-11-27 21:10:03.894328: Epoch time: 335.53 s
2024-11-27 21:10:05.496816: 
2024-11-27 21:10:05.498369: Epoch 837
2024-11-27 21:10:05.499204: Current learning rate: 0.00195
2024-11-27 21:15:15.808854: train_loss -0.8039
2024-11-27 21:15:15.809802: val_loss -0.5326
2024-11-27 21:15:15.810564: Pseudo dice [0.7371]
2024-11-27 21:15:15.811283: Epoch time: 310.31 s
2024-11-27 21:15:18.105968: 
2024-11-27 21:15:18.107515: Epoch 838
2024-11-27 21:15:18.108599: Current learning rate: 0.00194
2024-11-27 21:20:27.693654: train_loss -0.812
2024-11-27 21:20:27.694438: val_loss -0.5783
2024-11-27 21:20:27.695168: Pseudo dice [0.7824]
2024-11-27 21:20:27.695861: Epoch time: 309.59 s
2024-11-27 21:20:29.323416: 
2024-11-27 21:20:29.324669: Epoch 839
2024-11-27 21:20:29.325419: Current learning rate: 0.00193
2024-11-27 21:25:36.433210: train_loss -0.8072
2024-11-27 21:25:36.434596: val_loss -0.4969
2024-11-27 21:25:36.435268: Pseudo dice [0.7238]
2024-11-27 21:25:36.435894: Epoch time: 307.11 s
2024-11-27 21:25:38.023937: 
2024-11-27 21:25:38.025381: Epoch 840
2024-11-27 21:25:38.026202: Current learning rate: 0.00192
2024-11-27 21:30:59.528296: train_loss -0.8071
2024-11-27 21:30:59.529357: val_loss -0.5374
2024-11-27 21:30:59.530389: Pseudo dice [0.7425]
2024-11-27 21:30:59.531339: Epoch time: 321.51 s
2024-11-27 21:31:01.102674: 
2024-11-27 21:31:01.104195: Epoch 841
2024-11-27 21:31:01.105168: Current learning rate: 0.00191
2024-11-27 21:36:12.831111: train_loss -0.8138
2024-11-27 21:36:12.832025: val_loss -0.5156
2024-11-27 21:36:12.832994: Pseudo dice [0.7262]
2024-11-27 21:36:12.833842: Epoch time: 311.73 s
2024-11-27 21:36:14.483233: 
2024-11-27 21:36:14.484556: Epoch 842
2024-11-27 21:36:14.485396: Current learning rate: 0.0019
2024-11-27 21:41:25.112402: train_loss -0.8166
2024-11-27 21:41:25.114452: val_loss -0.5524
2024-11-27 21:41:25.115498: Pseudo dice [0.7629]
2024-11-27 21:41:25.116905: Epoch time: 310.63 s
2024-11-27 21:41:26.791617: 
2024-11-27 21:41:26.793047: Epoch 843
2024-11-27 21:41:26.793927: Current learning rate: 0.00189
2024-11-27 21:46:28.670171: train_loss -0.8152
2024-11-27 21:46:28.673624: val_loss -0.5416
2024-11-27 21:46:28.674781: Pseudo dice [0.7506]
2024-11-27 21:46:28.675719: Epoch time: 301.88 s
2024-11-27 21:46:30.355883: 
2024-11-27 21:46:30.357057: Epoch 844
2024-11-27 21:46:30.357801: Current learning rate: 0.00188
2024-11-27 21:51:36.353978: train_loss -0.8131
2024-11-27 21:51:36.354882: val_loss -0.4438
2024-11-27 21:51:36.355855: Pseudo dice [0.7109]
2024-11-27 21:51:36.356599: Epoch time: 306.0 s
2024-11-27 21:51:38.027130: 
2024-11-27 21:51:38.028692: Epoch 845
2024-11-27 21:51:38.029581: Current learning rate: 0.00187
2024-11-27 21:56:46.891978: train_loss -0.8144
2024-11-27 21:56:46.893052: val_loss -0.551
2024-11-27 21:56:46.893833: Pseudo dice [0.7519]
2024-11-27 21:56:46.894681: Epoch time: 308.87 s
2024-11-27 21:56:48.453171: 
2024-11-27 21:56:48.455137: Epoch 846
2024-11-27 21:56:48.456126: Current learning rate: 0.00186
2024-11-27 22:02:09.315314: train_loss -0.8058
2024-11-27 22:02:09.316469: val_loss -0.5266
2024-11-27 22:02:09.317280: Pseudo dice [0.7426]
2024-11-27 22:02:09.317995: Epoch time: 320.86 s
2024-11-27 22:02:10.874457: 
2024-11-27 22:02:10.875837: Epoch 847
2024-11-27 22:02:10.876597: Current learning rate: 0.00185
2024-11-27 22:07:39.168735: train_loss -0.8062
2024-11-27 22:07:39.169765: val_loss -0.5115
2024-11-27 22:07:39.170569: Pseudo dice [0.7376]
2024-11-27 22:07:39.171351: Epoch time: 328.3 s
2024-11-27 22:07:40.815938: 
2024-11-27 22:07:40.817386: Epoch 848
2024-11-27 22:07:40.818161: Current learning rate: 0.00184
2024-11-27 22:12:51.470201: train_loss -0.8106
2024-11-27 22:12:51.472497: val_loss -0.3769
2024-11-27 22:12:51.473526: Pseudo dice [0.6463]
2024-11-27 22:12:51.474486: Epoch time: 310.66 s
2024-11-27 22:12:53.710284: 
2024-11-27 22:12:53.711787: Epoch 849
2024-11-27 22:12:53.712515: Current learning rate: 0.00182
2024-11-27 22:17:45.933041: train_loss -0.8126
2024-11-27 22:17:45.934036: val_loss -0.5348
2024-11-27 22:17:45.934894: Pseudo dice [0.7334]
2024-11-27 22:17:45.935706: Epoch time: 292.22 s
2024-11-27 22:17:48.344921: 
2024-11-27 22:17:48.346297: Epoch 850
2024-11-27 22:17:48.347154: Current learning rate: 0.00181
2024-11-27 22:23:10.152212: train_loss -0.8196
2024-11-27 22:23:10.153304: val_loss -0.5432
2024-11-27 22:23:10.154086: Pseudo dice [0.7455]
2024-11-27 22:23:10.154844: Epoch time: 321.81 s
2024-11-27 22:23:11.748512: 
2024-11-27 22:23:11.749698: Epoch 851
2024-11-27 22:23:11.750511: Current learning rate: 0.0018
2024-11-27 22:28:30.038883: train_loss -0.8164
2024-11-27 22:28:30.040495: val_loss -0.5099
2024-11-27 22:28:30.041674: Pseudo dice [0.7248]
2024-11-27 22:28:30.042621: Epoch time: 318.29 s
2024-11-27 22:28:31.695296: 
2024-11-27 22:28:31.696626: Epoch 852
2024-11-27 22:28:31.697458: Current learning rate: 0.00179
2024-11-27 22:33:39.823911: train_loss -0.8146
2024-11-27 22:33:39.824883: val_loss -0.5324
2024-11-27 22:33:39.825925: Pseudo dice [0.7407]
2024-11-27 22:33:39.826914: Epoch time: 308.13 s
2024-11-27 22:33:41.515656: 
2024-11-27 22:33:41.517109: Epoch 853
2024-11-27 22:33:41.518048: Current learning rate: 0.00178
2024-11-27 22:38:43.019826: train_loss -0.8168
2024-11-27 22:38:43.020740: val_loss -0.5533
2024-11-27 22:38:43.021744: Pseudo dice [0.7763]
2024-11-27 22:38:43.022767: Epoch time: 301.51 s
2024-11-27 22:38:44.635451: 
2024-11-27 22:38:44.636885: Epoch 854
2024-11-27 22:38:44.637967: Current learning rate: 0.00177
2024-11-27 22:43:27.660813: train_loss -0.8131
2024-11-27 22:43:27.661854: val_loss -0.4615
2024-11-27 22:43:27.662632: Pseudo dice [0.693]
2024-11-27 22:43:27.663384: Epoch time: 283.03 s
2024-11-27 22:43:29.232822: 
2024-11-27 22:43:29.234191: Epoch 855
2024-11-27 22:43:29.235049: Current learning rate: 0.00176
2024-11-27 22:48:50.498183: train_loss -0.8177
2024-11-27 22:48:50.501109: val_loss -0.4881
2024-11-27 22:48:50.502000: Pseudo dice [0.7136]
2024-11-27 22:48:50.503454: Epoch time: 321.27 s
2024-11-27 22:48:52.116489: 
2024-11-27 22:48:52.117946: Epoch 856
2024-11-27 22:48:52.119053: Current learning rate: 0.00175
2024-11-27 22:53:59.202405: train_loss -0.814
2024-11-27 22:53:59.204757: val_loss -0.4497
2024-11-27 22:53:59.205928: Pseudo dice [0.6927]
2024-11-27 22:53:59.207052: Epoch time: 307.09 s
2024-11-27 22:54:00.805514: 
2024-11-27 22:54:00.807060: Epoch 857
2024-11-27 22:54:00.808009: Current learning rate: 0.00174
2024-11-27 22:59:24.054903: train_loss -0.812
2024-11-27 22:59:24.055962: val_loss -0.3994
2024-11-27 22:59:24.056919: Pseudo dice [0.6918]
2024-11-27 22:59:24.057927: Epoch time: 323.25 s
2024-11-27 22:59:25.697936: 
2024-11-27 22:59:25.699363: Epoch 858
2024-11-27 22:59:25.700472: Current learning rate: 0.00173
2024-11-27 23:04:19.907420: train_loss -0.8109
2024-11-27 23:04:19.908294: val_loss -0.5352
2024-11-27 23:04:19.909232: Pseudo dice [0.7235]
2024-11-27 23:04:19.909990: Epoch time: 294.21 s
2024-11-27 23:04:21.549795: 
2024-11-27 23:04:21.551781: Epoch 859
2024-11-27 23:04:21.552797: Current learning rate: 0.00172
2024-11-27 23:09:49.444395: train_loss -0.8134
2024-11-27 23:09:49.445378: val_loss -0.502
2024-11-27 23:09:49.446206: Pseudo dice [0.7283]
2024-11-27 23:09:49.446951: Epoch time: 327.9 s
2024-11-27 23:09:51.512888: 
2024-11-27 23:09:51.514189: Epoch 860
2024-11-27 23:09:51.515015: Current learning rate: 0.0017
2024-11-27 23:15:05.314301: train_loss -0.8058
2024-11-27 23:15:05.315369: val_loss -0.5161
2024-11-27 23:15:05.316131: Pseudo dice [0.7202]
2024-11-27 23:15:05.316882: Epoch time: 313.8 s
2024-11-27 23:15:06.998909: 
2024-11-27 23:15:07.000293: Epoch 861
2024-11-27 23:15:07.001046: Current learning rate: 0.00169
2024-11-27 23:20:13.397651: train_loss -0.816
2024-11-27 23:20:13.398659: val_loss -0.4673
2024-11-27 23:20:13.399468: Pseudo dice [0.7014]
2024-11-27 23:20:13.400226: Epoch time: 306.4 s
2024-11-27 23:20:15.006041: 
2024-11-27 23:20:15.007473: Epoch 862
2024-11-27 23:20:15.008621: Current learning rate: 0.00168
2024-11-27 23:25:11.403819: train_loss -0.8166
2024-11-27 23:25:11.404944: val_loss -0.5112
2024-11-27 23:25:11.405777: Pseudo dice [0.7186]
2024-11-27 23:25:11.406612: Epoch time: 296.4 s
2024-11-27 23:25:13.090659: 
2024-11-27 23:25:13.092027: Epoch 863
2024-11-27 23:25:13.092916: Current learning rate: 0.00167
2024-11-27 23:30:42.407202: train_loss -0.8107
2024-11-27 23:30:42.408171: val_loss -0.5227
2024-11-27 23:30:42.408911: Pseudo dice [0.7241]
2024-11-27 23:30:42.409633: Epoch time: 329.32 s
2024-11-27 23:30:43.946250: 
2024-11-27 23:30:43.947557: Epoch 864
2024-11-27 23:30:43.948428: Current learning rate: 0.00166
2024-11-27 23:36:00.364506: train_loss -0.8133
2024-11-27 23:36:00.366126: val_loss -0.5409
2024-11-27 23:36:00.367128: Pseudo dice [0.7402]
2024-11-27 23:36:00.368116: Epoch time: 316.42 s
2024-11-27 23:36:01.992793: 
2024-11-27 23:36:01.994394: Epoch 865
2024-11-27 23:36:01.995407: Current learning rate: 0.00165
2024-11-27 23:41:26.669526: train_loss -0.806
2024-11-27 23:41:26.670521: val_loss -0.5717
2024-11-27 23:41:26.671334: Pseudo dice [0.7509]
2024-11-27 23:41:26.672142: Epoch time: 324.68 s
2024-11-27 23:41:28.219492: 
2024-11-27 23:41:28.220808: Epoch 866
2024-11-27 23:41:28.221699: Current learning rate: 0.00164
2024-11-27 23:46:47.899562: train_loss -0.8085
2024-11-27 23:46:47.900521: val_loss -0.4614
2024-11-27 23:46:47.901454: Pseudo dice [0.7177]
2024-11-27 23:46:47.902577: Epoch time: 319.68 s
2024-11-27 23:46:49.509458: 
2024-11-27 23:46:49.511082: Epoch 867
2024-11-27 23:46:49.512232: Current learning rate: 0.00163
2024-11-27 23:52:03.634540: train_loss -0.8138
2024-11-27 23:52:03.637739: val_loss -0.529
2024-11-27 23:52:03.638761: Pseudo dice [0.7414]
2024-11-27 23:52:03.640500: Epoch time: 314.13 s
2024-11-27 23:52:05.226824: 
2024-11-27 23:52:05.228170: Epoch 868
2024-11-27 23:52:05.228950: Current learning rate: 0.00162
2024-11-27 23:57:22.297299: train_loss -0.8084
2024-11-27 23:57:22.298292: val_loss -0.5561
2024-11-27 23:57:22.299074: Pseudo dice [0.757]
2024-11-27 23:57:22.299847: Epoch time: 317.07 s
2024-11-27 23:57:23.881859: 
2024-11-27 23:57:23.883329: Epoch 869
2024-11-27 23:57:23.884384: Current learning rate: 0.00161
2024-11-28 00:02:25.050991: train_loss -0.8162
2024-11-28 00:02:25.054625: val_loss -0.4912
2024-11-28 00:02:25.055492: Pseudo dice [0.7203]
2024-11-28 00:02:25.056448: Epoch time: 301.17 s
2024-11-28 00:02:26.631320: 
2024-11-28 00:02:26.632753: Epoch 870
2024-11-28 00:02:26.633562: Current learning rate: 0.00159
2024-11-28 00:07:21.259867: train_loss -0.8108
2024-11-28 00:07:21.260855: val_loss -0.5754
2024-11-28 00:07:21.261717: Pseudo dice [0.7675]
2024-11-28 00:07:21.262527: Epoch time: 294.63 s
2024-11-28 00:07:22.971630: 
2024-11-28 00:07:22.972842: Epoch 871
2024-11-28 00:07:22.973708: Current learning rate: 0.00158
2024-11-28 00:12:56.518471: train_loss -0.8073
2024-11-28 00:12:56.519419: val_loss -0.506
2024-11-28 00:12:56.520129: Pseudo dice [0.7199]
2024-11-28 00:12:56.520837: Epoch time: 333.55 s
2024-11-28 00:12:58.638062: 
2024-11-28 00:12:58.639371: Epoch 872
2024-11-28 00:12:58.640270: Current learning rate: 0.00157
2024-11-28 00:18:18.971037: train_loss -0.8086
2024-11-28 00:18:18.972091: val_loss -0.5847
2024-11-28 00:18:18.972960: Pseudo dice [0.7503]
2024-11-28 00:18:18.973805: Epoch time: 320.33 s
2024-11-28 00:18:20.509113: 
2024-11-28 00:18:20.510267: Epoch 873
2024-11-28 00:18:20.510968: Current learning rate: 0.00156
2024-11-28 00:23:43.468756: train_loss -0.8073
2024-11-28 00:23:43.469749: val_loss -0.5143
2024-11-28 00:23:43.470655: Pseudo dice [0.7293]
2024-11-28 00:23:43.471465: Epoch time: 322.96 s
2024-11-28 00:23:45.234365: 
2024-11-28 00:23:45.235773: Epoch 874
2024-11-28 00:23:45.236699: Current learning rate: 0.00155
2024-11-28 00:28:54.120260: train_loss -0.8125
2024-11-28 00:28:54.121385: val_loss -0.4752
2024-11-28 00:28:54.122195: Pseudo dice [0.7043]
2024-11-28 00:28:54.122906: Epoch time: 308.89 s
2024-11-28 00:28:55.670965: 
2024-11-28 00:28:55.672697: Epoch 875
2024-11-28 00:28:55.673769: Current learning rate: 0.00154
2024-11-28 00:34:34.150055: train_loss -0.8154
2024-11-28 00:34:34.151767: val_loss -0.4473
2024-11-28 00:34:34.152998: Pseudo dice [0.7012]
2024-11-28 00:34:34.154065: Epoch time: 338.48 s
2024-11-28 00:34:35.738128: 
2024-11-28 00:34:35.739584: Epoch 876
2024-11-28 00:34:35.740632: Current learning rate: 0.00153
2024-11-28 00:39:48.116141: train_loss -0.8184
2024-11-28 00:39:48.117315: val_loss -0.4578
2024-11-28 00:39:48.118173: Pseudo dice [0.7114]
2024-11-28 00:39:48.119030: Epoch time: 312.38 s
2024-11-28 00:39:49.715966: 
2024-11-28 00:39:49.717427: Epoch 877
2024-11-28 00:39:49.718489: Current learning rate: 0.00152
2024-11-28 00:45:13.517829: train_loss -0.8081
2024-11-28 00:45:13.520507: val_loss -0.5444
2024-11-28 00:45:13.521670: Pseudo dice [0.7322]
2024-11-28 00:45:13.522472: Epoch time: 323.8 s
2024-11-28 00:45:15.096551: 
2024-11-28 00:45:15.098062: Epoch 878
2024-11-28 00:45:15.099068: Current learning rate: 0.00151
2024-11-28 00:50:19.962202: train_loss -0.7974
2024-11-28 00:50:19.963566: val_loss -0.4997
2024-11-28 00:50:19.964579: Pseudo dice [0.7191]
2024-11-28 00:50:19.965464: Epoch time: 304.87 s
2024-11-28 00:50:21.556786: 
2024-11-28 00:50:21.558073: Epoch 879
2024-11-28 00:50:21.558835: Current learning rate: 0.00149
2024-11-28 00:55:41.339431: train_loss -0.8145
2024-11-28 00:55:41.340839: val_loss -0.5188
2024-11-28 00:55:41.341895: Pseudo dice [0.726]
2024-11-28 00:55:41.343086: Epoch time: 319.78 s
2024-11-28 00:55:43.131129: 
2024-11-28 00:55:43.132380: Epoch 880
2024-11-28 00:55:43.133392: Current learning rate: 0.00148
2024-11-28 01:00:57.864240: train_loss -0.8108
2024-11-28 01:00:57.866664: val_loss -0.5779
2024-11-28 01:00:57.867508: Pseudo dice [0.7713]
2024-11-28 01:00:57.868573: Epoch time: 314.73 s
2024-11-28 01:00:59.428701: 
2024-11-28 01:00:59.430212: Epoch 881
2024-11-28 01:00:59.431009: Current learning rate: 0.00147
2024-11-28 01:06:09.385013: train_loss -0.811
2024-11-28 01:06:09.387060: val_loss -0.4715
2024-11-28 01:06:09.387768: Pseudo dice [0.7054]
2024-11-28 01:06:09.388685: Epoch time: 309.96 s
2024-11-28 01:06:11.015397: 
2024-11-28 01:06:11.016770: Epoch 882
2024-11-28 01:06:11.017574: Current learning rate: 0.00146
2024-11-28 01:11:29.849909: train_loss -0.8138
2024-11-28 01:11:29.850747: val_loss -0.456
2024-11-28 01:11:29.851845: Pseudo dice [0.6738]
2024-11-28 01:11:29.852829: Epoch time: 318.84 s
2024-11-28 01:11:32.283329: 
2024-11-28 01:11:32.284659: Epoch 883
2024-11-28 01:11:32.285563: Current learning rate: 0.00145
2024-11-28 01:16:57.450922: train_loss -0.8163
2024-11-28 01:16:57.451710: val_loss -0.412
2024-11-28 01:16:57.452619: Pseudo dice [0.6859]
2024-11-28 01:16:57.453496: Epoch time: 325.17 s
2024-11-28 01:16:59.042569: 
2024-11-28 01:16:59.043983: Epoch 884
2024-11-28 01:16:59.045015: Current learning rate: 0.00144
2024-11-28 01:22:17.851897: train_loss -0.8126
2024-11-28 01:22:17.853105: val_loss -0.4924
2024-11-28 01:22:17.853987: Pseudo dice [0.7316]
2024-11-28 01:22:17.854784: Epoch time: 318.81 s
2024-11-28 01:22:19.492127: 
2024-11-28 01:22:19.493645: Epoch 885
2024-11-28 01:22:19.494526: Current learning rate: 0.00143
2024-11-28 01:27:29.799702: train_loss -0.8137
2024-11-28 01:27:29.800766: val_loss -0.5194
2024-11-28 01:27:29.801623: Pseudo dice [0.7425]
2024-11-28 01:27:29.802490: Epoch time: 310.31 s
2024-11-28 01:27:31.473342: 
2024-11-28 01:27:31.474853: Epoch 886
2024-11-28 01:27:31.475819: Current learning rate: 0.00142
2024-11-28 01:32:40.802729: train_loss -0.8152
2024-11-28 01:32:40.803862: val_loss -0.4656
2024-11-28 01:32:40.804806: Pseudo dice [0.7226]
2024-11-28 01:32:40.805718: Epoch time: 309.33 s
2024-11-28 01:32:42.422002: 
2024-11-28 01:32:42.423428: Epoch 887
2024-11-28 01:32:42.424284: Current learning rate: 0.00141
2024-11-28 01:37:53.581972: train_loss -0.8127
2024-11-28 01:37:53.583059: val_loss -0.481
2024-11-28 01:37:53.584071: Pseudo dice [0.7114]
2024-11-28 01:37:53.585022: Epoch time: 311.16 s
2024-11-28 01:37:55.165592: 
2024-11-28 01:37:55.167053: Epoch 888
2024-11-28 01:37:55.168112: Current learning rate: 0.00139
2024-11-28 01:43:12.055069: train_loss -0.8183
2024-11-28 01:43:12.055873: val_loss -0.4979
2024-11-28 01:43:12.056685: Pseudo dice [0.7289]
2024-11-28 01:43:12.057487: Epoch time: 316.89 s
2024-11-28 01:43:13.654474: 
2024-11-28 01:43:13.655807: Epoch 889
2024-11-28 01:43:13.656632: Current learning rate: 0.00138
2024-11-28 01:48:28.682653: train_loss -0.8188
2024-11-28 01:48:28.683540: val_loss -0.4906
2024-11-28 01:48:28.684523: Pseudo dice [0.723]
2024-11-28 01:48:28.685496: Epoch time: 315.03 s
2024-11-28 01:48:30.352301: 
2024-11-28 01:48:30.353434: Epoch 890
2024-11-28 01:48:30.354380: Current learning rate: 0.00137
2024-11-28 01:53:48.252459: train_loss -0.8196
2024-11-28 01:53:48.253893: val_loss -0.531
2024-11-28 01:53:48.254747: Pseudo dice [0.7362]
2024-11-28 01:53:48.258934: Epoch time: 317.9 s
2024-11-28 01:53:49.832455: 
2024-11-28 01:53:49.833852: Epoch 891
2024-11-28 01:53:49.835007: Current learning rate: 0.00136
2024-11-28 01:59:01.228880: train_loss -0.8153
2024-11-28 01:59:01.229803: val_loss -0.5475
2024-11-28 01:59:01.230696: Pseudo dice [0.7448]
2024-11-28 01:59:01.231466: Epoch time: 311.4 s
2024-11-28 01:59:02.758163: 
2024-11-28 01:59:02.759408: Epoch 892
2024-11-28 01:59:02.760312: Current learning rate: 0.00135
2024-11-28 02:03:51.554702: train_loss -0.8169
2024-11-28 02:03:51.557931: val_loss -0.5376
2024-11-28 02:03:51.558939: Pseudo dice [0.7488]
2024-11-28 02:03:51.560941: Epoch time: 288.8 s
2024-11-28 02:03:53.273776: 
2024-11-28 02:03:53.275218: Epoch 893
2024-11-28 02:03:53.276261: Current learning rate: 0.00134
2024-11-28 02:09:19.807448: train_loss -0.8221
2024-11-28 02:09:19.809832: val_loss -0.4783
2024-11-28 02:09:19.810657: Pseudo dice [0.7176]
2024-11-28 02:09:19.811469: Epoch time: 326.53 s
2024-11-28 02:09:21.417055: 
2024-11-28 02:09:21.418489: Epoch 894
2024-11-28 02:09:21.419509: Current learning rate: 0.00133
2024-11-28 02:14:27.527231: train_loss -0.8151
2024-11-28 02:14:27.528218: val_loss -0.5531
2024-11-28 02:14:27.529015: Pseudo dice [0.75]
2024-11-28 02:14:27.529890: Epoch time: 306.11 s
2024-11-28 02:14:29.956050: 
2024-11-28 02:14:29.957479: Epoch 895
2024-11-28 02:14:29.958297: Current learning rate: 0.00132
2024-11-28 02:19:45.385818: train_loss -0.8167
2024-11-28 02:19:45.386787: val_loss -0.5002
2024-11-28 02:19:45.387574: Pseudo dice [0.7257]
2024-11-28 02:19:45.388329: Epoch time: 315.43 s
2024-11-28 02:19:46.921790: 
2024-11-28 02:19:46.923034: Epoch 896
2024-11-28 02:19:46.923827: Current learning rate: 0.0013
2024-11-28 02:24:47.066238: train_loss -0.8197
2024-11-28 02:24:47.067020: val_loss -0.5118
2024-11-28 02:24:47.067795: Pseudo dice [0.7276]
2024-11-28 02:24:47.068588: Epoch time: 300.15 s
2024-11-28 02:24:48.673400: 
2024-11-28 02:24:48.674870: Epoch 897
2024-11-28 02:24:48.675653: Current learning rate: 0.00129
2024-11-28 02:29:46.203502: train_loss -0.8234
2024-11-28 02:29:46.204494: val_loss -0.4915
2024-11-28 02:29:46.205370: Pseudo dice [0.7042]
2024-11-28 02:29:46.206037: Epoch time: 297.53 s
2024-11-28 02:29:47.768618: 
2024-11-28 02:29:47.769867: Epoch 898
2024-11-28 02:29:47.770627: Current learning rate: 0.00128
2024-11-28 02:35:08.268553: train_loss -0.8129
2024-11-28 02:35:08.269524: val_loss -0.4799
2024-11-28 02:35:08.270334: Pseudo dice [0.7198]
2024-11-28 02:35:08.270996: Epoch time: 320.5 s
2024-11-28 02:35:09.816086: 
2024-11-28 02:35:09.818075: Epoch 899
2024-11-28 02:35:09.818884: Current learning rate: 0.00127
2024-11-28 02:40:33.739422: train_loss -0.8213
2024-11-28 02:40:33.740830: val_loss -0.5159
2024-11-28 02:40:33.741553: Pseudo dice [0.7278]
2024-11-28 02:40:33.742307: Epoch time: 323.93 s
2024-11-28 02:40:35.880480: 
2024-11-28 02:40:35.881551: Epoch 900
2024-11-28 02:40:35.882335: Current learning rate: 0.00126
2024-11-28 02:45:49.776465: train_loss -0.8203
2024-11-28 02:45:49.777306: val_loss -0.4617
2024-11-28 02:45:49.778307: Pseudo dice [0.6992]
2024-11-28 02:45:49.779126: Epoch time: 313.9 s
2024-11-28 02:45:51.302307: 
2024-11-28 02:45:51.303589: Epoch 901
2024-11-28 02:45:51.304358: Current learning rate: 0.00125
2024-11-28 02:50:31.064428: train_loss -0.8209
2024-11-28 02:50:31.065334: val_loss -0.4877
2024-11-28 02:50:31.066087: Pseudo dice [0.7065]
2024-11-28 02:50:31.066852: Epoch time: 279.76 s
2024-11-28 02:50:32.624238: 
2024-11-28 02:50:32.625497: Epoch 902
2024-11-28 02:50:32.626216: Current learning rate: 0.00124
2024-11-28 02:54:26.730695: train_loss -0.8235
2024-11-28 02:54:26.731585: val_loss -0.4535
2024-11-28 02:54:26.732382: Pseudo dice [0.7181]
2024-11-28 02:54:26.733068: Epoch time: 234.11 s
2024-11-28 02:54:28.222745: 
2024-11-28 02:54:28.224142: Epoch 903
2024-11-28 02:54:28.224929: Current learning rate: 0.00122
2024-11-28 02:58:38.234636: train_loss -0.8237
2024-11-28 02:58:38.235717: val_loss -0.528
2024-11-28 02:58:38.236829: Pseudo dice [0.739]
2024-11-28 02:58:38.237786: Epoch time: 250.01 s
2024-11-28 02:58:39.734284: 
2024-11-28 02:58:39.736223: Epoch 904
2024-11-28 02:58:39.737375: Current learning rate: 0.00121
2024-11-28 03:03:37.658771: train_loss -0.8176
2024-11-28 03:03:37.659739: val_loss -0.5239
2024-11-28 03:03:37.660825: Pseudo dice [0.7359]
2024-11-28 03:03:37.661753: Epoch time: 297.93 s
2024-11-28 03:03:39.156261: 
2024-11-28 03:03:39.157765: Epoch 905
2024-11-28 03:03:39.159009: Current learning rate: 0.0012
2024-11-28 03:08:38.542718: train_loss -0.8227
2024-11-28 03:08:38.545655: val_loss -0.5191
2024-11-28 03:08:38.546919: Pseudo dice [0.7318]
2024-11-28 03:08:38.548195: Epoch time: 299.39 s
2024-11-28 03:08:40.913439: 
2024-11-28 03:08:40.917083: Epoch 906
2024-11-28 03:08:40.918259: Current learning rate: 0.00119
2024-11-28 03:13:51.515809: train_loss -0.8229
2024-11-28 03:13:51.543980: val_loss -0.4976
2024-11-28 03:13:51.544951: Pseudo dice [0.7167]
2024-11-28 03:13:51.546050: Epoch time: 310.61 s
2024-11-28 03:13:53.228350: 
2024-11-28 03:13:53.229819: Epoch 907
2024-11-28 03:13:53.230763: Current learning rate: 0.00118
2024-11-28 03:19:12.338993: train_loss -0.821
2024-11-28 03:19:12.339852: val_loss -0.4986
2024-11-28 03:19:12.340934: Pseudo dice [0.7288]
2024-11-28 03:19:12.341900: Epoch time: 319.11 s
2024-11-28 03:19:13.893252: 
2024-11-28 03:19:13.894797: Epoch 908
2024-11-28 03:19:13.895838: Current learning rate: 0.00117
2024-11-28 03:23:25.697026: train_loss -0.8147
2024-11-28 03:23:25.697992: val_loss -0.5289
2024-11-28 03:23:25.698832: Pseudo dice [0.7297]
2024-11-28 03:23:25.699740: Epoch time: 251.81 s
2024-11-28 03:23:27.300196: 
2024-11-28 03:23:27.301610: Epoch 909
2024-11-28 03:23:27.302766: Current learning rate: 0.00116
2024-11-28 03:27:06.875020: train_loss -0.8209
2024-11-28 03:27:06.876010: val_loss -0.5041
2024-11-28 03:27:06.876891: Pseudo dice [0.7289]
2024-11-28 03:27:06.877822: Epoch time: 219.58 s
2024-11-28 03:27:08.459119: 
2024-11-28 03:27:08.460340: Epoch 910
2024-11-28 03:27:08.461088: Current learning rate: 0.00115
2024-11-28 03:31:11.552403: train_loss -0.8137
2024-11-28 03:31:11.553389: val_loss -0.5685
2024-11-28 03:31:11.554317: Pseudo dice [0.7547]
2024-11-28 03:31:11.555397: Epoch time: 243.09 s
2024-11-28 03:31:13.142673: 
2024-11-28 03:31:13.144192: Epoch 911
2024-11-28 03:31:13.145131: Current learning rate: 0.00113
2024-11-28 03:35:14.396383: train_loss -0.8185
2024-11-28 03:35:14.397164: val_loss -0.57
2024-11-28 03:35:14.397921: Pseudo dice [0.7562]
2024-11-28 03:35:14.398659: Epoch time: 241.26 s
2024-11-28 03:35:15.893839: 
2024-11-28 03:35:15.895106: Epoch 912
2024-11-28 03:35:15.895899: Current learning rate: 0.00112
2024-11-28 03:38:54.697490: train_loss -0.8165
2024-11-28 03:38:54.698435: val_loss -0.455
2024-11-28 03:38:54.699191: Pseudo dice [0.7305]
2024-11-28 03:38:54.699937: Epoch time: 218.8 s
2024-11-28 03:38:56.257691: 
2024-11-28 03:38:56.259018: Epoch 913
2024-11-28 03:38:56.259776: Current learning rate: 0.00111
2024-11-28 03:43:10.402263: train_loss -0.8166
2024-11-28 03:43:10.403008: val_loss -0.5271
2024-11-28 03:43:10.403984: Pseudo dice [0.7361]
2024-11-28 03:43:10.404885: Epoch time: 254.15 s
2024-11-28 03:43:11.950160: 
2024-11-28 03:43:11.951547: Epoch 914
2024-11-28 03:43:11.952441: Current learning rate: 0.0011
2024-11-28 03:46:37.355479: train_loss -0.819
2024-11-28 03:46:37.356748: val_loss -0.4737
2024-11-28 03:46:37.357552: Pseudo dice [0.6931]
2024-11-28 03:46:37.358332: Epoch time: 205.41 s
2024-11-28 03:46:39.002401: 
2024-11-28 03:46:39.003872: Epoch 915
2024-11-28 03:46:39.004887: Current learning rate: 0.00109
2024-11-28 03:50:16.264196: train_loss -0.8188
2024-11-28 03:50:16.265041: val_loss -0.4189
2024-11-28 03:50:16.265900: Pseudo dice [0.6501]
2024-11-28 03:50:16.266683: Epoch time: 217.26 s
2024-11-28 03:50:17.808825: 
2024-11-28 03:50:17.810084: Epoch 916
2024-11-28 03:50:17.810809: Current learning rate: 0.00108
2024-11-28 03:54:17.889128: train_loss -0.8147
2024-11-28 03:54:17.889954: val_loss -0.4807
2024-11-28 03:54:17.890836: Pseudo dice [0.7122]
2024-11-28 03:54:17.891701: Epoch time: 240.08 s
2024-11-28 03:54:19.409469: 
2024-11-28 03:54:19.410471: Epoch 917
2024-11-28 03:54:19.411241: Current learning rate: 0.00106
2024-11-28 03:58:10.468662: train_loss -0.8189
2024-11-28 03:58:10.469606: val_loss -0.4926
2024-11-28 03:58:10.470357: Pseudo dice [0.7266]
2024-11-28 03:58:10.471130: Epoch time: 231.06 s
2024-11-28 03:58:12.923354: 
2024-11-28 03:58:12.924760: Epoch 918
2024-11-28 03:58:12.925601: Current learning rate: 0.00105
2024-11-28 04:02:05.383924: train_loss -0.8252
2024-11-28 04:02:05.384855: val_loss -0.4994
2024-11-28 04:02:05.385799: Pseudo dice [0.7216]
2024-11-28 04:02:05.386629: Epoch time: 232.46 s
2024-11-28 04:02:06.946410: 
2024-11-28 04:02:06.947675: Epoch 919
2024-11-28 04:02:06.948533: Current learning rate: 0.00104
2024-11-28 04:06:13.069910: train_loss -0.8133
2024-11-28 04:06:13.070997: val_loss -0.4562
2024-11-28 04:06:13.072113: Pseudo dice [0.7134]
2024-11-28 04:06:13.073182: Epoch time: 246.12 s
2024-11-28 04:06:14.719371: 
2024-11-28 04:06:14.720802: Epoch 920
2024-11-28 04:06:14.721712: Current learning rate: 0.00103
2024-11-28 04:10:23.930993: train_loss -0.817
2024-11-28 04:10:23.931971: val_loss -0.5438
2024-11-28 04:10:23.932717: Pseudo dice [0.7523]
2024-11-28 04:10:23.933428: Epoch time: 249.21 s
2024-11-28 04:10:25.544794: 
2024-11-28 04:10:25.546791: Epoch 921
2024-11-28 04:10:25.547885: Current learning rate: 0.00102
2024-11-28 04:14:24.856249: train_loss -0.8213
2024-11-28 04:14:24.860963: val_loss -0.4737
2024-11-28 04:14:24.861855: Pseudo dice [0.7288]
2024-11-28 04:14:24.863381: Epoch time: 239.32 s
2024-11-28 04:14:26.397958: 
2024-11-28 04:14:26.399161: Epoch 922
2024-11-28 04:14:26.400251: Current learning rate: 0.00101
2024-11-28 04:17:38.350002: train_loss -0.8187
2024-11-28 04:17:38.350803: val_loss -0.4572
2024-11-28 04:17:38.351842: Pseudo dice [0.7131]
2024-11-28 04:17:38.352803: Epoch time: 191.95 s
2024-11-28 04:17:39.856693: 
2024-11-28 04:17:39.857881: Epoch 923
2024-11-28 04:17:39.858835: Current learning rate: 0.001
2024-11-28 04:21:29.972942: train_loss -0.8208
2024-11-28 04:21:29.975804: val_loss -0.5226
2024-11-28 04:21:29.976514: Pseudo dice [0.7334]
2024-11-28 04:21:29.977368: Epoch time: 230.12 s
2024-11-28 04:21:31.496762: 
2024-11-28 04:21:31.497970: Epoch 924
2024-11-28 04:21:31.498726: Current learning rate: 0.00098
2024-11-28 04:25:22.769633: train_loss -0.8243
2024-11-28 04:25:22.771156: val_loss -0.5473
2024-11-28 04:25:22.771932: Pseudo dice [0.7376]
2024-11-28 04:25:22.772729: Epoch time: 231.27 s
2024-11-28 04:25:24.285188: 
2024-11-28 04:25:24.286623: Epoch 925
2024-11-28 04:25:24.287450: Current learning rate: 0.00097
2024-11-28 04:29:06.028750: train_loss -0.8248
2024-11-28 04:29:06.029810: val_loss -0.5612
2024-11-28 04:29:06.030567: Pseudo dice [0.7431]
2024-11-28 04:29:06.031292: Epoch time: 221.74 s
2024-11-28 04:29:07.529772: 
2024-11-28 04:29:07.531115: Epoch 926
2024-11-28 04:29:07.531846: Current learning rate: 0.00096
2024-11-28 04:32:54.432324: train_loss -0.8273
2024-11-28 04:32:54.433189: val_loss -0.5229
2024-11-28 04:32:54.433908: Pseudo dice [0.7258]
2024-11-28 04:32:54.434579: Epoch time: 226.9 s
2024-11-28 04:32:55.940660: 
2024-11-28 04:32:55.941772: Epoch 927
2024-11-28 04:32:55.942511: Current learning rate: 0.00095
2024-11-28 04:36:59.745172: train_loss -0.8176
2024-11-28 04:36:59.746047: val_loss -0.4851
2024-11-28 04:36:59.746922: Pseudo dice [0.7129]
2024-11-28 04:36:59.747542: Epoch time: 243.81 s
2024-11-28 04:37:01.257716: 
2024-11-28 04:37:01.259041: Epoch 928
2024-11-28 04:37:01.259899: Current learning rate: 0.00094
2024-11-28 04:40:33.170600: train_loss -0.8196
2024-11-28 04:40:33.171631: val_loss -0.4798
2024-11-28 04:40:33.172626: Pseudo dice [0.7246]
2024-11-28 04:40:33.173454: Epoch time: 211.91 s
2024-11-28 04:40:34.706558: 
2024-11-28 04:40:34.707794: Epoch 929
2024-11-28 04:40:34.708644: Current learning rate: 0.00092
2024-11-28 04:44:19.329549: train_loss -0.8286
2024-11-28 04:44:19.330465: val_loss -0.529
2024-11-28 04:44:19.331363: Pseudo dice [0.7395]
2024-11-28 04:44:19.332080: Epoch time: 224.62 s
2024-11-28 04:44:21.493316: 
2024-11-28 04:44:21.494546: Epoch 930
2024-11-28 04:44:21.495260: Current learning rate: 0.00091
2024-11-28 04:48:19.790954: train_loss -0.8255
2024-11-28 04:48:19.791984: val_loss -0.5075
2024-11-28 04:48:19.793047: Pseudo dice [0.732]
2024-11-28 04:48:19.794043: Epoch time: 238.3 s
2024-11-28 04:48:21.293826: 
2024-11-28 04:48:21.295125: Epoch 931
2024-11-28 04:48:21.295886: Current learning rate: 0.0009
2024-11-28 04:52:09.315027: train_loss -0.8175
2024-11-28 04:52:09.316531: val_loss -0.5339
2024-11-28 04:52:09.317314: Pseudo dice [0.7333]
2024-11-28 04:52:09.318024: Epoch time: 228.02 s
2024-11-28 04:52:10.917694: 
2024-11-28 04:52:10.919716: Epoch 932
2024-11-28 04:52:10.920769: Current learning rate: 0.00089
2024-11-28 04:56:15.278590: train_loss -0.8218
2024-11-28 04:56:15.279869: val_loss -0.5049
2024-11-28 04:56:15.280613: Pseudo dice [0.7131]
2024-11-28 04:56:15.281556: Epoch time: 244.36 s
2024-11-28 04:56:16.898105: 
2024-11-28 04:56:16.899532: Epoch 933
2024-11-28 04:56:16.900554: Current learning rate: 0.00088
2024-11-28 05:00:24.523049: train_loss -0.8237
2024-11-28 05:00:24.523980: val_loss -0.4871
2024-11-28 05:00:24.524811: Pseudo dice [0.7184]
2024-11-28 05:00:24.525559: Epoch time: 247.63 s
2024-11-28 05:00:26.146274: 
2024-11-28 05:00:26.147573: Epoch 934
2024-11-28 05:00:26.148498: Current learning rate: 0.00087
2024-11-28 05:04:22.796026: train_loss -0.8264
2024-11-28 05:04:22.796954: val_loss -0.482
2024-11-28 05:04:22.797801: Pseudo dice [0.7121]
2024-11-28 05:04:22.798590: Epoch time: 236.65 s
2024-11-28 05:04:24.303113: 
2024-11-28 05:04:24.304517: Epoch 935
2024-11-28 05:04:24.305601: Current learning rate: 0.00085
2024-11-28 05:08:23.619582: train_loss -0.8217
2024-11-28 05:08:23.620625: val_loss -0.5016
2024-11-28 05:08:23.621697: Pseudo dice [0.7093]
2024-11-28 05:08:23.622455: Epoch time: 239.32 s
2024-11-28 05:08:25.192546: 
2024-11-28 05:08:25.194108: Epoch 936
2024-11-28 05:08:25.195072: Current learning rate: 0.00084
2024-11-28 05:12:04.806362: train_loss -0.828
2024-11-28 05:12:04.812893: val_loss -0.4654
2024-11-28 05:12:04.813797: Pseudo dice [0.701]
2024-11-28 05:12:04.814614: Epoch time: 219.62 s
2024-11-28 05:12:06.354236: 
2024-11-28 05:12:06.355616: Epoch 937
2024-11-28 05:12:06.356536: Current learning rate: 0.00083
2024-11-28 05:15:49.581055: train_loss -0.8272
2024-11-28 05:15:49.582584: val_loss -0.5166
2024-11-28 05:15:49.583507: Pseudo dice [0.7331]
2024-11-28 05:15:49.585105: Epoch time: 223.23 s
2024-11-28 05:15:51.185082: 
2024-11-28 05:15:51.186504: Epoch 938
2024-11-28 05:15:51.187516: Current learning rate: 0.00082
2024-11-28 05:19:43.022770: train_loss -0.8269
2024-11-28 05:19:43.025348: val_loss -0.517
2024-11-28 05:19:43.026284: Pseudo dice [0.7448]
2024-11-28 05:19:43.027214: Epoch time: 231.84 s
2024-11-28 05:19:44.600835: 
2024-11-28 05:19:44.602230: Epoch 939
2024-11-28 05:19:44.603093: Current learning rate: 0.00081
2024-11-28 05:23:58.490466: train_loss -0.8209
2024-11-28 05:23:58.491513: val_loss -0.4249
2024-11-28 05:23:58.492343: Pseudo dice [0.6938]
2024-11-28 05:23:58.493055: Epoch time: 253.89 s
2024-11-28 05:24:00.058530: 
2024-11-28 05:24:00.059661: Epoch 940
2024-11-28 05:24:00.060526: Current learning rate: 0.00079
2024-11-28 05:28:01.935208: train_loss -0.8206
2024-11-28 05:28:01.937374: val_loss -0.4628
2024-11-28 05:28:01.938345: Pseudo dice [0.699]
2024-11-28 05:28:01.939510: Epoch time: 241.88 s
2024-11-28 05:28:04.452707: 
2024-11-28 05:28:04.453965: Epoch 941
2024-11-28 05:28:04.454840: Current learning rate: 0.00078
2024-11-28 05:31:21.168040: train_loss -0.8268
2024-11-28 05:31:21.168974: val_loss -0.4901
2024-11-28 05:31:21.169707: Pseudo dice [0.7188]
2024-11-28 05:31:21.170385: Epoch time: 196.72 s
2024-11-28 05:31:22.844407: 
2024-11-28 05:31:22.845511: Epoch 942
2024-11-28 05:31:22.846313: Current learning rate: 0.00077
2024-11-28 05:35:11.316343: train_loss -0.8186
2024-11-28 05:35:11.317256: val_loss -0.4941
2024-11-28 05:35:11.318228: Pseudo dice [0.7217]
2024-11-28 05:35:11.318986: Epoch time: 228.47 s
2024-11-28 05:35:12.952600: 
2024-11-28 05:35:12.953896: Epoch 943
2024-11-28 05:35:12.954615: Current learning rate: 0.00076
2024-11-28 05:39:08.934015: train_loss -0.8232
2024-11-28 05:39:08.935024: val_loss -0.5147
2024-11-28 05:39:08.935934: Pseudo dice [0.7264]
2024-11-28 05:39:08.936902: Epoch time: 235.98 s
2024-11-28 05:39:10.486565: 
2024-11-28 05:39:10.488076: Epoch 944
2024-11-28 05:39:10.489002: Current learning rate: 0.00075
2024-11-28 05:42:31.472384: train_loss -0.8245
2024-11-28 05:42:31.473413: val_loss -0.507
2024-11-28 05:42:31.474281: Pseudo dice [0.75]
2024-11-28 05:42:31.475264: Epoch time: 200.99 s
2024-11-28 05:42:33.020495: 
2024-11-28 05:42:33.021816: Epoch 945
2024-11-28 05:42:33.022603: Current learning rate: 0.00074
2024-11-28 05:46:14.730657: train_loss -0.8238
2024-11-28 05:46:14.731660: val_loss -0.5533
2024-11-28 05:46:14.732481: Pseudo dice [0.7391]
2024-11-28 05:46:14.733186: Epoch time: 221.71 s
2024-11-28 05:46:16.347535: 
2024-11-28 05:46:16.348686: Epoch 946
2024-11-28 05:46:16.349488: Current learning rate: 0.00072
2024-11-28 05:50:19.762505: train_loss -0.8282
2024-11-28 05:50:19.763485: val_loss -0.4586
2024-11-28 05:50:19.764273: Pseudo dice [0.7057]
2024-11-28 05:50:19.765042: Epoch time: 243.42 s
2024-11-28 05:50:21.300866: 
2024-11-28 05:50:21.302173: Epoch 947
2024-11-28 05:50:21.303174: Current learning rate: 0.00071
2024-11-28 05:54:50.139277: train_loss -0.8185
2024-11-28 05:54:50.140174: val_loss -0.4864
2024-11-28 05:54:50.140977: Pseudo dice [0.7248]
2024-11-28 05:54:50.141678: Epoch time: 268.84 s
2024-11-28 05:54:51.652897: 
2024-11-28 05:54:51.654005: Epoch 948
2024-11-28 05:54:51.654784: Current learning rate: 0.0007
2024-11-28 05:58:49.541038: train_loss -0.8222
2024-11-28 05:58:49.542558: val_loss -0.4992
2024-11-28 05:58:49.543475: Pseudo dice [0.7279]
2024-11-28 05:58:49.544364: Epoch time: 237.89 s
2024-11-28 05:58:51.165122: 
2024-11-28 05:58:51.166394: Epoch 949
2024-11-28 05:58:51.167150: Current learning rate: 0.00069
2024-11-28 06:02:55.393698: train_loss -0.8214
2024-11-28 06:02:55.394616: val_loss -0.4621
2024-11-28 06:02:55.395419: Pseudo dice [0.6992]
2024-11-28 06:02:55.396340: Epoch time: 244.23 s
2024-11-28 06:02:57.753966: 
2024-11-28 06:02:57.755629: Epoch 950
2024-11-28 06:02:57.756608: Current learning rate: 0.00067
2024-11-28 06:06:43.458621: train_loss -0.8242
2024-11-28 06:06:43.459569: val_loss -0.5032
2024-11-28 06:06:43.460400: Pseudo dice [0.7314]
2024-11-28 06:06:43.461238: Epoch time: 225.71 s
2024-11-28 06:06:45.024166: 
2024-11-28 06:06:45.025510: Epoch 951
2024-11-28 06:06:45.026318: Current learning rate: 0.00066
2024-11-28 06:10:45.892504: train_loss -0.8266
2024-11-28 06:10:45.893255: val_loss -0.5085
2024-11-28 06:10:45.894038: Pseudo dice [0.7214]
2024-11-28 06:10:45.894782: Epoch time: 240.87 s
2024-11-28 06:10:47.415761: 
2024-11-28 06:10:47.417119: Epoch 952
2024-11-28 06:10:47.417863: Current learning rate: 0.00065
2024-11-28 06:14:41.218744: train_loss -0.8188
2024-11-28 06:14:41.219764: val_loss -0.443
2024-11-28 06:14:41.220715: Pseudo dice [0.7002]
2024-11-28 06:14:41.221611: Epoch time: 233.8 s
2024-11-28 06:14:43.189563: 
2024-11-28 06:14:43.191058: Epoch 953
2024-11-28 06:14:43.191993: Current learning rate: 0.00064
2024-11-28 06:18:32.218639: train_loss -0.8248
2024-11-28 06:18:32.219772: val_loss -0.5464
2024-11-28 06:18:32.220629: Pseudo dice [0.7477]
2024-11-28 06:18:32.221427: Epoch time: 229.03 s
2024-11-28 06:18:33.820022: 
2024-11-28 06:18:33.821270: Epoch 954
2024-11-28 06:18:33.822144: Current learning rate: 0.00063
2024-11-28 06:22:28.391177: train_loss -0.8247
2024-11-28 06:22:28.394750: val_loss -0.5111
2024-11-28 06:22:28.395647: Pseudo dice [0.7171]
2024-11-28 06:22:28.397313: Epoch time: 234.57 s
2024-11-28 06:22:30.064661: 
2024-11-28 06:22:30.066028: Epoch 955
2024-11-28 06:22:30.067120: Current learning rate: 0.00061
2024-11-28 06:26:23.174844: train_loss -0.8282
2024-11-28 06:26:23.176411: val_loss -0.4945
2024-11-28 06:26:23.177292: Pseudo dice [0.7232]
2024-11-28 06:26:23.178203: Epoch time: 233.11 s
2024-11-28 06:26:24.774340: 
2024-11-28 06:26:24.775754: Epoch 956
2024-11-28 06:26:24.776796: Current learning rate: 0.0006
2024-11-28 06:30:09.311227: train_loss -0.8278
2024-11-28 06:30:09.312169: val_loss -0.4933
2024-11-28 06:30:09.312937: Pseudo dice [0.721]
2024-11-28 06:30:09.313633: Epoch time: 224.54 s
2024-11-28 06:30:10.858320: 
2024-11-28 06:30:10.859569: Epoch 957
2024-11-28 06:30:10.860380: Current learning rate: 0.00059
2024-11-28 06:33:31.440558: train_loss -0.828
2024-11-28 06:33:31.442708: val_loss -0.5186
2024-11-28 06:33:31.443770: Pseudo dice [0.737]
2024-11-28 06:33:31.444791: Epoch time: 200.58 s
2024-11-28 06:33:33.078853: 
2024-11-28 06:33:33.080512: Epoch 958
2024-11-28 06:33:33.081488: Current learning rate: 0.00058
2024-11-28 06:37:08.248939: train_loss -0.8292
2024-11-28 06:37:08.249774: val_loss -0.5031
2024-11-28 06:37:08.250553: Pseudo dice [0.7372]
2024-11-28 06:37:08.251364: Epoch time: 215.17 s
2024-11-28 06:37:09.985591: 
2024-11-28 06:37:09.987133: Epoch 959
2024-11-28 06:37:09.988190: Current learning rate: 0.00056
2024-11-28 06:40:58.709704: train_loss -0.8238
2024-11-28 06:40:58.710571: val_loss -0.47
2024-11-28 06:40:58.711528: Pseudo dice [0.7129]
2024-11-28 06:40:58.712397: Epoch time: 228.73 s
2024-11-28 06:41:00.278169: 
2024-11-28 06:41:00.279621: Epoch 960
2024-11-28 06:41:00.280398: Current learning rate: 0.00055
2024-11-28 06:44:32.501777: train_loss -0.8265
2024-11-28 06:44:32.502864: val_loss -0.5369
2024-11-28 06:44:32.503898: Pseudo dice [0.7369]
2024-11-28 06:44:32.504861: Epoch time: 212.22 s
2024-11-28 06:44:34.092354: 
2024-11-28 06:44:34.093820: Epoch 961
2024-11-28 06:44:34.094958: Current learning rate: 0.00054
2024-11-28 06:48:02.418592: train_loss -0.8248
2024-11-28 06:48:02.419492: val_loss -0.5017
2024-11-28 06:48:02.420295: Pseudo dice [0.7267]
2024-11-28 06:48:02.420985: Epoch time: 208.33 s
2024-11-28 06:48:04.055964: 
2024-11-28 06:48:04.057278: Epoch 962
2024-11-28 06:48:04.058077: Current learning rate: 0.00053
2024-11-28 06:52:16.350045: train_loss -0.8291
2024-11-28 06:52:16.350700: val_loss -0.4543
2024-11-28 06:52:16.351488: Pseudo dice [0.7118]
2024-11-28 06:52:16.352151: Epoch time: 252.3 s
2024-11-28 06:52:17.936175: 
2024-11-28 06:52:17.937579: Epoch 963
2024-11-28 06:52:17.938497: Current learning rate: 0.00051
2024-11-28 06:56:24.543527: train_loss -0.8227
2024-11-28 06:56:24.544446: val_loss -0.4756
2024-11-28 06:56:24.545275: Pseudo dice [0.6964]
2024-11-28 06:56:24.546046: Epoch time: 246.61 s
2024-11-28 06:56:26.624713: 
2024-11-28 06:56:26.626148: Epoch 964
2024-11-28 06:56:26.627156: Current learning rate: 0.0005
2024-11-28 07:00:09.529288: train_loss -0.8247
2024-11-28 07:00:09.530310: val_loss -0.5191
2024-11-28 07:00:09.531306: Pseudo dice [0.7197]
2024-11-28 07:00:09.532230: Epoch time: 222.91 s
2024-11-28 07:00:11.096412: 
2024-11-28 07:00:11.097954: Epoch 965
2024-11-28 07:00:11.098879: Current learning rate: 0.00049
2024-11-28 07:04:14.431274: train_loss -0.8247
2024-11-28 07:04:14.432202: val_loss -0.4915
2024-11-28 07:04:14.433145: Pseudo dice [0.7431]
2024-11-28 07:04:14.434063: Epoch time: 243.34 s
2024-11-28 07:04:16.010682: 
2024-11-28 07:04:16.012017: Epoch 966
2024-11-28 07:04:16.013016: Current learning rate: 0.00048
2024-11-28 07:08:44.109318: train_loss -0.8259
2024-11-28 07:08:44.110131: val_loss -0.4924
2024-11-28 07:08:44.110975: Pseudo dice [0.7362]
2024-11-28 07:08:44.111881: Epoch time: 268.1 s
2024-11-28 07:08:45.721220: 
2024-11-28 07:08:45.722908: Epoch 967
2024-11-28 07:08:45.724007: Current learning rate: 0.00046
2024-11-28 07:12:33.635791: train_loss -0.8306
2024-11-28 07:12:33.636817: val_loss -0.4746
2024-11-28 07:12:33.637812: Pseudo dice [0.7239]
2024-11-28 07:12:33.638609: Epoch time: 227.92 s
2024-11-28 07:12:35.201144: 
2024-11-28 07:12:35.202518: Epoch 968
2024-11-28 07:12:35.203492: Current learning rate: 0.00045
2024-11-28 07:16:14.863941: train_loss -0.8299
2024-11-28 07:16:14.864866: val_loss -0.5127
2024-11-28 07:16:14.865956: Pseudo dice [0.7396]
2024-11-28 07:16:14.866996: Epoch time: 219.66 s
2024-11-28 07:16:16.468256: 
2024-11-28 07:16:16.469456: Epoch 969
2024-11-28 07:16:16.470519: Current learning rate: 0.00044
2024-11-28 07:20:04.877609: train_loss -0.8324
2024-11-28 07:20:04.878601: val_loss -0.5297
2024-11-28 07:20:04.879515: Pseudo dice [0.7383]
2024-11-28 07:20:04.880315: Epoch time: 228.41 s
2024-11-28 07:20:06.495219: 
2024-11-28 07:20:06.496622: Epoch 970
2024-11-28 07:20:06.497363: Current learning rate: 0.00043
2024-11-28 07:24:00.193912: train_loss -0.8309
2024-11-28 07:24:00.195509: val_loss -0.5178
2024-11-28 07:24:00.196521: Pseudo dice [0.7249]
2024-11-28 07:24:00.197672: Epoch time: 233.7 s
2024-11-28 07:24:01.775750: 
2024-11-28 07:24:01.777055: Epoch 971
2024-11-28 07:24:01.777866: Current learning rate: 0.00041
2024-11-28 07:27:33.142477: train_loss -0.8328
2024-11-28 07:27:33.144467: val_loss -0.4567
2024-11-28 07:27:33.145259: Pseudo dice [0.7225]
2024-11-28 07:27:33.146515: Epoch time: 211.37 s
2024-11-28 07:27:34.733953: 
2024-11-28 07:27:34.735264: Epoch 972
2024-11-28 07:27:34.736011: Current learning rate: 0.0004
2024-11-28 07:31:39.221247: train_loss -0.8311
2024-11-28 07:31:39.223171: val_loss -0.5557
2024-11-28 07:31:39.224369: Pseudo dice [0.7496]
2024-11-28 07:31:39.225306: Epoch time: 244.49 s
2024-11-28 07:31:40.773554: 
2024-11-28 07:31:40.775085: Epoch 973
2024-11-28 07:31:40.776386: Current learning rate: 0.00039
2024-11-28 07:35:15.367110: train_loss -0.8263
2024-11-28 07:35:15.368248: val_loss -0.509
2024-11-28 07:35:15.369045: Pseudo dice [0.7273]
2024-11-28 07:35:15.369916: Epoch time: 214.59 s
2024-11-28 07:35:17.009038: 
2024-11-28 07:35:17.010382: Epoch 974
2024-11-28 07:35:17.011300: Current learning rate: 0.00037
2024-11-28 07:39:08.394066: train_loss -0.8285
2024-11-28 07:39:08.396600: val_loss -0.546
2024-11-28 07:39:08.397518: Pseudo dice [0.7333]
2024-11-28 07:39:08.398455: Epoch time: 231.39 s
2024-11-28 07:39:09.985665: 
2024-11-28 07:39:09.987181: Epoch 975
2024-11-28 07:39:09.988294: Current learning rate: 0.00036
2024-11-28 07:42:59.603330: train_loss -0.8296
2024-11-28 07:42:59.604326: val_loss -0.5075
2024-11-28 07:42:59.605072: Pseudo dice [0.7321]
2024-11-28 07:42:59.605734: Epoch time: 229.62 s
2024-11-28 07:43:01.537119: 
2024-11-28 07:43:01.538368: Epoch 976
2024-11-28 07:43:01.539059: Current learning rate: 0.00035
2024-11-28 07:47:23.089977: train_loss -0.8292
2024-11-28 07:47:23.091013: val_loss -0.5007
2024-11-28 07:47:23.091839: Pseudo dice [0.7258]
2024-11-28 07:47:23.092838: Epoch time: 261.55 s
2024-11-28 07:47:24.690411: 
2024-11-28 07:47:24.691793: Epoch 977
2024-11-28 07:47:24.692529: Current learning rate: 0.00034
2024-11-28 07:51:33.196707: train_loss -0.8374
2024-11-28 07:51:33.197576: val_loss -0.5254
2024-11-28 07:51:33.198264: Pseudo dice [0.7521]
2024-11-28 07:51:33.199095: Epoch time: 248.51 s
2024-11-28 07:51:34.736378: 
2024-11-28 07:51:34.737674: Epoch 978
2024-11-28 07:51:34.738462: Current learning rate: 0.00032
2024-11-28 07:55:43.102690: train_loss -0.822
2024-11-28 07:55:43.103647: val_loss -0.5179
2024-11-28 07:55:43.104511: Pseudo dice [0.7412]
2024-11-28 07:55:43.105446: Epoch time: 248.37 s
2024-11-28 07:55:44.638318: 
2024-11-28 07:55:44.640100: Epoch 979
2024-11-28 07:55:44.640921: Current learning rate: 0.00031
2024-11-28 07:59:29.331799: train_loss -0.8191
2024-11-28 07:59:29.332781: val_loss -0.5121
2024-11-28 07:59:29.333658: Pseudo dice [0.7284]
2024-11-28 07:59:29.334543: Epoch time: 224.69 s
2024-11-28 07:59:30.907045: 
2024-11-28 07:59:30.908403: Epoch 980
2024-11-28 07:59:30.909169: Current learning rate: 0.0003
2024-11-28 08:03:13.436189: train_loss -0.825
2024-11-28 08:03:13.437233: val_loss -0.5009
2024-11-28 08:03:13.437946: Pseudo dice [0.7265]
2024-11-28 08:03:13.438648: Epoch time: 222.53 s
2024-11-28 08:03:14.971529: 
2024-11-28 08:03:14.972883: Epoch 981
2024-11-28 08:03:14.973813: Current learning rate: 0.00028
2024-11-28 08:06:59.135567: train_loss -0.831
2024-11-28 08:06:59.136666: val_loss -0.5163
2024-11-28 08:06:59.137582: Pseudo dice [0.7192]
2024-11-28 08:06:59.138394: Epoch time: 224.17 s
2024-11-28 08:07:00.725916: 
2024-11-28 08:07:00.727246: Epoch 982
2024-11-28 08:07:00.727987: Current learning rate: 0.00027
2024-11-28 08:10:51.118423: train_loss -0.8315
2024-11-28 08:10:51.119290: val_loss -0.4738
2024-11-28 08:10:51.120088: Pseudo dice [0.7234]
2024-11-28 08:10:51.120850: Epoch time: 230.39 s
2024-11-28 08:10:52.761889: 
2024-11-28 08:10:52.763224: Epoch 983
2024-11-28 08:10:52.764001: Current learning rate: 0.00026
2024-11-28 08:14:51.646771: train_loss -0.8293
2024-11-28 08:14:51.647656: val_loss -0.4691
2024-11-28 08:14:51.648550: Pseudo dice [0.7156]
2024-11-28 08:14:51.649316: Epoch time: 238.89 s
2024-11-28 08:14:53.187285: 
2024-11-28 08:14:53.188595: Epoch 984
2024-11-28 08:14:53.189401: Current learning rate: 0.00024
2024-11-28 08:17:53.161977: train_loss -0.8288
2024-11-28 08:17:53.162935: val_loss -0.4753
2024-11-28 08:17:53.163796: Pseudo dice [0.714]
2024-11-28 08:17:53.164677: Epoch time: 179.98 s
2024-11-28 08:17:54.752051: 
2024-11-28 08:17:54.753857: Epoch 985
2024-11-28 08:17:54.754811: Current learning rate: 0.00023
2024-11-28 08:21:42.915658: train_loss -0.8322
2024-11-28 08:21:42.916580: val_loss -0.4703
2024-11-28 08:21:42.917463: Pseudo dice [0.7125]
2024-11-28 08:21:42.918261: Epoch time: 228.17 s
2024-11-28 08:21:44.550837: 
2024-11-28 08:21:44.552059: Epoch 986
2024-11-28 08:21:44.552780: Current learning rate: 0.00021
2024-11-28 08:25:27.355696: train_loss -0.8269
2024-11-28 08:25:27.357586: val_loss -0.4938
2024-11-28 08:25:27.358635: Pseudo dice [0.7155]
2024-11-28 08:25:27.359760: Epoch time: 222.81 s
2024-11-28 08:25:29.481065: 
2024-11-28 08:25:29.482607: Epoch 987
2024-11-28 08:25:29.483574: Current learning rate: 0.0002
2024-11-28 08:28:56.333916: train_loss -0.8327
2024-11-28 08:28:56.335240: val_loss -0.4829
2024-11-28 08:28:56.336027: Pseudo dice [0.7313]
2024-11-28 08:28:56.336709: Epoch time: 206.86 s
2024-11-28 08:28:57.915690: 
2024-11-28 08:28:57.917041: Epoch 988
2024-11-28 08:28:57.917841: Current learning rate: 0.00019
2024-11-28 08:31:56.247454: train_loss -0.8291
2024-11-28 08:31:56.249465: val_loss -0.4733
2024-11-28 08:31:56.250378: Pseudo dice [0.7149]
2024-11-28 08:31:56.252223: Epoch time: 178.33 s
2024-11-28 08:31:57.854569: 
2024-11-28 08:31:57.856119: Epoch 989
2024-11-28 08:31:57.857115: Current learning rate: 0.00017
2024-11-28 08:35:37.671661: train_loss -0.8291
2024-11-28 08:35:37.673018: val_loss -0.4859
2024-11-28 08:35:37.673862: Pseudo dice [0.7157]
2024-11-28 08:35:37.674594: Epoch time: 219.82 s
2024-11-28 08:35:39.241701: 
2024-11-28 08:35:39.243017: Epoch 990
2024-11-28 08:35:39.243799: Current learning rate: 0.00016
2024-11-28 08:39:33.862047: train_loss -0.8306
2024-11-28 08:39:33.863091: val_loss -0.5077
2024-11-28 08:39:33.863876: Pseudo dice [0.736]
2024-11-28 08:39:33.864511: Epoch time: 234.62 s
2024-11-28 08:39:35.374782: 
2024-11-28 08:39:35.376460: Epoch 991
2024-11-28 08:39:35.377630: Current learning rate: 0.00014
2024-11-28 08:43:40.305606: train_loss -0.832
2024-11-28 08:43:40.306926: val_loss -0.4544
2024-11-28 08:43:40.308172: Pseudo dice [0.715]
2024-11-28 08:43:40.309378: Epoch time: 244.93 s
2024-11-28 08:43:41.931417: 
2024-11-28 08:43:41.932924: Epoch 992
2024-11-28 08:43:41.933785: Current learning rate: 0.00013
2024-11-28 08:47:03.261621: train_loss -0.8329
2024-11-28 08:47:03.263838: val_loss -0.5147
2024-11-28 08:47:03.264592: Pseudo dice [0.7229]
2024-11-28 08:47:03.265367: Epoch time: 201.33 s
2024-11-28 08:47:04.802796: 
2024-11-28 08:47:04.804128: Epoch 993
2024-11-28 08:47:04.804809: Current learning rate: 0.00011
2024-11-28 08:51:04.189924: train_loss -0.8335
2024-11-28 08:51:04.191127: val_loss -0.4805
2024-11-28 08:51:04.191932: Pseudo dice [0.7202]
2024-11-28 08:51:04.192696: Epoch time: 239.39 s
2024-11-28 08:51:05.755232: 
2024-11-28 08:51:05.756520: Epoch 994
2024-11-28 08:51:05.757383: Current learning rate: 0.0001
2024-11-28 08:55:10.860724: train_loss -0.8326
2024-11-28 08:55:10.861745: val_loss -0.4754
2024-11-28 08:55:10.862509: Pseudo dice [0.7113]
2024-11-28 08:55:10.863204: Epoch time: 245.11 s
2024-11-28 08:55:12.372204: 
2024-11-28 08:55:12.373601: Epoch 995
2024-11-28 08:55:12.374326: Current learning rate: 8e-05
2024-11-28 08:59:46.349738: train_loss -0.8292
2024-11-28 08:59:46.350618: val_loss -0.4816
2024-11-28 08:59:46.351390: Pseudo dice [0.6992]
2024-11-28 08:59:46.352120: Epoch time: 273.98 s
2024-11-28 08:59:47.898256: 
2024-11-28 08:59:47.899586: Epoch 996
2024-11-28 08:59:47.900300: Current learning rate: 7e-05
2024-11-28 09:03:44.449122: train_loss -0.8297
2024-11-28 09:03:44.450042: val_loss -0.499
2024-11-28 09:03:44.451005: Pseudo dice [0.7074]
2024-11-28 09:03:44.451689: Epoch time: 236.55 s
2024-11-28 09:03:46.003324: 
2024-11-28 09:03:46.004453: Epoch 997
2024-11-28 09:03:46.005223: Current learning rate: 5e-05
2024-11-28 09:07:46.689913: train_loss -0.8285
2024-11-28 09:07:46.691019: val_loss -0.4997
2024-11-28 09:07:46.691929: Pseudo dice [0.7204]
2024-11-28 09:07:46.692861: Epoch time: 240.69 s
2024-11-28 09:07:48.243911: 
2024-11-28 09:07:48.245767: Epoch 998
2024-11-28 09:07:48.246984: Current learning rate: 4e-05
2024-11-28 09:11:44.641346: train_loss -0.8314
2024-11-28 09:11:44.642481: val_loss -0.5081
2024-11-28 09:11:44.643332: Pseudo dice [0.7271]
2024-11-28 09:11:44.644104: Epoch time: 236.4 s
2024-11-28 09:11:46.857996: 
2024-11-28 09:11:46.859369: Epoch 999
2024-11-28 09:11:46.860276: Current learning rate: 2e-05
2024-11-28 09:15:51.349494: train_loss -0.8283
2024-11-28 09:15:51.350591: val_loss -0.481
2024-11-28 09:15:51.351616: Pseudo dice [0.7159]
2024-11-28 09:15:51.352700: Epoch time: 244.49 s
2024-11-28 09:15:53.391494: Training done.
2024-11-28 09:15:53.630445: Using splits from existing split file: /home/gridsan/nchutisilp/datasets/nnUNet_Datasets/nnUNet_preprocessed/Dataset306_Sohee_Ajay_Calcium_OCT/splits_final.json
2024-11-28 09:15:53.632854: The split file contains 5 splits.
2024-11-28 09:15:53.633666: Desired fold for training: 3
2024-11-28 09:15:53.634326: This split has 11 training and 2 validation cases.
2024-11-28 09:15:53.635129: predicting 03009Pre
2024-11-28 09:15:53.700517: 03009Pre, shape torch.Size([1, 400, 498, 498]), rank 0
2024-11-28 09:18:08.689215: predicting 101-044
2024-11-28 09:18:08.705908: 101-044, shape torch.Size([1, 404, 498, 498]), rank 0
2024-11-28 09:20:43.192195: Validation complete
2024-11-28 09:20:43.192960: Mean Validation Dice:  0.7184797447612405
