/home/gridsan/nchutisilp/.conda/envs/nnunet/bin/python

############################
INFO: You are using the old nnU-Net default plans. We have updated our recommendations. Please consider using those instead! Read more here: https://github.com/MIC-DKFZ/nnUNet/blob/master/documentation/resenc_presets.md
############################

Using device: cuda:0

#######################################################################
Please cite the following paper when using nnU-Net:
Isensee, F., Jaeger, P. F., Kohl, S. A., Petersen, J., & Maier-Hein, K. H. (2021). nnU-Net: a self-configuring method for deep learning-based biomedical image segmentation. Nature methods, 18(2), 203-211.
#######################################################################

2024-11-29 17:29:26.565870: Using torch.compile...

############################
INFO: You are using the old nnU-Net default plans. We have updated our recommendations. Please consider using those instead! Read more here: https://github.com/MIC-DKFZ/nnUNet/blob/master/documentation/resenc_presets.md
############################

Using device: cuda:0

#######################################################################
Please cite the following paper when using nnU-Net:
Isensee, F., Jaeger, P. F., Kohl, S. A., Petersen, J., & Maier-Hein, K. H. (2021). nnU-Net: a self-configuring method for deep learning-based biomedical image segmentation. Nature methods, 18(2), 203-211.
#######################################################################

2024-11-29 17:29:26.566113: Using torch.compile...
/home/gridsan/nchutisilp/.local/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn("The verbose parameter is deprecated. Please use get_last_lr() "
/home/gridsan/nchutisilp/.local/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn("The verbose parameter is deprecated. Please use get_last_lr() "
################### Loading pretrained weights from file  /home/gridsan/nchutisilp/projects/OpenAI-CLIP/clip_pretrained_nnUNet.pt ###################
Below is the list of overlapping blocks in pretrained model and nnUNet architecture:
encoder.stages.0.0.convs.0.conv.weight shape torch.Size([32, 1, 3, 3, 3])
encoder.stages.0.0.convs.0.conv.bias shape torch.Size([32])
encoder.stages.0.0.convs.0.norm.weight shape torch.Size([32])
encoder.stages.0.0.convs.0.norm.bias shape torch.Size([32])
encoder.stages.0.0.convs.0.all_modules.0.weight shape torch.Size([32, 1, 3, 3, 3])
encoder.stages.0.0.convs.0.all_modules.0.bias shape torch.Size([32])
encoder.stages.0.0.convs.0.all_modules.1.weight shape torch.Size([32])
encoder.stages.0.0.convs.0.all_modules.1.bias shape torch.Size([32])
encoder.stages.0.0.convs.1.conv.weight shape torch.Size([32, 32, 3, 3, 3])
encoder.stages.0.0.convs.1.conv.bias shape torch.Size([32])
encoder.stages.0.0.convs.1.norm.weight shape torch.Size([32])
encoder.stages.0.0.convs.1.norm.bias shape torch.Size([32])
encoder.stages.0.0.convs.1.all_modules.0.weight shape torch.Size([32, 32, 3, 3, 3])
encoder.stages.0.0.convs.1.all_modules.0.bias shape torch.Size([32])
encoder.stages.0.0.convs.1.all_modules.1.weight shape torch.Size([32])
encoder.stages.0.0.convs.1.all_modules.1.bias shape torch.Size([32])
encoder.stages.1.0.convs.0.conv.weight shape torch.Size([64, 32, 3, 3, 3])
encoder.stages.1.0.convs.0.conv.bias shape torch.Size([64])
encoder.stages.1.0.convs.0.norm.weight shape torch.Size([64])
encoder.stages.1.0.convs.0.norm.bias shape torch.Size([64])
encoder.stages.1.0.convs.0.all_modules.0.weight shape torch.Size([64, 32, 3, 3, 3])
encoder.stages.1.0.convs.0.all_modules.0.bias shape torch.Size([64])
encoder.stages.1.0.convs.0.all_modules.1.weight shape torch.Size([64])
encoder.stages.1.0.convs.0.all_modules.1.bias shape torch.Size([64])
encoder.stages.1.0.convs.1.conv.weight shape torch.Size([64, 64, 3, 3, 3])
encoder.stages.1.0.convs.1.conv.bias shape torch.Size([64])
encoder.stages.1.0.convs.1.norm.weight shape torch.Size([64])
encoder.stages.1.0.convs.1.norm.bias shape torch.Size([64])
encoder.stages.1.0.convs.1.all_modules.0.weight shape torch.Size([64, 64, 3, 3, 3])
encoder.stages.1.0.convs.1.all_modules.0.bias shape torch.Size([64])
encoder.stages.1.0.convs.1.all_modules.1.weight shape torch.Size([64])
encoder.stages.1.0.convs.1.all_modules.1.bias shape torch.Size([64])
encoder.stages.2.0.convs.0.conv.weight shape torch.Size([128, 64, 3, 3, 3])
encoder.stages.2.0.convs.0.conv.bias shape torch.Size([128])
encoder.stages.2.0.convs.0.norm.weight shape torch.Size([128])
encoder.stages.2.0.convs.0.norm.bias shape torch.Size([128])
encoder.stages.2.0.convs.0.all_modules.0.weight shape torch.Size([128, 64, 3, 3, 3])
encoder.stages.2.0.convs.0.all_modules.0.bias shape torch.Size([128])
encoder.stages.2.0.convs.0.all_modules.1.weight shape torch.Size([128])
encoder.stages.2.0.convs.0.all_modules.1.bias shape torch.Size([128])
encoder.stages.2.0.convs.1.conv.weight shape torch.Size([128, 128, 3, 3, 3])
encoder.stages.2.0.convs.1.conv.bias shape torch.Size([128])
encoder.stages.2.0.convs.1.norm.weight shape torch.Size([128])
encoder.stages.2.0.convs.1.norm.bias shape torch.Size([128])
encoder.stages.2.0.convs.1.all_modules.0.weight shape torch.Size([128, 128, 3, 3, 3])
encoder.stages.2.0.convs.1.all_modules.0.bias shape torch.Size([128])
encoder.stages.2.0.convs.1.all_modules.1.weight shape torch.Size([128])
encoder.stages.2.0.convs.1.all_modules.1.bias shape torch.Size([128])
encoder.stages.3.0.convs.0.conv.weight shape torch.Size([256, 128, 3, 3, 3])
encoder.stages.3.0.convs.0.conv.bias shape torch.Size([256])
encoder.stages.3.0.convs.0.norm.weight shape torch.Size([256])
encoder.stages.3.0.convs.0.norm.bias shape torch.Size([256])
encoder.stages.3.0.convs.0.all_modules.0.weight shape torch.Size([256, 128, 3, 3, 3])
encoder.stages.3.0.convs.0.all_modules.0.bias shape torch.Size([256])
encoder.stages.3.0.convs.0.all_modules.1.weight shape torch.Size([256])
encoder.stages.3.0.convs.0.all_modules.1.bias shape torch.Size([256])
encoder.stages.3.0.convs.1.conv.weight shape torch.Size([256, 256, 3, 3, 3])
encoder.stages.3.0.convs.1.conv.bias shape torch.Size([256])
encoder.stages.3.0.convs.1.norm.weight shape torch.Size([256])
encoder.stages.3.0.convs.1.norm.bias shape torch.Size([256])
encoder.stages.3.0.convs.1.all_modules.0.weight shape torch.Size([256, 256, 3, 3, 3])
encoder.stages.3.0.convs.1.all_modules.0.bias shape torch.Size([256])
encoder.stages.3.0.convs.1.all_modules.1.weight shape torch.Size([256])
encoder.stages.3.0.convs.1.all_modules.1.bias shape torch.Size([256])
encoder.stages.4.0.convs.0.conv.weight shape torch.Size([320, 256, 3, 3, 3])
encoder.stages.4.0.convs.0.conv.bias shape torch.Size([320])
encoder.stages.4.0.convs.0.norm.weight shape torch.Size([320])
encoder.stages.4.0.convs.0.norm.bias shape torch.Size([320])
encoder.stages.4.0.convs.0.all_modules.0.weight shape torch.Size([320, 256, 3, 3, 3])
encoder.stages.4.0.convs.0.all_modules.0.bias shape torch.Size([320])
encoder.stages.4.0.convs.0.all_modules.1.weight shape torch.Size([320])
encoder.stages.4.0.convs.0.all_modules.1.bias shape torch.Size([320])
encoder.stages.4.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.4.0.convs.1.conv.bias shape torch.Size([320])
encoder.stages.4.0.convs.1.norm.weight shape torch.Size([320])
encoder.stages.4.0.convs.1.norm.bias shape torch.Size([320])
encoder.stages.4.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.4.0.convs.1.all_modules.0.bias shape torch.Size([320])
encoder.stages.4.0.convs.1.all_modules.1.weight shape torch.Size([320])
encoder.stages.4.0.convs.1.all_modules.1.bias shape torch.Size([320])
encoder.stages.5.0.convs.0.conv.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.5.0.convs.0.conv.bias shape torch.Size([320])
encoder.stages.5.0.convs.0.norm.weight shape torch.Size([320])
encoder.stages.5.0.convs.0.norm.bias shape torch.Size([320])
encoder.stages.5.0.convs.0.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.5.0.convs.0.all_modules.0.bias shape torch.Size([320])
encoder.stages.5.0.convs.0.all_modules.1.weight shape torch.Size([320])
encoder.stages.5.0.convs.0.all_modules.1.bias shape torch.Size([320])
encoder.stages.5.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.5.0.convs.1.conv.bias shape torch.Size([320])
encoder.stages.5.0.convs.1.norm.weight shape torch.Size([320])
encoder.stages.5.0.convs.1.norm.bias shape torch.Size([320])
encoder.stages.5.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.5.0.convs.1.all_modules.0.bias shape torch.Size([320])
encoder.stages.5.0.convs.1.all_modules.1.weight shape torch.Size([320])
encoder.stages.5.0.convs.1.all_modules.1.bias shape torch.Size([320])
decoder.encoder.stages.0.0.convs.0.conv.weight shape torch.Size([32, 1, 3, 3, 3])
decoder.encoder.stages.0.0.convs.0.conv.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.norm.weight shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.norm.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.all_modules.0.weight shape torch.Size([32, 1, 3, 3, 3])
decoder.encoder.stages.0.0.convs.0.all_modules.0.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.all_modules.1.weight shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.all_modules.1.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.conv.weight shape torch.Size([32, 32, 3, 3, 3])
decoder.encoder.stages.0.0.convs.1.conv.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.norm.weight shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.norm.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.all_modules.0.weight shape torch.Size([32, 32, 3, 3, 3])
decoder.encoder.stages.0.0.convs.1.all_modules.0.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.all_modules.1.weight shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.all_modules.1.bias shape torch.Size([32])
decoder.encoder.stages.1.0.convs.0.conv.weight shape torch.Size([64, 32, 3, 3, 3])
decoder.encoder.stages.1.0.convs.0.conv.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.norm.weight shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.norm.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.all_modules.0.weight shape torch.Size([64, 32, 3, 3, 3])
decoder.encoder.stages.1.0.convs.0.all_modules.0.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.all_modules.1.weight shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.all_modules.1.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.conv.weight shape torch.Size([64, 64, 3, 3, 3])
decoder.encoder.stages.1.0.convs.1.conv.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.norm.weight shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.norm.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.all_modules.0.weight shape torch.Size([64, 64, 3, 3, 3])
decoder.encoder.stages.1.0.convs.1.all_modules.0.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.all_modules.1.weight shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.all_modules.1.bias shape torch.Size([64])
decoder.encoder.stages.2.0.convs.0.conv.weight shape torch.Size([128, 64, 3, 3, 3])
decoder.encoder.stages.2.0.convs.0.conv.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.norm.weight shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.norm.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.all_modules.0.weight shape torch.Size([128, 64, 3, 3, 3])
decoder.encoder.stages.2.0.convs.0.all_modules.0.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.all_modules.1.weight shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.all_modules.1.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.conv.weight shape torch.Size([128, 128, 3, 3, 3])
decoder.encoder.stages.2.0.convs.1.conv.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.norm.weight shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.norm.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.all_modules.0.weight shape torch.Size([128, 128, 3, 3, 3])
decoder.encoder.stages.2.0.convs.1.all_modules.0.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.all_modules.1.weight shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.all_modules.1.bias shape torch.Size([128])
decoder.encoder.stages.3.0.convs.0.conv.weight shape torch.Size([256, 128, 3, 3, 3])
decoder.encoder.stages.3.0.convs.0.conv.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.norm.weight shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.norm.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.all_modules.0.weight shape torch.Size([256, 128, 3, 3, 3])
decoder.encoder.stages.3.0.convs.0.all_modules.0.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.all_modules.1.weight shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.all_modules.1.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.conv.weight shape torch.Size([256, 256, 3, 3, 3])
decoder.encoder.stages.3.0.convs.1.conv.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.norm.weight shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.norm.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.all_modules.0.weight shape torch.Size([256, 256, 3, 3, 3])
decoder.encoder.stages.3.0.convs.1.all_modules.0.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.all_modules.1.weight shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.all_modules.1.bias shape torch.Size([256])
decoder.encoder.stages.4.0.convs.0.conv.weight shape torch.Size([320, 256, 3, 3, 3])
decoder.encoder.stages.4.0.convs.0.conv.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.norm.weight shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.norm.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.all_modules.0.weight shape torch.Size([320, 256, 3, 3, 3])
decoder.encoder.stages.4.0.convs.0.all_modules.0.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.all_modules.1.weight shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.all_modules.1.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.4.0.convs.1.conv.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.norm.weight shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.norm.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.4.0.convs.1.all_modules.0.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.all_modules.1.weight shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.all_modules.1.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.conv.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.5.0.convs.0.conv.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.norm.weight shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.norm.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.5.0.convs.0.all_modules.0.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.all_modules.1.weight shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.all_modules.1.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.5.0.convs.1.conv.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.norm.weight shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.norm.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.5.0.convs.1.all_modules.0.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.all_modules.1.weight shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.all_modules.1.bias shape torch.Size([320])
decoder.stages.0.convs.0.conv.weight shape torch.Size([320, 640, 3, 3, 3])
decoder.stages.0.convs.0.conv.bias shape torch.Size([320])
decoder.stages.0.convs.0.norm.weight shape torch.Size([320])
decoder.stages.0.convs.0.norm.bias shape torch.Size([320])
decoder.stages.0.convs.0.all_modules.0.weight shape torch.Size([320, 640, 3, 3, 3])
decoder.stages.0.convs.0.all_modules.0.bias shape torch.Size([320])
decoder.stages.0.convs.0.all_modules.1.weight shape torch.Size([320])
decoder.stages.0.convs.0.all_modules.1.bias shape torch.Size([320])
decoder.stages.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.stages.0.convs.1.conv.bias shape torch.Size([320])
decoder.stages.0.convs.1.norm.weight shape torch.Size([320])
decoder.stages.0.convs.1.norm.bias shape torch.Size([320])
decoder.stages.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.stages.0.convs.1.all_modules.0.bias shape torch.Size([320])
decoder.stages.0.convs.1.all_modules.1.weight shape torch.Size([320])
decoder.stages.0.convs.1.all_modules.1.bias shape torch.Size([320])
decoder.stages.1.convs.0.conv.weight shape torch.Size([256, 512, 3, 3, 3])
decoder.stages.1.convs.0.conv.bias shape torch.Size([256])
decoder.stages.1.convs.0.norm.weight shape torch.Size([256])
decoder.stages.1.convs.0.norm.bias shape torch.Size([256])
decoder.stages.1.convs.0.all_modules.0.weight shape torch.Size([256, 512, 3, 3, 3])
decoder.stages.1.convs.0.all_modules.0.bias shape torch.Size([256])
decoder.stages.1.convs.0.all_modules.1.weight shape torch.Size([256])
decoder.stages.1.convs.0.all_modules.1.bias shape torch.Size([256])
decoder.stages.1.convs.1.conv.weight shape torch.Size([256, 256, 3, 3, 3])
decoder.stages.1.convs.1.conv.bias shape torch.Size([256])
decoder.stages.1.convs.1.norm.weight shape torch.Size([256])
decoder.stages.1.convs.1.norm.bias shape torch.Size([256])
decoder.stages.1.convs.1.all_modules.0.weight shape torch.Size([256, 256, 3, 3, 3])
decoder.stages.1.convs.1.all_modules.0.bias shape torch.Size([256])
decoder.stages.1.convs.1.all_modules.1.weight shape torch.Size([256])
decoder.stages.1.convs.1.all_modules.1.bias shape torch.Size([256])
decoder.stages.2.convs.0.conv.weight shape torch.Size([128, 256, 3, 3, 3])
decoder.stages.2.convs.0.conv.bias shape torch.Size([128])
decoder.stages.2.convs.0.norm.weight shape torch.Size([128])
decoder.stages.2.convs.0.norm.bias shape torch.Size([128])
decoder.stages.2.convs.0.all_modules.0.weight shape torch.Size([128, 256, 3, 3, 3])
decoder.stages.2.convs.0.all_modules.0.bias shape torch.Size([128])
decoder.stages.2.convs.0.all_modules.1.weight shape torch.Size([128])
decoder.stages.2.convs.0.all_modules.1.bias shape torch.Size([128])
decoder.stages.2.convs.1.conv.weight shape torch.Size([128, 128, 3, 3, 3])
decoder.stages.2.convs.1.conv.bias shape torch.Size([128])
decoder.stages.2.convs.1.norm.weight shape torch.Size([128])
decoder.stages.2.convs.1.norm.bias shape torch.Size([128])
decoder.stages.2.convs.1.all_modules.0.weight shape torch.Size([128, 128, 3, 3, 3])
decoder.stages.2.convs.1.all_modules.0.bias shape torch.Size([128])
decoder.stages.2.convs.1.all_modules.1.weight shape torch.Size([128])
decoder.stages.2.convs.1.all_modules.1.bias shape torch.Size([128])
decoder.stages.3.convs.0.conv.weight shape torch.Size([64, 128, 3, 3, 3])
decoder.stages.3.convs.0.conv.bias shape torch.Size([64])
decoder.stages.3.convs.0.norm.weight shape torch.Size([64])
decoder.stages.3.convs.0.norm.bias shape torch.Size([64])
decoder.stages.3.convs.0.all_modules.0.weight shape torch.Size([64, 128, 3, 3, 3])
decoder.stages.3.convs.0.all_modules.0.bias shape torch.Size([64])
decoder.stages.3.convs.0.all_modules.1.weight shape torch.Size([64])
decoder.stages.3.convs.0.all_modules.1.bias shape torch.Size([64])
decoder.stages.3.convs.1.conv.weight shape torch.Size([64, 64, 3, 3, 3])
decoder.stages.3.convs.1.conv.bias shape torch.Size([64])
decoder.stages.3.convs.1.norm.weight shape torch.Size([64])
decoder.stages.3.convs.1.norm.bias shape torch.Size([64])
decoder.stages.3.convs.1.all_modules.0.weight shape torch.Size([64, 64, 3, 3, 3])
decoder.stages.3.convs.1.all_modules.0.bias shape torch.Size([64])
decoder.stages.3.convs.1.all_modules.1.weight shape torch.Size([64])
decoder.stages.3.convs.1.all_modules.1.bias shape torch.Size([64])
decoder.stages.4.convs.0.conv.weight shape torch.Size([32, 64, 3, 3, 3])
decoder.stages.4.convs.0.conv.bias shape torch.Size([32])
decoder.stages.4.convs.0.norm.weight shape torch.Size([32])
decoder.stages.4.convs.0.norm.bias shape torch.Size([32])
decoder.stages.4.convs.0.all_modules.0.weight shape torch.Size([32, 64, 3, 3, 3])
decoder.stages.4.convs.0.all_modules.0.bias shape torch.Size([32])
decoder.stages.4.convs.0.all_modules.1.weight shape torch.Size([32])
decoder.stages.4.convs.0.all_modules.1.bias shape torch.Size([32])
decoder.stages.4.convs.1.conv.weight shape torch.Size([32, 32, 3, 3, 3])
decoder.stages.4.convs.1.conv.bias shape torch.Size([32])
decoder.stages.4.convs.1.norm.weight shape torch.Size([32])
decoder.stages.4.convs.1.norm.bias shape torch.Size([32])
decoder.stages.4.convs.1.all_modules.0.weight shape torch.Size([32, 32, 3, 3, 3])
decoder.stages.4.convs.1.all_modules.0.bias shape torch.Size([32])
decoder.stages.4.convs.1.all_modules.1.weight shape torch.Size([32])
decoder.stages.4.convs.1.all_modules.1.bias shape torch.Size([32])
decoder.transpconvs.0.weight shape torch.Size([320, 320, 1, 2, 2])
decoder.transpconvs.0.bias shape torch.Size([320])
decoder.transpconvs.1.weight shape torch.Size([320, 256, 2, 2, 2])
decoder.transpconvs.1.bias shape torch.Size([256])
decoder.transpconvs.2.weight shape torch.Size([256, 128, 2, 2, 2])
decoder.transpconvs.2.bias shape torch.Size([128])
decoder.transpconvs.3.weight shape torch.Size([128, 64, 2, 2, 2])
decoder.transpconvs.3.bias shape torch.Size([64])
decoder.transpconvs.4.weight shape torch.Size([64, 32, 2, 2, 2])
decoder.transpconvs.4.bias shape torch.Size([32])
################### Done ###################
2024-11-29 17:29:51.133342: do_dummy_2d_data_aug: True
2024-11-29 17:29:51.175307: Using splits from existing split file: /home/gridsan/nchutisilp/datasets/nnUNet_Datasets/nnUNet_preprocessed/Dataset306_Sohee_Ajay_Calcium_OCT/splits_final.json
2024-11-29 17:29:51.192673: The split file contains 5 splits.
2024-11-29 17:29:51.194775: Desired fold for training: 0
2024-11-29 17:29:51.195689: This split has 10 training and 3 validation cases.
################### Loading pretrained weights from file  /home/gridsan/nchutisilp/projects/OpenAI-CLIP/clip_pretrained_nnUNet.pt ###################
Below is the list of overlapping blocks in pretrained model and nnUNet architecture:
encoder.stages.0.0.convs.0.conv.weight shape torch.Size([32, 1, 3, 3, 3])
encoder.stages.0.0.convs.0.conv.bias shape torch.Size([32])
encoder.stages.0.0.convs.0.norm.weight shape torch.Size([32])
encoder.stages.0.0.convs.0.norm.bias shape torch.Size([32])
encoder.stages.0.0.convs.0.all_modules.0.weight shape torch.Size([32, 1, 3, 3, 3])
encoder.stages.0.0.convs.0.all_modules.0.bias shape torch.Size([32])
encoder.stages.0.0.convs.0.all_modules.1.weight shape torch.Size([32])
encoder.stages.0.0.convs.0.all_modules.1.bias shape torch.Size([32])
encoder.stages.0.0.convs.1.conv.weight shape torch.Size([32, 32, 3, 3, 3])
encoder.stages.0.0.convs.1.conv.bias shape torch.Size([32])
encoder.stages.0.0.convs.1.norm.weight shape torch.Size([32])
encoder.stages.0.0.convs.1.norm.bias shape torch.Size([32])
encoder.stages.0.0.convs.1.all_modules.0.weight shape torch.Size([32, 32, 3, 3, 3])
encoder.stages.0.0.convs.1.all_modules.0.bias shape torch.Size([32])
encoder.stages.0.0.convs.1.all_modules.1.weight shape torch.Size([32])
encoder.stages.0.0.convs.1.all_modules.1.bias shape torch.Size([32])
encoder.stages.1.0.convs.0.conv.weight shape torch.Size([64, 32, 3, 3, 3])
encoder.stages.1.0.convs.0.conv.bias shape torch.Size([64])
encoder.stages.1.0.convs.0.norm.weight shape torch.Size([64])
encoder.stages.1.0.convs.0.norm.bias shape torch.Size([64])
encoder.stages.1.0.convs.0.all_modules.0.weight shape torch.Size([64, 32, 3, 3, 3])
encoder.stages.1.0.convs.0.all_modules.0.bias shape torch.Size([64])
encoder.stages.1.0.convs.0.all_modules.1.weight shape torch.Size([64])
encoder.stages.1.0.convs.0.all_modules.1.bias shape torch.Size([64])
encoder.stages.1.0.convs.1.conv.weight shape torch.Size([64, 64, 3, 3, 3])
encoder.stages.1.0.convs.1.conv.bias shape torch.Size([64])
encoder.stages.1.0.convs.1.norm.weight shape torch.Size([64])
encoder.stages.1.0.convs.1.norm.bias shape torch.Size([64])
encoder.stages.1.0.convs.1.all_modules.0.weight shape torch.Size([64, 64, 3, 3, 3])
encoder.stages.1.0.convs.1.all_modules.0.bias shape torch.Size([64])
encoder.stages.1.0.convs.1.all_modules.1.weight shape torch.Size([64])
encoder.stages.1.0.convs.1.all_modules.1.bias shape torch.Size([64])
encoder.stages.2.0.convs.0.conv.weight shape torch.Size([128, 64, 3, 3, 3])
encoder.stages.2.0.convs.0.conv.bias shape torch.Size([128])
encoder.stages.2.0.convs.0.norm.weight shape torch.Size([128])
encoder.stages.2.0.convs.0.norm.bias shape torch.Size([128])
encoder.stages.2.0.convs.0.all_modules.0.weight shape torch.Size([128, 64, 3, 3, 3])
encoder.stages.2.0.convs.0.all_modules.0.bias shape torch.Size([128])
encoder.stages.2.0.convs.0.all_modules.1.weight shape torch.Size([128])
encoder.stages.2.0.convs.0.all_modules.1.bias shape torch.Size([128])
encoder.stages.2.0.convs.1.conv.weight shape torch.Size([128, 128, 3, 3, 3])
encoder.stages.2.0.convs.1.conv.bias shape torch.Size([128])
encoder.stages.2.0.convs.1.norm.weight shape torch.Size([128])
encoder.stages.2.0.convs.1.norm.bias shape torch.Size([128])
encoder.stages.2.0.convs.1.all_modules.0.weight shape torch.Size([128, 128, 3, 3, 3])
encoder.stages.2.0.convs.1.all_modules.0.bias shape torch.Size([128])
encoder.stages.2.0.convs.1.all_modules.1.weight shape torch.Size([128])
encoder.stages.2.0.convs.1.all_modules.1.bias shape torch.Size([128])
encoder.stages.3.0.convs.0.conv.weight shape torch.Size([256, 128, 3, 3, 3])
encoder.stages.3.0.convs.0.conv.bias shape torch.Size([256])
encoder.stages.3.0.convs.0.norm.weight shape torch.Size([256])
encoder.stages.3.0.convs.0.norm.bias shape torch.Size([256])
encoder.stages.3.0.convs.0.all_modules.0.weight shape torch.Size([256, 128, 3, 3, 3])
encoder.stages.3.0.convs.0.all_modules.0.bias shape torch.Size([256])
encoder.stages.3.0.convs.0.all_modules.1.weight shape torch.Size([256])
encoder.stages.3.0.convs.0.all_modules.1.bias shape torch.Size([256])
encoder.stages.3.0.convs.1.conv.weight shape torch.Size([256, 256, 3, 3, 3])
encoder.stages.3.0.convs.1.conv.bias shape torch.Size([256])
encoder.stages.3.0.convs.1.norm.weight shape torch.Size([256])
encoder.stages.3.0.convs.1.norm.bias shape torch.Size([256])
encoder.stages.3.0.convs.1.all_modules.0.weight shape torch.Size([256, 256, 3, 3, 3])
encoder.stages.3.0.convs.1.all_modules.0.bias shape torch.Size([256])
encoder.stages.3.0.convs.1.all_modules.1.weight shape torch.Size([256])
encoder.stages.3.0.convs.1.all_modules.1.bias shape torch.Size([256])
encoder.stages.4.0.convs.0.conv.weight shape torch.Size([320, 256, 3, 3, 3])
encoder.stages.4.0.convs.0.conv.bias shape torch.Size([320])
encoder.stages.4.0.convs.0.norm.weight shape torch.Size([320])
encoder.stages.4.0.convs.0.norm.bias shape torch.Size([320])
encoder.stages.4.0.convs.0.all_modules.0.weight shape torch.Size([320, 256, 3, 3, 3])
encoder.stages.4.0.convs.0.all_modules.0.bias shape torch.Size([320])
encoder.stages.4.0.convs.0.all_modules.1.weight shape torch.Size([320])
encoder.stages.4.0.convs.0.all_modules.1.bias shape torch.Size([320])
encoder.stages.4.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.4.0.convs.1.conv.bias shape torch.Size([320])
encoder.stages.4.0.convs.1.norm.weight shape torch.Size([320])
encoder.stages.4.0.convs.1.norm.bias shape torch.Size([320])
encoder.stages.4.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.4.0.convs.1.all_modules.0.bias shape torch.Size([320])
encoder.stages.4.0.convs.1.all_modules.1.weight shape torch.Size([320])
encoder.stages.4.0.convs.1.all_modules.1.bias shape torch.Size([320])
encoder.stages.5.0.convs.0.conv.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.5.0.convs.0.conv.bias shape torch.Size([320])
encoder.stages.5.0.convs.0.norm.weight shape torch.Size([320])
encoder.stages.5.0.convs.0.norm.bias shape torch.Size([320])
encoder.stages.5.0.convs.0.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.5.0.convs.0.all_modules.0.bias shape torch.Size([320])
encoder.stages.5.0.convs.0.all_modules.1.weight shape torch.Size([320])
encoder.stages.5.0.convs.0.all_modules.1.bias shape torch.Size([320])
encoder.stages.5.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.5.0.convs.1.conv.bias shape torch.Size([320])
encoder.stages.5.0.convs.1.norm.weight shape torch.Size([320])
encoder.stages.5.0.convs.1.norm.bias shape torch.Size([320])
encoder.stages.5.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.5.0.convs.1.all_modules.0.bias shape torch.Size([320])
encoder.stages.5.0.convs.1.all_modules.1.weight shape torch.Size([320])
encoder.stages.5.0.convs.1.all_modules.1.bias shape torch.Size([320])
decoder.encoder.stages.0.0.convs.0.conv.weight shape torch.Size([32, 1, 3, 3, 3])
decoder.encoder.stages.0.0.convs.0.conv.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.norm.weight shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.norm.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.all_modules.0.weight shape torch.Size([32, 1, 3, 3, 3])
decoder.encoder.stages.0.0.convs.0.all_modules.0.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.all_modules.1.weight shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.all_modules.1.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.conv.weight shape torch.Size([32, 32, 3, 3, 3])
decoder.encoder.stages.0.0.convs.1.conv.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.norm.weight shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.norm.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.all_modules.0.weight shape torch.Size([32, 32, 3, 3, 3])
decoder.encoder.stages.0.0.convs.1.all_modules.0.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.all_modules.1.weight shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.all_modules.1.bias shape torch.Size([32])
decoder.encoder.stages.1.0.convs.0.conv.weight shape torch.Size([64, 32, 3, 3, 3])
decoder.encoder.stages.1.0.convs.0.conv.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.norm.weight shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.norm.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.all_modules.0.weight shape torch.Size([64, 32, 3, 3, 3])
decoder.encoder.stages.1.0.convs.0.all_modules.0.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.all_modules.1.weight shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.all_modules.1.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.conv.weight shape torch.Size([64, 64, 3, 3, 3])
decoder.encoder.stages.1.0.convs.1.conv.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.norm.weight shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.norm.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.all_modules.0.weight shape torch.Size([64, 64, 3, 3, 3])
decoder.encoder.stages.1.0.convs.1.all_modules.0.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.all_modules.1.weight shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.all_modules.1.bias shape torch.Size([64])
decoder.encoder.stages.2.0.convs.0.conv.weight shape torch.Size([128, 64, 3, 3, 3])
decoder.encoder.stages.2.0.convs.0.conv.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.norm.weight shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.norm.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.all_modules.0.weight shape torch.Size([128, 64, 3, 3, 3])
decoder.encoder.stages.2.0.convs.0.all_modules.0.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.all_modules.1.weight shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.all_modules.1.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.conv.weight shape torch.Size([128, 128, 3, 3, 3])
decoder.encoder.stages.2.0.convs.1.conv.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.norm.weight shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.norm.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.all_modules.0.weight shape torch.Size([128, 128, 3, 3, 3])
decoder.encoder.stages.2.0.convs.1.all_modules.0.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.all_modules.1.weight shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.all_modules.1.bias shape torch.Size([128])
decoder.encoder.stages.3.0.convs.0.conv.weight shape torch.Size([256, 128, 3, 3, 3])
decoder.encoder.stages.3.0.convs.0.conv.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.norm.weight shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.norm.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.all_modules.0.weight shape torch.Size([256, 128, 3, 3, 3])
decoder.encoder.stages.3.0.convs.0.all_modules.0.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.all_modules.1.weight shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.all_modules.1.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.conv.weight shape torch.Size([256, 256, 3, 3, 3])
decoder.encoder.stages.3.0.convs.1.conv.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.norm.weight shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.norm.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.all_modules.0.weight shape torch.Size([256, 256, 3, 3, 3])
decoder.encoder.stages.3.0.convs.1.all_modules.0.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.all_modules.1.weight shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.all_modules.1.bias shape torch.Size([256])
decoder.encoder.stages.4.0.convs.0.conv.weight shape torch.Size([320, 256, 3, 3, 3])
decoder.encoder.stages.4.0.convs.0.conv.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.norm.weight shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.norm.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.all_modules.0.weight shape torch.Size([320, 256, 3, 3, 3])
decoder.encoder.stages.4.0.convs.0.all_modules.0.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.all_modules.1.weight shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.all_modules.1.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.4.0.convs.1.conv.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.norm.weight shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.norm.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.4.0.convs.1.all_modules.0.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.all_modules.1.weight shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.all_modules.1.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.conv.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.5.0.convs.0.conv.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.norm.weight shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.norm.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.5.0.convs.0.all_modules.0.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.all_modules.1.weight shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.all_modules.1.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.5.0.convs.1.conv.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.norm.weight shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.norm.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.5.0.convs.1.all_modules.0.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.all_modules.1.weight shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.all_modules.1.bias shape torch.Size([320])
decoder.stages.0.convs.0.conv.weight shape torch.Size([320, 640, 3, 3, 3])
decoder.stages.0.convs.0.conv.bias shape torch.Size([320])
decoder.stages.0.convs.0.norm.weight shape torch.Size([320])
decoder.stages.0.convs.0.norm.bias shape torch.Size([320])
decoder.stages.0.convs.0.all_modules.0.weight shape torch.Size([320, 640, 3, 3, 3])
decoder.stages.0.convs.0.all_modules.0.bias shape torch.Size([320])
decoder.stages.0.convs.0.all_modules.1.weight shape torch.Size([320])
decoder.stages.0.convs.0.all_modules.1.bias shape torch.Size([320])
decoder.stages.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.stages.0.convs.1.conv.bias shape torch.Size([320])
decoder.stages.0.convs.1.norm.weight shape torch.Size([320])
decoder.stages.0.convs.1.norm.bias shape torch.Size([320])
decoder.stages.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.stages.0.convs.1.all_modules.0.bias shape torch.Size([320])
decoder.stages.0.convs.1.all_modules.1.weight shape torch.Size([320])
decoder.stages.0.convs.1.all_modules.1.bias shape torch.Size([320])
decoder.stages.1.convs.0.conv.weight shape torch.Size([256, 512, 3, 3, 3])
decoder.stages.1.convs.0.conv.bias shape torch.Size([256])
decoder.stages.1.convs.0.norm.weight shape torch.Size([256])
decoder.stages.1.convs.0.norm.bias shape torch.Size([256])
decoder.stages.1.convs.0.all_modules.0.weight shape torch.Size([256, 512, 3, 3, 3])
decoder.stages.1.convs.0.all_modules.0.bias shape torch.Size([256])
decoder.stages.1.convs.0.all_modules.1.weight shape torch.Size([256])
decoder.stages.1.convs.0.all_modules.1.bias shape torch.Size([256])
decoder.stages.1.convs.1.conv.weight shape torch.Size([256, 256, 3, 3, 3])
decoder.stages.1.convs.1.conv.bias shape torch.Size([256])
decoder.stages.1.convs.1.norm.weight shape torch.Size([256])
decoder.stages.1.convs.1.norm.bias shape torch.Size([256])
decoder.stages.1.convs.1.all_modules.0.weight shape torch.Size([256, 256, 3, 3, 3])
decoder.stages.1.convs.1.all_modules.0.bias shape torch.Size([256])
decoder.stages.1.convs.1.all_modules.1.weight shape torch.Size([256])
decoder.stages.1.convs.1.all_modules.1.bias shape torch.Size([256])
decoder.stages.2.convs.0.conv.weight shape torch.Size([128, 256, 3, 3, 3])
decoder.stages.2.convs.0.conv.bias shape torch.Size([128])
decoder.stages.2.convs.0.norm.weight shape torch.Size([128])
decoder.stages.2.convs.0.norm.bias shape torch.Size([128])
decoder.stages.2.convs.0.all_modules.0.weight shape torch.Size([128, 256, 3, 3, 3])
decoder.stages.2.convs.0.all_modules.0.bias shape torch.Size([128])
decoder.stages.2.convs.0.all_modules.1.weight shape torch.Size([128])
decoder.stages.2.convs.0.all_modules.1.bias shape torch.Size([128])
decoder.stages.2.convs.1.conv.weight shape torch.Size([128, 128, 3, 3, 3])
decoder.stages.2.convs.1.conv.bias shape torch.Size([128])
decoder.stages.2.convs.1.norm.weight shape torch.Size([128])
decoder.stages.2.convs.1.norm.bias shape torch.Size([128])
decoder.stages.2.convs.1.all_modules.0.weight shape torch.Size([128, 128, 3, 3, 3])
decoder.stages.2.convs.1.all_modules.0.bias shape torch.Size([128])
decoder.stages.2.convs.1.all_modules.1.weight shape torch.Size([128])
decoder.stages.2.convs.1.all_modules.1.bias shape torch.Size([128])
decoder.stages.3.convs.0.conv.weight shape torch.Size([64, 128, 3, 3, 3])
decoder.stages.3.convs.0.conv.bias shape torch.Size([64])
decoder.stages.3.convs.0.norm.weight shape torch.Size([64])
decoder.stages.3.convs.0.norm.bias shape torch.Size([64])
decoder.stages.3.convs.0.all_modules.0.weight shape torch.Size([64, 128, 3, 3, 3])
decoder.stages.3.convs.0.all_modules.0.bias shape torch.Size([64])
decoder.stages.3.convs.0.all_modules.1.weight shape torch.Size([64])
decoder.stages.3.convs.0.all_modules.1.bias shape torch.Size([64])
decoder.stages.3.convs.1.conv.weight shape torch.Size([64, 64, 3, 3, 3])
decoder.stages.3.convs.1.conv.bias shape torch.Size([64])
decoder.stages.3.convs.1.norm.weight shape torch.Size([64])
decoder.stages.3.convs.1.norm.bias shape torch.Size([64])
decoder.stages.3.convs.1.all_modules.0.weight shape torch.Size([64, 64, 3, 3, 3])
decoder.stages.3.convs.1.all_modules.0.bias shape torch.Size([64])
decoder.stages.3.convs.1.all_modules.1.weight shape torch.Size([64])
decoder.stages.3.convs.1.all_modules.1.bias shape torch.Size([64])
decoder.stages.4.convs.0.conv.weight shape torch.Size([32, 64, 3, 3, 3])
decoder.stages.4.convs.0.conv.bias shape torch.Size([32])
decoder.stages.4.convs.0.norm.weight shape torch.Size([32])
decoder.stages.4.convs.0.norm.bias shape torch.Size([32])
decoder.stages.4.convs.0.all_modules.0.weight shape torch.Size([32, 64, 3, 3, 3])
decoder.stages.4.convs.0.all_modules.0.bias shape torch.Size([32])
decoder.stages.4.convs.0.all_modules.1.weight shape torch.Size([32])
decoder.stages.4.convs.0.all_modules.1.bias shape torch.Size([32])
decoder.stages.4.convs.1.conv.weight shape torch.Size([32, 32, 3, 3, 3])
decoder.stages.4.convs.1.conv.bias shape torch.Size([32])
decoder.stages.4.convs.1.norm.weight shape torch.Size([32])
decoder.stages.4.convs.1.norm.bias shape torch.Size([32])
decoder.stages.4.convs.1.all_modules.0.weight shape torch.Size([32, 32, 3, 3, 3])
decoder.stages.4.convs.1.all_modules.0.bias shape torch.Size([32])
decoder.stages.4.convs.1.all_modules.1.weight shape torch.Size([32])
decoder.stages.4.convs.1.all_modules.1.bias shape torch.Size([32])
decoder.transpconvs.0.weight shape torch.Size([320, 320, 1, 2, 2])
decoder.transpconvs.0.bias shape torch.Size([320])
decoder.transpconvs.1.weight shape torch.Size([320, 256, 2, 2, 2])
decoder.transpconvs.1.bias shape torch.Size([256])
decoder.transpconvs.2.weight shape torch.Size([256, 128, 2, 2, 2])
decoder.transpconvs.2.bias shape torch.Size([128])
decoder.transpconvs.3.weight shape torch.Size([128, 64, 2, 2, 2])
decoder.transpconvs.3.bias shape torch.Size([64])
decoder.transpconvs.4.weight shape torch.Size([64, 32, 2, 2, 2])
decoder.transpconvs.4.bias shape torch.Size([32])
################### Done ###################
2024-11-29 17:29:51.133336: do_dummy_2d_data_aug: True
2024-11-29 17:29:51.175284: Using splits from existing split file: /home/gridsan/nchutisilp/datasets/nnUNet_Datasets/nnUNet_preprocessed/Dataset306_Sohee_Ajay_Calcium_OCT/splits_final.json
2024-11-29 17:29:51.193268: The split file contains 5 splits.
2024-11-29 17:29:51.194862: Desired fold for training: 1
2024-11-29 17:29:51.195822: This split has 10 training and 3 validation cases.
using pin_memory on device 0
using pin_memory on device 0

This is the configuration used by this training:
Configuration name: 3d_32x160x128_b10
 {'data_identifier': 'nnUNetPlans_3d_fullres', 'preprocessor_name': 'DefaultPreprocessor', 'batch_size': 10, 'patch_size': [32, 160, 128], 'median_image_size_in_voxels': [375.0, 498.0, 498.0], 'spacing': [1.0, 1.0, 1.0], 'normalization_schemes': ['ZScoreNormalization'], 'use_mask_for_norm': [False], 'resampling_fn_data': 'resample_data_or_seg_to_shape', 'resampling_fn_seg': 'resample_data_or_seg_to_shape', 'resampling_fn_data_kwargs': {'is_seg': False, 'order': 3, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_seg_kwargs': {'is_seg': True, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_probabilities': 'resample_data_or_seg_to_shape', 'resampling_fn_probabilities_kwargs': {'is_seg': False, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'architecture': {'network_class_name': 'dynamic_network_architectures.architectures.unet.PlainConvUNet', 'arch_kwargs': {'n_stages': 6, 'features_per_stage': [32, 64, 128, 256, 320, 320], 'conv_op': 'torch.nn.modules.conv.Conv3d', 'kernel_sizes': [[3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3]], 'strides': [[1, 1, 1], [2, 2, 2], [2, 2, 2], [2, 2, 2], [2, 2, 2], [1, 2, 2]], 'n_conv_per_stage': [2, 2, 2, 2, 2, 2], 'n_conv_per_stage_decoder': [2, 2, 2, 2, 2], 'conv_bias': True, 'norm_op': 'torch.nn.modules.instancenorm.InstanceNorm3d', 'norm_op_kwargs': {'eps': 1e-05, 'affine': True}, 'dropout_op': None, 'dropout_op_kwargs': None, 'nonlin': 'torch.nn.LeakyReLU', 'nonlin_kwargs': {'inplace': True}, 'deep_supervision': True}, '_kw_requires_import': ['conv_op', 'norm_op', 'dropout_op', 'nonlin']}, 'batch_dice': True, 'inherits_from': '3d_fullres'} 

These are the global plan.json settings:
 {'dataset_name': 'Dataset306_Sohee_Ajay_Calcium_OCT', 'plans_name': 'nnUNetPlans', 'original_median_spacing_after_transp': [1.0, 1.0, 1.0], 'original_median_shape_after_transp': [375, 498, 498], 'image_reader_writer': 'SimpleITKIO', 'transpose_forward': [0, 1, 2], 'transpose_backward': [0, 1, 2], 'experiment_planner_used': 'ExperimentPlanner', 'label_manager': 'LabelManager', 'foreground_intensity_properties_per_channel': {'0': {'max': 0.49803921580314636, 'mean': 0.14296366274356842, 'median': 0.10980392247438431, 'min': 0.0, 'percentile_00_5': 0.01558823511004448, 'percentile_99_5': 0.49803921580314636, 'std': 0.11562220752239227}}} 

2024-11-29 17:30:02.223727: unpacking dataset...
using pin_memory on device 0
using pin_memory on device 0

This is the configuration used by this training:
Configuration name: 3d_32x160x128_b10
 {'data_identifier': 'nnUNetPlans_3d_fullres', 'preprocessor_name': 'DefaultPreprocessor', 'batch_size': 10, 'patch_size': [32, 160, 128], 'median_image_size_in_voxels': [375.0, 498.0, 498.0], 'spacing': [1.0, 1.0, 1.0], 'normalization_schemes': ['ZScoreNormalization'], 'use_mask_for_norm': [False], 'resampling_fn_data': 'resample_data_or_seg_to_shape', 'resampling_fn_seg': 'resample_data_or_seg_to_shape', 'resampling_fn_data_kwargs': {'is_seg': False, 'order': 3, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_seg_kwargs': {'is_seg': True, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_probabilities': 'resample_data_or_seg_to_shape', 'resampling_fn_probabilities_kwargs': {'is_seg': False, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'architecture': {'network_class_name': 'dynamic_network_architectures.architectures.unet.PlainConvUNet', 'arch_kwargs': {'n_stages': 6, 'features_per_stage': [32, 64, 128, 256, 320, 320], 'conv_op': 'torch.nn.modules.conv.Conv3d', 'kernel_sizes': [[3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3]], 'strides': [[1, 1, 1], [2, 2, 2], [2, 2, 2], [2, 2, 2], [2, 2, 2], [1, 2, 2]], 'n_conv_per_stage': [2, 2, 2, 2, 2, 2], 'n_conv_per_stage_decoder': [2, 2, 2, 2, 2], 'conv_bias': True, 'norm_op': 'torch.nn.modules.instancenorm.InstanceNorm3d', 'norm_op_kwargs': {'eps': 1e-05, 'affine': True}, 'dropout_op': None, 'dropout_op_kwargs': None, 'nonlin': 'torch.nn.LeakyReLU', 'nonlin_kwargs': {'inplace': True}, 'deep_supervision': True}, '_kw_requires_import': ['conv_op', 'norm_op', 'dropout_op', 'nonlin']}, 'batch_dice': True, 'inherits_from': '3d_fullres'} 

These are the global plan.json settings:
 {'dataset_name': 'Dataset306_Sohee_Ajay_Calcium_OCT', 'plans_name': 'nnUNetPlans', 'original_median_spacing_after_transp': [1.0, 1.0, 1.0], 'original_median_shape_after_transp': [375, 498, 498], 'image_reader_writer': 'SimpleITKIO', 'transpose_forward': [0, 1, 2], 'transpose_backward': [0, 1, 2], 'experiment_planner_used': 'ExperimentPlanner', 'label_manager': 'LabelManager', 'foreground_intensity_properties_per_channel': {'0': {'max': 0.49803921580314636, 'mean': 0.14296366274356842, 'median': 0.10980392247438431, 'min': 0.0, 'percentile_00_5': 0.01558823511004448, 'percentile_99_5': 0.49803921580314636, 'std': 0.11562220752239227}}} 

2024-11-29 17:30:03.164117: unpacking dataset...
2024-11-29 17:30:07.901848: unpacking done...
2024-11-29 17:30:07.999045: Unable to plot network architecture: nnUNet_compile is enabled!
2024-11-29 17:30:08.154115: 
2024-11-29 17:30:08.155374: Epoch 0
2024-11-29 17:30:08.156374: Current learning rate: 0.01
2024-11-29 17:33:29.135018: Validation loss improved from 1000.00000 to -0.20015! Patience: 0/50
2024-11-29 17:33:29.136678: train_loss -0.0419
2024-11-29 17:33:29.137910: val_loss -0.2001
2024-11-29 17:33:29.138683: Pseudo dice [0.5113]
2024-11-29 17:33:29.139435: Epoch time: 200.98 s
2024-11-29 17:33:29.140151: Yayy! New best EMA pseudo Dice: 0.5113
2024-11-29 17:33:31.547535: 
2024-11-29 17:33:31.549238: Epoch 1
2024-11-29 17:33:31.549996: Current learning rate: 0.00999
2024-11-29 17:34:57.631794: Validation loss improved from -0.20015 to -0.22334! Patience: 0/50
2024-11-29 17:34:57.632982: train_loss -0.2004
2024-11-29 17:34:57.633828: val_loss -0.2233
2024-11-29 17:34:57.634474: Pseudo dice [0.5481]
2024-11-29 17:34:57.635184: Epoch time: 86.09 s
2024-11-29 17:34:57.635953: Yayy! New best EMA pseudo Dice: 0.515
2024-11-29 17:34:59.210874: 
2024-11-29 17:34:59.212405: Epoch 2
2024-11-29 17:34:59.213049: Current learning rate: 0.00998
2024-11-29 17:36:25.246109: Validation loss improved from -0.22334 to -0.27682! Patience: 0/50
2024-11-29 17:36:25.247128: train_loss -0.246
2024-11-29 17:36:25.248086: val_loss -0.2768
2024-11-29 17:36:25.248981: Pseudo dice [0.5791]
2024-11-29 17:36:25.249751: Epoch time: 86.04 s
2024-11-29 17:36:25.250509: Yayy! New best EMA pseudo Dice: 0.5214
2024-11-29 17:36:26.885209: 
2024-11-29 17:36:26.886799: Epoch 3
2024-11-29 17:36:26.887914: Current learning rate: 0.00997
2024-11-29 17:37:52.918280: Validation loss improved from -0.27682 to -0.29739! Patience: 0/50
2024-11-29 17:37:52.919454: train_loss -0.2913
2024-11-29 17:37:52.920325: val_loss -0.2974
2024-11-29 17:37:52.921120: Pseudo dice [0.5899]
2024-11-29 17:37:52.921831: Epoch time: 86.04 s
2024-11-29 17:37:52.922587: Yayy! New best EMA pseudo Dice: 0.5283
2024-11-29 17:37:54.535430: 
2024-11-29 17:37:54.537230: Epoch 4
2024-11-29 17:37:54.538252: Current learning rate: 0.00996
2024-11-29 17:39:20.546327: Validation loss improved from -0.29739 to -0.33602! Patience: 0/50
2024-11-29 17:39:20.547556: train_loss -0.3127
2024-11-29 17:39:20.548609: val_loss -0.336
2024-11-29 17:39:20.549611: Pseudo dice [0.6285]
2024-11-29 17:39:20.551707: Epoch time: 86.01 s
2024-11-29 17:39:20.862770: Yayy! New best EMA pseudo Dice: 0.5383
2024-11-29 17:39:22.497597: 
2024-11-29 17:39:22.499554: Epoch 5
2024-11-29 17:39:22.500752: Current learning rate: 0.00995
2024-11-29 17:40:48.572309: Validation loss improved from -0.33602 to -0.40381! Patience: 0/50
2024-11-29 17:40:48.573661: train_loss -0.3574
2024-11-29 17:40:48.574620: val_loss -0.4038
2024-11-29 17:40:48.575308: Pseudo dice [0.6767]
2024-11-29 17:40:48.576076: Epoch time: 86.08 s
2024-11-29 17:40:48.576730: Yayy! New best EMA pseudo Dice: 0.5521
2024-11-29 17:40:50.083403: 
2024-11-29 17:40:50.085227: Epoch 6
2024-11-29 17:40:50.086107: Current learning rate: 0.00995
2024-11-29 17:42:16.213079: Validation loss did not improve from -0.40381. Patience: 1/50
2024-11-29 17:42:16.215659: train_loss -0.3593
2024-11-29 17:42:16.216962: val_loss -0.349
2024-11-29 17:42:16.217982: Pseudo dice [0.6314]
2024-11-29 17:42:16.218826: Epoch time: 86.13 s
2024-11-29 17:42:16.219616: Yayy! New best EMA pseudo Dice: 0.5601
2024-11-29 17:42:17.800965: 
2024-11-29 17:42:17.803049: Epoch 7
2024-11-29 17:42:17.803893: Current learning rate: 0.00994
2024-11-29 17:43:43.813730: Validation loss did not improve from -0.40381. Patience: 2/50
2024-11-29 17:43:43.814851: train_loss -0.3856
2024-11-29 17:43:43.815854: val_loss -0.3891
2024-11-29 17:43:43.816775: Pseudo dice [0.6694]
2024-11-29 17:43:43.817711: Epoch time: 86.02 s
2024-11-29 17:43:43.818627: Yayy! New best EMA pseudo Dice: 0.571
2024-11-29 17:43:45.402508: 
2024-11-29 17:43:45.404131: Epoch 8
2024-11-29 17:43:45.405211: Current learning rate: 0.00993
2024-11-29 17:45:11.538494: Validation loss did not improve from -0.40381. Patience: 3/50
2024-11-29 17:45:11.539401: train_loss -0.3733
2024-11-29 17:45:11.540406: val_loss -0.389
2024-11-29 17:45:11.541064: Pseudo dice [0.6669]
2024-11-29 17:45:11.541749: Epoch time: 86.14 s
2024-11-29 17:45:11.542480: Yayy! New best EMA pseudo Dice: 0.5806
2024-11-29 17:45:13.504772: 
2024-11-29 17:45:13.506726: Epoch 9
2024-11-29 17:45:13.507626: Current learning rate: 0.00992
2024-11-29 17:46:39.585880: Validation loss improved from -0.40381 to -0.44639! Patience: 3/50
2024-11-29 17:46:39.587068: train_loss -0.4228
2024-11-29 17:46:39.588200: val_loss -0.4464
2024-11-29 17:46:39.589112: Pseudo dice [0.6993]
2024-11-29 17:46:39.590033: Epoch time: 86.08 s
2024-11-29 17:46:39.899012: Yayy! New best EMA pseudo Dice: 0.5925
2024-11-29 17:46:41.488685: 
2024-11-29 17:46:41.490530: Epoch 10
2024-11-29 17:46:41.491703: Current learning rate: 0.00991
2024-11-29 17:48:07.668740: Validation loss did not improve from -0.44639. Patience: 1/50
2024-11-29 17:48:07.670505: train_loss -0.4203
2024-11-29 17:48:07.671604: val_loss -0.4115
2024-11-29 17:48:07.672369: Pseudo dice [0.6897]
2024-11-29 17:48:07.673123: Epoch time: 86.18 s
2024-11-29 17:48:07.673861: Yayy! New best EMA pseudo Dice: 0.6022
2024-11-29 17:48:09.308810: 
2024-11-29 17:48:09.310532: Epoch 11
2024-11-29 17:48:09.311337: Current learning rate: 0.0099
2024-11-29 17:49:35.297840: Validation loss improved from -0.44639 to -0.45424! Patience: 1/50
2024-11-29 17:49:35.299635: train_loss -0.4495
2024-11-29 17:49:35.300864: val_loss -0.4542
2024-11-29 17:49:35.301645: Pseudo dice [0.7052]
2024-11-29 17:49:35.302854: Epoch time: 85.99 s
2024-11-29 17:49:35.303911: Yayy! New best EMA pseudo Dice: 0.6125
2024-11-29 17:49:36.899556: 
2024-11-29 17:49:36.901232: Epoch 12
2024-11-29 17:49:36.902172: Current learning rate: 0.00989
2024-11-29 17:51:02.993569: Validation loss did not improve from -0.45424. Patience: 1/50
2024-11-29 17:51:02.994568: train_loss -0.4511
2024-11-29 17:51:02.995371: val_loss -0.4156
2024-11-29 17:51:02.996066: Pseudo dice [0.6734]
2024-11-29 17:51:02.996659: Epoch time: 86.1 s
2024-11-29 17:51:02.997489: Yayy! New best EMA pseudo Dice: 0.6186
2024-11-29 17:51:04.646699: 
2024-11-29 17:51:04.648743: Epoch 13
2024-11-29 17:51:04.649441: Current learning rate: 0.00988
2024-11-29 17:52:30.581197: Validation loss did not improve from -0.45424. Patience: 2/50
2024-11-29 17:52:30.582326: train_loss -0.451
2024-11-29 17:52:30.583078: val_loss -0.4061
2024-11-29 17:52:30.583877: Pseudo dice [0.6874]
2024-11-29 17:52:30.584565: Epoch time: 85.94 s
2024-11-29 17:52:30.585273: Yayy! New best EMA pseudo Dice: 0.6255
2024-11-29 17:52:32.147206: 
2024-11-29 17:52:32.148980: Epoch 14
2024-11-29 17:52:32.149881: Current learning rate: 0.00987
2024-11-29 17:53:58.107288: Validation loss improved from -0.45424 to -0.47114! Patience: 2/50
2024-11-29 17:53:58.109481: train_loss -0.4629
2024-11-29 17:53:58.111077: val_loss -0.4711
2024-11-29 17:53:58.111791: Pseudo dice [0.7103]
2024-11-29 17:53:58.112617: Epoch time: 85.96 s
2024-11-29 17:53:58.456665: Yayy! New best EMA pseudo Dice: 0.6339
2024-11-29 17:54:00.064729: 
2024-11-29 17:54:00.066434: Epoch 15
2024-11-29 17:54:00.067331: Current learning rate: 0.00986
2024-11-29 17:55:26.060481: Validation loss did not improve from -0.47114. Patience: 1/50
2024-11-29 17:55:26.061329: train_loss -0.4627
2024-11-29 17:55:26.062476: val_loss -0.427
2024-11-29 17:55:26.063267: Pseudo dice [0.6974]
2024-11-29 17:55:26.064085: Epoch time: 86.0 s
2024-11-29 17:55:26.064962: Yayy! New best EMA pseudo Dice: 0.6403
2024-11-29 17:55:27.663248: 
2024-11-29 17:55:27.665344: Epoch 16
2024-11-29 17:55:27.666217: Current learning rate: 0.00986
2024-11-29 17:56:53.785496: Validation loss did not improve from -0.47114. Patience: 2/50
2024-11-29 17:56:53.786421: train_loss -0.4754
2024-11-29 17:56:53.787293: val_loss -0.431
2024-11-29 17:56:53.788057: Pseudo dice [0.6919]
2024-11-29 17:56:53.788834: Epoch time: 86.12 s
2024-11-29 17:56:53.789669: Yayy! New best EMA pseudo Dice: 0.6454
2024-11-29 17:56:55.436222: 
2024-11-29 17:56:55.437918: Epoch 17
2024-11-29 17:56:55.438839: Current learning rate: 0.00985
2024-11-29 17:58:21.371221: Validation loss did not improve from -0.47114. Patience: 3/50
2024-11-29 17:58:21.372168: train_loss -0.4723
2024-11-29 17:58:21.373122: val_loss -0.4467
2024-11-29 17:58:21.373829: Pseudo dice [0.6975]
2024-11-29 17:58:21.374540: Epoch time: 85.94 s
2024-11-29 17:58:21.375144: Yayy! New best EMA pseudo Dice: 0.6507
2024-11-29 17:58:23.003560: 
2024-11-29 17:58:23.006030: Epoch 18
2024-11-29 17:58:23.006923: Current learning rate: 0.00984
2024-11-29 17:59:49.011397: Validation loss did not improve from -0.47114. Patience: 4/50
2024-11-29 17:59:49.012599: train_loss -0.5019
2024-11-29 17:59:49.013454: val_loss -0.4268
2024-11-29 17:59:49.014357: Pseudo dice [0.6863]
2024-11-29 17:59:49.015101: Epoch time: 86.01 s
2024-11-29 17:59:49.015911: Yayy! New best EMA pseudo Dice: 0.6542
2024-11-29 17:59:51.075913: 
2024-11-29 17:59:51.076994: Epoch 19
2024-11-29 17:59:51.077828: Current learning rate: 0.00983
2024-11-29 18:01:17.155132: Validation loss did not improve from -0.47114. Patience: 5/50
2024-11-29 18:01:17.156198: train_loss -0.4849
2024-11-29 18:01:17.157008: val_loss -0.4382
2024-11-29 18:01:17.157797: Pseudo dice [0.6975]
2024-11-29 18:01:17.158608: Epoch time: 86.08 s
2024-11-29 18:01:17.554823: Yayy! New best EMA pseudo Dice: 0.6585
2024-11-29 18:01:19.226141: 
2024-11-29 18:01:19.227681: Epoch 20
2024-11-29 18:01:19.228965: Current learning rate: 0.00982
2024-11-29 18:02:45.219476: Validation loss did not improve from -0.47114. Patience: 6/50
2024-11-29 18:02:45.220546: train_loss -0.5032
2024-11-29 18:02:45.221379: val_loss -0.4552
2024-11-29 18:02:45.222139: Pseudo dice [0.6954]
2024-11-29 18:02:45.222797: Epoch time: 86.0 s
2024-11-29 18:02:45.223506: Yayy! New best EMA pseudo Dice: 0.6622
2024-11-29 18:02:46.901366: 
2024-11-29 18:02:46.902812: Epoch 21
2024-11-29 18:02:46.903578: Current learning rate: 0.00981
2024-11-29 18:04:12.913807: Validation loss did not improve from -0.47114. Patience: 7/50
2024-11-29 18:04:12.915080: train_loss -0.5031
2024-11-29 18:04:12.915890: val_loss -0.3503
2024-11-29 18:04:12.916657: Pseudo dice [0.6392]
2024-11-29 18:04:12.917277: Epoch time: 86.01 s
2024-11-29 18:04:14.199340: 
2024-11-29 18:04:14.201218: Epoch 22
2024-11-29 18:04:14.202034: Current learning rate: 0.0098
2024-11-29 18:05:40.227578: Validation loss did not improve from -0.47114. Patience: 8/50
2024-11-29 18:05:40.228607: train_loss -0.5023
2024-11-29 18:05:40.229717: val_loss -0.4533
2024-11-29 18:05:40.230687: Pseudo dice [0.6897]
2024-11-29 18:05:40.231371: Epoch time: 86.03 s
2024-11-29 18:05:40.232137: Yayy! New best EMA pseudo Dice: 0.6629
2024-11-29 18:05:41.803763: 
2024-11-29 18:05:41.804920: Epoch 23
2024-11-29 18:05:41.805777: Current learning rate: 0.00979
2024-11-29 18:07:07.768835: Validation loss did not improve from -0.47114. Patience: 9/50
2024-11-29 18:07:07.770037: train_loss -0.5077
2024-11-29 18:07:07.771262: val_loss -0.4492
2024-11-29 18:07:07.772207: Pseudo dice [0.7102]
2024-11-29 18:07:07.773448: Epoch time: 85.97 s
2024-11-29 18:07:07.774504: Yayy! New best EMA pseudo Dice: 0.6676
2024-11-29 18:07:09.301650: 
2024-11-29 18:07:09.303549: Epoch 24
2024-11-29 18:07:09.304724: Current learning rate: 0.00978
2024-11-29 18:08:35.214703: Validation loss improved from -0.47114 to -0.48826! Patience: 9/50
2024-11-29 18:08:35.215492: train_loss -0.5133
2024-11-29 18:08:35.216393: val_loss -0.4883
2024-11-29 18:08:35.217323: Pseudo dice [0.7349]
2024-11-29 18:08:35.218093: Epoch time: 85.91 s
2024-11-29 18:08:35.659210: Yayy! New best EMA pseudo Dice: 0.6744
2024-11-29 18:08:37.247290: 
2024-11-29 18:08:37.248952: Epoch 25
2024-11-29 18:08:37.249804: Current learning rate: 0.00977
2024-11-29 18:10:03.268587: Validation loss did not improve from -0.48826. Patience: 1/50
2024-11-29 18:10:03.269565: train_loss -0.5109
2024-11-29 18:10:03.270707: val_loss -0.436
2024-11-29 18:10:03.271383: Pseudo dice [0.7032]
2024-11-29 18:10:03.272109: Epoch time: 86.02 s
2024-11-29 18:10:03.272757: Yayy! New best EMA pseudo Dice: 0.6772
2024-11-29 18:10:04.836729: 
2024-11-29 18:10:04.838300: Epoch 26
2024-11-29 18:10:04.839198: Current learning rate: 0.00977
2024-11-29 18:11:30.810350: Validation loss did not improve from -0.48826. Patience: 2/50
2024-11-29 18:11:30.811024: train_loss -0.5199
2024-11-29 18:11:30.812038: val_loss -0.4625
2024-11-29 18:11:30.812942: Pseudo dice [0.7133]
2024-11-29 18:11:30.813792: Epoch time: 85.98 s
2024-11-29 18:11:30.814426: Yayy! New best EMA pseudo Dice: 0.6809
2024-11-29 18:11:32.349652: 
2024-11-29 18:11:32.351150: Epoch 27
2024-11-29 18:11:32.351916: Current learning rate: 0.00976
2024-11-29 18:12:58.307063: Validation loss did not improve from -0.48826. Patience: 3/50
2024-11-29 18:12:58.307954: train_loss -0.5241
2024-11-29 18:12:58.309014: val_loss -0.3909
2024-11-29 18:12:58.309955: Pseudo dice [0.6696]
2024-11-29 18:12:58.310786: Epoch time: 85.96 s
2024-11-29 18:12:59.548816: 
2024-11-29 18:12:59.550337: Epoch 28
2024-11-29 18:12:59.551287: Current learning rate: 0.00975
2024-11-29 18:14:25.468095: Validation loss did not improve from -0.48826. Patience: 4/50
2024-11-29 18:14:25.469294: train_loss -0.5381
2024-11-29 18:14:25.470209: val_loss -0.4587
2024-11-29 18:14:25.470999: Pseudo dice [0.7097]
2024-11-29 18:14:25.471791: Epoch time: 85.92 s
2024-11-29 18:14:25.472610: Yayy! New best EMA pseudo Dice: 0.6827
2024-11-29 18:14:27.358099: 
2024-11-29 18:14:27.359459: Epoch 29
2024-11-29 18:14:27.360463: Current learning rate: 0.00974
2024-11-29 18:15:53.396291: Validation loss did not improve from -0.48826. Patience: 5/50
2024-11-29 18:15:53.397699: train_loss -0.5286
2024-11-29 18:15:53.398577: val_loss -0.4812
2024-11-29 18:15:53.399396: Pseudo dice [0.728]
2024-11-29 18:15:53.400053: Epoch time: 86.04 s
2024-11-29 18:15:53.777747: Yayy! New best EMA pseudo Dice: 0.6872
2024-11-29 18:15:55.384621: 
2024-11-29 18:15:55.386860: Epoch 30
2024-11-29 18:15:55.388204: Current learning rate: 0.00973
2024-11-29 18:17:21.365208: Validation loss did not improve from -0.48826. Patience: 6/50
2024-11-29 18:17:21.366220: train_loss -0.5345
2024-11-29 18:17:21.367151: val_loss -0.4673
2024-11-29 18:17:21.367887: Pseudo dice [0.7265]
2024-11-29 18:17:21.368704: Epoch time: 85.98 s
2024-11-29 18:17:21.369576: Yayy! New best EMA pseudo Dice: 0.6912
2024-11-29 18:17:22.957992: 
2024-11-29 18:17:22.959960: Epoch 31
2024-11-29 18:17:22.961087: Current learning rate: 0.00972
2024-11-29 18:18:48.897535: Validation loss did not improve from -0.48826. Patience: 7/50
2024-11-29 18:18:48.899046: train_loss -0.5415
2024-11-29 18:18:48.900130: val_loss -0.4631
2024-11-29 18:18:48.901127: Pseudo dice [0.7154]
2024-11-29 18:18:48.902205: Epoch time: 85.94 s
2024-11-29 18:18:48.903262: Yayy! New best EMA pseudo Dice: 0.6936
2024-11-29 18:18:50.494179: 
2024-11-29 18:18:50.495792: Epoch 32
2024-11-29 18:18:50.496616: Current learning rate: 0.00971
2024-11-29 18:20:16.521080: Validation loss improved from -0.48826 to -0.50388! Patience: 7/50
2024-11-29 18:20:16.522007: train_loss -0.5545
2024-11-29 18:20:16.523235: val_loss -0.5039
2024-11-29 18:20:16.524196: Pseudo dice [0.7408]
2024-11-29 18:20:16.525067: Epoch time: 86.03 s
2024-11-29 18:20:16.526146: Yayy! New best EMA pseudo Dice: 0.6983
2024-11-29 18:20:18.085003: 
2024-11-29 18:20:18.086606: Epoch 33
2024-11-29 18:20:18.087640: Current learning rate: 0.0097
2024-11-29 18:21:44.065604: Validation loss did not improve from -0.50388. Patience: 1/50
2024-11-29 18:21:44.066674: train_loss -0.537
2024-11-29 18:21:44.067658: val_loss -0.426
2024-11-29 18:21:44.068389: Pseudo dice [0.6882]
2024-11-29 18:21:44.069053: Epoch time: 85.98 s
2024-11-29 18:21:45.294174: 
2024-11-29 18:21:45.296050: Epoch 34
2024-11-29 18:21:45.296881: Current learning rate: 0.00969
2024-11-29 18:23:11.232122: Validation loss did not improve from -0.50388. Patience: 2/50
2024-11-29 18:23:11.233283: train_loss -0.5515
2024-11-29 18:23:11.234867: val_loss -0.4806
2024-11-29 18:23:11.235870: Pseudo dice [0.7193]
2024-11-29 18:23:11.236789: Epoch time: 85.94 s
2024-11-29 18:23:11.602700: Yayy! New best EMA pseudo Dice: 0.6995
2024-11-29 18:23:13.228426: 
2024-11-29 18:23:13.230343: Epoch 35
2024-11-29 18:23:13.231770: Current learning rate: 0.00968
2024-11-29 18:24:39.277185: Validation loss did not improve from -0.50388. Patience: 3/50
2024-11-29 18:24:39.278530: train_loss -0.5502
2024-11-29 18:24:39.279588: val_loss -0.4703
2024-11-29 18:24:39.280285: Pseudo dice [0.7278]
2024-11-29 18:24:39.281061: Epoch time: 86.05 s
2024-11-29 18:24:39.281780: Yayy! New best EMA pseudo Dice: 0.7023
2024-11-29 18:24:40.969341: 
2024-11-29 18:24:40.970850: Epoch 36
2024-11-29 18:24:40.971869: Current learning rate: 0.00968
2024-11-29 18:26:07.095644: Validation loss did not improve from -0.50388. Patience: 4/50
2024-11-29 18:26:07.096872: train_loss -0.5661
2024-11-29 18:26:07.097929: val_loss -0.4766
2024-11-29 18:26:07.098762: Pseudo dice [0.7206]
2024-11-29 18:26:07.099585: Epoch time: 86.13 s
2024-11-29 18:26:07.100462: Yayy! New best EMA pseudo Dice: 0.7042
2024-11-29 18:26:08.747884: 
2024-11-29 18:26:08.749424: Epoch 37
2024-11-29 18:26:08.750295: Current learning rate: 0.00967
2024-11-29 18:27:34.740594: Validation loss did not improve from -0.50388. Patience: 5/50
2024-11-29 18:27:34.741698: train_loss -0.5599
2024-11-29 18:27:34.742852: val_loss -0.4474
2024-11-29 18:27:34.743822: Pseudo dice [0.7142]
2024-11-29 18:27:34.744539: Epoch time: 85.99 s
2024-11-29 18:27:34.745385: Yayy! New best EMA pseudo Dice: 0.7052
2024-11-29 18:27:36.410000: 
2024-11-29 18:27:36.411902: Epoch 38
2024-11-29 18:27:36.413014: Current learning rate: 0.00966
2024-11-29 18:29:02.384572: Validation loss did not improve from -0.50388. Patience: 6/50
2024-11-29 18:29:02.385789: train_loss -0.546
2024-11-29 18:29:02.386813: val_loss -0.433
2024-11-29 18:29:02.387841: Pseudo dice [0.702]
2024-11-29 18:29:02.388806: Epoch time: 85.98 s
2024-11-29 18:29:04.019813: 
2024-11-29 18:29:04.021858: Epoch 39
2024-11-29 18:29:04.023016: Current learning rate: 0.00965
2024-11-29 18:30:30.070915: Validation loss did not improve from -0.50388. Patience: 7/50
2024-11-29 18:30:30.071859: train_loss -0.5667
2024-11-29 18:30:30.072787: val_loss -0.4459
2024-11-29 18:30:30.073609: Pseudo dice [0.6952]
2024-11-29 18:30:30.074501: Epoch time: 86.05 s
2024-11-29 18:30:31.714699: 
2024-11-29 18:30:31.716269: Epoch 40
2024-11-29 18:30:31.717236: Current learning rate: 0.00964
2024-11-29 18:31:57.644536: Validation loss did not improve from -0.50388. Patience: 8/50
2024-11-29 18:31:57.645384: train_loss -0.5803
2024-11-29 18:31:57.646262: val_loss -0.5015
2024-11-29 18:31:57.646987: Pseudo dice [0.7259]
2024-11-29 18:31:57.647653: Epoch time: 85.93 s
2024-11-29 18:31:57.648330: Yayy! New best EMA pseudo Dice: 0.7061
2024-11-29 18:31:59.299225: 
2024-11-29 18:31:59.300389: Epoch 41
2024-11-29 18:31:59.301354: Current learning rate: 0.00963
2024-11-29 18:33:25.252164: Validation loss did not improve from -0.50388. Patience: 9/50
2024-11-29 18:33:25.253158: train_loss -0.5776
2024-11-29 18:33:25.254004: val_loss -0.4782
2024-11-29 18:33:25.254857: Pseudo dice [0.7162]
2024-11-29 18:33:25.255698: Epoch time: 85.95 s
2024-11-29 18:33:25.256649: Yayy! New best EMA pseudo Dice: 0.7071
2024-11-29 18:33:26.792123: 
2024-11-29 18:33:26.793988: Epoch 42
2024-11-29 18:33:26.794950: Current learning rate: 0.00962
2024-11-29 18:34:52.798201: Validation loss did not improve from -0.50388. Patience: 10/50
2024-11-29 18:34:52.799313: train_loss -0.5646
2024-11-29 18:34:52.800165: val_loss -0.494
2024-11-29 18:34:52.800934: Pseudo dice [0.7291]
2024-11-29 18:34:52.801726: Epoch time: 86.01 s
2024-11-29 18:34:52.802519: Yayy! New best EMA pseudo Dice: 0.7093
2024-11-29 18:34:54.374523: 
2024-11-29 18:34:54.376204: Epoch 43
2024-11-29 18:34:54.376903: Current learning rate: 0.00961
2024-11-29 18:36:20.282277: Validation loss did not improve from -0.50388. Patience: 11/50
2024-11-29 18:36:20.284110: train_loss -0.5827
2024-11-29 18:36:20.285992: val_loss -0.4949
2024-11-29 18:36:20.286837: Pseudo dice [0.7274]
2024-11-29 18:36:20.287675: Epoch time: 85.91 s
2024-11-29 18:36:20.288557: Yayy! New best EMA pseudo Dice: 0.7111
2024-11-29 18:36:21.896644: 
2024-11-29 18:36:21.898556: Epoch 44
2024-11-29 18:36:21.899826: Current learning rate: 0.0096
2024-11-29 18:37:47.758667: Validation loss did not improve from -0.50388. Patience: 12/50
2024-11-29 18:37:47.759902: train_loss -0.5748
2024-11-29 18:37:47.760823: val_loss -0.4626
2024-11-29 18:37:47.761484: Pseudo dice [0.712]
2024-11-29 18:37:47.762441: Epoch time: 85.86 s
2024-11-29 18:37:48.133725: Yayy! New best EMA pseudo Dice: 0.7112
2024-11-29 18:37:49.727609: 
2024-11-29 18:37:49.729213: Epoch 45
2024-11-29 18:37:49.729913: Current learning rate: 0.00959
2024-11-29 18:39:15.707887: Validation loss improved from -0.50388 to -0.52142! Patience: 12/50
2024-11-29 18:39:15.709210: train_loss -0.5805
2024-11-29 18:39:15.710166: val_loss -0.5214
2024-11-29 18:39:15.711253: Pseudo dice [0.758]
2024-11-29 18:39:15.712153: Epoch time: 85.98 s
2024-11-29 18:39:15.713092: Yayy! New best EMA pseudo Dice: 0.7159
2024-11-29 18:39:17.278230: 
2024-11-29 18:39:17.280014: Epoch 46
2024-11-29 18:39:17.281011: Current learning rate: 0.00959
2024-11-29 18:40:43.330473: Validation loss did not improve from -0.52142. Patience: 1/50
2024-11-29 18:40:43.331478: train_loss -0.588
2024-11-29 18:40:43.332417: val_loss -0.4931
2024-11-29 18:40:43.333076: Pseudo dice [0.7255]
2024-11-29 18:40:43.333781: Epoch time: 86.05 s
2024-11-29 18:40:43.334426: Yayy! New best EMA pseudo Dice: 0.7168
2024-11-29 18:40:44.867188: 
2024-11-29 18:40:44.869050: Epoch 47
2024-11-29 18:40:44.870478: Current learning rate: 0.00958
2024-11-29 18:42:10.829767: Validation loss did not improve from -0.52142. Patience: 2/50
2024-11-29 18:42:10.831051: train_loss -0.5721
2024-11-29 18:42:10.832002: val_loss -0.5053
2024-11-29 18:42:10.832901: Pseudo dice [0.7366]
2024-11-29 18:42:10.833712: Epoch time: 85.96 s
2024-11-29 18:42:10.834552: Yayy! New best EMA pseudo Dice: 0.7188
2024-11-29 18:42:12.436762: 
2024-11-29 18:42:12.438614: Epoch 48
2024-11-29 18:42:12.440063: Current learning rate: 0.00957
2024-11-29 18:43:38.409957: Validation loss did not improve from -0.52142. Patience: 3/50
2024-11-29 18:43:38.410981: train_loss -0.5885
2024-11-29 18:43:38.411729: val_loss -0.4071
2024-11-29 18:43:38.412493: Pseudo dice [0.6727]
2024-11-29 18:43:38.413317: Epoch time: 85.98 s
2024-11-29 18:43:39.660609: 
2024-11-29 18:43:39.661499: Epoch 49
2024-11-29 18:43:39.662272: Current learning rate: 0.00956
2024-11-29 18:45:05.737036: Validation loss did not improve from -0.52142. Patience: 4/50
2024-11-29 18:45:05.738256: train_loss -0.5825
2024-11-29 18:45:05.739223: val_loss -0.4858
2024-11-29 18:45:05.739995: Pseudo dice [0.7171]
2024-11-29 18:45:05.740701: Epoch time: 86.08 s
2024-11-29 18:45:07.605571: 
2024-11-29 18:45:07.607705: Epoch 50
2024-11-29 18:45:07.608767: Current learning rate: 0.00955
2024-11-29 18:46:33.552942: Validation loss improved from -0.52142 to -0.52531! Patience: 4/50
2024-11-29 18:46:33.554128: train_loss -0.5797
2024-11-29 18:46:33.555028: val_loss -0.5253
2024-11-29 18:46:33.555895: Pseudo dice [0.7501]
2024-11-29 18:46:33.556798: Epoch time: 85.95 s
2024-11-29 18:46:34.776943: 
2024-11-29 18:46:34.778691: Epoch 51
2024-11-29 18:46:34.779489: Current learning rate: 0.00954
2024-11-29 18:48:00.816805: Validation loss did not improve from -0.52531. Patience: 1/50
2024-11-29 18:48:00.817680: train_loss -0.5905
2024-11-29 18:48:00.818634: val_loss -0.5036
2024-11-29 18:48:00.819369: Pseudo dice [0.7373]
2024-11-29 18:48:00.820192: Epoch time: 86.04 s
2024-11-29 18:48:00.821032: Yayy! New best EMA pseudo Dice: 0.72
2024-11-29 18:48:02.458585: 
2024-11-29 18:48:02.460328: Epoch 52
2024-11-29 18:48:02.461184: Current learning rate: 0.00953
2024-11-29 18:49:28.481368: Validation loss did not improve from -0.52531. Patience: 2/50
2024-11-29 18:49:28.482905: train_loss -0.6037
2024-11-29 18:49:28.484178: val_loss -0.4624
2024-11-29 18:49:28.485136: Pseudo dice [0.7104]
2024-11-29 18:49:28.486160: Epoch time: 86.03 s
2024-11-29 18:49:29.717669: 
2024-11-29 18:49:29.719566: Epoch 53
2024-11-29 18:49:29.720524: Current learning rate: 0.00952
2024-11-29 18:50:55.703188: Validation loss did not improve from -0.52531. Patience: 3/50
2024-11-29 18:50:55.704398: train_loss -0.5924
2024-11-29 18:50:55.705326: val_loss -0.516
2024-11-29 18:50:55.706245: Pseudo dice [0.7308]
2024-11-29 18:50:55.707074: Epoch time: 85.99 s
2024-11-29 18:50:55.707782: Yayy! New best EMA pseudo Dice: 0.7202
2024-11-29 18:50:57.284962: 
2024-11-29 18:50:57.287256: Epoch 54
2024-11-29 18:50:57.288536: Current learning rate: 0.00951
2024-11-29 18:52:23.234694: Validation loss did not improve from -0.52531. Patience: 4/50
2024-11-29 18:52:23.235804: train_loss -0.5854
2024-11-29 18:52:23.236932: val_loss -0.4644
2024-11-29 18:52:23.237983: Pseudo dice [0.7149]
2024-11-29 18:52:23.238795: Epoch time: 85.95 s
2024-11-29 18:52:24.849190: 
2024-11-29 18:52:24.851091: Epoch 55
2024-11-29 18:52:24.851938: Current learning rate: 0.0095
2024-11-29 18:53:50.872058: Validation loss did not improve from -0.52531. Patience: 5/50
2024-11-29 18:53:50.873232: train_loss -0.5956
2024-11-29 18:53:50.874142: val_loss -0.5146
2024-11-29 18:53:50.874831: Pseudo dice [0.7418]
2024-11-29 18:53:50.875638: Epoch time: 86.02 s
2024-11-29 18:53:50.876470: Yayy! New best EMA pseudo Dice: 0.7219
2024-11-29 18:53:52.478553: 
2024-11-29 18:53:52.480459: Epoch 56
2024-11-29 18:53:52.481278: Current learning rate: 0.00949
2024-11-29 18:55:18.582906: Validation loss did not improve from -0.52531. Patience: 6/50
2024-11-29 18:55:18.584023: train_loss -0.6007
2024-11-29 18:55:18.585074: val_loss -0.4901
2024-11-29 18:55:18.586099: Pseudo dice [0.7277]
2024-11-29 18:55:18.587030: Epoch time: 86.11 s
2024-11-29 18:55:18.587808: Yayy! New best EMA pseudo Dice: 0.7225
2024-11-29 18:55:20.212296: 
2024-11-29 18:55:20.214218: Epoch 57
2024-11-29 18:55:20.215088: Current learning rate: 0.00949
2024-11-29 18:56:46.196144: Validation loss did not improve from -0.52531. Patience: 7/50
2024-11-29 18:56:46.197029: train_loss -0.5876
2024-11-29 18:56:46.198240: val_loss -0.4971
2024-11-29 18:56:46.199014: Pseudo dice [0.7248]
2024-11-29 18:56:46.199749: Epoch time: 85.99 s
2024-11-29 18:56:46.200542: Yayy! New best EMA pseudo Dice: 0.7227
2024-11-29 18:56:47.777143: 
2024-11-29 18:56:47.778928: Epoch 58
2024-11-29 18:56:47.780140: Current learning rate: 0.00948
2024-11-29 18:58:13.743073: Validation loss did not improve from -0.52531. Patience: 8/50
2024-11-29 18:58:13.744249: train_loss -0.5934
2024-11-29 18:58:13.745410: val_loss -0.508
2024-11-29 18:58:13.746309: Pseudo dice [0.7443]
2024-11-29 18:58:13.746971: Epoch time: 85.97 s
2024-11-29 18:58:13.747712: Yayy! New best EMA pseudo Dice: 0.7249
2024-11-29 18:58:15.358558: 
2024-11-29 18:58:15.360380: Epoch 59
2024-11-29 18:58:15.361448: Current learning rate: 0.00947
2024-11-29 18:59:41.401975: Validation loss did not improve from -0.52531. Patience: 9/50
2024-11-29 18:59:41.402859: train_loss -0.6201
2024-11-29 18:59:41.403743: val_loss -0.4406
2024-11-29 18:59:41.404550: Pseudo dice [0.6988]
2024-11-29 18:59:41.405373: Epoch time: 86.05 s
2024-11-29 18:59:43.019920: 
2024-11-29 18:59:43.021570: Epoch 60
2024-11-29 18:59:43.022410: Current learning rate: 0.00946
2024-11-29 19:01:08.991068: Validation loss did not improve from -0.52531. Patience: 10/50
2024-11-29 19:01:08.992331: train_loss -0.6187
2024-11-29 19:01:08.993538: val_loss -0.454
2024-11-29 19:01:08.995149: Pseudo dice [0.7051]
2024-11-29 19:01:08.996302: Epoch time: 85.97 s
2024-11-29 19:01:10.668746: 
2024-11-29 19:01:10.670765: Epoch 61
2024-11-29 19:01:10.672163: Current learning rate: 0.00945
2024-11-29 19:02:36.611523: Validation loss improved from -0.52531 to -0.52557! Patience: 10/50
2024-11-29 19:02:36.612469: train_loss -0.6119
2024-11-29 19:02:36.613511: val_loss -0.5256
2024-11-29 19:02:36.614244: Pseudo dice [0.7515]
2024-11-29 19:02:36.614933: Epoch time: 85.94 s
2024-11-29 19:02:37.891344: 
2024-11-29 19:02:37.893098: Epoch 62
2024-11-29 19:02:37.893967: Current learning rate: 0.00944
2024-11-29 19:04:03.905397: Validation loss did not improve from -0.52557. Patience: 1/50
2024-11-29 19:04:03.906562: train_loss -0.6159
2024-11-29 19:04:03.907666: val_loss -0.5096
2024-11-29 19:04:03.908329: Pseudo dice [0.736]
2024-11-29 19:04:03.909224: Epoch time: 86.02 s
2024-11-29 19:04:03.910063: Yayy! New best EMA pseudo Dice: 0.7249
2024-11-29 19:04:05.586304: 
2024-11-29 19:04:05.588092: Epoch 63
2024-11-29 19:04:05.588931: Current learning rate: 0.00943
2024-11-29 19:05:31.571613: Validation loss improved from -0.52557 to -0.52881! Patience: 1/50
2024-11-29 19:05:31.572836: train_loss -0.6206
2024-11-29 19:05:31.573907: val_loss -0.5288
2024-11-29 19:05:31.574898: Pseudo dice [0.747]
2024-11-29 19:05:31.575868: Epoch time: 85.99 s
2024-11-29 19:05:31.576818: Yayy! New best EMA pseudo Dice: 0.7271
2024-11-29 19:05:33.167547: 
2024-11-29 19:05:33.169258: Epoch 64
2024-11-29 19:05:33.170282: Current learning rate: 0.00942
2024-11-29 19:06:59.110178: Validation loss did not improve from -0.52881. Patience: 1/50
2024-11-29 19:06:59.111252: train_loss -0.6148
2024-11-29 19:06:59.112079: val_loss -0.4995
2024-11-29 19:06:59.113049: Pseudo dice [0.7245]
2024-11-29 19:06:59.113921: Epoch time: 85.94 s
2024-11-29 19:07:00.778053: 
2024-11-29 19:07:00.779855: Epoch 65
2024-11-29 19:07:00.780705: Current learning rate: 0.00941
2024-11-29 19:08:26.767214: Validation loss improved from -0.52881 to -0.53469! Patience: 1/50
2024-11-29 19:08:26.768578: train_loss -0.6177
2024-11-29 19:08:26.769732: val_loss -0.5347
2024-11-29 19:08:26.770555: Pseudo dice [0.7533]
2024-11-29 19:08:26.771267: Epoch time: 85.99 s
2024-11-29 19:08:26.771954: Yayy! New best EMA pseudo Dice: 0.7295
2024-11-29 19:08:28.415386: 
2024-11-29 19:08:28.416894: Epoch 66
2024-11-29 19:08:28.417669: Current learning rate: 0.0094
2024-11-29 19:09:54.491570: Validation loss did not improve from -0.53469. Patience: 1/50
2024-11-29 19:09:54.492841: train_loss -0.629
2024-11-29 19:09:54.493987: val_loss -0.5285
2024-11-29 19:09:54.494988: Pseudo dice [0.7531]
2024-11-29 19:09:54.495994: Epoch time: 86.08 s
2024-11-29 19:09:54.497009: Yayy! New best EMA pseudo Dice: 0.7318
2024-11-29 19:09:56.137742: 
2024-11-29 19:09:56.139483: Epoch 67
2024-11-29 19:09:56.140790: Current learning rate: 0.00939
2024-11-29 19:11:22.084234: Validation loss did not improve from -0.53469. Patience: 2/50
2024-11-29 19:11:22.085562: train_loss -0.63
2024-11-29 19:11:22.086745: val_loss -0.4829
2024-11-29 19:11:22.087628: Pseudo dice [0.7218]
2024-11-29 19:11:22.088426: Epoch time: 85.95 s
2024-11-29 19:11:23.373507: 
2024-11-29 19:11:23.376193: Epoch 68
2024-11-29 19:11:23.377642: Current learning rate: 0.00939
2024-11-29 19:12:49.306852: Validation loss did not improve from -0.53469. Patience: 3/50
2024-11-29 19:12:49.307719: train_loss -0.6248
2024-11-29 19:12:49.308624: val_loss -0.515
2024-11-29 19:12:49.309463: Pseudo dice [0.7454]
2024-11-29 19:12:49.310159: Epoch time: 85.94 s
2024-11-29 19:12:49.310836: Yayy! New best EMA pseudo Dice: 0.7323
2024-11-29 19:12:51.002048: 
2024-11-29 19:12:51.003858: Epoch 69
2024-11-29 19:12:51.004753: Current learning rate: 0.00938
2024-11-29 19:14:17.078682: Validation loss did not improve from -0.53469. Patience: 4/50
2024-11-29 19:14:17.079870: train_loss -0.6362
2024-11-29 19:14:17.080923: val_loss -0.5036
2024-11-29 19:14:17.081808: Pseudo dice [0.7349]
2024-11-29 19:14:17.082525: Epoch time: 86.08 s
2024-11-29 19:14:17.518000: Yayy! New best EMA pseudo Dice: 0.7325
2024-11-29 19:14:19.183479: 
2024-11-29 19:14:19.185268: Epoch 70
2024-11-29 19:14:19.186062: Current learning rate: 0.00937
2024-11-29 19:15:45.145719: Validation loss did not improve from -0.53469. Patience: 5/50
2024-11-29 19:15:45.146821: train_loss -0.6224
2024-11-29 19:15:45.147813: val_loss -0.464
2024-11-29 19:15:45.148620: Pseudo dice [0.7121]
2024-11-29 19:15:45.149470: Epoch time: 85.96 s
2024-11-29 19:15:46.787525: 
2024-11-29 19:15:46.789365: Epoch 71
2024-11-29 19:15:46.790366: Current learning rate: 0.00936
2024-11-29 19:17:12.695604: Validation loss did not improve from -0.53469. Patience: 6/50
2024-11-29 19:17:12.696568: train_loss -0.6282
2024-11-29 19:17:12.697660: val_loss -0.4781
2024-11-29 19:17:12.698455: Pseudo dice [0.7203]
2024-11-29 19:17:12.699219: Epoch time: 85.91 s
2024-11-29 19:17:13.949132: 
2024-11-29 19:17:13.950686: Epoch 72
2024-11-29 19:17:13.951703: Current learning rate: 0.00935
2024-11-29 19:18:39.920134: Validation loss did not improve from -0.53469. Patience: 7/50
2024-11-29 19:18:39.920923: train_loss -0.6356
2024-11-29 19:18:39.921974: val_loss -0.5146
2024-11-29 19:18:39.922823: Pseudo dice [0.7418]
2024-11-29 19:18:39.923666: Epoch time: 85.97 s
2024-11-29 19:18:41.221219: 
2024-11-29 19:18:41.222891: Epoch 73
2024-11-29 19:18:41.223823: Current learning rate: 0.00934
2024-11-29 19:20:07.209442: Validation loss did not improve from -0.53469. Patience: 8/50
2024-11-29 19:20:07.210660: train_loss -0.6395
2024-11-29 19:20:07.211501: val_loss -0.5235
2024-11-29 19:20:07.212332: Pseudo dice [0.7524]
2024-11-29 19:20:07.213136: Epoch time: 85.99 s
2024-11-29 19:20:07.213846: Yayy! New best EMA pseudo Dice: 0.7329
2024-11-29 19:20:08.884518: 
2024-11-29 19:20:08.886357: Epoch 74
2024-11-29 19:20:08.887504: Current learning rate: 0.00933
2024-11-29 19:21:34.861156: Validation loss did not improve from -0.53469. Patience: 9/50
2024-11-29 19:21:34.862086: train_loss -0.64
2024-11-29 19:21:34.863106: val_loss -0.4994
2024-11-29 19:21:34.863825: Pseudo dice [0.7377]
2024-11-29 19:21:34.864700: Epoch time: 85.98 s
2024-11-29 19:21:35.243717: Yayy! New best EMA pseudo Dice: 0.7334
2024-11-29 19:21:36.860555: 
2024-11-29 19:21:36.862201: Epoch 75
2024-11-29 19:21:36.863067: Current learning rate: 0.00932
2024-11-29 19:23:02.776693: Validation loss did not improve from -0.53469. Patience: 10/50
2024-11-29 19:23:02.778034: train_loss -0.6379
2024-11-29 19:23:02.779068: val_loss -0.5028
2024-11-29 19:23:02.779911: Pseudo dice [0.7477]
2024-11-29 19:23:02.780862: Epoch time: 85.92 s
2024-11-29 19:23:02.781665: Yayy! New best EMA pseudo Dice: 0.7348
2024-11-29 19:23:04.428402: 
2024-11-29 19:23:04.430191: Epoch 76
2024-11-29 19:23:04.431038: Current learning rate: 0.00931
2024-11-29 19:24:30.523170: Validation loss improved from -0.53469 to -0.53692! Patience: 10/50
2024-11-29 19:24:30.523950: train_loss -0.639
2024-11-29 19:24:30.524864: val_loss -0.5369
2024-11-29 19:24:30.525672: Pseudo dice [0.76]
2024-11-29 19:24:30.526488: Epoch time: 86.1 s
2024-11-29 19:24:30.527313: Yayy! New best EMA pseudo Dice: 0.7373
2024-11-29 19:24:32.136577: 
2024-11-29 19:24:32.137966: Epoch 77
2024-11-29 19:24:32.139066: Current learning rate: 0.0093
2024-11-29 19:25:58.104746: Validation loss did not improve from -0.53692. Patience: 1/50
2024-11-29 19:25:58.106124: train_loss -0.6377
2024-11-29 19:25:58.107512: val_loss -0.5181
2024-11-29 19:25:58.108452: Pseudo dice [0.7504]
2024-11-29 19:25:58.109436: Epoch time: 85.97 s
2024-11-29 19:25:58.110345: Yayy! New best EMA pseudo Dice: 0.7386
2024-11-29 19:25:59.743535: 
2024-11-29 19:25:59.745630: Epoch 78
2024-11-29 19:25:59.746587: Current learning rate: 0.0093
2024-11-29 19:27:25.754889: Validation loss did not improve from -0.53692. Patience: 2/50
2024-11-29 19:27:25.755952: train_loss -0.6451
2024-11-29 19:27:25.756905: val_loss -0.4887
2024-11-29 19:27:25.757891: Pseudo dice [0.7249]
2024-11-29 19:27:25.758645: Epoch time: 86.01 s
2024-11-29 19:27:27.064557: 
2024-11-29 19:27:27.066175: Epoch 79
2024-11-29 19:27:27.067048: Current learning rate: 0.00929
2024-11-29 19:28:53.192256: Validation loss did not improve from -0.53692. Patience: 3/50
2024-11-29 19:28:53.193161: train_loss -0.6405
2024-11-29 19:28:53.194179: val_loss -0.4904
2024-11-29 19:28:53.194985: Pseudo dice [0.7253]
2024-11-29 19:28:53.195836: Epoch time: 86.13 s
2024-11-29 19:28:54.915716: 
2024-11-29 19:28:54.917258: Epoch 80
2024-11-29 19:28:54.918191: Current learning rate: 0.00928
2024-11-29 19:30:20.881367: Validation loss did not improve from -0.53692. Patience: 4/50
2024-11-29 19:30:20.882887: train_loss -0.6405
2024-11-29 19:30:20.884093: val_loss -0.4975
2024-11-29 19:30:20.884866: Pseudo dice [0.7402]
2024-11-29 19:30:20.885550: Epoch time: 85.97 s
2024-11-29 19:30:22.211822: 
2024-11-29 19:30:22.213055: Epoch 81
2024-11-29 19:30:22.213754: Current learning rate: 0.00927
2024-11-29 19:31:48.199221: Validation loss did not improve from -0.53692. Patience: 5/50
2024-11-29 19:31:48.200060: train_loss -0.6396
2024-11-29 19:31:48.200824: val_loss -0.4938
2024-11-29 19:31:48.201594: Pseudo dice [0.7315]
2024-11-29 19:31:48.202229: Epoch time: 85.99 s
2024-11-29 19:31:49.869783: 
2024-11-29 19:31:49.871649: Epoch 82
2024-11-29 19:31:49.872447: Current learning rate: 0.00926
2024-11-29 19:33:15.846172: Validation loss did not improve from -0.53692. Patience: 6/50
2024-11-29 19:33:15.847349: train_loss -0.6409
2024-11-29 19:33:15.848248: val_loss -0.5226
2024-11-29 19:33:15.848967: Pseudo dice [0.7447]
2024-11-29 19:33:15.849753: Epoch time: 85.98 s
2024-11-29 19:33:17.090649: 
2024-11-29 19:33:17.092395: Epoch 83
2024-11-29 19:33:17.093366: Current learning rate: 0.00925
2024-11-29 19:34:43.075777: Validation loss did not improve from -0.53692. Patience: 7/50
2024-11-29 19:34:43.077136: train_loss -0.6453
2024-11-29 19:34:43.078606: val_loss -0.4834
2024-11-29 19:34:43.079590: Pseudo dice [0.7128]
2024-11-29 19:34:43.080406: Epoch time: 85.99 s
2024-11-29 19:34:44.341179: 
2024-11-29 19:34:44.342784: Epoch 84
2024-11-29 19:34:44.343834: Current learning rate: 0.00924
2024-11-29 19:36:10.270260: Validation loss did not improve from -0.53692. Patience: 8/50
2024-11-29 19:36:10.271576: train_loss -0.6488
2024-11-29 19:36:10.272928: val_loss -0.49
2024-11-29 19:36:10.274343: Pseudo dice [0.725]
2024-11-29 19:36:10.275562: Epoch time: 85.93 s
2024-11-29 19:36:11.887012: 
2024-11-29 19:36:11.888478: Epoch 85
2024-11-29 19:36:11.889301: Current learning rate: 0.00923
2024-11-29 19:37:37.869861: Validation loss did not improve from -0.53692. Patience: 9/50
2024-11-29 19:37:37.870898: train_loss -0.6498
2024-11-29 19:37:37.871939: val_loss -0.4878
2024-11-29 19:37:37.872748: Pseudo dice [0.7322]
2024-11-29 19:37:37.873580: Epoch time: 85.99 s
2024-11-29 19:37:39.099303: 
2024-11-29 19:37:39.101118: Epoch 86
2024-11-29 19:37:39.102238: Current learning rate: 0.00922
2024-11-29 19:39:05.565783: Validation loss did not improve from -0.53692. Patience: 10/50
2024-11-29 19:39:05.567304: train_loss -0.6465
2024-11-29 19:39:05.568491: val_loss -0.4971
2024-11-29 19:39:05.569473: Pseudo dice [0.7286]
2024-11-29 19:39:05.570646: Epoch time: 86.47 s
2024-11-29 19:39:06.846754: 
2024-11-29 19:39:06.847929: Epoch 87
2024-11-29 19:39:06.848840: Current learning rate: 0.00921
2024-11-29 19:40:36.369703: Validation loss did not improve from -0.53692. Patience: 11/50
2024-11-29 19:40:36.370626: train_loss -0.6454
2024-11-29 19:40:36.371762: val_loss -0.5055
2024-11-29 19:40:36.373116: Pseudo dice [0.7387]
2024-11-29 19:40:36.374045: Epoch time: 89.53 s
2024-11-29 19:40:37.579912: 
2024-11-29 19:40:37.581284: Epoch 88
2024-11-29 19:40:37.582035: Current learning rate: 0.0092
2024-11-29 19:42:03.895163: Validation loss did not improve from -0.53692. Patience: 12/50
2024-11-29 19:42:03.895898: train_loss -0.6598
2024-11-29 19:42:03.896779: val_loss -0.4925
2024-11-29 19:42:03.897697: Pseudo dice [0.7324]
2024-11-29 19:42:03.898425: Epoch time: 86.32 s
2024-11-29 19:42:05.122005: 
2024-11-29 19:42:05.124259: Epoch 89
2024-11-29 19:42:05.125376: Current learning rate: 0.0092
2024-11-29 19:43:32.895402: Validation loss did not improve from -0.53692. Patience: 13/50
2024-11-29 19:43:32.896434: train_loss -0.6525
2024-11-29 19:43:32.897213: val_loss -0.5085
2024-11-29 19:43:32.897931: Pseudo dice [0.7457]
2024-11-29 19:43:32.898693: Epoch time: 87.78 s
2024-11-29 19:43:34.461360: 
2024-11-29 19:43:34.463238: Epoch 90
2024-11-29 19:43:34.464069: Current learning rate: 0.00919
2024-11-29 19:45:00.422943: Validation loss did not improve from -0.53692. Patience: 14/50
2024-11-29 19:45:00.424347: train_loss -0.6572
2024-11-29 19:45:00.425268: val_loss -0.5334
2024-11-29 19:45:00.425928: Pseudo dice [0.7556]
2024-11-29 19:45:00.426847: Epoch time: 85.96 s
2024-11-29 19:45:01.803822: 
2024-11-29 19:45:01.805604: Epoch 91
2024-11-29 19:45:01.806396: Current learning rate: 0.00918
2024-11-29 19:46:27.769937: Validation loss did not improve from -0.53692. Patience: 15/50
2024-11-29 19:46:27.771021: train_loss -0.6562
2024-11-29 19:46:27.771827: val_loss -0.4601
2024-11-29 19:46:27.772637: Pseudo dice [0.7141]
2024-11-29 19:46:27.773334: Epoch time: 85.97 s
2024-11-29 19:46:28.983020: 
2024-11-29 19:46:28.985263: Epoch 92
2024-11-29 19:46:28.986184: Current learning rate: 0.00917
2024-11-29 19:47:54.967783: Validation loss did not improve from -0.53692. Patience: 16/50
2024-11-29 19:47:54.968733: train_loss -0.6551
2024-11-29 19:47:54.969734: val_loss -0.5133
2024-11-29 19:47:54.970708: Pseudo dice [0.754]
2024-11-29 19:47:54.971482: Epoch time: 85.99 s
2024-11-29 19:47:56.524285: 
2024-11-29 19:47:56.525753: Epoch 93
2024-11-29 19:47:56.526505: Current learning rate: 0.00916
2024-11-29 19:49:22.972964: Validation loss did not improve from -0.53692. Patience: 17/50
2024-11-29 19:49:22.974256: train_loss -0.6505
2024-11-29 19:49:22.975341: val_loss -0.5323
2024-11-29 19:49:22.976098: Pseudo dice [0.7529]
2024-11-29 19:49:22.977035: Epoch time: 86.45 s
2024-11-29 19:49:24.168538: 
2024-11-29 19:49:24.170305: Epoch 94
2024-11-29 19:49:24.171116: Current learning rate: 0.00915
2024-11-29 19:50:50.056374: Validation loss did not improve from -0.53692. Patience: 18/50
2024-11-29 19:50:50.057354: train_loss -0.6499
2024-11-29 19:50:50.058487: val_loss -0.4467
2024-11-29 19:50:50.059517: Pseudo dice [0.7062]
2024-11-29 19:50:50.060364: Epoch time: 85.89 s
2024-11-29 19:50:51.611330: 
2024-11-29 19:50:51.613161: Epoch 95
2024-11-29 19:50:51.614352: Current learning rate: 0.00914
2024-11-29 19:52:17.538546: Validation loss did not improve from -0.53692. Patience: 19/50
2024-11-29 19:52:17.539629: train_loss -0.6643
2024-11-29 19:52:17.540767: val_loss -0.5091
2024-11-29 19:52:17.541586: Pseudo dice [0.7422]
2024-11-29 19:52:17.542451: Epoch time: 85.93 s
2024-11-29 19:52:18.803575: 
2024-11-29 19:52:18.805422: Epoch 96
2024-11-29 19:52:18.806319: Current learning rate: 0.00913
2024-11-29 19:53:44.828957: Validation loss improved from -0.53692 to -0.55171! Patience: 19/50
2024-11-29 19:53:44.829979: train_loss -0.6641
2024-11-29 19:53:44.830965: val_loss -0.5517
2024-11-29 19:53:44.831693: Pseudo dice [0.7681]
2024-11-29 19:53:44.832446: Epoch time: 86.03 s
2024-11-29 19:53:44.833129: Yayy! New best EMA pseudo Dice: 0.7389
2024-11-29 19:53:46.429303: 
2024-11-29 19:53:46.431198: Epoch 97
2024-11-29 19:53:46.432160: Current learning rate: 0.00912
2024-11-29 19:55:12.386183: Validation loss did not improve from -0.55171. Patience: 1/50
2024-11-29 19:55:12.387573: train_loss -0.6607
2024-11-29 19:55:12.388793: val_loss -0.533
2024-11-29 19:55:12.389669: Pseudo dice [0.757]
2024-11-29 19:55:12.390616: Epoch time: 85.96 s
2024-11-29 19:55:12.391543: Yayy! New best EMA pseudo Dice: 0.7407
2024-11-29 19:55:13.951485: 
2024-11-29 19:55:13.953096: Epoch 98
2024-11-29 19:55:13.954047: Current learning rate: 0.00911
2024-11-29 19:56:39.883203: Validation loss did not improve from -0.55171. Patience: 2/50
2024-11-29 19:56:39.884034: train_loss -0.6622
2024-11-29 19:56:39.885030: val_loss -0.5268
2024-11-29 19:56:39.885746: Pseudo dice [0.744]
2024-11-29 19:56:39.886552: Epoch time: 85.93 s
2024-11-29 19:56:39.887386: Yayy! New best EMA pseudo Dice: 0.741
2024-11-29 19:56:41.503663: 
2024-11-29 19:56:41.505439: Epoch 99
2024-11-29 19:56:41.506454: Current learning rate: 0.0091
2024-11-29 19:58:07.533255: Validation loss did not improve from -0.55171. Patience: 3/50
2024-11-29 19:58:07.534484: train_loss -0.6527
2024-11-29 19:58:07.535791: val_loss -0.4513
2024-11-29 19:58:07.536616: Pseudo dice [0.7108]
2024-11-29 19:58:07.537638: Epoch time: 86.03 s
2024-11-29 19:58:09.131523: 
2024-11-29 19:58:09.133564: Epoch 100
2024-11-29 19:58:09.134932: Current learning rate: 0.0091
2024-11-29 19:59:35.064739: Validation loss did not improve from -0.55171. Patience: 4/50
2024-11-29 19:59:35.065764: train_loss -0.6697
2024-11-29 19:59:35.066776: val_loss -0.4699
2024-11-29 19:59:35.067601: Pseudo dice [0.7216]
2024-11-29 19:59:35.068303: Epoch time: 85.94 s
2024-11-29 19:59:36.279084: 
2024-11-29 19:59:36.280955: Epoch 101
2024-11-29 19:59:36.281657: Current learning rate: 0.00909
2024-11-29 20:01:02.212957: Validation loss did not improve from -0.55171. Patience: 5/50
2024-11-29 20:01:02.214091: train_loss -0.6691
2024-11-29 20:01:02.215027: val_loss -0.5018
2024-11-29 20:01:02.215980: Pseudo dice [0.7462]
2024-11-29 20:01:02.216650: Epoch time: 85.94 s
2024-11-29 20:01:03.444186: 
2024-11-29 20:01:03.445910: Epoch 102
2024-11-29 20:01:03.446709: Current learning rate: 0.00908
2024-11-29 20:02:29.502579: Validation loss did not improve from -0.55171. Patience: 6/50
2024-11-29 20:02:29.503860: train_loss -0.6654
2024-11-29 20:02:29.504753: val_loss -0.4974
2024-11-29 20:02:29.505402: Pseudo dice [0.745]
2024-11-29 20:02:29.506046: Epoch time: 86.06 s
2024-11-29 20:02:30.754191: 
2024-11-29 20:02:30.755945: Epoch 103
2024-11-29 20:02:30.757115: Current learning rate: 0.00907
2024-11-29 20:03:56.774271: Validation loss did not improve from -0.55171. Patience: 7/50
2024-11-29 20:03:56.775294: train_loss -0.6627
2024-11-29 20:03:56.776529: val_loss -0.4839
2024-11-29 20:03:56.777652: Pseudo dice [0.7331]
2024-11-29 20:03:56.778633: Epoch time: 86.02 s
2024-11-29 20:03:58.322136: 
2024-11-29 20:03:58.324005: Epoch 104
2024-11-29 20:03:58.325154: Current learning rate: 0.00906
2024-11-29 20:05:24.258455: Validation loss did not improve from -0.55171. Patience: 8/50
2024-11-29 20:05:24.259678: train_loss -0.6697
2024-11-29 20:05:24.260711: val_loss -0.4756
2024-11-29 20:05:24.261663: Pseudo dice [0.7304]
2024-11-29 20:05:24.262403: Epoch time: 85.94 s
2024-11-29 20:05:25.817850: 
2024-11-29 20:05:25.819434: Epoch 105
2024-11-29 20:05:25.820331: Current learning rate: 0.00905
2024-11-29 20:06:51.782263: Validation loss did not improve from -0.55171. Patience: 9/50
2024-11-29 20:06:51.783380: train_loss -0.6719
2024-11-29 20:06:51.784553: val_loss -0.5111
2024-11-29 20:06:51.785287: Pseudo dice [0.7365]
2024-11-29 20:06:51.786091: Epoch time: 85.97 s
2024-11-29 20:06:53.064692: 
2024-11-29 20:06:53.066840: Epoch 106
2024-11-29 20:06:53.067750: Current learning rate: 0.00904
2024-11-29 20:08:19.085393: Validation loss did not improve from -0.55171. Patience: 10/50
2024-11-29 20:08:19.086806: train_loss -0.6764
2024-11-29 20:08:19.088020: val_loss -0.5083
2024-11-29 20:08:19.088908: Pseudo dice [0.7435]
2024-11-29 20:08:19.089622: Epoch time: 86.02 s
2024-11-29 20:08:20.325113: 
2024-11-29 20:08:20.327142: Epoch 107
2024-11-29 20:08:20.328153: Current learning rate: 0.00903
2024-11-29 20:09:46.310864: Validation loss did not improve from -0.55171. Patience: 11/50
2024-11-29 20:09:46.311931: train_loss -0.6759
2024-11-29 20:09:46.312804: val_loss -0.5466
2024-11-29 20:09:46.313457: Pseudo dice [0.7597]
2024-11-29 20:09:46.314376: Epoch time: 85.99 s
2024-11-29 20:09:47.564312: 
2024-11-29 20:09:47.565884: Epoch 108
2024-11-29 20:09:47.566764: Current learning rate: 0.00902
2024-11-29 20:11:13.541250: Validation loss did not improve from -0.55171. Patience: 12/50
2024-11-29 20:11:13.542534: train_loss -0.676
2024-11-29 20:11:13.543460: val_loss -0.4949
2024-11-29 20:11:13.544204: Pseudo dice [0.723]
2024-11-29 20:11:13.544979: Epoch time: 85.98 s
2024-11-29 20:11:14.769651: 
2024-11-29 20:11:14.771521: Epoch 109
2024-11-29 20:11:14.772369: Current learning rate: 0.00901
2024-11-29 20:12:40.863867: Validation loss did not improve from -0.55171. Patience: 13/50
2024-11-29 20:12:40.864900: train_loss -0.6789
2024-11-29 20:12:40.866071: val_loss -0.5298
2024-11-29 20:12:40.867114: Pseudo dice [0.7568]
2024-11-29 20:12:40.867957: Epoch time: 86.1 s
2024-11-29 20:12:42.482345: 
2024-11-29 20:12:42.484438: Epoch 110
2024-11-29 20:12:42.485271: Current learning rate: 0.009
2024-11-29 20:14:08.438566: Validation loss did not improve from -0.55171. Patience: 14/50
2024-11-29 20:14:08.439566: train_loss -0.677
2024-11-29 20:14:08.440793: val_loss -0.5071
2024-11-29 20:14:08.441478: Pseudo dice [0.7361]
2024-11-29 20:14:08.442237: Epoch time: 85.96 s
2024-11-29 20:14:09.654179: 
2024-11-29 20:14:09.656353: Epoch 111
2024-11-29 20:14:09.657695: Current learning rate: 0.009
2024-11-29 20:15:35.606069: Validation loss did not improve from -0.55171. Patience: 15/50
2024-11-29 20:15:35.607091: train_loss -0.6849
2024-11-29 20:15:35.608075: val_loss -0.4924
2024-11-29 20:15:35.608907: Pseudo dice [0.7318]
2024-11-29 20:15:35.609635: Epoch time: 85.95 s
2024-11-29 20:15:36.851598: 
2024-11-29 20:15:36.853667: Epoch 112
2024-11-29 20:15:36.854627: Current learning rate: 0.00899
2024-11-29 20:17:02.825332: Validation loss did not improve from -0.55171. Patience: 16/50
2024-11-29 20:17:02.826658: train_loss -0.6741
2024-11-29 20:17:02.828022: val_loss -0.4496
2024-11-29 20:17:02.829089: Pseudo dice [0.7071]
2024-11-29 20:17:02.830028: Epoch time: 85.98 s
2024-11-29 20:17:04.056155: 
2024-11-29 20:17:04.057516: Epoch 113
2024-11-29 20:17:04.058631: Current learning rate: 0.00898
2024-11-29 20:18:30.019818: Validation loss did not improve from -0.55171. Patience: 17/50
2024-11-29 20:18:30.020922: train_loss -0.678
2024-11-29 20:18:30.021862: val_loss -0.4784
2024-11-29 20:18:30.022605: Pseudo dice [0.7292]
2024-11-29 20:18:30.023348: Epoch time: 85.97 s
2024-11-29 20:18:31.237557: 
2024-11-29 20:18:31.239340: Epoch 114
2024-11-29 20:18:31.240562: Current learning rate: 0.00897
2024-11-29 20:19:57.215181: Validation loss did not improve from -0.55171. Patience: 18/50
2024-11-29 20:19:57.216466: train_loss -0.6764
2024-11-29 20:19:57.217505: val_loss -0.5189
2024-11-29 20:19:57.218237: Pseudo dice [0.754]
2024-11-29 20:19:57.219088: Epoch time: 85.98 s
2024-11-29 20:19:59.178126: 
2024-11-29 20:19:59.180231: Epoch 115
2024-11-29 20:19:59.181156: Current learning rate: 0.00896
2024-11-29 20:21:25.148974: Validation loss did not improve from -0.55171. Patience: 19/50
2024-11-29 20:21:25.150065: train_loss -0.6916
2024-11-29 20:21:25.150954: val_loss -0.4882
2024-11-29 20:21:25.151845: Pseudo dice [0.7263]
2024-11-29 20:21:25.152796: Epoch time: 85.97 s
2024-11-29 20:21:26.406658: 
2024-11-29 20:21:26.408313: Epoch 116
2024-11-29 20:21:26.409449: Current learning rate: 0.00895
2024-11-29 20:22:52.515484: Validation loss did not improve from -0.55171. Patience: 20/50
2024-11-29 20:22:52.516495: train_loss -0.6818
2024-11-29 20:22:52.517339: val_loss -0.4841
2024-11-29 20:22:52.518066: Pseudo dice [0.7347]
2024-11-29 20:22:52.518826: Epoch time: 86.11 s
2024-11-29 20:22:53.797235: 
2024-11-29 20:22:53.799093: Epoch 117
2024-11-29 20:22:53.800051: Current learning rate: 0.00894
2024-11-29 20:24:19.736959: Validation loss did not improve from -0.55171. Patience: 21/50
2024-11-29 20:24:19.738275: train_loss -0.6813
2024-11-29 20:24:19.739211: val_loss -0.4961
2024-11-29 20:24:19.740111: Pseudo dice [0.7318]
2024-11-29 20:24:19.740972: Epoch time: 85.94 s
2024-11-29 20:24:20.981788: 
2024-11-29 20:24:20.983414: Epoch 118
2024-11-29 20:24:20.984654: Current learning rate: 0.00893
2024-11-29 20:25:46.936716: Validation loss did not improve from -0.55171. Patience: 22/50
2024-11-29 20:25:46.937988: train_loss -0.6887
2024-11-29 20:25:46.938983: val_loss -0.48
2024-11-29 20:25:46.939837: Pseudo dice [0.7365]
2024-11-29 20:25:46.940913: Epoch time: 85.96 s
2024-11-29 20:25:48.192554: 
2024-11-29 20:25:48.194360: Epoch 119
2024-11-29 20:25:48.195706: Current learning rate: 0.00892
2024-11-29 20:27:14.216880: Validation loss did not improve from -0.55171. Patience: 23/50
2024-11-29 20:27:14.218173: train_loss -0.6907
2024-11-29 20:27:14.219113: val_loss -0.4864
2024-11-29 20:27:14.219929: Pseudo dice [0.7433]
2024-11-29 20:27:14.220799: Epoch time: 86.03 s
2024-11-29 20:27:15.845798: 
2024-11-29 20:27:15.848032: Epoch 120
2024-11-29 20:27:15.849602: Current learning rate: 0.00891
2024-11-29 20:28:41.822900: Validation loss did not improve from -0.55171. Patience: 24/50
2024-11-29 20:28:41.824106: train_loss -0.6839
2024-11-29 20:28:41.825213: val_loss -0.47
2024-11-29 20:28:41.825997: Pseudo dice [0.7243]
2024-11-29 20:28:41.827001: Epoch time: 85.98 s
2024-11-29 20:28:43.058074: 
2024-11-29 20:28:43.060306: Epoch 121
2024-11-29 20:28:43.061150: Current learning rate: 0.0089
2024-11-29 20:30:08.928985: Validation loss did not improve from -0.55171. Patience: 25/50
2024-11-29 20:30:08.929973: train_loss -0.6915
2024-11-29 20:30:08.931066: val_loss -0.5168
2024-11-29 20:30:08.931995: Pseudo dice [0.7455]
2024-11-29 20:30:08.932836: Epoch time: 85.87 s
2024-11-29 20:30:10.211084: 
2024-11-29 20:30:10.212885: Epoch 122
2024-11-29 20:30:10.214090: Current learning rate: 0.00889
2024-11-29 20:31:36.172636: Validation loss did not improve from -0.55171. Patience: 26/50
2024-11-29 20:31:36.175158: train_loss -0.6882
2024-11-29 20:31:36.176487: val_loss -0.4972
2024-11-29 20:31:36.177301: Pseudo dice [0.7305]
2024-11-29 20:31:36.177997: Epoch time: 85.97 s
2024-11-29 20:31:37.502883: 
2024-11-29 20:31:37.504812: Epoch 123
2024-11-29 20:31:37.505965: Current learning rate: 0.00889
2024-11-29 20:33:03.584527: Validation loss did not improve from -0.55171. Patience: 27/50
2024-11-29 20:33:03.585702: train_loss -0.6791
2024-11-29 20:33:03.586751: val_loss -0.4955
2024-11-29 20:33:03.587478: Pseudo dice [0.7349]
2024-11-29 20:33:03.588273: Epoch time: 86.08 s
2024-11-29 20:33:04.851016: 
2024-11-29 20:33:04.852822: Epoch 124
2024-11-29 20:33:04.853596: Current learning rate: 0.00888
2024-11-29 20:34:30.809172: Validation loss did not improve from -0.55171. Patience: 28/50
2024-11-29 20:34:30.810369: train_loss -0.6845
2024-11-29 20:34:30.811235: val_loss -0.4945
2024-11-29 20:34:30.812076: Pseudo dice [0.7309]
2024-11-29 20:34:30.812841: Epoch time: 85.96 s
2024-11-29 20:34:32.480729: 
2024-11-29 20:34:32.482344: Epoch 125
2024-11-29 20:34:32.483115: Current learning rate: 0.00887
2024-11-29 20:35:58.425453: Validation loss did not improve from -0.55171. Patience: 29/50
2024-11-29 20:35:58.426817: train_loss -0.6946
2024-11-29 20:35:58.427775: val_loss -0.4924
2024-11-29 20:35:58.428774: Pseudo dice [0.737]
2024-11-29 20:35:58.429475: Epoch time: 85.95 s
2024-11-29 20:36:00.010937: 
2024-11-29 20:36:00.012131: Epoch 126
2024-11-29 20:36:00.012929: Current learning rate: 0.00886
2024-11-29 20:37:26.032575: Validation loss did not improve from -0.55171. Patience: 30/50
2024-11-29 20:37:26.033715: train_loss -0.6989
2024-11-29 20:37:26.035015: val_loss -0.5218
2024-11-29 20:37:26.036061: Pseudo dice [0.7468]
2024-11-29 20:37:26.037118: Epoch time: 86.02 s
2024-11-29 20:37:27.252201: 
2024-11-29 20:37:27.254070: Epoch 127
2024-11-29 20:37:27.255026: Current learning rate: 0.00885
2024-11-29 20:38:53.223216: Validation loss did not improve from -0.55171. Patience: 31/50
2024-11-29 20:38:53.224084: train_loss -0.6839
2024-11-29 20:38:53.225322: val_loss -0.5148
2024-11-29 20:38:53.226092: Pseudo dice [0.741]
2024-11-29 20:38:53.226860: Epoch time: 85.97 s
2024-11-29 20:38:54.486712: 
2024-11-29 20:38:54.488407: Epoch 128
2024-11-29 20:38:54.489485: Current learning rate: 0.00884
2024-11-29 20:40:20.415496: Validation loss did not improve from -0.55171. Patience: 32/50
2024-11-29 20:40:20.416458: train_loss -0.7024
2024-11-29 20:40:20.417602: val_loss -0.4934
2024-11-29 20:40:20.418931: Pseudo dice [0.7401]
2024-11-29 20:40:20.419879: Epoch time: 85.93 s
2024-11-29 20:40:21.653655: 
2024-11-29 20:40:21.655451: Epoch 129
2024-11-29 20:40:21.656573: Current learning rate: 0.00883
2024-11-29 20:41:47.660535: Validation loss did not improve from -0.55171. Patience: 33/50
2024-11-29 20:41:47.661536: train_loss -0.6977
2024-11-29 20:41:47.662419: val_loss -0.47
2024-11-29 20:41:47.663271: Pseudo dice [0.7259]
2024-11-29 20:41:47.664164: Epoch time: 86.01 s
2024-11-29 20:41:49.316276: 
2024-11-29 20:41:49.318127: Epoch 130
2024-11-29 20:41:49.318926: Current learning rate: 0.00882
2024-11-29 20:43:15.335740: Validation loss did not improve from -0.55171. Patience: 34/50
2024-11-29 20:43:15.337125: train_loss -0.7007
2024-11-29 20:43:15.338038: val_loss -0.514
2024-11-29 20:43:15.338830: Pseudo dice [0.7493]
2024-11-29 20:43:15.339573: Epoch time: 86.02 s
2024-11-29 20:43:16.563286: 
2024-11-29 20:43:16.565332: Epoch 131
2024-11-29 20:43:16.566345: Current learning rate: 0.00881
2024-11-29 20:44:43.535839: Validation loss did not improve from -0.55171. Patience: 35/50
2024-11-29 20:44:43.569664: train_loss -0.6963
2024-11-29 20:44:43.570959: val_loss -0.463
2024-11-29 20:44:43.571947: Pseudo dice [0.7138]
2024-11-29 20:44:43.576312: Epoch time: 87.01 s
2024-11-29 20:44:44.982744: 
2024-11-29 20:44:44.984492: Epoch 132
2024-11-29 20:44:44.985278: Current learning rate: 0.0088
2024-11-29 20:46:11.908391: Validation loss did not improve from -0.55171. Patience: 36/50
2024-11-29 20:46:11.909691: train_loss -0.6936
2024-11-29 20:46:11.910884: val_loss -0.5086
2024-11-29 20:46:11.911577: Pseudo dice [0.7457]
2024-11-29 20:46:11.912364: Epoch time: 86.93 s
2024-11-29 20:46:13.174999: 
2024-11-29 20:46:13.176609: Epoch 133
2024-11-29 20:46:13.177704: Current learning rate: 0.00879
2024-11-29 20:47:39.417419: Validation loss did not improve from -0.55171. Patience: 37/50
2024-11-29 20:47:39.418252: train_loss -0.7017
2024-11-29 20:47:39.419061: val_loss -0.5265
2024-11-29 20:47:39.419851: Pseudo dice [0.7438]
2024-11-29 20:47:39.420511: Epoch time: 86.24 s
2024-11-29 20:47:40.652892: 
2024-11-29 20:47:40.654215: Epoch 134
2024-11-29 20:47:40.655092: Current learning rate: 0.00879
2024-11-29 20:49:06.638884: Validation loss did not improve from -0.55171. Patience: 38/50
2024-11-29 20:49:06.639987: train_loss -0.6979
2024-11-29 20:49:06.641416: val_loss -0.4634
2024-11-29 20:49:06.642543: Pseudo dice [0.7278]
2024-11-29 20:49:06.643526: Epoch time: 85.99 s
2024-11-29 20:49:08.231068: 
2024-11-29 20:49:08.232845: Epoch 135
2024-11-29 20:49:08.233596: Current learning rate: 0.00878
2024-11-29 20:50:34.200136: Validation loss did not improve from -0.55171. Patience: 39/50
2024-11-29 20:50:34.200982: train_loss -0.7033
2024-11-29 20:50:34.203122: val_loss -0.4562
2024-11-29 20:50:34.204148: Pseudo dice [0.72]
2024-11-29 20:50:34.205202: Epoch time: 85.97 s
2024-11-29 20:50:35.527610: 
2024-11-29 20:50:35.529407: Epoch 136
2024-11-29 20:50:35.530566: Current learning rate: 0.00877
2024-11-29 20:52:01.572515: Validation loss did not improve from -0.55171. Patience: 40/50
2024-11-29 20:52:01.573218: train_loss -0.6963
2024-11-29 20:52:01.574056: val_loss -0.4606
2024-11-29 20:52:01.574832: Pseudo dice [0.7154]
2024-11-29 20:52:01.575575: Epoch time: 86.05 s
2024-11-29 20:52:03.231286: 
2024-11-29 20:52:03.233169: Epoch 137
2024-11-29 20:52:03.233980: Current learning rate: 0.00876
2024-11-29 20:53:29.213844: Validation loss did not improve from -0.55171. Patience: 41/50
2024-11-29 20:53:29.214755: train_loss -0.6963
2024-11-29 20:53:29.215456: val_loss -0.5447
2024-11-29 20:53:29.216256: Pseudo dice [0.7651]
2024-11-29 20:53:29.217017: Epoch time: 85.98 s
2024-11-29 20:53:30.529386: 
2024-11-29 20:53:30.531209: Epoch 138
2024-11-29 20:53:30.532161: Current learning rate: 0.00875
2024-11-29 20:54:56.534475: Validation loss did not improve from -0.55171. Patience: 42/50
2024-11-29 20:54:56.535275: train_loss -0.71
2024-11-29 20:54:56.536031: val_loss -0.5201
2024-11-29 20:54:56.536813: Pseudo dice [0.7509]
2024-11-29 20:54:56.537444: Epoch time: 86.01 s
2024-11-29 20:54:57.821852: 
2024-11-29 20:54:57.823447: Epoch 139
2024-11-29 20:54:57.824339: Current learning rate: 0.00874
2024-11-29 20:56:23.810575: Validation loss did not improve from -0.55171. Patience: 43/50
2024-11-29 20:56:23.811479: train_loss -0.7066
2024-11-29 20:56:23.812706: val_loss -0.4899
2024-11-29 20:56:23.813668: Pseudo dice [0.7357]
2024-11-29 20:56:23.814704: Epoch time: 85.99 s
2024-11-29 20:56:25.426111: 
2024-11-29 20:56:25.427918: Epoch 140
2024-11-29 20:56:25.428860: Current learning rate: 0.00873
2024-11-29 20:57:51.457406: Validation loss did not improve from -0.55171. Patience: 44/50
2024-11-29 20:57:51.458679: train_loss -0.7083
2024-11-29 20:57:51.459856: val_loss -0.5294
2024-11-29 20:57:51.460554: Pseudo dice [0.7575]
2024-11-29 20:57:51.461295: Epoch time: 86.03 s
2024-11-29 20:57:52.740525: 
2024-11-29 20:57:52.742139: Epoch 141
2024-11-29 20:57:52.743252: Current learning rate: 0.00872
2024-11-29 20:59:18.751155: Validation loss did not improve from -0.55171. Patience: 45/50
2024-11-29 20:59:18.752293: train_loss -0.7022
2024-11-29 20:59:18.753492: val_loss -0.489
2024-11-29 20:59:18.754201: Pseudo dice [0.7283]
2024-11-29 20:59:18.754985: Epoch time: 86.01 s
2024-11-29 20:59:20.002212: 
2024-11-29 20:59:20.004179: Epoch 142
2024-11-29 20:59:20.004934: Current learning rate: 0.00871
2024-11-29 21:00:46.036425: Validation loss did not improve from -0.55171. Patience: 46/50
2024-11-29 21:00:46.037426: train_loss -0.7074
2024-11-29 21:00:46.038615: val_loss -0.5137
2024-11-29 21:00:46.039534: Pseudo dice [0.7532]
2024-11-29 21:00:46.040557: Epoch time: 86.04 s
2024-11-29 21:00:47.412196: 
2024-11-29 21:00:47.413507: Epoch 143
2024-11-29 21:00:47.415144: Current learning rate: 0.0087
2024-11-29 21:02:13.510009: Validation loss did not improve from -0.55171. Patience: 47/50
2024-11-29 21:02:13.510963: train_loss -0.7094
2024-11-29 21:02:13.511977: val_loss -0.5109
2024-11-29 21:02:13.512878: Pseudo dice [0.7558]
2024-11-29 21:02:13.513925: Epoch time: 86.1 s
2024-11-29 21:02:13.515188: Yayy! New best EMA pseudo Dice: 0.7412
2024-11-29 21:02:15.135768: 
2024-11-29 21:02:15.137549: Epoch 144
2024-11-29 21:02:15.138500: Current learning rate: 0.00869
2024-11-29 21:03:41.056546: Validation loss did not improve from -0.55171. Patience: 48/50
2024-11-29 21:03:41.057738: train_loss -0.7056
2024-11-29 21:03:41.058683: val_loss -0.4711
2024-11-29 21:03:41.059327: Pseudo dice [0.7108]
2024-11-29 21:03:41.060128: Epoch time: 85.92 s
2024-11-29 21:03:42.691441: 
2024-11-29 21:03:42.693017: Epoch 145
2024-11-29 21:03:42.693861: Current learning rate: 0.00868
2024-11-29 21:05:08.659679: Validation loss did not improve from -0.55171. Patience: 49/50
2024-11-29 21:05:08.660627: train_loss -0.7057
2024-11-29 21:05:08.661435: val_loss -0.4803
2024-11-29 21:05:08.662273: Pseudo dice [0.7228]
2024-11-29 21:05:08.663007: Epoch time: 85.97 s
2024-11-29 21:05:09.991770: 
2024-11-29 21:05:09.993144: Epoch 146
2024-11-29 21:05:09.993993: Current learning rate: 0.00868
2024-11-29 21:06:36.058170: Validation loss did not improve from -0.55171. Patience: 50/50
2024-11-29 21:06:36.059310: train_loss -0.7009
2024-11-29 21:06:36.060233: val_loss -0.4554
2024-11-29 21:06:36.061082: Pseudo dice [0.7089]
2024-11-29 21:06:36.061785: Epoch time: 86.07 s
2024-11-29 21:06:37.670432: Patience reached. Stopping training.
2024-11-29 21:06:38.155664: Training done.
2024-11-29 21:06:38.876941: Using splits from existing split file: /home/gridsan/nchutisilp/datasets/nnUNet_Datasets/nnUNet_preprocessed/Dataset306_Sohee_Ajay_Calcium_OCT/splits_final.json
2024-11-29 21:06:38.893749: The split file contains 5 splits.
2024-11-29 21:06:38.895213: Desired fold for training: 0
2024-11-29 21:06:38.896282: This split has 10 training and 3 validation cases.
2024-11-29 21:06:38.897175: predicting 02008Pre
2024-11-29 21:06:38.962688: 02008Pre, shape torch.Size([1, 171, 498, 498]), rank 0
2024-11-29 21:08:03.907549: predicting 106-002
2024-11-29 21:08:03.928899: 106-002, shape torch.Size([1, 539, 498, 498]), rank 0
2024-11-29 21:10:07.081780: predicting 701-013
2024-11-29 21:10:07.112872: 701-013, shape torch.Size([1, 375, 498, 498]), rank 0
2024-11-29 21:11:58.993724: Validation complete
2024-11-29 21:11:58.994753: Mean Validation Dice:  0.7456219886210317
2024-11-29 17:30:07.902173: unpacking done...
2024-11-29 17:30:07.998746: Unable to plot network architecture: nnUNet_compile is enabled!
2024-11-29 17:30:08.156322: 
2024-11-29 17:30:08.157607: Epoch 0
2024-11-29 17:30:08.158744: Current learning rate: 0.01
2024-11-29 17:33:29.049305: Validation loss improved from 1000.00000 to -0.21463! Patience: 0/50
2024-11-29 17:33:29.090069: train_loss -0.0893
2024-11-29 17:33:29.108896: val_loss -0.2146
2024-11-29 17:33:29.110382: Pseudo dice [0.5296]
2024-11-29 17:33:29.111199: Epoch time: 200.9 s
2024-11-29 17:33:29.111848: Yayy! New best EMA pseudo Dice: 0.5296
2024-11-29 17:33:31.568075: 
2024-11-29 17:33:31.570723: Epoch 1
2024-11-29 17:33:31.572400: Current learning rate: 0.00999
2024-11-29 17:34:57.484067: Validation loss improved from -0.21463 to -0.26020! Patience: 0/50
2024-11-29 17:34:57.485143: train_loss -0.2241
2024-11-29 17:34:57.485980: val_loss -0.2602
2024-11-29 17:34:57.486794: Pseudo dice [0.5296]
2024-11-29 17:34:57.487569: Epoch time: 85.92 s
2024-11-29 17:34:57.488347: Yayy! New best EMA pseudo Dice: 0.5296
2024-11-29 17:34:59.138632: 
2024-11-29 17:34:59.140653: Epoch 2
2024-11-29 17:34:59.141356: Current learning rate: 0.00998
2024-11-29 17:36:25.164660: Validation loss did not improve from -0.26020. Patience: 1/50
2024-11-29 17:36:25.166069: train_loss -0.2592
2024-11-29 17:36:25.166906: val_loss -0.2595
2024-11-29 17:36:25.167862: Pseudo dice [0.5391]
2024-11-29 17:36:25.168711: Epoch time: 86.03 s
2024-11-29 17:36:25.169544: Yayy! New best EMA pseudo Dice: 0.5305
2024-11-29 17:36:26.803357: 
2024-11-29 17:36:26.804779: Epoch 3
2024-11-29 17:36:26.805842: Current learning rate: 0.00997
2024-11-29 17:37:52.730082: Validation loss improved from -0.26020 to -0.27107! Patience: 1/50
2024-11-29 17:37:52.731683: train_loss -0.3058
2024-11-29 17:37:52.733020: val_loss -0.2711
2024-11-29 17:37:52.733689: Pseudo dice [0.5619]
2024-11-29 17:37:52.734563: Epoch time: 85.93 s
2024-11-29 17:37:52.735488: Yayy! New best EMA pseudo Dice: 0.5337
2024-11-29 17:37:54.333188: 
2024-11-29 17:37:54.334826: Epoch 4
2024-11-29 17:37:54.335763: Current learning rate: 0.00996
2024-11-29 17:39:20.257597: Validation loss improved from -0.27107 to -0.30503! Patience: 0/50
2024-11-29 17:39:20.258433: train_loss -0.3079
2024-11-29 17:39:20.259197: val_loss -0.305
2024-11-29 17:39:20.259992: Pseudo dice [0.5636]
2024-11-29 17:39:20.260880: Epoch time: 85.93 s
2024-11-29 17:39:20.565140: Yayy! New best EMA pseudo Dice: 0.5367
2024-11-29 17:39:22.161061: 
2024-11-29 17:39:22.163396: Epoch 5
2024-11-29 17:39:22.164480: Current learning rate: 0.00995
2024-11-29 17:40:48.124325: Validation loss did not improve from -0.30503. Patience: 1/50
2024-11-29 17:40:48.125415: train_loss -0.3357
2024-11-29 17:40:48.126256: val_loss -0.2737
2024-11-29 17:40:48.127240: Pseudo dice [0.5742]
2024-11-29 17:40:48.127976: Epoch time: 85.97 s
2024-11-29 17:40:48.128661: Yayy! New best EMA pseudo Dice: 0.5404
2024-11-29 17:40:49.664484: 
2024-11-29 17:40:49.666619: Epoch 6
2024-11-29 17:40:49.667470: Current learning rate: 0.00995
2024-11-29 17:42:15.593649: Validation loss improved from -0.30503 to -0.33697! Patience: 1/50
2024-11-29 17:42:15.594531: train_loss -0.3553
2024-11-29 17:42:15.595865: val_loss -0.337
2024-11-29 17:42:15.596968: Pseudo dice [0.5901]
2024-11-29 17:42:15.597893: Epoch time: 85.93 s
2024-11-29 17:42:15.598937: Yayy! New best EMA pseudo Dice: 0.5454
2024-11-29 17:42:17.195525: 
2024-11-29 17:42:17.197146: Epoch 7
2024-11-29 17:42:17.197952: Current learning rate: 0.00994
2024-11-29 17:43:43.068780: Validation loss improved from -0.33697 to -0.38145! Patience: 0/50
2024-11-29 17:43:43.069641: train_loss -0.3755
2024-11-29 17:43:43.070654: val_loss -0.3815
2024-11-29 17:43:43.071632: Pseudo dice [0.62]
2024-11-29 17:43:43.072598: Epoch time: 85.88 s
2024-11-29 17:43:43.073528: Yayy! New best EMA pseudo Dice: 0.5528
2024-11-29 17:43:45.000911: 
2024-11-29 17:43:45.002530: Epoch 8
2024-11-29 17:43:45.003602: Current learning rate: 0.00993
2024-11-29 17:45:10.931656: Validation loss did not improve from -0.38145. Patience: 1/50
2024-11-29 17:45:10.932837: train_loss -0.3928
2024-11-29 17:45:10.934103: val_loss -0.3622
2024-11-29 17:45:10.934861: Pseudo dice [0.6216]
2024-11-29 17:45:10.935652: Epoch time: 85.93 s
2024-11-29 17:45:10.936496: Yayy! New best EMA pseudo Dice: 0.5597
2024-11-29 17:45:12.535065: 
2024-11-29 17:45:12.536812: Epoch 9
2024-11-29 17:45:12.537587: Current learning rate: 0.00992
2024-11-29 17:46:38.567521: Validation loss did not improve from -0.38145. Patience: 2/50
2024-11-29 17:46:38.568642: train_loss -0.404
2024-11-29 17:46:38.569498: val_loss -0.3549
2024-11-29 17:46:38.570193: Pseudo dice [0.6251]
2024-11-29 17:46:38.570982: Epoch time: 86.03 s
2024-11-29 17:46:38.890774: Yayy! New best EMA pseudo Dice: 0.5663
2024-11-29 17:46:40.471507: 
2024-11-29 17:46:40.473398: Epoch 10
2024-11-29 17:46:40.474393: Current learning rate: 0.00991
2024-11-29 17:48:06.396518: Validation loss improved from -0.38145 to -0.40012! Patience: 2/50
2024-11-29 17:48:06.397767: train_loss -0.4154
2024-11-29 17:48:06.398827: val_loss -0.4001
2024-11-29 17:48:06.399470: Pseudo dice [0.6378]
2024-11-29 17:48:06.400110: Epoch time: 85.93 s
2024-11-29 17:48:06.400789: Yayy! New best EMA pseudo Dice: 0.5734
2024-11-29 17:48:07.978008: 
2024-11-29 17:48:07.979955: Epoch 11
2024-11-29 17:48:07.980706: Current learning rate: 0.0099
2024-11-29 17:49:33.916281: Validation loss improved from -0.40012 to -0.42272! Patience: 0/50
2024-11-29 17:49:33.917427: train_loss -0.4345
2024-11-29 17:49:33.918474: val_loss -0.4227
2024-11-29 17:49:33.919772: Pseudo dice [0.6541]
2024-11-29 17:49:33.920499: Epoch time: 85.94 s
2024-11-29 17:49:33.921151: Yayy! New best EMA pseudo Dice: 0.5815
2024-11-29 17:49:35.501760: 
2024-11-29 17:49:35.504573: Epoch 12
2024-11-29 17:49:35.505580: Current learning rate: 0.00989
2024-11-29 17:51:01.560049: Validation loss did not improve from -0.42272. Patience: 1/50
2024-11-29 17:51:01.560827: train_loss -0.4234
2024-11-29 17:51:01.561457: val_loss -0.3731
2024-11-29 17:51:01.562090: Pseudo dice [0.6336]
2024-11-29 17:51:01.562746: Epoch time: 86.06 s
2024-11-29 17:51:01.563370: Yayy! New best EMA pseudo Dice: 0.5867
2024-11-29 17:51:03.174575: 
2024-11-29 17:51:03.178788: Epoch 13
2024-11-29 17:51:03.180264: Current learning rate: 0.00988
2024-11-29 17:52:29.149209: Validation loss did not improve from -0.42272. Patience: 2/50
2024-11-29 17:52:29.149960: train_loss -0.4207
2024-11-29 17:52:29.150669: val_loss -0.3522
2024-11-29 17:52:29.151261: Pseudo dice [0.6164]
2024-11-29 17:52:29.151964: Epoch time: 85.98 s
2024-11-29 17:52:29.152807: Yayy! New best EMA pseudo Dice: 0.5897
2024-11-29 17:52:30.789198: 
2024-11-29 17:52:30.791017: Epoch 14
2024-11-29 17:52:30.791907: Current learning rate: 0.00987
2024-11-29 17:53:56.725431: Validation loss did not improve from -0.42272. Patience: 3/50
2024-11-29 17:53:56.726672: train_loss -0.4267
2024-11-29 17:53:56.727487: val_loss -0.3793
2024-11-29 17:53:56.728137: Pseudo dice [0.6312]
2024-11-29 17:53:56.728915: Epoch time: 85.94 s
2024-11-29 17:53:57.065553: Yayy! New best EMA pseudo Dice: 0.5938
2024-11-29 17:53:58.684098: 
2024-11-29 17:53:58.687987: Epoch 15
2024-11-29 17:53:58.689248: Current learning rate: 0.00986
2024-11-29 17:55:24.660776: Validation loss improved from -0.42272 to -0.45787! Patience: 3/50
2024-11-29 17:55:24.661715: train_loss -0.4544
2024-11-29 17:55:24.662578: val_loss -0.4579
2024-11-29 17:55:24.663412: Pseudo dice [0.6707]
2024-11-29 17:55:24.664102: Epoch time: 85.98 s
2024-11-29 17:55:24.664913: Yayy! New best EMA pseudo Dice: 0.6015
2024-11-29 17:55:26.298019: 
2024-11-29 17:55:26.300951: Epoch 16
2024-11-29 17:55:26.302208: Current learning rate: 0.00986
2024-11-29 17:56:52.216495: Validation loss did not improve from -0.45787. Patience: 1/50
2024-11-29 17:56:52.217209: train_loss -0.4716
2024-11-29 17:56:52.218024: val_loss -0.4365
2024-11-29 17:56:52.218862: Pseudo dice [0.6622]
2024-11-29 17:56:52.220052: Epoch time: 85.92 s
2024-11-29 17:56:52.221056: Yayy! New best EMA pseudo Dice: 0.6076
2024-11-29 17:56:53.869161: 
2024-11-29 17:56:53.870863: Epoch 17
2024-11-29 17:56:53.872016: Current learning rate: 0.00985
2024-11-29 17:58:19.760436: Validation loss did not improve from -0.45787. Patience: 2/50
2024-11-29 17:58:19.761297: train_loss -0.4802
2024-11-29 17:58:19.762118: val_loss -0.4283
2024-11-29 17:58:19.763313: Pseudo dice [0.668]
2024-11-29 17:58:19.764167: Epoch time: 85.89 s
2024-11-29 17:58:19.765207: Yayy! New best EMA pseudo Dice: 0.6136
2024-11-29 17:58:21.834994: 
2024-11-29 17:58:21.836930: Epoch 18
2024-11-29 17:58:21.838102: Current learning rate: 0.00984
2024-11-29 17:59:47.809572: Validation loss did not improve from -0.45787. Patience: 3/50
2024-11-29 17:59:47.810702: train_loss -0.4734
2024-11-29 17:59:47.811613: val_loss -0.4137
2024-11-29 17:59:47.812407: Pseudo dice [0.6415]
2024-11-29 17:59:47.813179: Epoch time: 85.98 s
2024-11-29 17:59:47.814100: Yayy! New best EMA pseudo Dice: 0.6164
2024-11-29 17:59:49.418217: 
2024-11-29 17:59:49.420187: Epoch 19
2024-11-29 17:59:49.421396: Current learning rate: 0.00983
2024-11-29 18:01:15.386207: Validation loss did not improve from -0.45787. Patience: 4/50
2024-11-29 18:01:15.387408: train_loss -0.4889
2024-11-29 18:01:15.388475: val_loss -0.4422
2024-11-29 18:01:15.389159: Pseudo dice [0.6677]
2024-11-29 18:01:15.389851: Epoch time: 85.97 s
2024-11-29 18:01:15.748626: Yayy! New best EMA pseudo Dice: 0.6215
2024-11-29 18:01:17.377646: 
2024-11-29 18:01:17.379624: Epoch 20
2024-11-29 18:01:17.380547: Current learning rate: 0.00982
2024-11-29 18:02:43.291269: Validation loss improved from -0.45787 to -0.46400! Patience: 4/50
2024-11-29 18:02:43.292320: train_loss -0.5002
2024-11-29 18:02:43.293282: val_loss -0.464
2024-11-29 18:02:43.294010: Pseudo dice [0.6802]
2024-11-29 18:02:43.294750: Epoch time: 85.92 s
2024-11-29 18:02:43.295362: Yayy! New best EMA pseudo Dice: 0.6274
2024-11-29 18:02:44.917856: 
2024-11-29 18:02:44.919312: Epoch 21
2024-11-29 18:02:44.920046: Current learning rate: 0.00981
2024-11-29 18:04:10.851060: Validation loss did not improve from -0.46400. Patience: 1/50
2024-11-29 18:04:10.851839: train_loss -0.5018
2024-11-29 18:04:10.852635: val_loss -0.4505
2024-11-29 18:04:10.853368: Pseudo dice [0.6825]
2024-11-29 18:04:10.854045: Epoch time: 85.93 s
2024-11-29 18:04:10.854784: Yayy! New best EMA pseudo Dice: 0.6329
2024-11-29 18:04:12.417015: 
2024-11-29 18:04:12.418354: Epoch 22
2024-11-29 18:04:12.419013: Current learning rate: 0.0098
2024-11-29 18:05:38.510713: Validation loss did not improve from -0.46400. Patience: 2/50
2024-11-29 18:05:38.511571: train_loss -0.4946
2024-11-29 18:05:38.512576: val_loss -0.4554
2024-11-29 18:05:38.513493: Pseudo dice [0.6733]
2024-11-29 18:05:38.514402: Epoch time: 86.1 s
2024-11-29 18:05:38.515154: Yayy! New best EMA pseudo Dice: 0.6369
2024-11-29 18:05:40.090702: 
2024-11-29 18:05:40.091715: Epoch 23
2024-11-29 18:05:40.092913: Current learning rate: 0.00979
2024-11-29 18:07:05.987280: Validation loss improved from -0.46400 to -0.49381! Patience: 2/50
2024-11-29 18:07:05.988327: train_loss -0.507
2024-11-29 18:07:05.989314: val_loss -0.4938
2024-11-29 18:07:05.989979: Pseudo dice [0.6972]
2024-11-29 18:07:05.990756: Epoch time: 85.9 s
2024-11-29 18:07:05.991590: Yayy! New best EMA pseudo Dice: 0.643
2024-11-29 18:07:07.506614: 
2024-11-29 18:07:07.508457: Epoch 24
2024-11-29 18:07:07.509488: Current learning rate: 0.00978
2024-11-29 18:08:33.427706: Validation loss improved from -0.49381 to -0.50299! Patience: 0/50
2024-11-29 18:08:33.429162: train_loss -0.5206
2024-11-29 18:08:33.430486: val_loss -0.503
2024-11-29 18:08:33.431196: Pseudo dice [0.7068]
2024-11-29 18:08:33.431832: Epoch time: 85.92 s
2024-11-29 18:08:33.782694: Yayy! New best EMA pseudo Dice: 0.6494
2024-11-29 18:08:35.396121: 
2024-11-29 18:08:35.398782: Epoch 25
2024-11-29 18:08:35.399987: Current learning rate: 0.00977
2024-11-29 18:10:01.429327: Validation loss did not improve from -0.50299. Patience: 1/50
2024-11-29 18:10:01.430232: train_loss -0.5237
2024-11-29 18:10:01.431412: val_loss -0.4648
2024-11-29 18:10:01.432563: Pseudo dice [0.6801]
2024-11-29 18:10:01.433592: Epoch time: 86.03 s
2024-11-29 18:10:01.434631: Yayy! New best EMA pseudo Dice: 0.6524
2024-11-29 18:10:03.007365: 
2024-11-29 18:10:03.008788: Epoch 26
2024-11-29 18:10:03.009577: Current learning rate: 0.00977
2024-11-29 18:11:28.930283: Validation loss did not improve from -0.50299. Patience: 2/50
2024-11-29 18:11:28.931159: train_loss -0.5428
2024-11-29 18:11:28.932188: val_loss -0.4772
2024-11-29 18:11:28.933173: Pseudo dice [0.6902]
2024-11-29 18:11:28.934050: Epoch time: 85.92 s
2024-11-29 18:11:28.934935: Yayy! New best EMA pseudo Dice: 0.6562
2024-11-29 18:11:30.581386: 
2024-11-29 18:11:30.582433: Epoch 27
2024-11-29 18:11:30.583558: Current learning rate: 0.00976
2024-11-29 18:12:56.490520: Validation loss did not improve from -0.50299. Patience: 3/50
2024-11-29 18:12:56.491626: train_loss -0.517
2024-11-29 18:12:56.492507: val_loss -0.386
2024-11-29 18:12:56.493443: Pseudo dice [0.6507]
2024-11-29 18:12:56.494180: Epoch time: 85.91 s
2024-11-29 18:12:58.070648: 
2024-11-29 18:12:58.072089: Epoch 28
2024-11-29 18:12:58.073116: Current learning rate: 0.00975
2024-11-29 18:14:24.005619: Validation loss did not improve from -0.50299. Patience: 4/50
2024-11-29 18:14:24.006886: train_loss -0.52
2024-11-29 18:14:24.007853: val_loss -0.4973
2024-11-29 18:14:24.008466: Pseudo dice [0.6983]
2024-11-29 18:14:24.009082: Epoch time: 85.94 s
2024-11-29 18:14:24.009923: Yayy! New best EMA pseudo Dice: 0.6599
2024-11-29 18:14:25.613137: 
2024-11-29 18:14:25.617544: Epoch 29
2024-11-29 18:14:25.619270: Current learning rate: 0.00974
2024-11-29 18:15:51.509847: Validation loss did not improve from -0.50299. Patience: 5/50
2024-11-29 18:15:51.510914: train_loss -0.5425
2024-11-29 18:15:51.512028: val_loss -0.4726
2024-11-29 18:15:51.512748: Pseudo dice [0.6919]
2024-11-29 18:15:51.513589: Epoch time: 85.9 s
2024-11-29 18:15:51.874959: Yayy! New best EMA pseudo Dice: 0.6631
2024-11-29 18:15:53.453369: 
2024-11-29 18:15:53.455330: Epoch 30
2024-11-29 18:15:53.456182: Current learning rate: 0.00973
2024-11-29 18:17:19.334443: Validation loss did not improve from -0.50299. Patience: 6/50
2024-11-29 18:17:19.335485: train_loss -0.528
2024-11-29 18:17:19.336396: val_loss -0.4692
2024-11-29 18:17:19.337462: Pseudo dice [0.6895]
2024-11-29 18:17:19.338576: Epoch time: 85.88 s
2024-11-29 18:17:19.339406: Yayy! New best EMA pseudo Dice: 0.6658
2024-11-29 18:17:20.943788: 
2024-11-29 18:17:20.945277: Epoch 31
2024-11-29 18:17:20.946045: Current learning rate: 0.00972
2024-11-29 18:18:46.822008: Validation loss improved from -0.50299 to -0.50978! Patience: 6/50
2024-11-29 18:18:46.823008: train_loss -0.5265
2024-11-29 18:18:46.823950: val_loss -0.5098
2024-11-29 18:18:46.824769: Pseudo dice [0.7089]
2024-11-29 18:18:46.825402: Epoch time: 85.88 s
2024-11-29 18:18:46.826103: Yayy! New best EMA pseudo Dice: 0.6701
2024-11-29 18:18:48.393963: 
2024-11-29 18:18:48.395583: Epoch 32
2024-11-29 18:18:48.396547: Current learning rate: 0.00971
2024-11-29 18:20:14.398738: Validation loss did not improve from -0.50978. Patience: 1/50
2024-11-29 18:20:14.399776: train_loss -0.5309
2024-11-29 18:20:14.400644: val_loss -0.5038
2024-11-29 18:20:14.401300: Pseudo dice [0.7014]
2024-11-29 18:20:14.402136: Epoch time: 86.01 s
2024-11-29 18:20:14.402959: Yayy! New best EMA pseudo Dice: 0.6732
2024-11-29 18:20:15.972751: 
2024-11-29 18:20:15.974378: Epoch 33
2024-11-29 18:20:15.975616: Current learning rate: 0.0097
2024-11-29 18:21:41.864724: Validation loss improved from -0.50978 to -0.52144! Patience: 1/50
2024-11-29 18:21:41.865522: train_loss -0.5317
2024-11-29 18:21:41.866420: val_loss -0.5214
2024-11-29 18:21:41.867127: Pseudo dice [0.7218]
2024-11-29 18:21:41.867734: Epoch time: 85.89 s
2024-11-29 18:21:41.868331: Yayy! New best EMA pseudo Dice: 0.6781
2024-11-29 18:21:43.476588: 
2024-11-29 18:21:43.478429: Epoch 34
2024-11-29 18:21:43.479184: Current learning rate: 0.00969
2024-11-29 18:23:09.348637: Validation loss did not improve from -0.52144. Patience: 1/50
2024-11-29 18:23:09.349677: train_loss -0.5371
2024-11-29 18:23:09.351017: val_loss -0.4934
2024-11-29 18:23:09.352159: Pseudo dice [0.708]
2024-11-29 18:23:09.353232: Epoch time: 85.87 s
2024-11-29 18:23:09.703500: Yayy! New best EMA pseudo Dice: 0.6811
2024-11-29 18:23:11.315559: 
2024-11-29 18:23:11.317666: Epoch 35
2024-11-29 18:23:11.318653: Current learning rate: 0.00968
2024-11-29 18:24:37.286500: Validation loss did not improve from -0.52144. Patience: 2/50
2024-11-29 18:24:37.287838: train_loss -0.5324
2024-11-29 18:24:37.288865: val_loss -0.5194
2024-11-29 18:24:37.289683: Pseudo dice [0.7089]
2024-11-29 18:24:37.290385: Epoch time: 85.97 s
2024-11-29 18:24:37.291130: Yayy! New best EMA pseudo Dice: 0.6838
2024-11-29 18:24:38.905674: 
2024-11-29 18:24:38.907403: Epoch 36
2024-11-29 18:24:38.908220: Current learning rate: 0.00968
2024-11-29 18:26:04.834585: Validation loss did not improve from -0.52144. Patience: 3/50
2024-11-29 18:26:04.835480: train_loss -0.5484
2024-11-29 18:26:04.836458: val_loss -0.499
2024-11-29 18:26:04.837250: Pseudo dice [0.6911]
2024-11-29 18:26:04.837930: Epoch time: 85.93 s
2024-11-29 18:26:04.838658: Yayy! New best EMA pseudo Dice: 0.6846
2024-11-29 18:26:06.454712: 
2024-11-29 18:26:06.456630: Epoch 37
2024-11-29 18:26:06.457872: Current learning rate: 0.00967
2024-11-29 18:27:32.362066: Validation loss did not improve from -0.52144. Patience: 4/50
2024-11-29 18:27:32.363302: train_loss -0.5572
2024-11-29 18:27:32.364096: val_loss -0.4868
2024-11-29 18:27:32.364833: Pseudo dice [0.6891]
2024-11-29 18:27:32.365500: Epoch time: 85.91 s
2024-11-29 18:27:32.366153: Yayy! New best EMA pseudo Dice: 0.685
2024-11-29 18:27:33.997548: 
2024-11-29 18:27:33.999061: Epoch 38
2024-11-29 18:27:34.000012: Current learning rate: 0.00966
2024-11-29 18:28:59.923739: Validation loss did not improve from -0.52144. Patience: 5/50
2024-11-29 18:28:59.925170: train_loss -0.5515
2024-11-29 18:28:59.927018: val_loss -0.4936
2024-11-29 18:28:59.927964: Pseudo dice [0.7008]
2024-11-29 18:28:59.928742: Epoch time: 85.93 s
2024-11-29 18:28:59.929579: Yayy! New best EMA pseudo Dice: 0.6866
2024-11-29 18:29:01.902469: 
2024-11-29 18:29:01.904356: Epoch 39
2024-11-29 18:29:01.905395: Current learning rate: 0.00965
2024-11-29 18:30:27.980483: Validation loss did not improve from -0.52144. Patience: 6/50
2024-11-29 18:30:27.981865: train_loss -0.5469
2024-11-29 18:30:27.982791: val_loss -0.4995
2024-11-29 18:30:27.983770: Pseudo dice [0.7068]
2024-11-29 18:30:27.984993: Epoch time: 86.08 s
2024-11-29 18:30:28.336268: Yayy! New best EMA pseudo Dice: 0.6886
2024-11-29 18:30:29.996027: 
2024-11-29 18:30:29.997821: Epoch 40
2024-11-29 18:30:29.998852: Current learning rate: 0.00964
2024-11-29 18:31:55.872927: Validation loss did not improve from -0.52144. Patience: 7/50
2024-11-29 18:31:55.874080: train_loss -0.5629
2024-11-29 18:31:55.875037: val_loss -0.5008
2024-11-29 18:31:55.875766: Pseudo dice [0.6982]
2024-11-29 18:31:55.876564: Epoch time: 85.88 s
2024-11-29 18:31:55.877472: Yayy! New best EMA pseudo Dice: 0.6896
2024-11-29 18:31:57.560139: 
2024-11-29 18:31:57.561056: Epoch 41
2024-11-29 18:31:57.562002: Current learning rate: 0.00963
2024-11-29 18:33:23.487075: Validation loss did not improve from -0.52144. Patience: 8/50
2024-11-29 18:33:23.488353: train_loss -0.5578
2024-11-29 18:33:23.489463: val_loss -0.4714
2024-11-29 18:33:23.490287: Pseudo dice [0.6913]
2024-11-29 18:33:23.491028: Epoch time: 85.93 s
2024-11-29 18:33:23.491740: Yayy! New best EMA pseudo Dice: 0.6897
2024-11-29 18:33:25.108944: 
2024-11-29 18:33:25.109986: Epoch 42
2024-11-29 18:33:25.110738: Current learning rate: 0.00962
2024-11-29 18:34:51.093140: Validation loss did not improve from -0.52144. Patience: 9/50
2024-11-29 18:34:51.095846: train_loss -0.5655
2024-11-29 18:34:51.096614: val_loss -0.5117
2024-11-29 18:34:51.097523: Pseudo dice [0.711]
2024-11-29 18:34:51.098616: Epoch time: 85.99 s
2024-11-29 18:34:51.099437: Yayy! New best EMA pseudo Dice: 0.6919
2024-11-29 18:34:52.718059: 
2024-11-29 18:34:52.719933: Epoch 43
2024-11-29 18:34:52.720726: Current learning rate: 0.00961
2024-11-29 18:36:18.592292: Validation loss improved from -0.52144 to -0.52397! Patience: 9/50
2024-11-29 18:36:18.594082: train_loss -0.5657
2024-11-29 18:36:18.594883: val_loss -0.524
2024-11-29 18:36:18.595561: Pseudo dice [0.7246]
2024-11-29 18:36:18.596397: Epoch time: 85.88 s
2024-11-29 18:36:18.597113: Yayy! New best EMA pseudo Dice: 0.6951
2024-11-29 18:36:20.164949: 
2024-11-29 18:36:20.167726: Epoch 44
2024-11-29 18:36:20.169666: Current learning rate: 0.0096
2024-11-29 18:37:46.159032: Validation loss did not improve from -0.52397. Patience: 1/50
2024-11-29 18:37:46.159876: train_loss -0.5607
2024-11-29 18:37:46.160566: val_loss -0.5043
2024-11-29 18:37:46.161609: Pseudo dice [0.7039]
2024-11-29 18:37:46.162673: Epoch time: 86.0 s
2024-11-29 18:37:46.504026: Yayy! New best EMA pseudo Dice: 0.696
2024-11-29 18:37:48.102820: 
2024-11-29 18:37:48.106754: Epoch 45
2024-11-29 18:37:48.107709: Current learning rate: 0.00959
2024-11-29 18:39:14.159966: Validation loss did not improve from -0.52397. Patience: 2/50
2024-11-29 18:39:14.161321: train_loss -0.5615
2024-11-29 18:39:14.162323: val_loss -0.465
2024-11-29 18:39:14.163208: Pseudo dice [0.6907]
2024-11-29 18:39:14.164574: Epoch time: 86.06 s
2024-11-29 18:39:15.379370: 
2024-11-29 18:39:15.381091: Epoch 46
2024-11-29 18:39:15.382417: Current learning rate: 0.00959
2024-11-29 18:40:41.232535: Validation loss improved from -0.52397 to -0.52558! Patience: 2/50
2024-11-29 18:40:41.233566: train_loss -0.5743
2024-11-29 18:40:41.234399: val_loss -0.5256
2024-11-29 18:40:41.235123: Pseudo dice [0.7229]
2024-11-29 18:40:41.236048: Epoch time: 85.86 s
2024-11-29 18:40:41.236809: Yayy! New best EMA pseudo Dice: 0.6982
2024-11-29 18:40:42.861968: 
2024-11-29 18:40:42.863922: Epoch 47
2024-11-29 18:40:42.864639: Current learning rate: 0.00958
2024-11-29 18:42:08.750532: Validation loss did not improve from -0.52558. Patience: 1/50
2024-11-29 18:42:08.751796: train_loss -0.5641
2024-11-29 18:42:08.753146: val_loss -0.4788
2024-11-29 18:42:08.753912: Pseudo dice [0.6954]
2024-11-29 18:42:08.754817: Epoch time: 85.89 s
2024-11-29 18:42:09.948974: 
2024-11-29 18:42:09.950865: Epoch 48
2024-11-29 18:42:09.951885: Current learning rate: 0.00957
2024-11-29 18:43:35.853153: Validation loss did not improve from -0.52558. Patience: 2/50
2024-11-29 18:43:35.854179: train_loss -0.5679
2024-11-29 18:43:35.855054: val_loss -0.5108
2024-11-29 18:43:35.855724: Pseudo dice [0.7151]
2024-11-29 18:43:35.856574: Epoch time: 85.91 s
2024-11-29 18:43:35.857187: Yayy! New best EMA pseudo Dice: 0.6997
2024-11-29 18:43:37.978347: 
2024-11-29 18:43:37.980085: Epoch 49
2024-11-29 18:43:37.980953: Current learning rate: 0.00956
2024-11-29 18:45:03.841849: Validation loss did not improve from -0.52558. Patience: 3/50
2024-11-29 18:45:03.842971: train_loss -0.5802
2024-11-29 18:45:03.843870: val_loss -0.4947
2024-11-29 18:45:03.844686: Pseudo dice [0.7056]
2024-11-29 18:45:03.845469: Epoch time: 85.87 s
2024-11-29 18:45:04.220573: Yayy! New best EMA pseudo Dice: 0.7002
2024-11-29 18:45:05.850015: 
2024-11-29 18:45:05.851475: Epoch 50
2024-11-29 18:45:05.852439: Current learning rate: 0.00955
2024-11-29 18:46:31.659881: Validation loss improved from -0.52558 to -0.53324! Patience: 3/50
2024-11-29 18:46:31.661003: train_loss -0.582
2024-11-29 18:46:31.661943: val_loss -0.5332
2024-11-29 18:46:31.662507: Pseudo dice [0.7255]
2024-11-29 18:46:31.663397: Epoch time: 85.81 s
2024-11-29 18:46:31.663999: Yayy! New best EMA pseudo Dice: 0.7028
2024-11-29 18:46:33.271588: 
2024-11-29 18:46:33.273399: Epoch 51
2024-11-29 18:46:33.274317: Current learning rate: 0.00954
2024-11-29 18:47:59.172950: Validation loss did not improve from -0.53324. Patience: 1/50
2024-11-29 18:47:59.173895: train_loss -0.5869
2024-11-29 18:47:59.174673: val_loss -0.526
2024-11-29 18:47:59.175384: Pseudo dice [0.7284]
2024-11-29 18:47:59.176123: Epoch time: 85.9 s
2024-11-29 18:47:59.176753: Yayy! New best EMA pseudo Dice: 0.7053
2024-11-29 18:48:00.756485: 
2024-11-29 18:48:00.757758: Epoch 52
2024-11-29 18:48:00.758622: Current learning rate: 0.00953
2024-11-29 18:49:26.701786: Validation loss improved from -0.53324 to -0.55283! Patience: 1/50
2024-11-29 18:49:26.702869: train_loss -0.5931
2024-11-29 18:49:26.703892: val_loss -0.5528
2024-11-29 18:49:26.704670: Pseudo dice [0.7336]
2024-11-29 18:49:26.705477: Epoch time: 85.95 s
2024-11-29 18:49:26.706074: Yayy! New best EMA pseudo Dice: 0.7082
2024-11-29 18:49:28.266339: 
2024-11-29 18:49:28.268752: Epoch 53
2024-11-29 18:49:28.269792: Current learning rate: 0.00952
2024-11-29 18:50:54.185771: Validation loss did not improve from -0.55283. Patience: 1/50
2024-11-29 18:50:54.186688: train_loss -0.5973
2024-11-29 18:50:54.187554: val_loss -0.5126
2024-11-29 18:50:54.188464: Pseudo dice [0.714]
2024-11-29 18:50:54.189350: Epoch time: 85.92 s
2024-11-29 18:50:54.190089: Yayy! New best EMA pseudo Dice: 0.7087
2024-11-29 18:50:55.777286: 
2024-11-29 18:50:55.778937: Epoch 54
2024-11-29 18:50:55.780202: Current learning rate: 0.00951
2024-11-29 18:52:21.729660: Validation loss did not improve from -0.55283. Patience: 2/50
2024-11-29 18:52:21.730671: train_loss -0.5895
2024-11-29 18:52:21.731382: val_loss -0.4754
2024-11-29 18:52:21.732116: Pseudo dice [0.6968]
2024-11-29 18:52:21.732734: Epoch time: 85.95 s
2024-11-29 18:52:23.323357: 
2024-11-29 18:52:23.325642: Epoch 55
2024-11-29 18:52:23.326970: Current learning rate: 0.0095
2024-11-29 18:53:49.328505: Validation loss did not improve from -0.55283. Patience: 3/50
2024-11-29 18:53:49.329484: train_loss -0.5955
2024-11-29 18:53:49.330374: val_loss -0.5244
2024-11-29 18:53:49.331062: Pseudo dice [0.7121]
2024-11-29 18:53:49.331792: Epoch time: 86.01 s
2024-11-29 18:53:50.578512: 
2024-11-29 18:53:50.579902: Epoch 56
2024-11-29 18:53:50.581220: Current learning rate: 0.00949
2024-11-29 18:55:16.495673: Validation loss did not improve from -0.55283. Patience: 4/50
2024-11-29 18:55:16.496407: train_loss -0.6011
2024-11-29 18:55:16.497112: val_loss -0.5092
2024-11-29 18:55:16.497880: Pseudo dice [0.7175]
2024-11-29 18:55:16.498626: Epoch time: 85.92 s
2024-11-29 18:55:16.499417: Yayy! New best EMA pseudo Dice: 0.709
2024-11-29 18:55:18.118231: 
2024-11-29 18:55:18.120080: Epoch 57
2024-11-29 18:55:18.121093: Current learning rate: 0.00949
2024-11-29 18:56:43.979912: Validation loss did not improve from -0.55283. Patience: 5/50
2024-11-29 18:56:43.981142: train_loss -0.6075
2024-11-29 18:56:43.982038: val_loss -0.4949
2024-11-29 18:56:43.983341: Pseudo dice [0.6996]
2024-11-29 18:56:43.984310: Epoch time: 85.86 s
2024-11-29 18:56:45.203309: 
2024-11-29 18:56:45.205073: Epoch 58
2024-11-29 18:56:45.206252: Current learning rate: 0.00948
2024-11-29 18:58:11.137781: Validation loss improved from -0.55283 to -0.55443! Patience: 5/50
2024-11-29 18:58:11.138834: train_loss -0.6107
2024-11-29 18:58:11.139625: val_loss -0.5544
2024-11-29 18:58:11.140423: Pseudo dice [0.7363]
2024-11-29 18:58:11.141079: Epoch time: 85.94 s
2024-11-29 18:58:11.141816: Yayy! New best EMA pseudo Dice: 0.7109
2024-11-29 18:58:12.760966: 
2024-11-29 18:58:12.762335: Epoch 59
2024-11-29 18:58:12.763069: Current learning rate: 0.00947
2024-11-29 18:59:38.769666: Validation loss did not improve from -0.55443. Patience: 1/50
2024-11-29 18:59:38.770896: train_loss -0.6137
2024-11-29 18:59:38.772319: val_loss -0.5381
2024-11-29 18:59:38.773390: Pseudo dice [0.7259]
2024-11-29 18:59:38.774630: Epoch time: 86.01 s
2024-11-29 18:59:39.125779: Yayy! New best EMA pseudo Dice: 0.7124
2024-11-29 18:59:41.034857: 
2024-11-29 18:59:41.036832: Epoch 60
2024-11-29 18:59:41.037704: Current learning rate: 0.00946
2024-11-29 19:01:06.958737: Validation loss did not improve from -0.55443. Patience: 2/50
2024-11-29 19:01:06.959849: train_loss -0.5877
2024-11-29 19:01:06.960732: val_loss -0.4897
2024-11-29 19:01:06.961557: Pseudo dice [0.7036]
2024-11-29 19:01:06.962355: Epoch time: 85.93 s
2024-11-29 19:01:08.184775: 
2024-11-29 19:01:08.185847: Epoch 61
2024-11-29 19:01:08.186631: Current learning rate: 0.00945
2024-11-29 19:02:34.052297: Validation loss did not improve from -0.55443. Patience: 3/50
2024-11-29 19:02:34.053236: train_loss -0.6189
2024-11-29 19:02:34.054317: val_loss -0.4995
2024-11-29 19:02:34.055094: Pseudo dice [0.7073]
2024-11-29 19:02:34.055724: Epoch time: 85.87 s
2024-11-29 19:02:35.276742: 
2024-11-29 19:02:35.278403: Epoch 62
2024-11-29 19:02:35.279195: Current learning rate: 0.00944
2024-11-29 19:04:01.310774: Validation loss did not improve from -0.55443. Patience: 4/50
2024-11-29 19:04:01.311923: train_loss -0.6085
2024-11-29 19:04:01.312814: val_loss -0.506
2024-11-29 19:04:01.313613: Pseudo dice [0.7105]
2024-11-29 19:04:01.314535: Epoch time: 86.04 s
2024-11-29 19:04:02.545001: 
2024-11-29 19:04:02.546577: Epoch 63
2024-11-29 19:04:02.547436: Current learning rate: 0.00943
2024-11-29 19:05:28.436336: Validation loss did not improve from -0.55443. Patience: 5/50
2024-11-29 19:05:28.437793: train_loss -0.6212
2024-11-29 19:05:28.438731: val_loss -0.5532
2024-11-29 19:05:28.439548: Pseudo dice [0.7372]
2024-11-29 19:05:28.440444: Epoch time: 85.89 s
2024-11-29 19:05:28.441270: Yayy! New best EMA pseudo Dice: 0.7136
2024-11-29 19:05:30.038597: 
2024-11-29 19:05:30.040332: Epoch 64
2024-11-29 19:05:30.041094: Current learning rate: 0.00942
2024-11-29 19:06:55.973931: Validation loss did not improve from -0.55443. Patience: 6/50
2024-11-29 19:06:55.974867: train_loss -0.6168
2024-11-29 19:06:55.975929: val_loss -0.4985
2024-11-29 19:06:55.976900: Pseudo dice [0.7078]
2024-11-29 19:06:55.977770: Epoch time: 85.94 s
2024-11-29 19:06:57.931475: 
2024-11-29 19:06:57.933083: Epoch 65
2024-11-29 19:06:57.934285: Current learning rate: 0.00941
2024-11-29 19:08:23.905019: Validation loss did not improve from -0.55443. Patience: 7/50
2024-11-29 19:08:23.906478: train_loss -0.6118
2024-11-29 19:08:23.907351: val_loss -0.5169
2024-11-29 19:08:23.908244: Pseudo dice [0.7097]
2024-11-29 19:08:23.909132: Epoch time: 85.98 s
2024-11-29 19:08:25.147453: 
2024-11-29 19:08:25.149209: Epoch 66
2024-11-29 19:08:25.150149: Current learning rate: 0.0094
2024-11-29 19:09:51.049200: Validation loss did not improve from -0.55443. Patience: 8/50
2024-11-29 19:09:51.049963: train_loss -0.6165
2024-11-29 19:09:51.050947: val_loss -0.5252
2024-11-29 19:09:51.051951: Pseudo dice [0.7137]
2024-11-29 19:09:51.052660: Epoch time: 85.9 s
2024-11-29 19:09:52.289804: 
2024-11-29 19:09:52.291636: Epoch 67
2024-11-29 19:09:52.292838: Current learning rate: 0.00939
2024-11-29 19:11:18.277630: Validation loss did not improve from -0.55443. Patience: 9/50
2024-11-29 19:11:18.278771: train_loss -0.6084
2024-11-29 19:11:18.279763: val_loss -0.5109
2024-11-29 19:11:18.280548: Pseudo dice [0.7179]
2024-11-29 19:11:18.281313: Epoch time: 85.99 s
2024-11-29 19:11:19.562272: 
2024-11-29 19:11:19.563414: Epoch 68
2024-11-29 19:11:19.564422: Current learning rate: 0.00939
2024-11-29 19:12:45.516363: Validation loss did not improve from -0.55443. Patience: 10/50
2024-11-29 19:12:45.517755: train_loss -0.6247
2024-11-29 19:12:45.519128: val_loss -0.5365
2024-11-29 19:12:45.520309: Pseudo dice [0.721]
2024-11-29 19:12:45.521476: Epoch time: 85.96 s
2024-11-29 19:12:45.522537: Yayy! New best EMA pseudo Dice: 0.7141
2024-11-29 19:12:47.218467: 
2024-11-29 19:12:47.220414: Epoch 69
2024-11-29 19:12:47.221802: Current learning rate: 0.00938
2024-11-29 19:14:13.227227: Validation loss did not improve from -0.55443. Patience: 11/50
2024-11-29 19:14:13.228374: train_loss -0.6297
2024-11-29 19:14:13.229350: val_loss -0.528
2024-11-29 19:14:13.230074: Pseudo dice [0.7253]
2024-11-29 19:14:13.230894: Epoch time: 86.01 s
2024-11-29 19:14:13.653810: Yayy! New best EMA pseudo Dice: 0.7152
2024-11-29 19:14:15.292389: 
2024-11-29 19:14:15.294332: Epoch 70
2024-11-29 19:14:15.295459: Current learning rate: 0.00937
2024-11-29 19:15:41.216942: Validation loss did not improve from -0.55443. Patience: 12/50
2024-11-29 19:15:41.217958: train_loss -0.623
2024-11-29 19:15:41.218777: val_loss -0.5193
2024-11-29 19:15:41.219535: Pseudo dice [0.7153]
2024-11-29 19:15:41.220265: Epoch time: 85.93 s
2024-11-29 19:15:41.220927: Yayy! New best EMA pseudo Dice: 0.7152
2024-11-29 19:15:43.210655: 
2024-11-29 19:15:43.212215: Epoch 71
2024-11-29 19:15:43.213145: Current learning rate: 0.00936
2024-11-29 19:17:09.171107: Validation loss did not improve from -0.55443. Patience: 13/50
2024-11-29 19:17:09.172729: train_loss -0.6283
2024-11-29 19:17:09.174330: val_loss -0.5177
2024-11-29 19:17:09.175511: Pseudo dice [0.7153]
2024-11-29 19:17:09.176408: Epoch time: 85.96 s
2024-11-29 19:17:09.177180: Yayy! New best EMA pseudo Dice: 0.7152
2024-11-29 19:17:10.834498: 
2024-11-29 19:17:10.836106: Epoch 72
2024-11-29 19:17:10.836914: Current learning rate: 0.00935
2024-11-29 19:18:36.883651: Validation loss did not improve from -0.55443. Patience: 14/50
2024-11-29 19:18:36.884791: train_loss -0.6429
2024-11-29 19:18:36.886381: val_loss -0.5089
2024-11-29 19:18:36.887164: Pseudo dice [0.7054]
2024-11-29 19:18:36.888038: Epoch time: 86.05 s
2024-11-29 19:18:38.189296: 
2024-11-29 19:18:38.190453: Epoch 73
2024-11-29 19:18:38.191076: Current learning rate: 0.00934
2024-11-29 19:20:04.082282: Validation loss did not improve from -0.55443. Patience: 15/50
2024-11-29 19:20:04.083183: train_loss -0.6352
2024-11-29 19:20:04.084109: val_loss -0.5476
2024-11-29 19:20:04.084960: Pseudo dice [0.7289]
2024-11-29 19:20:04.085879: Epoch time: 85.9 s
2024-11-29 19:20:04.086582: Yayy! New best EMA pseudo Dice: 0.7157
2024-11-29 19:20:05.738786: 
2024-11-29 19:20:05.740706: Epoch 74
2024-11-29 19:20:05.741339: Current learning rate: 0.00933
2024-11-29 19:21:31.645049: Validation loss did not improve from -0.55443. Patience: 16/50
2024-11-29 19:21:31.646096: train_loss -0.6111
2024-11-29 19:21:31.647132: val_loss -0.5112
2024-11-29 19:21:31.648004: Pseudo dice [0.7123]
2024-11-29 19:21:31.648842: Epoch time: 85.91 s
2024-11-29 19:21:33.280968: 
2024-11-29 19:21:33.283816: Epoch 75
2024-11-29 19:21:33.285326: Current learning rate: 0.00932
2024-11-29 19:22:59.310793: Validation loss did not improve from -0.55443. Patience: 17/50
2024-11-29 19:22:59.311610: train_loss -0.6065
2024-11-29 19:22:59.312586: val_loss -0.5281
2024-11-29 19:22:59.313670: Pseudo dice [0.7194]
2024-11-29 19:22:59.314678: Epoch time: 86.03 s
2024-11-29 19:22:59.315672: Yayy! New best EMA pseudo Dice: 0.7158
2024-11-29 19:23:00.986699: 
2024-11-29 19:23:00.987920: Epoch 76
2024-11-29 19:23:00.988731: Current learning rate: 0.00931
2024-11-29 19:24:26.881944: Validation loss improved from -0.55443 to -0.55961! Patience: 17/50
2024-11-29 19:24:26.882837: train_loss -0.6238
2024-11-29 19:24:26.883736: val_loss -0.5596
2024-11-29 19:24:26.884407: Pseudo dice [0.7397]
2024-11-29 19:24:26.885240: Epoch time: 85.9 s
2024-11-29 19:24:26.885913: Yayy! New best EMA pseudo Dice: 0.7182
2024-11-29 19:24:28.499182: 
2024-11-29 19:24:28.501028: Epoch 77
2024-11-29 19:24:28.501845: Current learning rate: 0.0093
2024-11-29 19:25:54.357076: Validation loss did not improve from -0.55961. Patience: 1/50
2024-11-29 19:25:54.358364: train_loss -0.6306
2024-11-29 19:25:54.359224: val_loss -0.4916
2024-11-29 19:25:54.359992: Pseudo dice [0.7027]
2024-11-29 19:25:54.360806: Epoch time: 85.86 s
2024-11-29 19:25:55.664563: 
2024-11-29 19:25:55.666504: Epoch 78
2024-11-29 19:25:55.667573: Current learning rate: 0.0093
2024-11-29 19:27:21.637586: Validation loss did not improve from -0.55961. Patience: 2/50
2024-11-29 19:27:21.638453: train_loss -0.6096
2024-11-29 19:27:21.639406: val_loss -0.5179
2024-11-29 19:27:21.640023: Pseudo dice [0.7222]
2024-11-29 19:27:21.640647: Epoch time: 85.97 s
2024-11-29 19:27:22.955185: 
2024-11-29 19:27:22.956923: Epoch 79
2024-11-29 19:27:22.958085: Current learning rate: 0.00929
2024-11-29 19:28:49.002748: Validation loss did not improve from -0.55961. Patience: 3/50
2024-11-29 19:28:49.003726: train_loss -0.6256
2024-11-29 19:28:49.004568: val_loss -0.5433
2024-11-29 19:28:49.005289: Pseudo dice [0.7311]
2024-11-29 19:28:49.006359: Epoch time: 86.05 s
2024-11-29 19:28:49.385446: Yayy! New best EMA pseudo Dice: 0.7186
2024-11-29 19:28:51.043207: 
2024-11-29 19:28:51.045486: Epoch 80
2024-11-29 19:28:51.046558: Current learning rate: 0.00928
2024-11-29 19:30:16.933841: Validation loss did not improve from -0.55961. Patience: 4/50
2024-11-29 19:30:16.934783: train_loss -0.6264
2024-11-29 19:30:16.935607: val_loss -0.554
2024-11-29 19:30:16.936349: Pseudo dice [0.7355]
2024-11-29 19:30:16.937232: Epoch time: 85.89 s
2024-11-29 19:30:16.938037: Yayy! New best EMA pseudo Dice: 0.7203
2024-11-29 19:30:18.908698: 
2024-11-29 19:30:18.910177: Epoch 81
2024-11-29 19:30:18.911290: Current learning rate: 0.00927
2024-11-29 19:31:44.820503: Validation loss did not improve from -0.55961. Patience: 5/50
2024-11-29 19:31:44.821390: train_loss -0.6344
2024-11-29 19:31:44.822743: val_loss -0.4687
2024-11-29 19:31:44.823689: Pseudo dice [0.6927]
2024-11-29 19:31:44.824612: Epoch time: 85.91 s
2024-11-29 19:31:46.095070: 
2024-11-29 19:31:46.096478: Epoch 82
2024-11-29 19:31:46.097333: Current learning rate: 0.00926
2024-11-29 19:33:12.073476: Validation loss did not improve from -0.55961. Patience: 6/50
2024-11-29 19:33:12.074601: train_loss -0.6322
2024-11-29 19:33:12.075369: val_loss -0.4735
2024-11-29 19:33:12.076047: Pseudo dice [0.683]
2024-11-29 19:33:12.076659: Epoch time: 85.98 s
2024-11-29 19:33:13.331643: 
2024-11-29 19:33:13.333224: Epoch 83
2024-11-29 19:33:13.333920: Current learning rate: 0.00925
2024-11-29 19:34:39.194087: Validation loss did not improve from -0.55961. Patience: 7/50
2024-11-29 19:34:39.195375: train_loss -0.6376
2024-11-29 19:34:39.196218: val_loss -0.5129
2024-11-29 19:34:39.196976: Pseudo dice [0.7113]
2024-11-29 19:34:39.197728: Epoch time: 85.86 s
2024-11-29 19:34:40.379098: 
2024-11-29 19:34:40.380378: Epoch 84
2024-11-29 19:34:40.381711: Current learning rate: 0.00924
2024-11-29 19:36:06.249525: Validation loss did not improve from -0.55961. Patience: 8/50
2024-11-29 19:36:06.250336: train_loss -0.6297
2024-11-29 19:36:06.251421: val_loss -0.51
2024-11-29 19:36:06.252367: Pseudo dice [0.7054]
2024-11-29 19:36:06.253189: Epoch time: 85.87 s
2024-11-29 19:36:07.805384: 
2024-11-29 19:36:07.807609: Epoch 85
2024-11-29 19:36:07.808832: Current learning rate: 0.00923
2024-11-29 19:37:33.790830: Validation loss did not improve from -0.55961. Patience: 9/50
2024-11-29 19:37:33.791625: train_loss -0.6267
2024-11-29 19:37:33.792830: val_loss -0.5309
2024-11-29 19:37:33.793762: Pseudo dice [0.7265]
2024-11-29 19:37:33.794710: Epoch time: 85.99 s
2024-11-29 19:37:35.086226: 
2024-11-29 19:37:35.087775: Epoch 86
2024-11-29 19:37:35.088956: Current learning rate: 0.00922
2024-11-29 19:39:01.202697: Validation loss did not improve from -0.55961. Patience: 10/50
2024-11-29 19:39:01.204306: train_loss -0.6414
2024-11-29 19:39:01.205575: val_loss -0.4861
2024-11-29 19:39:01.206980: Pseudo dice [0.692]
2024-11-29 19:39:01.207983: Epoch time: 86.12 s
2024-11-29 19:39:02.456267: 
2024-11-29 19:39:02.457857: Epoch 87
2024-11-29 19:39:02.458672: Current learning rate: 0.00921
2024-11-29 19:40:31.875772: Validation loss did not improve from -0.55961. Patience: 11/50
2024-11-29 19:40:31.922121: train_loss -0.6291
2024-11-29 19:40:31.925395: val_loss -0.5522
2024-11-29 19:40:31.926659: Pseudo dice [0.7308]
2024-11-29 19:40:31.927589: Epoch time: 89.46 s
2024-11-29 19:40:34.182639: 
2024-11-29 19:40:34.184208: Epoch 88
2024-11-29 19:40:34.185072: Current learning rate: 0.0092
2024-11-29 19:42:00.675112: Validation loss improved from -0.55961 to -0.56540! Patience: 11/50
2024-11-29 19:42:00.676642: train_loss -0.6416
2024-11-29 19:42:00.677544: val_loss -0.5654
2024-11-29 19:42:00.678355: Pseudo dice [0.7445]
2024-11-29 19:42:00.679075: Epoch time: 86.5 s
2024-11-29 19:42:01.934108: 
2024-11-29 19:42:01.936626: Epoch 89
2024-11-29 19:42:01.937750: Current learning rate: 0.0092
2024-11-29 19:43:28.232547: Validation loss did not improve from -0.56540. Patience: 1/50
2024-11-29 19:43:28.233467: train_loss -0.6526
2024-11-29 19:43:28.234685: val_loss -0.5313
2024-11-29 19:43:28.235658: Pseudo dice [0.7273]
2024-11-29 19:43:28.236711: Epoch time: 86.3 s
2024-11-29 19:43:30.440734: 
2024-11-29 19:43:30.442293: Epoch 90
2024-11-29 19:43:30.443624: Current learning rate: 0.00919
2024-11-29 19:44:56.355417: Validation loss did not improve from -0.56540. Patience: 2/50
2024-11-29 19:44:56.356234: train_loss -0.6519
2024-11-29 19:44:56.357192: val_loss -0.5369
2024-11-29 19:44:56.357890: Pseudo dice [0.7309]
2024-11-29 19:44:56.358483: Epoch time: 85.92 s
2024-11-29 19:45:00.257126: 
2024-11-29 19:45:00.258779: Epoch 91
2024-11-29 19:45:00.259655: Current learning rate: 0.00918
2024-11-29 19:46:26.206737: Validation loss did not improve from -0.56540. Patience: 3/50
2024-11-29 19:46:26.207662: train_loss -0.6369
2024-11-29 19:46:26.208345: val_loss -0.5109
2024-11-29 19:46:26.209035: Pseudo dice [0.7229]
2024-11-29 19:46:26.209735: Epoch time: 85.95 s
2024-11-29 19:46:27.457865: 
2024-11-29 19:46:27.459559: Epoch 92
2024-11-29 19:46:27.460356: Current learning rate: 0.00917
2024-11-29 19:47:53.455074: Validation loss did not improve from -0.56540. Patience: 4/50
2024-11-29 19:47:53.456631: train_loss -0.6493
2024-11-29 19:47:53.458482: val_loss -0.5285
2024-11-29 19:47:53.459799: Pseudo dice [0.7234]
2024-11-29 19:47:53.461227: Epoch time: 86.0 s
2024-11-29 19:47:54.697439: 
2024-11-29 19:47:54.698640: Epoch 93
2024-11-29 19:47:54.699597: Current learning rate: 0.00916
2024-11-29 19:49:20.964459: Validation loss did not improve from -0.56540. Patience: 5/50
2024-11-29 19:49:20.965622: train_loss -0.6528
2024-11-29 19:49:20.966552: val_loss -0.5373
2024-11-29 19:49:20.967249: Pseudo dice [0.7324]
2024-11-29 19:49:20.967929: Epoch time: 86.27 s
2024-11-29 19:49:20.968536: Yayy! New best EMA pseudo Dice: 0.7213
2024-11-29 19:49:22.567934: 
2024-11-29 19:49:22.570157: Epoch 94
2024-11-29 19:49:22.571114: Current learning rate: 0.00915
2024-11-29 19:50:48.468432: Validation loss did not improve from -0.56540. Patience: 6/50
2024-11-29 19:50:48.469431: train_loss -0.6469
2024-11-29 19:50:48.470277: val_loss -0.541
2024-11-29 19:50:48.470967: Pseudo dice [0.7381]
2024-11-29 19:50:48.471763: Epoch time: 85.9 s
2024-11-29 19:50:48.846801: Yayy! New best EMA pseudo Dice: 0.723
2024-11-29 19:50:50.378613: 
2024-11-29 19:50:50.380828: Epoch 95
2024-11-29 19:50:50.381835: Current learning rate: 0.00914
2024-11-29 19:52:16.376769: Validation loss did not improve from -0.56540. Patience: 7/50
2024-11-29 19:52:16.377982: train_loss -0.6422
2024-11-29 19:52:16.378974: val_loss -0.506
2024-11-29 19:52:16.379832: Pseudo dice [0.7141]
2024-11-29 19:52:16.380675: Epoch time: 86.0 s
2024-11-29 19:52:17.581137: 
2024-11-29 19:52:17.582865: Epoch 96
2024-11-29 19:52:17.583671: Current learning rate: 0.00913
2024-11-29 19:53:43.430007: Validation loss did not improve from -0.56540. Patience: 8/50
2024-11-29 19:53:43.430808: train_loss -0.6429
2024-11-29 19:53:43.431757: val_loss -0.5419
2024-11-29 19:53:43.432710: Pseudo dice [0.7332]
2024-11-29 19:53:43.433623: Epoch time: 85.85 s
2024-11-29 19:53:43.434492: Yayy! New best EMA pseudo Dice: 0.7232
2024-11-29 19:53:44.994480: 
2024-11-29 19:53:44.996265: Epoch 97
2024-11-29 19:53:44.997397: Current learning rate: 0.00912
2024-11-29 19:55:10.891199: Validation loss did not improve from -0.56540. Patience: 9/50
2024-11-29 19:55:10.892392: train_loss -0.6458
2024-11-29 19:55:10.893134: val_loss -0.5351
2024-11-29 19:55:10.893800: Pseudo dice [0.7266]
2024-11-29 19:55:10.894498: Epoch time: 85.9 s
2024-11-29 19:55:10.895152: Yayy! New best EMA pseudo Dice: 0.7235
2024-11-29 19:55:12.478204: 
2024-11-29 19:55:12.479825: Epoch 98
2024-11-29 19:55:12.480691: Current learning rate: 0.00911
2024-11-29 19:56:38.436781: Validation loss did not improve from -0.56540. Patience: 10/50
2024-11-29 19:56:38.438258: train_loss -0.6523
2024-11-29 19:56:38.440009: val_loss -0.5001
2024-11-29 19:56:38.440911: Pseudo dice [0.7138]
2024-11-29 19:56:38.442008: Epoch time: 85.96 s
2024-11-29 19:56:39.740914: 
2024-11-29 19:56:39.742764: Epoch 99
2024-11-29 19:56:39.743677: Current learning rate: 0.0091
2024-11-29 19:58:05.661788: Validation loss did not improve from -0.56540. Patience: 11/50
2024-11-29 19:58:05.662730: train_loss -0.6582
2024-11-29 19:58:05.663740: val_loss -0.5124
2024-11-29 19:58:05.664488: Pseudo dice [0.7201]
2024-11-29 19:58:05.665177: Epoch time: 85.92 s
2024-11-29 19:58:07.270007: 
2024-11-29 19:58:07.271898: Epoch 100
2024-11-29 19:58:07.273042: Current learning rate: 0.0091
2024-11-29 19:59:33.158730: Validation loss did not improve from -0.56540. Patience: 12/50
2024-11-29 19:59:33.159662: train_loss -0.6691
2024-11-29 19:59:33.160584: val_loss -0.5408
2024-11-29 19:59:33.161322: Pseudo dice [0.7339]
2024-11-29 19:59:33.161959: Epoch time: 85.89 s
2024-11-29 19:59:34.385449: 
2024-11-29 19:59:34.387251: Epoch 101
2024-11-29 19:59:34.388052: Current learning rate: 0.00909
2024-11-29 20:01:00.316784: Validation loss did not improve from -0.56540. Patience: 13/50
2024-11-29 20:01:00.317679: train_loss -0.662
2024-11-29 20:01:00.318470: val_loss -0.5109
2024-11-29 20:01:00.319321: Pseudo dice [0.7215]
2024-11-29 20:01:00.319960: Epoch time: 85.93 s
2024-11-29 20:01:01.525073: 
2024-11-29 20:01:01.526949: Epoch 102
2024-11-29 20:01:01.528074: Current learning rate: 0.00908
2024-11-29 20:02:27.520049: Validation loss did not improve from -0.56540. Patience: 14/50
2024-11-29 20:02:27.521004: train_loss -0.6543
2024-11-29 20:02:27.521749: val_loss -0.5232
2024-11-29 20:02:27.522474: Pseudo dice [0.7209]
2024-11-29 20:02:27.523196: Epoch time: 86.0 s
2024-11-29 20:02:29.331582: 
2024-11-29 20:02:29.333051: Epoch 103
2024-11-29 20:02:29.333712: Current learning rate: 0.00907
2024-11-29 20:03:55.173040: Validation loss did not improve from -0.56540. Patience: 15/50
2024-11-29 20:03:55.173877: train_loss -0.6688
2024-11-29 20:03:55.174806: val_loss -0.5372
2024-11-29 20:03:55.175678: Pseudo dice [0.7283]
2024-11-29 20:03:55.176348: Epoch time: 85.84 s
2024-11-29 20:03:55.177148: Yayy! New best EMA pseudo Dice: 0.7236
2024-11-29 20:03:56.762527: 
2024-11-29 20:03:56.764408: Epoch 104
2024-11-29 20:03:56.765557: Current learning rate: 0.00906
2024-11-29 20:05:22.691494: Validation loss did not improve from -0.56540. Patience: 16/50
2024-11-29 20:05:22.692641: train_loss -0.6688
2024-11-29 20:05:22.693965: val_loss -0.5171
2024-11-29 20:05:22.694854: Pseudo dice [0.7094]
2024-11-29 20:05:22.695706: Epoch time: 85.93 s
2024-11-29 20:05:24.281665: 
2024-11-29 20:05:24.283434: Epoch 105
2024-11-29 20:05:24.284299: Current learning rate: 0.00905
2024-11-29 20:06:50.325496: Validation loss did not improve from -0.56540. Patience: 17/50
2024-11-29 20:06:50.326890: train_loss -0.6642
2024-11-29 20:06:50.327965: val_loss -0.5412
2024-11-29 20:06:50.328641: Pseudo dice [0.7333]
2024-11-29 20:06:50.329391: Epoch time: 86.05 s
2024-11-29 20:06:51.560566: 
2024-11-29 20:06:51.561995: Epoch 106
2024-11-29 20:06:51.563000: Current learning rate: 0.00904
2024-11-29 20:08:17.486899: Validation loss did not improve from -0.56540. Patience: 18/50
2024-11-29 20:08:17.488160: train_loss -0.6665
2024-11-29 20:08:17.489436: val_loss -0.5198
2024-11-29 20:08:17.490113: Pseudo dice [0.7267]
2024-11-29 20:08:17.490821: Epoch time: 85.93 s
2024-11-29 20:08:17.491418: Yayy! New best EMA pseudo Dice: 0.7236
2024-11-29 20:08:19.059063: 
2024-11-29 20:08:19.060589: Epoch 107
2024-11-29 20:08:19.061602: Current learning rate: 0.00903
2024-11-29 20:09:44.973332: Validation loss did not improve from -0.56540. Patience: 19/50
2024-11-29 20:09:44.974059: train_loss -0.6636
2024-11-29 20:09:44.975036: val_loss -0.5153
2024-11-29 20:09:44.975803: Pseudo dice [0.7233]
2024-11-29 20:09:44.976773: Epoch time: 85.92 s
2024-11-29 20:09:46.199293: 
2024-11-29 20:09:46.201083: Epoch 108
2024-11-29 20:09:46.202080: Current learning rate: 0.00902
2024-11-29 20:11:12.159471: Validation loss did not improve from -0.56540. Patience: 20/50
2024-11-29 20:11:12.160664: train_loss -0.6628
2024-11-29 20:11:12.161407: val_loss -0.5497
2024-11-29 20:11:12.162051: Pseudo dice [0.731]
2024-11-29 20:11:12.162801: Epoch time: 85.96 s
2024-11-29 20:11:12.163736: Yayy! New best EMA pseudo Dice: 0.7243
2024-11-29 20:11:13.764259: 
2024-11-29 20:11:13.766574: Epoch 109
2024-11-29 20:11:13.767294: Current learning rate: 0.00901
2024-11-29 20:12:39.662066: Validation loss did not improve from -0.56540. Patience: 21/50
2024-11-29 20:12:39.662897: train_loss -0.6588
2024-11-29 20:12:39.663862: val_loss -0.5569
2024-11-29 20:12:39.664630: Pseudo dice [0.7398]
2024-11-29 20:12:39.665464: Epoch time: 85.9 s
2024-11-29 20:12:40.057159: Yayy! New best EMA pseudo Dice: 0.7259
2024-11-29 20:12:41.696603: 
2024-11-29 20:12:41.699116: Epoch 110
2024-11-29 20:12:41.700159: Current learning rate: 0.009
2024-11-29 20:14:07.568135: Validation loss did not improve from -0.56540. Patience: 22/50
2024-11-29 20:14:07.569232: train_loss -0.6561
2024-11-29 20:14:07.570193: val_loss -0.5133
2024-11-29 20:14:07.570857: Pseudo dice [0.7137]
2024-11-29 20:14:07.571501: Epoch time: 85.87 s
2024-11-29 20:14:08.788446: 
2024-11-29 20:14:08.790394: Epoch 111
2024-11-29 20:14:08.791202: Current learning rate: 0.009
2024-11-29 20:15:34.732234: Validation loss did not improve from -0.56540. Patience: 23/50
2024-11-29 20:15:34.733012: train_loss -0.6727
2024-11-29 20:15:34.733994: val_loss -0.543
2024-11-29 20:15:34.734919: Pseudo dice [0.7305]
2024-11-29 20:15:34.735726: Epoch time: 85.95 s
2024-11-29 20:15:35.966642: 
2024-11-29 20:15:35.967988: Epoch 112
2024-11-29 20:15:35.968865: Current learning rate: 0.00899
2024-11-29 20:17:01.980812: Validation loss did not improve from -0.56540. Patience: 24/50
2024-11-29 20:17:01.981616: train_loss -0.6727
2024-11-29 20:17:01.982607: val_loss -0.4882
2024-11-29 20:17:01.983464: Pseudo dice [0.7082]
2024-11-29 20:17:01.984270: Epoch time: 86.02 s
2024-11-29 20:17:03.241709: 
2024-11-29 20:17:03.244148: Epoch 113
2024-11-29 20:17:03.245150: Current learning rate: 0.00898
2024-11-29 20:18:29.160897: Validation loss did not improve from -0.56540. Patience: 25/50
2024-11-29 20:18:29.161810: train_loss -0.6712
2024-11-29 20:18:29.162647: val_loss -0.5501
2024-11-29 20:18:29.163355: Pseudo dice [0.7388]
2024-11-29 20:18:29.164508: Epoch time: 85.92 s
2024-11-29 20:18:30.743107: 
2024-11-29 20:18:30.745060: Epoch 114
2024-11-29 20:18:30.745863: Current learning rate: 0.00897
2024-11-29 20:19:56.626830: Validation loss did not improve from -0.56540. Patience: 26/50
2024-11-29 20:19:56.627562: train_loss -0.6574
2024-11-29 20:19:56.628255: val_loss -0.5302
2024-11-29 20:19:56.628910: Pseudo dice [0.7196]
2024-11-29 20:19:56.629570: Epoch time: 85.89 s
2024-11-29 20:19:58.210453: 
2024-11-29 20:19:58.212241: Epoch 115
2024-11-29 20:19:58.213289: Current learning rate: 0.00896
2024-11-29 20:21:24.191211: Validation loss did not improve from -0.56540. Patience: 27/50
2024-11-29 20:21:24.192122: train_loss -0.6787
2024-11-29 20:21:24.192946: val_loss -0.5398
2024-11-29 20:21:24.193692: Pseudo dice [0.726]
2024-11-29 20:21:24.194426: Epoch time: 85.98 s
2024-11-29 20:21:25.468240: 
2024-11-29 20:21:25.470331: Epoch 116
2024-11-29 20:21:25.471528: Current learning rate: 0.00895
2024-11-29 20:22:51.418290: Validation loss did not improve from -0.56540. Patience: 28/50
2024-11-29 20:22:51.419424: train_loss -0.6644
2024-11-29 20:22:51.420542: val_loss -0.5368
2024-11-29 20:22:51.421295: Pseudo dice [0.7341]
2024-11-29 20:22:51.422028: Epoch time: 85.95 s
2024-11-29 20:22:52.654829: 
2024-11-29 20:22:52.657388: Epoch 117
2024-11-29 20:22:52.658480: Current learning rate: 0.00894
2024-11-29 20:24:18.547032: Validation loss did not improve from -0.56540. Patience: 29/50
2024-11-29 20:24:18.548100: train_loss -0.6769
2024-11-29 20:24:18.549190: val_loss -0.5586
2024-11-29 20:24:18.550135: Pseudo dice [0.7408]
2024-11-29 20:24:18.551024: Epoch time: 85.89 s
2024-11-29 20:24:18.551940: Yayy! New best EMA pseudo Dice: 0.7271
2024-11-29 20:24:20.173938: 
2024-11-29 20:24:20.175618: Epoch 118
2024-11-29 20:24:20.176374: Current learning rate: 0.00893
2024-11-29 20:25:46.064859: Validation loss did not improve from -0.56540. Patience: 30/50
2024-11-29 20:25:46.066042: train_loss -0.6798
2024-11-29 20:25:46.067041: val_loss -0.5377
2024-11-29 20:25:46.067866: Pseudo dice [0.7342]
2024-11-29 20:25:46.068804: Epoch time: 85.89 s
2024-11-29 20:25:46.069659: Yayy! New best EMA pseudo Dice: 0.7278
2024-11-29 20:25:47.677837: 
2024-11-29 20:25:47.679988: Epoch 119
2024-11-29 20:25:47.680909: Current learning rate: 0.00892
2024-11-29 20:27:13.677202: Validation loss did not improve from -0.56540. Patience: 31/50
2024-11-29 20:27:13.678440: train_loss -0.676
2024-11-29 20:27:13.679314: val_loss -0.5297
2024-11-29 20:27:13.679950: Pseudo dice [0.7205]
2024-11-29 20:27:13.680645: Epoch time: 86.0 s
2024-11-29 20:27:15.281201: 
2024-11-29 20:27:15.283878: Epoch 120
2024-11-29 20:27:15.285028: Current learning rate: 0.00891
2024-11-29 20:28:41.169974: Validation loss did not improve from -0.56540. Patience: 32/50
2024-11-29 20:28:41.171089: train_loss -0.6744
2024-11-29 20:28:41.171795: val_loss -0.5425
2024-11-29 20:28:41.172659: Pseudo dice [0.7294]
2024-11-29 20:28:41.173532: Epoch time: 85.89 s
2024-11-29 20:28:42.416570: 
2024-11-29 20:28:42.418946: Epoch 121
2024-11-29 20:28:42.420119: Current learning rate: 0.0089
2024-11-29 20:30:08.372683: Validation loss did not improve from -0.56540. Patience: 33/50
2024-11-29 20:30:08.373632: train_loss -0.6778
2024-11-29 20:30:08.374740: val_loss -0.5397
2024-11-29 20:30:08.375776: Pseudo dice [0.7375]
2024-11-29 20:30:08.376711: Epoch time: 85.96 s
2024-11-29 20:30:08.377522: Yayy! New best EMA pseudo Dice: 0.7283
2024-11-29 20:30:09.994688: 
2024-11-29 20:30:09.996948: Epoch 122
2024-11-29 20:30:09.997937: Current learning rate: 0.00889
2024-11-29 20:31:36.011106: Validation loss did not improve from -0.56540. Patience: 34/50
2024-11-29 20:31:36.011731: train_loss -0.6855
2024-11-29 20:31:36.012430: val_loss -0.5607
2024-11-29 20:31:36.013366: Pseudo dice [0.7501]
2024-11-29 20:31:36.014293: Epoch time: 86.02 s
2024-11-29 20:31:36.014958: Yayy! New best EMA pseudo Dice: 0.7305
2024-11-29 20:31:37.771688: 
2024-11-29 20:31:37.773715: Epoch 123
2024-11-29 20:31:37.774434: Current learning rate: 0.00889
2024-11-29 20:33:03.635201: Validation loss did not improve from -0.56540. Patience: 35/50
2024-11-29 20:33:03.636170: train_loss -0.6853
2024-11-29 20:33:03.637466: val_loss -0.556
2024-11-29 20:33:03.638277: Pseudo dice [0.737]
2024-11-29 20:33:03.639430: Epoch time: 85.87 s
2024-11-29 20:33:03.640365: Yayy! New best EMA pseudo Dice: 0.7312
2024-11-29 20:33:05.527782: 
2024-11-29 20:33:05.529346: Epoch 124
2024-11-29 20:33:05.530256: Current learning rate: 0.00888
2024-11-29 20:34:31.436421: Validation loss did not improve from -0.56540. Patience: 36/50
2024-11-29 20:34:31.437403: train_loss -0.6744
2024-11-29 20:34:31.438492: val_loss -0.5131
2024-11-29 20:34:31.439292: Pseudo dice [0.7141]
2024-11-29 20:34:31.440263: Epoch time: 85.91 s
2024-11-29 20:34:33.006470: 
2024-11-29 20:34:33.008758: Epoch 125
2024-11-29 20:34:33.009856: Current learning rate: 0.00887
2024-11-29 20:35:58.999187: Validation loss did not improve from -0.56540. Patience: 37/50
2024-11-29 20:35:59.000067: train_loss -0.6735
2024-11-29 20:35:59.000715: val_loss -0.5383
2024-11-29 20:35:59.001459: Pseudo dice [0.7347]
2024-11-29 20:35:59.002181: Epoch time: 85.99 s
2024-11-29 20:36:00.230406: 
2024-11-29 20:36:00.232447: Epoch 126
2024-11-29 20:36:00.233576: Current learning rate: 0.00886
2024-11-29 20:37:26.169977: Validation loss did not improve from -0.56540. Patience: 38/50
2024-11-29 20:37:26.170727: train_loss -0.6838
2024-11-29 20:37:26.171599: val_loss -0.5445
2024-11-29 20:37:26.172525: Pseudo dice [0.7306]
2024-11-29 20:37:26.173463: Epoch time: 85.94 s
2024-11-29 20:37:27.396419: 
2024-11-29 20:37:27.398345: Epoch 127
2024-11-29 20:37:27.399386: Current learning rate: 0.00885
2024-11-29 20:38:53.256524: Validation loss did not improve from -0.56540. Patience: 39/50
2024-11-29 20:38:53.257235: train_loss -0.6819
2024-11-29 20:38:53.258085: val_loss -0.5602
2024-11-29 20:38:53.258746: Pseudo dice [0.7394]
2024-11-29 20:38:53.259501: Epoch time: 85.86 s
2024-11-29 20:38:54.507440: 
2024-11-29 20:38:54.508708: Epoch 128
2024-11-29 20:38:54.509424: Current learning rate: 0.00884
2024-11-29 20:40:20.453755: Validation loss did not improve from -0.56540. Patience: 40/50
2024-11-29 20:40:20.454983: train_loss -0.6653
2024-11-29 20:40:20.455879: val_loss -0.529
2024-11-29 20:40:20.456511: Pseudo dice [0.7287]
2024-11-29 20:40:20.457324: Epoch time: 85.95 s
2024-11-29 20:40:21.703382: 
2024-11-29 20:40:21.704781: Epoch 129
2024-11-29 20:40:21.705551: Current learning rate: 0.00883
2024-11-29 20:41:47.708272: Validation loss did not improve from -0.56540. Patience: 41/50
2024-11-29 20:41:47.709285: train_loss -0.6794
2024-11-29 20:41:47.710228: val_loss -0.527
2024-11-29 20:41:47.711050: Pseudo dice [0.7225]
2024-11-29 20:41:47.711929: Epoch time: 86.01 s
2024-11-29 20:41:49.349735: 
2024-11-29 20:41:49.351092: Epoch 130
2024-11-29 20:41:49.351847: Current learning rate: 0.00882
2024-11-29 20:43:15.268994: Validation loss did not improve from -0.56540. Patience: 42/50
2024-11-29 20:43:15.270077: train_loss -0.6817
2024-11-29 20:43:15.270973: val_loss -0.5015
2024-11-29 20:43:15.271590: Pseudo dice [0.7171]
2024-11-29 20:43:15.272254: Epoch time: 85.92 s
2024-11-29 20:43:16.576987: 
2024-11-29 20:43:16.578469: Epoch 131
2024-11-29 20:43:16.579148: Current learning rate: 0.00881
2024-11-29 20:44:43.535999: Validation loss did not improve from -0.56540. Patience: 43/50
2024-11-29 20:44:43.569467: train_loss -0.6815
2024-11-29 20:44:43.570831: val_loss -0.5222
2024-11-29 20:44:43.571741: Pseudo dice [0.7175]
2024-11-29 20:44:43.576234: Epoch time: 86.99 s
2024-11-29 20:44:44.925988: 
2024-11-29 20:44:44.927819: Epoch 132
2024-11-29 20:44:44.928678: Current learning rate: 0.0088
2024-11-29 20:46:11.976118: Validation loss did not improve from -0.56540. Patience: 44/50
2024-11-29 20:46:11.977274: train_loss -0.6887
2024-11-29 20:46:11.978268: val_loss -0.539
2024-11-29 20:46:11.978875: Pseudo dice [0.7323]
2024-11-29 20:46:11.979535: Epoch time: 87.05 s
2024-11-29 20:46:13.234629: 
2024-11-29 20:46:13.235931: Epoch 133
2024-11-29 20:46:13.236813: Current learning rate: 0.00879
2024-11-29 20:47:39.330629: Validation loss did not improve from -0.56540. Patience: 45/50
2024-11-29 20:47:39.331537: train_loss -0.6892
2024-11-29 20:47:39.332441: val_loss -0.5348
2024-11-29 20:47:39.333048: Pseudo dice [0.7355]
2024-11-29 20:47:39.333709: Epoch time: 86.1 s
2024-11-29 20:47:40.649711: 
2024-11-29 20:47:40.651502: Epoch 134
2024-11-29 20:47:40.652651: Current learning rate: 0.00879
2024-11-29 20:49:06.553958: Validation loss did not improve from -0.56540. Patience: 46/50
2024-11-29 20:49:06.554992: train_loss -0.692
2024-11-29 20:49:06.555887: val_loss -0.5533
2024-11-29 20:49:06.556499: Pseudo dice [0.7473]
2024-11-29 20:49:06.557106: Epoch time: 85.91 s
2024-11-29 20:49:09.226954: 
2024-11-29 20:49:09.228419: Epoch 135
2024-11-29 20:49:09.229553: Current learning rate: 0.00878
2024-11-29 20:50:35.178629: Validation loss improved from -0.56540 to -0.56706! Patience: 46/50
2024-11-29 20:50:35.179440: train_loss -0.7027
2024-11-29 20:50:35.180306: val_loss -0.5671
2024-11-29 20:50:35.180997: Pseudo dice [0.7522]
2024-11-29 20:50:35.181758: Epoch time: 85.95 s
2024-11-29 20:50:35.182502: Yayy! New best EMA pseudo Dice: 0.7328
2024-11-29 20:50:36.781624: 
2024-11-29 20:50:36.783594: Epoch 136
2024-11-29 20:50:36.784412: Current learning rate: 0.00877
2024-11-29 20:52:02.703497: Validation loss did not improve from -0.56706. Patience: 1/50
2024-11-29 20:52:02.704142: train_loss -0.6934
2024-11-29 20:52:02.704843: val_loss -0.5607
2024-11-29 20:52:02.705839: Pseudo dice [0.7482]
2024-11-29 20:52:02.706568: Epoch time: 85.92 s
2024-11-29 20:52:02.707123: Yayy! New best EMA pseudo Dice: 0.7343
2024-11-29 20:52:04.340194: 
2024-11-29 20:52:04.341387: Epoch 137
2024-11-29 20:52:04.342044: Current learning rate: 0.00876
2024-11-29 20:53:30.243976: Validation loss did not improve from -0.56706. Patience: 2/50
2024-11-29 20:53:30.244765: train_loss -0.685
2024-11-29 20:53:30.245827: val_loss -0.549
2024-11-29 20:53:30.246696: Pseudo dice [0.7352]
2024-11-29 20:53:30.247542: Epoch time: 85.91 s
2024-11-29 20:53:30.248437: Yayy! New best EMA pseudo Dice: 0.7344
2024-11-29 20:53:31.862094: 
2024-11-29 20:53:31.863579: Epoch 138
2024-11-29 20:53:31.864549: Current learning rate: 0.00875
2024-11-29 20:54:57.792220: Validation loss did not improve from -0.56706. Patience: 3/50
2024-11-29 20:54:57.793068: train_loss -0.6822
2024-11-29 20:54:57.793893: val_loss -0.55
2024-11-29 20:54:57.794672: Pseudo dice [0.7336]
2024-11-29 20:54:57.795433: Epoch time: 85.93 s
2024-11-29 20:54:59.058870: 
2024-11-29 20:54:59.060732: Epoch 139
2024-11-29 20:54:59.061635: Current learning rate: 0.00874
2024-11-29 20:56:25.070513: Validation loss did not improve from -0.56706. Patience: 4/50
2024-11-29 20:56:25.071561: train_loss -0.6959
2024-11-29 20:56:25.072286: val_loss -0.5547
2024-11-29 20:56:25.073039: Pseudo dice [0.7415]
2024-11-29 20:56:25.073750: Epoch time: 86.01 s
2024-11-29 20:56:25.420368: Yayy! New best EMA pseudo Dice: 0.735
2024-11-29 20:56:27.043656: 
2024-11-29 20:56:27.045423: Epoch 140
2024-11-29 20:56:27.046201: Current learning rate: 0.00873
2024-11-29 20:57:52.941702: Validation loss did not improve from -0.56706. Patience: 5/50
2024-11-29 20:57:52.942555: train_loss -0.6833
2024-11-29 20:57:52.943427: val_loss -0.5352
2024-11-29 20:57:52.944232: Pseudo dice [0.725]
2024-11-29 20:57:52.945001: Epoch time: 85.9 s
2024-11-29 20:57:54.218204: 
2024-11-29 20:57:54.220109: Epoch 141
2024-11-29 20:57:54.220883: Current learning rate: 0.00872
2024-11-29 20:59:20.122400: Validation loss did not improve from -0.56706. Patience: 6/50
2024-11-29 20:59:20.123557: train_loss -0.6822
2024-11-29 20:59:20.124358: val_loss -0.5382
2024-11-29 20:59:20.125028: Pseudo dice [0.743]
2024-11-29 20:59:20.125629: Epoch time: 85.91 s
2024-11-29 20:59:21.427992: 
2024-11-29 20:59:21.429483: Epoch 142
2024-11-29 20:59:21.430244: Current learning rate: 0.00871
2024-11-29 21:00:47.460210: Validation loss did not improve from -0.56706. Patience: 7/50
2024-11-29 21:00:47.461029: train_loss -0.6959
2024-11-29 21:00:47.461951: val_loss -0.5528
2024-11-29 21:00:47.462775: Pseudo dice [0.7437]
2024-11-29 21:00:47.463701: Epoch time: 86.03 s
2024-11-29 21:00:47.464515: Yayy! New best EMA pseudo Dice: 0.7358
2024-11-29 21:00:49.105858: 
2024-11-29 21:00:49.107521: Epoch 143
2024-11-29 21:00:49.108191: Current learning rate: 0.0087
2024-11-29 21:02:14.999707: Validation loss improved from -0.56706 to -0.56734! Patience: 7/50
2024-11-29 21:02:15.000866: train_loss -0.6986
2024-11-29 21:02:15.001834: val_loss -0.5673
2024-11-29 21:02:15.002656: Pseudo dice [0.757]
2024-11-29 21:02:15.003600: Epoch time: 85.9 s
2024-11-29 21:02:15.004333: Yayy! New best EMA pseudo Dice: 0.7379
2024-11-29 21:02:16.644347: 
2024-11-29 21:02:16.645924: Epoch 144
2024-11-29 21:02:16.646770: Current learning rate: 0.00869
2024-11-29 21:03:42.502252: Validation loss did not improve from -0.56734. Patience: 1/50
2024-11-29 21:03:42.503423: train_loss -0.7028
2024-11-29 21:03:42.504391: val_loss -0.5359
2024-11-29 21:03:42.505153: Pseudo dice [0.7317]
2024-11-29 21:03:42.506011: Epoch time: 85.86 s
2024-11-29 21:03:44.473770: 
2024-11-29 21:03:44.475392: Epoch 145
2024-11-29 21:03:44.476130: Current learning rate: 0.00868
2024-11-29 21:05:10.461998: Validation loss did not improve from -0.56734. Patience: 2/50
2024-11-29 21:05:10.462705: train_loss -0.7119
2024-11-29 21:05:10.463478: val_loss -0.5593
2024-11-29 21:05:10.464119: Pseudo dice [0.7408]
2024-11-29 21:05:10.465038: Epoch time: 85.99 s
2024-11-29 21:05:11.718608: 
2024-11-29 21:05:11.720034: Epoch 146
2024-11-29 21:05:11.720758: Current learning rate: 0.00868
2024-11-29 21:06:37.623987: Validation loss improved from -0.56734 to -0.57103! Patience: 2/50
2024-11-29 21:06:37.625098: train_loss -0.7017
2024-11-29 21:06:37.625962: val_loss -0.571
2024-11-29 21:06:37.626613: Pseudo dice [0.75]
2024-11-29 21:06:37.627470: Epoch time: 85.91 s
2024-11-29 21:06:37.628144: Yayy! New best EMA pseudo Dice: 0.7389
2024-11-29 21:06:39.240014: 
2024-11-29 21:06:39.241724: Epoch 147
2024-11-29 21:06:39.242741: Current learning rate: 0.00867
2024-11-29 21:08:05.173977: Validation loss did not improve from -0.57103. Patience: 1/50
2024-11-29 21:08:05.175186: train_loss -0.696
2024-11-29 21:08:05.176345: val_loss -0.5453
2024-11-29 21:08:05.177440: Pseudo dice [0.7231]
2024-11-29 21:08:05.178372: Epoch time: 85.94 s
2024-11-29 21:08:06.422543: 
2024-11-29 21:08:06.423655: Epoch 148
2024-11-29 21:08:06.424389: Current learning rate: 0.00866
2024-11-29 21:09:32.343570: Validation loss improved from -0.57103 to -0.57271! Patience: 1/50
2024-11-29 21:09:32.345509: train_loss -0.6998
2024-11-29 21:09:32.346832: val_loss -0.5727
2024-11-29 21:09:32.347873: Pseudo dice [0.7407]
2024-11-29 21:09:32.349001: Epoch time: 85.92 s
2024-11-29 21:09:33.604457: 
2024-11-29 21:09:33.606550: Epoch 149
2024-11-29 21:09:33.607708: Current learning rate: 0.00865
2024-11-29 21:10:59.579684: Validation loss did not improve from -0.57271. Patience: 1/50
2024-11-29 21:10:59.580467: train_loss -0.7045
2024-11-29 21:10:59.581264: val_loss -0.527
2024-11-29 21:10:59.582019: Pseudo dice [0.7337]
2024-11-29 21:10:59.582906: Epoch time: 85.98 s
2024-11-29 21:11:01.117364: 
2024-11-29 21:11:01.119511: Epoch 150
2024-11-29 21:11:01.120612: Current learning rate: 0.00864
2024-11-29 21:12:26.960767: Validation loss did not improve from -0.57271. Patience: 2/50
2024-11-29 21:12:26.961650: train_loss -0.7074
2024-11-29 21:12:26.962846: val_loss -0.5514
2024-11-29 21:12:26.964016: Pseudo dice [0.7305]
2024-11-29 21:12:26.965461: Epoch time: 85.85 s
2024-11-29 21:12:28.208073: 
2024-11-29 21:12:28.210581: Epoch 151
2024-11-29 21:12:28.212052: Current learning rate: 0.00863
2024-11-29 21:13:54.110172: Validation loss did not improve from -0.57271. Patience: 3/50
2024-11-29 21:13:54.111178: train_loss -0.7044
2024-11-29 21:13:54.112054: val_loss -0.5551
2024-11-29 21:13:54.113033: Pseudo dice [0.7453]
2024-11-29 21:13:54.113836: Epoch time: 85.9 s
2024-11-29 21:13:55.352206: 
2024-11-29 21:13:55.355005: Epoch 152
2024-11-29 21:13:55.356651: Current learning rate: 0.00862
2024-11-29 21:15:21.355519: Validation loss did not improve from -0.57271. Patience: 4/50
2024-11-29 21:15:21.356802: train_loss -0.6984
2024-11-29 21:15:21.357781: val_loss -0.5449
2024-11-29 21:15:21.358690: Pseudo dice [0.7358]
2024-11-29 21:15:21.359622: Epoch time: 86.01 s
2024-11-29 21:15:22.555290: 
2024-11-29 21:15:22.557071: Epoch 153
2024-11-29 21:15:22.558029: Current learning rate: 0.00861
2024-11-29 21:16:48.401673: Validation loss did not improve from -0.57271. Patience: 5/50
2024-11-29 21:16:48.402929: train_loss -0.7
2024-11-29 21:16:48.404240: val_loss -0.518
2024-11-29 21:16:48.405036: Pseudo dice [0.7219]
2024-11-29 21:16:48.406026: Epoch time: 85.85 s
2024-11-29 21:16:49.672786: 
2024-11-29 21:16:49.674589: Epoch 154
2024-11-29 21:16:49.675631: Current learning rate: 0.0086
2024-11-29 21:18:15.508088: Validation loss did not improve from -0.57271. Patience: 6/50
2024-11-29 21:18:15.509609: train_loss -0.7023
2024-11-29 21:18:15.510893: val_loss -0.5516
2024-11-29 21:18:15.511838: Pseudo dice [0.7339]
2024-11-29 21:18:15.512669: Epoch time: 85.84 s
2024-11-29 21:18:17.118421: 
2024-11-29 21:18:17.120440: Epoch 155
2024-11-29 21:18:17.121468: Current learning rate: 0.00859
2024-11-29 21:19:43.078259: Validation loss did not improve from -0.57271. Patience: 7/50
2024-11-29 21:19:43.079485: train_loss -0.7018
2024-11-29 21:19:43.080938: val_loss -0.5595
2024-11-29 21:19:43.082254: Pseudo dice [0.7547]
2024-11-29 21:19:43.083304: Epoch time: 85.96 s
2024-11-29 21:19:44.706208: 
2024-11-29 21:19:44.707760: Epoch 156
2024-11-29 21:19:44.708909: Current learning rate: 0.00858
2024-11-29 21:21:10.614062: Validation loss did not improve from -0.57271. Patience: 8/50
2024-11-29 21:21:10.615117: train_loss -0.7044
2024-11-29 21:21:10.616268: val_loss -0.5045
2024-11-29 21:21:10.617357: Pseudo dice [0.7117]
2024-11-29 21:21:10.618363: Epoch time: 85.91 s
2024-11-29 21:21:11.838558: 
2024-11-29 21:21:11.840464: Epoch 157
2024-11-29 21:21:11.841315: Current learning rate: 0.00858
2024-11-29 21:22:37.703490: Validation loss did not improve from -0.57271. Patience: 9/50
2024-11-29 21:22:37.704594: train_loss -0.707
2024-11-29 21:22:37.705936: val_loss -0.55
2024-11-29 21:22:37.706923: Pseudo dice [0.7449]
2024-11-29 21:22:37.707851: Epoch time: 85.87 s
2024-11-29 21:22:38.940165: 
2024-11-29 21:22:38.942126: Epoch 158
2024-11-29 21:22:38.943427: Current learning rate: 0.00857
2024-11-29 21:24:04.856207: Validation loss did not improve from -0.57271. Patience: 10/50
2024-11-29 21:24:04.857564: train_loss -0.7196
2024-11-29 21:24:04.858725: val_loss -0.5458
2024-11-29 21:24:04.859636: Pseudo dice [0.7415]
2024-11-29 21:24:04.860532: Epoch time: 85.92 s
2024-11-29 21:24:06.104512: 
2024-11-29 21:24:06.106920: Epoch 159
2024-11-29 21:24:06.108041: Current learning rate: 0.00856
2024-11-29 21:25:32.090194: Validation loss did not improve from -0.57271. Patience: 11/50
2024-11-29 21:25:32.091339: train_loss -0.7153
2024-11-29 21:25:32.092624: val_loss -0.5561
2024-11-29 21:25:32.093496: Pseudo dice [0.7411]
2024-11-29 21:25:32.094203: Epoch time: 85.99 s
2024-11-29 21:25:33.656652: 
2024-11-29 21:25:33.659015: Epoch 160
2024-11-29 21:25:33.660124: Current learning rate: 0.00855
2024-11-29 21:26:59.517113: Validation loss did not improve from -0.57271. Patience: 12/50
2024-11-29 21:26:59.518436: train_loss -0.7065
2024-11-29 21:26:59.519469: val_loss -0.5538
2024-11-29 21:26:59.520531: Pseudo dice [0.7375]
2024-11-29 21:26:59.521508: Epoch time: 85.86 s
2024-11-29 21:27:00.765560: 
2024-11-29 21:27:00.767782: Epoch 161
2024-11-29 21:27:00.769228: Current learning rate: 0.00854
2024-11-29 21:28:26.611492: Validation loss did not improve from -0.57271. Patience: 13/50
2024-11-29 21:28:26.612844: train_loss -0.7059
2024-11-29 21:28:26.613972: val_loss -0.5264
2024-11-29 21:28:26.614592: Pseudo dice [0.7238]
2024-11-29 21:28:26.615226: Epoch time: 85.85 s
2024-11-29 21:28:27.884141: 
2024-11-29 21:28:27.886104: Epoch 162
2024-11-29 21:28:27.887107: Current learning rate: 0.00853
2024-11-29 21:29:53.851395: Validation loss did not improve from -0.57271. Patience: 14/50
2024-11-29 21:29:53.852651: train_loss -0.7053
2024-11-29 21:29:53.853956: val_loss -0.5563
2024-11-29 21:29:53.854789: Pseudo dice [0.7528]
2024-11-29 21:29:53.855613: Epoch time: 85.97 s
2024-11-29 21:29:55.061327: 
2024-11-29 21:29:55.063385: Epoch 163
2024-11-29 21:29:55.064473: Current learning rate: 0.00852
2024-11-29 21:31:20.910978: Validation loss did not improve from -0.57271. Patience: 15/50
2024-11-29 21:31:20.912259: train_loss -0.7004
2024-11-29 21:31:20.913273: val_loss -0.5126
2024-11-29 21:31:20.914032: Pseudo dice [0.7239]
2024-11-29 21:31:20.914855: Epoch time: 85.85 s
2024-11-29 21:31:22.161480: 
2024-11-29 21:31:22.163655: Epoch 164
2024-11-29 21:31:22.164776: Current learning rate: 0.00851
2024-11-29 21:32:48.025442: Validation loss did not improve from -0.57271. Patience: 16/50
2024-11-29 21:32:48.026693: train_loss -0.6926
2024-11-29 21:32:48.027772: val_loss -0.5241
2024-11-29 21:32:48.028642: Pseudo dice [0.7318]
2024-11-29 21:32:48.029303: Epoch time: 85.87 s
2024-11-29 21:32:49.551226: 
2024-11-29 21:32:49.553178: Epoch 165
2024-11-29 21:32:49.554435: Current learning rate: 0.0085
2024-11-29 21:34:15.486135: Validation loss did not improve from -0.57271. Patience: 17/50
2024-11-29 21:34:15.487564: train_loss -0.7014
2024-11-29 21:34:15.488714: val_loss -0.5679
2024-11-29 21:34:15.489423: Pseudo dice [0.7417]
2024-11-29 21:34:15.490245: Epoch time: 85.94 s
2024-11-29 21:34:17.146966: 
2024-11-29 21:34:17.149066: Epoch 166
2024-11-29 21:34:17.150228: Current learning rate: 0.00849
2024-11-29 21:35:43.105021: Validation loss did not improve from -0.57271. Patience: 18/50
2024-11-29 21:35:43.106333: train_loss -0.7073
2024-11-29 21:35:43.107673: val_loss -0.5195
2024-11-29 21:35:43.108569: Pseudo dice [0.7218]
2024-11-29 21:35:43.109460: Epoch time: 85.96 s
2024-11-29 21:35:44.288682: 
2024-11-29 21:35:44.291044: Epoch 167
2024-11-29 21:35:44.291910: Current learning rate: 0.00848
2024-11-29 21:37:10.141310: Validation loss did not improve from -0.57271. Patience: 19/50
2024-11-29 21:37:10.142766: train_loss -0.7125
2024-11-29 21:37:10.144013: val_loss -0.5568
2024-11-29 21:37:10.144862: Pseudo dice [0.7427]
2024-11-29 21:37:10.145648: Epoch time: 85.85 s
2024-11-29 21:37:11.384890: 
2024-11-29 21:37:11.386992: Epoch 168
2024-11-29 21:37:11.388078: Current learning rate: 0.00847
2024-11-29 21:38:37.257014: Validation loss did not improve from -0.57271. Patience: 20/50
2024-11-29 21:38:37.258029: train_loss -0.7181
2024-11-29 21:38:37.258974: val_loss -0.5466
2024-11-29 21:38:37.259741: Pseudo dice [0.7451]
2024-11-29 21:38:37.260614: Epoch time: 85.87 s
2024-11-29 21:38:38.477514: 
2024-11-29 21:38:38.479662: Epoch 169
2024-11-29 21:38:38.480568: Current learning rate: 0.00847
2024-11-29 21:40:04.449780: Validation loss improved from -0.57271 to -0.58183! Patience: 20/50
2024-11-29 21:40:04.450884: train_loss -0.7179
2024-11-29 21:40:04.452064: val_loss -0.5818
2024-11-29 21:40:04.453214: Pseudo dice [0.7577]
2024-11-29 21:40:04.454057: Epoch time: 85.97 s
2024-11-29 21:40:05.958925: 
2024-11-29 21:40:05.961136: Epoch 170
2024-11-29 21:40:05.962216: Current learning rate: 0.00846
2024-11-29 21:41:31.807308: Validation loss did not improve from -0.58183. Patience: 1/50
2024-11-29 21:41:31.808531: train_loss -0.712
2024-11-29 21:41:31.809607: val_loss -0.5613
2024-11-29 21:41:31.810611: Pseudo dice [0.7422]
2024-11-29 21:41:31.811550: Epoch time: 85.85 s
2024-11-29 21:41:31.812296: Yayy! New best EMA pseudo Dice: 0.739
2024-11-29 21:41:33.434798: 
2024-11-29 21:41:33.436735: Epoch 171
2024-11-29 21:41:33.437806: Current learning rate: 0.00845
2024-11-29 21:42:59.326151: Validation loss did not improve from -0.58183. Patience: 2/50
2024-11-29 21:42:59.327253: train_loss -0.7214
2024-11-29 21:42:59.328543: val_loss -0.5251
2024-11-29 21:42:59.329487: Pseudo dice [0.721]
2024-11-29 21:42:59.330366: Epoch time: 85.89 s
2024-11-29 21:43:00.558113: 
2024-11-29 21:43:00.560592: Epoch 172
2024-11-29 21:43:00.561719: Current learning rate: 0.00844
2024-11-29 21:44:26.521851: Validation loss did not improve from -0.58183. Patience: 3/50
2024-11-29 21:44:26.523031: train_loss -0.7225
2024-11-29 21:44:26.524506: val_loss -0.5667
2024-11-29 21:44:26.525441: Pseudo dice [0.7469]
2024-11-29 21:44:26.526516: Epoch time: 85.97 s
2024-11-29 21:44:27.761209: 
2024-11-29 21:44:27.763540: Epoch 173
2024-11-29 21:44:27.764642: Current learning rate: 0.00843
2024-11-29 21:45:53.651404: Validation loss did not improve from -0.58183. Patience: 4/50
2024-11-29 21:45:53.652390: train_loss -0.7268
2024-11-29 21:45:53.653138: val_loss -0.5665
2024-11-29 21:45:53.654049: Pseudo dice [0.7553]
2024-11-29 21:45:53.654694: Epoch time: 85.89 s
2024-11-29 21:45:53.655618: Yayy! New best EMA pseudo Dice: 0.7399
2024-11-29 21:45:55.280214: 
2024-11-29 21:45:55.282582: Epoch 174
2024-11-29 21:45:55.284318: Current learning rate: 0.00842
2024-11-29 21:47:21.148098: Validation loss did not improve from -0.58183. Patience: 5/50
2024-11-29 21:47:21.148840: train_loss -0.7213
2024-11-29 21:47:21.149752: val_loss -0.5599
2024-11-29 21:47:21.150746: Pseudo dice [0.7451]
2024-11-29 21:47:21.151680: Epoch time: 85.87 s
2024-11-29 21:47:21.532799: Yayy! New best EMA pseudo Dice: 0.7404
2024-11-29 21:47:23.093941: 
2024-11-29 21:47:23.096196: Epoch 175
2024-11-29 21:47:23.097062: Current learning rate: 0.00841
2024-11-29 21:48:49.009742: Validation loss did not improve from -0.58183. Patience: 6/50
2024-11-29 21:48:49.010875: train_loss -0.7184
2024-11-29 21:48:49.012169: val_loss -0.5488
2024-11-29 21:48:49.013368: Pseudo dice [0.7369]
2024-11-29 21:48:49.014451: Epoch time: 85.92 s
2024-11-29 21:48:50.238452: 
2024-11-29 21:48:50.241047: Epoch 176
2024-11-29 21:48:50.242247: Current learning rate: 0.0084
2024-11-29 21:50:16.268442: Validation loss did not improve from -0.58183. Patience: 7/50
2024-11-29 21:50:16.269373: train_loss -0.7113
2024-11-29 21:50:16.270368: val_loss -0.5613
2024-11-29 21:50:16.271394: Pseudo dice [0.7461]
2024-11-29 21:50:16.272161: Epoch time: 86.03 s
2024-11-29 21:50:16.273043: Yayy! New best EMA pseudo Dice: 0.7407
2024-11-29 21:50:18.157006: 
2024-11-29 21:50:18.159713: Epoch 177
2024-11-29 21:50:18.161345: Current learning rate: 0.00839
2024-11-29 21:51:44.139797: Validation loss did not improve from -0.58183. Patience: 8/50
2024-11-29 21:51:44.165860: train_loss -0.7243
2024-11-29 21:51:44.167165: val_loss -0.535
2024-11-29 21:51:44.167897: Pseudo dice [0.7284]
2024-11-29 21:51:44.168774: Epoch time: 86.01 s
2024-11-29 21:51:45.501471: 
2024-11-29 21:51:45.503754: Epoch 178
2024-11-29 21:51:45.504816: Current learning rate: 0.00838
2024-11-29 21:53:11.359401: Validation loss did not improve from -0.58183. Patience: 9/50
2024-11-29 21:53:11.360726: train_loss -0.7201
2024-11-29 21:53:11.361889: val_loss -0.5444
2024-11-29 21:53:11.362808: Pseudo dice [0.74]
2024-11-29 21:53:11.363737: Epoch time: 85.86 s
2024-11-29 21:53:12.623914: 
2024-11-29 21:53:12.625974: Epoch 179
2024-11-29 21:53:12.627114: Current learning rate: 0.00837
2024-11-29 21:54:38.625435: Validation loss did not improve from -0.58183. Patience: 10/50
2024-11-29 21:54:38.626376: train_loss -0.7266
2024-11-29 21:54:38.627385: val_loss -0.5273
2024-11-29 21:54:38.628338: Pseudo dice [0.7312]
2024-11-29 21:54:38.629358: Epoch time: 86.0 s
2024-11-29 21:54:40.211776: 
2024-11-29 21:54:40.214008: Epoch 180
2024-11-29 21:54:40.215031: Current learning rate: 0.00836
2024-11-29 21:56:06.046828: Validation loss did not improve from -0.58183. Patience: 11/50
2024-11-29 21:56:06.047916: train_loss -0.7273
2024-11-29 21:56:06.048821: val_loss -0.5365
2024-11-29 21:56:06.049830: Pseudo dice [0.7254]
2024-11-29 21:56:06.050615: Epoch time: 85.84 s
2024-11-29 21:56:07.265450: 
2024-11-29 21:56:07.267319: Epoch 181
2024-11-29 21:56:07.268442: Current learning rate: 0.00836
2024-11-29 21:57:33.118967: Validation loss did not improve from -0.58183. Patience: 12/50
2024-11-29 21:57:33.120284: train_loss -0.7222
2024-11-29 21:57:33.121604: val_loss -0.5522
2024-11-29 21:57:33.122775: Pseudo dice [0.7409]
2024-11-29 21:57:33.123960: Epoch time: 85.86 s
2024-11-29 21:57:34.333766: 
2024-11-29 21:57:34.336166: Epoch 182
2024-11-29 21:57:34.337303: Current learning rate: 0.00835
2024-11-29 21:59:00.266610: Validation loss did not improve from -0.58183. Patience: 13/50
2024-11-29 21:59:00.267997: train_loss -0.7213
2024-11-29 21:59:00.269162: val_loss -0.547
2024-11-29 21:59:00.269949: Pseudo dice [0.737]
2024-11-29 21:59:00.270740: Epoch time: 85.94 s
2024-11-29 21:59:01.477432: 
2024-11-29 21:59:01.479026: Epoch 183
2024-11-29 21:59:01.479851: Current learning rate: 0.00834
2024-11-29 22:00:27.399290: Validation loss did not improve from -0.58183. Patience: 14/50
2024-11-29 22:00:27.400444: train_loss -0.7231
2024-11-29 22:00:27.401555: val_loss -0.5093
2024-11-29 22:00:27.402621: Pseudo dice [0.7181]
2024-11-29 22:00:27.403669: Epoch time: 85.92 s
2024-11-29 22:00:28.636430: 
2024-11-29 22:00:28.638587: Epoch 184
2024-11-29 22:00:28.639820: Current learning rate: 0.00833
2024-11-29 22:01:54.482385: Validation loss did not improve from -0.58183. Patience: 15/50
2024-11-29 22:01:54.483556: train_loss -0.7291
2024-11-29 22:01:54.485007: val_loss -0.5559
2024-11-29 22:01:54.485944: Pseudo dice [0.7463]
2024-11-29 22:01:54.486804: Epoch time: 85.85 s
2024-11-29 22:01:56.048896: 
2024-11-29 22:01:56.051028: Epoch 185
2024-11-29 22:01:56.052050: Current learning rate: 0.00832
2024-11-29 22:03:21.911113: Validation loss did not improve from -0.58183. Patience: 16/50
2024-11-29 22:03:21.912361: train_loss -0.7195
2024-11-29 22:03:21.913766: val_loss -0.5517
2024-11-29 22:03:21.915147: Pseudo dice [0.7417]
2024-11-29 22:03:21.916257: Epoch time: 85.86 s
2024-11-29 22:03:23.118234: 
2024-11-29 22:03:23.120474: Epoch 186
2024-11-29 22:03:23.121531: Current learning rate: 0.00831
2024-11-29 22:04:49.113505: Validation loss did not improve from -0.58183. Patience: 17/50
2024-11-29 22:04:49.114942: train_loss -0.7211
2024-11-29 22:04:49.116489: val_loss -0.5709
2024-11-29 22:04:49.117374: Pseudo dice [0.7471]
2024-11-29 22:04:49.118359: Epoch time: 86.0 s
2024-11-29 22:04:50.824752: 
2024-11-29 22:04:50.826813: Epoch 187
2024-11-29 22:04:50.828066: Current learning rate: 0.0083
2024-11-29 22:06:16.677142: Validation loss improved from -0.58183 to -0.59262! Patience: 17/50
2024-11-29 22:06:16.678690: train_loss -0.7231
2024-11-29 22:06:16.680255: val_loss -0.5926
2024-11-29 22:06:16.681423: Pseudo dice [0.759]
2024-11-29 22:06:16.682417: Epoch time: 85.85 s
2024-11-29 22:06:17.919495: 
2024-11-29 22:06:17.921206: Epoch 188
2024-11-29 22:06:17.922261: Current learning rate: 0.00829
2024-11-29 22:07:43.804151: Validation loss did not improve from -0.59262. Patience: 1/50
2024-11-29 22:07:43.805485: train_loss -0.7236
2024-11-29 22:07:43.806743: val_loss -0.5664
2024-11-29 22:07:43.807490: Pseudo dice [0.7499]
2024-11-29 22:07:43.808188: Epoch time: 85.89 s
2024-11-29 22:07:43.808916: Yayy! New best EMA pseudo Dice: 0.7413
2024-11-29 22:07:45.329171: 
2024-11-29 22:07:45.331098: Epoch 189
2024-11-29 22:07:45.332053: Current learning rate: 0.00828
2024-11-29 22:09:11.302220: Validation loss did not improve from -0.59262. Patience: 2/50
2024-11-29 22:09:11.303478: train_loss -0.6914
2024-11-29 22:09:11.304863: val_loss -0.5101
2024-11-29 22:09:11.306143: Pseudo dice [0.7119]
2024-11-29 22:09:11.307359: Epoch time: 85.98 s
2024-11-29 22:09:12.860682: 
2024-11-29 22:09:12.862700: Epoch 190
2024-11-29 22:09:12.863929: Current learning rate: 0.00827
2024-11-29 22:10:38.700128: Validation loss did not improve from -0.59262. Patience: 3/50
2024-11-29 22:10:38.701030: train_loss -0.707
2024-11-29 22:10:38.702136: val_loss -0.5378
2024-11-29 22:10:38.703323: Pseudo dice [0.7287]
2024-11-29 22:10:38.704206: Epoch time: 85.84 s
2024-11-29 22:10:39.929132: 
2024-11-29 22:10:39.931514: Epoch 191
2024-11-29 22:10:39.932887: Current learning rate: 0.00826
2024-11-29 22:12:06.208189: Validation loss did not improve from -0.59262. Patience: 4/50
2024-11-29 22:12:06.210635: train_loss -0.7111
2024-11-29 22:12:06.211895: val_loss -0.5413
2024-11-29 22:12:06.212743: Pseudo dice [0.7389]
2024-11-29 22:12:06.214193: Epoch time: 86.28 s
2024-11-29 22:12:07.563143: 
2024-11-29 22:12:07.565221: Epoch 192
2024-11-29 22:12:07.566445: Current learning rate: 0.00825
2024-11-29 22:13:33.709659: Validation loss did not improve from -0.59262. Patience: 5/50
2024-11-29 22:13:33.710929: train_loss -0.7163
2024-11-29 22:13:33.712046: val_loss -0.5397
2024-11-29 22:13:33.712811: Pseudo dice [0.732]
2024-11-29 22:13:33.713568: Epoch time: 86.15 s
2024-11-29 22:13:34.987617: 
2024-11-29 22:13:34.989693: Epoch 193
2024-11-29 22:13:34.990811: Current learning rate: 0.00824
2024-11-29 22:15:00.950380: Validation loss did not improve from -0.59262. Patience: 6/50
2024-11-29 22:15:00.951380: train_loss -0.7275
2024-11-29 22:15:00.952695: val_loss -0.5335
2024-11-29 22:15:00.953660: Pseudo dice [0.7214]
2024-11-29 22:15:00.954571: Epoch time: 85.97 s
2024-11-29 22:15:02.183450: 
2024-11-29 22:15:02.185112: Epoch 194
2024-11-29 22:15:02.185996: Current learning rate: 0.00824
2024-11-29 22:16:28.230973: Validation loss did not improve from -0.59262. Patience: 7/50
2024-11-29 22:16:28.231946: train_loss -0.7356
2024-11-29 22:16:28.233036: val_loss -0.5633
2024-11-29 22:16:28.234038: Pseudo dice [0.7575]
2024-11-29 22:16:28.234987: Epoch time: 86.05 s
2024-11-29 22:16:29.923268: 
2024-11-29 22:16:29.925513: Epoch 195
2024-11-29 22:16:29.926876: Current learning rate: 0.00823
2024-11-29 22:17:56.791441: Validation loss did not improve from -0.59262. Patience: 8/50
2024-11-29 22:17:56.796811: train_loss -0.7316
2024-11-29 22:17:56.800573: val_loss -0.5477
2024-11-29 22:17:56.801558: Pseudo dice [0.7432]
2024-11-29 22:17:56.802995: Epoch time: 86.87 s
2024-11-29 22:17:58.343284: 
2024-11-29 22:17:58.345797: Epoch 196
2024-11-29 22:17:58.347140: Current learning rate: 0.00822
2024-11-29 22:19:24.327306: Validation loss did not improve from -0.59262. Patience: 9/50
2024-11-29 22:19:24.327981: train_loss -0.7281
2024-11-29 22:19:24.328997: val_loss -0.5623
2024-11-29 22:19:24.329836: Pseudo dice [0.7458]
2024-11-29 22:19:24.330584: Epoch time: 85.99 s
2024-11-29 22:19:26.830518: 
2024-11-29 22:19:26.832859: Epoch 197
2024-11-29 22:19:26.834146: Current learning rate: 0.00821
2024-11-29 22:20:52.670337: Validation loss did not improve from -0.59262. Patience: 10/50
2024-11-29 22:20:52.671223: train_loss -0.7317
2024-11-29 22:20:52.672235: val_loss -0.5654
2024-11-29 22:20:52.673222: Pseudo dice [0.75]
2024-11-29 22:20:52.674199: Epoch time: 85.84 s
2024-11-29 22:20:53.915231: 
2024-11-29 22:20:53.917655: Epoch 198
2024-11-29 22:20:53.918948: Current learning rate: 0.0082
2024-11-29 22:22:19.838995: Validation loss did not improve from -0.59262. Patience: 11/50
2024-11-29 22:22:19.840122: train_loss -0.7312
2024-11-29 22:22:19.841221: val_loss -0.5585
2024-11-29 22:22:19.842275: Pseudo dice [0.7478]
2024-11-29 22:22:19.843174: Epoch time: 85.93 s
2024-11-29 22:22:21.141098: 
2024-11-29 22:22:21.144002: Epoch 199
2024-11-29 22:22:21.145320: Current learning rate: 0.00819
2024-11-29 22:23:47.125892: Validation loss did not improve from -0.59262. Patience: 12/50
2024-11-29 22:23:47.127807: train_loss -0.7327
2024-11-29 22:23:47.129171: val_loss -0.5594
2024-11-29 22:23:47.130287: Pseudo dice [0.7454]
2024-11-29 22:23:47.131367: Epoch time: 85.99 s
2024-11-29 22:23:47.563138: Yayy! New best EMA pseudo Dice: 0.7413
2024-11-29 22:23:49.218026: 
2024-11-29 22:23:49.220064: Epoch 200
2024-11-29 22:23:49.221060: Current learning rate: 0.00818
2024-11-29 22:25:15.049286: Validation loss did not improve from -0.59262. Patience: 13/50
2024-11-29 22:25:15.050744: train_loss -0.7404
2024-11-29 22:25:15.051820: val_loss -0.5544
2024-11-29 22:25:15.052583: Pseudo dice [0.7424]
2024-11-29 22:25:15.053281: Epoch time: 85.83 s
2024-11-29 22:25:15.054183: Yayy! New best EMA pseudo Dice: 0.7414
2024-11-29 22:25:16.647830: 
2024-11-29 22:25:16.650635: Epoch 201
2024-11-29 22:25:16.652147: Current learning rate: 0.00817
2024-11-29 22:26:42.509678: Validation loss did not improve from -0.59262. Patience: 14/50
2024-11-29 22:26:42.512205: train_loss -0.7225
2024-11-29 22:26:42.514009: val_loss -0.5351
2024-11-29 22:26:42.514897: Pseudo dice [0.7299]
2024-11-29 22:26:42.515688: Epoch time: 85.87 s
2024-11-29 22:26:43.779208: 
2024-11-29 22:26:43.781479: Epoch 202
2024-11-29 22:26:43.782517: Current learning rate: 0.00816
2024-11-29 22:28:09.703136: Validation loss did not improve from -0.59262. Patience: 15/50
2024-11-29 22:28:09.704345: train_loss -0.7177
2024-11-29 22:28:09.705146: val_loss -0.5371
2024-11-29 22:28:09.705799: Pseudo dice [0.741]
2024-11-29 22:28:09.706475: Epoch time: 85.93 s
2024-11-29 22:28:10.965373: 
2024-11-29 22:28:10.967458: Epoch 203
2024-11-29 22:28:10.968367: Current learning rate: 0.00815
2024-11-29 22:29:36.842687: Validation loss did not improve from -0.59262. Patience: 16/50
2024-11-29 22:29:36.843946: train_loss -0.7193
2024-11-29 22:29:36.845094: val_loss -0.5158
2024-11-29 22:29:36.846067: Pseudo dice [0.716]
2024-11-29 22:29:36.846977: Epoch time: 85.88 s
2024-11-29 22:29:38.093101: 
2024-11-29 22:29:38.095670: Epoch 204
2024-11-29 22:29:38.096936: Current learning rate: 0.00814
2024-11-29 22:31:03.970345: Validation loss did not improve from -0.59262. Patience: 17/50
2024-11-29 22:31:03.971498: train_loss -0.7318
2024-11-29 22:31:03.972478: val_loss -0.5713
2024-11-29 22:31:03.973465: Pseudo dice [0.7435]
2024-11-29 22:31:03.974300: Epoch time: 85.88 s
2024-11-29 22:31:05.618026: 
2024-11-29 22:31:05.620249: Epoch 205
2024-11-29 22:31:05.621411: Current learning rate: 0.00813
2024-11-29 22:32:31.519130: Validation loss did not improve from -0.59262. Patience: 18/50
2024-11-29 22:32:31.520448: train_loss -0.7279
2024-11-29 22:32:31.521429: val_loss -0.5534
2024-11-29 22:32:31.522362: Pseudo dice [0.7363]
2024-11-29 22:32:31.523250: Epoch time: 85.9 s
2024-11-29 22:32:32.698017: 
2024-11-29 22:32:32.699869: Epoch 206
2024-11-29 22:32:32.700696: Current learning rate: 0.00813
2024-11-29 22:33:58.683666: Validation loss did not improve from -0.59262. Patience: 19/50
2024-11-29 22:33:58.684950: train_loss -0.728
2024-11-29 22:33:58.686107: val_loss -0.5189
2024-11-29 22:33:58.687005: Pseudo dice [0.7277]
2024-11-29 22:33:58.687951: Epoch time: 85.99 s
2024-11-29 22:33:59.836221: 
2024-11-29 22:33:59.838326: Epoch 207
2024-11-29 22:33:59.839410: Current learning rate: 0.00812
2024-11-29 22:35:25.665170: Validation loss did not improve from -0.59262. Patience: 20/50
2024-11-29 22:35:25.666452: train_loss -0.7269
2024-11-29 22:35:25.667625: val_loss -0.5515
2024-11-29 22:35:25.668608: Pseudo dice [0.73]
2024-11-29 22:35:25.669507: Epoch time: 85.83 s
2024-11-29 22:35:27.186916: 
2024-11-29 22:35:27.189297: Epoch 208
2024-11-29 22:35:27.190550: Current learning rate: 0.00811
2024-11-29 22:36:52.976935: Validation loss did not improve from -0.59262. Patience: 21/50
2024-11-29 22:36:52.978173: train_loss -0.7365
2024-11-29 22:36:52.979354: val_loss -0.5672
2024-11-29 22:36:52.980500: Pseudo dice [0.7507]
2024-11-29 22:36:52.981498: Epoch time: 85.79 s
2024-11-29 22:36:54.180251: 
2024-11-29 22:36:54.182182: Epoch 209
2024-11-29 22:36:54.183359: Current learning rate: 0.0081
2024-11-29 22:38:20.090265: Validation loss did not improve from -0.59262. Patience: 22/50
2024-11-29 22:38:20.091290: train_loss -0.7342
2024-11-29 22:38:20.092264: val_loss -0.5502
2024-11-29 22:38:20.093167: Pseudo dice [0.7433]
2024-11-29 22:38:20.094178: Epoch time: 85.91 s
2024-11-29 22:38:21.610070: 
2024-11-29 22:38:21.612375: Epoch 210
2024-11-29 22:38:21.613403: Current learning rate: 0.00809
2024-11-29 22:39:47.488248: Validation loss did not improve from -0.59262. Patience: 23/50
2024-11-29 22:39:47.489628: train_loss -0.7383
2024-11-29 22:39:47.490968: val_loss -0.5414
2024-11-29 22:39:47.492055: Pseudo dice [0.7297]
2024-11-29 22:39:47.493110: Epoch time: 85.88 s
2024-11-29 22:39:48.682341: 
2024-11-29 22:39:48.684498: Epoch 211
2024-11-29 22:39:48.685765: Current learning rate: 0.00808
2024-11-29 22:41:14.529850: Validation loss did not improve from -0.59262. Patience: 24/50
2024-11-29 22:41:14.530984: train_loss -0.7328
2024-11-29 22:41:14.532574: val_loss -0.569
2024-11-29 22:41:14.534041: Pseudo dice [0.7479]
2024-11-29 22:41:14.535116: Epoch time: 85.85 s
2024-11-29 22:41:15.723889: 
2024-11-29 22:41:15.725908: Epoch 212
2024-11-29 22:41:15.726976: Current learning rate: 0.00807
2024-11-29 22:42:41.657720: Validation loss did not improve from -0.59262. Patience: 25/50
2024-11-29 22:42:41.659448: train_loss -0.7439
2024-11-29 22:42:41.660758: val_loss -0.5603
2024-11-29 22:42:41.661487: Pseudo dice [0.7497]
2024-11-29 22:42:41.662551: Epoch time: 85.94 s
2024-11-29 22:42:42.826057: 
2024-11-29 22:42:42.828152: Epoch 213
2024-11-29 22:42:42.829494: Current learning rate: 0.00806
2024-11-29 22:44:08.819993: Validation loss did not improve from -0.59262. Patience: 26/50
2024-11-29 22:44:08.821145: train_loss -0.7428
2024-11-29 22:44:08.822450: val_loss -0.542
2024-11-29 22:44:08.823551: Pseudo dice [0.732]
2024-11-29 22:44:08.824574: Epoch time: 86.0 s
2024-11-29 22:44:10.031266: 
2024-11-29 22:44:10.033168: Epoch 214
2024-11-29 22:44:10.034189: Current learning rate: 0.00805
2024-11-29 22:45:35.901950: Validation loss did not improve from -0.59262. Patience: 27/50
2024-11-29 22:45:35.903442: train_loss -0.7434
2024-11-29 22:45:35.904696: val_loss -0.5251
2024-11-29 22:45:35.905597: Pseudo dice [0.7253]
2024-11-29 22:45:35.906640: Epoch time: 85.87 s
2024-11-29 22:45:37.442241: 
2024-11-29 22:45:37.444944: Epoch 215
2024-11-29 22:45:37.445970: Current learning rate: 0.00804
2024-11-29 22:47:03.316411: Validation loss did not improve from -0.59262. Patience: 28/50
2024-11-29 22:47:03.317442: train_loss -0.7433
2024-11-29 22:47:03.318606: val_loss -0.5321
2024-11-29 22:47:03.319603: Pseudo dice [0.7257]
2024-11-29 22:47:03.320448: Epoch time: 85.88 s
2024-11-29 22:47:04.478751: 
2024-11-29 22:47:04.481128: Epoch 216
2024-11-29 22:47:04.482256: Current learning rate: 0.00803
2024-11-29 22:48:30.459573: Validation loss did not improve from -0.59262. Patience: 29/50
2024-11-29 22:48:30.460693: train_loss -0.74
2024-11-29 22:48:30.461729: val_loss -0.5584
2024-11-29 22:48:30.462565: Pseudo dice [0.7484]
2024-11-29 22:48:30.463480: Epoch time: 85.98 s
2024-11-29 22:48:31.616701: 
2024-11-29 22:48:31.618469: Epoch 217
2024-11-29 22:48:31.619639: Current learning rate: 0.00802
2024-11-29 22:49:57.456431: Validation loss did not improve from -0.59262. Patience: 30/50
2024-11-29 22:49:57.457567: train_loss -0.7385
2024-11-29 22:49:57.458568: val_loss -0.5281
2024-11-29 22:49:57.459345: Pseudo dice [0.7238]
2024-11-29 22:49:57.460219: Epoch time: 85.84 s
2024-11-29 22:49:58.974249: 
2024-11-29 22:49:58.976514: Epoch 218
2024-11-29 22:49:58.977775: Current learning rate: 0.00801
2024-11-29 22:51:24.846657: Validation loss did not improve from -0.59262. Patience: 31/50
2024-11-29 22:51:24.847900: train_loss -0.7409
2024-11-29 22:51:24.849174: val_loss -0.5367
2024-11-29 22:51:24.849849: Pseudo dice [0.7251]
2024-11-29 22:51:24.850792: Epoch time: 85.87 s
2024-11-29 22:51:25.983347: 
2024-11-29 22:51:25.985397: Epoch 219
2024-11-29 22:51:25.986494: Current learning rate: 0.00801
2024-11-29 22:52:51.897091: Validation loss did not improve from -0.59262. Patience: 32/50
2024-11-29 22:52:51.898439: train_loss -0.7402
2024-11-29 22:52:51.899981: val_loss -0.5527
2024-11-29 22:52:51.900865: Pseudo dice [0.7427]
2024-11-29 22:52:51.901818: Epoch time: 85.92 s
2024-11-29 22:52:53.427042: 
2024-11-29 22:52:53.429181: Epoch 220
2024-11-29 22:52:53.430363: Current learning rate: 0.008
2024-11-29 22:54:19.377454: Validation loss did not improve from -0.59262. Patience: 33/50
2024-11-29 22:54:19.378805: train_loss -0.7309
2024-11-29 22:54:19.380416: val_loss -0.5151
2024-11-29 22:54:19.381617: Pseudo dice [0.7179]
2024-11-29 22:54:19.382854: Epoch time: 85.95 s
2024-11-29 22:54:20.559754: 
2024-11-29 22:54:20.562016: Epoch 221
2024-11-29 22:54:20.563490: Current learning rate: 0.00799
2024-11-29 22:55:46.391364: Validation loss did not improve from -0.59262. Patience: 34/50
2024-11-29 22:55:46.395200: train_loss -0.7383
2024-11-29 22:55:46.396345: val_loss -0.5512
2024-11-29 22:55:46.397157: Pseudo dice [0.7426]
2024-11-29 22:55:46.397835: Epoch time: 85.84 s
2024-11-29 22:55:47.580744: 
2024-11-29 22:55:47.583362: Epoch 222
2024-11-29 22:55:47.584738: Current learning rate: 0.00798
2024-11-29 22:57:13.515065: Validation loss did not improve from -0.59262. Patience: 35/50
2024-11-29 22:57:13.516351: train_loss -0.7444
2024-11-29 22:57:13.517470: val_loss -0.5421
2024-11-29 22:57:13.518504: Pseudo dice [0.7445]
2024-11-29 22:57:13.519388: Epoch time: 85.94 s
2024-11-29 22:57:14.701625: 
2024-11-29 22:57:14.703929: Epoch 223
2024-11-29 22:57:14.705379: Current learning rate: 0.00797
2024-11-29 22:58:40.725936: Validation loss did not improve from -0.59262. Patience: 36/50
2024-11-29 22:58:40.727205: train_loss -0.7358
2024-11-29 22:58:40.728162: val_loss -0.5592
2024-11-29 22:58:40.728895: Pseudo dice [0.7473]
2024-11-29 22:58:40.729585: Epoch time: 86.03 s
2024-11-29 22:58:41.897470: 
2024-11-29 22:58:41.899611: Epoch 224
2024-11-29 22:58:41.900887: Current learning rate: 0.00796
2024-11-29 23:00:07.747970: Validation loss did not improve from -0.59262. Patience: 37/50
2024-11-29 23:00:07.749331: train_loss -0.7441
2024-11-29 23:00:07.750852: val_loss -0.5497
2024-11-29 23:00:07.751655: Pseudo dice [0.7403]
2024-11-29 23:00:07.752551: Epoch time: 85.85 s
2024-11-29 23:00:09.279427: 
2024-11-29 23:00:09.281301: Epoch 225
2024-11-29 23:00:09.282213: Current learning rate: 0.00795
2024-11-29 23:01:35.166320: Validation loss did not improve from -0.59262. Patience: 38/50
2024-11-29 23:01:35.167645: train_loss -0.7395
2024-11-29 23:01:35.168545: val_loss -0.5721
2024-11-29 23:01:35.169344: Pseudo dice [0.75]
2024-11-29 23:01:35.170064: Epoch time: 85.89 s
2024-11-29 23:01:36.318971: 
2024-11-29 23:01:36.320919: Epoch 226
2024-11-29 23:01:36.322096: Current learning rate: 0.00794
2024-11-29 23:03:02.269727: Validation loss did not improve from -0.59262. Patience: 39/50
2024-11-29 23:03:02.270644: train_loss -0.7402
2024-11-29 23:03:02.271849: val_loss -0.568
2024-11-29 23:03:02.273098: Pseudo dice [0.7409]
2024-11-29 23:03:02.274136: Epoch time: 85.95 s
2024-11-29 23:03:03.428813: 
2024-11-29 23:03:03.430906: Epoch 227
2024-11-29 23:03:03.432229: Current learning rate: 0.00793
2024-11-29 23:04:29.333440: Validation loss did not improve from -0.59262. Patience: 40/50
2024-11-29 23:04:29.334915: train_loss -0.7329
2024-11-29 23:04:29.336094: val_loss -0.5261
2024-11-29 23:04:29.337163: Pseudo dice [0.7283]
2024-11-29 23:04:29.338180: Epoch time: 85.91 s
2024-11-29 23:04:30.487694: 
2024-11-29 23:04:30.489974: Epoch 228
2024-11-29 23:04:30.491043: Current learning rate: 0.00792
2024-11-29 23:05:56.337761: Validation loss did not improve from -0.59262. Patience: 41/50
2024-11-29 23:05:56.339037: train_loss -0.7303
2024-11-29 23:05:56.340048: val_loss -0.5844
2024-11-29 23:05:56.340767: Pseudo dice [0.7562]
2024-11-29 23:05:56.341696: Epoch time: 85.85 s
2024-11-29 23:05:57.833105: 
2024-11-29 23:05:57.835390: Epoch 229
2024-11-29 23:05:57.836803: Current learning rate: 0.00791
2024-11-29 23:07:23.727968: Validation loss did not improve from -0.59262. Patience: 42/50
2024-11-29 23:07:23.729021: train_loss -0.7353
2024-11-29 23:07:23.730155: val_loss -0.5555
2024-11-29 23:07:23.731202: Pseudo dice [0.75]
2024-11-29 23:07:23.732047: Epoch time: 85.9 s
2024-11-29 23:07:25.175071: 
2024-11-29 23:07:25.177358: Epoch 230
2024-11-29 23:07:25.178764: Current learning rate: 0.0079
2024-11-29 23:08:51.173448: Validation loss did not improve from -0.59262. Patience: 43/50
2024-11-29 23:08:51.174588: train_loss -0.7346
2024-11-29 23:08:51.175510: val_loss -0.5414
2024-11-29 23:08:51.176157: Pseudo dice [0.7359]
2024-11-29 23:08:51.176896: Epoch time: 86.0 s
2024-11-29 23:08:52.347254: 
2024-11-29 23:08:52.349188: Epoch 231
2024-11-29 23:08:52.350448: Current learning rate: 0.00789
2024-11-29 23:10:18.179650: Validation loss did not improve from -0.59262. Patience: 44/50
2024-11-29 23:10:18.180717: train_loss -0.7401
2024-11-29 23:10:18.181878: val_loss -0.5864
2024-11-29 23:10:18.183038: Pseudo dice [0.7584]
2024-11-29 23:10:18.184197: Epoch time: 85.83 s
2024-11-29 23:10:18.185221: Yayy! New best EMA pseudo Dice: 0.742
2024-11-29 23:10:19.695090: 
2024-11-29 23:10:19.696937: Epoch 232
2024-11-29 23:10:19.698442: Current learning rate: 0.00789
2024-11-29 23:11:45.585206: Validation loss did not improve from -0.59262. Patience: 45/50
2024-11-29 23:11:45.586476: train_loss -0.7433
2024-11-29 23:11:45.587558: val_loss -0.5706
2024-11-29 23:11:45.588457: Pseudo dice [0.7467]
2024-11-29 23:11:45.589129: Epoch time: 85.89 s
2024-11-29 23:11:45.589818: Yayy! New best EMA pseudo Dice: 0.7425
2024-11-29 23:11:47.093212: 
2024-11-29 23:11:47.094746: Epoch 233
2024-11-29 23:11:47.095874: Current learning rate: 0.00788
2024-11-29 23:13:13.079499: Validation loss did not improve from -0.59262. Patience: 46/50
2024-11-29 23:13:13.080711: train_loss -0.732
2024-11-29 23:13:13.081803: val_loss -0.5443
2024-11-29 23:13:13.082575: Pseudo dice [0.7327]
2024-11-29 23:13:13.083400: Epoch time: 85.99 s
2024-11-29 23:13:14.250355: 
2024-11-29 23:13:14.252410: Epoch 234
2024-11-29 23:13:14.253782: Current learning rate: 0.00787
2024-11-29 23:14:40.099060: Validation loss did not improve from -0.59262. Patience: 47/50
2024-11-29 23:14:40.100265: train_loss -0.7404
2024-11-29 23:14:40.101215: val_loss -0.5455
2024-11-29 23:14:40.102045: Pseudo dice [0.7334]
2024-11-29 23:14:40.102831: Epoch time: 85.85 s
2024-11-29 23:14:41.598441: 
2024-11-29 23:14:41.600901: Epoch 235
2024-11-29 23:14:41.601910: Current learning rate: 0.00786
2024-11-29 23:16:07.442395: Validation loss did not improve from -0.59262. Patience: 48/50
2024-11-29 23:16:07.443336: train_loss -0.7428
2024-11-29 23:16:07.444537: val_loss -0.5475
2024-11-29 23:16:07.445288: Pseudo dice [0.7406]
2024-11-29 23:16:07.446209: Epoch time: 85.85 s
2024-11-29 23:16:08.588176: 
2024-11-29 23:16:08.590630: Epoch 236
2024-11-29 23:16:08.591793: Current learning rate: 0.00785
2024-11-29 23:17:34.526243: Validation loss did not improve from -0.59262. Patience: 49/50
2024-11-29 23:17:34.527513: train_loss -0.7414
2024-11-29 23:17:34.528863: val_loss -0.5209
2024-11-29 23:17:34.529929: Pseudo dice [0.7229]
2024-11-29 23:17:34.531095: Epoch time: 85.94 s
2024-11-29 23:17:35.687267: 
2024-11-29 23:17:35.689561: Epoch 237
2024-11-29 23:17:35.690813: Current learning rate: 0.00784
2024-11-29 23:19:01.839043: Validation loss did not improve from -0.59262. Patience: 50/50
2024-11-29 23:19:01.840430: train_loss -0.738
2024-11-29 23:19:01.841414: val_loss -0.5413
2024-11-29 23:19:01.842201: Pseudo dice [0.738]
2024-11-29 23:19:01.843127: Epoch time: 86.15 s
2024-11-29 23:19:03.007186: Patience reached. Stopping training.
2024-11-29 23:19:03.426182: Training done.
2024-11-29 23:19:03.632440: Using splits from existing split file: /home/gridsan/nchutisilp/datasets/nnUNet_Datasets/nnUNet_preprocessed/Dataset306_Sohee_Ajay_Calcium_OCT/splits_final.json
2024-11-29 23:19:03.635784: The split file contains 5 splits.
2024-11-29 23:19:03.637248: Desired fold for training: 1
2024-11-29 23:19:03.638265: This split has 10 training and 3 validation cases.
2024-11-29 23:19:03.639268: predicting 101-019
2024-11-29 23:19:03.691437: 101-019, shape torch.Size([1, 375, 498, 498]), rank 0
2024-11-29 23:20:43.399183: predicting 101009Pre
2024-11-29 23:20:43.423279: 101009Pre, shape torch.Size([1, 230, 498, 498]), rank 0
2024-11-29 23:21:34.672246: predicting 704-003
2024-11-29 23:21:34.697531: 704-003, shape torch.Size([1, 375, 498, 498]), rank 0
2024-11-29 23:23:23.088585: Validation complete
2024-11-29 23:23:23.089794: Mean Validation Dice:  0.7388985757361537

############################
INFO: You are using the old nnU-Net default plans. We have updated our recommendations. Please consider using those instead! Read more here: https://github.com/MIC-DKFZ/nnUNet/blob/master/documentation/resenc_presets.md
############################

Using device: cuda:0

#######################################################################
Please cite the following paper when using nnU-Net:
Isensee, F., Jaeger, P. F., Kohl, S. A., Petersen, J., & Maier-Hein, K. H. (2021). nnU-Net: a self-configuring method for deep learning-based biomedical image segmentation. Nature methods, 18(2), 203-211.
#######################################################################

2024-11-29 23:23:31.682369: Using torch.compile...

############################
INFO: You are using the old nnU-Net default plans. We have updated our recommendations. Please consider using those instead! Read more here: https://github.com/MIC-DKFZ/nnUNet/blob/master/documentation/resenc_presets.md
############################

Using device: cuda:0

#######################################################################
Please cite the following paper when using nnU-Net:
Isensee, F., Jaeger, P. F., Kohl, S. A., Petersen, J., & Maier-Hein, K. H. (2021). nnU-Net: a self-configuring method for deep learning-based biomedical image segmentation. Nature methods, 18(2), 203-211.
#######################################################################

2024-11-29 23:23:31.705291: Using torch.compile...
/home/gridsan/nchutisilp/.local/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn("The verbose parameter is deprecated. Please use get_last_lr() "
/home/gridsan/nchutisilp/.local/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn("The verbose parameter is deprecated. Please use get_last_lr() "
################### Loading pretrained weights from file  /home/gridsan/nchutisilp/projects/OpenAI-CLIP/clip_pretrained_nnUNet.pt ###################
Below is the list of overlapping blocks in pretrained model and nnUNet architecture:
encoder.stages.0.0.convs.0.conv.weight shape torch.Size([32, 1, 3, 3, 3])
encoder.stages.0.0.convs.0.conv.bias shape torch.Size([32])
encoder.stages.0.0.convs.0.norm.weight shape torch.Size([32])
encoder.stages.0.0.convs.0.norm.bias shape torch.Size([32])
encoder.stages.0.0.convs.0.all_modules.0.weight shape torch.Size([32, 1, 3, 3, 3])
encoder.stages.0.0.convs.0.all_modules.0.bias shape torch.Size([32])
encoder.stages.0.0.convs.0.all_modules.1.weight shape torch.Size([32])
encoder.stages.0.0.convs.0.all_modules.1.bias shape torch.Size([32])
encoder.stages.0.0.convs.1.conv.weight shape torch.Size([32, 32, 3, 3, 3])
encoder.stages.0.0.convs.1.conv.bias shape torch.Size([32])
encoder.stages.0.0.convs.1.norm.weight shape torch.Size([32])
encoder.stages.0.0.convs.1.norm.bias shape torch.Size([32])
encoder.stages.0.0.convs.1.all_modules.0.weight shape torch.Size([32, 32, 3, 3, 3])
encoder.stages.0.0.convs.1.all_modules.0.bias shape torch.Size([32])
encoder.stages.0.0.convs.1.all_modules.1.weight shape torch.Size([32])
encoder.stages.0.0.convs.1.all_modules.1.bias shape torch.Size([32])
encoder.stages.1.0.convs.0.conv.weight shape torch.Size([64, 32, 3, 3, 3])
encoder.stages.1.0.convs.0.conv.bias shape torch.Size([64])
encoder.stages.1.0.convs.0.norm.weight shape torch.Size([64])
encoder.stages.1.0.convs.0.norm.bias shape torch.Size([64])
encoder.stages.1.0.convs.0.all_modules.0.weight shape torch.Size([64, 32, 3, 3, 3])
encoder.stages.1.0.convs.0.all_modules.0.bias shape torch.Size([64])
encoder.stages.1.0.convs.0.all_modules.1.weight shape torch.Size([64])
encoder.stages.1.0.convs.0.all_modules.1.bias shape torch.Size([64])
encoder.stages.1.0.convs.1.conv.weight shape torch.Size([64, 64, 3, 3, 3])
encoder.stages.1.0.convs.1.conv.bias shape torch.Size([64])
encoder.stages.1.0.convs.1.norm.weight shape torch.Size([64])
encoder.stages.1.0.convs.1.norm.bias shape torch.Size([64])
encoder.stages.1.0.convs.1.all_modules.0.weight shape torch.Size([64, 64, 3, 3, 3])
encoder.stages.1.0.convs.1.all_modules.0.bias shape torch.Size([64])
encoder.stages.1.0.convs.1.all_modules.1.weight shape torch.Size([64])
encoder.stages.1.0.convs.1.all_modules.1.bias shape torch.Size([64])
encoder.stages.2.0.convs.0.conv.weight shape torch.Size([128, 64, 3, 3, 3])
encoder.stages.2.0.convs.0.conv.bias shape torch.Size([128])
encoder.stages.2.0.convs.0.norm.weight shape torch.Size([128])
encoder.stages.2.0.convs.0.norm.bias shape torch.Size([128])
encoder.stages.2.0.convs.0.all_modules.0.weight shape torch.Size([128, 64, 3, 3, 3])
encoder.stages.2.0.convs.0.all_modules.0.bias shape torch.Size([128])
encoder.stages.2.0.convs.0.all_modules.1.weight shape torch.Size([128])
encoder.stages.2.0.convs.0.all_modules.1.bias shape torch.Size([128])
encoder.stages.2.0.convs.1.conv.weight shape torch.Size([128, 128, 3, 3, 3])
encoder.stages.2.0.convs.1.conv.bias shape torch.Size([128])
encoder.stages.2.0.convs.1.norm.weight shape torch.Size([128])
encoder.stages.2.0.convs.1.norm.bias shape torch.Size([128])
encoder.stages.2.0.convs.1.all_modules.0.weight shape torch.Size([128, 128, 3, 3, 3])
encoder.stages.2.0.convs.1.all_modules.0.bias shape torch.Size([128])
encoder.stages.2.0.convs.1.all_modules.1.weight shape torch.Size([128])
encoder.stages.2.0.convs.1.all_modules.1.bias shape torch.Size([128])
encoder.stages.3.0.convs.0.conv.weight shape torch.Size([256, 128, 3, 3, 3])
encoder.stages.3.0.convs.0.conv.bias shape torch.Size([256])
encoder.stages.3.0.convs.0.norm.weight shape torch.Size([256])
encoder.stages.3.0.convs.0.norm.bias shape torch.Size([256])
encoder.stages.3.0.convs.0.all_modules.0.weight shape torch.Size([256, 128, 3, 3, 3])
encoder.stages.3.0.convs.0.all_modules.0.bias shape torch.Size([256])
encoder.stages.3.0.convs.0.all_modules.1.weight shape torch.Size([256])
encoder.stages.3.0.convs.0.all_modules.1.bias shape torch.Size([256])
encoder.stages.3.0.convs.1.conv.weight shape torch.Size([256, 256, 3, 3, 3])
encoder.stages.3.0.convs.1.conv.bias shape torch.Size([256])
encoder.stages.3.0.convs.1.norm.weight shape torch.Size([256])
encoder.stages.3.0.convs.1.norm.bias shape torch.Size([256])
encoder.stages.3.0.convs.1.all_modules.0.weight shape torch.Size([256, 256, 3, 3, 3])
encoder.stages.3.0.convs.1.all_modules.0.bias shape torch.Size([256])
encoder.stages.3.0.convs.1.all_modules.1.weight shape torch.Size([256])
encoder.stages.3.0.convs.1.all_modules.1.bias shape torch.Size([256])
encoder.stages.4.0.convs.0.conv.weight shape torch.Size([320, 256, 3, 3, 3])
encoder.stages.4.0.convs.0.conv.bias shape torch.Size([320])
encoder.stages.4.0.convs.0.norm.weight shape torch.Size([320])
encoder.stages.4.0.convs.0.norm.bias shape torch.Size([320])
encoder.stages.4.0.convs.0.all_modules.0.weight shape torch.Size([320, 256, 3, 3, 3])
encoder.stages.4.0.convs.0.all_modules.0.bias shape torch.Size([320])
encoder.stages.4.0.convs.0.all_modules.1.weight shape torch.Size([320])
encoder.stages.4.0.convs.0.all_modules.1.bias shape torch.Size([320])
encoder.stages.4.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.4.0.convs.1.conv.bias shape torch.Size([320])
encoder.stages.4.0.convs.1.norm.weight shape torch.Size([320])
encoder.stages.4.0.convs.1.norm.bias shape torch.Size([320])
encoder.stages.4.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.4.0.convs.1.all_modules.0.bias shape torch.Size([320])
encoder.stages.4.0.convs.1.all_modules.1.weight shape torch.Size([320])
encoder.stages.4.0.convs.1.all_modules.1.bias shape torch.Size([320])
encoder.stages.5.0.convs.0.conv.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.5.0.convs.0.conv.bias shape torch.Size([320])
encoder.stages.5.0.convs.0.norm.weight shape torch.Size([320])
encoder.stages.5.0.convs.0.norm.bias shape torch.Size([320])
encoder.stages.5.0.convs.0.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.5.0.convs.0.all_modules.0.bias shape torch.Size([320])
encoder.stages.5.0.convs.0.all_modules.1.weight shape torch.Size([320])
encoder.stages.5.0.convs.0.all_modules.1.bias shape torch.Size([320])
encoder.stages.5.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.5.0.convs.1.conv.bias shape torch.Size([320])
encoder.stages.5.0.convs.1.norm.weight shape torch.Size([320])
encoder.stages.5.0.convs.1.norm.bias shape torch.Size([320])
encoder.stages.5.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.5.0.convs.1.all_modules.0.bias shape torch.Size([320])
encoder.stages.5.0.convs.1.all_modules.1.weight shape torch.Size([320])
encoder.stages.5.0.convs.1.all_modules.1.bias shape torch.Size([320])
decoder.encoder.stages.0.0.convs.0.conv.weight shape torch.Size([32, 1, 3, 3, 3])
decoder.encoder.stages.0.0.convs.0.conv.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.norm.weight shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.norm.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.all_modules.0.weight shape torch.Size([32, 1, 3, 3, 3])
decoder.encoder.stages.0.0.convs.0.all_modules.0.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.all_modules.1.weight shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.all_modules.1.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.conv.weight shape torch.Size([32, 32, 3, 3, 3])
decoder.encoder.stages.0.0.convs.1.conv.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.norm.weight shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.norm.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.all_modules.0.weight shape torch.Size([32, 32, 3, 3, 3])
decoder.encoder.stages.0.0.convs.1.all_modules.0.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.all_modules.1.weight shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.all_modules.1.bias shape torch.Size([32])
decoder.encoder.stages.1.0.convs.0.conv.weight shape torch.Size([64, 32, 3, 3, 3])
decoder.encoder.stages.1.0.convs.0.conv.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.norm.weight shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.norm.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.all_modules.0.weight shape torch.Size([64, 32, 3, 3, 3])
decoder.encoder.stages.1.0.convs.0.all_modules.0.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.all_modules.1.weight shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.all_modules.1.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.conv.weight shape torch.Size([64, 64, 3, 3, 3])
decoder.encoder.stages.1.0.convs.1.conv.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.norm.weight shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.norm.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.all_modules.0.weight shape torch.Size([64, 64, 3, 3, 3])
decoder.encoder.stages.1.0.convs.1.all_modules.0.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.all_modules.1.weight shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.all_modules.1.bias shape torch.Size([64])
decoder.encoder.stages.2.0.convs.0.conv.weight shape torch.Size([128, 64, 3, 3, 3])
decoder.encoder.stages.2.0.convs.0.conv.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.norm.weight shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.norm.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.all_modules.0.weight shape torch.Size([128, 64, 3, 3, 3])
decoder.encoder.stages.2.0.convs.0.all_modules.0.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.all_modules.1.weight shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.all_modules.1.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.conv.weight shape torch.Size([128, 128, 3, 3, 3])
decoder.encoder.stages.2.0.convs.1.conv.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.norm.weight shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.norm.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.all_modules.0.weight shape torch.Size([128, 128, 3, 3, 3])
decoder.encoder.stages.2.0.convs.1.all_modules.0.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.all_modules.1.weight shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.all_modules.1.bias shape torch.Size([128])
decoder.encoder.stages.3.0.convs.0.conv.weight shape torch.Size([256, 128, 3, 3, 3])
decoder.encoder.stages.3.0.convs.0.conv.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.norm.weight shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.norm.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.all_modules.0.weight shape torch.Size([256, 128, 3, 3, 3])
decoder.encoder.stages.3.0.convs.0.all_modules.0.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.all_modules.1.weight shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.all_modules.1.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.conv.weight shape torch.Size([256, 256, 3, 3, 3])
decoder.encoder.stages.3.0.convs.1.conv.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.norm.weight shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.norm.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.all_modules.0.weight shape torch.Size([256, 256, 3, 3, 3])
decoder.encoder.stages.3.0.convs.1.all_modules.0.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.all_modules.1.weight shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.all_modules.1.bias shape torch.Size([256])
decoder.encoder.stages.4.0.convs.0.conv.weight shape torch.Size([320, 256, 3, 3, 3])
decoder.encoder.stages.4.0.convs.0.conv.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.norm.weight shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.norm.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.all_modules.0.weight shape torch.Size([320, 256, 3, 3, 3])
decoder.encoder.stages.4.0.convs.0.all_modules.0.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.all_modules.1.weight shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.all_modules.1.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.4.0.convs.1.conv.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.norm.weight shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.norm.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.4.0.convs.1.all_modules.0.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.all_modules.1.weight shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.all_modules.1.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.conv.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.5.0.convs.0.conv.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.norm.weight shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.norm.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.5.0.convs.0.all_modules.0.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.all_modules.1.weight shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.all_modules.1.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.5.0.convs.1.conv.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.norm.weight shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.norm.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.5.0.convs.1.all_modules.0.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.all_modules.1.weight shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.all_modules.1.bias shape torch.Size([320])
decoder.stages.0.convs.0.conv.weight shape torch.Size([320, 640, 3, 3, 3])
decoder.stages.0.convs.0.conv.bias shape torch.Size([320])
decoder.stages.0.convs.0.norm.weight shape torch.Size([320])
decoder.stages.0.convs.0.norm.bias shape torch.Size([320])
decoder.stages.0.convs.0.all_modules.0.weight shape torch.Size([320, 640, 3, 3, 3])
decoder.stages.0.convs.0.all_modules.0.bias shape torch.Size([320])
decoder.stages.0.convs.0.all_modules.1.weight shape torch.Size([320])
decoder.stages.0.convs.0.all_modules.1.bias shape torch.Size([320])
decoder.stages.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.stages.0.convs.1.conv.bias shape torch.Size([320])
decoder.stages.0.convs.1.norm.weight shape torch.Size([320])
decoder.stages.0.convs.1.norm.bias shape torch.Size([320])
decoder.stages.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.stages.0.convs.1.all_modules.0.bias shape torch.Size([320])
decoder.stages.0.convs.1.all_modules.1.weight shape torch.Size([320])
decoder.stages.0.convs.1.all_modules.1.bias shape torch.Size([320])
decoder.stages.1.convs.0.conv.weight shape torch.Size([256, 512, 3, 3, 3])
decoder.stages.1.convs.0.conv.bias shape torch.Size([256])
decoder.stages.1.convs.0.norm.weight shape torch.Size([256])
decoder.stages.1.convs.0.norm.bias shape torch.Size([256])
decoder.stages.1.convs.0.all_modules.0.weight shape torch.Size([256, 512, 3, 3, 3])
decoder.stages.1.convs.0.all_modules.0.bias shape torch.Size([256])
decoder.stages.1.convs.0.all_modules.1.weight shape torch.Size([256])
decoder.stages.1.convs.0.all_modules.1.bias shape torch.Size([256])
decoder.stages.1.convs.1.conv.weight shape torch.Size([256, 256, 3, 3, 3])
decoder.stages.1.convs.1.conv.bias shape torch.Size([256])
decoder.stages.1.convs.1.norm.weight shape torch.Size([256])
decoder.stages.1.convs.1.norm.bias shape torch.Size([256])
decoder.stages.1.convs.1.all_modules.0.weight shape torch.Size([256, 256, 3, 3, 3])
decoder.stages.1.convs.1.all_modules.0.bias shape torch.Size([256])
decoder.stages.1.convs.1.all_modules.1.weight shape torch.Size([256])
decoder.stages.1.convs.1.all_modules.1.bias shape torch.Size([256])
decoder.stages.2.convs.0.conv.weight shape torch.Size([128, 256, 3, 3, 3])
decoder.stages.2.convs.0.conv.bias shape torch.Size([128])
decoder.stages.2.convs.0.norm.weight shape torch.Size([128])
decoder.stages.2.convs.0.norm.bias shape torch.Size([128])
decoder.stages.2.convs.0.all_modules.0.weight shape torch.Size([128, 256, 3, 3, 3])
decoder.stages.2.convs.0.all_modules.0.bias shape torch.Size([128])
decoder.stages.2.convs.0.all_modules.1.weight shape torch.Size([128])
decoder.stages.2.convs.0.all_modules.1.bias shape torch.Size([128])
decoder.stages.2.convs.1.conv.weight shape torch.Size([128, 128, 3, 3, 3])
decoder.stages.2.convs.1.conv.bias shape torch.Size([128])
decoder.stages.2.convs.1.norm.weight shape torch.Size([128])
decoder.stages.2.convs.1.norm.bias shape torch.Size([128])
decoder.stages.2.convs.1.all_modules.0.weight shape torch.Size([128, 128, 3, 3, 3])
decoder.stages.2.convs.1.all_modules.0.bias shape torch.Size([128])
decoder.stages.2.convs.1.all_modules.1.weight shape torch.Size([128])
decoder.stages.2.convs.1.all_modules.1.bias shape torch.Size([128])
decoder.stages.3.convs.0.conv.weight shape torch.Size([64, 128, 3, 3, 3])
decoder.stages.3.convs.0.conv.bias shape torch.Size([64])
decoder.stages.3.convs.0.norm.weight shape torch.Size([64])
decoder.stages.3.convs.0.norm.bias shape torch.Size([64])
decoder.stages.3.convs.0.all_modules.0.weight shape torch.Size([64, 128, 3, 3, 3])
decoder.stages.3.convs.0.all_modules.0.bias shape torch.Size([64])
decoder.stages.3.convs.0.all_modules.1.weight shape torch.Size([64])
decoder.stages.3.convs.0.all_modules.1.bias shape torch.Size([64])
decoder.stages.3.convs.1.conv.weight shape torch.Size([64, 64, 3, 3, 3])
decoder.stages.3.convs.1.conv.bias shape torch.Size([64])
decoder.stages.3.convs.1.norm.weight shape torch.Size([64])
decoder.stages.3.convs.1.norm.bias shape torch.Size([64])
decoder.stages.3.convs.1.all_modules.0.weight shape torch.Size([64, 64, 3, 3, 3])
decoder.stages.3.convs.1.all_modules.0.bias shape torch.Size([64])
decoder.stages.3.convs.1.all_modules.1.weight shape torch.Size([64])
decoder.stages.3.convs.1.all_modules.1.bias shape torch.Size([64])
decoder.stages.4.convs.0.conv.weight shape torch.Size([32, 64, 3, 3, 3])
decoder.stages.4.convs.0.conv.bias shape torch.Size([32])
decoder.stages.4.convs.0.norm.weight shape torch.Size([32])
decoder.stages.4.convs.0.norm.bias shape torch.Size([32])
decoder.stages.4.convs.0.all_modules.0.weight shape torch.Size([32, 64, 3, 3, 3])
decoder.stages.4.convs.0.all_modules.0.bias shape torch.Size([32])
decoder.stages.4.convs.0.all_modules.1.weight shape torch.Size([32])
decoder.stages.4.convs.0.all_modules.1.bias shape torch.Size([32])
decoder.stages.4.convs.1.conv.weight shape torch.Size([32, 32, 3, 3, 3])
decoder.stages.4.convs.1.conv.bias shape torch.Size([32])
decoder.stages.4.convs.1.norm.weight shape torch.Size([32])
decoder.stages.4.convs.1.norm.bias shape torch.Size([32])
decoder.stages.4.convs.1.all_modules.0.weight shape torch.Size([32, 32, 3, 3, 3])
decoder.stages.4.convs.1.all_modules.0.bias shape torch.Size([32])
decoder.stages.4.convs.1.all_modules.1.weight shape torch.Size([32])
decoder.stages.4.convs.1.all_modules.1.bias shape torch.Size([32])
decoder.transpconvs.0.weight shape torch.Size([320, 320, 1, 2, 2])
decoder.transpconvs.0.bias shape torch.Size([320])
decoder.transpconvs.1.weight shape torch.Size([320, 256, 2, 2, 2])
decoder.transpconvs.1.bias shape torch.Size([256])
decoder.transpconvs.2.weight shape torch.Size([256, 128, 2, 2, 2])
decoder.transpconvs.2.bias shape torch.Size([128])
decoder.transpconvs.3.weight shape torch.Size([128, 64, 2, 2, 2])
decoder.transpconvs.3.bias shape torch.Size([64])
decoder.transpconvs.4.weight shape torch.Size([64, 32, 2, 2, 2])
decoder.transpconvs.4.bias shape torch.Size([32])
################### Done ###################
2024-11-29 23:23:52.210813: do_dummy_2d_data_aug: True
2024-11-29 23:23:52.212944: Using splits from existing split file: /home/gridsan/nchutisilp/datasets/nnUNet_Datasets/nnUNet_preprocessed/Dataset306_Sohee_Ajay_Calcium_OCT/splits_final.json
2024-11-29 23:23:52.214651: The split file contains 5 splits.
2024-11-29 23:23:52.215553: Desired fold for training: 2
2024-11-29 23:23:52.216337: This split has 10 training and 3 validation cases.
################### Loading pretrained weights from file  /home/gridsan/nchutisilp/projects/OpenAI-CLIP/clip_pretrained_nnUNet.pt ###################
Below is the list of overlapping blocks in pretrained model and nnUNet architecture:
encoder.stages.0.0.convs.0.conv.weight shape torch.Size([32, 1, 3, 3, 3])
encoder.stages.0.0.convs.0.conv.bias shape torch.Size([32])
encoder.stages.0.0.convs.0.norm.weight shape torch.Size([32])
encoder.stages.0.0.convs.0.norm.bias shape torch.Size([32])
encoder.stages.0.0.convs.0.all_modules.0.weight shape torch.Size([32, 1, 3, 3, 3])
encoder.stages.0.0.convs.0.all_modules.0.bias shape torch.Size([32])
encoder.stages.0.0.convs.0.all_modules.1.weight shape torch.Size([32])
encoder.stages.0.0.convs.0.all_modules.1.bias shape torch.Size([32])
encoder.stages.0.0.convs.1.conv.weight shape torch.Size([32, 32, 3, 3, 3])
encoder.stages.0.0.convs.1.conv.bias shape torch.Size([32])
encoder.stages.0.0.convs.1.norm.weight shape torch.Size([32])
encoder.stages.0.0.convs.1.norm.bias shape torch.Size([32])
encoder.stages.0.0.convs.1.all_modules.0.weight shape torch.Size([32, 32, 3, 3, 3])
encoder.stages.0.0.convs.1.all_modules.0.bias shape torch.Size([32])
encoder.stages.0.0.convs.1.all_modules.1.weight shape torch.Size([32])
encoder.stages.0.0.convs.1.all_modules.1.bias shape torch.Size([32])
encoder.stages.1.0.convs.0.conv.weight shape torch.Size([64, 32, 3, 3, 3])
encoder.stages.1.0.convs.0.conv.bias shape torch.Size([64])
encoder.stages.1.0.convs.0.norm.weight shape torch.Size([64])
encoder.stages.1.0.convs.0.norm.bias shape torch.Size([64])
encoder.stages.1.0.convs.0.all_modules.0.weight shape torch.Size([64, 32, 3, 3, 3])
encoder.stages.1.0.convs.0.all_modules.0.bias shape torch.Size([64])
encoder.stages.1.0.convs.0.all_modules.1.weight shape torch.Size([64])
encoder.stages.1.0.convs.0.all_modules.1.bias shape torch.Size([64])
encoder.stages.1.0.convs.1.conv.weight shape torch.Size([64, 64, 3, 3, 3])
encoder.stages.1.0.convs.1.conv.bias shape torch.Size([64])
encoder.stages.1.0.convs.1.norm.weight shape torch.Size([64])
encoder.stages.1.0.convs.1.norm.bias shape torch.Size([64])
encoder.stages.1.0.convs.1.all_modules.0.weight shape torch.Size([64, 64, 3, 3, 3])
encoder.stages.1.0.convs.1.all_modules.0.bias shape torch.Size([64])
encoder.stages.1.0.convs.1.all_modules.1.weight shape torch.Size([64])
encoder.stages.1.0.convs.1.all_modules.1.bias shape torch.Size([64])
encoder.stages.2.0.convs.0.conv.weight shape torch.Size([128, 64, 3, 3, 3])
encoder.stages.2.0.convs.0.conv.bias shape torch.Size([128])
encoder.stages.2.0.convs.0.norm.weight shape torch.Size([128])
encoder.stages.2.0.convs.0.norm.bias shape torch.Size([128])
encoder.stages.2.0.convs.0.all_modules.0.weight shape torch.Size([128, 64, 3, 3, 3])
encoder.stages.2.0.convs.0.all_modules.0.bias shape torch.Size([128])
encoder.stages.2.0.convs.0.all_modules.1.weight shape torch.Size([128])
encoder.stages.2.0.convs.0.all_modules.1.bias shape torch.Size([128])
encoder.stages.2.0.convs.1.conv.weight shape torch.Size([128, 128, 3, 3, 3])
encoder.stages.2.0.convs.1.conv.bias shape torch.Size([128])
encoder.stages.2.0.convs.1.norm.weight shape torch.Size([128])
encoder.stages.2.0.convs.1.norm.bias shape torch.Size([128])
encoder.stages.2.0.convs.1.all_modules.0.weight shape torch.Size([128, 128, 3, 3, 3])
encoder.stages.2.0.convs.1.all_modules.0.bias shape torch.Size([128])
encoder.stages.2.0.convs.1.all_modules.1.weight shape torch.Size([128])
encoder.stages.2.0.convs.1.all_modules.1.bias shape torch.Size([128])
encoder.stages.3.0.convs.0.conv.weight shape torch.Size([256, 128, 3, 3, 3])
encoder.stages.3.0.convs.0.conv.bias shape torch.Size([256])
encoder.stages.3.0.convs.0.norm.weight shape torch.Size([256])
encoder.stages.3.0.convs.0.norm.bias shape torch.Size([256])
encoder.stages.3.0.convs.0.all_modules.0.weight shape torch.Size([256, 128, 3, 3, 3])
encoder.stages.3.0.convs.0.all_modules.0.bias shape torch.Size([256])
encoder.stages.3.0.convs.0.all_modules.1.weight shape torch.Size([256])
encoder.stages.3.0.convs.0.all_modules.1.bias shape torch.Size([256])
encoder.stages.3.0.convs.1.conv.weight shape torch.Size([256, 256, 3, 3, 3])
encoder.stages.3.0.convs.1.conv.bias shape torch.Size([256])
encoder.stages.3.0.convs.1.norm.weight shape torch.Size([256])
encoder.stages.3.0.convs.1.norm.bias shape torch.Size([256])
encoder.stages.3.0.convs.1.all_modules.0.weight shape torch.Size([256, 256, 3, 3, 3])
encoder.stages.3.0.convs.1.all_modules.0.bias shape torch.Size([256])
encoder.stages.3.0.convs.1.all_modules.1.weight shape torch.Size([256])
encoder.stages.3.0.convs.1.all_modules.1.bias shape torch.Size([256])
encoder.stages.4.0.convs.0.conv.weight shape torch.Size([320, 256, 3, 3, 3])
encoder.stages.4.0.convs.0.conv.bias shape torch.Size([320])
encoder.stages.4.0.convs.0.norm.weight shape torch.Size([320])
encoder.stages.4.0.convs.0.norm.bias shape torch.Size([320])
encoder.stages.4.0.convs.0.all_modules.0.weight shape torch.Size([320, 256, 3, 3, 3])
encoder.stages.4.0.convs.0.all_modules.0.bias shape torch.Size([320])
encoder.stages.4.0.convs.0.all_modules.1.weight shape torch.Size([320])
encoder.stages.4.0.convs.0.all_modules.1.bias shape torch.Size([320])
encoder.stages.4.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.4.0.convs.1.conv.bias shape torch.Size([320])
encoder.stages.4.0.convs.1.norm.weight shape torch.Size([320])
encoder.stages.4.0.convs.1.norm.bias shape torch.Size([320])
encoder.stages.4.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.4.0.convs.1.all_modules.0.bias shape torch.Size([320])
encoder.stages.4.0.convs.1.all_modules.1.weight shape torch.Size([320])
encoder.stages.4.0.convs.1.all_modules.1.bias shape torch.Size([320])
encoder.stages.5.0.convs.0.conv.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.5.0.convs.0.conv.bias shape torch.Size([320])
encoder.stages.5.0.convs.0.norm.weight shape torch.Size([320])
encoder.stages.5.0.convs.0.norm.bias shape torch.Size([320])
encoder.stages.5.0.convs.0.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.5.0.convs.0.all_modules.0.bias shape torch.Size([320])
encoder.stages.5.0.convs.0.all_modules.1.weight shape torch.Size([320])
encoder.stages.5.0.convs.0.all_modules.1.bias shape torch.Size([320])
encoder.stages.5.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.5.0.convs.1.conv.bias shape torch.Size([320])
encoder.stages.5.0.convs.1.norm.weight shape torch.Size([320])
encoder.stages.5.0.convs.1.norm.bias shape torch.Size([320])
encoder.stages.5.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.5.0.convs.1.all_modules.0.bias shape torch.Size([320])
encoder.stages.5.0.convs.1.all_modules.1.weight shape torch.Size([320])
encoder.stages.5.0.convs.1.all_modules.1.bias shape torch.Size([320])
decoder.encoder.stages.0.0.convs.0.conv.weight shape torch.Size([32, 1, 3, 3, 3])
decoder.encoder.stages.0.0.convs.0.conv.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.norm.weight shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.norm.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.all_modules.0.weight shape torch.Size([32, 1, 3, 3, 3])
decoder.encoder.stages.0.0.convs.0.all_modules.0.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.all_modules.1.weight shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.all_modules.1.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.conv.weight shape torch.Size([32, 32, 3, 3, 3])
decoder.encoder.stages.0.0.convs.1.conv.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.norm.weight shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.norm.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.all_modules.0.weight shape torch.Size([32, 32, 3, 3, 3])
decoder.encoder.stages.0.0.convs.1.all_modules.0.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.all_modules.1.weight shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.all_modules.1.bias shape torch.Size([32])
decoder.encoder.stages.1.0.convs.0.conv.weight shape torch.Size([64, 32, 3, 3, 3])
decoder.encoder.stages.1.0.convs.0.conv.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.norm.weight shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.norm.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.all_modules.0.weight shape torch.Size([64, 32, 3, 3, 3])
decoder.encoder.stages.1.0.convs.0.all_modules.0.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.all_modules.1.weight shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.all_modules.1.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.conv.weight shape torch.Size([64, 64, 3, 3, 3])
decoder.encoder.stages.1.0.convs.1.conv.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.norm.weight shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.norm.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.all_modules.0.weight shape torch.Size([64, 64, 3, 3, 3])
decoder.encoder.stages.1.0.convs.1.all_modules.0.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.all_modules.1.weight shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.all_modules.1.bias shape torch.Size([64])
decoder.encoder.stages.2.0.convs.0.conv.weight shape torch.Size([128, 64, 3, 3, 3])
decoder.encoder.stages.2.0.convs.0.conv.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.norm.weight shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.norm.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.all_modules.0.weight shape torch.Size([128, 64, 3, 3, 3])
decoder.encoder.stages.2.0.convs.0.all_modules.0.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.all_modules.1.weight shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.all_modules.1.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.conv.weight shape torch.Size([128, 128, 3, 3, 3])
decoder.encoder.stages.2.0.convs.1.conv.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.norm.weight shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.norm.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.all_modules.0.weight shape torch.Size([128, 128, 3, 3, 3])
decoder.encoder.stages.2.0.convs.1.all_modules.0.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.all_modules.1.weight shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.all_modules.1.bias shape torch.Size([128])
decoder.encoder.stages.3.0.convs.0.conv.weight shape torch.Size([256, 128, 3, 3, 3])
decoder.encoder.stages.3.0.convs.0.conv.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.norm.weight shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.norm.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.all_modules.0.weight shape torch.Size([256, 128, 3, 3, 3])
decoder.encoder.stages.3.0.convs.0.all_modules.0.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.all_modules.1.weight shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.all_modules.1.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.conv.weight shape torch.Size([256, 256, 3, 3, 3])
decoder.encoder.stages.3.0.convs.1.conv.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.norm.weight shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.norm.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.all_modules.0.weight shape torch.Size([256, 256, 3, 3, 3])
decoder.encoder.stages.3.0.convs.1.all_modules.0.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.all_modules.1.weight shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.all_modules.1.bias shape torch.Size([256])
decoder.encoder.stages.4.0.convs.0.conv.weight shape torch.Size([320, 256, 3, 3, 3])
decoder.encoder.stages.4.0.convs.0.conv.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.norm.weight shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.norm.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.all_modules.0.weight shape torch.Size([320, 256, 3, 3, 3])
decoder.encoder.stages.4.0.convs.0.all_modules.0.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.all_modules.1.weight shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.all_modules.1.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.4.0.convs.1.conv.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.norm.weight shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.norm.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.4.0.convs.1.all_modules.0.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.all_modules.1.weight shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.all_modules.1.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.conv.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.5.0.convs.0.conv.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.norm.weight shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.norm.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.5.0.convs.0.all_modules.0.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.all_modules.1.weight shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.all_modules.1.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.5.0.convs.1.conv.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.norm.weight shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.norm.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.5.0.convs.1.all_modules.0.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.all_modules.1.weight shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.all_modules.1.bias shape torch.Size([320])
decoder.stages.0.convs.0.conv.weight shape torch.Size([320, 640, 3, 3, 3])
decoder.stages.0.convs.0.conv.bias shape torch.Size([320])
decoder.stages.0.convs.0.norm.weight shape torch.Size([320])
decoder.stages.0.convs.0.norm.bias shape torch.Size([320])
decoder.stages.0.convs.0.all_modules.0.weight shape torch.Size([320, 640, 3, 3, 3])
decoder.stages.0.convs.0.all_modules.0.bias shape torch.Size([320])
decoder.stages.0.convs.0.all_modules.1.weight shape torch.Size([320])
decoder.stages.0.convs.0.all_modules.1.bias shape torch.Size([320])
decoder.stages.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.stages.0.convs.1.conv.bias shape torch.Size([320])
decoder.stages.0.convs.1.norm.weight shape torch.Size([320])
decoder.stages.0.convs.1.norm.bias shape torch.Size([320])
decoder.stages.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.stages.0.convs.1.all_modules.0.bias shape torch.Size([320])
decoder.stages.0.convs.1.all_modules.1.weight shape torch.Size([320])
decoder.stages.0.convs.1.all_modules.1.bias shape torch.Size([320])
decoder.stages.1.convs.0.conv.weight shape torch.Size([256, 512, 3, 3, 3])
decoder.stages.1.convs.0.conv.bias shape torch.Size([256])
decoder.stages.1.convs.0.norm.weight shape torch.Size([256])
decoder.stages.1.convs.0.norm.bias shape torch.Size([256])
decoder.stages.1.convs.0.all_modules.0.weight shape torch.Size([256, 512, 3, 3, 3])
decoder.stages.1.convs.0.all_modules.0.bias shape torch.Size([256])
decoder.stages.1.convs.0.all_modules.1.weight shape torch.Size([256])
decoder.stages.1.convs.0.all_modules.1.bias shape torch.Size([256])
decoder.stages.1.convs.1.conv.weight shape torch.Size([256, 256, 3, 3, 3])
decoder.stages.1.convs.1.conv.bias shape torch.Size([256])
decoder.stages.1.convs.1.norm.weight shape torch.Size([256])
decoder.stages.1.convs.1.norm.bias shape torch.Size([256])
decoder.stages.1.convs.1.all_modules.0.weight shape torch.Size([256, 256, 3, 3, 3])
decoder.stages.1.convs.1.all_modules.0.bias shape torch.Size([256])
decoder.stages.1.convs.1.all_modules.1.weight shape torch.Size([256])
decoder.stages.1.convs.1.all_modules.1.bias shape torch.Size([256])
decoder.stages.2.convs.0.conv.weight shape torch.Size([128, 256, 3, 3, 3])
decoder.stages.2.convs.0.conv.bias shape torch.Size([128])
decoder.stages.2.convs.0.norm.weight shape torch.Size([128])
decoder.stages.2.convs.0.norm.bias shape torch.Size([128])
decoder.stages.2.convs.0.all_modules.0.weight shape torch.Size([128, 256, 3, 3, 3])
decoder.stages.2.convs.0.all_modules.0.bias shape torch.Size([128])
decoder.stages.2.convs.0.all_modules.1.weight shape torch.Size([128])
decoder.stages.2.convs.0.all_modules.1.bias shape torch.Size([128])
decoder.stages.2.convs.1.conv.weight shape torch.Size([128, 128, 3, 3, 3])
decoder.stages.2.convs.1.conv.bias shape torch.Size([128])
decoder.stages.2.convs.1.norm.weight shape torch.Size([128])
decoder.stages.2.convs.1.norm.bias shape torch.Size([128])
decoder.stages.2.convs.1.all_modules.0.weight shape torch.Size([128, 128, 3, 3, 3])
decoder.stages.2.convs.1.all_modules.0.bias shape torch.Size([128])
decoder.stages.2.convs.1.all_modules.1.weight shape torch.Size([128])
decoder.stages.2.convs.1.all_modules.1.bias shape torch.Size([128])
decoder.stages.3.convs.0.conv.weight shape torch.Size([64, 128, 3, 3, 3])
decoder.stages.3.convs.0.conv.bias shape torch.Size([64])
decoder.stages.3.convs.0.norm.weight shape torch.Size([64])
decoder.stages.3.convs.0.norm.bias shape torch.Size([64])
decoder.stages.3.convs.0.all_modules.0.weight shape torch.Size([64, 128, 3, 3, 3])
decoder.stages.3.convs.0.all_modules.0.bias shape torch.Size([64])
decoder.stages.3.convs.0.all_modules.1.weight shape torch.Size([64])
decoder.stages.3.convs.0.all_modules.1.bias shape torch.Size([64])
decoder.stages.3.convs.1.conv.weight shape torch.Size([64, 64, 3, 3, 3])
decoder.stages.3.convs.1.conv.bias shape torch.Size([64])
decoder.stages.3.convs.1.norm.weight shape torch.Size([64])
decoder.stages.3.convs.1.norm.bias shape torch.Size([64])
decoder.stages.3.convs.1.all_modules.0.weight shape torch.Size([64, 64, 3, 3, 3])
decoder.stages.3.convs.1.all_modules.0.bias shape torch.Size([64])
decoder.stages.3.convs.1.all_modules.1.weight shape torch.Size([64])
decoder.stages.3.convs.1.all_modules.1.bias shape torch.Size([64])
decoder.stages.4.convs.0.conv.weight shape torch.Size([32, 64, 3, 3, 3])
decoder.stages.4.convs.0.conv.bias shape torch.Size([32])
decoder.stages.4.convs.0.norm.weight shape torch.Size([32])
decoder.stages.4.convs.0.norm.bias shape torch.Size([32])
decoder.stages.4.convs.0.all_modules.0.weight shape torch.Size([32, 64, 3, 3, 3])
decoder.stages.4.convs.0.all_modules.0.bias shape torch.Size([32])
decoder.stages.4.convs.0.all_modules.1.weight shape torch.Size([32])
decoder.stages.4.convs.0.all_modules.1.bias shape torch.Size([32])
decoder.stages.4.convs.1.conv.weight shape torch.Size([32, 32, 3, 3, 3])
decoder.stages.4.convs.1.conv.bias shape torch.Size([32])
decoder.stages.4.convs.1.norm.weight shape torch.Size([32])
decoder.stages.4.convs.1.norm.bias shape torch.Size([32])
decoder.stages.4.convs.1.all_modules.0.weight shape torch.Size([32, 32, 3, 3, 3])
decoder.stages.4.convs.1.all_modules.0.bias shape torch.Size([32])
decoder.stages.4.convs.1.all_modules.1.weight shape torch.Size([32])
decoder.stages.4.convs.1.all_modules.1.bias shape torch.Size([32])
decoder.transpconvs.0.weight shape torch.Size([320, 320, 1, 2, 2])
decoder.transpconvs.0.bias shape torch.Size([320])
decoder.transpconvs.1.weight shape torch.Size([320, 256, 2, 2, 2])
decoder.transpconvs.1.bias shape torch.Size([256])
decoder.transpconvs.2.weight shape torch.Size([256, 128, 2, 2, 2])
decoder.transpconvs.2.bias shape torch.Size([128])
decoder.transpconvs.3.weight shape torch.Size([128, 64, 2, 2, 2])
decoder.transpconvs.3.bias shape torch.Size([64])
decoder.transpconvs.4.weight shape torch.Size([64, 32, 2, 2, 2])
decoder.transpconvs.4.bias shape torch.Size([32])
################### Done ###################
2024-11-29 23:23:52.210942: do_dummy_2d_data_aug: True
2024-11-29 23:23:52.212986: Using splits from existing split file: /home/gridsan/nchutisilp/datasets/nnUNet_Datasets/nnUNet_preprocessed/Dataset306_Sohee_Ajay_Calcium_OCT/splits_final.json
2024-11-29 23:23:52.214934: The split file contains 5 splits.
2024-11-29 23:23:52.215815: Desired fold for training: 3
2024-11-29 23:23:52.216659: This split has 11 training and 2 validation cases.
using pin_memory on device 0
using pin_memory on device 0

This is the configuration used by this training:
Configuration name: 3d_32x160x128_b10
 {'data_identifier': 'nnUNetPlans_3d_fullres', 'preprocessor_name': 'DefaultPreprocessor', 'batch_size': 10, 'patch_size': [32, 160, 128], 'median_image_size_in_voxels': [375.0, 498.0, 498.0], 'spacing': [1.0, 1.0, 1.0], 'normalization_schemes': ['ZScoreNormalization'], 'use_mask_for_norm': [False], 'resampling_fn_data': 'resample_data_or_seg_to_shape', 'resampling_fn_seg': 'resample_data_or_seg_to_shape', 'resampling_fn_data_kwargs': {'is_seg': False, 'order': 3, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_seg_kwargs': {'is_seg': True, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_probabilities': 'resample_data_or_seg_to_shape', 'resampling_fn_probabilities_kwargs': {'is_seg': False, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'architecture': {'network_class_name': 'dynamic_network_architectures.architectures.unet.PlainConvUNet', 'arch_kwargs': {'n_stages': 6, 'features_per_stage': [32, 64, 128, 256, 320, 320], 'conv_op': 'torch.nn.modules.conv.Conv3d', 'kernel_sizes': [[3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3]], 'strides': [[1, 1, 1], [2, 2, 2], [2, 2, 2], [2, 2, 2], [2, 2, 2], [1, 2, 2]], 'n_conv_per_stage': [2, 2, 2, 2, 2, 2], 'n_conv_per_stage_decoder': [2, 2, 2, 2, 2], 'conv_bias': True, 'norm_op': 'torch.nn.modules.instancenorm.InstanceNorm3d', 'norm_op_kwargs': {'eps': 1e-05, 'affine': True}, 'dropout_op': None, 'dropout_op_kwargs': None, 'nonlin': 'torch.nn.LeakyReLU', 'nonlin_kwargs': {'inplace': True}, 'deep_supervision': True}, '_kw_requires_import': ['conv_op', 'norm_op', 'dropout_op', 'nonlin']}, 'batch_dice': True, 'inherits_from': '3d_fullres'} 

These are the global plan.json settings:
 {'dataset_name': 'Dataset306_Sohee_Ajay_Calcium_OCT', 'plans_name': 'nnUNetPlans', 'original_median_spacing_after_transp': [1.0, 1.0, 1.0], 'original_median_shape_after_transp': [375, 498, 498], 'image_reader_writer': 'SimpleITKIO', 'transpose_forward': [0, 1, 2], 'transpose_backward': [0, 1, 2], 'experiment_planner_used': 'ExperimentPlanner', 'label_manager': 'LabelManager', 'foreground_intensity_properties_per_channel': {'0': {'max': 0.49803921580314636, 'mean': 0.14296366274356842, 'median': 0.10980392247438431, 'min': 0.0, 'percentile_00_5': 0.01558823511004448, 'percentile_99_5': 0.49803921580314636, 'std': 0.11562220752239227}}} 

2024-11-29 23:23:54.380211: unpacking dataset...
using pin_memory on device 0
using pin_memory on device 0

This is the configuration used by this training:
Configuration name: 3d_32x160x128_b10
 {'data_identifier': 'nnUNetPlans_3d_fullres', 'preprocessor_name': 'DefaultPreprocessor', 'batch_size': 10, 'patch_size': [32, 160, 128], 'median_image_size_in_voxels': [375.0, 498.0, 498.0], 'spacing': [1.0, 1.0, 1.0], 'normalization_schemes': ['ZScoreNormalization'], 'use_mask_for_norm': [False], 'resampling_fn_data': 'resample_data_or_seg_to_shape', 'resampling_fn_seg': 'resample_data_or_seg_to_shape', 'resampling_fn_data_kwargs': {'is_seg': False, 'order': 3, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_seg_kwargs': {'is_seg': True, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_probabilities': 'resample_data_or_seg_to_shape', 'resampling_fn_probabilities_kwargs': {'is_seg': False, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'architecture': {'network_class_name': 'dynamic_network_architectures.architectures.unet.PlainConvUNet', 'arch_kwargs': {'n_stages': 6, 'features_per_stage': [32, 64, 128, 256, 320, 320], 'conv_op': 'torch.nn.modules.conv.Conv3d', 'kernel_sizes': [[3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3]], 'strides': [[1, 1, 1], [2, 2, 2], [2, 2, 2], [2, 2, 2], [2, 2, 2], [1, 2, 2]], 'n_conv_per_stage': [2, 2, 2, 2, 2, 2], 'n_conv_per_stage_decoder': [2, 2, 2, 2, 2], 'conv_bias': True, 'norm_op': 'torch.nn.modules.instancenorm.InstanceNorm3d', 'norm_op_kwargs': {'eps': 1e-05, 'affine': True}, 'dropout_op': None, 'dropout_op_kwargs': None, 'nonlin': 'torch.nn.LeakyReLU', 'nonlin_kwargs': {'inplace': True}, 'deep_supervision': True}, '_kw_requires_import': ['conv_op', 'norm_op', 'dropout_op', 'nonlin']}, 'batch_dice': True, 'inherits_from': '3d_fullres'} 

These are the global plan.json settings:
 {'dataset_name': 'Dataset306_Sohee_Ajay_Calcium_OCT', 'plans_name': 'nnUNetPlans', 'original_median_spacing_after_transp': [1.0, 1.0, 1.0], 'original_median_shape_after_transp': [375, 498, 498], 'image_reader_writer': 'SimpleITKIO', 'transpose_forward': [0, 1, 2], 'transpose_backward': [0, 1, 2], 'experiment_planner_used': 'ExperimentPlanner', 'label_manager': 'LabelManager', 'foreground_intensity_properties_per_channel': {'0': {'max': 0.49803921580314636, 'mean': 0.14296366274356842, 'median': 0.10980392247438431, 'min': 0.0, 'percentile_00_5': 0.01558823511004448, 'percentile_99_5': 0.49803921580314636, 'std': 0.11562220752239227}}} 

2024-11-29 23:23:57.440456: unpacking dataset...
2024-11-29 23:24:01.480767: unpacking done...
2024-11-29 23:24:01.489198: Unable to plot network architecture: nnUNet_compile is enabled!
2024-11-29 23:24:01.541628: 
2024-11-29 23:24:01.543373: Epoch 0
2024-11-29 23:24:01.544383: Current learning rate: 0.01
2024-11-29 23:26:52.484938: Validation loss improved from 1000.00000 to -0.20995! Patience: 0/50
2024-11-29 23:26:52.485788: train_loss -0.0647
2024-11-29 23:26:52.486691: val_loss -0.2099
2024-11-29 23:26:52.487495: Pseudo dice [0.5068]
2024-11-29 23:26:52.488329: Epoch time: 170.95 s
2024-11-29 23:26:52.489062: Yayy! New best EMA pseudo Dice: 0.5068
2024-11-29 23:26:54.762491: 
2024-11-29 23:26:54.764010: Epoch 1
2024-11-29 23:26:54.764829: Current learning rate: 0.00999
2024-11-29 23:28:20.539640: Validation loss improved from -0.20995 to -0.29876! Patience: 0/50
2024-11-29 23:28:20.540536: train_loss -0.2038
2024-11-29 23:28:20.541638: val_loss -0.2988
2024-11-29 23:28:20.542523: Pseudo dice [0.563]
2024-11-29 23:28:20.543714: Epoch time: 85.78 s
2024-11-29 23:28:20.544787: Yayy! New best EMA pseudo Dice: 0.5124
2024-11-29 23:28:22.190250: 
2024-11-29 23:28:22.191800: Epoch 2
2024-11-29 23:28:22.193095: Current learning rate: 0.00998
2024-11-29 23:29:48.094548: Validation loss improved from -0.29876 to -0.33847! Patience: 0/50
2024-11-29 23:29:48.095493: train_loss -0.2393
2024-11-29 23:29:48.096323: val_loss -0.3385
2024-11-29 23:29:48.097007: Pseudo dice [0.5859]
2024-11-29 23:29:48.097674: Epoch time: 85.91 s
2024-11-29 23:29:48.098345: Yayy! New best EMA pseudo Dice: 0.5198
2024-11-29 23:29:49.783500: 
2024-11-29 23:29:49.785027: Epoch 3
2024-11-29 23:29:49.785790: Current learning rate: 0.00997
2024-11-29 23:31:15.753336: Validation loss did not improve from -0.33847. Patience: 1/50
2024-11-29 23:31:15.754304: train_loss -0.2873
2024-11-29 23:31:15.755213: val_loss -0.2957
2024-11-29 23:31:15.756001: Pseudo dice [0.5516]
2024-11-29 23:31:15.756794: Epoch time: 85.97 s
2024-11-29 23:31:15.757486: Yayy! New best EMA pseudo Dice: 0.5229
2024-11-29 23:31:17.423316: 
2024-11-29 23:31:17.425256: Epoch 4
2024-11-29 23:31:17.426466: Current learning rate: 0.00996
2024-11-29 23:32:43.226923: Validation loss improved from -0.33847 to -0.40773! Patience: 1/50
2024-11-29 23:32:43.228019: train_loss -0.2799
2024-11-29 23:32:43.229300: val_loss -0.4077
2024-11-29 23:32:43.230499: Pseudo dice [0.6434]
2024-11-29 23:32:43.231571: Epoch time: 85.81 s
2024-11-29 23:32:43.575081: Yayy! New best EMA pseudo Dice: 0.535
2024-11-29 23:32:45.194236: 
2024-11-29 23:32:45.195941: Epoch 5
2024-11-29 23:32:45.197266: Current learning rate: 0.00995
2024-11-29 23:34:11.058079: Validation loss improved from -0.40773 to -0.42770! Patience: 0/50
2024-11-29 23:34:11.059185: train_loss -0.3194
2024-11-29 23:34:11.060056: val_loss -0.4277
2024-11-29 23:34:11.060801: Pseudo dice [0.6441]
2024-11-29 23:34:11.061370: Epoch time: 85.87 s
2024-11-29 23:34:11.061944: Yayy! New best EMA pseudo Dice: 0.5459
2024-11-29 23:34:12.675253: 
2024-11-29 23:34:12.676684: Epoch 6
2024-11-29 23:34:12.677846: Current learning rate: 0.00995
2024-11-29 23:35:38.642696: Validation loss improved from -0.42770 to -0.47893! Patience: 0/50
2024-11-29 23:35:38.643963: train_loss -0.3555
2024-11-29 23:35:38.644908: val_loss -0.4789
2024-11-29 23:35:38.645710: Pseudo dice [0.6843]
2024-11-29 23:35:38.646456: Epoch time: 85.97 s
2024-11-29 23:35:38.647102: Yayy! New best EMA pseudo Dice: 0.5597
2024-11-29 23:35:40.259258: 
2024-11-29 23:35:40.261006: Epoch 7
2024-11-29 23:35:40.262181: Current learning rate: 0.00994
2024-11-29 23:37:06.124791: Validation loss improved from -0.47893 to -0.49611! Patience: 0/50
2024-11-29 23:37:06.125982: train_loss -0.3522
2024-11-29 23:37:06.126752: val_loss -0.4961
2024-11-29 23:37:06.127360: Pseudo dice [0.6998]
2024-11-29 23:37:06.128030: Epoch time: 85.87 s
2024-11-29 23:37:06.128918: Yayy! New best EMA pseudo Dice: 0.5737
2024-11-29 23:37:08.193830: 
2024-11-29 23:37:08.196422: Epoch 8
2024-11-29 23:37:08.197638: Current learning rate: 0.00993
2024-11-29 23:38:34.037326: Validation loss improved from -0.49611 to -0.51777! Patience: 0/50
2024-11-29 23:38:34.038081: train_loss -0.3783
2024-11-29 23:38:34.039169: val_loss -0.5178
2024-11-29 23:38:34.040166: Pseudo dice [0.7105]
2024-11-29 23:38:34.041200: Epoch time: 85.85 s
2024-11-29 23:38:34.042135: Yayy! New best EMA pseudo Dice: 0.5874
2024-11-29 23:38:35.761178: 
2024-11-29 23:38:35.763231: Epoch 9
2024-11-29 23:38:35.764268: Current learning rate: 0.00992
2024-11-29 23:40:01.707635: Validation loss did not improve from -0.51777. Patience: 1/50
2024-11-29 23:40:01.709009: train_loss -0.3861
2024-11-29 23:40:01.709993: val_loss -0.4428
2024-11-29 23:40:01.710779: Pseudo dice [0.6337]
2024-11-29 23:40:01.711365: Epoch time: 85.95 s
2024-11-29 23:40:02.079291: Yayy! New best EMA pseudo Dice: 0.592
2024-11-29 23:40:03.719311: 
2024-11-29 23:40:03.720677: Epoch 10
2024-11-29 23:40:03.722024: Current learning rate: 0.00991
2024-11-29 23:41:29.664021: Validation loss did not improve from -0.51777. Patience: 2/50
2024-11-29 23:41:29.664729: train_loss -0.4051
2024-11-29 23:41:29.665530: val_loss -0.493
2024-11-29 23:41:29.666180: Pseudo dice [0.6959]
2024-11-29 23:41:29.666803: Epoch time: 85.95 s
2024-11-29 23:41:29.667490: Yayy! New best EMA pseudo Dice: 0.6024
2024-11-29 23:41:31.303687: 
2024-11-29 23:41:31.305534: Epoch 11
2024-11-29 23:41:31.306300: Current learning rate: 0.0099
2024-11-29 23:42:57.144944: Validation loss did not improve from -0.51777. Patience: 3/50
2024-11-29 23:42:57.146305: train_loss -0.4211
2024-11-29 23:42:57.147522: val_loss -0.4994
2024-11-29 23:42:57.148336: Pseudo dice [0.6799]
2024-11-29 23:42:57.149018: Epoch time: 85.84 s
2024-11-29 23:42:57.149735: Yayy! New best EMA pseudo Dice: 0.6102
2024-11-29 23:42:58.800339: 
2024-11-29 23:42:58.801974: Epoch 12
2024-11-29 23:42:58.802912: Current learning rate: 0.00989
2024-11-29 23:44:24.700768: Validation loss improved from -0.51777 to -0.52686! Patience: 3/50
2024-11-29 23:44:24.701648: train_loss -0.4275
2024-11-29 23:44:24.702615: val_loss -0.5269
2024-11-29 23:44:24.703351: Pseudo dice [0.7105]
2024-11-29 23:44:24.703960: Epoch time: 85.9 s
2024-11-29 23:44:24.704552: Yayy! New best EMA pseudo Dice: 0.6202
2024-11-29 23:44:26.338616: 
2024-11-29 23:44:26.340014: Epoch 13
2024-11-29 23:44:26.341011: Current learning rate: 0.00988
2024-11-29 23:45:52.320465: Validation loss did not improve from -0.52686. Patience: 1/50
2024-11-29 23:45:52.321941: train_loss -0.4386
2024-11-29 23:45:52.323343: val_loss -0.4128
2024-11-29 23:45:52.324325: Pseudo dice [0.6017]
2024-11-29 23:45:52.325171: Epoch time: 85.98 s
2024-11-29 23:45:53.591076: 
2024-11-29 23:45:53.592502: Epoch 14
2024-11-29 23:45:53.593250: Current learning rate: 0.00987
2024-11-29 23:47:19.440401: Validation loss did not improve from -0.52686. Patience: 2/50
2024-11-29 23:47:19.441291: train_loss -0.4341
2024-11-29 23:47:19.442382: val_loss -0.5096
2024-11-29 23:47:19.443203: Pseudo dice [0.7012]
2024-11-29 23:47:19.444202: Epoch time: 85.85 s
2024-11-29 23:47:19.808130: Yayy! New best EMA pseudo Dice: 0.6266
2024-11-29 23:47:21.515546: 
2024-11-29 23:47:21.517304: Epoch 15
2024-11-29 23:47:21.518342: Current learning rate: 0.00986
2024-11-29 23:48:47.387480: Validation loss improved from -0.52686 to -0.53322! Patience: 2/50
2024-11-29 23:48:47.388713: train_loss -0.4308
2024-11-29 23:48:47.389618: val_loss -0.5332
2024-11-29 23:48:47.390416: Pseudo dice [0.7112]
2024-11-29 23:48:47.391169: Epoch time: 85.87 s
2024-11-29 23:48:47.392069: Yayy! New best EMA pseudo Dice: 0.6351
2024-11-29 23:48:49.058162: 
2024-11-29 23:48:49.059809: Epoch 16
2024-11-29 23:48:49.060912: Current learning rate: 0.00986
2024-11-29 23:50:15.023526: Validation loss did not improve from -0.53322. Patience: 1/50
2024-11-29 23:50:15.024569: train_loss -0.4618
2024-11-29 23:50:15.025349: val_loss -0.479
2024-11-29 23:50:15.026081: Pseudo dice [0.6458]
2024-11-29 23:50:15.027102: Epoch time: 85.97 s
2024-11-29 23:50:15.027992: Yayy! New best EMA pseudo Dice: 0.6362
2024-11-29 23:50:16.769171: 
2024-11-29 23:50:16.770855: Epoch 17
2024-11-29 23:50:16.771805: Current learning rate: 0.00985
2024-11-29 23:51:42.652927: Validation loss improved from -0.53322 to -0.57015! Patience: 1/50
2024-11-29 23:51:42.653960: train_loss -0.4611
2024-11-29 23:51:42.655211: val_loss -0.5701
2024-11-29 23:51:42.656084: Pseudo dice [0.7452]
2024-11-29 23:51:42.656875: Epoch time: 85.89 s
2024-11-29 23:51:42.657673: Yayy! New best EMA pseudo Dice: 0.6471
2024-11-29 23:51:44.357513: 
2024-11-29 23:51:44.358912: Epoch 18
2024-11-29 23:51:44.359659: Current learning rate: 0.00984
2024-11-29 23:53:10.205777: Validation loss did not improve from -0.57015. Patience: 1/50
2024-11-29 23:53:10.206783: train_loss -0.4727
2024-11-29 23:53:10.207576: val_loss -0.546
2024-11-29 23:53:10.208174: Pseudo dice [0.7212]
2024-11-29 23:53:10.208852: Epoch time: 85.85 s
2024-11-29 23:53:10.209416: Yayy! New best EMA pseudo Dice: 0.6545
2024-11-29 23:53:12.253410: 
2024-11-29 23:53:12.255072: Epoch 19
2024-11-29 23:53:12.256100: Current learning rate: 0.00983
2024-11-29 23:54:38.154086: Validation loss did not improve from -0.57015. Patience: 2/50
2024-11-29 23:54:38.154844: train_loss -0.4608
2024-11-29 23:54:38.155775: val_loss -0.5262
2024-11-29 23:54:38.156339: Pseudo dice [0.7044]
2024-11-29 23:54:38.157006: Epoch time: 85.9 s
2024-11-29 23:54:38.529244: Yayy! New best EMA pseudo Dice: 0.6595
2024-11-29 23:54:40.208869: 
2024-11-29 23:54:40.210103: Epoch 20
2024-11-29 23:54:40.210902: Current learning rate: 0.00982
2024-11-29 23:56:06.148282: Validation loss did not improve from -0.57015. Patience: 3/50
2024-11-29 23:56:06.149219: train_loss -0.4732
2024-11-29 23:56:06.150276: val_loss -0.4917
2024-11-29 23:56:06.150976: Pseudo dice [0.6676]
2024-11-29 23:56:06.151661: Epoch time: 85.94 s
2024-11-29 23:56:06.152456: Yayy! New best EMA pseudo Dice: 0.6603
2024-11-29 23:56:07.957724: 
2024-11-29 23:56:07.959134: Epoch 21
2024-11-29 23:56:07.960216: Current learning rate: 0.00981
2024-11-29 23:57:33.806564: Validation loss did not improve from -0.57015. Patience: 4/50
2024-11-29 23:57:33.807380: train_loss -0.4767
2024-11-29 23:57:33.808615: val_loss -0.5591
2024-11-29 23:57:33.809362: Pseudo dice [0.7192]
2024-11-29 23:57:33.809997: Epoch time: 85.85 s
2024-11-29 23:57:33.810570: Yayy! New best EMA pseudo Dice: 0.6662
2024-11-29 23:57:35.455104: 
2024-11-29 23:57:35.457121: Epoch 22
2024-11-29 23:57:35.458181: Current learning rate: 0.0098
2024-11-29 23:59:01.366652: Validation loss did not improve from -0.57015. Patience: 5/50
2024-11-29 23:59:01.368023: train_loss -0.4873
2024-11-29 23:59:01.368949: val_loss -0.5579
2024-11-29 23:59:01.369604: Pseudo dice [0.7312]
2024-11-29 23:59:01.370358: Epoch time: 85.91 s
2024-11-29 23:59:01.371138: Yayy! New best EMA pseudo Dice: 0.6727
2024-11-29 23:59:03.024169: 
2024-11-29 23:59:03.025474: Epoch 23
2024-11-29 23:59:03.026109: Current learning rate: 0.00979
2024-11-30 00:00:29.031127: Validation loss did not improve from -0.57015. Patience: 6/50
2024-11-30 00:00:29.032130: train_loss -0.4804
2024-11-30 00:00:29.032943: val_loss -0.4031
2024-11-30 00:00:29.033682: Pseudo dice [0.5911]
2024-11-30 00:00:29.034423: Epoch time: 86.01 s
2024-11-30 00:00:30.276839: 
2024-11-30 00:00:30.278550: Epoch 24
2024-11-30 00:00:30.279794: Current learning rate: 0.00978
2024-11-30 00:01:56.177164: Validation loss improved from -0.57015 to -0.59031! Patience: 6/50
2024-11-30 00:01:56.178118: train_loss -0.4812
2024-11-30 00:01:56.179064: val_loss -0.5903
2024-11-30 00:01:56.179845: Pseudo dice [0.7534]
2024-11-30 00:01:56.180606: Epoch time: 85.9 s
2024-11-30 00:01:56.600024: Yayy! New best EMA pseudo Dice: 0.6734
2024-11-30 00:01:58.381572: 
2024-11-30 00:01:58.383066: Epoch 25
2024-11-30 00:01:58.384005: Current learning rate: 0.00977
2024-11-30 00:03:24.266290: Validation loss improved from -0.59031 to -0.59451! Patience: 0/50
2024-11-30 00:03:24.267098: train_loss -0.4958
2024-11-30 00:03:24.267776: val_loss -0.5945
2024-11-30 00:03:24.268392: Pseudo dice [0.7562]
2024-11-30 00:03:24.269116: Epoch time: 85.89 s
2024-11-30 00:03:24.269758: Yayy! New best EMA pseudo Dice: 0.6817
2024-11-30 00:03:25.920811: 
2024-11-30 00:03:25.922022: Epoch 26
2024-11-30 00:03:25.922724: Current learning rate: 0.00977
2024-11-30 00:04:51.919947: Validation loss did not improve from -0.59451. Patience: 1/50
2024-11-30 00:04:51.920635: train_loss -0.5084
2024-11-30 00:04:51.921293: val_loss -0.5519
2024-11-30 00:04:51.922023: Pseudo dice [0.7281]
2024-11-30 00:04:51.922750: Epoch time: 86.0 s
2024-11-30 00:04:51.923349: Yayy! New best EMA pseudo Dice: 0.6863
2024-11-30 00:04:53.568370: 
2024-11-30 00:04:53.570050: Epoch 27
2024-11-30 00:04:53.570935: Current learning rate: 0.00976
2024-11-30 00:06:19.394459: Validation loss did not improve from -0.59451. Patience: 2/50
2024-11-30 00:06:19.395296: train_loss -0.504
2024-11-30 00:06:19.396229: val_loss -0.5699
2024-11-30 00:06:19.396946: Pseudo dice [0.7364]
2024-11-30 00:06:19.397825: Epoch time: 85.83 s
2024-11-30 00:06:19.398622: Yayy! New best EMA pseudo Dice: 0.6913
2024-11-30 00:06:21.064616: 
2024-11-30 00:06:21.066098: Epoch 28
2024-11-30 00:06:21.066830: Current learning rate: 0.00975
2024-11-30 00:07:46.928069: Validation loss did not improve from -0.59451. Patience: 3/50
2024-11-30 00:07:46.929297: train_loss -0.5098
2024-11-30 00:07:46.930300: val_loss -0.5822
2024-11-30 00:07:46.931089: Pseudo dice [0.7508]
2024-11-30 00:07:46.931716: Epoch time: 85.87 s
2024-11-30 00:07:46.932397: Yayy! New best EMA pseudo Dice: 0.6973
2024-11-30 00:07:49.072093: 
2024-11-30 00:07:49.073599: Epoch 29
2024-11-30 00:07:49.074270: Current learning rate: 0.00974
2024-11-30 00:09:14.991027: Validation loss did not improve from -0.59451. Patience: 4/50
2024-11-30 00:09:14.991923: train_loss -0.5176
2024-11-30 00:09:14.992679: val_loss -0.5343
2024-11-30 00:09:14.993506: Pseudo dice [0.7057]
2024-11-30 00:09:14.994328: Epoch time: 85.92 s
2024-11-30 00:09:15.354719: Yayy! New best EMA pseudo Dice: 0.6981
2024-11-30 00:09:16.986992: 
2024-11-30 00:09:16.988359: Epoch 30
2024-11-30 00:09:16.989076: Current learning rate: 0.00973
2024-11-30 00:10:42.899281: Validation loss improved from -0.59451 to -0.60888! Patience: 4/50
2024-11-30 00:10:42.900928: train_loss -0.5169
2024-11-30 00:10:42.902117: val_loss -0.6089
2024-11-30 00:10:42.903172: Pseudo dice [0.7701]
2024-11-30 00:10:42.904012: Epoch time: 85.91 s
2024-11-30 00:10:42.904955: Yayy! New best EMA pseudo Dice: 0.7053
2024-11-30 00:10:44.563769: 
2024-11-30 00:10:44.564791: Epoch 31
2024-11-30 00:10:44.565475: Current learning rate: 0.00972
2024-11-30 00:12:10.411614: Validation loss did not improve from -0.60888. Patience: 1/50
2024-11-30 00:12:10.412760: train_loss -0.5283
2024-11-30 00:12:10.413607: val_loss -0.5768
2024-11-30 00:12:10.414344: Pseudo dice [0.7442]
2024-11-30 00:12:10.414972: Epoch time: 85.85 s
2024-11-30 00:12:10.415604: Yayy! New best EMA pseudo Dice: 0.7092
2024-11-30 00:12:12.157566: 
2024-11-30 00:12:12.158842: Epoch 32
2024-11-30 00:12:12.159778: Current learning rate: 0.00971
2024-11-30 00:13:38.034735: Validation loss did not improve from -0.60888. Patience: 2/50
2024-11-30 00:13:38.035486: train_loss -0.5272
2024-11-30 00:13:38.036452: val_loss -0.52
2024-11-30 00:13:38.037355: Pseudo dice [0.6906]
2024-11-30 00:13:38.038134: Epoch time: 85.88 s
2024-11-30 00:13:39.371827: 
2024-11-30 00:13:39.373597: Epoch 33
2024-11-30 00:13:39.375000: Current learning rate: 0.0097
2024-11-30 00:15:05.366453: Validation loss did not improve from -0.60888. Patience: 3/50
2024-11-30 00:15:05.367347: train_loss -0.5136
2024-11-30 00:15:05.368213: val_loss -0.4719
2024-11-30 00:15:05.368961: Pseudo dice [0.6768]
2024-11-30 00:15:05.369829: Epoch time: 86.0 s
2024-11-30 00:15:06.632215: 
2024-11-30 00:15:06.633770: Epoch 34
2024-11-30 00:15:06.634840: Current learning rate: 0.00969
2024-11-30 00:16:32.513438: Validation loss did not improve from -0.60888. Patience: 4/50
2024-11-30 00:16:32.514503: train_loss -0.5419
2024-11-30 00:16:32.515477: val_loss -0.5953
2024-11-30 00:16:32.516141: Pseudo dice [0.7526]
2024-11-30 00:16:32.516774: Epoch time: 85.88 s
2024-11-30 00:16:34.189406: 
2024-11-30 00:16:34.191451: Epoch 35
2024-11-30 00:16:34.193004: Current learning rate: 0.00968
2024-11-30 00:18:00.066018: Validation loss did not improve from -0.60888. Patience: 5/50
2024-11-30 00:18:00.067342: train_loss -0.5327
2024-11-30 00:18:00.068513: val_loss -0.581
2024-11-30 00:18:00.069321: Pseudo dice [0.7446]
2024-11-30 00:18:00.070302: Epoch time: 85.88 s
2024-11-30 00:18:00.071156: Yayy! New best EMA pseudo Dice: 0.7127
2024-11-30 00:18:01.730696: 
2024-11-30 00:18:01.731924: Epoch 36
2024-11-30 00:18:01.732995: Current learning rate: 0.00968
2024-11-30 00:19:27.713346: Validation loss did not improve from -0.60888. Patience: 6/50
2024-11-30 00:19:27.714735: train_loss -0.5356
2024-11-30 00:19:27.715714: val_loss -0.5775
2024-11-30 00:19:27.716440: Pseudo dice [0.7435]
2024-11-30 00:19:27.717153: Epoch time: 85.98 s
2024-11-30 00:19:27.717836: Yayy! New best EMA pseudo Dice: 0.7158
2024-11-30 00:19:29.389535: 
2024-11-30 00:19:29.390932: Epoch 37
2024-11-30 00:19:29.391910: Current learning rate: 0.00967
2024-11-30 00:20:55.244276: Validation loss did not improve from -0.60888. Patience: 7/50
2024-11-30 00:20:55.245329: train_loss -0.5345
2024-11-30 00:20:55.246365: val_loss -0.4999
2024-11-30 00:20:55.247116: Pseudo dice [0.672]
2024-11-30 00:20:55.247997: Epoch time: 85.86 s
2024-11-30 00:20:56.547154: 
2024-11-30 00:20:56.549302: Epoch 38
2024-11-30 00:20:56.550677: Current learning rate: 0.00966
2024-11-30 00:22:22.428104: Validation loss did not improve from -0.60888. Patience: 8/50
2024-11-30 00:22:22.429443: train_loss -0.5269
2024-11-30 00:22:22.430640: val_loss -0.5989
2024-11-30 00:22:22.431596: Pseudo dice [0.7636]
2024-11-30 00:22:22.432427: Epoch time: 85.88 s
2024-11-30 00:22:22.433263: Yayy! New best EMA pseudo Dice: 0.7166
2024-11-30 00:22:24.134335: 
2024-11-30 00:22:24.135825: Epoch 39
2024-11-30 00:22:24.136726: Current learning rate: 0.00965
2024-11-30 00:23:50.101047: Validation loss improved from -0.60888 to -0.65037! Patience: 8/50
2024-11-30 00:23:50.102187: train_loss -0.552
2024-11-30 00:23:50.103044: val_loss -0.6504
2024-11-30 00:23:50.103804: Pseudo dice [0.7993]
2024-11-30 00:23:50.104549: Epoch time: 85.97 s
2024-11-30 00:23:50.478814: Yayy! New best EMA pseudo Dice: 0.7249
2024-11-30 00:23:52.609441: 
2024-11-30 00:23:52.611209: Epoch 40
2024-11-30 00:23:52.612096: Current learning rate: 0.00964
2024-11-30 00:25:18.527281: Validation loss did not improve from -0.65037. Patience: 1/50
2024-11-30 00:25:18.528534: train_loss -0.5351
2024-11-30 00:25:18.529438: val_loss -0.5538
2024-11-30 00:25:18.530252: Pseudo dice [0.7224]
2024-11-30 00:25:18.531091: Epoch time: 85.92 s
2024-11-30 00:25:19.952602: 
2024-11-30 00:25:19.954660: Epoch 41
2024-11-30 00:25:19.955573: Current learning rate: 0.00963
2024-11-30 00:26:45.845189: Validation loss did not improve from -0.65037. Patience: 2/50
2024-11-30 00:26:45.846358: train_loss -0.5453
2024-11-30 00:26:45.847238: val_loss -0.5582
2024-11-30 00:26:45.847924: Pseudo dice [0.714]
2024-11-30 00:26:45.848533: Epoch time: 85.9 s
2024-11-30 00:26:47.115895: 
2024-11-30 00:26:47.117608: Epoch 42
2024-11-30 00:26:47.118523: Current learning rate: 0.00962
2024-11-30 00:28:13.013775: Validation loss did not improve from -0.65037. Patience: 3/50
2024-11-30 00:28:13.014647: train_loss -0.5574
2024-11-30 00:28:13.015582: val_loss -0.5661
2024-11-30 00:28:13.016525: Pseudo dice [0.7252]
2024-11-30 00:28:13.017252: Epoch time: 85.9 s
2024-11-30 00:28:14.259198: 
2024-11-30 00:28:14.260992: Epoch 43
2024-11-30 00:28:14.261704: Current learning rate: 0.00961
2024-11-30 00:29:40.338912: Validation loss did not improve from -0.65037. Patience: 4/50
2024-11-30 00:29:40.366481: train_loss -0.5561
2024-11-30 00:29:40.369530: val_loss -0.6045
2024-11-30 00:29:40.370680: Pseudo dice [0.7552]
2024-11-30 00:29:40.371952: Epoch time: 86.11 s
2024-11-30 00:29:40.372705: Yayy! New best EMA pseudo Dice: 0.7269
2024-11-30 00:29:42.058785: 
2024-11-30 00:29:42.060544: Epoch 44
2024-11-30 00:29:42.061338: Current learning rate: 0.0096
2024-11-30 00:31:07.930503: Validation loss did not improve from -0.65037. Patience: 5/50
2024-11-30 00:31:07.932534: train_loss -0.5602
2024-11-30 00:31:07.933865: val_loss -0.5896
2024-11-30 00:31:07.934542: Pseudo dice [0.749]
2024-11-30 00:31:07.935259: Epoch time: 85.87 s
2024-11-30 00:31:08.396840: Yayy! New best EMA pseudo Dice: 0.7291
2024-11-30 00:31:10.034220: 
2024-11-30 00:31:10.036272: Epoch 45
2024-11-30 00:31:10.037201: Current learning rate: 0.00959
2024-11-30 00:32:35.943777: Validation loss did not improve from -0.65037. Patience: 6/50
2024-11-30 00:32:35.944383: train_loss -0.5578
2024-11-30 00:32:35.945053: val_loss -0.5479
2024-11-30 00:32:35.945816: Pseudo dice [0.7118]
2024-11-30 00:32:35.946614: Epoch time: 85.91 s
2024-11-30 00:32:37.217538: 
2024-11-30 00:32:37.218928: Epoch 46
2024-11-30 00:32:37.219719: Current learning rate: 0.00959
2024-11-30 00:34:03.206457: Validation loss did not improve from -0.65037. Patience: 7/50
2024-11-30 00:34:03.207553: train_loss -0.5504
2024-11-30 00:34:03.209124: val_loss -0.6412
2024-11-30 00:34:03.210047: Pseudo dice [0.7843]
2024-11-30 00:34:03.210742: Epoch time: 85.99 s
2024-11-30 00:34:03.211513: Yayy! New best EMA pseudo Dice: 0.733
2024-11-30 00:34:04.813589: 
2024-11-30 00:34:04.815423: Epoch 47
2024-11-30 00:34:04.816467: Current learning rate: 0.00958
2024-11-30 00:35:30.760669: Validation loss did not improve from -0.65037. Patience: 8/50
2024-11-30 00:35:30.761664: train_loss -0.5587
2024-11-30 00:35:30.762487: val_loss -0.6275
2024-11-30 00:35:30.763173: Pseudo dice [0.7666]
2024-11-30 00:35:30.763912: Epoch time: 85.95 s
2024-11-30 00:35:30.764501: Yayy! New best EMA pseudo Dice: 0.7364
2024-11-30 00:35:32.436042: 
2024-11-30 00:35:32.437770: Epoch 48
2024-11-30 00:35:32.438750: Current learning rate: 0.00957
2024-11-30 00:36:58.301896: Validation loss did not improve from -0.65037. Patience: 9/50
2024-11-30 00:36:58.303424: train_loss -0.5552
2024-11-30 00:36:58.304309: val_loss -0.5749
2024-11-30 00:36:58.305026: Pseudo dice [0.7458]
2024-11-30 00:36:58.305748: Epoch time: 85.87 s
2024-11-30 00:36:58.306401: Yayy! New best EMA pseudo Dice: 0.7373
2024-11-30 00:36:59.956563: 
2024-11-30 00:36:59.958451: Epoch 49
2024-11-30 00:36:59.959234: Current learning rate: 0.00956
2024-11-30 00:38:25.910030: Validation loss did not improve from -0.65037. Patience: 10/50
2024-11-30 00:38:25.911075: train_loss -0.5587
2024-11-30 00:38:25.911851: val_loss -0.5967
2024-11-30 00:38:25.912507: Pseudo dice [0.7521]
2024-11-30 00:38:25.913187: Epoch time: 85.96 s
2024-11-30 00:38:26.333748: Yayy! New best EMA pseudo Dice: 0.7388
2024-11-30 00:38:27.972116: 
2024-11-30 00:38:27.973983: Epoch 50
2024-11-30 00:38:27.974850: Current learning rate: 0.00955
2024-11-30 00:39:53.878740: Validation loss did not improve from -0.65037. Patience: 11/50
2024-11-30 00:39:53.879928: train_loss -0.5685
2024-11-30 00:39:53.880653: val_loss -0.5107
2024-11-30 00:39:53.881265: Pseudo dice [0.6985]
2024-11-30 00:39:53.881995: Epoch time: 85.91 s
2024-11-30 00:39:55.592881: 
2024-11-30 00:39:55.594639: Epoch 51
2024-11-30 00:39:55.595366: Current learning rate: 0.00954
2024-11-30 00:41:21.431452: Validation loss did not improve from -0.65037. Patience: 12/50
2024-11-30 00:41:21.432897: train_loss -0.557
2024-11-30 00:41:21.433935: val_loss -0.6019
2024-11-30 00:41:21.434818: Pseudo dice [0.7541]
2024-11-30 00:41:21.435749: Epoch time: 85.84 s
2024-11-30 00:41:22.744365: 
2024-11-30 00:41:22.746249: Epoch 52
2024-11-30 00:41:22.747050: Current learning rate: 0.00953
2024-11-30 00:42:48.643929: Validation loss did not improve from -0.65037. Patience: 13/50
2024-11-30 00:42:48.644947: train_loss -0.5626
2024-11-30 00:42:48.645922: val_loss -0.6194
2024-11-30 00:42:48.646706: Pseudo dice [0.7659]
2024-11-30 00:42:48.647540: Epoch time: 85.9 s
2024-11-30 00:42:48.648292: Yayy! New best EMA pseudo Dice: 0.7396
2024-11-30 00:42:50.356882: 
2024-11-30 00:42:50.359090: Epoch 53
2024-11-30 00:42:50.359789: Current learning rate: 0.00952
2024-11-30 00:44:16.361289: Validation loss did not improve from -0.65037. Patience: 14/50
2024-11-30 00:44:16.362525: train_loss -0.5621
2024-11-30 00:44:16.363508: val_loss -0.5769
2024-11-30 00:44:16.364118: Pseudo dice [0.7434]
2024-11-30 00:44:16.364756: Epoch time: 86.01 s
2024-11-30 00:44:16.365494: Yayy! New best EMA pseudo Dice: 0.74
2024-11-30 00:44:18.027492: 
2024-11-30 00:44:18.029472: Epoch 54
2024-11-30 00:44:18.030199: Current learning rate: 0.00951
2024-11-30 00:45:43.875073: Validation loss did not improve from -0.65037. Patience: 15/50
2024-11-30 00:45:43.876292: train_loss -0.5675
2024-11-30 00:45:43.877154: val_loss -0.5225
2024-11-30 00:45:43.877942: Pseudo dice [0.7068]
2024-11-30 00:45:43.878727: Epoch time: 85.85 s
2024-11-30 00:45:45.574173: 
2024-11-30 00:45:45.576282: Epoch 55
2024-11-30 00:45:45.577041: Current learning rate: 0.0095
2024-11-30 00:47:11.475809: Validation loss did not improve from -0.65037. Patience: 16/50
2024-11-30 00:47:11.477120: train_loss -0.5814
2024-11-30 00:47:11.478002: val_loss -0.5577
2024-11-30 00:47:11.478722: Pseudo dice [0.724]
2024-11-30 00:47:11.479355: Epoch time: 85.9 s
2024-11-30 00:47:12.786016: 
2024-11-30 00:47:12.787735: Epoch 56
2024-11-30 00:47:12.788714: Current learning rate: 0.00949
2024-11-30 00:48:38.904231: Validation loss did not improve from -0.65037. Patience: 17/50
2024-11-30 00:48:38.905019: train_loss -0.5845
2024-11-30 00:48:38.905747: val_loss -0.5562
2024-11-30 00:48:38.906483: Pseudo dice [0.7238]
2024-11-30 00:48:38.907156: Epoch time: 86.12 s
2024-11-30 00:48:40.241657: 
2024-11-30 00:48:40.243342: Epoch 57
2024-11-30 00:48:40.244249: Current learning rate: 0.00949
2024-11-30 00:50:06.084474: Validation loss did not improve from -0.65037. Patience: 18/50
2024-11-30 00:50:06.085587: train_loss -0.5732
2024-11-30 00:50:06.086678: val_loss -0.6443
2024-11-30 00:50:06.087420: Pseudo dice [0.789]
2024-11-30 00:50:06.088145: Epoch time: 85.85 s
2024-11-30 00:50:07.363550: 
2024-11-30 00:50:07.365288: Epoch 58
2024-11-30 00:50:07.366158: Current learning rate: 0.00948
2024-11-30 00:51:33.197813: Validation loss did not improve from -0.65037. Patience: 19/50
2024-11-30 00:51:33.198719: train_loss -0.5907
2024-11-30 00:51:33.199708: val_loss -0.6218
2024-11-30 00:51:33.200438: Pseudo dice [0.7755]
2024-11-30 00:51:33.201118: Epoch time: 85.84 s
2024-11-30 00:51:33.201878: Yayy! New best EMA pseudo Dice: 0.7433
2024-11-30 00:51:34.939668: 
2024-11-30 00:51:34.941343: Epoch 59
2024-11-30 00:51:34.942249: Current learning rate: 0.00947
2024-11-30 00:53:00.938970: Validation loss did not improve from -0.65037. Patience: 20/50
2024-11-30 00:53:00.940330: train_loss -0.5831
2024-11-30 00:53:00.941336: val_loss -0.5399
2024-11-30 00:53:00.942207: Pseudo dice [0.7089]
2024-11-30 00:53:00.942978: Epoch time: 86.0 s
2024-11-30 00:53:02.633480: 
2024-11-30 00:53:02.635389: Epoch 60
2024-11-30 00:53:02.636332: Current learning rate: 0.00946
2024-11-30 00:54:28.613246: Validation loss did not improve from -0.65037. Patience: 21/50
2024-11-30 00:54:28.614536: train_loss -0.5826
2024-11-30 00:54:28.615428: val_loss -0.5317
2024-11-30 00:54:28.616096: Pseudo dice [0.7013]
2024-11-30 00:54:28.616809: Epoch time: 85.98 s
2024-11-30 00:54:30.312457: 
2024-11-30 00:54:30.313431: Epoch 61
2024-11-30 00:54:30.314183: Current learning rate: 0.00945
2024-11-30 00:55:56.307411: Validation loss improved from -0.65037 to -0.65883! Patience: 21/50
2024-11-30 00:55:56.308145: train_loss -0.5808
2024-11-30 00:55:56.308957: val_loss -0.6588
2024-11-30 00:55:56.309744: Pseudo dice [0.7957]
2024-11-30 00:55:56.310487: Epoch time: 86.0 s
2024-11-30 00:55:57.611938: 
2024-11-30 00:55:57.613461: Epoch 62
2024-11-30 00:55:57.614235: Current learning rate: 0.00944
2024-11-30 00:57:23.500401: Validation loss did not improve from -0.65883. Patience: 1/50
2024-11-30 00:57:23.501329: train_loss -0.5815
2024-11-30 00:57:23.502593: val_loss -0.6412
2024-11-30 00:57:23.503581: Pseudo dice [0.7846]
2024-11-30 00:57:23.504528: Epoch time: 85.89 s
2024-11-30 00:57:23.505529: Yayy! New best EMA pseudo Dice: 0.7462
2024-11-30 00:57:25.185854: 
2024-11-30 00:57:25.187612: Epoch 63
2024-11-30 00:57:25.188632: Current learning rate: 0.00943
2024-11-30 00:58:51.404213: Validation loss did not improve from -0.65883. Patience: 2/50
2024-11-30 00:58:51.404941: train_loss -0.583
2024-11-30 00:58:51.405794: val_loss -0.5915
2024-11-30 00:58:51.406415: Pseudo dice [0.7424]
2024-11-30 00:58:51.407116: Epoch time: 86.22 s
2024-11-30 00:58:52.687865: 
2024-11-30 00:58:52.689420: Epoch 64
2024-11-30 00:58:52.690141: Current learning rate: 0.00942
2024-11-30 01:00:18.528647: Validation loss did not improve from -0.65883. Patience: 3/50
2024-11-30 01:00:18.529888: train_loss -0.589
2024-11-30 01:00:18.530711: val_loss -0.6306
2024-11-30 01:00:18.531494: Pseudo dice [0.7801]
2024-11-30 01:00:18.532169: Epoch time: 85.84 s
2024-11-30 01:00:18.934207: Yayy! New best EMA pseudo Dice: 0.7493
2024-11-30 01:00:20.590622: 
2024-11-30 01:00:20.592024: Epoch 65
2024-11-30 01:00:20.592843: Current learning rate: 0.00941
2024-11-30 01:01:46.471658: Validation loss did not improve from -0.65883. Patience: 4/50
2024-11-30 01:01:46.473075: train_loss -0.5954
2024-11-30 01:01:46.474420: val_loss -0.6307
2024-11-30 01:01:46.475178: Pseudo dice [0.7767]
2024-11-30 01:01:46.475907: Epoch time: 85.88 s
2024-11-30 01:01:46.476616: Yayy! New best EMA pseudo Dice: 0.752
2024-11-30 01:01:48.211028: 
2024-11-30 01:01:48.212897: Epoch 66
2024-11-30 01:01:48.213741: Current learning rate: 0.0094
2024-11-30 01:03:14.191634: Validation loss did not improve from -0.65883. Patience: 5/50
2024-11-30 01:03:14.193815: train_loss -0.6011
2024-11-30 01:03:14.195306: val_loss -0.6114
2024-11-30 01:03:14.196055: Pseudo dice [0.7599]
2024-11-30 01:03:14.196790: Epoch time: 85.98 s
2024-11-30 01:03:14.197599: Yayy! New best EMA pseudo Dice: 0.7528
2024-11-30 01:03:15.892449: 
2024-11-30 01:03:15.893945: Epoch 67
2024-11-30 01:03:15.894657: Current learning rate: 0.00939
2024-11-30 01:04:41.752336: Validation loss did not improve from -0.65883. Patience: 6/50
2024-11-30 01:04:41.753425: train_loss -0.5815
2024-11-30 01:04:41.754460: val_loss -0.5518
2024-11-30 01:04:41.755429: Pseudo dice [0.7268]
2024-11-30 01:04:41.756382: Epoch time: 85.86 s
2024-11-30 01:04:43.098802: 
2024-11-30 01:04:43.100736: Epoch 68
2024-11-30 01:04:43.101538: Current learning rate: 0.00939
2024-11-30 01:06:08.970658: Validation loss did not improve from -0.65883. Patience: 7/50
2024-11-30 01:06:08.971673: train_loss -0.5977
2024-11-30 01:06:08.972563: val_loss -0.5962
2024-11-30 01:06:08.973208: Pseudo dice [0.7512]
2024-11-30 01:06:08.974041: Epoch time: 85.87 s
2024-11-30 01:06:10.380517: 
2024-11-30 01:06:10.381898: Epoch 69
2024-11-30 01:06:10.382799: Current learning rate: 0.00938
2024-11-30 01:07:36.273924: Validation loss did not improve from -0.65883. Patience: 8/50
2024-11-30 01:07:36.275059: train_loss -0.5966
2024-11-30 01:07:36.275901: val_loss -0.6066
2024-11-30 01:07:36.276574: Pseudo dice [0.7486]
2024-11-30 01:07:36.277408: Epoch time: 85.9 s
2024-11-30 01:07:37.999996: 
2024-11-30 01:07:38.001380: Epoch 70
2024-11-30 01:07:38.002124: Current learning rate: 0.00937
2024-11-30 01:09:03.944979: Validation loss did not improve from -0.65883. Patience: 9/50
2024-11-30 01:09:03.946247: train_loss -0.5855
2024-11-30 01:09:03.947288: val_loss -0.5674
2024-11-30 01:09:03.948026: Pseudo dice [0.7214]
2024-11-30 01:09:03.948862: Epoch time: 85.95 s
2024-11-30 01:09:05.277267: 
2024-11-30 01:09:05.278994: Epoch 71
2024-11-30 01:09:05.279849: Current learning rate: 0.00936
2024-11-30 01:10:31.131284: Validation loss did not improve from -0.65883. Patience: 10/50
2024-11-30 01:10:31.132859: train_loss -0.6035
2024-11-30 01:10:31.134132: val_loss -0.6534
2024-11-30 01:10:31.134816: Pseudo dice [0.7949]
2024-11-30 01:10:31.135434: Epoch time: 85.86 s
2024-11-30 01:10:32.820031: 
2024-11-30 01:10:32.821692: Epoch 72
2024-11-30 01:10:32.822423: Current learning rate: 0.00935
2024-11-30 01:11:58.845257: Validation loss did not improve from -0.65883. Patience: 11/50
2024-11-30 01:11:58.846476: train_loss -0.5994
2024-11-30 01:11:58.847393: val_loss -0.6019
2024-11-30 01:11:58.848368: Pseudo dice [0.7611]
2024-11-30 01:11:58.849092: Epoch time: 86.03 s
2024-11-30 01:11:58.849804: Yayy! New best EMA pseudo Dice: 0.7529
2024-11-30 01:12:00.510066: 
2024-11-30 01:12:00.511690: Epoch 73
2024-11-30 01:12:00.512474: Current learning rate: 0.00934
2024-11-30 01:13:26.529052: Validation loss did not improve from -0.65883. Patience: 12/50
2024-11-30 01:13:26.531326: train_loss -0.6088
2024-11-30 01:13:26.532537: val_loss -0.6248
2024-11-30 01:13:26.533324: Pseudo dice [0.7701]
2024-11-30 01:13:26.534235: Epoch time: 86.02 s
2024-11-30 01:13:26.534942: Yayy! New best EMA pseudo Dice: 0.7547
2024-11-30 01:13:28.230301: 
2024-11-30 01:13:28.232165: Epoch 74
2024-11-30 01:13:28.233165: Current learning rate: 0.00933
2024-11-30 01:14:54.149436: Validation loss did not improve from -0.65883. Patience: 13/50
2024-11-30 01:14:54.150461: train_loss -0.6136
2024-11-30 01:14:54.151267: val_loss -0.6583
2024-11-30 01:14:54.152024: Pseudo dice [0.7931]
2024-11-30 01:14:54.152759: Epoch time: 85.92 s
2024-11-30 01:14:54.532198: Yayy! New best EMA pseudo Dice: 0.7585
2024-11-30 01:14:56.202899: 
2024-11-30 01:14:56.205033: Epoch 75
2024-11-30 01:14:56.205874: Current learning rate: 0.00932
2024-11-30 01:16:22.107511: Validation loss did not improve from -0.65883. Patience: 14/50
2024-11-30 01:16:22.108774: train_loss -0.6054
2024-11-30 01:16:22.109786: val_loss -0.5995
2024-11-30 01:16:22.110811: Pseudo dice [0.7576]
2024-11-30 01:16:22.111707: Epoch time: 85.91 s
2024-11-30 01:16:23.410484: 
2024-11-30 01:16:23.412107: Epoch 76
2024-11-30 01:16:23.413122: Current learning rate: 0.00931
2024-11-30 01:17:49.393336: Validation loss did not improve from -0.65883. Patience: 15/50
2024-11-30 01:17:49.394530: train_loss -0.6121
2024-11-30 01:17:49.395566: val_loss -0.5783
2024-11-30 01:17:49.396370: Pseudo dice [0.7402]
2024-11-30 01:17:49.397056: Epoch time: 85.99 s
2024-11-30 01:17:50.706597: 
2024-11-30 01:17:50.708233: Epoch 77
2024-11-30 01:17:50.709006: Current learning rate: 0.0093
2024-11-30 01:19:16.604755: Validation loss did not improve from -0.65883. Patience: 16/50
2024-11-30 01:19:16.605999: train_loss -0.6198
2024-11-30 01:19:16.607005: val_loss -0.6488
2024-11-30 01:19:16.607726: Pseudo dice [0.7858]
2024-11-30 01:19:16.608401: Epoch time: 85.9 s
2024-11-30 01:19:16.609140: Yayy! New best EMA pseudo Dice: 0.7595
2024-11-30 01:19:18.334608: 
2024-11-30 01:19:18.336562: Epoch 78
2024-11-30 01:19:18.337402: Current learning rate: 0.0093
2024-11-30 01:20:44.214305: Validation loss did not improve from -0.65883. Patience: 17/50
2024-11-30 01:20:44.215246: train_loss -0.6187
2024-11-30 01:20:44.216025: val_loss -0.6181
2024-11-30 01:20:44.216677: Pseudo dice [0.7811]
2024-11-30 01:20:44.217357: Epoch time: 85.88 s
2024-11-30 01:20:44.218104: Yayy! New best EMA pseudo Dice: 0.7617
2024-11-30 01:20:45.866015: 
2024-11-30 01:20:45.867695: Epoch 79
2024-11-30 01:20:45.868532: Current learning rate: 0.00929
2024-11-30 01:22:11.803252: Validation loss did not improve from -0.65883. Patience: 18/50
2024-11-30 01:22:11.804547: train_loss -0.6112
2024-11-30 01:22:11.805678: val_loss -0.6018
2024-11-30 01:22:11.806452: Pseudo dice [0.7575]
2024-11-30 01:22:11.807164: Epoch time: 85.94 s
2024-11-30 01:22:13.458276: 
2024-11-30 01:22:13.459878: Epoch 80
2024-11-30 01:22:13.460793: Current learning rate: 0.00928
2024-11-30 01:23:39.464964: Validation loss did not improve from -0.65883. Patience: 19/50
2024-11-30 01:23:39.465820: train_loss -0.6109
2024-11-30 01:23:39.466648: val_loss -0.5883
2024-11-30 01:23:39.467431: Pseudo dice [0.7489]
2024-11-30 01:23:39.468204: Epoch time: 86.01 s
2024-11-30 01:23:40.766695: 
2024-11-30 01:23:40.768276: Epoch 81
2024-11-30 01:23:40.769221: Current learning rate: 0.00927
2024-11-30 01:25:06.657309: Validation loss did not improve from -0.65883. Patience: 20/50
2024-11-30 01:25:06.658323: train_loss -0.6281
2024-11-30 01:25:06.659326: val_loss -0.6178
2024-11-30 01:25:06.660040: Pseudo dice [0.7701]
2024-11-30 01:25:06.660817: Epoch time: 85.89 s
2024-11-30 01:25:08.303811: 
2024-11-30 01:25:08.305315: Epoch 82
2024-11-30 01:25:08.306284: Current learning rate: 0.00926
2024-11-30 01:26:34.227089: Validation loss improved from -0.65883 to -0.66033! Patience: 20/50
2024-11-30 01:26:34.228036: train_loss -0.6147
2024-11-30 01:26:34.229043: val_loss -0.6603
2024-11-30 01:26:34.229931: Pseudo dice [0.7918]
2024-11-30 01:26:34.230608: Epoch time: 85.93 s
2024-11-30 01:26:34.231269: Yayy! New best EMA pseudo Dice: 0.7641
2024-11-30 01:26:35.854158: 
2024-11-30 01:26:35.855582: Epoch 83
2024-11-30 01:26:35.856302: Current learning rate: 0.00925
2024-11-30 01:28:01.832193: Validation loss did not improve from -0.66033. Patience: 1/50
2024-11-30 01:28:01.833392: train_loss -0.6226
2024-11-30 01:28:01.834431: val_loss -0.6564
2024-11-30 01:28:01.835261: Pseudo dice [0.7938]
2024-11-30 01:28:01.836141: Epoch time: 85.98 s
2024-11-30 01:28:01.837012: Yayy! New best EMA pseudo Dice: 0.7671
2024-11-30 01:28:03.435237: 
2024-11-30 01:28:03.437268: Epoch 84
2024-11-30 01:28:03.438143: Current learning rate: 0.00924
2024-11-30 01:29:29.324755: Validation loss improved from -0.66033 to -0.66069! Patience: 1/50
2024-11-30 01:29:29.326714: train_loss -0.6128
2024-11-30 01:29:29.327877: val_loss -0.6607
2024-11-30 01:29:29.328698: Pseudo dice [0.7986]
2024-11-30 01:29:29.329587: Epoch time: 85.89 s
2024-11-30 01:29:29.711113: Yayy! New best EMA pseudo Dice: 0.7702
2024-11-30 01:29:31.383287: 
2024-11-30 01:29:31.384874: Epoch 85
2024-11-30 01:29:31.385938: Current learning rate: 0.00923
2024-11-30 01:30:57.833861: Validation loss did not improve from -0.66069. Patience: 1/50
2024-11-30 01:30:57.840327: train_loss -0.6117
2024-11-30 01:30:57.841633: val_loss -0.5157
2024-11-30 01:30:57.842306: Pseudo dice [0.6903]
2024-11-30 01:30:57.843305: Epoch time: 86.46 s
2024-11-30 01:30:59.217912: 
2024-11-30 01:30:59.219697: Epoch 86
2024-11-30 01:30:59.220398: Current learning rate: 0.00922
2024-11-30 01:32:25.258291: Validation loss did not improve from -0.66069. Patience: 2/50
2024-11-30 01:32:25.259332: train_loss -0.6108
2024-11-30 01:32:25.325008: val_loss -0.6549
2024-11-30 01:32:25.326150: Pseudo dice [0.7844]
2024-11-30 01:32:25.332699: Epoch time: 86.04 s
2024-11-30 01:32:26.662864: 
2024-11-30 01:32:26.664496: Epoch 87
2024-11-30 01:32:26.665244: Current learning rate: 0.00921
2024-11-30 01:33:52.569838: Validation loss did not improve from -0.66069. Patience: 3/50
2024-11-30 01:33:52.572146: train_loss -0.6267
2024-11-30 01:33:52.573704: val_loss -0.5944
2024-11-30 01:33:52.574630: Pseudo dice [0.7518]
2024-11-30 01:33:52.575443: Epoch time: 85.91 s
2024-11-30 01:33:53.900664: 
2024-11-30 01:33:53.902313: Epoch 88
2024-11-30 01:33:53.903304: Current learning rate: 0.0092
2024-11-30 01:35:19.789148: Validation loss did not improve from -0.66069. Patience: 4/50
2024-11-30 01:35:19.790379: train_loss -0.6243
2024-11-30 01:35:19.791678: val_loss -0.6043
2024-11-30 01:35:19.792700: Pseudo dice [0.7498]
2024-11-30 01:35:19.793543: Epoch time: 85.89 s
2024-11-30 01:35:21.049293: 
2024-11-30 01:35:21.050879: Epoch 89
2024-11-30 01:35:21.051630: Current learning rate: 0.0092
2024-11-30 01:36:46.929983: Validation loss did not improve from -0.66069. Patience: 5/50
2024-11-30 01:36:46.930877: train_loss -0.6271
2024-11-30 01:36:46.931744: val_loss -0.6225
2024-11-30 01:36:46.932443: Pseudo dice [0.7786]
2024-11-30 01:36:46.933119: Epoch time: 85.88 s
2024-11-30 01:36:48.575946: 
2024-11-30 01:36:48.577950: Epoch 90
2024-11-30 01:36:48.578769: Current learning rate: 0.00919
2024-11-30 01:38:14.580920: Validation loss did not improve from -0.66069. Patience: 6/50
2024-11-30 01:38:14.582269: train_loss -0.6246
2024-11-30 01:38:14.583196: val_loss -0.6483
2024-11-30 01:38:14.583997: Pseudo dice [0.7876]
2024-11-30 01:38:14.584797: Epoch time: 86.01 s
2024-11-30 01:38:15.838203: 
2024-11-30 01:38:15.840055: Epoch 91
2024-11-30 01:38:15.840915: Current learning rate: 0.00918
2024-11-30 01:39:41.728937: Validation loss improved from -0.66069 to -0.68531! Patience: 6/50
2024-11-30 01:39:41.730206: train_loss -0.6195
2024-11-30 01:39:41.731241: val_loss -0.6853
2024-11-30 01:39:41.732079: Pseudo dice [0.8112]
2024-11-30 01:39:41.732753: Epoch time: 85.89 s
2024-11-30 01:39:41.733383: Yayy! New best EMA pseudo Dice: 0.7705
2024-11-30 01:39:43.323898: 
2024-11-30 01:39:43.324970: Epoch 92
2024-11-30 01:39:43.325760: Current learning rate: 0.00917
2024-11-30 01:41:09.221962: Validation loss did not improve from -0.68531. Patience: 1/50
2024-11-30 01:41:09.223079: train_loss -0.6303
2024-11-30 01:41:09.224303: val_loss -0.6642
2024-11-30 01:41:09.225203: Pseudo dice [0.8032]
2024-11-30 01:41:09.226033: Epoch time: 85.9 s
2024-11-30 01:41:09.226919: Yayy! New best EMA pseudo Dice: 0.7737
2024-11-30 01:41:11.272481: 
2024-11-30 01:41:11.274335: Epoch 93
2024-11-30 01:41:11.275092: Current learning rate: 0.00916
2024-11-30 01:42:37.569502: Validation loss did not improve from -0.68531. Patience: 2/50
2024-11-30 01:42:37.572064: train_loss -0.6419
2024-11-30 01:42:37.573166: val_loss -0.657
2024-11-30 01:42:37.573799: Pseudo dice [0.8]
2024-11-30 01:42:37.574494: Epoch time: 86.3 s
2024-11-30 01:42:37.575230: Yayy! New best EMA pseudo Dice: 0.7764
2024-11-30 01:42:39.123363: 
2024-11-30 01:42:39.124905: Epoch 94
2024-11-30 01:42:39.125784: Current learning rate: 0.00915
2024-11-30 01:44:05.101800: Validation loss did not improve from -0.68531. Patience: 3/50
2024-11-30 01:44:05.103602: train_loss -0.6379
2024-11-30 01:44:05.104909: val_loss -0.5959
2024-11-30 01:44:05.105727: Pseudo dice [0.7616]
2024-11-30 01:44:05.106489: Epoch time: 85.98 s
2024-11-30 01:44:06.672765: 
2024-11-30 01:44:06.674643: Epoch 95
2024-11-30 01:44:06.675672: Current learning rate: 0.00914
2024-11-30 01:45:32.561439: Validation loss did not improve from -0.68531. Patience: 4/50
2024-11-30 01:45:32.562703: train_loss -0.6329
2024-11-30 01:45:32.563544: val_loss -0.6592
2024-11-30 01:45:32.564347: Pseudo dice [0.799]
2024-11-30 01:45:32.564942: Epoch time: 85.89 s
2024-11-30 01:45:32.565637: Yayy! New best EMA pseudo Dice: 0.7773
2024-11-30 01:45:34.070417: 
2024-11-30 01:45:34.071747: Epoch 96
2024-11-30 01:45:34.072528: Current learning rate: 0.00913
2024-11-30 01:47:00.010556: Validation loss did not improve from -0.68531. Patience: 5/50
2024-11-30 01:47:00.011302: train_loss -0.6393
2024-11-30 01:47:00.012137: val_loss -0.631
2024-11-30 01:47:00.012863: Pseudo dice [0.7736]
2024-11-30 01:47:00.013507: Epoch time: 85.94 s
2024-11-30 01:47:01.236966: 
2024-11-30 01:47:01.238656: Epoch 97
2024-11-30 01:47:01.239439: Current learning rate: 0.00912
2024-11-30 01:48:27.177077: Validation loss did not improve from -0.68531. Patience: 6/50
2024-11-30 01:48:27.178143: train_loss -0.6352
2024-11-30 01:48:27.178941: val_loss -0.6193
2024-11-30 01:48:27.179707: Pseudo dice [0.7675]
2024-11-30 01:48:27.180507: Epoch time: 85.94 s
2024-11-30 01:48:28.426406: 
2024-11-30 01:48:28.428121: Epoch 98
2024-11-30 01:48:28.428959: Current learning rate: 0.00911
2024-11-30 01:49:54.291409: Validation loss did not improve from -0.68531. Patience: 7/50
2024-11-30 01:49:54.292394: train_loss -0.6373
2024-11-30 01:49:54.293438: val_loss -0.6111
2024-11-30 01:49:54.294442: Pseudo dice [0.7608]
2024-11-30 01:49:54.295419: Epoch time: 85.87 s
2024-11-30 01:49:55.531833: 
2024-11-30 01:49:55.533541: Epoch 99
2024-11-30 01:49:55.534476: Current learning rate: 0.0091
2024-11-30 01:51:21.419559: Validation loss did not improve from -0.68531. Patience: 8/50
2024-11-30 01:51:21.420793: train_loss -0.6492
2024-11-30 01:51:21.421590: val_loss -0.6663
2024-11-30 01:51:21.422220: Pseudo dice [0.7929]
2024-11-30 01:51:21.422865: Epoch time: 85.89 s
2024-11-30 01:51:23.039954: 
2024-11-30 01:51:23.041452: Epoch 100
2024-11-30 01:51:23.042280: Current learning rate: 0.0091
2024-11-30 01:52:49.086720: Validation loss did not improve from -0.68531. Patience: 9/50
2024-11-30 01:52:49.088002: train_loss -0.6444
2024-11-30 01:52:49.088832: val_loss -0.5701
2024-11-30 01:52:49.089506: Pseudo dice [0.7406]
2024-11-30 01:52:49.090215: Epoch time: 86.05 s
2024-11-30 01:52:50.337207: 
2024-11-30 01:52:50.339098: Epoch 101
2024-11-30 01:52:50.339906: Current learning rate: 0.00909
2024-11-30 01:54:16.231001: Validation loss did not improve from -0.68531. Patience: 10/50
2024-11-30 01:54:16.232272: train_loss -0.6371
2024-11-30 01:54:16.233465: val_loss -0.6504
2024-11-30 01:54:16.234240: Pseudo dice [0.7848]
2024-11-30 01:54:16.235003: Epoch time: 85.9 s
2024-11-30 01:54:17.467535: 
2024-11-30 01:54:17.468887: Epoch 102
2024-11-30 01:54:17.469553: Current learning rate: 0.00908
2024-11-30 01:55:43.372722: Validation loss did not improve from -0.68531. Patience: 11/50
2024-11-30 01:55:43.373486: train_loss -0.6522
2024-11-30 01:55:43.374293: val_loss -0.5976
2024-11-30 01:55:43.375082: Pseudo dice [0.7444]
2024-11-30 01:55:43.375771: Epoch time: 85.91 s
2024-11-30 01:55:44.612197: 
2024-11-30 01:55:44.613629: Epoch 103
2024-11-30 01:55:44.614384: Current learning rate: 0.00907
2024-11-30 01:57:10.608263: Validation loss did not improve from -0.68531. Patience: 12/50
2024-11-30 01:57:10.609622: train_loss -0.6561
2024-11-30 01:57:10.610498: val_loss -0.5531
2024-11-30 01:57:10.611252: Pseudo dice [0.715]
2024-11-30 01:57:10.611915: Epoch time: 86.0 s
2024-11-30 01:57:12.631064: 
2024-11-30 01:57:12.632795: Epoch 104
2024-11-30 01:57:12.633659: Current learning rate: 0.00906
2024-11-30 01:58:38.499038: Validation loss did not improve from -0.68531. Patience: 13/50
2024-11-30 01:58:38.500429: train_loss -0.6518
2024-11-30 01:58:38.501617: val_loss -0.6128
2024-11-30 01:58:38.502408: Pseudo dice [0.7625]
2024-11-30 01:58:38.503189: Epoch time: 85.87 s
2024-11-30 01:58:40.117554: 
2024-11-30 01:58:40.119224: Epoch 105
2024-11-30 01:58:40.120208: Current learning rate: 0.00905
2024-11-30 02:00:05.999243: Validation loss did not improve from -0.68531. Patience: 14/50
2024-11-30 02:00:05.999981: train_loss -0.6545
2024-11-30 02:00:06.000765: val_loss -0.649
2024-11-30 02:00:06.001585: Pseudo dice [0.7881]
2024-11-30 02:00:06.002381: Epoch time: 85.88 s
2024-11-30 02:00:07.301936: 
2024-11-30 02:00:07.303397: Epoch 106
2024-11-30 02:00:07.304271: Current learning rate: 0.00904
2024-11-30 02:01:33.249550: Validation loss did not improve from -0.68531. Patience: 15/50
2024-11-30 02:01:33.250484: train_loss -0.6587
2024-11-30 02:01:33.251313: val_loss -0.6017
2024-11-30 02:01:33.252122: Pseudo dice [0.7523]
2024-11-30 02:01:33.252801: Epoch time: 85.95 s
2024-11-30 02:01:34.544292: 
2024-11-30 02:01:34.545611: Epoch 107
2024-11-30 02:01:34.546421: Current learning rate: 0.00903
2024-11-30 02:03:00.535544: Validation loss did not improve from -0.68531. Patience: 16/50
2024-11-30 02:03:00.536629: train_loss -0.6456
2024-11-30 02:03:00.537557: val_loss -0.5183
2024-11-30 02:03:00.538516: Pseudo dice [0.7001]
2024-11-30 02:03:00.539385: Epoch time: 85.99 s
2024-11-30 02:03:01.827784: 
2024-11-30 02:03:01.829405: Epoch 108
2024-11-30 02:03:01.830197: Current learning rate: 0.00902
2024-11-30 02:04:27.779068: Validation loss did not improve from -0.68531. Patience: 17/50
2024-11-30 02:04:27.780077: train_loss -0.6508
2024-11-30 02:04:27.780900: val_loss -0.6189
2024-11-30 02:04:27.781561: Pseudo dice [0.772]
2024-11-30 02:04:27.782281: Epoch time: 85.95 s
2024-11-30 02:04:29.006051: 
2024-11-30 02:04:29.007380: Epoch 109
2024-11-30 02:04:29.008108: Current learning rate: 0.00901
2024-11-30 02:05:54.936147: Validation loss did not improve from -0.68531. Patience: 18/50
2024-11-30 02:05:54.937053: train_loss -0.6497
2024-11-30 02:05:54.938126: val_loss -0.6319
2024-11-30 02:05:54.938864: Pseudo dice [0.7751]
2024-11-30 02:05:54.939708: Epoch time: 85.93 s
2024-11-30 02:05:56.615590: 
2024-11-30 02:05:56.616742: Epoch 110
2024-11-30 02:05:56.617605: Current learning rate: 0.009
2024-11-30 02:07:22.661674: Validation loss did not improve from -0.68531. Patience: 19/50
2024-11-30 02:07:22.662999: train_loss -0.6284
2024-11-30 02:07:22.663963: val_loss -0.5835
2024-11-30 02:07:22.664668: Pseudo dice [0.757]
2024-11-30 02:07:22.665502: Epoch time: 86.05 s
2024-11-30 02:07:23.921420: 
2024-11-30 02:07:23.923184: Epoch 111
2024-11-30 02:07:23.924100: Current learning rate: 0.009
2024-11-30 02:08:49.825853: Validation loss did not improve from -0.68531. Patience: 20/50
2024-11-30 02:08:49.827078: train_loss -0.6504
2024-11-30 02:08:49.827943: val_loss -0.6161
2024-11-30 02:08:49.828758: Pseudo dice [0.77]
2024-11-30 02:08:49.829543: Epoch time: 85.91 s
2024-11-30 02:08:51.084352: 
2024-11-30 02:08:51.086227: Epoch 112
2024-11-30 02:08:51.087107: Current learning rate: 0.00899
2024-11-30 02:10:16.953039: Validation loss did not improve from -0.68531. Patience: 21/50
2024-11-30 02:10:16.954179: train_loss -0.6536
2024-11-30 02:10:16.955011: val_loss -0.6006
2024-11-30 02:10:16.955755: Pseudo dice [0.7573]
2024-11-30 02:10:16.956584: Epoch time: 85.87 s
2024-11-30 02:10:18.192444: 
2024-11-30 02:10:18.193961: Epoch 113
2024-11-30 02:10:18.194686: Current learning rate: 0.00898
2024-11-30 02:11:44.160584: Validation loss did not improve from -0.68531. Patience: 22/50
2024-11-30 02:11:44.161355: train_loss -0.6612
2024-11-30 02:11:44.162225: val_loss -0.6323
2024-11-30 02:11:44.162910: Pseudo dice [0.7673]
2024-11-30 02:11:44.163678: Epoch time: 85.97 s
2024-11-30 02:11:45.399835: 
2024-11-30 02:11:45.401421: Epoch 114
2024-11-30 02:11:45.402245: Current learning rate: 0.00897
2024-11-30 02:13:11.293533: Validation loss did not improve from -0.68531. Patience: 23/50
2024-11-30 02:13:11.294559: train_loss -0.6531
2024-11-30 02:13:11.295604: val_loss -0.5509
2024-11-30 02:13:11.296698: Pseudo dice [0.716]
2024-11-30 02:13:11.297658: Epoch time: 85.9 s
2024-11-30 02:13:13.271118: 
2024-11-30 02:13:13.272749: Epoch 115
2024-11-30 02:13:13.273831: Current learning rate: 0.00896
2024-11-30 02:14:39.173763: Validation loss did not improve from -0.68531. Patience: 24/50
2024-11-30 02:14:39.175133: train_loss -0.6577
2024-11-30 02:14:39.176185: val_loss -0.6087
2024-11-30 02:14:39.177036: Pseudo dice [0.7571]
2024-11-30 02:14:39.177780: Epoch time: 85.91 s
2024-11-30 02:14:40.449343: 
2024-11-30 02:14:40.451155: Epoch 116
2024-11-30 02:14:40.451979: Current learning rate: 0.00895
2024-11-30 02:16:06.362103: Validation loss did not improve from -0.68531. Patience: 25/50
2024-11-30 02:16:06.363069: train_loss -0.658
2024-11-30 02:16:06.364082: val_loss -0.549
2024-11-30 02:16:06.364816: Pseudo dice [0.7095]
2024-11-30 02:16:06.365551: Epoch time: 85.91 s
2024-11-30 02:16:07.608340: 
2024-11-30 02:16:07.609775: Epoch 117
2024-11-30 02:16:07.610667: Current learning rate: 0.00894
2024-11-30 02:17:33.627634: Validation loss did not improve from -0.68531. Patience: 26/50
2024-11-30 02:17:33.628563: train_loss -0.6617
2024-11-30 02:17:33.629668: val_loss -0.6317
2024-11-30 02:17:33.630464: Pseudo dice [0.7702]
2024-11-30 02:17:33.631259: Epoch time: 86.02 s
2024-11-30 02:17:34.887143: 
2024-11-30 02:17:34.888773: Epoch 118
2024-11-30 02:17:34.889539: Current learning rate: 0.00893
2024-11-30 02:19:00.765348: Validation loss did not improve from -0.68531. Patience: 27/50
2024-11-30 02:19:00.766417: train_loss -0.6607
2024-11-30 02:19:00.767484: val_loss -0.609
2024-11-30 02:19:00.768279: Pseudo dice [0.7647]
2024-11-30 02:19:00.769081: Epoch time: 85.88 s
2024-11-30 02:19:02.031366: 
2024-11-30 02:19:02.033126: Epoch 119
2024-11-30 02:19:02.033943: Current learning rate: 0.00892
2024-11-30 02:20:27.929954: Validation loss did not improve from -0.68531. Patience: 28/50
2024-11-30 02:20:27.931023: train_loss -0.6584
2024-11-30 02:20:27.931915: val_loss -0.6251
2024-11-30 02:20:27.932715: Pseudo dice [0.7695]
2024-11-30 02:20:27.933370: Epoch time: 85.9 s
2024-11-30 02:20:29.566309: 
2024-11-30 02:20:29.568082: Epoch 120
2024-11-30 02:20:29.568775: Current learning rate: 0.00891
2024-11-30 02:21:55.587672: Validation loss did not improve from -0.68531. Patience: 29/50
2024-11-30 02:21:55.589033: train_loss -0.6518
2024-11-30 02:21:55.590130: val_loss -0.6593
2024-11-30 02:21:55.590966: Pseudo dice [0.7968]
2024-11-30 02:21:55.591694: Epoch time: 86.02 s
2024-11-30 02:21:56.857980: 
2024-11-30 02:21:56.859570: Epoch 121
2024-11-30 02:21:56.860273: Current learning rate: 0.0089
2024-11-30 02:23:22.707058: Validation loss did not improve from -0.68531. Patience: 30/50
2024-11-30 02:23:22.708333: train_loss -0.6654
2024-11-30 02:23:22.709472: val_loss -0.6723
2024-11-30 02:23:22.710218: Pseudo dice [0.8071]
2024-11-30 02:23:22.710886: Epoch time: 85.85 s
2024-11-30 02:23:23.949034: 
2024-11-30 02:23:23.950756: Epoch 122
2024-11-30 02:23:23.951516: Current learning rate: 0.00889
2024-11-30 02:24:49.868909: Validation loss did not improve from -0.68531. Patience: 31/50
2024-11-30 02:24:49.870106: train_loss -0.6628
2024-11-30 02:24:49.870921: val_loss -0.5695
2024-11-30 02:24:49.871657: Pseudo dice [0.7222]
2024-11-30 02:24:49.872417: Epoch time: 85.92 s
2024-11-30 02:24:51.180429: 
2024-11-30 02:24:51.181562: Epoch 123
2024-11-30 02:24:51.182383: Current learning rate: 0.00889
2024-11-30 02:26:17.143364: Validation loss did not improve from -0.68531. Patience: 32/50
2024-11-30 02:26:17.144407: train_loss -0.6634
2024-11-30 02:26:17.145168: val_loss -0.5132
2024-11-30 02:26:17.145934: Pseudo dice [0.6972]
2024-11-30 02:26:17.146613: Epoch time: 85.96 s
2024-11-30 02:26:18.383701: 
2024-11-30 02:26:18.385548: Epoch 124
2024-11-30 02:26:18.386400: Current learning rate: 0.00888
2024-11-30 02:27:44.394618: Validation loss did not improve from -0.68531. Patience: 33/50
2024-11-30 02:27:44.395887: train_loss -0.6583
2024-11-30 02:27:44.397137: val_loss -0.6495
2024-11-30 02:27:44.398081: Pseudo dice [0.7852]
2024-11-30 02:27:44.399020: Epoch time: 86.01 s
2024-11-30 02:27:46.014287: 
2024-11-30 02:27:46.015888: Epoch 125
2024-11-30 02:27:46.016890: Current learning rate: 0.00887
2024-11-30 02:29:11.889660: Validation loss did not improve from -0.68531. Patience: 34/50
2024-11-30 02:29:11.890510: train_loss -0.6484
2024-11-30 02:29:11.891277: val_loss -0.6138
2024-11-30 02:29:11.892015: Pseudo dice [0.7656]
2024-11-30 02:29:11.892824: Epoch time: 85.88 s
2024-11-30 02:29:13.466398: 
2024-11-30 02:29:13.468360: Epoch 126
2024-11-30 02:29:13.469383: Current learning rate: 0.00886
2024-11-30 02:30:39.405989: Validation loss did not improve from -0.68531. Patience: 35/50
2024-11-30 02:30:39.407025: train_loss -0.6607
2024-11-30 02:30:39.408049: val_loss -0.646
2024-11-30 02:30:39.408888: Pseudo dice [0.7811]
2024-11-30 02:30:39.409541: Epoch time: 85.94 s
2024-11-30 02:30:40.669263: 
2024-11-30 02:30:40.671232: Epoch 127
2024-11-30 02:30:40.672131: Current learning rate: 0.00885
2024-11-30 02:32:06.704632: Validation loss did not improve from -0.68531. Patience: 36/50
2024-11-30 02:32:06.705699: train_loss -0.6666
2024-11-30 02:32:06.706689: val_loss -0.5906
2024-11-30 02:32:06.707469: Pseudo dice [0.7435]
2024-11-30 02:32:06.708252: Epoch time: 86.04 s
2024-11-30 02:32:08.040233: 
2024-11-30 02:32:08.041776: Epoch 128
2024-11-30 02:32:08.042547: Current learning rate: 0.00884
2024-11-30 02:33:33.948554: Validation loss did not improve from -0.68531. Patience: 37/50
2024-11-30 02:33:33.949508: train_loss -0.6647
2024-11-30 02:33:33.950745: val_loss -0.6276
2024-11-30 02:33:33.951570: Pseudo dice [0.7697]
2024-11-30 02:33:33.952423: Epoch time: 85.91 s
2024-11-30 02:33:35.286248: 
2024-11-30 02:33:35.288234: Epoch 129
2024-11-30 02:33:35.289010: Current learning rate: 0.00883
2024-11-30 02:35:01.203366: Validation loss did not improve from -0.68531. Patience: 38/50
2024-11-30 02:35:01.204666: train_loss -0.6682
2024-11-30 02:35:01.205643: val_loss -0.6698
2024-11-30 02:35:01.206318: Pseudo dice [0.8054]
2024-11-30 02:35:01.207010: Epoch time: 85.92 s
2024-11-30 02:35:02.832056: 
2024-11-30 02:35:02.833663: Epoch 130
2024-11-30 02:35:02.834388: Current learning rate: 0.00882
2024-11-30 02:36:29.500832: Validation loss did not improve from -0.68531. Patience: 39/50
2024-11-30 02:36:29.504246: train_loss -0.6744
2024-11-30 02:36:29.505543: val_loss -0.6155
2024-11-30 02:36:29.506271: Pseudo dice [0.7752]
2024-11-30 02:36:29.507250: Epoch time: 86.67 s
2024-11-30 02:36:30.878445: 
2024-11-30 02:36:30.880215: Epoch 131
2024-11-30 02:36:30.881045: Current learning rate: 0.00881
2024-11-30 02:37:56.776255: Validation loss did not improve from -0.68531. Patience: 40/50
2024-11-30 02:37:56.777271: train_loss -0.6684
2024-11-30 02:37:56.779748: val_loss -0.6248
2024-11-30 02:37:56.780742: Pseudo dice [0.7696]
2024-11-30 02:37:56.782256: Epoch time: 85.9 s
2024-11-30 02:37:58.091295: 
2024-11-30 02:37:58.093284: Epoch 132
2024-11-30 02:37:58.094272: Current learning rate: 0.0088
2024-11-30 02:39:24.054265: Validation loss did not improve from -0.68531. Patience: 41/50
2024-11-30 02:39:24.055688: train_loss -0.6716
2024-11-30 02:39:24.056909: val_loss -0.6109
2024-11-30 02:39:24.057726: Pseudo dice [0.754]
2024-11-30 02:39:24.058508: Epoch time: 85.97 s
2024-11-30 02:39:25.358729: 
2024-11-30 02:39:25.360420: Epoch 133
2024-11-30 02:39:25.361162: Current learning rate: 0.00879
2024-11-30 02:40:51.311041: Validation loss did not improve from -0.68531. Patience: 42/50
2024-11-30 02:40:51.312269: train_loss -0.6699
2024-11-30 02:40:51.313241: val_loss -0.5746
2024-11-30 02:40:51.313952: Pseudo dice [0.7374]
2024-11-30 02:40:51.314652: Epoch time: 85.95 s
2024-11-30 02:40:52.571099: 
2024-11-30 02:40:52.572718: Epoch 134
2024-11-30 02:40:52.573546: Current learning rate: 0.00879
2024-11-30 02:42:18.525016: Validation loss did not improve from -0.68531. Patience: 43/50
2024-11-30 02:42:18.525787: train_loss -0.6597
2024-11-30 02:42:18.526953: val_loss -0.5977
2024-11-30 02:42:18.527579: Pseudo dice [0.7641]
2024-11-30 02:42:18.528250: Epoch time: 85.96 s
2024-11-30 02:42:20.258944: 
2024-11-30 02:42:20.260844: Epoch 135
2024-11-30 02:42:20.261683: Current learning rate: 0.00878
2024-11-30 02:43:46.163697: Validation loss did not improve from -0.68531. Patience: 44/50
2024-11-30 02:43:46.164492: train_loss -0.6647
2024-11-30 02:43:46.165375: val_loss -0.6285
2024-11-30 02:43:46.166211: Pseudo dice [0.7715]
2024-11-30 02:43:46.166894: Epoch time: 85.91 s
2024-11-30 02:43:47.449400: 
2024-11-30 02:43:47.451081: Epoch 136
2024-11-30 02:43:47.451766: Current learning rate: 0.00877
2024-11-30 02:45:13.370697: Validation loss did not improve from -0.68531. Patience: 45/50
2024-11-30 02:45:13.371798: train_loss -0.6776
2024-11-30 02:45:13.372679: val_loss -0.625
2024-11-30 02:45:13.373436: Pseudo dice [0.7765]
2024-11-30 02:45:13.374120: Epoch time: 85.92 s
2024-11-30 02:45:14.985013: 
2024-11-30 02:45:14.986816: Epoch 137
2024-11-30 02:45:14.987563: Current learning rate: 0.00876
2024-11-30 02:46:41.056104: Validation loss did not improve from -0.68531. Patience: 46/50
2024-11-30 02:46:41.057194: train_loss -0.68
2024-11-30 02:46:41.058075: val_loss -0.6661
2024-11-30 02:46:41.059010: Pseudo dice [0.8009]
2024-11-30 02:46:41.059763: Epoch time: 86.07 s
2024-11-30 02:46:42.396451: 
2024-11-30 02:46:42.397924: Epoch 138
2024-11-30 02:46:42.398736: Current learning rate: 0.00875
2024-11-30 02:48:08.286237: Validation loss did not improve from -0.68531. Patience: 47/50
2024-11-30 02:48:08.288672: train_loss -0.6686
2024-11-30 02:48:08.289640: val_loss -0.634
2024-11-30 02:48:08.290500: Pseudo dice [0.7686]
2024-11-30 02:48:08.291299: Epoch time: 85.89 s
2024-11-30 02:48:09.631048: 
2024-11-30 02:48:09.633043: Epoch 139
2024-11-30 02:48:09.633871: Current learning rate: 0.00874
2024-11-30 02:49:35.515217: Validation loss did not improve from -0.68531. Patience: 48/50
2024-11-30 02:49:35.517437: train_loss -0.6759
2024-11-30 02:49:35.518867: val_loss -0.6499
2024-11-30 02:49:35.519681: Pseudo dice [0.7846]
2024-11-30 02:49:35.520526: Epoch time: 85.89 s
2024-11-30 02:49:37.254254: 
2024-11-30 02:49:37.256087: Epoch 140
2024-11-30 02:49:37.257012: Current learning rate: 0.00873
2024-11-30 02:51:03.242312: Validation loss did not improve from -0.68531. Patience: 49/50
2024-11-30 02:51:03.243638: train_loss -0.6782
2024-11-30 02:51:03.245055: val_loss -0.6642
2024-11-30 02:51:03.245899: Pseudo dice [0.7998]
2024-11-30 02:51:03.246674: Epoch time: 85.99 s
2024-11-30 02:51:04.543684: 
2024-11-30 02:51:04.545682: Epoch 141
2024-11-30 02:51:04.546691: Current learning rate: 0.00872
2024-11-30 02:52:30.466799: Validation loss did not improve from -0.68531. Patience: 50/50
2024-11-30 02:52:30.467914: train_loss -0.6789
2024-11-30 02:52:30.469153: val_loss -0.6513
2024-11-30 02:52:30.470503: Pseudo dice [0.7908]
2024-11-30 02:52:30.471695: Epoch time: 85.93 s
2024-11-30 02:52:31.785228: Patience reached. Stopping training.
2024-11-30 02:52:32.318145: Training done.
2024-11-30 02:52:32.869925: Using splits from existing split file: /home/gridsan/nchutisilp/datasets/nnUNet_Datasets/nnUNet_preprocessed/Dataset306_Sohee_Ajay_Calcium_OCT/splits_final.json
2024-11-30 02:52:32.902250: The split file contains 5 splits.
2024-11-30 02:52:32.903314: Desired fold for training: 3
2024-11-30 02:52:32.904058: This split has 11 training and 2 validation cases.
2024-11-30 02:52:32.904913: predicting 03009Pre
2024-11-30 02:52:32.955367: 03009Pre, shape torch.Size([1, 400, 498, 498]), rank 0
2024-11-30 02:54:37.144922: predicting 101-044
2024-11-30 02:54:37.177710: 101-044, shape torch.Size([1, 404, 498, 498]), rank 0
2024-11-30 02:57:57.249278: Validation complete
2024-11-30 02:57:57.250142: Mean Validation Dice:  0.7836674175985173

############################
INFO: You are using the old nnU-Net default plans. We have updated our recommendations. Please consider using those instead! Read more here: https://github.com/MIC-DKFZ/nnUNet/blob/master/documentation/resenc_presets.md
############################

Using device: cuda:0

#######################################################################
Please cite the following paper when using nnU-Net:
Isensee, F., Jaeger, P. F., Kohl, S. A., Petersen, J., & Maier-Hein, K. H. (2021). nnU-Net: a self-configuring method for deep learning-based biomedical image segmentation. Nature methods, 18(2), 203-211.
#######################################################################

2024-11-30 02:58:17.954967: Using torch.compile...
/home/gridsan/nchutisilp/.local/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn("The verbose parameter is deprecated. Please use get_last_lr() "
################### Loading pretrained weights from file  /home/gridsan/nchutisilp/projects/OpenAI-CLIP/clip_pretrained_nnUNet.pt ###################
Below is the list of overlapping blocks in pretrained model and nnUNet architecture:
encoder.stages.0.0.convs.0.conv.weight shape torch.Size([32, 1, 3, 3, 3])
encoder.stages.0.0.convs.0.conv.bias shape torch.Size([32])
encoder.stages.0.0.convs.0.norm.weight shape torch.Size([32])
encoder.stages.0.0.convs.0.norm.bias shape torch.Size([32])
encoder.stages.0.0.convs.0.all_modules.0.weight shape torch.Size([32, 1, 3, 3, 3])
encoder.stages.0.0.convs.0.all_modules.0.bias shape torch.Size([32])
encoder.stages.0.0.convs.0.all_modules.1.weight shape torch.Size([32])
encoder.stages.0.0.convs.0.all_modules.1.bias shape torch.Size([32])
encoder.stages.0.0.convs.1.conv.weight shape torch.Size([32, 32, 3, 3, 3])
encoder.stages.0.0.convs.1.conv.bias shape torch.Size([32])
encoder.stages.0.0.convs.1.norm.weight shape torch.Size([32])
encoder.stages.0.0.convs.1.norm.bias shape torch.Size([32])
encoder.stages.0.0.convs.1.all_modules.0.weight shape torch.Size([32, 32, 3, 3, 3])
encoder.stages.0.0.convs.1.all_modules.0.bias shape torch.Size([32])
encoder.stages.0.0.convs.1.all_modules.1.weight shape torch.Size([32])
encoder.stages.0.0.convs.1.all_modules.1.bias shape torch.Size([32])
encoder.stages.1.0.convs.0.conv.weight shape torch.Size([64, 32, 3, 3, 3])
encoder.stages.1.0.convs.0.conv.bias shape torch.Size([64])
encoder.stages.1.0.convs.0.norm.weight shape torch.Size([64])
encoder.stages.1.0.convs.0.norm.bias shape torch.Size([64])
encoder.stages.1.0.convs.0.all_modules.0.weight shape torch.Size([64, 32, 3, 3, 3])
encoder.stages.1.0.convs.0.all_modules.0.bias shape torch.Size([64])
encoder.stages.1.0.convs.0.all_modules.1.weight shape torch.Size([64])
encoder.stages.1.0.convs.0.all_modules.1.bias shape torch.Size([64])
encoder.stages.1.0.convs.1.conv.weight shape torch.Size([64, 64, 3, 3, 3])
encoder.stages.1.0.convs.1.conv.bias shape torch.Size([64])
encoder.stages.1.0.convs.1.norm.weight shape torch.Size([64])
encoder.stages.1.0.convs.1.norm.bias shape torch.Size([64])
encoder.stages.1.0.convs.1.all_modules.0.weight shape torch.Size([64, 64, 3, 3, 3])
encoder.stages.1.0.convs.1.all_modules.0.bias shape torch.Size([64])
encoder.stages.1.0.convs.1.all_modules.1.weight shape torch.Size([64])
encoder.stages.1.0.convs.1.all_modules.1.bias shape torch.Size([64])
encoder.stages.2.0.convs.0.conv.weight shape torch.Size([128, 64, 3, 3, 3])
encoder.stages.2.0.convs.0.conv.bias shape torch.Size([128])
encoder.stages.2.0.convs.0.norm.weight shape torch.Size([128])
encoder.stages.2.0.convs.0.norm.bias shape torch.Size([128])
encoder.stages.2.0.convs.0.all_modules.0.weight shape torch.Size([128, 64, 3, 3, 3])
encoder.stages.2.0.convs.0.all_modules.0.bias shape torch.Size([128])
encoder.stages.2.0.convs.0.all_modules.1.weight shape torch.Size([128])
encoder.stages.2.0.convs.0.all_modules.1.bias shape torch.Size([128])
encoder.stages.2.0.convs.1.conv.weight shape torch.Size([128, 128, 3, 3, 3])
encoder.stages.2.0.convs.1.conv.bias shape torch.Size([128])
encoder.stages.2.0.convs.1.norm.weight shape torch.Size([128])
encoder.stages.2.0.convs.1.norm.bias shape torch.Size([128])
encoder.stages.2.0.convs.1.all_modules.0.weight shape torch.Size([128, 128, 3, 3, 3])
encoder.stages.2.0.convs.1.all_modules.0.bias shape torch.Size([128])
encoder.stages.2.0.convs.1.all_modules.1.weight shape torch.Size([128])
encoder.stages.2.0.convs.1.all_modules.1.bias shape torch.Size([128])
encoder.stages.3.0.convs.0.conv.weight shape torch.Size([256, 128, 3, 3, 3])
encoder.stages.3.0.convs.0.conv.bias shape torch.Size([256])
encoder.stages.3.0.convs.0.norm.weight shape torch.Size([256])
encoder.stages.3.0.convs.0.norm.bias shape torch.Size([256])
encoder.stages.3.0.convs.0.all_modules.0.weight shape torch.Size([256, 128, 3, 3, 3])
encoder.stages.3.0.convs.0.all_modules.0.bias shape torch.Size([256])
encoder.stages.3.0.convs.0.all_modules.1.weight shape torch.Size([256])
encoder.stages.3.0.convs.0.all_modules.1.bias shape torch.Size([256])
encoder.stages.3.0.convs.1.conv.weight shape torch.Size([256, 256, 3, 3, 3])
encoder.stages.3.0.convs.1.conv.bias shape torch.Size([256])
encoder.stages.3.0.convs.1.norm.weight shape torch.Size([256])
encoder.stages.3.0.convs.1.norm.bias shape torch.Size([256])
encoder.stages.3.0.convs.1.all_modules.0.weight shape torch.Size([256, 256, 3, 3, 3])
encoder.stages.3.0.convs.1.all_modules.0.bias shape torch.Size([256])
encoder.stages.3.0.convs.1.all_modules.1.weight shape torch.Size([256])
encoder.stages.3.0.convs.1.all_modules.1.bias shape torch.Size([256])
encoder.stages.4.0.convs.0.conv.weight shape torch.Size([320, 256, 3, 3, 3])
encoder.stages.4.0.convs.0.conv.bias shape torch.Size([320])
encoder.stages.4.0.convs.0.norm.weight shape torch.Size([320])
encoder.stages.4.0.convs.0.norm.bias shape torch.Size([320])
encoder.stages.4.0.convs.0.all_modules.0.weight shape torch.Size([320, 256, 3, 3, 3])
encoder.stages.4.0.convs.0.all_modules.0.bias shape torch.Size([320])
encoder.stages.4.0.convs.0.all_modules.1.weight shape torch.Size([320])
encoder.stages.4.0.convs.0.all_modules.1.bias shape torch.Size([320])
encoder.stages.4.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.4.0.convs.1.conv.bias shape torch.Size([320])
encoder.stages.4.0.convs.1.norm.weight shape torch.Size([320])
encoder.stages.4.0.convs.1.norm.bias shape torch.Size([320])
encoder.stages.4.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.4.0.convs.1.all_modules.0.bias shape torch.Size([320])
encoder.stages.4.0.convs.1.all_modules.1.weight shape torch.Size([320])
encoder.stages.4.0.convs.1.all_modules.1.bias shape torch.Size([320])
encoder.stages.5.0.convs.0.conv.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.5.0.convs.0.conv.bias shape torch.Size([320])
encoder.stages.5.0.convs.0.norm.weight shape torch.Size([320])
encoder.stages.5.0.convs.0.norm.bias shape torch.Size([320])
encoder.stages.5.0.convs.0.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.5.0.convs.0.all_modules.0.bias shape torch.Size([320])
encoder.stages.5.0.convs.0.all_modules.1.weight shape torch.Size([320])
encoder.stages.5.0.convs.0.all_modules.1.bias shape torch.Size([320])
encoder.stages.5.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.5.0.convs.1.conv.bias shape torch.Size([320])
encoder.stages.5.0.convs.1.norm.weight shape torch.Size([320])
encoder.stages.5.0.convs.1.norm.bias shape torch.Size([320])
encoder.stages.5.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.5.0.convs.1.all_modules.0.bias shape torch.Size([320])
encoder.stages.5.0.convs.1.all_modules.1.weight shape torch.Size([320])
encoder.stages.5.0.convs.1.all_modules.1.bias shape torch.Size([320])
decoder.encoder.stages.0.0.convs.0.conv.weight shape torch.Size([32, 1, 3, 3, 3])
decoder.encoder.stages.0.0.convs.0.conv.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.norm.weight shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.norm.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.all_modules.0.weight shape torch.Size([32, 1, 3, 3, 3])
decoder.encoder.stages.0.0.convs.0.all_modules.0.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.all_modules.1.weight shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.all_modules.1.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.conv.weight shape torch.Size([32, 32, 3, 3, 3])
decoder.encoder.stages.0.0.convs.1.conv.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.norm.weight shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.norm.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.all_modules.0.weight shape torch.Size([32, 32, 3, 3, 3])
decoder.encoder.stages.0.0.convs.1.all_modules.0.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.all_modules.1.weight shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.all_modules.1.bias shape torch.Size([32])
decoder.encoder.stages.1.0.convs.0.conv.weight shape torch.Size([64, 32, 3, 3, 3])
decoder.encoder.stages.1.0.convs.0.conv.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.norm.weight shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.norm.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.all_modules.0.weight shape torch.Size([64, 32, 3, 3, 3])
decoder.encoder.stages.1.0.convs.0.all_modules.0.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.all_modules.1.weight shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.all_modules.1.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.conv.weight shape torch.Size([64, 64, 3, 3, 3])
decoder.encoder.stages.1.0.convs.1.conv.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.norm.weight shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.norm.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.all_modules.0.weight shape torch.Size([64, 64, 3, 3, 3])
decoder.encoder.stages.1.0.convs.1.all_modules.0.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.all_modules.1.weight shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.all_modules.1.bias shape torch.Size([64])
decoder.encoder.stages.2.0.convs.0.conv.weight shape torch.Size([128, 64, 3, 3, 3])
decoder.encoder.stages.2.0.convs.0.conv.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.norm.weight shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.norm.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.all_modules.0.weight shape torch.Size([128, 64, 3, 3, 3])
decoder.encoder.stages.2.0.convs.0.all_modules.0.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.all_modules.1.weight shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.all_modules.1.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.conv.weight shape torch.Size([128, 128, 3, 3, 3])
decoder.encoder.stages.2.0.convs.1.conv.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.norm.weight shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.norm.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.all_modules.0.weight shape torch.Size([128, 128, 3, 3, 3])
decoder.encoder.stages.2.0.convs.1.all_modules.0.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.all_modules.1.weight shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.all_modules.1.bias shape torch.Size([128])
decoder.encoder.stages.3.0.convs.0.conv.weight shape torch.Size([256, 128, 3, 3, 3])
decoder.encoder.stages.3.0.convs.0.conv.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.norm.weight shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.norm.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.all_modules.0.weight shape torch.Size([256, 128, 3, 3, 3])
decoder.encoder.stages.3.0.convs.0.all_modules.0.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.all_modules.1.weight shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.all_modules.1.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.conv.weight shape torch.Size([256, 256, 3, 3, 3])
decoder.encoder.stages.3.0.convs.1.conv.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.norm.weight shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.norm.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.all_modules.0.weight shape torch.Size([256, 256, 3, 3, 3])
decoder.encoder.stages.3.0.convs.1.all_modules.0.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.all_modules.1.weight shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.all_modules.1.bias shape torch.Size([256])
decoder.encoder.stages.4.0.convs.0.conv.weight shape torch.Size([320, 256, 3, 3, 3])
decoder.encoder.stages.4.0.convs.0.conv.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.norm.weight shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.norm.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.all_modules.0.weight shape torch.Size([320, 256, 3, 3, 3])
decoder.encoder.stages.4.0.convs.0.all_modules.0.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.all_modules.1.weight shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.all_modules.1.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.4.0.convs.1.conv.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.norm.weight shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.norm.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.4.0.convs.1.all_modules.0.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.all_modules.1.weight shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.all_modules.1.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.conv.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.5.0.convs.0.conv.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.norm.weight shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.norm.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.5.0.convs.0.all_modules.0.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.all_modules.1.weight shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.all_modules.1.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.5.0.convs.1.conv.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.norm.weight shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.norm.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.5.0.convs.1.all_modules.0.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.all_modules.1.weight shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.all_modules.1.bias shape torch.Size([320])
decoder.stages.0.convs.0.conv.weight shape torch.Size([320, 640, 3, 3, 3])
decoder.stages.0.convs.0.conv.bias shape torch.Size([320])
decoder.stages.0.convs.0.norm.weight shape torch.Size([320])
decoder.stages.0.convs.0.norm.bias shape torch.Size([320])
decoder.stages.0.convs.0.all_modules.0.weight shape torch.Size([320, 640, 3, 3, 3])
decoder.stages.0.convs.0.all_modules.0.bias shape torch.Size([320])
decoder.stages.0.convs.0.all_modules.1.weight shape torch.Size([320])
decoder.stages.0.convs.0.all_modules.1.bias shape torch.Size([320])
decoder.stages.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.stages.0.convs.1.conv.bias shape torch.Size([320])
decoder.stages.0.convs.1.norm.weight shape torch.Size([320])
decoder.stages.0.convs.1.norm.bias shape torch.Size([320])
decoder.stages.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.stages.0.convs.1.all_modules.0.bias shape torch.Size([320])
decoder.stages.0.convs.1.all_modules.1.weight shape torch.Size([320])
decoder.stages.0.convs.1.all_modules.1.bias shape torch.Size([320])
decoder.stages.1.convs.0.conv.weight shape torch.Size([256, 512, 3, 3, 3])
decoder.stages.1.convs.0.conv.bias shape torch.Size([256])
decoder.stages.1.convs.0.norm.weight shape torch.Size([256])
decoder.stages.1.convs.0.norm.bias shape torch.Size([256])
decoder.stages.1.convs.0.all_modules.0.weight shape torch.Size([256, 512, 3, 3, 3])
decoder.stages.1.convs.0.all_modules.0.bias shape torch.Size([256])
decoder.stages.1.convs.0.all_modules.1.weight shape torch.Size([256])
decoder.stages.1.convs.0.all_modules.1.bias shape torch.Size([256])
decoder.stages.1.convs.1.conv.weight shape torch.Size([256, 256, 3, 3, 3])
decoder.stages.1.convs.1.conv.bias shape torch.Size([256])
decoder.stages.1.convs.1.norm.weight shape torch.Size([256])
decoder.stages.1.convs.1.norm.bias shape torch.Size([256])
decoder.stages.1.convs.1.all_modules.0.weight shape torch.Size([256, 256, 3, 3, 3])
decoder.stages.1.convs.1.all_modules.0.bias shape torch.Size([256])
decoder.stages.1.convs.1.all_modules.1.weight shape torch.Size([256])
decoder.stages.1.convs.1.all_modules.1.bias shape torch.Size([256])
decoder.stages.2.convs.0.conv.weight shape torch.Size([128, 256, 3, 3, 3])
decoder.stages.2.convs.0.conv.bias shape torch.Size([128])
decoder.stages.2.convs.0.norm.weight shape torch.Size([128])
decoder.stages.2.convs.0.norm.bias shape torch.Size([128])
decoder.stages.2.convs.0.all_modules.0.weight shape torch.Size([128, 256, 3, 3, 3])
decoder.stages.2.convs.0.all_modules.0.bias shape torch.Size([128])
decoder.stages.2.convs.0.all_modules.1.weight shape torch.Size([128])
decoder.stages.2.convs.0.all_modules.1.bias shape torch.Size([128])
decoder.stages.2.convs.1.conv.weight shape torch.Size([128, 128, 3, 3, 3])
decoder.stages.2.convs.1.conv.bias shape torch.Size([128])
decoder.stages.2.convs.1.norm.weight shape torch.Size([128])
decoder.stages.2.convs.1.norm.bias shape torch.Size([128])
decoder.stages.2.convs.1.all_modules.0.weight shape torch.Size([128, 128, 3, 3, 3])
decoder.stages.2.convs.1.all_modules.0.bias shape torch.Size([128])
decoder.stages.2.convs.1.all_modules.1.weight shape torch.Size([128])
decoder.stages.2.convs.1.all_modules.1.bias shape torch.Size([128])
decoder.stages.3.convs.0.conv.weight shape torch.Size([64, 128, 3, 3, 3])
decoder.stages.3.convs.0.conv.bias shape torch.Size([64])
decoder.stages.3.convs.0.norm.weight shape torch.Size([64])
decoder.stages.3.convs.0.norm.bias shape torch.Size([64])
decoder.stages.3.convs.0.all_modules.0.weight shape torch.Size([64, 128, 3, 3, 3])
decoder.stages.3.convs.0.all_modules.0.bias shape torch.Size([64])
decoder.stages.3.convs.0.all_modules.1.weight shape torch.Size([64])
decoder.stages.3.convs.0.all_modules.1.bias shape torch.Size([64])
decoder.stages.3.convs.1.conv.weight shape torch.Size([64, 64, 3, 3, 3])
decoder.stages.3.convs.1.conv.bias shape torch.Size([64])
decoder.stages.3.convs.1.norm.weight shape torch.Size([64])
decoder.stages.3.convs.1.norm.bias shape torch.Size([64])
decoder.stages.3.convs.1.all_modules.0.weight shape torch.Size([64, 64, 3, 3, 3])
decoder.stages.3.convs.1.all_modules.0.bias shape torch.Size([64])
decoder.stages.3.convs.1.all_modules.1.weight shape torch.Size([64])
decoder.stages.3.convs.1.all_modules.1.bias shape torch.Size([64])
decoder.stages.4.convs.0.conv.weight shape torch.Size([32, 64, 3, 3, 3])
decoder.stages.4.convs.0.conv.bias shape torch.Size([32])
decoder.stages.4.convs.0.norm.weight shape torch.Size([32])
decoder.stages.4.convs.0.norm.bias shape torch.Size([32])
decoder.stages.4.convs.0.all_modules.0.weight shape torch.Size([32, 64, 3, 3, 3])
decoder.stages.4.convs.0.all_modules.0.bias shape torch.Size([32])
decoder.stages.4.convs.0.all_modules.1.weight shape torch.Size([32])
decoder.stages.4.convs.0.all_modules.1.bias shape torch.Size([32])
decoder.stages.4.convs.1.conv.weight shape torch.Size([32, 32, 3, 3, 3])
decoder.stages.4.convs.1.conv.bias shape torch.Size([32])
decoder.stages.4.convs.1.norm.weight shape torch.Size([32])
decoder.stages.4.convs.1.norm.bias shape torch.Size([32])
decoder.stages.4.convs.1.all_modules.0.weight shape torch.Size([32, 32, 3, 3, 3])
decoder.stages.4.convs.1.all_modules.0.bias shape torch.Size([32])
decoder.stages.4.convs.1.all_modules.1.weight shape torch.Size([32])
decoder.stages.4.convs.1.all_modules.1.bias shape torch.Size([32])
decoder.transpconvs.0.weight shape torch.Size([320, 320, 1, 2, 2])
decoder.transpconvs.0.bias shape torch.Size([320])
decoder.transpconvs.1.weight shape torch.Size([320, 256, 2, 2, 2])
decoder.transpconvs.1.bias shape torch.Size([256])
decoder.transpconvs.2.weight shape torch.Size([256, 128, 2, 2, 2])
decoder.transpconvs.2.bias shape torch.Size([128])
decoder.transpconvs.3.weight shape torch.Size([128, 64, 2, 2, 2])
decoder.transpconvs.3.bias shape torch.Size([64])
decoder.transpconvs.4.weight shape torch.Size([64, 32, 2, 2, 2])
decoder.transpconvs.4.bias shape torch.Size([32])
################### Done ###################
2024-11-30 02:58:35.229550: do_dummy_2d_data_aug: True
2024-11-30 02:58:35.231519: Using splits from existing split file: /home/gridsan/nchutisilp/datasets/nnUNet_Datasets/nnUNet_preprocessed/Dataset306_Sohee_Ajay_Calcium_OCT/splits_final.json
2024-11-30 02:58:35.233055: The split file contains 5 splits.
2024-11-30 02:58:35.233776: Desired fold for training: 4
2024-11-30 02:58:35.234594: This split has 11 training and 2 validation cases.
using pin_memory on device 0
using pin_memory on device 0

This is the configuration used by this training:
Configuration name: 3d_32x160x128_b10
 {'data_identifier': 'nnUNetPlans_3d_fullres', 'preprocessor_name': 'DefaultPreprocessor', 'batch_size': 10, 'patch_size': [32, 160, 128], 'median_image_size_in_voxels': [375.0, 498.0, 498.0], 'spacing': [1.0, 1.0, 1.0], 'normalization_schemes': ['ZScoreNormalization'], 'use_mask_for_norm': [False], 'resampling_fn_data': 'resample_data_or_seg_to_shape', 'resampling_fn_seg': 'resample_data_or_seg_to_shape', 'resampling_fn_data_kwargs': {'is_seg': False, 'order': 3, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_seg_kwargs': {'is_seg': True, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_probabilities': 'resample_data_or_seg_to_shape', 'resampling_fn_probabilities_kwargs': {'is_seg': False, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'architecture': {'network_class_name': 'dynamic_network_architectures.architectures.unet.PlainConvUNet', 'arch_kwargs': {'n_stages': 6, 'features_per_stage': [32, 64, 128, 256, 320, 320], 'conv_op': 'torch.nn.modules.conv.Conv3d', 'kernel_sizes': [[3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3]], 'strides': [[1, 1, 1], [2, 2, 2], [2, 2, 2], [2, 2, 2], [2, 2, 2], [1, 2, 2]], 'n_conv_per_stage': [2, 2, 2, 2, 2, 2], 'n_conv_per_stage_decoder': [2, 2, 2, 2, 2], 'conv_bias': True, 'norm_op': 'torch.nn.modules.instancenorm.InstanceNorm3d', 'norm_op_kwargs': {'eps': 1e-05, 'affine': True}, 'dropout_op': None, 'dropout_op_kwargs': None, 'nonlin': 'torch.nn.LeakyReLU', 'nonlin_kwargs': {'inplace': True}, 'deep_supervision': True}, '_kw_requires_import': ['conv_op', 'norm_op', 'dropout_op', 'nonlin']}, 'batch_dice': True, 'inherits_from': '3d_fullres'} 

These are the global plan.json settings:
 {'dataset_name': 'Dataset306_Sohee_Ajay_Calcium_OCT', 'plans_name': 'nnUNetPlans', 'original_median_spacing_after_transp': [1.0, 1.0, 1.0], 'original_median_shape_after_transp': [375, 498, 498], 'image_reader_writer': 'SimpleITKIO', 'transpose_forward': [0, 1, 2], 'transpose_backward': [0, 1, 2], 'experiment_planner_used': 'ExperimentPlanner', 'label_manager': 'LabelManager', 'foreground_intensity_properties_per_channel': {'0': {'max': 0.49803921580314636, 'mean': 0.14296366274356842, 'median': 0.10980392247438431, 'min': 0.0, 'percentile_00_5': 0.01558823511004448, 'percentile_99_5': 0.49803921580314636, 'std': 0.11562220752239227}}} 

2024-11-30 02:58:38.119725: unpacking dataset...
2024-11-29 23:23:58.686328: unpacking done...
2024-11-29 23:23:58.860717: Unable to plot network architecture: nnUNet_compile is enabled!
2024-11-29 23:23:59.122985: 
2024-11-29 23:23:59.124891: Epoch 0
2024-11-29 23:23:59.125813: Current learning rate: 0.01
2024-11-29 23:26:52.571846: Validation loss improved from 1000.00000 to -0.15270! Patience: 0/50
2024-11-29 23:26:52.573649: train_loss -0.083
2024-11-29 23:26:52.578434: val_loss -0.1527
2024-11-29 23:26:52.579163: Pseudo dice [0.4964]
2024-11-29 23:26:52.579818: Epoch time: 173.45 s
2024-11-29 23:26:52.580672: Yayy! New best EMA pseudo Dice: 0.4964
2024-11-29 23:26:54.747568: 
2024-11-29 23:26:54.749568: Epoch 1
2024-11-29 23:26:54.750711: Current learning rate: 0.00999
2024-11-29 23:28:20.771365: Validation loss improved from -0.15270 to -0.20452! Patience: 0/50
2024-11-29 23:28:20.772546: train_loss -0.2169
2024-11-29 23:28:20.773492: val_loss -0.2045
2024-11-29 23:28:20.774339: Pseudo dice [0.5027]
2024-11-29 23:28:20.775083: Epoch time: 86.03 s
2024-11-29 23:28:20.775958: Yayy! New best EMA pseudo Dice: 0.497
2024-11-29 23:28:22.315863: 
2024-11-29 23:28:22.317771: Epoch 2
2024-11-29 23:28:22.318962: Current learning rate: 0.00998
2024-11-29 23:29:48.346170: Validation loss improved from -0.20452 to -0.23318! Patience: 0/50
2024-11-29 23:29:48.347327: train_loss -0.2454
2024-11-29 23:29:48.348133: val_loss -0.2332
2024-11-29 23:29:48.348790: Pseudo dice [0.5146]
2024-11-29 23:29:48.349550: Epoch time: 86.03 s
2024-11-29 23:29:48.350255: Yayy! New best EMA pseudo Dice: 0.4988
2024-11-29 23:29:50.071455: 
2024-11-29 23:29:50.074226: Epoch 3
2024-11-29 23:29:50.075724: Current learning rate: 0.00997
2024-11-29 23:31:16.102244: Validation loss did not improve from -0.23318. Patience: 1/50
2024-11-29 23:31:16.103197: train_loss -0.2869
2024-11-29 23:31:16.104146: val_loss -0.2161
2024-11-29 23:31:16.104972: Pseudo dice [0.5554]
2024-11-29 23:31:16.105872: Epoch time: 86.03 s
2024-11-29 23:31:16.106807: Yayy! New best EMA pseudo Dice: 0.5045
2024-11-29 23:31:17.681280: 
2024-11-29 23:31:17.683758: Epoch 4
2024-11-29 23:31:17.684792: Current learning rate: 0.00996
2024-11-29 23:32:43.763151: Validation loss improved from -0.23318 to -0.27405! Patience: 1/50
2024-11-29 23:32:43.764817: train_loss -0.3193
2024-11-29 23:32:43.765913: val_loss -0.2741
2024-11-29 23:32:43.766783: Pseudo dice [0.5467]
2024-11-29 23:32:43.767554: Epoch time: 86.08 s
2024-11-29 23:32:44.129482: Yayy! New best EMA pseudo Dice: 0.5087
2024-11-29 23:32:45.803853: 
2024-11-29 23:32:45.805663: Epoch 5
2024-11-29 23:32:45.806569: Current learning rate: 0.00995
2024-11-29 23:34:11.946261: Validation loss did not improve from -0.27405. Patience: 1/50
2024-11-29 23:34:11.947198: train_loss -0.3381
2024-11-29 23:34:11.948229: val_loss -0.2442
2024-11-29 23:34:11.949124: Pseudo dice [0.5795]
2024-11-29 23:34:11.949912: Epoch time: 86.14 s
2024-11-29 23:34:11.950804: Yayy! New best EMA pseudo Dice: 0.5158
2024-11-29 23:34:13.534578: 
2024-11-29 23:34:13.536520: Epoch 6
2024-11-29 23:34:13.537464: Current learning rate: 0.00995
2024-11-29 23:35:39.559780: Validation loss did not improve from -0.27405. Patience: 2/50
2024-11-29 23:35:39.561050: train_loss -0.3671
2024-11-29 23:35:39.561981: val_loss -0.2675
2024-11-29 23:35:39.562710: Pseudo dice [0.5786]
2024-11-29 23:35:39.563532: Epoch time: 86.03 s
2024-11-29 23:35:39.564192: Yayy! New best EMA pseudo Dice: 0.522
2024-11-29 23:35:41.135092: 
2024-11-29 23:35:41.137163: Epoch 7
2024-11-29 23:35:41.137983: Current learning rate: 0.00994
2024-11-29 23:37:07.168683: Validation loss improved from -0.27405 to -0.28665! Patience: 2/50
2024-11-29 23:37:07.169926: train_loss -0.3811
2024-11-29 23:37:07.170897: val_loss -0.2867
2024-11-29 23:37:07.171620: Pseudo dice [0.5642]
2024-11-29 23:37:07.172295: Epoch time: 86.04 s
2024-11-29 23:37:07.173088: Yayy! New best EMA pseudo Dice: 0.5263
2024-11-29 23:37:09.151417: 
2024-11-29 23:37:09.153651: Epoch 8
2024-11-29 23:37:09.154553: Current learning rate: 0.00993
2024-11-29 23:38:35.267437: Validation loss improved from -0.28665 to -0.31700! Patience: 0/50
2024-11-29 23:38:35.268577: train_loss -0.4019
2024-11-29 23:38:35.269742: val_loss -0.317
2024-11-29 23:38:35.271010: Pseudo dice [0.5957]
2024-11-29 23:38:35.272582: Epoch time: 86.12 s
2024-11-29 23:38:35.273728: Yayy! New best EMA pseudo Dice: 0.5332
2024-11-29 23:38:36.930829: 
2024-11-29 23:38:36.932498: Epoch 9
2024-11-29 23:38:36.933419: Current learning rate: 0.00992
2024-11-29 23:40:02.951537: Validation loss improved from -0.31700 to -0.33662! Patience: 0/50
2024-11-29 23:40:02.952540: train_loss -0.3949
2024-11-29 23:40:02.953380: val_loss -0.3366
2024-11-29 23:40:02.954034: Pseudo dice [0.5963]
2024-11-29 23:40:02.954641: Epoch time: 86.02 s
2024-11-29 23:40:03.282326: Yayy! New best EMA pseudo Dice: 0.5395
2024-11-29 23:40:04.792831: 
2024-11-29 23:40:04.794795: Epoch 10
2024-11-29 23:40:04.795708: Current learning rate: 0.00991
2024-11-29 23:41:30.858756: Validation loss improved from -0.33662 to -0.38078! Patience: 0/50
2024-11-29 23:41:30.860140: train_loss -0.4299
2024-11-29 23:41:30.861210: val_loss -0.3808
2024-11-29 23:41:30.862298: Pseudo dice [0.6415]
2024-11-29 23:41:30.863250: Epoch time: 86.07 s
2024-11-29 23:41:30.864119: Yayy! New best EMA pseudo Dice: 0.5497
2024-11-29 23:41:32.457283: 
2024-11-29 23:41:32.459006: Epoch 11
2024-11-29 23:41:32.459930: Current learning rate: 0.0099
2024-11-29 23:42:58.522089: Validation loss did not improve from -0.38078. Patience: 1/50
2024-11-29 23:42:58.523076: train_loss -0.4284
2024-11-29 23:42:58.524175: val_loss -0.3727
2024-11-29 23:42:58.525122: Pseudo dice [0.6321]
2024-11-29 23:42:58.526038: Epoch time: 86.07 s
2024-11-29 23:42:58.526927: Yayy! New best EMA pseudo Dice: 0.558
2024-11-29 23:43:00.158730: 
2024-11-29 23:43:00.160590: Epoch 12
2024-11-29 23:43:00.161575: Current learning rate: 0.00989
2024-11-29 23:44:26.201275: Validation loss did not improve from -0.38078. Patience: 2/50
2024-11-29 23:44:26.202343: train_loss -0.4533
2024-11-29 23:44:26.203301: val_loss -0.3697
2024-11-29 23:44:26.204135: Pseudo dice [0.6284]
2024-11-29 23:44:26.204869: Epoch time: 86.04 s
2024-11-29 23:44:26.205454: Yayy! New best EMA pseudo Dice: 0.565
2024-11-29 23:44:27.851493: 
2024-11-29 23:44:27.853465: Epoch 13
2024-11-29 23:44:27.854247: Current learning rate: 0.00988
2024-11-29 23:45:53.884706: Validation loss improved from -0.38078 to -0.38199! Patience: 2/50
2024-11-29 23:45:53.885774: train_loss -0.4597
2024-11-29 23:45:53.886744: val_loss -0.382
2024-11-29 23:45:53.887447: Pseudo dice [0.6292]
2024-11-29 23:45:53.888170: Epoch time: 86.04 s
2024-11-29 23:45:53.889090: Yayy! New best EMA pseudo Dice: 0.5714
2024-11-29 23:45:55.554347: 
2024-11-29 23:45:55.555956: Epoch 14
2024-11-29 23:45:55.556721: Current learning rate: 0.00987
2024-11-29 23:47:21.601995: Validation loss improved from -0.38199 to -0.39151! Patience: 0/50
2024-11-29 23:47:21.603209: train_loss -0.4697
2024-11-29 23:47:21.604682: val_loss -0.3915
2024-11-29 23:47:21.605952: Pseudo dice [0.6425]
2024-11-29 23:47:21.607471: Epoch time: 86.05 s
2024-11-29 23:47:22.078937: Yayy! New best EMA pseudo Dice: 0.5785
2024-11-29 23:47:23.709929: 
2024-11-29 23:47:23.711974: Epoch 15
2024-11-29 23:47:23.713025: Current learning rate: 0.00986
2024-11-29 23:48:49.785262: Validation loss did not improve from -0.39151. Patience: 1/50
2024-11-29 23:48:49.786188: train_loss -0.4855
2024-11-29 23:48:49.787005: val_loss -0.3856
2024-11-29 23:48:49.787882: Pseudo dice [0.6411]
2024-11-29 23:48:49.788751: Epoch time: 86.08 s
2024-11-29 23:48:49.789514: Yayy! New best EMA pseudo Dice: 0.5848
2024-11-29 23:48:51.483178: 
2024-11-29 23:48:51.484927: Epoch 16
2024-11-29 23:48:51.485888: Current learning rate: 0.00986
2024-11-29 23:50:17.552931: Validation loss did not improve from -0.39151. Patience: 2/50
2024-11-29 23:50:17.554302: train_loss -0.481
2024-11-29 23:50:17.555399: val_loss -0.3468
2024-11-29 23:50:17.556388: Pseudo dice [0.6255]
2024-11-29 23:50:17.557313: Epoch time: 86.07 s
2024-11-29 23:50:17.558154: Yayy! New best EMA pseudo Dice: 0.5889
2024-11-29 23:50:19.307576: 
2024-11-29 23:50:19.309093: Epoch 17
2024-11-29 23:50:19.309890: Current learning rate: 0.00985
2024-11-29 23:51:45.452331: Validation loss improved from -0.39151 to -0.39658! Patience: 2/50
2024-11-29 23:51:45.453689: train_loss -0.4772
2024-11-29 23:51:45.454608: val_loss -0.3966
2024-11-29 23:51:45.455375: Pseudo dice [0.642]
2024-11-29 23:51:45.456074: Epoch time: 86.15 s
2024-11-29 23:51:45.456879: Yayy! New best EMA pseudo Dice: 0.5942
2024-11-29 23:51:47.198946: 
2024-11-29 23:51:47.200855: Epoch 18
2024-11-29 23:51:47.201645: Current learning rate: 0.00984
2024-11-29 23:53:13.328245: Validation loss did not improve from -0.39658. Patience: 1/50
2024-11-29 23:53:13.329140: train_loss -0.4996
2024-11-29 23:53:13.330084: val_loss -0.3754
2024-11-29 23:53:13.330769: Pseudo dice [0.6221]
2024-11-29 23:53:13.331557: Epoch time: 86.13 s
2024-11-29 23:53:13.332343: Yayy! New best EMA pseudo Dice: 0.597
2024-11-29 23:53:15.385140: 
2024-11-29 23:53:15.386805: Epoch 19
2024-11-29 23:53:15.387595: Current learning rate: 0.00983
2024-11-29 23:54:41.431234: Validation loss did not improve from -0.39658. Patience: 2/50
2024-11-29 23:54:41.432273: train_loss -0.5136
2024-11-29 23:54:41.433180: val_loss -0.3937
2024-11-29 23:54:41.433949: Pseudo dice [0.632]
2024-11-29 23:54:41.434798: Epoch time: 86.05 s
2024-11-29 23:54:41.791559: Yayy! New best EMA pseudo Dice: 0.6005
2024-11-29 23:54:43.443964: 
2024-11-29 23:54:43.446399: Epoch 20
2024-11-29 23:54:43.447715: Current learning rate: 0.00982
2024-11-29 23:56:09.549776: Validation loss improved from -0.39658 to -0.39681! Patience: 2/50
2024-11-29 23:56:09.550791: train_loss -0.5259
2024-11-29 23:56:09.551637: val_loss -0.3968
2024-11-29 23:56:09.552472: Pseudo dice [0.6405]
2024-11-29 23:56:09.553258: Epoch time: 86.11 s
2024-11-29 23:56:09.554113: Yayy! New best EMA pseudo Dice: 0.6045
2024-11-29 23:56:11.204144: 
2024-11-29 23:56:11.205823: Epoch 21
2024-11-29 23:56:11.206705: Current learning rate: 0.00981
2024-11-29 23:57:37.268362: Validation loss improved from -0.39681 to -0.40599! Patience: 0/50
2024-11-29 23:57:37.269420: train_loss -0.5144
2024-11-29 23:57:37.270333: val_loss -0.406
2024-11-29 23:57:37.271155: Pseudo dice [0.6528]
2024-11-29 23:57:37.271846: Epoch time: 86.07 s
2024-11-29 23:57:37.272572: Yayy! New best EMA pseudo Dice: 0.6093
2024-11-29 23:57:38.839741: 
2024-11-29 23:57:38.841388: Epoch 22
2024-11-29 23:57:38.842276: Current learning rate: 0.0098
2024-11-29 23:59:04.880589: Validation loss did not improve from -0.40599. Patience: 1/50
2024-11-29 23:59:04.881593: train_loss -0.5169
2024-11-29 23:59:04.882516: val_loss -0.3919
2024-11-29 23:59:04.883261: Pseudo dice [0.64]
2024-11-29 23:59:04.883929: Epoch time: 86.04 s
2024-11-29 23:59:04.884607: Yayy! New best EMA pseudo Dice: 0.6124
2024-11-29 23:59:06.457684: 
2024-11-29 23:59:06.459395: Epoch 23
2024-11-29 23:59:06.460332: Current learning rate: 0.00979
2024-11-30 00:00:32.489420: Validation loss did not improve from -0.40599. Patience: 2/50
2024-11-30 00:00:32.490494: train_loss -0.5269
2024-11-30 00:00:32.491491: val_loss -0.3483
2024-11-30 00:00:32.492418: Pseudo dice [0.6297]
2024-11-30 00:00:32.493306: Epoch time: 86.03 s
2024-11-30 00:00:32.494042: Yayy! New best EMA pseudo Dice: 0.6141
2024-11-30 00:00:34.046433: 
2024-11-30 00:00:34.048106: Epoch 24
2024-11-30 00:00:34.048938: Current learning rate: 0.00978
2024-11-30 00:02:00.070408: Validation loss improved from -0.40599 to -0.42884! Patience: 2/50
2024-11-30 00:02:00.071368: train_loss -0.5282
2024-11-30 00:02:00.072162: val_loss -0.4288
2024-11-30 00:02:00.073022: Pseudo dice [0.6614]
2024-11-30 00:02:00.073889: Epoch time: 86.03 s
2024-11-30 00:02:00.436294: Yayy! New best EMA pseudo Dice: 0.6188
2024-11-30 00:02:01.969496: 
2024-11-30 00:02:01.970577: Epoch 25
2024-11-30 00:02:01.971450: Current learning rate: 0.00977
2024-11-30 00:03:28.042547: Validation loss did not improve from -0.42884. Patience: 1/50
2024-11-30 00:03:28.043534: train_loss -0.5363
2024-11-30 00:03:28.044501: val_loss -0.4217
2024-11-30 00:03:28.045173: Pseudo dice [0.6548]
2024-11-30 00:03:28.045902: Epoch time: 86.08 s
2024-11-30 00:03:28.046530: Yayy! New best EMA pseudo Dice: 0.6224
2024-11-30 00:03:29.633949: 
2024-11-30 00:03:29.635566: Epoch 26
2024-11-30 00:03:29.636339: Current learning rate: 0.00977
2024-11-30 00:04:55.634881: Validation loss improved from -0.42884 to -0.44964! Patience: 1/50
2024-11-30 00:04:55.636243: train_loss -0.5331
2024-11-30 00:04:55.637251: val_loss -0.4496
2024-11-30 00:04:55.638196: Pseudo dice [0.6823]
2024-11-30 00:04:55.638983: Epoch time: 86.0 s
2024-11-30 00:04:55.639741: Yayy! New best EMA pseudo Dice: 0.6284
2024-11-30 00:04:57.211792: 
2024-11-30 00:04:57.213497: Epoch 27
2024-11-30 00:04:57.214564: Current learning rate: 0.00976
2024-11-30 00:06:23.274356: Validation loss did not improve from -0.44964. Patience: 1/50
2024-11-30 00:06:23.275412: train_loss -0.5363
2024-11-30 00:06:23.276315: val_loss -0.3788
2024-11-30 00:06:23.276997: Pseudo dice [0.6402]
2024-11-30 00:06:23.277623: Epoch time: 86.06 s
2024-11-30 00:06:23.278265: Yayy! New best EMA pseudo Dice: 0.6296
2024-11-30 00:06:24.865167: 
2024-11-30 00:06:24.867076: Epoch 28
2024-11-30 00:06:24.867903: Current learning rate: 0.00975
2024-11-30 00:07:51.023456: Validation loss did not improve from -0.44964. Patience: 2/50
2024-11-30 00:07:51.024695: train_loss -0.5504
2024-11-30 00:07:51.025802: val_loss -0.4074
2024-11-30 00:07:51.026587: Pseudo dice [0.6515]
2024-11-30 00:07:51.027317: Epoch time: 86.16 s
2024-11-30 00:07:51.028053: Yayy! New best EMA pseudo Dice: 0.6318
2024-11-30 00:07:53.031690: 
2024-11-30 00:07:53.032820: Epoch 29
2024-11-30 00:07:53.033619: Current learning rate: 0.00974
2024-11-30 00:09:19.094307: Validation loss did not improve from -0.44964. Patience: 3/50
2024-11-30 00:09:19.095237: train_loss -0.545
2024-11-30 00:09:19.096092: val_loss -0.4307
2024-11-30 00:09:19.096841: Pseudo dice [0.6686]
2024-11-30 00:09:19.097643: Epoch time: 86.06 s
2024-11-30 00:09:19.507076: Yayy! New best EMA pseudo Dice: 0.6355
2024-11-30 00:09:21.072121: 
2024-11-30 00:09:21.073518: Epoch 30
2024-11-30 00:09:21.074227: Current learning rate: 0.00973
2024-11-30 00:10:47.133990: Validation loss did not improve from -0.44964. Patience: 4/50
2024-11-30 00:10:47.135189: train_loss -0.5481
2024-11-30 00:10:47.136166: val_loss -0.4418
2024-11-30 00:10:47.137063: Pseudo dice [0.6712]
2024-11-30 00:10:47.137808: Epoch time: 86.06 s
2024-11-30 00:10:47.138519: Yayy! New best EMA pseudo Dice: 0.639
2024-11-30 00:10:48.756165: 
2024-11-30 00:10:48.757886: Epoch 31
2024-11-30 00:10:48.758674: Current learning rate: 0.00972
2024-11-30 00:12:14.878182: Validation loss did not improve from -0.44964. Patience: 5/50
2024-11-30 00:12:14.879325: train_loss -0.5642
2024-11-30 00:12:14.880102: val_loss -0.4186
2024-11-30 00:12:14.880874: Pseudo dice [0.6646]
2024-11-30 00:12:14.881678: Epoch time: 86.12 s
2024-11-30 00:12:14.882447: Yayy! New best EMA pseudo Dice: 0.6416
2024-11-30 00:12:16.540662: 
2024-11-30 00:12:16.542161: Epoch 32
2024-11-30 00:12:16.543309: Current learning rate: 0.00971
2024-11-30 00:13:42.527791: Validation loss did not improve from -0.44964. Patience: 6/50
2024-11-30 00:13:42.528799: train_loss -0.5605
2024-11-30 00:13:42.529739: val_loss -0.3846
2024-11-30 00:13:42.530544: Pseudo dice [0.6538]
2024-11-30 00:13:42.531198: Epoch time: 85.99 s
2024-11-30 00:13:42.532004: Yayy! New best EMA pseudo Dice: 0.6428
2024-11-30 00:13:44.111577: 
2024-11-30 00:13:44.113725: Epoch 33
2024-11-30 00:13:44.114855: Current learning rate: 0.0097
2024-11-30 00:15:10.131315: Validation loss did not improve from -0.44964. Patience: 7/50
2024-11-30 00:15:10.132688: train_loss -0.5546
2024-11-30 00:15:10.134029: val_loss -0.4435
2024-11-30 00:15:10.134811: Pseudo dice [0.6766]
2024-11-30 00:15:10.135673: Epoch time: 86.02 s
2024-11-30 00:15:10.136341: Yayy! New best EMA pseudo Dice: 0.6462
2024-11-30 00:15:11.779111: 
2024-11-30 00:15:11.780828: Epoch 34
2024-11-30 00:15:11.781874: Current learning rate: 0.00969
2024-11-30 00:16:37.780879: Validation loss did not improve from -0.44964. Patience: 8/50
2024-11-30 00:16:37.782115: train_loss -0.5711
2024-11-30 00:16:37.783192: val_loss -0.4106
2024-11-30 00:16:37.784020: Pseudo dice [0.6572]
2024-11-30 00:16:37.784955: Epoch time: 86.0 s
2024-11-30 00:16:38.168271: Yayy! New best EMA pseudo Dice: 0.6473
2024-11-30 00:16:39.768699: 
2024-11-30 00:16:39.770150: Epoch 35
2024-11-30 00:16:39.771242: Current learning rate: 0.00968
2024-11-30 00:18:05.803210: Validation loss did not improve from -0.44964. Patience: 9/50
2024-11-30 00:18:05.804615: train_loss -0.5633
2024-11-30 00:18:05.805557: val_loss -0.4122
2024-11-30 00:18:05.806320: Pseudo dice [0.6573]
2024-11-30 00:18:05.807133: Epoch time: 86.04 s
2024-11-30 00:18:05.808002: Yayy! New best EMA pseudo Dice: 0.6483
2024-11-30 00:18:07.467111: 
2024-11-30 00:18:07.468766: Epoch 36
2024-11-30 00:18:07.469620: Current learning rate: 0.00968
2024-11-30 00:19:33.508656: Validation loss did not improve from -0.44964. Patience: 10/50
2024-11-30 00:19:33.509907: train_loss -0.5674
2024-11-30 00:19:33.511160: val_loss -0.4297
2024-11-30 00:19:33.512014: Pseudo dice [0.6619]
2024-11-30 00:19:33.512922: Epoch time: 86.04 s
2024-11-30 00:19:33.513781: Yayy! New best EMA pseudo Dice: 0.6497
2024-11-30 00:19:35.211415: 
2024-11-30 00:19:35.213770: Epoch 37
2024-11-30 00:19:35.215109: Current learning rate: 0.00967
2024-11-30 00:21:01.263812: Validation loss did not improve from -0.44964. Patience: 11/50
2024-11-30 00:21:01.265060: train_loss -0.5797
2024-11-30 00:21:01.266224: val_loss -0.4216
2024-11-30 00:21:01.267009: Pseudo dice [0.6698]
2024-11-30 00:21:01.267714: Epoch time: 86.05 s
2024-11-30 00:21:01.268473: Yayy! New best EMA pseudo Dice: 0.6517
2024-11-30 00:21:02.923095: 
2024-11-30 00:21:02.924986: Epoch 38
2024-11-30 00:21:02.926021: Current learning rate: 0.00966
2024-11-30 00:22:29.055506: Validation loss did not improve from -0.44964. Patience: 12/50
2024-11-30 00:22:29.056770: train_loss -0.5762
2024-11-30 00:22:29.057794: val_loss -0.4471
2024-11-30 00:22:29.058609: Pseudo dice [0.6771]
2024-11-30 00:22:29.059366: Epoch time: 86.13 s
2024-11-30 00:22:29.060024: Yayy! New best EMA pseudo Dice: 0.6542
2024-11-30 00:22:31.116510: 
2024-11-30 00:22:31.117985: Epoch 39
2024-11-30 00:22:31.118835: Current learning rate: 0.00965
2024-11-30 00:23:57.133276: Validation loss did not improve from -0.44964. Patience: 13/50
2024-11-30 00:23:57.134320: train_loss -0.576
2024-11-30 00:23:57.135541: val_loss -0.439
2024-11-30 00:23:57.136395: Pseudo dice [0.6785]
2024-11-30 00:23:57.137146: Epoch time: 86.02 s
2024-11-30 00:23:57.497349: Yayy! New best EMA pseudo Dice: 0.6567
2024-11-30 00:23:59.167354: 
2024-11-30 00:23:59.168513: Epoch 40
2024-11-30 00:23:59.169307: Current learning rate: 0.00964
2024-11-30 00:25:25.229361: Validation loss did not improve from -0.44964. Patience: 14/50
2024-11-30 00:25:25.230508: train_loss -0.5741
2024-11-30 00:25:25.231584: val_loss -0.4379
2024-11-30 00:25:25.232302: Pseudo dice [0.6849]
2024-11-30 00:25:25.233079: Epoch time: 86.06 s
2024-11-30 00:25:25.233819: Yayy! New best EMA pseudo Dice: 0.6595
2024-11-30 00:25:26.905485: 
2024-11-30 00:25:26.907039: Epoch 41
2024-11-30 00:25:26.907792: Current learning rate: 0.00963
2024-11-30 00:26:53.102228: Validation loss did not improve from -0.44964. Patience: 15/50
2024-11-30 00:26:53.103356: train_loss -0.5833
2024-11-30 00:26:53.104412: val_loss -0.4182
2024-11-30 00:26:53.105273: Pseudo dice [0.6613]
2024-11-30 00:26:53.106262: Epoch time: 86.2 s
2024-11-30 00:26:53.107016: Yayy! New best EMA pseudo Dice: 0.6597
2024-11-30 00:26:54.790993: 
2024-11-30 00:26:54.792703: Epoch 42
2024-11-30 00:26:54.793706: Current learning rate: 0.00962
2024-11-30 00:28:20.851102: Validation loss did not improve from -0.44964. Patience: 16/50
2024-11-30 00:28:20.851955: train_loss -0.5836
2024-11-30 00:28:20.853041: val_loss -0.3928
2024-11-30 00:28:20.853696: Pseudo dice [0.6372]
2024-11-30 00:28:20.854407: Epoch time: 86.06 s
2024-11-30 00:28:22.132089: 
2024-11-30 00:28:22.133992: Epoch 43
2024-11-30 00:28:22.135040: Current learning rate: 0.00961
2024-11-30 00:29:48.260732: Validation loss did not improve from -0.44964. Patience: 17/50
2024-11-30 00:29:48.261865: train_loss -0.5754
2024-11-30 00:29:48.262825: val_loss -0.4413
2024-11-30 00:29:48.263491: Pseudo dice [0.6704]
2024-11-30 00:29:48.264260: Epoch time: 86.13 s
2024-11-30 00:29:49.523939: 
2024-11-30 00:29:49.525671: Epoch 44
2024-11-30 00:29:49.526788: Current learning rate: 0.0096
2024-11-30 00:31:15.614214: Validation loss improved from -0.44964 to -0.45497! Patience: 17/50
2024-11-30 00:31:15.615292: train_loss -0.5824
2024-11-30 00:31:15.616304: val_loss -0.455
2024-11-30 00:31:15.617231: Pseudo dice [0.6807]
2024-11-30 00:31:15.618207: Epoch time: 86.09 s
2024-11-30 00:31:15.980202: Yayy! New best EMA pseudo Dice: 0.6609
2024-11-30 00:31:17.560815: 
2024-11-30 00:31:17.562222: Epoch 45
2024-11-30 00:31:17.562936: Current learning rate: 0.00959
2024-11-30 00:32:43.648107: Validation loss did not improve from -0.45497. Patience: 1/50
2024-11-30 00:32:43.649482: train_loss -0.61
2024-11-30 00:32:43.651263: val_loss -0.4382
2024-11-30 00:32:43.652587: Pseudo dice [0.6765]
2024-11-30 00:32:43.653664: Epoch time: 86.09 s
2024-11-30 00:32:43.654843: Yayy! New best EMA pseudo Dice: 0.6625
2024-11-30 00:32:45.238931: 
2024-11-30 00:32:45.240625: Epoch 46
2024-11-30 00:32:45.241422: Current learning rate: 0.00959
2024-11-30 00:34:11.260859: Validation loss did not improve from -0.45497. Patience: 2/50
2024-11-30 00:34:11.261897: train_loss -0.6029
2024-11-30 00:34:11.262922: val_loss -0.4314
2024-11-30 00:34:11.263679: Pseudo dice [0.6575]
2024-11-30 00:34:11.264570: Epoch time: 86.02 s
2024-11-30 00:34:12.501819: 
2024-11-30 00:34:12.503759: Epoch 47
2024-11-30 00:34:12.504570: Current learning rate: 0.00958
2024-11-30 00:35:38.575016: Validation loss did not improve from -0.45497. Patience: 3/50
2024-11-30 00:35:38.576148: train_loss -0.6023
2024-11-30 00:35:38.577142: val_loss -0.4169
2024-11-30 00:35:38.577902: Pseudo dice [0.6612]
2024-11-30 00:35:38.578632: Epoch time: 86.08 s
2024-11-30 00:35:39.790934: 
2024-11-30 00:35:39.792827: Epoch 48
2024-11-30 00:35:39.793713: Current learning rate: 0.00957
2024-11-30 00:37:05.915167: Validation loss did not improve from -0.45497. Patience: 4/50
2024-11-30 00:37:05.915919: train_loss -0.5825
2024-11-30 00:37:05.916700: val_loss -0.4393
2024-11-30 00:37:05.917423: Pseudo dice [0.6806]
2024-11-30 00:37:05.918112: Epoch time: 86.13 s
2024-11-30 00:37:05.918743: Yayy! New best EMA pseudo Dice: 0.6638
2024-11-30 00:37:07.587917: 
2024-11-30 00:37:07.589401: Epoch 49
2024-11-30 00:37:07.590285: Current learning rate: 0.00956
2024-11-30 00:38:33.610077: Validation loss did not improve from -0.45497. Patience: 5/50
2024-11-30 00:38:33.611009: train_loss -0.6017
2024-11-30 00:38:33.611850: val_loss -0.4371
2024-11-30 00:38:33.612750: Pseudo dice [0.6741]
2024-11-30 00:38:33.613589: Epoch time: 86.02 s
2024-11-30 00:38:35.044955: Yayy! New best EMA pseudo Dice: 0.6648
2024-11-30 00:38:36.664539: 
2024-11-30 00:38:36.666548: Epoch 50
2024-11-30 00:38:36.667475: Current learning rate: 0.00955
2024-11-30 00:40:02.750922: Validation loss improved from -0.45497 to -0.45734! Patience: 5/50
2024-11-30 00:40:02.752126: train_loss -0.6087
2024-11-30 00:40:02.752985: val_loss -0.4573
2024-11-30 00:40:02.753743: Pseudo dice [0.6946]
2024-11-30 00:40:02.754687: Epoch time: 86.09 s
2024-11-30 00:40:02.755632: Yayy! New best EMA pseudo Dice: 0.6678
2024-11-30 00:40:04.383130: 
2024-11-30 00:40:04.384918: Epoch 51
2024-11-30 00:40:04.386072: Current learning rate: 0.00954
2024-11-30 00:41:30.575955: Validation loss improved from -0.45734 to -0.47287! Patience: 0/50
2024-11-30 00:41:30.577016: train_loss -0.616
2024-11-30 00:41:30.577833: val_loss -0.4729
2024-11-30 00:41:30.578570: Pseudo dice [0.7014]
2024-11-30 00:41:30.579266: Epoch time: 86.19 s
2024-11-30 00:41:30.580073: Yayy! New best EMA pseudo Dice: 0.6711
2024-11-30 00:41:32.180010: 
2024-11-30 00:41:32.181130: Epoch 52
2024-11-30 00:41:32.182199: Current learning rate: 0.00953
2024-11-30 00:42:58.192550: Validation loss did not improve from -0.47287. Patience: 1/50
2024-11-30 00:42:58.193646: train_loss -0.6152
2024-11-30 00:42:58.194412: val_loss -0.447
2024-11-30 00:42:58.195203: Pseudo dice [0.6755]
2024-11-30 00:42:58.196058: Epoch time: 86.01 s
2024-11-30 00:42:58.196827: Yayy! New best EMA pseudo Dice: 0.6716
2024-11-30 00:42:59.830777: 
2024-11-30 00:42:59.832016: Epoch 53
2024-11-30 00:42:59.832757: Current learning rate: 0.00952
2024-11-30 00:44:25.886637: Validation loss did not improve from -0.47287. Patience: 2/50
2024-11-30 00:44:25.887707: train_loss -0.6144
2024-11-30 00:44:25.888730: val_loss -0.4033
2024-11-30 00:44:25.889425: Pseudo dice [0.6474]
2024-11-30 00:44:25.890198: Epoch time: 86.06 s
2024-11-30 00:44:27.149513: 
2024-11-30 00:44:27.151145: Epoch 54
2024-11-30 00:44:27.151886: Current learning rate: 0.00951
2024-11-30 00:45:53.201307: Validation loss did not improve from -0.47287. Patience: 3/50
2024-11-30 00:45:53.202632: train_loss -0.6167
2024-11-30 00:45:53.204175: val_loss -0.4367
2024-11-30 00:45:53.205213: Pseudo dice [0.6725]
2024-11-30 00:45:53.206232: Epoch time: 86.05 s
2024-11-30 00:45:54.854687: 
2024-11-30 00:45:54.856619: Epoch 55
2024-11-30 00:45:54.857717: Current learning rate: 0.0095
2024-11-30 00:47:20.850006: Validation loss did not improve from -0.47287. Patience: 4/50
2024-11-30 00:47:20.851078: train_loss -0.6258
2024-11-30 00:47:20.852145: val_loss -0.4462
2024-11-30 00:47:20.853109: Pseudo dice [0.6853]
2024-11-30 00:47:20.854082: Epoch time: 86.0 s
2024-11-30 00:47:22.109542: 
2024-11-30 00:47:22.111202: Epoch 56
2024-11-30 00:47:22.112020: Current learning rate: 0.00949
2024-11-30 00:48:48.113257: Validation loss did not improve from -0.47287. Patience: 5/50
2024-11-30 00:48:48.114563: train_loss -0.613
2024-11-30 00:48:48.115696: val_loss -0.4486
2024-11-30 00:48:48.116527: Pseudo dice [0.6815]
2024-11-30 00:48:48.117370: Epoch time: 86.01 s
2024-11-30 00:48:48.118172: Yayy! New best EMA pseudo Dice: 0.6721
2024-11-30 00:48:49.740105: 
2024-11-30 00:48:49.741685: Epoch 57
2024-11-30 00:48:49.742424: Current learning rate: 0.00949
2024-11-30 00:50:15.822471: Validation loss did not improve from -0.47287. Patience: 6/50
2024-11-30 00:50:15.823352: train_loss -0.6243
2024-11-30 00:50:15.824365: val_loss -0.4086
2024-11-30 00:50:15.825144: Pseudo dice [0.6582]
2024-11-30 00:50:15.825925: Epoch time: 86.08 s
2024-11-30 00:50:17.082816: 
2024-11-30 00:50:17.084229: Epoch 58
2024-11-30 00:50:17.085730: Current learning rate: 0.00948
2024-11-30 00:51:43.191870: Validation loss did not improve from -0.47287. Patience: 7/50
2024-11-30 00:51:43.193129: train_loss -0.5986
2024-11-30 00:51:43.194230: val_loss -0.3879
2024-11-30 00:51:43.195177: Pseudo dice [0.6357]
2024-11-30 00:51:43.196049: Epoch time: 86.11 s
2024-11-30 00:51:44.426597: 
2024-11-30 00:51:44.428648: Epoch 59
2024-11-30 00:51:44.429880: Current learning rate: 0.00947
2024-11-30 00:53:10.492846: Validation loss did not improve from -0.47287. Patience: 8/50
2024-11-30 00:53:10.493682: train_loss -0.6212
2024-11-30 00:53:10.494809: val_loss -0.4585
2024-11-30 00:53:10.495570: Pseudo dice [0.6795]
2024-11-30 00:53:10.496332: Epoch time: 86.07 s
2024-11-30 00:53:12.493587: 
2024-11-30 00:53:12.495071: Epoch 60
2024-11-30 00:53:12.495863: Current learning rate: 0.00946
2024-11-30 00:54:38.598909: Validation loss did not improve from -0.47287. Patience: 9/50
2024-11-30 00:54:38.599931: train_loss -0.6168
2024-11-30 00:54:38.600996: val_loss -0.4575
2024-11-30 00:54:38.601928: Pseudo dice [0.6916]
2024-11-30 00:54:38.602876: Epoch time: 86.11 s
2024-11-30 00:54:39.872817: 
2024-11-30 00:54:39.874576: Epoch 61
2024-11-30 00:54:39.875473: Current learning rate: 0.00945
2024-11-30 00:56:06.043100: Validation loss did not improve from -0.47287. Patience: 10/50
2024-11-30 00:56:06.044287: train_loss -0.6252
2024-11-30 00:56:06.045274: val_loss -0.4279
2024-11-30 00:56:06.046188: Pseudo dice [0.6714]
2024-11-30 00:56:06.047061: Epoch time: 86.17 s
2024-11-30 00:56:07.348178: 
2024-11-30 00:56:07.350328: Epoch 62
2024-11-30 00:56:07.351173: Current learning rate: 0.00944
2024-11-30 00:57:33.394408: Validation loss did not improve from -0.47287. Patience: 11/50
2024-11-30 00:57:33.395439: train_loss -0.6313
2024-11-30 00:57:33.396269: val_loss -0.4526
2024-11-30 00:57:33.396966: Pseudo dice [0.6908]
2024-11-30 00:57:33.397763: Epoch time: 86.05 s
2024-11-30 00:57:33.398466: Yayy! New best EMA pseudo Dice: 0.6728
2024-11-30 00:57:35.285142: 
2024-11-30 00:57:35.286741: Epoch 63
2024-11-30 00:57:35.287521: Current learning rate: 0.00943
2024-11-30 00:59:01.292744: Validation loss did not improve from -0.47287. Patience: 12/50
2024-11-30 00:59:01.294318: train_loss -0.6372
2024-11-30 00:59:01.295894: val_loss -0.4248
2024-11-30 00:59:01.296942: Pseudo dice [0.6675]
2024-11-30 00:59:01.298001: Epoch time: 86.01 s
2024-11-30 00:59:02.549248: 
2024-11-30 00:59:02.551082: Epoch 64
2024-11-30 00:59:02.551984: Current learning rate: 0.00942
2024-11-30 01:00:28.595504: Validation loss did not improve from -0.47287. Patience: 13/50
2024-11-30 01:00:28.596697: train_loss -0.6381
2024-11-30 01:00:28.597739: val_loss -0.4429
2024-11-30 01:00:28.598489: Pseudo dice [0.6844]
2024-11-30 01:00:28.599335: Epoch time: 86.05 s
2024-11-30 01:00:28.963176: Yayy! New best EMA pseudo Dice: 0.6735
2024-11-30 01:00:30.586995: 
2024-11-30 01:00:30.589262: Epoch 65
2024-11-30 01:00:30.590406: Current learning rate: 0.00941
2024-11-30 01:01:56.614019: Validation loss did not improve from -0.47287. Patience: 14/50
2024-11-30 01:01:56.614859: train_loss -0.6246
2024-11-30 01:01:56.615839: val_loss -0.396
2024-11-30 01:01:56.616534: Pseudo dice [0.6639]
2024-11-30 01:01:56.617310: Epoch time: 86.03 s
2024-11-30 01:01:57.863738: 
2024-11-30 01:01:57.865729: Epoch 66
2024-11-30 01:01:57.866885: Current learning rate: 0.0094
2024-11-30 01:03:23.896547: Validation loss did not improve from -0.47287. Patience: 15/50
2024-11-30 01:03:23.897524: train_loss -0.6401
2024-11-30 01:03:23.898704: val_loss -0.4396
2024-11-30 01:03:23.899602: Pseudo dice [0.6809]
2024-11-30 01:03:23.900571: Epoch time: 86.03 s
2024-11-30 01:03:25.153385: 
2024-11-30 01:03:25.155081: Epoch 67
2024-11-30 01:03:25.156092: Current learning rate: 0.00939
2024-11-30 01:04:51.205642: Validation loss did not improve from -0.47287. Patience: 16/50
2024-11-30 01:04:51.206982: train_loss -0.6482
2024-11-30 01:04:51.208257: val_loss -0.4373
2024-11-30 01:04:51.209409: Pseudo dice [0.6805]
2024-11-30 01:04:51.210596: Epoch time: 86.05 s
2024-11-30 01:04:51.211511: Yayy! New best EMA pseudo Dice: 0.6741
2024-11-30 01:04:52.841526: 
2024-11-30 01:04:52.843029: Epoch 68
2024-11-30 01:04:52.844008: Current learning rate: 0.00939
2024-11-30 01:06:18.965491: Validation loss did not improve from -0.47287. Patience: 17/50
2024-11-30 01:06:18.966805: train_loss -0.644
2024-11-30 01:06:18.967729: val_loss -0.4656
2024-11-30 01:06:18.968456: Pseudo dice [0.6901]
2024-11-30 01:06:18.969150: Epoch time: 86.13 s
2024-11-30 01:06:18.969945: Yayy! New best EMA pseudo Dice: 0.6757
2024-11-30 01:06:20.658936: 
2024-11-30 01:06:20.660748: Epoch 69
2024-11-30 01:06:20.661708: Current learning rate: 0.00938
2024-11-30 01:07:46.698008: Validation loss did not improve from -0.47287. Patience: 18/50
2024-11-30 01:07:46.699250: train_loss -0.6417
2024-11-30 01:07:46.700359: val_loss -0.455
2024-11-30 01:07:46.701109: Pseudo dice [0.6867]
2024-11-30 01:07:46.701924: Epoch time: 86.04 s
2024-11-30 01:07:47.085515: Yayy! New best EMA pseudo Dice: 0.6768
2024-11-30 01:07:48.715367: 
2024-11-30 01:07:48.717302: Epoch 70
2024-11-30 01:07:48.718290: Current learning rate: 0.00937
2024-11-30 01:09:14.766573: Validation loss did not improve from -0.47287. Patience: 19/50
2024-11-30 01:09:14.767839: train_loss -0.6433
2024-11-30 01:09:14.768784: val_loss -0.4463
2024-11-30 01:09:14.769557: Pseudo dice [0.6911]
2024-11-30 01:09:14.770346: Epoch time: 86.05 s
2024-11-30 01:09:14.770973: Yayy! New best EMA pseudo Dice: 0.6782
2024-11-30 01:09:16.789600: 
2024-11-30 01:09:16.791347: Epoch 71
2024-11-30 01:09:16.792262: Current learning rate: 0.00936
2024-11-30 01:10:42.957901: Validation loss did not improve from -0.47287. Patience: 20/50
2024-11-30 01:10:42.959110: train_loss -0.6364
2024-11-30 01:10:42.960140: val_loss -0.4368
2024-11-30 01:10:42.960870: Pseudo dice [0.6837]
2024-11-30 01:10:42.961653: Epoch time: 86.17 s
2024-11-30 01:10:42.962321: Yayy! New best EMA pseudo Dice: 0.6788
2024-11-30 01:10:44.627597: 
2024-11-30 01:10:44.629116: Epoch 72
2024-11-30 01:10:44.629852: Current learning rate: 0.00935
2024-11-30 01:12:10.662075: Validation loss did not improve from -0.47287. Patience: 21/50
2024-11-30 01:12:10.663203: train_loss -0.6402
2024-11-30 01:12:10.664077: val_loss -0.4558
2024-11-30 01:12:10.664772: Pseudo dice [0.6874]
2024-11-30 01:12:10.665542: Epoch time: 86.04 s
2024-11-30 01:12:10.666221: Yayy! New best EMA pseudo Dice: 0.6796
2024-11-30 01:12:12.301250: 
2024-11-30 01:12:12.303038: Epoch 73
2024-11-30 01:12:12.304042: Current learning rate: 0.00934
2024-11-30 01:13:38.316821: Validation loss did not improve from -0.47287. Patience: 22/50
2024-11-30 01:13:38.317851: train_loss -0.6421
2024-11-30 01:13:38.318668: val_loss -0.4487
2024-11-30 01:13:38.319318: Pseudo dice [0.6785]
2024-11-30 01:13:38.319987: Epoch time: 86.02 s
2024-11-30 01:13:39.568621: 
2024-11-30 01:13:39.570279: Epoch 74
2024-11-30 01:13:39.570949: Current learning rate: 0.00933
2024-11-30 01:15:05.640069: Validation loss did not improve from -0.47287. Patience: 23/50
2024-11-30 01:15:05.640929: train_loss -0.6447
2024-11-30 01:15:05.641741: val_loss -0.415
2024-11-30 01:15:05.642512: Pseudo dice [0.6723]
2024-11-30 01:15:05.643167: Epoch time: 86.07 s
2024-11-30 01:15:07.286230: 
2024-11-30 01:15:07.287704: Epoch 75
2024-11-30 01:15:07.288649: Current learning rate: 0.00932
2024-11-30 01:16:33.300785: Validation loss improved from -0.47287 to -0.47372! Patience: 23/50
2024-11-30 01:16:33.302129: train_loss -0.6494
2024-11-30 01:16:33.303328: val_loss -0.4737
2024-11-30 01:16:33.304236: Pseudo dice [0.7028]
2024-11-30 01:16:33.305177: Epoch time: 86.02 s
2024-11-30 01:16:33.306034: Yayy! New best EMA pseudo Dice: 0.6812
2024-11-30 01:16:35.020840: 
2024-11-30 01:16:35.022346: Epoch 76
2024-11-30 01:16:35.023301: Current learning rate: 0.00931
2024-11-30 01:18:01.036070: Validation loss did not improve from -0.47372. Patience: 1/50
2024-11-30 01:18:01.037071: train_loss -0.6414
2024-11-30 01:18:01.038072: val_loss -0.4521
2024-11-30 01:18:01.038829: Pseudo dice [0.682]
2024-11-30 01:18:01.039679: Epoch time: 86.02 s
2024-11-30 01:18:01.040536: Yayy! New best EMA pseudo Dice: 0.6813
2024-11-30 01:18:02.651711: 
2024-11-30 01:18:02.653439: Epoch 77
2024-11-30 01:18:02.654183: Current learning rate: 0.0093
2024-11-30 01:19:28.737678: Validation loss did not improve from -0.47372. Patience: 2/50
2024-11-30 01:19:28.738773: train_loss -0.6527
2024-11-30 01:19:28.740213: val_loss -0.465
2024-11-30 01:19:28.741261: Pseudo dice [0.7058]
2024-11-30 01:19:28.742312: Epoch time: 86.09 s
2024-11-30 01:19:28.743299: Yayy! New best EMA pseudo Dice: 0.6837
2024-11-30 01:19:30.471457: 
2024-11-30 01:19:30.473289: Epoch 78
2024-11-30 01:19:30.474425: Current learning rate: 0.0093
2024-11-30 01:20:56.512972: Validation loss did not improve from -0.47372. Patience: 3/50
2024-11-30 01:20:56.514241: train_loss -0.6492
2024-11-30 01:20:56.515482: val_loss -0.4528
2024-11-30 01:20:56.516133: Pseudo dice [0.6804]
2024-11-30 01:20:56.516906: Epoch time: 86.04 s
2024-11-30 01:20:57.814340: 
2024-11-30 01:20:57.816100: Epoch 79
2024-11-30 01:20:57.816993: Current learning rate: 0.00929
2024-11-30 01:22:23.843729: Validation loss did not improve from -0.47372. Patience: 4/50
2024-11-30 01:22:23.844879: train_loss -0.6515
2024-11-30 01:22:23.846006: val_loss -0.4685
2024-11-30 01:22:23.846682: Pseudo dice [0.6949]
2024-11-30 01:22:23.847370: Epoch time: 86.03 s
2024-11-30 01:22:24.207580: Yayy! New best EMA pseudo Dice: 0.6845
2024-11-30 01:22:25.844237: 
2024-11-30 01:22:25.846232: Epoch 80
2024-11-30 01:22:25.847265: Current learning rate: 0.00928
2024-11-30 01:23:51.935319: Validation loss did not improve from -0.47372. Patience: 5/50
2024-11-30 01:23:51.936347: train_loss -0.6569
2024-11-30 01:23:51.937408: val_loss -0.4044
2024-11-30 01:23:51.938170: Pseudo dice [0.6626]
2024-11-30 01:23:51.938995: Epoch time: 86.09 s
2024-11-30 01:23:53.267353: 
2024-11-30 01:23:53.269293: Epoch 81
2024-11-30 01:23:53.270081: Current learning rate: 0.00927
2024-11-30 01:25:19.638636: Validation loss did not improve from -0.47372. Patience: 6/50
2024-11-30 01:25:19.639685: train_loss -0.6526
2024-11-30 01:25:19.640676: val_loss -0.4373
2024-11-30 01:25:19.641380: Pseudo dice [0.6784]
2024-11-30 01:25:19.642077: Epoch time: 86.37 s
2024-11-30 01:25:20.968137: 
2024-11-30 01:25:20.969662: Epoch 82
2024-11-30 01:25:20.970562: Current learning rate: 0.00926
2024-11-30 01:26:47.031261: Validation loss did not improve from -0.47372. Patience: 7/50
2024-11-30 01:26:47.032347: train_loss -0.6523
2024-11-30 01:26:47.033572: val_loss -0.4137
2024-11-30 01:26:47.034422: Pseudo dice [0.6605]
2024-11-30 01:26:47.035234: Epoch time: 86.07 s
2024-11-30 01:26:48.262561: 
2024-11-30 01:26:48.264022: Epoch 83
2024-11-30 01:26:48.265042: Current learning rate: 0.00925
2024-11-30 01:28:14.306824: Validation loss did not improve from -0.47372. Patience: 8/50
2024-11-30 01:28:14.307965: train_loss -0.6599
2024-11-30 01:28:14.309028: val_loss -0.4373
2024-11-30 01:28:14.309858: Pseudo dice [0.682]
2024-11-30 01:28:14.310857: Epoch time: 86.05 s
2024-11-30 01:28:15.498050: 
2024-11-30 01:28:15.499765: Epoch 84
2024-11-30 01:28:15.500726: Current learning rate: 0.00924
2024-11-30 01:29:41.548495: Validation loss did not improve from -0.47372. Patience: 9/50
2024-11-30 01:29:41.549454: train_loss -0.6682
2024-11-30 01:29:41.550500: val_loss -0.4239
2024-11-30 01:29:41.551185: Pseudo dice [0.6641]
2024-11-30 01:29:41.552092: Epoch time: 86.05 s
2024-11-30 01:29:43.221462: 
2024-11-30 01:29:43.223324: Epoch 85
2024-11-30 01:29:43.224177: Current learning rate: 0.00923
2024-11-30 01:31:09.698231: Validation loss did not improve from -0.47372. Patience: 10/50
2024-11-30 01:31:09.698955: train_loss -0.6549
2024-11-30 01:31:09.699746: val_loss -0.4541
2024-11-30 01:31:09.700516: Pseudo dice [0.683]
2024-11-30 01:31:09.701276: Epoch time: 86.48 s
2024-11-30 01:31:10.950990: 
2024-11-30 01:31:10.952369: Epoch 86
2024-11-30 01:31:10.953102: Current learning rate: 0.00922
2024-11-30 01:32:37.046292: Validation loss did not improve from -0.47372. Patience: 11/50
2024-11-30 01:32:37.047050: train_loss -0.6617
2024-11-30 01:32:37.047870: val_loss -0.4364
2024-11-30 01:32:37.048748: Pseudo dice [0.6701]
2024-11-30 01:32:37.049505: Epoch time: 86.1 s
2024-11-30 01:32:38.284252: 
2024-11-30 01:32:38.286272: Epoch 87
2024-11-30 01:32:38.287370: Current learning rate: 0.00921
2024-11-30 01:34:04.383508: Validation loss did not improve from -0.47372. Patience: 12/50
2024-11-30 01:34:04.384457: train_loss -0.6699
2024-11-30 01:34:04.385288: val_loss -0.4681
2024-11-30 01:34:04.385931: Pseudo dice [0.6877]
2024-11-30 01:34:04.386626: Epoch time: 86.1 s
2024-11-30 01:34:05.622387: 
2024-11-30 01:34:05.623929: Epoch 88
2024-11-30 01:34:05.624842: Current learning rate: 0.0092
2024-11-30 01:35:31.716219: Validation loss improved from -0.47372 to -0.48356! Patience: 12/50
2024-11-30 01:35:31.717860: train_loss -0.6681
2024-11-30 01:35:31.718971: val_loss -0.4836
2024-11-30 01:35:31.719864: Pseudo dice [0.7034]
2024-11-30 01:35:31.720817: Epoch time: 86.1 s
2024-11-30 01:35:32.937228: 
2024-11-30 01:35:32.939142: Epoch 89
2024-11-30 01:35:32.940124: Current learning rate: 0.0092
2024-11-30 01:36:58.901559: Validation loss did not improve from -0.48356. Patience: 1/50
2024-11-30 01:36:58.902671: train_loss -0.6622
2024-11-30 01:36:58.903519: val_loss -0.4559
2024-11-30 01:36:58.904269: Pseudo dice [0.684]
2024-11-30 01:36:58.904982: Epoch time: 85.97 s
2024-11-30 01:37:00.474746: 
2024-11-30 01:37:00.476351: Epoch 90
2024-11-30 01:37:00.477257: Current learning rate: 0.00919
2024-11-30 01:38:26.544694: Validation loss did not improve from -0.48356. Patience: 2/50
2024-11-30 01:38:26.545753: train_loss -0.6781
2024-11-30 01:38:26.546662: val_loss -0.4772
2024-11-30 01:38:26.547382: Pseudo dice [0.7025]
2024-11-30 01:38:26.548358: Epoch time: 86.07 s
2024-11-30 01:38:27.787932: 
2024-11-30 01:38:27.789682: Epoch 91
2024-11-30 01:38:27.790524: Current learning rate: 0.00918
2024-11-30 01:39:53.909964: Validation loss did not improve from -0.48356. Patience: 3/50
2024-11-30 01:39:53.910941: train_loss -0.6702
2024-11-30 01:39:53.911728: val_loss -0.4611
2024-11-30 01:39:53.912470: Pseudo dice [0.6869]
2024-11-30 01:39:53.913281: Epoch time: 86.12 s
2024-11-30 01:39:55.542006: 
2024-11-30 01:39:55.543725: Epoch 92
2024-11-30 01:39:55.544403: Current learning rate: 0.00917
2024-11-30 01:41:21.611706: Validation loss did not improve from -0.48356. Patience: 4/50
2024-11-30 01:41:21.612401: train_loss -0.6688
2024-11-30 01:41:21.613396: val_loss -0.4523
2024-11-30 01:41:21.614239: Pseudo dice [0.6924]
2024-11-30 01:41:21.615016: Epoch time: 86.07 s
2024-11-30 01:41:21.615901: Yayy! New best EMA pseudo Dice: 0.6849
2024-11-30 01:41:23.176995: 
2024-11-30 01:41:23.178892: Epoch 93
2024-11-30 01:41:23.179822: Current learning rate: 0.00916
2024-11-30 01:42:49.498055: Validation loss did not improve from -0.48356. Patience: 5/50
2024-11-30 01:42:49.499114: train_loss -0.6696
2024-11-30 01:42:49.500013: val_loss -0.4016
2024-11-30 01:42:49.500985: Pseudo dice [0.6662]
2024-11-30 01:42:49.501656: Epoch time: 86.32 s
2024-11-30 01:42:50.739672: 
2024-11-30 01:42:50.741275: Epoch 94
2024-11-30 01:42:50.742077: Current learning rate: 0.00915
2024-11-30 01:44:16.714962: Validation loss did not improve from -0.48356. Patience: 6/50
2024-11-30 01:44:16.716192: train_loss -0.6789
2024-11-30 01:44:16.717008: val_loss -0.467
2024-11-30 01:44:16.717754: Pseudo dice [0.6914]
2024-11-30 01:44:16.718564: Epoch time: 85.98 s
2024-11-30 01:44:18.248183: 
2024-11-30 01:44:18.250004: Epoch 95
2024-11-30 01:44:18.250790: Current learning rate: 0.00914
2024-11-30 01:45:44.314718: Validation loss did not improve from -0.48356. Patience: 7/50
2024-11-30 01:45:44.315921: train_loss -0.6639
2024-11-30 01:45:44.316684: val_loss -0.4579
2024-11-30 01:45:44.317332: Pseudo dice [0.7078]
2024-11-30 01:45:44.317988: Epoch time: 86.07 s
2024-11-30 01:45:44.318595: Yayy! New best EMA pseudo Dice: 0.6863
2024-11-30 01:45:45.832034: 
2024-11-30 01:45:45.833562: Epoch 96
2024-11-30 01:45:45.834413: Current learning rate: 0.00913
2024-11-30 01:47:11.869458: Validation loss did not improve from -0.48356. Patience: 8/50
2024-11-30 01:47:11.870632: train_loss -0.6614
2024-11-30 01:47:11.871457: val_loss -0.4784
2024-11-30 01:47:11.872257: Pseudo dice [0.7032]
2024-11-30 01:47:11.873056: Epoch time: 86.04 s
2024-11-30 01:47:11.873821: Yayy! New best EMA pseudo Dice: 0.688
2024-11-30 01:47:13.442467: 
2024-11-30 01:47:13.444381: Epoch 97
2024-11-30 01:47:13.445143: Current learning rate: 0.00912
2024-11-30 01:48:39.489921: Validation loss did not improve from -0.48356. Patience: 9/50
2024-11-30 01:48:39.491028: train_loss -0.6627
2024-11-30 01:48:39.491898: val_loss -0.4646
2024-11-30 01:48:39.492691: Pseudo dice [0.6886]
2024-11-30 01:48:39.493499: Epoch time: 86.05 s
2024-11-30 01:48:39.494251: Yayy! New best EMA pseudo Dice: 0.688
2024-11-30 01:48:41.062249: 
2024-11-30 01:48:41.063780: Epoch 98
2024-11-30 01:48:41.064542: Current learning rate: 0.00911
2024-11-30 01:50:07.196800: Validation loss did not improve from -0.48356. Patience: 10/50
2024-11-30 01:50:07.197941: train_loss -0.6741
2024-11-30 01:50:07.198899: val_loss -0.445
2024-11-30 01:50:07.199656: Pseudo dice [0.6782]
2024-11-30 01:50:07.200482: Epoch time: 86.14 s
2024-11-30 01:50:08.441591: 
2024-11-30 01:50:08.443562: Epoch 99
2024-11-30 01:50:08.444666: Current learning rate: 0.0091
2024-11-30 01:51:34.490652: Validation loss did not improve from -0.48356. Patience: 11/50
2024-11-30 01:51:34.491571: train_loss -0.6669
2024-11-30 01:51:34.492432: val_loss -0.4716
2024-11-30 01:51:34.493278: Pseudo dice [0.7052]
2024-11-30 01:51:34.494071: Epoch time: 86.05 s
2024-11-30 01:51:34.864544: Yayy! New best EMA pseudo Dice: 0.6889
2024-11-30 01:51:36.567103: 
2024-11-30 01:51:36.569127: Epoch 100
2024-11-30 01:51:36.570242: Current learning rate: 0.0091
2024-11-30 01:53:02.641002: Validation loss did not improve from -0.48356. Patience: 12/50
2024-11-30 01:53:02.642380: train_loss -0.6744
2024-11-30 01:53:02.643292: val_loss -0.4708
2024-11-30 01:53:02.644207: Pseudo dice [0.6965]
2024-11-30 01:53:02.645036: Epoch time: 86.08 s
2024-11-30 01:53:02.645721: Yayy! New best EMA pseudo Dice: 0.6896
2024-11-30 01:53:04.245803: 
2024-11-30 01:53:04.247649: Epoch 101
2024-11-30 01:53:04.248368: Current learning rate: 0.00909
2024-11-30 01:54:30.346293: Validation loss did not improve from -0.48356. Patience: 13/50
2024-11-30 01:54:30.347383: train_loss -0.6836
2024-11-30 01:54:30.348339: val_loss -0.4471
2024-11-30 01:54:30.348970: Pseudo dice [0.6824]
2024-11-30 01:54:30.349649: Epoch time: 86.1 s
2024-11-30 01:54:31.534878: 
2024-11-30 01:54:31.536673: Epoch 102
2024-11-30 01:54:31.537506: Current learning rate: 0.00908
2024-11-30 01:55:57.556357: Validation loss did not improve from -0.48356. Patience: 14/50
2024-11-30 01:55:57.557606: train_loss -0.691
2024-11-30 01:55:57.558689: val_loss -0.4571
2024-11-30 01:55:57.559615: Pseudo dice [0.6956]
2024-11-30 01:55:57.560566: Epoch time: 86.02 s
2024-11-30 01:55:58.811289: 
2024-11-30 01:55:58.813096: Epoch 103
2024-11-30 01:55:58.813931: Current learning rate: 0.00907
2024-11-30 01:57:24.847275: Validation loss did not improve from -0.48356. Patience: 15/50
2024-11-30 01:57:24.848646: train_loss -0.6896
2024-11-30 01:57:24.849401: val_loss -0.4672
2024-11-30 01:57:24.850052: Pseudo dice [0.7038]
2024-11-30 01:57:24.850847: Epoch time: 86.04 s
2024-11-30 01:57:24.851565: Yayy! New best EMA pseudo Dice: 0.691
2024-11-30 01:57:26.753222: 
2024-11-30 01:57:26.755110: Epoch 104
2024-11-30 01:57:26.756053: Current learning rate: 0.00906
2024-11-30 01:58:52.786669: Validation loss did not improve from -0.48356. Patience: 16/50
2024-11-30 01:58:52.787503: train_loss -0.6809
2024-11-30 01:58:52.788320: val_loss -0.4482
2024-11-30 01:58:52.789040: Pseudo dice [0.6923]
2024-11-30 01:58:52.789629: Epoch time: 86.04 s
2024-11-30 01:58:53.178985: Yayy! New best EMA pseudo Dice: 0.6911
2024-11-30 01:58:54.697104: 
2024-11-30 01:58:54.698915: Epoch 105
2024-11-30 01:58:54.699816: Current learning rate: 0.00905
2024-11-30 02:00:20.717159: Validation loss did not improve from -0.48356. Patience: 17/50
2024-11-30 02:00:20.718278: train_loss -0.6788
2024-11-30 02:00:20.719002: val_loss -0.422
2024-11-30 02:00:20.719967: Pseudo dice [0.6589]
2024-11-30 02:00:20.720883: Epoch time: 86.02 s
2024-11-30 02:00:21.962982: 
2024-11-30 02:00:21.964502: Epoch 106
2024-11-30 02:00:21.965367: Current learning rate: 0.00904
2024-11-30 02:01:47.966284: Validation loss did not improve from -0.48356. Patience: 18/50
2024-11-30 02:01:47.967504: train_loss -0.6795
2024-11-30 02:01:47.968373: val_loss -0.4352
2024-11-30 02:01:47.969149: Pseudo dice [0.6772]
2024-11-30 02:01:47.969927: Epoch time: 86.01 s
2024-11-30 02:01:49.167473: 
2024-11-30 02:01:49.168963: Epoch 107
2024-11-30 02:01:49.169766: Current learning rate: 0.00903
2024-11-30 02:03:15.177905: Validation loss did not improve from -0.48356. Patience: 19/50
2024-11-30 02:03:15.178945: train_loss -0.6898
2024-11-30 02:03:15.179953: val_loss -0.4599
2024-11-30 02:03:15.180748: Pseudo dice [0.6876]
2024-11-30 02:03:15.181431: Epoch time: 86.01 s
2024-11-30 02:03:16.402328: 
2024-11-30 02:03:16.404272: Epoch 108
2024-11-30 02:03:16.405195: Current learning rate: 0.00902
2024-11-30 02:04:42.553715: Validation loss did not improve from -0.48356. Patience: 20/50
2024-11-30 02:04:42.554858: train_loss -0.6909
2024-11-30 02:04:42.556056: val_loss -0.4514
2024-11-30 02:04:42.557022: Pseudo dice [0.6849]
2024-11-30 02:04:42.557828: Epoch time: 86.15 s
2024-11-30 02:04:43.827917: 
2024-11-30 02:04:43.829938: Epoch 109
2024-11-30 02:04:43.830857: Current learning rate: 0.00901
2024-11-30 02:06:09.811116: Validation loss did not improve from -0.48356. Patience: 21/50
2024-11-30 02:06:09.811979: train_loss -0.6812
2024-11-30 02:06:09.812911: val_loss -0.4485
2024-11-30 02:06:09.813704: Pseudo dice [0.6853]
2024-11-30 02:06:09.814464: Epoch time: 85.99 s
2024-11-30 02:06:11.363240: 
2024-11-30 02:06:11.364840: Epoch 110
2024-11-30 02:06:11.365619: Current learning rate: 0.009
2024-11-30 02:07:37.420182: Validation loss did not improve from -0.48356. Patience: 22/50
2024-11-30 02:07:37.421135: train_loss -0.6837
2024-11-30 02:07:37.421835: val_loss -0.4389
2024-11-30 02:07:37.422613: Pseudo dice [0.6786]
2024-11-30 02:07:37.423297: Epoch time: 86.06 s
2024-11-30 02:07:38.744653: 
2024-11-30 02:07:38.746410: Epoch 111
2024-11-30 02:07:38.747397: Current learning rate: 0.009
2024-11-30 02:09:04.865727: Validation loss did not improve from -0.48356. Patience: 23/50
2024-11-30 02:09:04.867025: train_loss -0.6935
2024-11-30 02:09:04.868260: val_loss -0.4775
2024-11-30 02:09:04.869236: Pseudo dice [0.7094]
2024-11-30 02:09:04.870178: Epoch time: 86.12 s
2024-11-30 02:09:06.091786: 
2024-11-30 02:09:06.093450: Epoch 112
2024-11-30 02:09:06.094375: Current learning rate: 0.00899
2024-11-30 02:10:32.125452: Validation loss did not improve from -0.48356. Patience: 24/50
2024-11-30 02:10:32.126816: train_loss -0.6807
2024-11-30 02:10:32.127856: val_loss -0.4256
2024-11-30 02:10:32.128508: Pseudo dice [0.6714]
2024-11-30 02:10:32.129263: Epoch time: 86.04 s
2024-11-30 02:10:33.333036: 
2024-11-30 02:10:33.334646: Epoch 113
2024-11-30 02:10:33.335559: Current learning rate: 0.00898
2024-11-30 02:11:59.350091: Validation loss did not improve from -0.48356. Patience: 25/50
2024-11-30 02:11:59.350999: train_loss -0.6861
2024-11-30 02:11:59.352052: val_loss -0.4776
2024-11-30 02:11:59.352987: Pseudo dice [0.7074]
2024-11-30 02:11:59.353989: Epoch time: 86.02 s
2024-11-30 02:12:00.964420: 
2024-11-30 02:12:00.965714: Epoch 114
2024-11-30 02:12:00.966715: Current learning rate: 0.00897
2024-11-30 02:13:27.037704: Validation loss did not improve from -0.48356. Patience: 26/50
2024-11-30 02:13:27.038736: train_loss -0.6864
2024-11-30 02:13:27.039623: val_loss -0.4561
2024-11-30 02:13:27.040519: Pseudo dice [0.6862]
2024-11-30 02:13:27.041345: Epoch time: 86.08 s
2024-11-30 02:13:28.658825: 
2024-11-30 02:13:28.660494: Epoch 115
2024-11-30 02:13:28.661303: Current learning rate: 0.00896
2024-11-30 02:14:54.744314: Validation loss improved from -0.48356 to -0.48523! Patience: 26/50
2024-11-30 02:14:54.745232: train_loss -0.6957
2024-11-30 02:14:54.746401: val_loss -0.4852
2024-11-30 02:14:54.747434: Pseudo dice [0.714]
2024-11-30 02:14:54.748248: Epoch time: 86.09 s
2024-11-30 02:14:55.958468: 
2024-11-30 02:14:55.960372: Epoch 116
2024-11-30 02:14:55.961271: Current learning rate: 0.00895
2024-11-30 02:16:22.002576: Validation loss did not improve from -0.48523. Patience: 1/50
2024-11-30 02:16:22.003589: train_loss -0.6912
2024-11-30 02:16:22.004601: val_loss -0.4752
2024-11-30 02:16:22.005301: Pseudo dice [0.6928]
2024-11-30 02:16:22.006032: Epoch time: 86.05 s
2024-11-30 02:16:23.334151: 
2024-11-30 02:16:23.335910: Epoch 117
2024-11-30 02:16:23.336939: Current learning rate: 0.00894
2024-11-30 02:17:49.387981: Validation loss did not improve from -0.48523. Patience: 2/50
2024-11-30 02:17:49.389149: train_loss -0.6888
2024-11-30 02:17:49.390091: val_loss -0.4714
2024-11-30 02:17:49.390986: Pseudo dice [0.7027]
2024-11-30 02:17:49.391775: Epoch time: 86.06 s
2024-11-30 02:17:49.392606: Yayy! New best EMA pseudo Dice: 0.6922
2024-11-30 02:17:51.081179: 
2024-11-30 02:17:51.082596: Epoch 118
2024-11-30 02:17:51.083429: Current learning rate: 0.00893
2024-11-30 02:19:17.202284: Validation loss did not improve from -0.48523. Patience: 3/50
2024-11-30 02:19:17.203029: train_loss -0.6851
2024-11-30 02:19:17.203779: val_loss -0.4205
2024-11-30 02:19:17.204577: Pseudo dice [0.6566]
2024-11-30 02:19:17.205301: Epoch time: 86.12 s
2024-11-30 02:19:18.441789: 
2024-11-30 02:19:18.443874: Epoch 119
2024-11-30 02:19:18.444746: Current learning rate: 0.00892
2024-11-30 02:20:44.518993: Validation loss did not improve from -0.48523. Patience: 4/50
2024-11-30 02:20:44.520365: train_loss -0.6933
2024-11-30 02:20:44.521204: val_loss -0.4566
2024-11-30 02:20:44.521966: Pseudo dice [0.6925]
2024-11-30 02:20:44.522770: Epoch time: 86.08 s
2024-11-30 02:20:46.191802: 
2024-11-30 02:20:46.193359: Epoch 120
2024-11-30 02:20:46.194192: Current learning rate: 0.00891
2024-11-30 02:22:12.238529: Validation loss did not improve from -0.48523. Patience: 5/50
2024-11-30 02:22:12.239683: train_loss -0.7019
2024-11-30 02:22:12.240711: val_loss -0.4685
2024-11-30 02:22:12.241512: Pseudo dice [0.7074]
2024-11-30 02:22:12.242378: Epoch time: 86.05 s
2024-11-30 02:22:13.481944: 
2024-11-30 02:22:13.483532: Epoch 121
2024-11-30 02:22:13.484499: Current learning rate: 0.0089
2024-11-30 02:23:39.531635: Validation loss did not improve from -0.48523. Patience: 6/50
2024-11-30 02:23:39.532711: train_loss -0.6964
2024-11-30 02:23:39.533649: val_loss -0.4453
2024-11-30 02:23:39.534412: Pseudo dice [0.681]
2024-11-30 02:23:39.535074: Epoch time: 86.05 s
2024-11-30 02:23:40.763424: 
2024-11-30 02:23:40.765199: Epoch 122
2024-11-30 02:23:40.766282: Current learning rate: 0.00889
2024-11-30 02:25:06.805472: Validation loss did not improve from -0.48523. Patience: 7/50
2024-11-30 02:25:06.806515: train_loss -0.7009
2024-11-30 02:25:06.807387: val_loss -0.4474
2024-11-30 02:25:06.808164: Pseudo dice [0.6773]
2024-11-30 02:25:06.808910: Epoch time: 86.04 s
2024-11-30 02:25:08.076809: 
2024-11-30 02:25:08.078412: Epoch 123
2024-11-30 02:25:08.079291: Current learning rate: 0.00889
2024-11-30 02:26:34.076883: Validation loss did not improve from -0.48523. Patience: 8/50
2024-11-30 02:26:34.077966: train_loss -0.6913
2024-11-30 02:26:34.078842: val_loss -0.4313
2024-11-30 02:26:34.079494: Pseudo dice [0.6654]
2024-11-30 02:26:34.080218: Epoch time: 86.0 s
2024-11-30 02:26:35.319217: 
2024-11-30 02:26:35.320985: Epoch 124
2024-11-30 02:26:35.321841: Current learning rate: 0.00888
2024-11-30 02:28:01.333513: Validation loss did not improve from -0.48523. Patience: 9/50
2024-11-30 02:28:01.334799: train_loss -0.6985
2024-11-30 02:28:01.335678: val_loss -0.4749
2024-11-30 02:28:01.336436: Pseudo dice [0.6955]
2024-11-30 02:28:01.337132: Epoch time: 86.02 s
2024-11-30 02:28:03.266774: 
2024-11-30 02:28:03.268415: Epoch 125
2024-11-30 02:28:03.269168: Current learning rate: 0.00887
2024-11-30 02:29:29.371701: Validation loss did not improve from -0.48523. Patience: 10/50
2024-11-30 02:29:29.372983: train_loss -0.697
2024-11-30 02:29:29.373872: val_loss -0.4795
2024-11-30 02:29:29.374683: Pseudo dice [0.6959]
2024-11-30 02:29:29.375513: Epoch time: 86.11 s
2024-11-30 02:29:30.638449: 
2024-11-30 02:29:30.639879: Epoch 126
2024-11-30 02:29:30.640684: Current learning rate: 0.00886
2024-11-30 02:30:56.651961: Validation loss did not improve from -0.48523. Patience: 11/50
2024-11-30 02:30:56.653024: train_loss -0.7008
2024-11-30 02:30:56.654030: val_loss -0.3941
2024-11-30 02:30:56.654986: Pseudo dice [0.6372]
2024-11-30 02:30:56.655878: Epoch time: 86.02 s
2024-11-30 02:30:57.924458: 
2024-11-30 02:30:57.926287: Epoch 127
2024-11-30 02:30:57.927366: Current learning rate: 0.00885
2024-11-30 02:32:23.970435: Validation loss improved from -0.48523 to -0.49034! Patience: 11/50
2024-11-30 02:32:23.971451: train_loss -0.6963
2024-11-30 02:32:23.972494: val_loss -0.4903
2024-11-30 02:32:23.973163: Pseudo dice [0.7049]
2024-11-30 02:32:23.973806: Epoch time: 86.05 s
2024-11-30 02:32:25.209977: 
2024-11-30 02:32:25.211447: Epoch 128
2024-11-30 02:32:25.212206: Current learning rate: 0.00884
2024-11-30 02:33:51.366728: Validation loss did not improve from -0.49034. Patience: 1/50
2024-11-30 02:33:51.367969: train_loss -0.6973
2024-11-30 02:33:51.369104: val_loss -0.485
2024-11-30 02:33:51.370009: Pseudo dice [0.6977]
2024-11-30 02:33:51.370692: Epoch time: 86.16 s
2024-11-30 02:33:52.645130: 
2024-11-30 02:33:52.646969: Epoch 129
2024-11-30 02:33:52.647814: Current learning rate: 0.00883
2024-11-30 02:35:18.686477: Validation loss did not improve from -0.49034. Patience: 2/50
2024-11-30 02:35:18.687745: train_loss -0.7055
2024-11-30 02:35:18.688742: val_loss -0.4758
2024-11-30 02:35:18.689534: Pseudo dice [0.6906]
2024-11-30 02:35:18.690499: Epoch time: 86.04 s
2024-11-30 02:35:20.413923: 
2024-11-30 02:35:20.415728: Epoch 130
2024-11-30 02:35:20.416522: Current learning rate: 0.00882
2024-11-30 02:36:46.731917: Validation loss did not improve from -0.49034. Patience: 3/50
2024-11-30 02:36:46.733047: train_loss -0.7015
2024-11-30 02:36:46.733879: val_loss -0.4761
2024-11-30 02:36:46.734595: Pseudo dice [0.706]
2024-11-30 02:36:46.735408: Epoch time: 86.32 s
2024-11-30 02:36:47.947751: 
2024-11-30 02:36:47.949527: Epoch 131
2024-11-30 02:36:47.950449: Current learning rate: 0.00881
2024-11-30 02:38:14.021323: Validation loss did not improve from -0.49034. Patience: 4/50
2024-11-30 02:38:14.022446: train_loss -0.6992
2024-11-30 02:38:14.023581: val_loss -0.4821
2024-11-30 02:38:14.024378: Pseudo dice [0.7095]
2024-11-30 02:38:14.025084: Epoch time: 86.08 s
2024-11-30 02:38:15.315578: 
2024-11-30 02:38:15.317436: Epoch 132
2024-11-30 02:38:15.318161: Current learning rate: 0.0088
2024-11-30 02:39:41.437297: Validation loss did not improve from -0.49034. Patience: 5/50
2024-11-30 02:39:41.438464: train_loss -0.7032
2024-11-30 02:39:41.439238: val_loss -0.426
2024-11-30 02:39:41.439983: Pseudo dice [0.6854]
2024-11-30 02:39:41.440660: Epoch time: 86.12 s
2024-11-30 02:39:42.733865: 
2024-11-30 02:39:42.736234: Epoch 133
2024-11-30 02:39:42.737175: Current learning rate: 0.00879
2024-11-30 02:41:08.719734: Validation loss did not improve from -0.49034. Patience: 6/50
2024-11-30 02:41:08.720727: train_loss -0.712
2024-11-30 02:41:08.721567: val_loss -0.481
2024-11-30 02:41:08.722234: Pseudo dice [0.7026]
2024-11-30 02:41:08.722995: Epoch time: 85.99 s
2024-11-30 02:41:09.994978: 
2024-11-30 02:41:09.996681: Epoch 134
2024-11-30 02:41:09.997366: Current learning rate: 0.00879
2024-11-30 02:42:36.036541: Validation loss did not improve from -0.49034. Patience: 7/50
2024-11-30 02:42:36.037523: train_loss -0.7006
2024-11-30 02:42:36.038282: val_loss -0.4044
2024-11-30 02:42:36.038972: Pseudo dice [0.6678]
2024-11-30 02:42:36.039673: Epoch time: 86.04 s
2024-11-30 02:42:37.623863: 
2024-11-30 02:42:37.625648: Epoch 135
2024-11-30 02:42:37.626436: Current learning rate: 0.00878
2024-11-30 02:44:03.749387: Validation loss did not improve from -0.49034. Patience: 8/50
2024-11-30 02:44:03.750450: train_loss -0.7028
2024-11-30 02:44:03.751291: val_loss -0.428
2024-11-30 02:44:03.752071: Pseudo dice [0.6872]
2024-11-30 02:44:03.752819: Epoch time: 86.13 s
2024-11-30 02:44:05.640195: 
2024-11-30 02:44:05.641582: Epoch 136
2024-11-30 02:44:05.642334: Current learning rate: 0.00877
2024-11-30 02:45:31.738559: Validation loss did not improve from -0.49034. Patience: 9/50
2024-11-30 02:45:31.739568: train_loss -0.7014
2024-11-30 02:45:31.740546: val_loss -0.4549
2024-11-30 02:45:31.741398: Pseudo dice [0.6829]
2024-11-30 02:45:31.742303: Epoch time: 86.1 s
2024-11-30 02:45:33.043772: 
2024-11-30 02:45:33.045577: Epoch 137
2024-11-30 02:45:33.046460: Current learning rate: 0.00876
2024-11-30 02:46:59.155379: Validation loss did not improve from -0.49034. Patience: 10/50
2024-11-30 02:46:59.156593: train_loss -0.7066
2024-11-30 02:46:59.157867: val_loss -0.4688
2024-11-30 02:46:59.158829: Pseudo dice [0.6935]
2024-11-30 02:46:59.159591: Epoch time: 86.11 s
2024-11-30 02:47:00.438339: 
2024-11-30 02:47:00.440215: Epoch 138
2024-11-30 02:47:00.441219: Current learning rate: 0.00875
2024-11-30 02:48:26.557445: Validation loss did not improve from -0.49034. Patience: 11/50
2024-11-30 02:48:26.558367: train_loss -0.7075
2024-11-30 02:48:26.559348: val_loss -0.4318
2024-11-30 02:48:26.560198: Pseudo dice [0.6763]
2024-11-30 02:48:26.560948: Epoch time: 86.12 s
2024-11-30 02:48:27.859093: 
2024-11-30 02:48:27.860205: Epoch 139
2024-11-30 02:48:27.861051: Current learning rate: 0.00874
2024-11-30 02:49:53.865631: Validation loss did not improve from -0.49034. Patience: 12/50
2024-11-30 02:49:53.866849: train_loss -0.7163
2024-11-30 02:49:53.867744: val_loss -0.4296
2024-11-30 02:49:53.868518: Pseudo dice [0.692]
2024-11-30 02:49:53.869317: Epoch time: 86.01 s
2024-11-30 02:49:55.501321: 
2024-11-30 02:49:55.502609: Epoch 140
2024-11-30 02:49:55.503648: Current learning rate: 0.00873
2024-11-30 02:51:21.467007: Validation loss improved from -0.49034 to -0.51014! Patience: 12/50
2024-11-30 02:51:21.468215: train_loss -0.7131
2024-11-30 02:51:21.469222: val_loss -0.5101
2024-11-30 02:51:21.470363: Pseudo dice [0.7144]
2024-11-30 02:51:21.471256: Epoch time: 85.97 s
2024-11-30 02:51:22.784742: 
2024-11-30 02:51:22.786565: Epoch 141
2024-11-30 02:51:22.787459: Current learning rate: 0.00872
2024-11-30 02:52:48.815487: Validation loss did not improve from -0.51014. Patience: 1/50
2024-11-30 02:52:48.816630: train_loss -0.7226
2024-11-30 02:52:48.817943: val_loss -0.4865
2024-11-30 02:52:48.819094: Pseudo dice [0.7112]
2024-11-30 02:52:48.820027: Epoch time: 86.03 s
2024-11-30 02:52:48.820863: Yayy! New best EMA pseudo Dice: 0.6927
2024-11-30 02:52:50.415469: 
2024-11-30 02:52:50.417784: Epoch 142
2024-11-30 02:52:50.418988: Current learning rate: 0.00871
2024-11-30 02:54:16.432935: Validation loss did not improve from -0.51014. Patience: 2/50
2024-11-30 02:54:16.433928: train_loss -0.7117
2024-11-30 02:54:16.434747: val_loss -0.479
2024-11-30 02:54:16.435577: Pseudo dice [0.7087]
2024-11-30 02:54:16.436366: Epoch time: 86.02 s
2024-11-30 02:54:16.437133: Yayy! New best EMA pseudo Dice: 0.6943
2024-11-30 02:54:18.108104: 
2024-11-30 02:54:18.109735: Epoch 143
2024-11-30 02:54:18.110589: Current learning rate: 0.0087
2024-11-30 02:55:44.088829: Validation loss did not improve from -0.51014. Patience: 3/50
2024-11-30 02:55:44.090079: train_loss -0.7115
2024-11-30 02:55:44.091354: val_loss -0.479
2024-11-30 02:55:44.092422: Pseudo dice [0.7174]
2024-11-30 02:55:44.093405: Epoch time: 85.98 s
2024-11-30 02:55:44.094367: Yayy! New best EMA pseudo Dice: 0.6966
2024-11-30 02:55:45.678244: 
2024-11-30 02:55:45.679429: Epoch 144
2024-11-30 02:55:45.680589: Current learning rate: 0.00869
2024-11-30 02:57:11.673696: Validation loss did not improve from -0.51014. Patience: 4/50
2024-11-30 02:57:11.674717: train_loss -0.7023
2024-11-30 02:57:11.676026: val_loss -0.4012
2024-11-30 02:57:11.676921: Pseudo dice [0.6545]
2024-11-30 02:57:11.677707: Epoch time: 86.0 s
2024-11-30 02:57:13.254991: 
2024-11-30 02:57:13.257070: Epoch 145
2024-11-30 02:57:13.258291: Current learning rate: 0.00868
2024-11-30 02:58:39.436058: Validation loss did not improve from -0.51014. Patience: 5/50
2024-11-30 02:58:39.437036: train_loss -0.698
2024-11-30 02:58:39.438098: val_loss -0.4451
2024-11-30 02:58:39.439181: Pseudo dice [0.6744]
2024-11-30 02:58:39.440145: Epoch time: 86.18 s
2024-11-30 02:58:40.801819: 
2024-11-30 02:58:40.802673: Epoch 146
2024-11-30 02:58:40.803503: Current learning rate: 0.00868
2024-11-30 03:00:38.109184: Validation loss did not improve from -0.51014. Patience: 6/50
2024-11-30 03:00:38.110386: train_loss -0.7108
2024-11-30 03:00:38.111434: val_loss -0.4609
2024-11-30 03:00:38.112175: Pseudo dice [0.7003]
2024-11-30 03:00:38.112848: Epoch time: 117.31 s
2024-11-30 03:00:39.724384: 
2024-11-30 03:00:39.726123: Epoch 147
2024-11-30 03:00:39.726990: Current learning rate: 0.00867
2024-11-30 03:03:30.776454: Validation loss did not improve from -0.51014. Patience: 7/50
2024-11-30 03:03:30.777574: train_loss -0.7142
2024-11-30 03:03:30.778360: val_loss -0.4336
2024-11-30 03:03:30.779112: Pseudo dice [0.6701]
2024-11-30 03:03:30.779840: Epoch time: 171.05 s
2024-11-30 03:03:32.074653: 
2024-11-30 03:03:32.076361: Epoch 148
2024-11-30 03:03:32.077122: Current learning rate: 0.00866
2024-11-30 03:06:33.925136: Validation loss did not improve from -0.51014. Patience: 8/50
2024-11-30 03:06:33.926406: train_loss -0.7143
2024-11-30 03:06:33.927358: val_loss -0.4914
2024-11-30 03:06:33.928151: Pseudo dice [0.7033]
2024-11-30 03:06:33.928839: Epoch time: 181.85 s
2024-11-30 03:06:35.168149: 
2024-11-30 03:06:35.169759: Epoch 149
2024-11-30 03:06:35.170469: Current learning rate: 0.00865
2024-11-30 03:09:36.731490: Validation loss did not improve from -0.51014. Patience: 9/50
2024-11-30 03:09:36.732893: train_loss -0.7162
2024-11-30 03:09:36.734335: val_loss -0.3968
2024-11-30 03:09:36.735303: Pseudo dice [0.662]
2024-11-30 03:09:36.736341: Epoch time: 181.57 s
2024-11-30 03:09:38.400663: 
2024-11-30 03:09:38.402089: Epoch 150
2024-11-30 03:09:38.402958: Current learning rate: 0.00864
2024-11-30 03:12:39.471404: Validation loss did not improve from -0.51014. Patience: 10/50
2024-11-30 03:12:39.472553: train_loss -0.7172
2024-11-30 03:12:39.473773: val_loss -0.4206
2024-11-30 03:12:39.474522: Pseudo dice [0.6642]
2024-11-30 03:12:39.475369: Epoch time: 181.07 s
2024-11-30 03:12:40.750900: 
2024-11-30 03:12:40.752556: Epoch 151
2024-11-30 03:12:40.753442: Current learning rate: 0.00863
2024-11-30 03:15:41.405636: Validation loss did not improve from -0.51014. Patience: 11/50
2024-11-30 03:15:41.406857: train_loss -0.706
2024-11-30 03:15:41.407765: val_loss -0.4551
2024-11-30 03:15:41.408671: Pseudo dice [0.6902]
2024-11-30 03:15:41.409502: Epoch time: 180.66 s
2024-11-30 03:15:42.665178: 
2024-11-30 03:15:42.666476: Epoch 152
2024-11-30 03:15:42.667433: Current learning rate: 0.00862
2024-11-30 03:18:44.920182: Validation loss did not improve from -0.51014. Patience: 12/50
2024-11-30 03:18:44.921361: train_loss -0.7107
2024-11-30 03:18:44.922374: val_loss -0.447
2024-11-30 03:18:44.923333: Pseudo dice [0.6823]
2024-11-30 03:18:44.924385: Epoch time: 182.26 s
2024-11-30 03:18:46.221482: 
2024-11-30 03:18:46.222997: Epoch 153
2024-11-30 03:18:46.223990: Current learning rate: 0.00861
2024-11-30 03:21:49.307774: Validation loss did not improve from -0.51014. Patience: 13/50
2024-11-30 03:21:49.308745: train_loss -0.7116
2024-11-30 03:21:49.309641: val_loss -0.4304
2024-11-30 03:21:49.310481: Pseudo dice [0.6697]
2024-11-30 03:21:49.311190: Epoch time: 183.09 s
2024-11-30 03:21:50.598329: 
2024-11-30 03:21:50.599583: Epoch 154
2024-11-30 03:21:50.600411: Current learning rate: 0.0086
2024-11-30 03:24:52.035427: Validation loss did not improve from -0.51014. Patience: 14/50
2024-11-30 03:24:52.036600: train_loss -0.7083
2024-11-30 03:24:52.037694: val_loss -0.4263
2024-11-30 03:24:52.038389: Pseudo dice [0.6744]
2024-11-30 03:24:52.039323: Epoch time: 181.44 s
2024-11-30 03:24:53.755841: 
2024-11-30 03:24:53.757255: Epoch 155
2024-11-30 03:24:53.757973: Current learning rate: 0.00859
2024-11-30 03:27:55.288171: Validation loss did not improve from -0.51014. Patience: 15/50
2024-11-30 03:27:55.289356: train_loss -0.7081
2024-11-30 03:27:55.290193: val_loss -0.4747
2024-11-30 03:27:55.290855: Pseudo dice [0.6984]
2024-11-30 03:27:55.291683: Epoch time: 181.53 s
2024-11-30 03:27:56.663220: 
2024-11-30 03:27:56.664300: Epoch 156
2024-11-30 03:27:56.665049: Current learning rate: 0.00858
2024-11-30 03:30:57.743870: Validation loss did not improve from -0.51014. Patience: 16/50
2024-11-30 03:30:57.745088: train_loss -0.7147
2024-11-30 03:30:57.746080: val_loss -0.4513
2024-11-30 03:30:57.746895: Pseudo dice [0.69]
2024-11-30 03:30:57.747700: Epoch time: 181.08 s
2024-11-30 03:30:59.417969: 
2024-11-30 03:30:59.419711: Epoch 157
2024-11-30 03:30:59.420668: Current learning rate: 0.00858
2024-11-30 03:34:00.377518: Validation loss did not improve from -0.51014. Patience: 17/50
2024-11-30 03:34:00.378768: train_loss -0.7198
2024-11-30 03:34:00.379682: val_loss -0.4741
2024-11-30 03:34:00.380407: Pseudo dice [0.6971]
2024-11-30 03:34:00.381138: Epoch time: 180.96 s
2024-11-30 03:34:01.663049: 
2024-11-30 03:34:01.664608: Epoch 158
2024-11-30 03:34:01.665464: Current learning rate: 0.00857
2024-11-30 03:37:03.059615: Validation loss did not improve from -0.51014. Patience: 18/50
2024-11-30 03:37:03.060807: train_loss -0.7197
2024-11-30 03:37:03.061947: val_loss -0.4809
2024-11-30 03:37:03.062916: Pseudo dice [0.7177]
2024-11-30 03:37:03.063949: Epoch time: 181.4 s
2024-11-30 03:37:04.494370: 
2024-11-30 03:37:04.496262: Epoch 159
2024-11-30 03:37:04.497173: Current learning rate: 0.00856
2024-11-30 03:40:05.947085: Validation loss did not improve from -0.51014. Patience: 19/50
2024-11-30 03:40:05.948350: train_loss -0.7162
2024-11-30 03:40:05.949637: val_loss -0.4698
2024-11-30 03:40:05.950561: Pseudo dice [0.6956]
2024-11-30 03:40:05.951543: Epoch time: 181.46 s
2024-11-30 03:40:07.822925: 
2024-11-30 03:40:07.825000: Epoch 160
2024-11-30 03:40:07.826136: Current learning rate: 0.00855
2024-11-30 03:43:09.471655: Validation loss did not improve from -0.51014. Patience: 20/50
2024-11-30 03:43:09.472953: train_loss -0.7186
2024-11-30 03:43:09.474151: val_loss -0.4627
2024-11-30 03:43:09.475198: Pseudo dice [0.6917]
2024-11-30 03:43:09.476176: Epoch time: 181.65 s
2024-11-30 03:43:10.834722: 
2024-11-30 03:43:10.836593: Epoch 161
2024-11-30 03:43:10.837419: Current learning rate: 0.00854
2024-11-30 03:46:12.168736: Validation loss did not improve from -0.51014. Patience: 21/50
2024-11-30 03:46:12.171826: train_loss -0.7223
2024-11-30 03:46:12.173573: val_loss -0.485
2024-11-30 03:46:12.174624: Pseudo dice [0.7072]
2024-11-30 03:46:12.175642: Epoch time: 181.34 s
2024-11-30 03:46:13.503030: 
2024-11-30 03:46:13.504750: Epoch 162
2024-11-30 03:46:13.505850: Current learning rate: 0.00853
2024-11-30 03:49:14.122865: Validation loss did not improve from -0.51014. Patience: 22/50
2024-11-30 03:49:14.124127: train_loss -0.7226
2024-11-30 03:49:14.124974: val_loss -0.4246
2024-11-30 03:49:14.125626: Pseudo dice [0.6607]
2024-11-30 03:49:14.126442: Epoch time: 180.62 s
2024-11-30 03:49:15.401719: 
2024-11-30 03:49:15.403418: Epoch 163
2024-11-30 03:49:15.404201: Current learning rate: 0.00852
2024-11-30 03:52:16.708912: Validation loss did not improve from -0.51014. Patience: 23/50
2024-11-30 03:52:16.710032: train_loss -0.7267
2024-11-30 03:52:16.711207: val_loss -0.4642
2024-11-30 03:52:16.712226: Pseudo dice [0.6966]
2024-11-30 03:52:16.713106: Epoch time: 181.31 s
2024-11-30 03:52:18.025385: 
2024-11-30 03:52:18.027086: Epoch 164
2024-11-30 03:52:18.028035: Current learning rate: 0.00851
2024-11-30 03:55:19.332999: Validation loss did not improve from -0.51014. Patience: 24/50
2024-11-30 03:55:19.334254: train_loss -0.7359
2024-11-30 03:55:19.335207: val_loss -0.4499
2024-11-30 03:55:19.336019: Pseudo dice [0.688]
2024-11-30 03:55:19.336902: Epoch time: 181.31 s
2024-11-30 03:55:20.970934: 
2024-11-30 03:55:20.971944: Epoch 165
2024-11-30 03:55:20.972783: Current learning rate: 0.0085
2024-11-30 03:58:22.179483: Validation loss did not improve from -0.51014. Patience: 25/50
2024-11-30 03:58:22.181098: train_loss -0.7266
2024-11-30 03:58:22.182325: val_loss -0.4872
2024-11-30 03:58:22.183443: Pseudo dice [0.7176]
2024-11-30 03:58:22.184463: Epoch time: 181.21 s
2024-11-30 03:58:23.424820: 
2024-11-30 03:58:23.426559: Epoch 166
2024-11-30 03:58:23.427392: Current learning rate: 0.00849
2024-11-30 04:01:24.542161: Validation loss did not improve from -0.51014. Patience: 26/50
2024-11-30 04:01:24.543529: train_loss -0.7239
2024-11-30 04:01:24.544897: val_loss -0.4322
2024-11-30 04:01:24.545876: Pseudo dice [0.6751]
2024-11-30 04:01:24.546921: Epoch time: 181.12 s
2024-11-30 04:01:26.218930: 
2024-11-30 04:01:26.220858: Epoch 167
2024-11-30 04:01:26.221728: Current learning rate: 0.00848
2024-11-30 04:04:26.686324: Validation loss did not improve from -0.51014. Patience: 27/50
2024-11-30 04:04:26.689432: train_loss -0.7219
2024-11-30 04:04:26.692626: val_loss -0.4278
2024-11-30 04:04:26.693709: Pseudo dice [0.6797]
2024-11-30 04:04:26.695530: Epoch time: 180.47 s
2024-11-30 04:04:28.053705: 
2024-11-30 04:04:28.055573: Epoch 168
2024-11-30 04:04:28.056444: Current learning rate: 0.00847
2024-11-30 04:07:29.108621: Validation loss did not improve from -0.51014. Patience: 28/50
2024-11-30 04:07:29.109901: train_loss -0.73
2024-11-30 04:07:29.111368: val_loss -0.4555
2024-11-30 04:07:29.112427: Pseudo dice [0.6843]
2024-11-30 04:07:29.113403: Epoch time: 181.06 s
2024-11-30 04:07:30.361251: 
2024-11-30 04:07:30.363153: Epoch 169
2024-11-30 04:07:30.364458: Current learning rate: 0.00847
2024-11-30 04:10:31.602273: Validation loss did not improve from -0.51014. Patience: 29/50
2024-11-30 04:10:31.603619: train_loss -0.719
2024-11-30 04:10:31.604588: val_loss -0.4544
2024-11-30 04:10:31.605337: Pseudo dice [0.6884]
2024-11-30 04:10:31.606127: Epoch time: 181.24 s
2024-11-30 04:10:33.255123: 
2024-11-30 04:10:33.256702: Epoch 170
2024-11-30 04:10:33.257584: Current learning rate: 0.00846
2024-11-30 04:13:34.537967: Validation loss did not improve from -0.51014. Patience: 30/50
2024-11-30 04:13:34.539549: train_loss -0.7255
2024-11-30 04:13:34.540720: val_loss -0.4869
2024-11-30 04:13:34.541686: Pseudo dice [0.7012]
2024-11-30 04:13:34.542583: Epoch time: 181.29 s
2024-11-30 04:13:35.944719: 
2024-11-30 04:13:35.946337: Epoch 171
2024-11-30 04:13:35.947330: Current learning rate: 0.00845
2024-11-30 04:16:37.216051: Validation loss did not improve from -0.51014. Patience: 31/50
2024-11-30 04:16:37.218131: train_loss -0.7157
2024-11-30 04:16:37.219166: val_loss -0.44
2024-11-30 04:16:37.219864: Pseudo dice [0.674]
2024-11-30 04:16:37.220558: Epoch time: 181.27 s
2024-11-30 04:16:38.610950: 
2024-11-30 04:16:38.612787: Epoch 172
2024-11-30 04:16:38.613617: Current learning rate: 0.00844
2024-11-30 04:19:39.251076: Validation loss did not improve from -0.51014. Patience: 32/50
2024-11-30 04:19:39.252198: train_loss -0.7218
2024-11-30 04:19:39.252976: val_loss -0.4409
2024-11-30 04:19:39.253629: Pseudo dice [0.6828]
2024-11-30 04:19:39.254262: Epoch time: 180.64 s
2024-11-30 04:19:40.632467: 
2024-11-30 04:19:40.634176: Epoch 173
2024-11-30 04:19:40.634889: Current learning rate: 0.00843
2024-11-30 04:22:41.723577: Validation loss did not improve from -0.51014. Patience: 33/50
2024-11-30 04:22:41.724788: train_loss -0.7299
2024-11-30 04:22:41.725699: val_loss -0.4052
2024-11-30 04:22:41.726484: Pseudo dice [0.6587]
2024-11-30 04:22:41.727224: Epoch time: 181.09 s
2024-11-30 04:22:43.035351: 
2024-11-30 04:22:43.037082: Epoch 174
2024-11-30 04:22:43.037917: Current learning rate: 0.00842
2024-11-30 04:25:44.165454: Validation loss did not improve from -0.51014. Patience: 34/50
2024-11-30 04:25:44.168011: train_loss -0.7195
2024-11-30 04:25:44.169759: val_loss -0.4753
2024-11-30 04:25:44.170742: Pseudo dice [0.7092]
2024-11-30 04:25:44.171585: Epoch time: 181.13 s
2024-11-30 04:25:45.849803: 
2024-11-30 04:25:45.850982: Epoch 175
2024-11-30 04:25:45.851788: Current learning rate: 0.00841
2024-11-30 04:28:47.053430: Validation loss did not improve from -0.51014. Patience: 35/50
2024-11-30 04:28:47.056664: train_loss -0.7137
2024-11-30 04:28:47.058404: val_loss -0.4609
2024-11-30 04:28:47.059137: Pseudo dice [0.6881]
2024-11-30 04:28:47.060076: Epoch time: 181.21 s
2024-11-30 04:28:48.361611: 
2024-11-30 04:28:48.363364: Epoch 176
2024-11-30 04:28:48.364262: Current learning rate: 0.0084
2024-11-30 04:31:50.018954: Validation loss did not improve from -0.51014. Patience: 36/50
2024-11-30 04:31:50.020406: train_loss -0.7294
2024-11-30 04:31:50.021590: val_loss -0.4772
2024-11-30 04:31:50.022353: Pseudo dice [0.7047]
2024-11-30 04:31:50.023145: Epoch time: 181.66 s
2024-11-30 04:31:51.799938: 
2024-11-30 04:31:51.801222: Epoch 177
2024-11-30 04:31:51.801958: Current learning rate: 0.00839
2024-11-30 04:34:52.170881: Validation loss did not improve from -0.51014. Patience: 37/50
2024-11-30 04:34:52.172116: train_loss -0.729
2024-11-30 04:34:52.173434: val_loss -0.4248
2024-11-30 04:34:52.174471: Pseudo dice [0.6688]
2024-11-30 04:34:52.175375: Epoch time: 180.37 s
2024-11-30 04:34:53.481932: 
2024-11-30 04:34:53.483865: Epoch 178
2024-11-30 04:34:53.484701: Current learning rate: 0.00838
2024-11-30 04:37:54.564246: Validation loss did not improve from -0.51014. Patience: 38/50
2024-11-30 04:37:54.565399: train_loss -0.7317
2024-11-30 04:37:54.566296: val_loss -0.4398
2024-11-30 04:37:54.566885: Pseudo dice [0.6817]
2024-11-30 04:37:54.567435: Epoch time: 181.08 s
2024-11-30 04:37:55.881770: 
2024-11-30 04:37:55.883316: Epoch 179
2024-11-30 04:37:55.884190: Current learning rate: 0.00837
2024-11-30 04:40:56.984254: Validation loss did not improve from -0.51014. Patience: 39/50
2024-11-30 04:40:56.985421: train_loss -0.7331
2024-11-30 04:40:56.986406: val_loss -0.4661
2024-11-30 04:40:56.987221: Pseudo dice [0.6933]
2024-11-30 04:40:56.988046: Epoch time: 181.1 s
2024-11-30 04:40:58.733982: 
2024-11-30 04:40:58.735666: Epoch 180
2024-11-30 04:40:58.736850: Current learning rate: 0.00836
2024-11-30 04:44:00.341239: Validation loss did not improve from -0.51014. Patience: 40/50
2024-11-30 04:44:00.342578: train_loss -0.7334
2024-11-30 04:44:00.343534: val_loss -0.4866
2024-11-30 04:44:00.344329: Pseudo dice [0.7126]
2024-11-30 04:44:00.345285: Epoch time: 181.61 s
2024-11-30 04:44:01.697003: 
2024-11-30 04:44:01.698947: Epoch 181
2024-11-30 04:44:01.699915: Current learning rate: 0.00836
2024-11-30 04:47:03.351032: Validation loss did not improve from -0.51014. Patience: 41/50
2024-11-30 04:47:03.352648: train_loss -0.7319
2024-11-30 04:47:03.353502: val_loss -0.455
2024-11-30 04:47:03.354273: Pseudo dice [0.6869]
2024-11-30 04:47:03.355123: Epoch time: 181.66 s
2024-11-30 04:47:04.706480: 
2024-11-30 04:47:04.708331: Epoch 182
2024-11-30 04:47:04.709082: Current learning rate: 0.00835
2024-11-30 04:50:05.419605: Validation loss did not improve from -0.51014. Patience: 42/50
2024-11-30 04:50:05.420915: train_loss -0.7259
2024-11-30 04:50:05.421855: val_loss -0.4833
2024-11-30 04:50:05.422733: Pseudo dice [0.7125]
2024-11-30 04:50:05.423512: Epoch time: 180.72 s
2024-11-30 04:50:06.744159: 
2024-11-30 04:50:06.745967: Epoch 183
2024-11-30 04:50:06.746801: Current learning rate: 0.00834
2024-11-30 04:53:08.043756: Validation loss did not improve from -0.51014. Patience: 43/50
2024-11-30 04:53:08.045150: train_loss -0.7272
2024-11-30 04:53:08.046058: val_loss -0.4403
2024-11-30 04:53:08.046675: Pseudo dice [0.6806]
2024-11-30 04:53:08.047308: Epoch time: 181.3 s
2024-11-30 04:53:09.371893: 
2024-11-30 04:53:09.373031: Epoch 184
2024-11-30 04:53:09.373809: Current learning rate: 0.00833
2024-11-30 04:56:10.646032: Validation loss did not improve from -0.51014. Patience: 44/50
2024-11-30 04:56:10.647671: train_loss -0.7235
2024-11-30 04:56:10.649173: val_loss -0.4393
2024-11-30 04:56:10.650094: Pseudo dice [0.6751]
2024-11-30 04:56:10.651048: Epoch time: 181.28 s
2024-11-30 04:56:12.423786: 
2024-11-30 04:56:12.425267: Epoch 185
2024-11-30 04:56:12.426129: Current learning rate: 0.00832
2024-11-30 04:59:13.554593: Validation loss did not improve from -0.51014. Patience: 45/50
2024-11-30 04:59:13.555966: train_loss -0.7358
2024-11-30 04:59:13.557204: val_loss -0.4822
2024-11-30 04:59:13.558051: Pseudo dice [0.7078]
2024-11-30 04:59:13.558745: Epoch time: 181.13 s
2024-11-30 04:59:14.873264: 
2024-11-30 04:59:14.875405: Epoch 186
2024-11-30 04:59:14.876165: Current learning rate: 0.00831
2024-11-30 05:02:15.973402: Validation loss did not improve from -0.51014. Patience: 46/50
2024-11-30 05:02:15.974184: train_loss -0.7409
2024-11-30 05:02:15.975024: val_loss -0.419
2024-11-30 05:02:15.975702: Pseudo dice [0.6728]
2024-11-30 05:02:15.976412: Epoch time: 181.1 s
2024-11-30 05:02:17.327948: 
2024-11-30 05:02:17.329507: Epoch 187
2024-11-30 05:02:17.330335: Current learning rate: 0.0083
2024-11-30 05:05:18.096316: Validation loss did not improve from -0.51014. Patience: 47/50
2024-11-30 05:05:18.099989: train_loss -0.7394
2024-11-30 05:05:18.101952: val_loss -0.4505
2024-11-30 05:05:18.102887: Pseudo dice [0.687]
2024-11-30 05:05:18.103881: Epoch time: 180.77 s
2024-11-30 05:05:19.898641: 
2024-11-30 05:05:19.899575: Epoch 188
2024-11-30 05:05:19.900441: Current learning rate: 0.00829
2024-11-30 05:08:20.814518: Validation loss did not improve from -0.51014. Patience: 48/50
2024-11-30 05:08:20.815514: train_loss -0.7395
2024-11-30 05:08:20.816888: val_loss -0.4302
2024-11-30 05:08:20.818316: Pseudo dice [0.6754]
2024-11-30 05:08:20.819832: Epoch time: 180.92 s
2024-11-30 05:08:22.136091: 
2024-11-30 05:08:22.137672: Epoch 189
2024-11-30 05:08:22.138584: Current learning rate: 0.00828
2024-11-30 05:11:23.409181: Validation loss did not improve from -0.51014. Patience: 49/50
2024-11-30 05:11:23.410438: train_loss -0.7357
2024-11-30 05:11:23.411722: val_loss -0.4302
2024-11-30 05:11:23.412674: Pseudo dice [0.671]
2024-11-30 05:11:23.413444: Epoch time: 181.28 s
2024-11-30 05:11:25.067914: 
2024-11-30 05:11:25.069541: Epoch 190
2024-11-30 05:11:25.070261: Current learning rate: 0.00827
2024-11-30 05:14:26.196653: Validation loss did not improve from -0.51014. Patience: 50/50
2024-11-30 05:14:26.198102: train_loss -0.7384
2024-11-30 05:14:26.199018: val_loss -0.464
2024-11-30 05:14:26.199651: Pseudo dice [0.6925]
2024-11-30 05:14:26.200331: Epoch time: 181.13 s
2024-11-30 05:14:27.495682: Patience reached. Stopping training.
2024-11-30 05:14:27.975089: Training done.
2024-11-30 05:14:28.141162: Using splits from existing split file: /home/gridsan/nchutisilp/datasets/nnUNet_Datasets/nnUNet_preprocessed/Dataset306_Sohee_Ajay_Calcium_OCT/splits_final.json
2024-11-30 05:14:28.162631: The split file contains 5 splits.
2024-11-30 05:14:28.164085: Desired fold for training: 2
2024-11-30 05:14:28.165051: This split has 10 training and 3 validation cases.
2024-11-30 05:14:28.166111: predicting 401-004
2024-11-30 05:14:28.202085: 401-004, shape torch.Size([1, 375, 498, 498]), rank 0
2024-11-30 05:17:51.655169: predicting 706-005
2024-11-30 05:17:51.677360: 706-005, shape torch.Size([1, 375, 498, 498]), rank 0
2024-11-30 05:20:59.442564: predicting 708006Pre
2024-11-30 05:20:59.476305: 708006Pre, shape torch.Size([1, 253, 498, 498]), rank 0
2024-11-30 05:23:23.262239: Validation complete
2024-11-30 05:23:23.263057: Mean Validation Dice:  0.6389829675835241
2024-11-30 02:58:42.553655: unpacking done...
2024-11-30 02:58:42.883464: Unable to plot network architecture: nnUNet_compile is enabled!
2024-11-30 02:58:42.942598: 
2024-11-30 02:58:42.944030: Epoch 0
2024-11-30 02:58:42.944971: Current learning rate: 0.01
2024-11-30 03:03:08.507122: Validation loss improved from 1000.00000 to -0.16359! Patience: 0/50
2024-11-30 03:03:08.508365: train_loss -0.0592
2024-11-30 03:03:08.509306: val_loss -0.1636
2024-11-30 03:03:08.510034: Pseudo dice [0.455]
2024-11-30 03:03:08.510988: Epoch time: 265.57 s
2024-11-30 03:03:08.511895: Yayy! New best EMA pseudo Dice: 0.455
2024-11-30 03:03:10.561949: 
2024-11-30 03:03:10.563772: Epoch 1
2024-11-30 03:03:10.564662: Current learning rate: 0.00999
2024-11-30 03:06:16.271318: Validation loss improved from -0.16359 to -0.16969! Patience: 0/50
2024-11-30 03:06:16.272741: train_loss -0.2098
2024-11-30 03:06:16.273776: val_loss -0.1697
2024-11-30 03:06:16.274500: Pseudo dice [0.4534]
2024-11-30 03:06:16.275235: Epoch time: 185.71 s
2024-11-30 03:06:17.457490: 
2024-11-30 03:06:17.459303: Epoch 2
2024-11-30 03:06:17.460326: Current learning rate: 0.00998
2024-11-30 03:09:23.038350: Validation loss did not improve from -0.16969. Patience: 1/50
2024-11-30 03:09:23.039761: train_loss -0.26
2024-11-30 03:09:23.040713: val_loss -0.165
2024-11-30 03:09:23.041583: Pseudo dice [0.4535]
2024-11-30 03:09:23.042370: Epoch time: 185.58 s
2024-11-30 03:09:24.324460: 
2024-11-30 03:09:24.326544: Epoch 3
2024-11-30 03:09:24.327414: Current learning rate: 0.00997
2024-11-30 03:12:29.506615: Validation loss improved from -0.16969 to -0.20351! Patience: 1/50
2024-11-30 03:12:29.508141: train_loss -0.3035
2024-11-30 03:12:29.509206: val_loss -0.2035
2024-11-30 03:12:29.510000: Pseudo dice [0.4877]
2024-11-30 03:12:29.510810: Epoch time: 185.18 s
2024-11-30 03:12:29.511623: Yayy! New best EMA pseudo Dice: 0.458
2024-11-30 03:12:31.203547: 
2024-11-30 03:12:31.205023: Epoch 4
2024-11-30 03:12:31.206098: Current learning rate: 0.00996
2024-11-30 03:15:36.679169: Validation loss did not improve from -0.20351. Patience: 1/50
2024-11-30 03:15:36.680532: train_loss -0.3264
2024-11-30 03:15:36.681374: val_loss -0.1728
2024-11-30 03:15:36.682080: Pseudo dice [0.4697]
2024-11-30 03:15:36.682751: Epoch time: 185.48 s
2024-11-30 03:15:37.070860: Yayy! New best EMA pseudo Dice: 0.4592
2024-11-30 03:15:38.756995: 
2024-11-30 03:15:38.758549: Epoch 5
2024-11-30 03:15:38.759414: Current learning rate: 0.00995
2024-11-30 03:18:44.251244: Validation loss improved from -0.20351 to -0.22569! Patience: 1/50
2024-11-30 03:18:44.252498: train_loss -0.3536
2024-11-30 03:18:44.253466: val_loss -0.2257
2024-11-30 03:18:44.254381: Pseudo dice [0.5067]
2024-11-30 03:18:44.255235: Epoch time: 185.5 s
2024-11-30 03:18:44.256099: Yayy! New best EMA pseudo Dice: 0.4639
2024-11-30 03:18:45.810193: 
2024-11-30 03:18:45.812177: Epoch 6
2024-11-30 03:18:45.813229: Current learning rate: 0.00995
2024-11-30 03:21:50.960830: Validation loss improved from -0.22569 to -0.25991! Patience: 0/50
2024-11-30 03:21:50.962108: train_loss -0.3649
2024-11-30 03:21:50.963343: val_loss -0.2599
2024-11-30 03:21:50.964155: Pseudo dice [0.5303]
2024-11-30 03:21:50.964959: Epoch time: 185.15 s
2024-11-30 03:21:50.965682: Yayy! New best EMA pseudo Dice: 0.4706
2024-11-30 03:21:52.520653: 
2024-11-30 03:21:52.522083: Epoch 7
2024-11-30 03:21:52.522845: Current learning rate: 0.00994
2024-11-30 03:24:57.696975: Validation loss improved from -0.25991 to -0.27614! Patience: 0/50
2024-11-30 03:24:57.698321: train_loss -0.3928
2024-11-30 03:24:57.699336: val_loss -0.2761
2024-11-30 03:24:57.700022: Pseudo dice [0.5391]
2024-11-30 03:24:57.700702: Epoch time: 185.18 s
2024-11-30 03:24:57.701414: Yayy! New best EMA pseudo Dice: 0.4774
2024-11-30 03:24:59.250285: 
2024-11-30 03:24:59.251517: Epoch 8
2024-11-30 03:24:59.252205: Current learning rate: 0.00993
2024-11-30 03:28:04.936120: Validation loss did not improve from -0.27614. Patience: 1/50
2024-11-30 03:28:04.937442: train_loss -0.4116
2024-11-30 03:28:04.938340: val_loss -0.2667
2024-11-30 03:28:04.939091: Pseudo dice [0.5199]
2024-11-30 03:28:04.940054: Epoch time: 185.69 s
2024-11-30 03:28:04.940722: Yayy! New best EMA pseudo Dice: 0.4817
2024-11-30 03:28:06.928091: 
2024-11-30 03:28:06.929544: Epoch 9
2024-11-30 03:28:06.930349: Current learning rate: 0.00992
2024-11-30 03:31:12.277723: Validation loss improved from -0.27614 to -0.31466! Patience: 1/50
2024-11-30 03:31:12.278875: train_loss -0.416
2024-11-30 03:31:12.280275: val_loss -0.3147
2024-11-30 03:31:12.281428: Pseudo dice [0.5608]
2024-11-30 03:31:12.282745: Epoch time: 185.35 s
2024-11-30 03:31:12.686721: Yayy! New best EMA pseudo Dice: 0.4896
2024-11-30 03:31:14.370289: 
2024-11-30 03:31:14.372419: Epoch 10
2024-11-30 03:31:14.373446: Current learning rate: 0.00991
2024-11-30 03:34:20.364488: Validation loss improved from -0.31466 to -0.34384! Patience: 0/50
2024-11-30 03:34:20.365770: train_loss -0.4266
2024-11-30 03:34:20.366619: val_loss -0.3438
2024-11-30 03:34:20.367295: Pseudo dice [0.5789]
2024-11-30 03:34:20.367998: Epoch time: 186.0 s
2024-11-30 03:34:20.368670: Yayy! New best EMA pseudo Dice: 0.4985
2024-11-30 03:34:21.898519: 
2024-11-30 03:34:21.900198: Epoch 11
2024-11-30 03:34:21.901316: Current learning rate: 0.0099
2024-11-30 03:37:27.534886: Validation loss did not improve from -0.34384. Patience: 1/50
2024-11-30 03:37:27.536289: train_loss -0.439
2024-11-30 03:37:27.537506: val_loss -0.3046
2024-11-30 03:37:27.538307: Pseudo dice [0.5633]
2024-11-30 03:37:27.539223: Epoch time: 185.64 s
2024-11-30 03:37:27.540012: Yayy! New best EMA pseudo Dice: 0.505
2024-11-30 03:37:29.144679: 
2024-11-30 03:37:29.146829: Epoch 12
2024-11-30 03:37:29.147726: Current learning rate: 0.00989
2024-11-30 03:40:34.045256: Validation loss did not improve from -0.34384. Patience: 2/50
2024-11-30 03:40:34.046252: train_loss -0.448
2024-11-30 03:40:34.047496: val_loss -0.302
2024-11-30 03:40:34.048465: Pseudo dice [0.5467]
2024-11-30 03:40:34.049470: Epoch time: 184.9 s
2024-11-30 03:40:34.050421: Yayy! New best EMA pseudo Dice: 0.5092
2024-11-30 03:40:35.631327: 
2024-11-30 03:40:35.633384: Epoch 13
2024-11-30 03:40:35.634571: Current learning rate: 0.00988
2024-11-30 03:43:41.294789: Validation loss did not improve from -0.34384. Patience: 3/50
2024-11-30 03:43:41.296239: train_loss -0.4471
2024-11-30 03:43:41.297215: val_loss -0.327
2024-11-30 03:43:41.297945: Pseudo dice [0.5665]
2024-11-30 03:43:41.298849: Epoch time: 185.67 s
2024-11-30 03:43:41.299648: Yayy! New best EMA pseudo Dice: 0.5149
2024-11-30 03:43:42.910197: 
2024-11-30 03:43:42.911791: Epoch 14
2024-11-30 03:43:42.912764: Current learning rate: 0.00987
2024-11-30 03:46:48.545853: Validation loss did not improve from -0.34384. Patience: 4/50
2024-11-30 03:46:48.546749: train_loss -0.4518
2024-11-30 03:46:48.547638: val_loss -0.2937
2024-11-30 03:46:48.548736: Pseudo dice [0.5568]
2024-11-30 03:46:48.549561: Epoch time: 185.64 s
2024-11-30 03:46:48.933195: Yayy! New best EMA pseudo Dice: 0.5191
2024-11-30 03:46:50.632474: 
2024-11-30 03:46:50.634299: Epoch 15
2024-11-30 03:46:50.635453: Current learning rate: 0.00986
2024-11-30 03:49:56.286673: Validation loss did not improve from -0.34384. Patience: 5/50
2024-11-30 03:49:56.287992: train_loss -0.4757
2024-11-30 03:49:56.289206: val_loss -0.3335
2024-11-30 03:49:56.290143: Pseudo dice [0.5749]
2024-11-30 03:49:56.290916: Epoch time: 185.66 s
2024-11-30 03:49:56.291619: Yayy! New best EMA pseudo Dice: 0.5247
2024-11-30 03:49:57.872845: 
2024-11-30 03:49:57.874735: Epoch 16
2024-11-30 03:49:57.875762: Current learning rate: 0.00986
2024-11-30 03:53:03.382867: Validation loss did not improve from -0.34384. Patience: 6/50
2024-11-30 03:53:03.384085: train_loss -0.4842
2024-11-30 03:53:03.385245: val_loss -0.3064
2024-11-30 03:53:03.386130: Pseudo dice [0.5826]
2024-11-30 03:53:03.386944: Epoch time: 185.51 s
2024-11-30 03:53:03.387809: Yayy! New best EMA pseudo Dice: 0.5305
2024-11-30 03:53:05.019374: 
2024-11-30 03:53:05.021105: Epoch 17
2024-11-30 03:53:05.022038: Current learning rate: 0.00985
2024-11-30 03:56:10.292036: Validation loss improved from -0.34384 to -0.37706! Patience: 6/50
2024-11-30 03:56:10.293653: train_loss -0.4982
2024-11-30 03:56:10.294838: val_loss -0.3771
2024-11-30 03:56:10.295851: Pseudo dice [0.6068]
2024-11-30 03:56:10.296897: Epoch time: 185.28 s
2024-11-30 03:56:10.297628: Yayy! New best EMA pseudo Dice: 0.5381
2024-11-30 03:56:11.919607: 
2024-11-30 03:56:11.921090: Epoch 18
2024-11-30 03:56:11.921837: Current learning rate: 0.00984
2024-11-30 03:59:17.477903: Validation loss did not improve from -0.37706. Patience: 1/50
2024-11-30 03:59:17.479162: train_loss -0.4916
2024-11-30 03:59:17.480477: val_loss -0.3418
2024-11-30 03:59:17.481380: Pseudo dice [0.5816]
2024-11-30 03:59:17.482130: Epoch time: 185.56 s
2024-11-30 03:59:17.483057: Yayy! New best EMA pseudo Dice: 0.5424
2024-11-30 03:59:19.064921: 
2024-11-30 03:59:19.066573: Epoch 19
2024-11-30 03:59:19.067403: Current learning rate: 0.00983
2024-11-30 04:02:24.143814: Validation loss did not improve from -0.37706. Patience: 2/50
2024-11-30 04:02:24.145098: train_loss -0.4901
2024-11-30 04:02:24.146325: val_loss -0.3621
2024-11-30 04:02:24.147348: Pseudo dice [0.5933]
2024-11-30 04:02:24.148503: Epoch time: 185.08 s
2024-11-30 04:02:24.866029: Yayy! New best EMA pseudo Dice: 0.5475
2024-11-30 04:02:26.469877: 
2024-11-30 04:02:26.471082: Epoch 20
2024-11-30 04:02:26.471965: Current learning rate: 0.00982
2024-11-30 04:05:31.708426: Validation loss improved from -0.37706 to -0.38740! Patience: 2/50
2024-11-30 04:05:31.711529: train_loss -0.5151
2024-11-30 04:05:31.712980: val_loss -0.3874
2024-11-30 04:05:31.713776: Pseudo dice [0.5972]
2024-11-30 04:05:31.714696: Epoch time: 185.24 s
2024-11-30 04:05:31.715367: Yayy! New best EMA pseudo Dice: 0.5525
2024-11-30 04:05:33.391123: 
2024-11-30 04:05:33.392987: Epoch 21
2024-11-30 04:05:33.393773: Current learning rate: 0.00981
2024-11-30 04:08:38.926054: Validation loss did not improve from -0.38740. Patience: 1/50
2024-11-30 04:08:38.927479: train_loss -0.5161
2024-11-30 04:08:38.929008: val_loss -0.3733
2024-11-30 04:08:38.930098: Pseudo dice [0.5904]
2024-11-30 04:08:38.930961: Epoch time: 185.54 s
2024-11-30 04:08:38.932011: Yayy! New best EMA pseudo Dice: 0.5563
2024-11-30 04:08:40.492944: 
2024-11-30 04:08:40.495121: Epoch 22
2024-11-30 04:08:40.496494: Current learning rate: 0.0098
2024-11-30 04:11:45.528641: Validation loss did not improve from -0.38740. Patience: 2/50
2024-11-30 04:11:45.529550: train_loss -0.5216
2024-11-30 04:11:45.530569: val_loss -0.3223
2024-11-30 04:11:45.531243: Pseudo dice [0.5804]
2024-11-30 04:11:45.532026: Epoch time: 185.04 s
2024-11-30 04:11:45.532806: Yayy! New best EMA pseudo Dice: 0.5587
2024-11-30 04:11:47.078056: 
2024-11-30 04:11:47.079962: Epoch 23
2024-11-30 04:11:47.080730: Current learning rate: 0.00979
2024-11-30 04:14:52.387021: Validation loss did not improve from -0.38740. Patience: 3/50
2024-11-30 04:14:52.388470: train_loss -0.5236
2024-11-30 04:14:52.389864: val_loss -0.3837
2024-11-30 04:14:52.390900: Pseudo dice [0.6043]
2024-11-30 04:14:52.391896: Epoch time: 185.31 s
2024-11-30 04:14:52.392846: Yayy! New best EMA pseudo Dice: 0.5633
2024-11-30 04:14:53.955280: 
2024-11-30 04:14:53.957278: Epoch 24
2024-11-30 04:14:53.958145: Current learning rate: 0.00978
2024-11-30 04:17:59.702394: Validation loss did not improve from -0.38740. Patience: 4/50
2024-11-30 04:17:59.703286: train_loss -0.5297
2024-11-30 04:17:59.704111: val_loss -0.3375
2024-11-30 04:17:59.704839: Pseudo dice [0.5979]
2024-11-30 04:17:59.705762: Epoch time: 185.75 s
2024-11-30 04:18:00.075765: Yayy! New best EMA pseudo Dice: 0.5667
2024-11-30 04:18:01.578856: 
2024-11-30 04:18:01.580625: Epoch 25
2024-11-30 04:18:01.581530: Current learning rate: 0.00977
2024-11-30 04:21:07.813383: Validation loss did not improve from -0.38740. Patience: 5/50
2024-11-30 04:21:07.814619: train_loss -0.5216
2024-11-30 04:21:07.815443: val_loss -0.3664
2024-11-30 04:21:07.816130: Pseudo dice [0.5954]
2024-11-30 04:21:07.816794: Epoch time: 186.24 s
2024-11-30 04:21:07.817464: Yayy! New best EMA pseudo Dice: 0.5696
2024-11-30 04:21:09.390878: 
2024-11-30 04:21:09.392719: Epoch 26
2024-11-30 04:21:09.393637: Current learning rate: 0.00977
2024-11-30 04:24:15.403802: Validation loss did not improve from -0.38740. Patience: 6/50
2024-11-30 04:24:15.404998: train_loss -0.5241
2024-11-30 04:24:15.406058: val_loss -0.3861
2024-11-30 04:24:15.406717: Pseudo dice [0.6183]
2024-11-30 04:24:15.407434: Epoch time: 186.02 s
2024-11-30 04:24:15.408145: Yayy! New best EMA pseudo Dice: 0.5745
2024-11-30 04:24:16.989753: 
2024-11-30 04:24:16.991353: Epoch 27
2024-11-30 04:24:16.992082: Current learning rate: 0.00976
2024-11-30 04:27:22.621971: Validation loss did not improve from -0.38740. Patience: 7/50
2024-11-30 04:27:22.623278: train_loss -0.5421
2024-11-30 04:27:22.624564: val_loss -0.3293
2024-11-30 04:27:22.625557: Pseudo dice [0.5879]
2024-11-30 04:27:22.626477: Epoch time: 185.63 s
2024-11-30 04:27:22.627266: Yayy! New best EMA pseudo Dice: 0.5758
2024-11-30 04:27:24.217757: 
2024-11-30 04:27:24.219397: Epoch 28
2024-11-30 04:27:24.220432: Current learning rate: 0.00975
2024-11-30 04:30:30.206814: Validation loss did not improve from -0.38740. Patience: 8/50
2024-11-30 04:30:30.209578: train_loss -0.5398
2024-11-30 04:30:30.210980: val_loss -0.3162
2024-11-30 04:30:30.211878: Pseudo dice [0.5744]
2024-11-30 04:30:30.212826: Epoch time: 185.99 s
2024-11-30 04:30:31.402444: 
2024-11-30 04:30:31.404519: Epoch 29
2024-11-30 04:30:31.405275: Current learning rate: 0.00974
2024-11-30 04:33:36.912395: Validation loss improved from -0.38740 to -0.45432! Patience: 8/50
2024-11-30 04:33:36.913719: train_loss -0.5577
2024-11-30 04:33:36.914846: val_loss -0.4543
2024-11-30 04:33:36.915943: Pseudo dice [0.662]
2024-11-30 04:33:36.916971: Epoch time: 185.51 s
2024-11-30 04:33:37.262574: Yayy! New best EMA pseudo Dice: 0.5843
2024-11-30 04:33:39.216748: 
2024-11-30 04:33:39.218670: Epoch 30
2024-11-30 04:33:39.219647: Current learning rate: 0.00973
2024-11-30 04:36:45.314176: Validation loss did not improve from -0.45432. Patience: 1/50
2024-11-30 04:36:45.315569: train_loss -0.564
2024-11-30 04:36:45.316407: val_loss -0.3826
2024-11-30 04:36:45.317165: Pseudo dice [0.6104]
2024-11-30 04:36:45.318027: Epoch time: 186.1 s
2024-11-30 04:36:45.318841: Yayy! New best EMA pseudo Dice: 0.5869
2024-11-30 04:36:46.912863: 
2024-11-30 04:36:46.914866: Epoch 31
2024-11-30 04:36:46.915783: Current learning rate: 0.00972
2024-11-30 04:39:53.017528: Validation loss did not improve from -0.45432. Patience: 2/50
2024-11-30 04:39:53.018876: train_loss -0.5559
2024-11-30 04:39:53.019956: val_loss -0.3875
2024-11-30 04:39:53.020617: Pseudo dice [0.6126]
2024-11-30 04:39:53.021372: Epoch time: 186.11 s
2024-11-30 04:39:53.022180: Yayy! New best EMA pseudo Dice: 0.5895
2024-11-30 04:39:54.618334: 
2024-11-30 04:39:54.620168: Epoch 32
2024-11-30 04:39:54.621247: Current learning rate: 0.00971
2024-11-30 04:43:00.169375: Validation loss did not improve from -0.45432. Patience: 3/50
2024-11-30 04:43:00.170856: train_loss -0.554
2024-11-30 04:43:00.171809: val_loss -0.3724
2024-11-30 04:43:00.172492: Pseudo dice [0.5829]
2024-11-30 04:43:00.173242: Epoch time: 185.55 s
2024-11-30 04:43:01.382106: 
2024-11-30 04:43:01.383519: Epoch 33
2024-11-30 04:43:01.384411: Current learning rate: 0.0097
2024-11-30 04:46:07.529198: Validation loss did not improve from -0.45432. Patience: 4/50
2024-11-30 04:46:07.530648: train_loss -0.568
2024-11-30 04:46:07.532027: val_loss -0.3261
2024-11-30 04:46:07.533258: Pseudo dice [0.5877]
2024-11-30 04:46:07.534194: Epoch time: 186.15 s
2024-11-30 04:46:08.772056: 
2024-11-30 04:46:08.774188: Epoch 34
2024-11-30 04:46:08.775181: Current learning rate: 0.00969
2024-11-30 04:49:14.885990: Validation loss did not improve from -0.45432. Patience: 5/50
2024-11-30 04:49:14.887132: train_loss -0.5811
2024-11-30 04:49:14.887954: val_loss -0.4205
2024-11-30 04:49:14.888742: Pseudo dice [0.6267]
2024-11-30 04:49:14.889430: Epoch time: 186.12 s
2024-11-30 04:49:15.356260: Yayy! New best EMA pseudo Dice: 0.5925
2024-11-30 04:49:17.034753: 
2024-11-30 04:49:17.037308: Epoch 35
2024-11-30 04:49:17.038910: Current learning rate: 0.00968
2024-11-30 04:52:23.320065: Validation loss did not improve from -0.45432. Patience: 6/50
2024-11-30 04:52:23.321232: train_loss -0.5636
2024-11-30 04:52:23.322182: val_loss -0.3877
2024-11-30 04:52:23.322977: Pseudo dice [0.6216]
2024-11-30 04:52:23.323662: Epoch time: 186.29 s
2024-11-30 04:52:23.324389: Yayy! New best EMA pseudo Dice: 0.5954
2024-11-30 04:52:24.941684: 
2024-11-30 04:52:24.943208: Epoch 36
2024-11-30 04:52:24.944029: Current learning rate: 0.00968
2024-11-30 04:55:31.153468: Validation loss did not improve from -0.45432. Patience: 7/50
2024-11-30 04:55:31.154729: train_loss -0.5586
2024-11-30 04:55:31.155963: val_loss -0.4086
2024-11-30 04:55:31.156810: Pseudo dice [0.6088]
2024-11-30 04:55:31.157555: Epoch time: 186.21 s
2024-11-30 04:55:31.158295: Yayy! New best EMA pseudo Dice: 0.5968
2024-11-30 04:55:32.790030: 
2024-11-30 04:55:32.791560: Epoch 37
2024-11-30 04:55:32.792532: Current learning rate: 0.00967
2024-11-30 04:58:38.494161: Validation loss did not improve from -0.45432. Patience: 8/50
2024-11-30 04:58:38.495379: train_loss -0.5583
2024-11-30 04:58:38.496277: val_loss -0.3809
2024-11-30 04:58:38.497040: Pseudo dice [0.6029]
2024-11-30 04:58:38.497714: Epoch time: 185.71 s
2024-11-30 04:58:38.498531: Yayy! New best EMA pseudo Dice: 0.5974
2024-11-30 04:58:40.153687: 
2024-11-30 04:58:40.155087: Epoch 38
2024-11-30 04:58:40.155810: Current learning rate: 0.00966
2024-11-30 05:01:46.202790: Validation loss did not improve from -0.45432. Patience: 9/50
2024-11-30 05:01:46.204011: train_loss -0.5831
2024-11-30 05:01:46.205307: val_loss -0.4237
2024-11-30 05:01:46.206443: Pseudo dice [0.6376]
2024-11-30 05:01:46.207349: Epoch time: 186.05 s
2024-11-30 05:01:46.208326: Yayy! New best EMA pseudo Dice: 0.6014
2024-11-30 05:01:47.834599: 
2024-11-30 05:01:47.836583: Epoch 39
2024-11-30 05:01:47.837378: Current learning rate: 0.00965
2024-11-30 05:04:53.984695: Validation loss did not improve from -0.45432. Patience: 10/50
2024-11-30 05:04:53.987978: train_loss -0.5685
2024-11-30 05:04:53.989797: val_loss -0.3855
2024-11-30 05:04:53.990959: Pseudo dice [0.6124]
2024-11-30 05:04:53.991795: Epoch time: 186.15 s
2024-11-30 05:04:54.315085: Yayy! New best EMA pseudo Dice: 0.6025
2024-11-30 05:04:55.858755: 
2024-11-30 05:04:55.860391: Epoch 40
2024-11-30 05:04:55.861083: Current learning rate: 0.00964
2024-11-30 05:08:01.438690: Validation loss did not improve from -0.45432. Patience: 11/50
2024-11-30 05:08:01.446778: train_loss -0.589
2024-11-30 05:08:01.448905: val_loss -0.3396
2024-11-30 05:08:01.450106: Pseudo dice [0.6006]
2024-11-30 05:08:01.451274: Epoch time: 185.59 s
2024-11-30 05:08:03.314731: 
2024-11-30 05:08:03.315790: Epoch 41
2024-11-30 05:08:03.316735: Current learning rate: 0.00963
2024-11-30 05:11:09.640963: Validation loss did not improve from -0.45432. Patience: 12/50
2024-11-30 05:11:09.642760: train_loss -0.5798
2024-11-30 05:11:09.645267: val_loss -0.3982
2024-11-30 05:11:09.646204: Pseudo dice [0.6186]
2024-11-30 05:11:09.647462: Epoch time: 186.33 s
2024-11-30 05:11:09.648334: Yayy! New best EMA pseudo Dice: 0.6039
2024-11-30 05:11:11.278478: 
2024-11-30 05:11:11.280426: Epoch 42
2024-11-30 05:11:11.281141: Current learning rate: 0.00962
2024-11-30 05:14:17.104280: Validation loss did not improve from -0.45432. Patience: 13/50
2024-11-30 05:14:17.105665: train_loss -0.598
2024-11-30 05:14:17.106748: val_loss -0.4097
2024-11-30 05:14:17.107575: Pseudo dice [0.6255]
2024-11-30 05:14:17.108306: Epoch time: 185.83 s
2024-11-30 05:14:17.108904: Yayy! New best EMA pseudo Dice: 0.6061
2024-11-30 05:14:18.744124: 
2024-11-30 05:14:18.746110: Epoch 43
2024-11-30 05:14:18.747060: Current learning rate: 0.00961
2024-11-30 05:17:06.299900: Validation loss did not improve from -0.45432. Patience: 14/50
2024-11-30 05:17:06.300943: train_loss -0.5964
2024-11-30 05:17:06.302206: val_loss -0.4028
2024-11-30 05:17:06.303222: Pseudo dice [0.6173]
2024-11-30 05:17:06.304432: Epoch time: 167.56 s
2024-11-30 05:17:06.305592: Yayy! New best EMA pseudo Dice: 0.6072
2024-11-30 05:17:07.819984: 
2024-11-30 05:17:07.821618: Epoch 44
2024-11-30 05:17:07.822966: Current learning rate: 0.0096
2024-11-30 05:20:14.136097: Validation loss did not improve from -0.45432. Patience: 15/50
2024-11-30 05:20:14.137335: train_loss -0.6027
2024-11-30 05:20:14.138346: val_loss -0.4204
2024-11-30 05:20:14.139140: Pseudo dice [0.6313]
2024-11-30 05:20:14.139928: Epoch time: 186.32 s
2024-11-30 05:20:14.473982: Yayy! New best EMA pseudo Dice: 0.6096
2024-11-30 05:20:15.970744: 
2024-11-30 05:20:15.972195: Epoch 45
2024-11-30 05:20:15.972970: Current learning rate: 0.00959
2024-11-30 05:23:12.323942: Validation loss did not improve from -0.45432. Patience: 16/50
2024-11-30 05:23:12.324820: train_loss -0.5967
2024-11-30 05:23:12.325872: val_loss -0.3693
2024-11-30 05:23:12.326577: Pseudo dice [0.6008]
2024-11-30 05:23:12.327363: Epoch time: 176.36 s
2024-11-30 05:23:13.493701: 
2024-11-30 05:23:13.495540: Epoch 46
2024-11-30 05:23:13.496321: Current learning rate: 0.00959
2024-11-30 05:24:41.146532: Validation loss did not improve from -0.45432. Patience: 17/50
2024-11-30 05:24:41.147708: train_loss -0.5937
2024-11-30 05:24:41.148580: val_loss -0.4113
2024-11-30 05:24:41.149580: Pseudo dice [0.6265]
2024-11-30 05:24:41.150538: Epoch time: 87.66 s
2024-11-30 05:24:41.151288: Yayy! New best EMA pseudo Dice: 0.6105
2024-11-30 05:24:42.622434: 
2024-11-30 05:24:42.624429: Epoch 47
2024-11-30 05:24:42.625667: Current learning rate: 0.00958
2024-11-30 05:26:10.324149: Validation loss did not improve from -0.45432. Patience: 18/50
2024-11-30 05:26:10.325683: train_loss -0.604
2024-11-30 05:26:10.327431: val_loss -0.3783
2024-11-30 05:26:10.328428: Pseudo dice [0.6162]
2024-11-30 05:26:10.329654: Epoch time: 87.7 s
2024-11-30 05:26:10.330435: Yayy! New best EMA pseudo Dice: 0.6111
2024-11-30 05:26:11.802637: 
2024-11-30 05:26:11.805039: Epoch 48
2024-11-30 05:26:11.806355: Current learning rate: 0.00957
2024-11-30 05:27:39.477225: Validation loss did not improve from -0.45432. Patience: 19/50
2024-11-30 05:27:39.478547: train_loss -0.5993
2024-11-30 05:27:39.479928: val_loss -0.3858
2024-11-30 05:27:39.481245: Pseudo dice [0.6296]
2024-11-30 05:27:39.482270: Epoch time: 87.68 s
2024-11-30 05:27:39.483424: Yayy! New best EMA pseudo Dice: 0.6129
2024-11-30 05:27:40.957709: 
2024-11-30 05:27:40.959638: Epoch 49
2024-11-30 05:27:40.960633: Current learning rate: 0.00956
2024-11-30 05:29:08.580375: Validation loss did not improve from -0.45432. Patience: 20/50
2024-11-30 05:29:08.581820: train_loss -0.5968
2024-11-30 05:29:08.583376: val_loss -0.3329
2024-11-30 05:29:08.584311: Pseudo dice [0.5881]
2024-11-30 05:29:08.585245: Epoch time: 87.63 s
2024-11-30 05:29:10.067304: 
2024-11-30 05:29:10.068967: Epoch 50
2024-11-30 05:29:10.069648: Current learning rate: 0.00955
2024-11-30 05:30:37.694380: Validation loss did not improve from -0.45432. Patience: 21/50
2024-11-30 05:30:37.695920: train_loss -0.6076
2024-11-30 05:30:37.697397: val_loss -0.4032
2024-11-30 05:30:37.698790: Pseudo dice [0.6379]
2024-11-30 05:30:37.700073: Epoch time: 87.63 s
2024-11-30 05:30:37.701339: Yayy! New best EMA pseudo Dice: 0.6132
2024-11-30 05:30:39.186068: 
2024-11-30 05:30:39.188260: Epoch 51
2024-11-30 05:30:39.189450: Current learning rate: 0.00954
2024-11-30 05:32:06.790881: Validation loss did not improve from -0.45432. Patience: 22/50
2024-11-30 05:32:06.791871: train_loss -0.6156
2024-11-30 05:32:06.793310: val_loss -0.4025
2024-11-30 05:32:06.795072: Pseudo dice [0.6328]
2024-11-30 05:32:06.796293: Epoch time: 87.61 s
2024-11-30 05:32:06.797460: Yayy! New best EMA pseudo Dice: 0.6151
2024-11-30 05:32:08.262918: 
2024-11-30 05:32:08.265229: Epoch 52
2024-11-30 05:32:08.266545: Current learning rate: 0.00953
2024-11-30 05:33:35.844600: Validation loss did not improve from -0.45432. Patience: 23/50
2024-11-30 05:33:35.845784: train_loss -0.608
2024-11-30 05:33:35.846984: val_loss -0.3789
2024-11-30 05:33:35.847741: Pseudo dice [0.6185]
2024-11-30 05:33:35.848509: Epoch time: 87.58 s
2024-11-30 05:33:35.849407: Yayy! New best EMA pseudo Dice: 0.6155
2024-11-30 05:33:37.634758: 
2024-11-30 05:33:37.636483: Epoch 53
2024-11-30 05:33:37.637414: Current learning rate: 0.00952
2024-11-30 05:35:05.253430: Validation loss did not improve from -0.45432. Patience: 24/50
2024-11-30 05:35:05.254963: train_loss -0.6161
2024-11-30 05:35:05.256091: val_loss -0.4123
2024-11-30 05:35:05.256980: Pseudo dice [0.6392]
2024-11-30 05:35:05.257842: Epoch time: 87.62 s
2024-11-30 05:35:05.258653: Yayy! New best EMA pseudo Dice: 0.6178
2024-11-30 05:35:06.757547: 
2024-11-30 05:35:06.759364: Epoch 54
2024-11-30 05:35:06.760341: Current learning rate: 0.00951
2024-11-30 05:36:34.461930: Validation loss did not improve from -0.45432. Patience: 25/50
2024-11-30 05:36:34.463202: train_loss -0.6112
2024-11-30 05:36:34.464215: val_loss -0.4113
2024-11-30 05:36:34.465244: Pseudo dice [0.6403]
2024-11-30 05:36:34.466257: Epoch time: 87.71 s
2024-11-30 05:36:34.813588: Yayy! New best EMA pseudo Dice: 0.6201
2024-11-30 05:36:36.323513: 
2024-11-30 05:36:36.325764: Epoch 55
2024-11-30 05:36:36.326689: Current learning rate: 0.0095
2024-11-30 05:38:03.883368: Validation loss did not improve from -0.45432. Patience: 26/50
2024-11-30 05:38:03.884759: train_loss -0.6263
2024-11-30 05:38:03.886026: val_loss -0.4278
2024-11-30 05:38:03.887109: Pseudo dice [0.6436]
2024-11-30 05:38:03.888107: Epoch time: 87.56 s
2024-11-30 05:38:03.889135: Yayy! New best EMA pseudo Dice: 0.6224
2024-11-30 05:38:05.387486: 
2024-11-30 05:38:05.389774: Epoch 56
2024-11-30 05:38:05.391127: Current learning rate: 0.00949
2024-11-30 05:39:32.988385: Validation loss did not improve from -0.45432. Patience: 27/50
2024-11-30 05:39:32.989343: train_loss -0.6182
2024-11-30 05:39:32.990092: val_loss -0.4233
2024-11-30 05:39:32.990911: Pseudo dice [0.6283]
2024-11-30 05:39:32.991831: Epoch time: 87.6 s
2024-11-30 05:39:32.992691: Yayy! New best EMA pseudo Dice: 0.623
2024-11-30 05:39:34.462035: 
2024-11-30 05:39:34.464526: Epoch 57
2024-11-30 05:39:34.465979: Current learning rate: 0.00949
2024-11-30 05:41:02.112250: Validation loss did not improve from -0.45432. Patience: 28/50
2024-11-30 05:41:02.113823: train_loss -0.6283
2024-11-30 05:41:02.115304: val_loss -0.3923
2024-11-30 05:41:02.116168: Pseudo dice [0.6308]
2024-11-30 05:41:02.117206: Epoch time: 87.65 s
2024-11-30 05:41:02.117943: Yayy! New best EMA pseudo Dice: 0.6238
2024-11-30 05:41:03.599986: 
2024-11-30 05:41:03.601832: Epoch 58
2024-11-30 05:41:03.602880: Current learning rate: 0.00948
2024-11-30 05:42:31.178938: Validation loss did not improve from -0.45432. Patience: 29/50
2024-11-30 05:42:31.180172: train_loss -0.6286
2024-11-30 05:42:31.181241: val_loss -0.4175
2024-11-30 05:42:31.182033: Pseudo dice [0.6297]
2024-11-30 05:42:31.182979: Epoch time: 87.58 s
2024-11-30 05:42:31.183887: Yayy! New best EMA pseudo Dice: 0.6244
2024-11-30 05:42:32.682523: 
2024-11-30 05:42:32.684844: Epoch 59
2024-11-30 05:42:32.685999: Current learning rate: 0.00947
2024-11-30 05:44:00.210633: Validation loss did not improve from -0.45432. Patience: 30/50
2024-11-30 05:44:00.211620: train_loss -0.625
2024-11-30 05:44:00.212907: val_loss -0.4267
2024-11-30 05:44:00.213987: Pseudo dice [0.6409]
2024-11-30 05:44:00.214929: Epoch time: 87.53 s
2024-11-30 05:44:00.577311: Yayy! New best EMA pseudo Dice: 0.626
2024-11-30 05:44:02.075695: 
2024-11-30 05:44:02.077805: Epoch 60
2024-11-30 05:44:02.078795: Current learning rate: 0.00946
2024-11-30 05:45:29.515247: Validation loss did not improve from -0.45432. Patience: 31/50
2024-11-30 05:45:29.516306: train_loss -0.6223
2024-11-30 05:45:29.517647: val_loss -0.3636
2024-11-30 05:45:29.518635: Pseudo dice [0.5899]
2024-11-30 05:45:29.519422: Epoch time: 87.44 s
2024-11-30 05:45:30.696913: 
2024-11-30 05:45:30.699094: Epoch 61
2024-11-30 05:45:30.700222: Current learning rate: 0.00945
2024-11-30 05:46:58.086621: Validation loss did not improve from -0.45432. Patience: 32/50
2024-11-30 05:46:58.087717: train_loss -0.6277
2024-11-30 05:46:58.089064: val_loss -0.3592
2024-11-30 05:46:58.090093: Pseudo dice [0.5948]
2024-11-30 05:46:58.090983: Epoch time: 87.39 s
2024-11-30 05:46:59.248616: 
2024-11-30 05:46:59.250762: Epoch 62
2024-11-30 05:46:59.251796: Current learning rate: 0.00944
2024-11-30 05:48:26.611456: Validation loss did not improve from -0.45432. Patience: 33/50
2024-11-30 05:48:26.612208: train_loss -0.6219
2024-11-30 05:48:26.613062: val_loss -0.4235
2024-11-30 05:48:26.613855: Pseudo dice [0.6311]
2024-11-30 05:48:26.614678: Epoch time: 87.36 s
2024-11-30 05:48:28.089785: 
2024-11-30 05:48:28.091603: Epoch 63
2024-11-30 05:48:28.092818: Current learning rate: 0.00943
2024-11-30 05:49:55.604933: Validation loss did not improve from -0.45432. Patience: 34/50
2024-11-30 05:49:55.605745: train_loss -0.6252
2024-11-30 05:49:55.607136: val_loss -0.4206
2024-11-30 05:49:55.607980: Pseudo dice [0.6328]
2024-11-30 05:49:55.608817: Epoch time: 87.52 s
2024-11-30 05:49:56.755725: 
2024-11-30 05:49:56.757893: Epoch 64
2024-11-30 05:49:56.758903: Current learning rate: 0.00942
2024-11-30 05:51:24.170822: Validation loss did not improve from -0.45432. Patience: 35/50
2024-11-30 05:51:24.172067: train_loss -0.6255
2024-11-30 05:51:24.173126: val_loss -0.4252
2024-11-30 05:51:24.174072: Pseudo dice [0.634]
2024-11-30 05:51:24.174848: Epoch time: 87.42 s
2024-11-30 05:51:25.697222: 
2024-11-30 05:51:25.699415: Epoch 65
2024-11-30 05:51:25.700730: Current learning rate: 0.00941
2024-11-30 05:52:53.063353: Validation loss did not improve from -0.45432. Patience: 36/50
2024-11-30 05:52:53.064915: train_loss -0.6364
2024-11-30 05:52:53.066103: val_loss -0.4469
2024-11-30 05:52:53.067193: Pseudo dice [0.6475]
2024-11-30 05:52:53.068040: Epoch time: 87.37 s
2024-11-30 05:52:54.220688: 
2024-11-30 05:52:54.222851: Epoch 66
2024-11-30 05:52:54.224312: Current learning rate: 0.0094
2024-11-30 05:54:21.569489: Validation loss did not improve from -0.45432. Patience: 37/50
2024-11-30 05:54:21.570775: train_loss -0.6321
2024-11-30 05:54:21.571589: val_loss -0.4367
2024-11-30 05:54:21.572643: Pseudo dice [0.6502]
2024-11-30 05:54:21.573567: Epoch time: 87.35 s
2024-11-30 05:54:21.574501: Yayy! New best EMA pseudo Dice: 0.6281
2024-11-30 05:54:23.110707: 
2024-11-30 05:54:23.112558: Epoch 67
2024-11-30 05:54:23.113235: Current learning rate: 0.00939
2024-11-30 05:55:50.564374: Validation loss did not improve from -0.45432. Patience: 38/50
2024-11-30 05:55:50.565709: train_loss -0.6337
2024-11-30 05:55:50.566680: val_loss -0.4211
2024-11-30 05:55:50.567306: Pseudo dice [0.6375]
2024-11-30 05:55:50.568068: Epoch time: 87.46 s
2024-11-30 05:55:50.568769: Yayy! New best EMA pseudo Dice: 0.629
2024-11-30 05:55:52.211059: 
2024-11-30 05:55:52.213215: Epoch 68
2024-11-30 05:55:52.214294: Current learning rate: 0.00939
2024-11-30 05:57:19.656413: Validation loss did not improve from -0.45432. Patience: 39/50
2024-11-30 05:57:19.657350: train_loss -0.6339
2024-11-30 05:57:19.658641: val_loss -0.3789
2024-11-30 05:57:19.660017: Pseudo dice [0.6084]
2024-11-30 05:57:19.661417: Epoch time: 87.45 s
2024-11-30 05:57:20.835820: 
2024-11-30 05:57:20.838098: Epoch 69
2024-11-30 05:57:20.839602: Current learning rate: 0.00938
2024-11-30 05:58:48.235064: Validation loss did not improve from -0.45432. Patience: 40/50
2024-11-30 05:58:48.235874: train_loss -0.6366
2024-11-30 05:58:48.237145: val_loss -0.3844
2024-11-30 05:58:48.238249: Pseudo dice [0.6105]
2024-11-30 05:58:48.239075: Epoch time: 87.4 s
2024-11-30 05:58:49.775395: 
2024-11-30 05:58:49.778162: Epoch 70
2024-11-30 05:58:49.779403: Current learning rate: 0.00937
2024-11-30 06:00:17.133494: Validation loss did not improve from -0.45432. Patience: 41/50
2024-11-30 06:00:17.134596: train_loss -0.649
2024-11-30 06:00:17.135597: val_loss -0.3831
2024-11-30 06:00:17.136515: Pseudo dice [0.6226]
2024-11-30 06:00:17.137208: Epoch time: 87.36 s
2024-11-30 06:00:18.321520: 
2024-11-30 06:00:18.323600: Epoch 71
2024-11-30 06:00:18.324447: Current learning rate: 0.00936
2024-11-30 06:01:45.698876: Validation loss did not improve from -0.45432. Patience: 42/50
2024-11-30 06:01:45.700220: train_loss -0.6402
2024-11-30 06:01:45.701457: val_loss -0.442
2024-11-30 06:01:45.702822: Pseudo dice [0.6483]
2024-11-30 06:01:45.703808: Epoch time: 87.38 s
2024-11-30 06:01:46.904449: 
2024-11-30 06:01:46.906539: Epoch 72
2024-11-30 06:01:46.907704: Current learning rate: 0.00935
2024-11-30 06:03:14.362559: Validation loss did not improve from -0.45432. Patience: 43/50
2024-11-30 06:03:14.363556: train_loss -0.6456
2024-11-30 06:03:14.364758: val_loss -0.3863
2024-11-30 06:03:14.365646: Pseudo dice [0.6244]
2024-11-30 06:03:14.366582: Epoch time: 87.46 s
2024-11-30 06:03:15.552897: 
2024-11-30 06:03:15.554981: Epoch 73
2024-11-30 06:03:15.556058: Current learning rate: 0.00934
2024-11-30 06:04:43.002532: Validation loss did not improve from -0.45432. Patience: 44/50
2024-11-30 06:04:43.003832: train_loss -0.6419
2024-11-30 06:04:43.005221: val_loss -0.3681
2024-11-30 06:04:43.006443: Pseudo dice [0.6136]
2024-11-30 06:04:43.007546: Epoch time: 87.45 s
2024-11-30 06:04:44.513770: 
2024-11-30 06:04:44.516041: Epoch 74
2024-11-30 06:04:44.517040: Current learning rate: 0.00933
2024-11-30 06:06:12.065912: Validation loss did not improve from -0.45432. Patience: 45/50
2024-11-30 06:06:12.067308: train_loss -0.6485
2024-11-30 06:06:12.068485: val_loss -0.44
2024-11-30 06:06:12.069609: Pseudo dice [0.6447]
2024-11-30 06:06:12.070928: Epoch time: 87.55 s
2024-11-30 06:06:13.559163: 
2024-11-30 06:06:13.561291: Epoch 75
2024-11-30 06:06:13.562696: Current learning rate: 0.00932
2024-11-30 06:07:41.035222: Validation loss did not improve from -0.45432. Patience: 46/50
2024-11-30 06:07:41.036818: train_loss -0.6485
2024-11-30 06:07:41.038048: val_loss -0.387
2024-11-30 06:07:41.039014: Pseudo dice [0.6234]
2024-11-30 06:07:41.040102: Epoch time: 87.48 s
2024-11-30 06:07:42.227616: 
2024-11-30 06:07:42.229573: Epoch 76
2024-11-30 06:07:42.230634: Current learning rate: 0.00931
2024-11-30 06:09:09.573622: Validation loss did not improve from -0.45432. Patience: 47/50
2024-11-30 06:09:09.574949: train_loss -0.6551
2024-11-30 06:09:09.576426: val_loss -0.4016
2024-11-30 06:09:09.577746: Pseudo dice [0.6226]
2024-11-30 06:09:09.579118: Epoch time: 87.35 s
2024-11-30 06:09:10.759090: 
2024-11-30 06:09:10.760989: Epoch 77
2024-11-30 06:09:10.762420: Current learning rate: 0.0093
2024-11-30 06:10:38.050294: Validation loss did not improve from -0.45432. Patience: 48/50
2024-11-30 06:10:38.051828: train_loss -0.6498
2024-11-30 06:10:38.053275: val_loss -0.414
2024-11-30 06:10:38.054422: Pseudo dice [0.6343]
2024-11-30 06:10:38.055512: Epoch time: 87.29 s
2024-11-30 06:10:39.239412: 
2024-11-30 06:10:39.241304: Epoch 78
2024-11-30 06:10:39.242284: Current learning rate: 0.0093
2024-11-30 06:12:06.587383: Validation loss did not improve from -0.45432. Patience: 49/50
2024-11-30 06:12:06.588408: train_loss -0.6508
2024-11-30 06:12:06.589646: val_loss -0.3928
2024-11-30 06:12:06.590864: Pseudo dice [0.6241]
2024-11-30 06:12:06.591747: Epoch time: 87.35 s
2024-11-30 06:12:07.807035: 
2024-11-30 06:12:07.809504: Epoch 79
2024-11-30 06:12:07.811123: Current learning rate: 0.00929
2024-11-30 06:13:35.100507: Validation loss did not improve from -0.45432. Patience: 50/50
2024-11-30 06:13:35.101611: train_loss -0.6487
2024-11-30 06:13:35.102388: val_loss -0.3984
2024-11-30 06:13:35.103199: Pseudo dice [0.6218]
2024-11-30 06:13:35.103901: Epoch time: 87.3 s
2024-11-30 06:13:36.681745: Patience reached. Stopping training.
2024-11-30 06:13:37.077267: Training done.
2024-11-30 06:13:37.193039: Using splits from existing split file: /home/gridsan/nchutisilp/datasets/nnUNet_Datasets/nnUNet_preprocessed/Dataset306_Sohee_Ajay_Calcium_OCT/splits_final.json
2024-11-30 06:13:37.194916: The split file contains 5 splits.
2024-11-30 06:13:37.195843: Desired fold for training: 4
2024-11-30 06:13:37.196686: This split has 11 training and 2 validation cases.
2024-11-30 06:13:37.197636: predicting 04010Pre
2024-11-30 06:13:37.209463: 04010Pre, shape torch.Size([1, 248, 498, 498]), rank 0
2024-11-30 06:14:44.017775: predicting 101-045
2024-11-30 06:14:44.041629: 101-045, shape torch.Size([1, 375, 498, 498]), rank 0
2024-11-30 06:16:33.455478: Validation complete
2024-11-30 06:16:33.457227: Mean Validation Dice:  0.5315655599109503
