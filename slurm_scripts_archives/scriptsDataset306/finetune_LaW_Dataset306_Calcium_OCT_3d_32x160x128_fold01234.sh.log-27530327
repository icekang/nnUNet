/home/gridsan/nchutisilp/.conda/envs/nnunet/bin/python

############################
INFO: You are using the old nnU-Net default plans. We have updated our recommendations. Please consider using those instead! Read more here: https://github.com/MIC-DKFZ/nnUNet/blob/master/documentation/resenc_presets.md
############################

Using device: cuda:0

#######################################################################
Please cite the following paper when using nnU-Net:
Isensee, F., Jaeger, P. F., Kohl, S. A., Petersen, J., & Maier-Hein, K. H. (2021). nnU-Net: a self-configuring method for deep learning-based biomedical image segmentation. Nature methods, 18(2), 203-211.
#######################################################################

2024-12-01 05:25:17.659057: Using torch.compile...

############################
INFO: You are using the old nnU-Net default plans. We have updated our recommendations. Please consider using those instead! Read more here: https://github.com/MIC-DKFZ/nnUNet/blob/master/documentation/resenc_presets.md
############################

Using device: cuda:0

#######################################################################
Please cite the following paper when using nnU-Net:
Isensee, F., Jaeger, P. F., Kohl, S. A., Petersen, J., & Maier-Hein, K. H. (2021). nnU-Net: a self-configuring method for deep learning-based biomedical image segmentation. Nature methods, 18(2), 203-211.
#######################################################################

2024-12-01 05:25:17.659526: Using torch.compile...
/home/gridsan/nchutisilp/.local/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn("The verbose parameter is deprecated. Please use get_last_lr() "
/home/gridsan/nchutisilp/.local/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn("The verbose parameter is deprecated. Please use get_last_lr() "
################### Loading pretrained weights from file  /home/gridsan/nchutisilp/datasets/nnUNet_Datasets/nnUNet_results/Dataset300_Lumen_and_Wall_OCT/nnUNetTrainer__nnUNetPreprocessPlans__3d_fullres/fold_all/checkpoint_best.pth ###################
Below is the list of overlapping blocks in pretrained model and nnUNet architecture:
encoder.stages.0.0.convs.0.conv.weight shape torch.Size([32, 1, 3, 3, 3])
encoder.stages.0.0.convs.0.conv.bias shape torch.Size([32])
encoder.stages.0.0.convs.0.norm.weight shape torch.Size([32])
encoder.stages.0.0.convs.0.norm.bias shape torch.Size([32])
encoder.stages.0.0.convs.0.all_modules.0.weight shape torch.Size([32, 1, 3, 3, 3])
encoder.stages.0.0.convs.0.all_modules.0.bias shape torch.Size([32])
encoder.stages.0.0.convs.0.all_modules.1.weight shape torch.Size([32])
encoder.stages.0.0.convs.0.all_modules.1.bias shape torch.Size([32])
encoder.stages.0.0.convs.1.conv.weight shape torch.Size([32, 32, 3, 3, 3])
encoder.stages.0.0.convs.1.conv.bias shape torch.Size([32])
encoder.stages.0.0.convs.1.norm.weight shape torch.Size([32])
encoder.stages.0.0.convs.1.norm.bias shape torch.Size([32])
encoder.stages.0.0.convs.1.all_modules.0.weight shape torch.Size([32, 32, 3, 3, 3])
encoder.stages.0.0.convs.1.all_modules.0.bias shape torch.Size([32])
encoder.stages.0.0.convs.1.all_modules.1.weight shape torch.Size([32])
encoder.stages.0.0.convs.1.all_modules.1.bias shape torch.Size([32])
encoder.stages.1.0.convs.0.conv.weight shape torch.Size([64, 32, 3, 3, 3])
encoder.stages.1.0.convs.0.conv.bias shape torch.Size([64])
encoder.stages.1.0.convs.0.norm.weight shape torch.Size([64])
encoder.stages.1.0.convs.0.norm.bias shape torch.Size([64])
encoder.stages.1.0.convs.0.all_modules.0.weight shape torch.Size([64, 32, 3, 3, 3])
encoder.stages.1.0.convs.0.all_modules.0.bias shape torch.Size([64])
encoder.stages.1.0.convs.0.all_modules.1.weight shape torch.Size([64])
encoder.stages.1.0.convs.0.all_modules.1.bias shape torch.Size([64])
encoder.stages.1.0.convs.1.conv.weight shape torch.Size([64, 64, 3, 3, 3])
encoder.stages.1.0.convs.1.conv.bias shape torch.Size([64])
encoder.stages.1.0.convs.1.norm.weight shape torch.Size([64])
encoder.stages.1.0.convs.1.norm.bias shape torch.Size([64])
encoder.stages.1.0.convs.1.all_modules.0.weight shape torch.Size([64, 64, 3, 3, 3])
encoder.stages.1.0.convs.1.all_modules.0.bias shape torch.Size([64])
encoder.stages.1.0.convs.1.all_modules.1.weight shape torch.Size([64])
encoder.stages.1.0.convs.1.all_modules.1.bias shape torch.Size([64])
encoder.stages.2.0.convs.0.conv.weight shape torch.Size([128, 64, 3, 3, 3])
encoder.stages.2.0.convs.0.conv.bias shape torch.Size([128])
encoder.stages.2.0.convs.0.norm.weight shape torch.Size([128])
encoder.stages.2.0.convs.0.norm.bias shape torch.Size([128])
encoder.stages.2.0.convs.0.all_modules.0.weight shape torch.Size([128, 64, 3, 3, 3])
encoder.stages.2.0.convs.0.all_modules.0.bias shape torch.Size([128])
encoder.stages.2.0.convs.0.all_modules.1.weight shape torch.Size([128])
encoder.stages.2.0.convs.0.all_modules.1.bias shape torch.Size([128])
encoder.stages.2.0.convs.1.conv.weight shape torch.Size([128, 128, 3, 3, 3])
encoder.stages.2.0.convs.1.conv.bias shape torch.Size([128])
encoder.stages.2.0.convs.1.norm.weight shape torch.Size([128])
encoder.stages.2.0.convs.1.norm.bias shape torch.Size([128])
encoder.stages.2.0.convs.1.all_modules.0.weight shape torch.Size([128, 128, 3, 3, 3])
encoder.stages.2.0.convs.1.all_modules.0.bias shape torch.Size([128])
encoder.stages.2.0.convs.1.all_modules.1.weight shape torch.Size([128])
encoder.stages.2.0.convs.1.all_modules.1.bias shape torch.Size([128])
encoder.stages.3.0.convs.0.conv.weight shape torch.Size([256, 128, 3, 3, 3])
encoder.stages.3.0.convs.0.conv.bias shape torch.Size([256])
encoder.stages.3.0.convs.0.norm.weight shape torch.Size([256])
encoder.stages.3.0.convs.0.norm.bias shape torch.Size([256])
encoder.stages.3.0.convs.0.all_modules.0.weight shape torch.Size([256, 128, 3, 3, 3])
encoder.stages.3.0.convs.0.all_modules.0.bias shape torch.Size([256])
encoder.stages.3.0.convs.0.all_modules.1.weight shape torch.Size([256])
encoder.stages.3.0.convs.0.all_modules.1.bias shape torch.Size([256])
encoder.stages.3.0.convs.1.conv.weight shape torch.Size([256, 256, 3, 3, 3])
encoder.stages.3.0.convs.1.conv.bias shape torch.Size([256])
encoder.stages.3.0.convs.1.norm.weight shape torch.Size([256])
encoder.stages.3.0.convs.1.norm.bias shape torch.Size([256])
encoder.stages.3.0.convs.1.all_modules.0.weight shape torch.Size([256, 256, 3, 3, 3])
encoder.stages.3.0.convs.1.all_modules.0.bias shape torch.Size([256])
encoder.stages.3.0.convs.1.all_modules.1.weight shape torch.Size([256])
encoder.stages.3.0.convs.1.all_modules.1.bias shape torch.Size([256])
encoder.stages.4.0.convs.0.conv.weight shape torch.Size([320, 256, 3, 3, 3])
encoder.stages.4.0.convs.0.conv.bias shape torch.Size([320])
encoder.stages.4.0.convs.0.norm.weight shape torch.Size([320])
encoder.stages.4.0.convs.0.norm.bias shape torch.Size([320])
encoder.stages.4.0.convs.0.all_modules.0.weight shape torch.Size([320, 256, 3, 3, 3])
encoder.stages.4.0.convs.0.all_modules.0.bias shape torch.Size([320])
encoder.stages.4.0.convs.0.all_modules.1.weight shape torch.Size([320])
encoder.stages.4.0.convs.0.all_modules.1.bias shape torch.Size([320])
encoder.stages.4.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.4.0.convs.1.conv.bias shape torch.Size([320])
encoder.stages.4.0.convs.1.norm.weight shape torch.Size([320])
encoder.stages.4.0.convs.1.norm.bias shape torch.Size([320])
encoder.stages.4.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.4.0.convs.1.all_modules.0.bias shape torch.Size([320])
encoder.stages.4.0.convs.1.all_modules.1.weight shape torch.Size([320])
encoder.stages.4.0.convs.1.all_modules.1.bias shape torch.Size([320])
encoder.stages.5.0.convs.0.conv.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.5.0.convs.0.conv.bias shape torch.Size([320])
encoder.stages.5.0.convs.0.norm.weight shape torch.Size([320])
encoder.stages.5.0.convs.0.norm.bias shape torch.Size([320])
encoder.stages.5.0.convs.0.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.5.0.convs.0.all_modules.0.bias shape torch.Size([320])
encoder.stages.5.0.convs.0.all_modules.1.weight shape torch.Size([320])
encoder.stages.5.0.convs.0.all_modules.1.bias shape torch.Size([320])
encoder.stages.5.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.5.0.convs.1.conv.bias shape torch.Size([320])
encoder.stages.5.0.convs.1.norm.weight shape torch.Size([320])
encoder.stages.5.0.convs.1.norm.bias shape torch.Size([320])
encoder.stages.5.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.5.0.convs.1.all_modules.0.bias shape torch.Size([320])
encoder.stages.5.0.convs.1.all_modules.1.weight shape torch.Size([320])
encoder.stages.5.0.convs.1.all_modules.1.bias shape torch.Size([320])
decoder.encoder.stages.0.0.convs.0.conv.weight shape torch.Size([32, 1, 3, 3, 3])
decoder.encoder.stages.0.0.convs.0.conv.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.norm.weight shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.norm.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.all_modules.0.weight shape torch.Size([32, 1, 3, 3, 3])
decoder.encoder.stages.0.0.convs.0.all_modules.0.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.all_modules.1.weight shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.all_modules.1.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.conv.weight shape torch.Size([32, 32, 3, 3, 3])
decoder.encoder.stages.0.0.convs.1.conv.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.norm.weight shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.norm.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.all_modules.0.weight shape torch.Size([32, 32, 3, 3, 3])
decoder.encoder.stages.0.0.convs.1.all_modules.0.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.all_modules.1.weight shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.all_modules.1.bias shape torch.Size([32])
decoder.encoder.stages.1.0.convs.0.conv.weight shape torch.Size([64, 32, 3, 3, 3])
decoder.encoder.stages.1.0.convs.0.conv.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.norm.weight shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.norm.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.all_modules.0.weight shape torch.Size([64, 32, 3, 3, 3])
decoder.encoder.stages.1.0.convs.0.all_modules.0.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.all_modules.1.weight shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.all_modules.1.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.conv.weight shape torch.Size([64, 64, 3, 3, 3])
decoder.encoder.stages.1.0.convs.1.conv.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.norm.weight shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.norm.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.all_modules.0.weight shape torch.Size([64, 64, 3, 3, 3])
decoder.encoder.stages.1.0.convs.1.all_modules.0.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.all_modules.1.weight shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.all_modules.1.bias shape torch.Size([64])
decoder.encoder.stages.2.0.convs.0.conv.weight shape torch.Size([128, 64, 3, 3, 3])
decoder.encoder.stages.2.0.convs.0.conv.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.norm.weight shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.norm.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.all_modules.0.weight shape torch.Size([128, 64, 3, 3, 3])
decoder.encoder.stages.2.0.convs.0.all_modules.0.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.all_modules.1.weight shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.all_modules.1.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.conv.weight shape torch.Size([128, 128, 3, 3, 3])
decoder.encoder.stages.2.0.convs.1.conv.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.norm.weight shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.norm.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.all_modules.0.weight shape torch.Size([128, 128, 3, 3, 3])
decoder.encoder.stages.2.0.convs.1.all_modules.0.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.all_modules.1.weight shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.all_modules.1.bias shape torch.Size([128])
decoder.encoder.stages.3.0.convs.0.conv.weight shape torch.Size([256, 128, 3, 3, 3])
decoder.encoder.stages.3.0.convs.0.conv.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.norm.weight shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.norm.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.all_modules.0.weight shape torch.Size([256, 128, 3, 3, 3])
decoder.encoder.stages.3.0.convs.0.all_modules.0.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.all_modules.1.weight shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.all_modules.1.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.conv.weight shape torch.Size([256, 256, 3, 3, 3])
decoder.encoder.stages.3.0.convs.1.conv.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.norm.weight shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.norm.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.all_modules.0.weight shape torch.Size([256, 256, 3, 3, 3])
decoder.encoder.stages.3.0.convs.1.all_modules.0.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.all_modules.1.weight shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.all_modules.1.bias shape torch.Size([256])
decoder.encoder.stages.4.0.convs.0.conv.weight shape torch.Size([320, 256, 3, 3, 3])
decoder.encoder.stages.4.0.convs.0.conv.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.norm.weight shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.norm.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.all_modules.0.weight shape torch.Size([320, 256, 3, 3, 3])
decoder.encoder.stages.4.0.convs.0.all_modules.0.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.all_modules.1.weight shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.all_modules.1.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.4.0.convs.1.conv.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.norm.weight shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.norm.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.4.0.convs.1.all_modules.0.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.all_modules.1.weight shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.all_modules.1.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.conv.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.5.0.convs.0.conv.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.norm.weight shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.norm.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.5.0.convs.0.all_modules.0.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.all_modules.1.weight shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.all_modules.1.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.5.0.convs.1.conv.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.norm.weight shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.norm.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.5.0.convs.1.all_modules.0.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.all_modules.1.weight shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.all_modules.1.bias shape torch.Size([320])
decoder.stages.0.convs.0.conv.weight shape torch.Size([320, 640, 3, 3, 3])
decoder.stages.0.convs.0.conv.bias shape torch.Size([320])
decoder.stages.0.convs.0.norm.weight shape torch.Size([320])
decoder.stages.0.convs.0.norm.bias shape torch.Size([320])
decoder.stages.0.convs.0.all_modules.0.weight shape torch.Size([320, 640, 3, 3, 3])
decoder.stages.0.convs.0.all_modules.0.bias shape torch.Size([320])
decoder.stages.0.convs.0.all_modules.1.weight shape torch.Size([320])
decoder.stages.0.convs.0.all_modules.1.bias shape torch.Size([320])
decoder.stages.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.stages.0.convs.1.conv.bias shape torch.Size([320])
decoder.stages.0.convs.1.norm.weight shape torch.Size([320])
decoder.stages.0.convs.1.norm.bias shape torch.Size([320])
decoder.stages.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.stages.0.convs.1.all_modules.0.bias shape torch.Size([320])
decoder.stages.0.convs.1.all_modules.1.weight shape torch.Size([320])
decoder.stages.0.convs.1.all_modules.1.bias shape torch.Size([320])
decoder.stages.1.convs.0.conv.weight shape torch.Size([256, 512, 3, 3, 3])
decoder.stages.1.convs.0.conv.bias shape torch.Size([256])
decoder.stages.1.convs.0.norm.weight shape torch.Size([256])
decoder.stages.1.convs.0.norm.bias shape torch.Size([256])
decoder.stages.1.convs.0.all_modules.0.weight shape torch.Size([256, 512, 3, 3, 3])
decoder.stages.1.convs.0.all_modules.0.bias shape torch.Size([256])
decoder.stages.1.convs.0.all_modules.1.weight shape torch.Size([256])
decoder.stages.1.convs.0.all_modules.1.bias shape torch.Size([256])
decoder.stages.1.convs.1.conv.weight shape torch.Size([256, 256, 3, 3, 3])
decoder.stages.1.convs.1.conv.bias shape torch.Size([256])
decoder.stages.1.convs.1.norm.weight shape torch.Size([256])
decoder.stages.1.convs.1.norm.bias shape torch.Size([256])
decoder.stages.1.convs.1.all_modules.0.weight shape torch.Size([256, 256, 3, 3, 3])
decoder.stages.1.convs.1.all_modules.0.bias shape torch.Size([256])
decoder.stages.1.convs.1.all_modules.1.weight shape torch.Size([256])
decoder.stages.1.convs.1.all_modules.1.bias shape torch.Size([256])
decoder.stages.2.convs.0.conv.weight shape torch.Size([128, 256, 3, 3, 3])
decoder.stages.2.convs.0.conv.bias shape torch.Size([128])
decoder.stages.2.convs.0.norm.weight shape torch.Size([128])
decoder.stages.2.convs.0.norm.bias shape torch.Size([128])
decoder.stages.2.convs.0.all_modules.0.weight shape torch.Size([128, 256, 3, 3, 3])
decoder.stages.2.convs.0.all_modules.0.bias shape torch.Size([128])
decoder.stages.2.convs.0.all_modules.1.weight shape torch.Size([128])
decoder.stages.2.convs.0.all_modules.1.bias shape torch.Size([128])
decoder.stages.2.convs.1.conv.weight shape torch.Size([128, 128, 3, 3, 3])
decoder.stages.2.convs.1.conv.bias shape torch.Size([128])
decoder.stages.2.convs.1.norm.weight shape torch.Size([128])
decoder.stages.2.convs.1.norm.bias shape torch.Size([128])
decoder.stages.2.convs.1.all_modules.0.weight shape torch.Size([128, 128, 3, 3, 3])
decoder.stages.2.convs.1.all_modules.0.bias shape torch.Size([128])
decoder.stages.2.convs.1.all_modules.1.weight shape torch.Size([128])
decoder.stages.2.convs.1.all_modules.1.bias shape torch.Size([128])
decoder.stages.3.convs.0.conv.weight shape torch.Size([64, 128, 3, 3, 3])
decoder.stages.3.convs.0.conv.bias shape torch.Size([64])
decoder.stages.3.convs.0.norm.weight shape torch.Size([64])
decoder.stages.3.convs.0.norm.bias shape torch.Size([64])
decoder.stages.3.convs.0.all_modules.0.weight shape torch.Size([64, 128, 3, 3, 3])
decoder.stages.3.convs.0.all_modules.0.bias shape torch.Size([64])
decoder.stages.3.convs.0.all_modules.1.weight shape torch.Size([64])
decoder.stages.3.convs.0.all_modules.1.bias shape torch.Size([64])
decoder.stages.3.convs.1.conv.weight shape torch.Size([64, 64, 3, 3, 3])
decoder.stages.3.convs.1.conv.bias shape torch.Size([64])
decoder.stages.3.convs.1.norm.weight shape torch.Size([64])
decoder.stages.3.convs.1.norm.bias shape torch.Size([64])
decoder.stages.3.convs.1.all_modules.0.weight shape torch.Size([64, 64, 3, 3, 3])
decoder.stages.3.convs.1.all_modules.0.bias shape torch.Size([64])
decoder.stages.3.convs.1.all_modules.1.weight shape torch.Size([64])
decoder.stages.3.convs.1.all_modules.1.bias shape torch.Size([64])
decoder.stages.4.convs.0.conv.weight shape torch.Size([32, 64, 3, 3, 3])
decoder.stages.4.convs.0.conv.bias shape torch.Size([32])
decoder.stages.4.convs.0.norm.weight shape torch.Size([32])
decoder.stages.4.convs.0.norm.bias shape torch.Size([32])
decoder.stages.4.convs.0.all_modules.0.weight shape torch.Size([32, 64, 3, 3, 3])
decoder.stages.4.convs.0.all_modules.0.bias shape torch.Size([32])
decoder.stages.4.convs.0.all_modules.1.weight shape torch.Size([32])
decoder.stages.4.convs.0.all_modules.1.bias shape torch.Size([32])
decoder.stages.4.convs.1.conv.weight shape torch.Size([32, 32, 3, 3, 3])
decoder.stages.4.convs.1.conv.bias shape torch.Size([32])
decoder.stages.4.convs.1.norm.weight shape torch.Size([32])
decoder.stages.4.convs.1.norm.bias shape torch.Size([32])
decoder.stages.4.convs.1.all_modules.0.weight shape torch.Size([32, 32, 3, 3, 3])
decoder.stages.4.convs.1.all_modules.0.bias shape torch.Size([32])
decoder.stages.4.convs.1.all_modules.1.weight shape torch.Size([32])
decoder.stages.4.convs.1.all_modules.1.bias shape torch.Size([32])
decoder.transpconvs.0.weight shape torch.Size([320, 320, 1, 2, 2])
decoder.transpconvs.0.bias shape torch.Size([320])
decoder.transpconvs.1.weight shape torch.Size([320, 256, 2, 2, 2])
decoder.transpconvs.1.bias shape torch.Size([256])
decoder.transpconvs.2.weight shape torch.Size([256, 128, 2, 2, 2])
decoder.transpconvs.2.bias shape torch.Size([128])
decoder.transpconvs.3.weight shape torch.Size([128, 64, 2, 2, 2])
decoder.transpconvs.3.bias shape torch.Size([64])
decoder.transpconvs.4.weight shape torch.Size([64, 32, 2, 2, 2])
decoder.transpconvs.4.bias shape torch.Size([32])
################### Done ###################
2024-12-01 05:25:34.976900: do_dummy_2d_data_aug: True
2024-12-01 05:25:35.012380: Using splits from existing split file: /home/gridsan/nchutisilp/datasets/nnUNet_Datasets/nnUNet_preprocessed/Dataset306_Sohee_Ajay_Calcium_OCT/splits_final.json
2024-12-01 05:25:35.028661: The split file contains 5 splits.
2024-12-01 05:25:35.030767: Desired fold for training: 0
2024-12-01 05:25:35.031931: This split has 10 training and 3 validation cases.
################### Loading pretrained weights from file  /home/gridsan/nchutisilp/datasets/nnUNet_Datasets/nnUNet_results/Dataset300_Lumen_and_Wall_OCT/nnUNetTrainer__nnUNetPreprocessPlans__3d_fullres/fold_all/checkpoint_best.pth ###################
Below is the list of overlapping blocks in pretrained model and nnUNet architecture:
encoder.stages.0.0.convs.0.conv.weight shape torch.Size([32, 1, 3, 3, 3])
encoder.stages.0.0.convs.0.conv.bias shape torch.Size([32])
encoder.stages.0.0.convs.0.norm.weight shape torch.Size([32])
encoder.stages.0.0.convs.0.norm.bias shape torch.Size([32])
encoder.stages.0.0.convs.0.all_modules.0.weight shape torch.Size([32, 1, 3, 3, 3])
encoder.stages.0.0.convs.0.all_modules.0.bias shape torch.Size([32])
encoder.stages.0.0.convs.0.all_modules.1.weight shape torch.Size([32])
encoder.stages.0.0.convs.0.all_modules.1.bias shape torch.Size([32])
encoder.stages.0.0.convs.1.conv.weight shape torch.Size([32, 32, 3, 3, 3])
encoder.stages.0.0.convs.1.conv.bias shape torch.Size([32])
encoder.stages.0.0.convs.1.norm.weight shape torch.Size([32])
encoder.stages.0.0.convs.1.norm.bias shape torch.Size([32])
encoder.stages.0.0.convs.1.all_modules.0.weight shape torch.Size([32, 32, 3, 3, 3])
encoder.stages.0.0.convs.1.all_modules.0.bias shape torch.Size([32])
encoder.stages.0.0.convs.1.all_modules.1.weight shape torch.Size([32])
encoder.stages.0.0.convs.1.all_modules.1.bias shape torch.Size([32])
encoder.stages.1.0.convs.0.conv.weight shape torch.Size([64, 32, 3, 3, 3])
encoder.stages.1.0.convs.0.conv.bias shape torch.Size([64])
encoder.stages.1.0.convs.0.norm.weight shape torch.Size([64])
encoder.stages.1.0.convs.0.norm.bias shape torch.Size([64])
encoder.stages.1.0.convs.0.all_modules.0.weight shape torch.Size([64, 32, 3, 3, 3])
encoder.stages.1.0.convs.0.all_modules.0.bias shape torch.Size([64])
encoder.stages.1.0.convs.0.all_modules.1.weight shape torch.Size([64])
encoder.stages.1.0.convs.0.all_modules.1.bias shape torch.Size([64])
encoder.stages.1.0.convs.1.conv.weight shape torch.Size([64, 64, 3, 3, 3])
encoder.stages.1.0.convs.1.conv.bias shape torch.Size([64])
encoder.stages.1.0.convs.1.norm.weight shape torch.Size([64])
encoder.stages.1.0.convs.1.norm.bias shape torch.Size([64])
encoder.stages.1.0.convs.1.all_modules.0.weight shape torch.Size([64, 64, 3, 3, 3])
encoder.stages.1.0.convs.1.all_modules.0.bias shape torch.Size([64])
encoder.stages.1.0.convs.1.all_modules.1.weight shape torch.Size([64])
encoder.stages.1.0.convs.1.all_modules.1.bias shape torch.Size([64])
encoder.stages.2.0.convs.0.conv.weight shape torch.Size([128, 64, 3, 3, 3])
encoder.stages.2.0.convs.0.conv.bias shape torch.Size([128])
encoder.stages.2.0.convs.0.norm.weight shape torch.Size([128])
encoder.stages.2.0.convs.0.norm.bias shape torch.Size([128])
encoder.stages.2.0.convs.0.all_modules.0.weight shape torch.Size([128, 64, 3, 3, 3])
encoder.stages.2.0.convs.0.all_modules.0.bias shape torch.Size([128])
encoder.stages.2.0.convs.0.all_modules.1.weight shape torch.Size([128])
encoder.stages.2.0.convs.0.all_modules.1.bias shape torch.Size([128])
encoder.stages.2.0.convs.1.conv.weight shape torch.Size([128, 128, 3, 3, 3])
encoder.stages.2.0.convs.1.conv.bias shape torch.Size([128])
encoder.stages.2.0.convs.1.norm.weight shape torch.Size([128])
encoder.stages.2.0.convs.1.norm.bias shape torch.Size([128])
encoder.stages.2.0.convs.1.all_modules.0.weight shape torch.Size([128, 128, 3, 3, 3])
encoder.stages.2.0.convs.1.all_modules.0.bias shape torch.Size([128])
encoder.stages.2.0.convs.1.all_modules.1.weight shape torch.Size([128])
encoder.stages.2.0.convs.1.all_modules.1.bias shape torch.Size([128])
encoder.stages.3.0.convs.0.conv.weight shape torch.Size([256, 128, 3, 3, 3])
encoder.stages.3.0.convs.0.conv.bias shape torch.Size([256])
encoder.stages.3.0.convs.0.norm.weight shape torch.Size([256])
encoder.stages.3.0.convs.0.norm.bias shape torch.Size([256])
encoder.stages.3.0.convs.0.all_modules.0.weight shape torch.Size([256, 128, 3, 3, 3])
encoder.stages.3.0.convs.0.all_modules.0.bias shape torch.Size([256])
encoder.stages.3.0.convs.0.all_modules.1.weight shape torch.Size([256])
encoder.stages.3.0.convs.0.all_modules.1.bias shape torch.Size([256])
encoder.stages.3.0.convs.1.conv.weight shape torch.Size([256, 256, 3, 3, 3])
encoder.stages.3.0.convs.1.conv.bias shape torch.Size([256])
encoder.stages.3.0.convs.1.norm.weight shape torch.Size([256])
encoder.stages.3.0.convs.1.norm.bias shape torch.Size([256])
encoder.stages.3.0.convs.1.all_modules.0.weight shape torch.Size([256, 256, 3, 3, 3])
encoder.stages.3.0.convs.1.all_modules.0.bias shape torch.Size([256])
encoder.stages.3.0.convs.1.all_modules.1.weight shape torch.Size([256])
encoder.stages.3.0.convs.1.all_modules.1.bias shape torch.Size([256])
encoder.stages.4.0.convs.0.conv.weight shape torch.Size([320, 256, 3, 3, 3])
encoder.stages.4.0.convs.0.conv.bias shape torch.Size([320])
encoder.stages.4.0.convs.0.norm.weight shape torch.Size([320])
encoder.stages.4.0.convs.0.norm.bias shape torch.Size([320])
encoder.stages.4.0.convs.0.all_modules.0.weight shape torch.Size([320, 256, 3, 3, 3])
encoder.stages.4.0.convs.0.all_modules.0.bias shape torch.Size([320])
encoder.stages.4.0.convs.0.all_modules.1.weight shape torch.Size([320])
encoder.stages.4.0.convs.0.all_modules.1.bias shape torch.Size([320])
encoder.stages.4.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.4.0.convs.1.conv.bias shape torch.Size([320])
encoder.stages.4.0.convs.1.norm.weight shape torch.Size([320])
encoder.stages.4.0.convs.1.norm.bias shape torch.Size([320])
encoder.stages.4.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.4.0.convs.1.all_modules.0.bias shape torch.Size([320])
encoder.stages.4.0.convs.1.all_modules.1.weight shape torch.Size([320])
encoder.stages.4.0.convs.1.all_modules.1.bias shape torch.Size([320])
encoder.stages.5.0.convs.0.conv.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.5.0.convs.0.conv.bias shape torch.Size([320])
encoder.stages.5.0.convs.0.norm.weight shape torch.Size([320])
encoder.stages.5.0.convs.0.norm.bias shape torch.Size([320])
encoder.stages.5.0.convs.0.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.5.0.convs.0.all_modules.0.bias shape torch.Size([320])
encoder.stages.5.0.convs.0.all_modules.1.weight shape torch.Size([320])
encoder.stages.5.0.convs.0.all_modules.1.bias shape torch.Size([320])
encoder.stages.5.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.5.0.convs.1.conv.bias shape torch.Size([320])
encoder.stages.5.0.convs.1.norm.weight shape torch.Size([320])
encoder.stages.5.0.convs.1.norm.bias shape torch.Size([320])
encoder.stages.5.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.5.0.convs.1.all_modules.0.bias shape torch.Size([320])
encoder.stages.5.0.convs.1.all_modules.1.weight shape torch.Size([320])
encoder.stages.5.0.convs.1.all_modules.1.bias shape torch.Size([320])
decoder.encoder.stages.0.0.convs.0.conv.weight shape torch.Size([32, 1, 3, 3, 3])
decoder.encoder.stages.0.0.convs.0.conv.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.norm.weight shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.norm.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.all_modules.0.weight shape torch.Size([32, 1, 3, 3, 3])
decoder.encoder.stages.0.0.convs.0.all_modules.0.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.all_modules.1.weight shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.all_modules.1.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.conv.weight shape torch.Size([32, 32, 3, 3, 3])
decoder.encoder.stages.0.0.convs.1.conv.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.norm.weight shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.norm.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.all_modules.0.weight shape torch.Size([32, 32, 3, 3, 3])
decoder.encoder.stages.0.0.convs.1.all_modules.0.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.all_modules.1.weight shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.all_modules.1.bias shape torch.Size([32])
decoder.encoder.stages.1.0.convs.0.conv.weight shape torch.Size([64, 32, 3, 3, 3])
decoder.encoder.stages.1.0.convs.0.conv.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.norm.weight shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.norm.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.all_modules.0.weight shape torch.Size([64, 32, 3, 3, 3])
decoder.encoder.stages.1.0.convs.0.all_modules.0.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.all_modules.1.weight shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.all_modules.1.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.conv.weight shape torch.Size([64, 64, 3, 3, 3])
decoder.encoder.stages.1.0.convs.1.conv.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.norm.weight shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.norm.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.all_modules.0.weight shape torch.Size([64, 64, 3, 3, 3])
decoder.encoder.stages.1.0.convs.1.all_modules.0.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.all_modules.1.weight shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.all_modules.1.bias shape torch.Size([64])
decoder.encoder.stages.2.0.convs.0.conv.weight shape torch.Size([128, 64, 3, 3, 3])
decoder.encoder.stages.2.0.convs.0.conv.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.norm.weight shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.norm.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.all_modules.0.weight shape torch.Size([128, 64, 3, 3, 3])
decoder.encoder.stages.2.0.convs.0.all_modules.0.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.all_modules.1.weight shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.all_modules.1.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.conv.weight shape torch.Size([128, 128, 3, 3, 3])
decoder.encoder.stages.2.0.convs.1.conv.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.norm.weight shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.norm.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.all_modules.0.weight shape torch.Size([128, 128, 3, 3, 3])
decoder.encoder.stages.2.0.convs.1.all_modules.0.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.all_modules.1.weight shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.all_modules.1.bias shape torch.Size([128])
decoder.encoder.stages.3.0.convs.0.conv.weight shape torch.Size([256, 128, 3, 3, 3])
decoder.encoder.stages.3.0.convs.0.conv.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.norm.weight shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.norm.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.all_modules.0.weight shape torch.Size([256, 128, 3, 3, 3])
decoder.encoder.stages.3.0.convs.0.all_modules.0.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.all_modules.1.weight shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.all_modules.1.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.conv.weight shape torch.Size([256, 256, 3, 3, 3])
decoder.encoder.stages.3.0.convs.1.conv.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.norm.weight shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.norm.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.all_modules.0.weight shape torch.Size([256, 256, 3, 3, 3])
decoder.encoder.stages.3.0.convs.1.all_modules.0.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.all_modules.1.weight shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.all_modules.1.bias shape torch.Size([256])
decoder.encoder.stages.4.0.convs.0.conv.weight shape torch.Size([320, 256, 3, 3, 3])
decoder.encoder.stages.4.0.convs.0.conv.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.norm.weight shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.norm.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.all_modules.0.weight shape torch.Size([320, 256, 3, 3, 3])
decoder.encoder.stages.4.0.convs.0.all_modules.0.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.all_modules.1.weight shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.all_modules.1.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.4.0.convs.1.conv.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.norm.weight shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.norm.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.4.0.convs.1.all_modules.0.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.all_modules.1.weight shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.all_modules.1.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.conv.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.5.0.convs.0.conv.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.norm.weight shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.norm.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.5.0.convs.0.all_modules.0.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.all_modules.1.weight shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.all_modules.1.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.5.0.convs.1.conv.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.norm.weight shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.norm.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.5.0.convs.1.all_modules.0.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.all_modules.1.weight shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.all_modules.1.bias shape torch.Size([320])
decoder.stages.0.convs.0.conv.weight shape torch.Size([320, 640, 3, 3, 3])
decoder.stages.0.convs.0.conv.bias shape torch.Size([320])
decoder.stages.0.convs.0.norm.weight shape torch.Size([320])
decoder.stages.0.convs.0.norm.bias shape torch.Size([320])
decoder.stages.0.convs.0.all_modules.0.weight shape torch.Size([320, 640, 3, 3, 3])
decoder.stages.0.convs.0.all_modules.0.bias shape torch.Size([320])
decoder.stages.0.convs.0.all_modules.1.weight shape torch.Size([320])
decoder.stages.0.convs.0.all_modules.1.bias shape torch.Size([320])
decoder.stages.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.stages.0.convs.1.conv.bias shape torch.Size([320])
decoder.stages.0.convs.1.norm.weight shape torch.Size([320])
decoder.stages.0.convs.1.norm.bias shape torch.Size([320])
decoder.stages.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.stages.0.convs.1.all_modules.0.bias shape torch.Size([320])
decoder.stages.0.convs.1.all_modules.1.weight shape torch.Size([320])
decoder.stages.0.convs.1.all_modules.1.bias shape torch.Size([320])
decoder.stages.1.convs.0.conv.weight shape torch.Size([256, 512, 3, 3, 3])
decoder.stages.1.convs.0.conv.bias shape torch.Size([256])
decoder.stages.1.convs.0.norm.weight shape torch.Size([256])
decoder.stages.1.convs.0.norm.bias shape torch.Size([256])
decoder.stages.1.convs.0.all_modules.0.weight shape torch.Size([256, 512, 3, 3, 3])
decoder.stages.1.convs.0.all_modules.0.bias shape torch.Size([256])
decoder.stages.1.convs.0.all_modules.1.weight shape torch.Size([256])
decoder.stages.1.convs.0.all_modules.1.bias shape torch.Size([256])
decoder.stages.1.convs.1.conv.weight shape torch.Size([256, 256, 3, 3, 3])
decoder.stages.1.convs.1.conv.bias shape torch.Size([256])
decoder.stages.1.convs.1.norm.weight shape torch.Size([256])
decoder.stages.1.convs.1.norm.bias shape torch.Size([256])
decoder.stages.1.convs.1.all_modules.0.weight shape torch.Size([256, 256, 3, 3, 3])
decoder.stages.1.convs.1.all_modules.0.bias shape torch.Size([256])
decoder.stages.1.convs.1.all_modules.1.weight shape torch.Size([256])
decoder.stages.1.convs.1.all_modules.1.bias shape torch.Size([256])
decoder.stages.2.convs.0.conv.weight shape torch.Size([128, 256, 3, 3, 3])
decoder.stages.2.convs.0.conv.bias shape torch.Size([128])
decoder.stages.2.convs.0.norm.weight shape torch.Size([128])
decoder.stages.2.convs.0.norm.bias shape torch.Size([128])
decoder.stages.2.convs.0.all_modules.0.weight shape torch.Size([128, 256, 3, 3, 3])
decoder.stages.2.convs.0.all_modules.0.bias shape torch.Size([128])
decoder.stages.2.convs.0.all_modules.1.weight shape torch.Size([128])
decoder.stages.2.convs.0.all_modules.1.bias shape torch.Size([128])
decoder.stages.2.convs.1.conv.weight shape torch.Size([128, 128, 3, 3, 3])
decoder.stages.2.convs.1.conv.bias shape torch.Size([128])
decoder.stages.2.convs.1.norm.weight shape torch.Size([128])
decoder.stages.2.convs.1.norm.bias shape torch.Size([128])
decoder.stages.2.convs.1.all_modules.0.weight shape torch.Size([128, 128, 3, 3, 3])
decoder.stages.2.convs.1.all_modules.0.bias shape torch.Size([128])
decoder.stages.2.convs.1.all_modules.1.weight shape torch.Size([128])
decoder.stages.2.convs.1.all_modules.1.bias shape torch.Size([128])
decoder.stages.3.convs.0.conv.weight shape torch.Size([64, 128, 3, 3, 3])
decoder.stages.3.convs.0.conv.bias shape torch.Size([64])
decoder.stages.3.convs.0.norm.weight shape torch.Size([64])
decoder.stages.3.convs.0.norm.bias shape torch.Size([64])
decoder.stages.3.convs.0.all_modules.0.weight shape torch.Size([64, 128, 3, 3, 3])
decoder.stages.3.convs.0.all_modules.0.bias shape torch.Size([64])
decoder.stages.3.convs.0.all_modules.1.weight shape torch.Size([64])
decoder.stages.3.convs.0.all_modules.1.bias shape torch.Size([64])
decoder.stages.3.convs.1.conv.weight shape torch.Size([64, 64, 3, 3, 3])
decoder.stages.3.convs.1.conv.bias shape torch.Size([64])
decoder.stages.3.convs.1.norm.weight shape torch.Size([64])
decoder.stages.3.convs.1.norm.bias shape torch.Size([64])
decoder.stages.3.convs.1.all_modules.0.weight shape torch.Size([64, 64, 3, 3, 3])
decoder.stages.3.convs.1.all_modules.0.bias shape torch.Size([64])
decoder.stages.3.convs.1.all_modules.1.weight shape torch.Size([64])
decoder.stages.3.convs.1.all_modules.1.bias shape torch.Size([64])
decoder.stages.4.convs.0.conv.weight shape torch.Size([32, 64, 3, 3, 3])
decoder.stages.4.convs.0.conv.bias shape torch.Size([32])
decoder.stages.4.convs.0.norm.weight shape torch.Size([32])
decoder.stages.4.convs.0.norm.bias shape torch.Size([32])
decoder.stages.4.convs.0.all_modules.0.weight shape torch.Size([32, 64, 3, 3, 3])
decoder.stages.4.convs.0.all_modules.0.bias shape torch.Size([32])
decoder.stages.4.convs.0.all_modules.1.weight shape torch.Size([32])
decoder.stages.4.convs.0.all_modules.1.bias shape torch.Size([32])
decoder.stages.4.convs.1.conv.weight shape torch.Size([32, 32, 3, 3, 3])
decoder.stages.4.convs.1.conv.bias shape torch.Size([32])
decoder.stages.4.convs.1.norm.weight shape torch.Size([32])
decoder.stages.4.convs.1.norm.bias shape torch.Size([32])
decoder.stages.4.convs.1.all_modules.0.weight shape torch.Size([32, 32, 3, 3, 3])
decoder.stages.4.convs.1.all_modules.0.bias shape torch.Size([32])
decoder.stages.4.convs.1.all_modules.1.weight shape torch.Size([32])
decoder.stages.4.convs.1.all_modules.1.bias shape torch.Size([32])
decoder.transpconvs.0.weight shape torch.Size([320, 320, 1, 2, 2])
decoder.transpconvs.0.bias shape torch.Size([320])
decoder.transpconvs.1.weight shape torch.Size([320, 256, 2, 2, 2])
decoder.transpconvs.1.bias shape torch.Size([256])
decoder.transpconvs.2.weight shape torch.Size([256, 128, 2, 2, 2])
decoder.transpconvs.2.bias shape torch.Size([128])
decoder.transpconvs.3.weight shape torch.Size([128, 64, 2, 2, 2])
decoder.transpconvs.3.bias shape torch.Size([64])
decoder.transpconvs.4.weight shape torch.Size([64, 32, 2, 2, 2])
decoder.transpconvs.4.bias shape torch.Size([32])
################### Done ###################
2024-12-01 05:25:34.976854: do_dummy_2d_data_aug: True
2024-12-01 05:25:35.012343: Using splits from existing split file: /home/gridsan/nchutisilp/datasets/nnUNet_Datasets/nnUNet_preprocessed/Dataset306_Sohee_Ajay_Calcium_OCT/splits_final.json
2024-12-01 05:25:35.029257: The split file contains 5 splits.
2024-12-01 05:25:35.031063: Desired fold for training: 1
2024-12-01 05:25:35.032413: This split has 10 training and 3 validation cases.
using pin_memory on device 0
using pin_memory on device 0
using pin_memory on device 0

This is the configuration used by this training:
Configuration name: 3d_32x160x128_b10
 {'data_identifier': 'nnUNetPlans_3d_fullres', 'preprocessor_name': 'DefaultPreprocessor', 'batch_size': 10, 'patch_size': [32, 160, 128], 'median_image_size_in_voxels': [375.0, 498.0, 498.0], 'spacing': [1.0, 1.0, 1.0], 'normalization_schemes': ['ZScoreNormalization'], 'use_mask_for_norm': [False], 'resampling_fn_data': 'resample_data_or_seg_to_shape', 'resampling_fn_seg': 'resample_data_or_seg_to_shape', 'resampling_fn_data_kwargs': {'is_seg': False, 'order': 3, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_seg_kwargs': {'is_seg': True, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_probabilities': 'resample_data_or_seg_to_shape', 'resampling_fn_probabilities_kwargs': {'is_seg': False, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'architecture': {'network_class_name': 'dynamic_network_architectures.architectures.unet.PlainConvUNet', 'arch_kwargs': {'n_stages': 6, 'features_per_stage': [32, 64, 128, 256, 320, 320], 'conv_op': 'torch.nn.modules.conv.Conv3d', 'kernel_sizes': [[3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3]], 'strides': [[1, 1, 1], [2, 2, 2], [2, 2, 2], [2, 2, 2], [2, 2, 2], [1, 2, 2]], 'n_conv_per_stage': [2, 2, 2, 2, 2, 2], 'n_conv_per_stage_decoder': [2, 2, 2, 2, 2], 'conv_bias': True, 'norm_op': 'torch.nn.modules.instancenorm.InstanceNorm3d', 'norm_op_kwargs': {'eps': 1e-05, 'affine': True}, 'dropout_op': None, 'dropout_op_kwargs': None, 'nonlin': 'torch.nn.LeakyReLU', 'nonlin_kwargs': {'inplace': True}, 'deep_supervision': True}, '_kw_requires_import': ['conv_op', 'norm_op', 'dropout_op', 'nonlin']}, 'batch_dice': True, 'inherits_from': '3d_fullres'} 

These are the global plan.json settings:
 {'dataset_name': 'Dataset306_Sohee_Ajay_Calcium_OCT', 'plans_name': 'nnUNetPlans', 'original_median_spacing_after_transp': [1.0, 1.0, 1.0], 'original_median_shape_after_transp': [375, 498, 498], 'image_reader_writer': 'SimpleITKIO', 'transpose_forward': [0, 1, 2], 'transpose_backward': [0, 1, 2], 'experiment_planner_used': 'ExperimentPlanner', 'label_manager': 'LabelManager', 'foreground_intensity_properties_per_channel': {'0': {'max': 0.49803921580314636, 'mean': 0.14296366274356842, 'median': 0.10980392247438431, 'min': 0.0, 'percentile_00_5': 0.01558823511004448, 'percentile_99_5': 0.49803921580314636, 'std': 0.11562220752239227}}} 

2024-12-01 05:25:48.437900: unpacking dataset...
using pin_memory on device 0

This is the configuration used by this training:
Configuration name: 3d_32x160x128_b10
 {'data_identifier': 'nnUNetPlans_3d_fullres', 'preprocessor_name': 'DefaultPreprocessor', 'batch_size': 10, 'patch_size': [32, 160, 128], 'median_image_size_in_voxels': [375.0, 498.0, 498.0], 'spacing': [1.0, 1.0, 1.0], 'normalization_schemes': ['ZScoreNormalization'], 'use_mask_for_norm': [False], 'resampling_fn_data': 'resample_data_or_seg_to_shape', 'resampling_fn_seg': 'resample_data_or_seg_to_shape', 'resampling_fn_data_kwargs': {'is_seg': False, 'order': 3, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_seg_kwargs': {'is_seg': True, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_probabilities': 'resample_data_or_seg_to_shape', 'resampling_fn_probabilities_kwargs': {'is_seg': False, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'architecture': {'network_class_name': 'dynamic_network_architectures.architectures.unet.PlainConvUNet', 'arch_kwargs': {'n_stages': 6, 'features_per_stage': [32, 64, 128, 256, 320, 320], 'conv_op': 'torch.nn.modules.conv.Conv3d', 'kernel_sizes': [[3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3]], 'strides': [[1, 1, 1], [2, 2, 2], [2, 2, 2], [2, 2, 2], [2, 2, 2], [1, 2, 2]], 'n_conv_per_stage': [2, 2, 2, 2, 2, 2], 'n_conv_per_stage_decoder': [2, 2, 2, 2, 2], 'conv_bias': True, 'norm_op': 'torch.nn.modules.instancenorm.InstanceNorm3d', 'norm_op_kwargs': {'eps': 1e-05, 'affine': True}, 'dropout_op': None, 'dropout_op_kwargs': None, 'nonlin': 'torch.nn.LeakyReLU', 'nonlin_kwargs': {'inplace': True}, 'deep_supervision': True}, '_kw_requires_import': ['conv_op', 'norm_op', 'dropout_op', 'nonlin']}, 'batch_dice': True, 'inherits_from': '3d_fullres'} 

These are the global plan.json settings:
 {'dataset_name': 'Dataset306_Sohee_Ajay_Calcium_OCT', 'plans_name': 'nnUNetPlans', 'original_median_spacing_after_transp': [1.0, 1.0, 1.0], 'original_median_shape_after_transp': [375, 498, 498], 'image_reader_writer': 'SimpleITKIO', 'transpose_forward': [0, 1, 2], 'transpose_backward': [0, 1, 2], 'experiment_planner_used': 'ExperimentPlanner', 'label_manager': 'LabelManager', 'foreground_intensity_properties_per_channel': {'0': {'max': 0.49803921580314636, 'mean': 0.14296366274356842, 'median': 0.10980392247438431, 'min': 0.0, 'percentile_00_5': 0.01558823511004448, 'percentile_99_5': 0.49803921580314636, 'std': 0.11562220752239227}}} 

2024-12-01 05:25:48.716721: unpacking dataset...
2024-12-01 05:25:54.298745: unpacking done...
2024-12-01 05:25:54.408518: Unable to plot network architecture: nnUNet_compile is enabled!
2024-12-01 05:25:54.852174: 
2024-12-01 05:25:54.853077: Epoch 0
2024-12-01 05:25:54.854041: Current learning rate: 0.01
2024-12-01 05:29:30.650732: Validation loss improved from 1000.00000 to -0.40505! Patience: 0/50
2024-12-01 05:29:30.670942: train_loss -0.2873
2024-12-01 05:29:30.681350: val_loss -0.405
2024-12-01 05:29:30.682217: Pseudo dice [0.6996]
2024-12-01 05:29:30.682977: Epoch time: 215.8 s
2024-12-01 05:29:30.683660: Yayy! New best EMA pseudo Dice: 0.6996
2024-12-01 05:29:33.477448: 
2024-12-01 05:29:33.478532: Epoch 1
2024-12-01 05:29:33.479470: Current learning rate: 0.00999
2024-12-01 05:31:00.324300: Validation loss improved from -0.40505 to -0.45096! Patience: 0/50
2024-12-01 05:31:00.325546: train_loss -0.4247
2024-12-01 05:31:00.326703: val_loss -0.451
2024-12-01 05:31:00.327652: Pseudo dice [0.7083]
2024-12-01 05:31:00.328602: Epoch time: 86.85 s
2024-12-01 05:31:00.329422: Yayy! New best EMA pseudo Dice: 0.7005
2024-12-01 05:31:01.919679: 
2024-12-01 05:31:01.921344: Epoch 2
2024-12-01 05:31:01.922331: Current learning rate: 0.00998
2024-12-01 05:32:29.232554: Validation loss did not improve from -0.45096. Patience: 1/50
2024-12-01 05:32:29.233605: train_loss -0.4863
2024-12-01 05:32:29.234340: val_loss -0.4184
2024-12-01 05:32:29.234885: Pseudo dice [0.6913]
2024-12-01 05:32:29.235662: Epoch time: 87.32 s
2024-12-01 05:32:30.567457: 
2024-12-01 05:32:30.569137: Epoch 3
2024-12-01 05:32:30.570486: Current learning rate: 0.00997
2024-12-01 05:33:58.150259: Validation loss improved from -0.45096 to -0.48727! Patience: 1/50
2024-12-01 05:33:58.151113: train_loss -0.4731
2024-12-01 05:33:58.152260: val_loss -0.4873
2024-12-01 05:33:58.152913: Pseudo dice [0.726]
2024-12-01 05:33:58.153440: Epoch time: 87.59 s
2024-12-01 05:33:58.153970: Yayy! New best EMA pseudo Dice: 0.7022
2024-12-01 05:33:59.866244: 
2024-12-01 05:33:59.867374: Epoch 4
2024-12-01 05:33:59.868034: Current learning rate: 0.00996
2024-12-01 05:35:27.725621: Validation loss did not improve from -0.48727. Patience: 1/50
2024-12-01 05:35:27.726466: train_loss -0.4833
2024-12-01 05:35:27.727454: val_loss -0.4664
2024-12-01 05:35:27.728225: Pseudo dice [0.7124]
2024-12-01 05:35:27.728959: Epoch time: 87.86 s
2024-12-01 05:35:28.043294: Yayy! New best EMA pseudo Dice: 0.7032
2024-12-01 05:35:29.697536: 
2024-12-01 05:35:29.698880: Epoch 5
2024-12-01 05:35:29.699480: Current learning rate: 0.00995
2024-12-01 05:36:57.494475: Validation loss did not improve from -0.48727. Patience: 2/50
2024-12-01 05:36:57.495658: train_loss -0.5086
2024-12-01 05:36:57.496524: val_loss -0.4471
2024-12-01 05:36:57.497173: Pseudo dice [0.7]
2024-12-01 05:36:57.497779: Epoch time: 87.8 s
2024-12-01 05:36:58.740404: 
2024-12-01 05:36:58.741925: Epoch 6
2024-12-01 05:36:58.742703: Current learning rate: 0.00995
2024-12-01 05:38:26.550148: Validation loss improved from -0.48727 to -0.53663! Patience: 2/50
2024-12-01 05:38:26.551146: train_loss -0.5348
2024-12-01 05:38:26.552128: val_loss -0.5366
2024-12-01 05:38:26.552868: Pseudo dice [0.7607]
2024-12-01 05:38:26.553711: Epoch time: 87.81 s
2024-12-01 05:38:26.554598: Yayy! New best EMA pseudo Dice: 0.7087
2024-12-01 05:38:28.145161: 
2024-12-01 05:38:28.146726: Epoch 7
2024-12-01 05:38:28.147636: Current learning rate: 0.00994
2024-12-01 05:39:55.938151: Validation loss did not improve from -0.53663. Patience: 1/50
2024-12-01 05:39:55.939185: train_loss -0.5407
2024-12-01 05:39:55.939990: val_loss -0.5366
2024-12-01 05:39:55.940707: Pseudo dice [0.7474]
2024-12-01 05:39:55.941434: Epoch time: 87.79 s
2024-12-01 05:39:55.942017: Yayy! New best EMA pseudo Dice: 0.7126
2024-12-01 05:39:57.608296: 
2024-12-01 05:39:57.609763: Epoch 8
2024-12-01 05:39:57.610725: Current learning rate: 0.00993
2024-12-01 05:41:25.823092: Validation loss did not improve from -0.53663. Patience: 2/50
2024-12-01 05:41:25.824024: train_loss -0.5448
2024-12-01 05:41:25.824749: val_loss -0.5212
2024-12-01 05:41:25.825342: Pseudo dice [0.7509]
2024-12-01 05:41:25.826005: Epoch time: 88.22 s
2024-12-01 05:41:25.826585: Yayy! New best EMA pseudo Dice: 0.7164
2024-12-01 05:41:28.065790: 
2024-12-01 05:41:28.067073: Epoch 9
2024-12-01 05:41:28.067898: Current learning rate: 0.00992
2024-12-01 05:42:56.094122: Validation loss did not improve from -0.53663. Patience: 3/50
2024-12-01 05:42:56.095150: train_loss -0.5488
2024-12-01 05:42:56.096159: val_loss -0.5165
2024-12-01 05:42:56.096846: Pseudo dice [0.7449]
2024-12-01 05:42:56.097606: Epoch time: 88.03 s
2024-12-01 05:42:56.446948: Yayy! New best EMA pseudo Dice: 0.7192
2024-12-01 05:42:58.030404: 
2024-12-01 05:42:58.032168: Epoch 10
2024-12-01 05:42:58.033062: Current learning rate: 0.00991
2024-12-01 05:44:26.112767: Validation loss did not improve from -0.53663. Patience: 4/50
2024-12-01 05:44:26.113492: train_loss -0.5667
2024-12-01 05:44:26.114650: val_loss -0.4814
2024-12-01 05:44:26.115625: Pseudo dice [0.7309]
2024-12-01 05:44:26.116584: Epoch time: 88.08 s
2024-12-01 05:44:26.117621: Yayy! New best EMA pseudo Dice: 0.7204
2024-12-01 05:44:27.777166: 
2024-12-01 05:44:27.778950: Epoch 11
2024-12-01 05:44:27.780024: Current learning rate: 0.0099
2024-12-01 05:45:55.997591: Validation loss did not improve from -0.53663. Patience: 5/50
2024-12-01 05:45:55.998849: train_loss -0.5622
2024-12-01 05:45:55.999827: val_loss -0.4775
2024-12-01 05:45:56.000542: Pseudo dice [0.7155]
2024-12-01 05:45:56.001311: Epoch time: 88.22 s
2024-12-01 05:45:57.281750: 
2024-12-01 05:45:57.283517: Epoch 12
2024-12-01 05:45:57.284441: Current learning rate: 0.00989
2024-12-01 05:47:25.519362: Validation loss did not improve from -0.53663. Patience: 6/50
2024-12-01 05:47:25.520373: train_loss -0.5625
2024-12-01 05:47:25.521107: val_loss -0.4636
2024-12-01 05:47:25.521829: Pseudo dice [0.7164]
2024-12-01 05:47:25.522616: Epoch time: 88.24 s
2024-12-01 05:47:26.815645: 
2024-12-01 05:47:26.816893: Epoch 13
2024-12-01 05:47:26.817690: Current learning rate: 0.00988
2024-12-01 05:48:54.816496: Validation loss did not improve from -0.53663. Patience: 7/50
2024-12-01 05:48:54.817947: train_loss -0.5855
2024-12-01 05:48:54.818982: val_loss -0.5243
2024-12-01 05:48:54.819751: Pseudo dice [0.7522]
2024-12-01 05:48:54.820533: Epoch time: 88.0 s
2024-12-01 05:48:54.821257: Yayy! New best EMA pseudo Dice: 0.7228
2024-12-01 05:48:56.444605: 
2024-12-01 05:48:56.446311: Epoch 14
2024-12-01 05:48:56.446926: Current learning rate: 0.00987
2024-12-01 05:50:24.647343: Validation loss did not improve from -0.53663. Patience: 8/50
2024-12-01 05:50:24.648525: train_loss -0.583
2024-12-01 05:50:24.649433: val_loss -0.529
2024-12-01 05:50:24.650246: Pseudo dice [0.7452]
2024-12-01 05:50:24.651046: Epoch time: 88.2 s
2024-12-01 05:50:25.008812: Yayy! New best EMA pseudo Dice: 0.7251
2024-12-01 05:50:26.672298: 
2024-12-01 05:50:26.673998: Epoch 15
2024-12-01 05:50:26.674993: Current learning rate: 0.00986
2024-12-01 05:51:54.841533: Validation loss did not improve from -0.53663. Patience: 9/50
2024-12-01 05:51:54.842812: train_loss -0.5875
2024-12-01 05:51:54.843972: val_loss -0.519
2024-12-01 05:51:54.844879: Pseudo dice [0.7419]
2024-12-01 05:51:54.845572: Epoch time: 88.17 s
2024-12-01 05:51:54.846237: Yayy! New best EMA pseudo Dice: 0.7268
2024-12-01 05:51:56.513333: 
2024-12-01 05:51:56.514956: Epoch 16
2024-12-01 05:51:56.515626: Current learning rate: 0.00986
2024-12-01 05:53:24.972615: Validation loss did not improve from -0.53663. Patience: 10/50
2024-12-01 05:53:24.973323: train_loss -0.5894
2024-12-01 05:53:24.974059: val_loss -0.4687
2024-12-01 05:53:24.974652: Pseudo dice [0.7185]
2024-12-01 05:53:24.975466: Epoch time: 88.46 s
2024-12-01 05:53:26.349314: 
2024-12-01 05:53:26.350708: Epoch 17
2024-12-01 05:53:26.351437: Current learning rate: 0.00985
2024-12-01 05:54:54.910465: Validation loss improved from -0.53663 to -0.54394! Patience: 10/50
2024-12-01 05:54:54.911191: train_loss -0.5858
2024-12-01 05:54:54.912035: val_loss -0.5439
2024-12-01 05:54:54.912720: Pseudo dice [0.7585]
2024-12-01 05:54:54.913380: Epoch time: 88.56 s
2024-12-01 05:54:54.914289: Yayy! New best EMA pseudo Dice: 0.7292
2024-12-01 05:54:56.631148: 
2024-12-01 05:54:56.632651: Epoch 18
2024-12-01 05:54:56.633425: Current learning rate: 0.00984
2024-12-01 05:56:25.092066: Validation loss improved from -0.54394 to -0.54760! Patience: 0/50
2024-12-01 05:56:25.093389: train_loss -0.604
2024-12-01 05:56:25.094668: val_loss -0.5476
2024-12-01 05:56:25.095642: Pseudo dice [0.7718]
2024-12-01 05:56:25.096633: Epoch time: 88.46 s
2024-12-01 05:56:25.097574: Yayy! New best EMA pseudo Dice: 0.7335
2024-12-01 05:56:27.511086: 
2024-12-01 05:56:27.512664: Epoch 19
2024-12-01 05:56:27.513699: Current learning rate: 0.00983
2024-12-01 05:57:56.034755: Validation loss did not improve from -0.54760. Patience: 1/50
2024-12-01 05:57:56.035774: train_loss -0.598
2024-12-01 05:57:56.036576: val_loss -0.5308
2024-12-01 05:57:56.037240: Pseudo dice [0.7599]
2024-12-01 05:57:56.038089: Epoch time: 88.53 s
2024-12-01 05:57:56.479902: Yayy! New best EMA pseudo Dice: 0.7361
2024-12-01 05:57:58.161967: 
2024-12-01 05:57:58.163509: Epoch 20
2024-12-01 05:57:58.164150: Current learning rate: 0.00982
2024-12-01 05:59:26.739061: Validation loss did not improve from -0.54760. Patience: 2/50
2024-12-01 05:59:26.740329: train_loss -0.607
2024-12-01 05:59:26.741400: val_loss -0.523
2024-12-01 05:59:26.742348: Pseudo dice [0.7491]
2024-12-01 05:59:26.743224: Epoch time: 88.58 s
2024-12-01 05:59:26.744046: Yayy! New best EMA pseudo Dice: 0.7374
2024-12-01 05:59:28.486556: 
2024-12-01 05:59:28.488299: Epoch 21
2024-12-01 05:59:28.489197: Current learning rate: 0.00981
2024-12-01 06:00:57.095839: Validation loss improved from -0.54760 to -0.55901! Patience: 2/50
2024-12-01 06:00:57.096759: train_loss -0.6142
2024-12-01 06:00:57.097597: val_loss -0.559
2024-12-01 06:00:57.098323: Pseudo dice [0.7632]
2024-12-01 06:00:57.098960: Epoch time: 88.61 s
2024-12-01 06:00:57.099613: Yayy! New best EMA pseudo Dice: 0.74
2024-12-01 06:00:58.839348: 
2024-12-01 06:00:58.840805: Epoch 22
2024-12-01 06:00:58.841616: Current learning rate: 0.0098
2024-12-01 06:02:27.504919: Validation loss did not improve from -0.55901. Patience: 1/50
2024-12-01 06:02:27.506174: train_loss -0.6089
2024-12-01 06:02:27.506908: val_loss -0.5421
2024-12-01 06:02:27.507603: Pseudo dice [0.7618]
2024-12-01 06:02:27.508225: Epoch time: 88.67 s
2024-12-01 06:02:27.508919: Yayy! New best EMA pseudo Dice: 0.7422
2024-12-01 06:02:29.163107: 
2024-12-01 06:02:29.164589: Epoch 23
2024-12-01 06:02:29.165411: Current learning rate: 0.00979
2024-12-01 06:03:57.828866: Validation loss did not improve from -0.55901. Patience: 2/50
2024-12-01 06:03:57.830182: train_loss -0.6114
2024-12-01 06:03:57.831295: val_loss -0.5077
2024-12-01 06:03:57.832070: Pseudo dice [0.7408]
2024-12-01 06:03:57.832705: Epoch time: 88.67 s
2024-12-01 06:03:59.232984: 
2024-12-01 06:03:59.235358: Epoch 24
2024-12-01 06:03:59.236477: Current learning rate: 0.00978
2024-12-01 06:05:28.272558: Validation loss did not improve from -0.55901. Patience: 3/50
2024-12-01 06:05:28.273817: train_loss -0.6152
2024-12-01 06:05:28.274765: val_loss -0.5428
2024-12-01 06:05:28.275607: Pseudo dice [0.7577]
2024-12-01 06:05:28.276294: Epoch time: 89.04 s
2024-12-01 06:05:28.643683: Yayy! New best EMA pseudo Dice: 0.7436
2024-12-01 06:05:30.276414: 
2024-12-01 06:05:30.278074: Epoch 25
2024-12-01 06:05:30.278754: Current learning rate: 0.00977
2024-12-01 06:06:59.110118: Validation loss did not improve from -0.55901. Patience: 4/50
2024-12-01 06:06:59.111965: train_loss -0.6212
2024-12-01 06:06:59.112802: val_loss -0.553
2024-12-01 06:06:59.113630: Pseudo dice [0.7623]
2024-12-01 06:06:59.114366: Epoch time: 88.84 s
2024-12-01 06:06:59.115004: Yayy! New best EMA pseudo Dice: 0.7455
2024-12-01 06:07:00.777502: 
2024-12-01 06:07:00.778941: Epoch 26
2024-12-01 06:07:00.779576: Current learning rate: 0.00977
2024-12-01 06:08:29.741531: Validation loss improved from -0.55901 to -0.56527! Patience: 4/50
2024-12-01 06:08:29.742705: train_loss -0.6318
2024-12-01 06:08:29.743581: val_loss -0.5653
2024-12-01 06:08:29.744426: Pseudo dice [0.7707]
2024-12-01 06:08:29.745306: Epoch time: 88.97 s
2024-12-01 06:08:29.746029: Yayy! New best EMA pseudo Dice: 0.748
2024-12-01 06:08:31.360783: 
2024-12-01 06:08:31.362761: Epoch 27
2024-12-01 06:08:31.363494: Current learning rate: 0.00976
2024-12-01 06:10:00.276207: Validation loss did not improve from -0.56527. Patience: 1/50
2024-12-01 06:10:00.276997: train_loss -0.6357
2024-12-01 06:10:00.277855: val_loss -0.5484
2024-12-01 06:10:00.278670: Pseudo dice [0.7587]
2024-12-01 06:10:00.279554: Epoch time: 88.92 s
2024-12-01 06:10:00.280339: Yayy! New best EMA pseudo Dice: 0.7491
2024-12-01 06:10:01.934174: 
2024-12-01 06:10:01.935400: Epoch 28
2024-12-01 06:10:01.936067: Current learning rate: 0.00975
2024-12-01 06:11:30.839971: Validation loss did not improve from -0.56527. Patience: 2/50
2024-12-01 06:11:30.841064: train_loss -0.6283
2024-12-01 06:11:30.842028: val_loss -0.468
2024-12-01 06:11:30.842924: Pseudo dice [0.7047]
2024-12-01 06:11:30.843761: Epoch time: 88.91 s
2024-12-01 06:11:32.644856: 
2024-12-01 06:11:32.646388: Epoch 29
2024-12-01 06:11:32.647213: Current learning rate: 0.00974
2024-12-01 06:13:01.560032: Validation loss did not improve from -0.56527. Patience: 3/50
2024-12-01 06:13:01.560902: train_loss -0.6252
2024-12-01 06:13:01.561521: val_loss -0.5476
2024-12-01 06:13:01.562107: Pseudo dice [0.7626]
2024-12-01 06:13:01.562765: Epoch time: 88.92 s
2024-12-01 06:13:03.211366: 
2024-12-01 06:13:03.212984: Epoch 30
2024-12-01 06:13:03.213661: Current learning rate: 0.00973
2024-12-01 06:14:32.020413: Validation loss did not improve from -0.56527. Patience: 4/50
2024-12-01 06:14:32.021480: train_loss -0.627
2024-12-01 06:14:32.022337: val_loss -0.5464
2024-12-01 06:14:32.023075: Pseudo dice [0.7605]
2024-12-01 06:14:32.023722: Epoch time: 88.81 s
2024-12-01 06:14:33.316943: 
2024-12-01 06:14:33.318298: Epoch 31
2024-12-01 06:14:33.319133: Current learning rate: 0.00972
2024-12-01 06:16:02.272757: Validation loss did not improve from -0.56527. Patience: 5/50
2024-12-01 06:16:02.273819: train_loss -0.6352
2024-12-01 06:16:02.274731: val_loss -0.5394
2024-12-01 06:16:02.275336: Pseudo dice [0.7609]
2024-12-01 06:16:02.275970: Epoch time: 88.96 s
2024-12-01 06:16:02.276676: Yayy! New best EMA pseudo Dice: 0.7491
2024-12-01 06:16:03.930543: 
2024-12-01 06:16:03.932030: Epoch 32
2024-12-01 06:16:03.933040: Current learning rate: 0.00971
2024-12-01 06:17:32.818186: Validation loss did not improve from -0.56527. Patience: 6/50
2024-12-01 06:17:32.818895: train_loss -0.6376
2024-12-01 06:17:32.819823: val_loss -0.5113
2024-12-01 06:17:32.820664: Pseudo dice [0.7474]
2024-12-01 06:17:32.821460: Epoch time: 88.89 s
2024-12-01 06:17:34.164057: 
2024-12-01 06:17:34.165831: Epoch 33
2024-12-01 06:17:34.166622: Current learning rate: 0.0097
2024-12-01 06:19:03.031563: Validation loss did not improve from -0.56527. Patience: 7/50
2024-12-01 06:19:03.032790: train_loss -0.6362
2024-12-01 06:19:03.033616: val_loss -0.509
2024-12-01 06:19:03.034175: Pseudo dice [0.7445]
2024-12-01 06:19:03.034791: Epoch time: 88.87 s
2024-12-01 06:19:04.329029: 
2024-12-01 06:19:04.330450: Epoch 34
2024-12-01 06:19:04.331460: Current learning rate: 0.00969
2024-12-01 06:20:33.222322: Validation loss did not improve from -0.56527. Patience: 8/50
2024-12-01 06:20:33.223060: train_loss -0.6291
2024-12-01 06:20:33.223754: val_loss -0.5406
2024-12-01 06:20:33.224608: Pseudo dice [0.7549]
2024-12-01 06:20:33.225348: Epoch time: 88.9 s
2024-12-01 06:20:33.607799: Yayy! New best EMA pseudo Dice: 0.7492
2024-12-01 06:20:35.341216: 
2024-12-01 06:20:35.343088: Epoch 35
2024-12-01 06:20:35.343837: Current learning rate: 0.00968
2024-12-01 06:22:04.243766: Validation loss did not improve from -0.56527. Patience: 9/50
2024-12-01 06:22:04.244847: train_loss -0.6457
2024-12-01 06:22:04.245738: val_loss -0.5442
2024-12-01 06:22:04.246436: Pseudo dice [0.759]
2024-12-01 06:22:04.247124: Epoch time: 88.91 s
2024-12-01 06:22:04.247757: Yayy! New best EMA pseudo Dice: 0.7502
2024-12-01 06:22:05.959149: 
2024-12-01 06:22:05.961213: Epoch 36
2024-12-01 06:22:05.961944: Current learning rate: 0.00968
2024-12-01 06:23:34.839184: Validation loss did not improve from -0.56527. Patience: 10/50
2024-12-01 06:23:34.840593: train_loss -0.6492
2024-12-01 06:23:34.841500: val_loss -0.5349
2024-12-01 06:23:34.842085: Pseudo dice [0.7466]
2024-12-01 06:23:34.842746: Epoch time: 88.88 s
2024-12-01 06:23:36.182158: 
2024-12-01 06:23:36.184072: Epoch 37
2024-12-01 06:23:36.185056: Current learning rate: 0.00967
2024-12-01 06:25:05.204162: Validation loss did not improve from -0.56527. Patience: 11/50
2024-12-01 06:25:05.204989: train_loss -0.6467
2024-12-01 06:25:05.206052: val_loss -0.5336
2024-12-01 06:25:05.206910: Pseudo dice [0.7559]
2024-12-01 06:25:05.207711: Epoch time: 89.02 s
2024-12-01 06:25:05.208359: Yayy! New best EMA pseudo Dice: 0.7504
2024-12-01 06:25:06.907997: 
2024-12-01 06:25:06.909378: Epoch 38
2024-12-01 06:25:06.910342: Current learning rate: 0.00966
2024-12-01 06:26:35.893400: Validation loss did not improve from -0.56527. Patience: 12/50
2024-12-01 06:26:35.894425: train_loss -0.6536
2024-12-01 06:26:35.895240: val_loss -0.523
2024-12-01 06:26:35.895872: Pseudo dice [0.7476]
2024-12-01 06:26:35.896491: Epoch time: 88.99 s
2024-12-01 06:26:37.843714: 
2024-12-01 06:26:37.845823: Epoch 39
2024-12-01 06:26:37.846852: Current learning rate: 0.00965
2024-12-01 06:28:06.728768: Validation loss did not improve from -0.56527. Patience: 13/50
2024-12-01 06:28:06.730052: train_loss -0.6491
2024-12-01 06:28:06.730999: val_loss -0.4499
2024-12-01 06:28:06.731932: Pseudo dice [0.7049]
2024-12-01 06:28:06.732662: Epoch time: 88.89 s
2024-12-01 06:28:08.515813: 
2024-12-01 06:28:08.517734: Epoch 40
2024-12-01 06:28:08.518813: Current learning rate: 0.00964
2024-12-01 06:29:37.698828: Validation loss did not improve from -0.56527. Patience: 14/50
2024-12-01 06:29:37.699862: train_loss -0.6431
2024-12-01 06:29:37.700613: val_loss -0.5459
2024-12-01 06:29:37.701357: Pseudo dice [0.7578]
2024-12-01 06:29:37.702083: Epoch time: 89.19 s
2024-12-01 06:29:39.079838: 
2024-12-01 06:29:39.081499: Epoch 41
2024-12-01 06:29:39.082371: Current learning rate: 0.00963
2024-12-01 06:31:08.200911: Validation loss did not improve from -0.56527. Patience: 15/50
2024-12-01 06:31:08.201757: train_loss -0.6566
2024-12-01 06:31:08.202491: val_loss -0.5599
2024-12-01 06:31:08.203151: Pseudo dice [0.7736]
2024-12-01 06:31:08.203882: Epoch time: 89.12 s
2024-12-01 06:31:09.501416: 
2024-12-01 06:31:09.503089: Epoch 42
2024-12-01 06:31:09.504082: Current learning rate: 0.00962
2024-12-01 06:32:38.482008: Validation loss did not improve from -0.56527. Patience: 16/50
2024-12-01 06:32:38.483485: train_loss -0.6533
2024-12-01 06:32:38.485222: val_loss -0.5391
2024-12-01 06:32:38.486469: Pseudo dice [0.7544]
2024-12-01 06:32:38.487312: Epoch time: 88.98 s
2024-12-01 06:32:39.774617: 
2024-12-01 06:32:39.775763: Epoch 43
2024-12-01 06:32:39.776557: Current learning rate: 0.00961
2024-12-01 06:34:08.785146: Validation loss did not improve from -0.56527. Patience: 17/50
2024-12-01 06:34:08.786426: train_loss -0.6584
2024-12-01 06:34:08.787677: val_loss -0.5277
2024-12-01 06:34:08.789008: Pseudo dice [0.7571]
2024-12-01 06:34:08.790241: Epoch time: 89.01 s
2024-12-01 06:34:08.791542: Yayy! New best EMA pseudo Dice: 0.7507
2024-12-01 06:34:10.423812: 
2024-12-01 06:34:10.425570: Epoch 44
2024-12-01 06:34:10.426561: Current learning rate: 0.0096
2024-12-01 06:35:39.287602: Validation loss did not improve from -0.56527. Patience: 18/50
2024-12-01 06:35:39.288705: train_loss -0.6662
2024-12-01 06:35:39.289389: val_loss -0.5129
2024-12-01 06:35:39.290214: Pseudo dice [0.7449]
2024-12-01 06:35:39.290939: Epoch time: 88.87 s
2024-12-01 06:35:40.979017: 
2024-12-01 06:35:40.981365: Epoch 45
2024-12-01 06:35:40.982707: Current learning rate: 0.00959
2024-12-01 06:37:09.837722: Validation loss did not improve from -0.56527. Patience: 19/50
2024-12-01 06:37:09.838572: train_loss -0.6716
2024-12-01 06:37:09.839482: val_loss -0.5315
2024-12-01 06:37:09.840311: Pseudo dice [0.7583]
2024-12-01 06:37:09.841122: Epoch time: 88.86 s
2024-12-01 06:37:09.841968: Yayy! New best EMA pseudo Dice: 0.7509
2024-12-01 06:37:11.502882: 
2024-12-01 06:37:11.504893: Epoch 46
2024-12-01 06:37:11.505872: Current learning rate: 0.00959
2024-12-01 06:38:40.469551: Validation loss did not improve from -0.56527. Patience: 20/50
2024-12-01 06:38:40.471517: train_loss -0.6609
2024-12-01 06:38:40.472887: val_loss -0.4853
2024-12-01 06:38:40.473735: Pseudo dice [0.7232]
2024-12-01 06:38:40.474692: Epoch time: 88.97 s
2024-12-01 06:38:41.727201: 
2024-12-01 06:38:41.728894: Epoch 47
2024-12-01 06:38:41.729719: Current learning rate: 0.00958
2024-12-01 06:40:10.674198: Validation loss did not improve from -0.56527. Patience: 21/50
2024-12-01 06:40:10.675356: train_loss -0.6645
2024-12-01 06:40:10.676142: val_loss -0.5344
2024-12-01 06:40:10.676735: Pseudo dice [0.7575]
2024-12-01 06:40:10.677384: Epoch time: 88.95 s
2024-12-01 06:40:11.918355: 
2024-12-01 06:40:11.919857: Epoch 48
2024-12-01 06:40:11.920634: Current learning rate: 0.00957
2024-12-01 06:41:40.862072: Validation loss did not improve from -0.56527. Patience: 22/50
2024-12-01 06:41:40.862933: train_loss -0.6681
2024-12-01 06:41:40.863681: val_loss -0.5296
2024-12-01 06:41:40.864537: Pseudo dice [0.7465]
2024-12-01 06:41:40.865300: Epoch time: 88.95 s
2024-12-01 06:41:42.126273: 
2024-12-01 06:41:42.127647: Epoch 49
2024-12-01 06:41:42.128429: Current learning rate: 0.00956
2024-12-01 06:43:11.492454: Validation loss did not improve from -0.56527. Patience: 23/50
2024-12-01 06:43:11.493778: train_loss -0.6648
2024-12-01 06:43:11.494673: val_loss -0.5308
2024-12-01 06:43:11.495237: Pseudo dice [0.7422]
2024-12-01 06:43:11.495972: Epoch time: 89.37 s
2024-12-01 06:43:13.109198: 
2024-12-01 06:43:13.110722: Epoch 50
2024-12-01 06:43:13.111417: Current learning rate: 0.00955
2024-12-01 06:44:42.196145: Validation loss did not improve from -0.56527. Patience: 24/50
2024-12-01 06:44:42.197043: train_loss -0.6668
2024-12-01 06:44:42.197805: val_loss -0.524
2024-12-01 06:44:42.198674: Pseudo dice [0.7597]
2024-12-01 06:44:42.199517: Epoch time: 89.09 s
2024-12-01 06:44:43.482840: 
2024-12-01 06:44:43.484368: Epoch 51
2024-12-01 06:44:43.485197: Current learning rate: 0.00954
2024-12-01 06:46:12.657062: Validation loss did not improve from -0.56527. Patience: 25/50
2024-12-01 06:46:12.657940: train_loss -0.6783
2024-12-01 06:46:12.658644: val_loss -0.4935
2024-12-01 06:46:12.659254: Pseudo dice [0.7295]
2024-12-01 06:46:12.659837: Epoch time: 89.18 s
2024-12-01 06:46:13.911058: 
2024-12-01 06:46:13.912400: Epoch 52
2024-12-01 06:46:13.913166: Current learning rate: 0.00953
2024-12-01 06:47:43.086228: Validation loss did not improve from -0.56527. Patience: 26/50
2024-12-01 06:47:43.087129: train_loss -0.6709
2024-12-01 06:47:43.088001: val_loss -0.5625
2024-12-01 06:47:43.088806: Pseudo dice [0.7656]
2024-12-01 06:47:43.089676: Epoch time: 89.18 s
2024-12-01 06:47:44.381440: 
2024-12-01 06:47:44.382838: Epoch 53
2024-12-01 06:47:44.383836: Current learning rate: 0.00952
2024-12-01 06:49:13.527587: Validation loss did not improve from -0.56527. Patience: 27/50
2024-12-01 06:49:13.528508: train_loss -0.6674
2024-12-01 06:49:13.529343: val_loss -0.5601
2024-12-01 06:49:13.530088: Pseudo dice [0.7689]
2024-12-01 06:49:13.530744: Epoch time: 89.15 s
2024-12-01 06:49:13.531335: Yayy! New best EMA pseudo Dice: 0.7511
2024-12-01 06:49:15.192868: 
2024-12-01 06:49:15.194167: Epoch 54
2024-12-01 06:49:15.194829: Current learning rate: 0.00951
2024-12-01 06:50:44.152160: Validation loss did not improve from -0.56527. Patience: 28/50
2024-12-01 06:50:44.152980: train_loss -0.6636
2024-12-01 06:50:44.153911: val_loss -0.4572
2024-12-01 06:50:44.154608: Pseudo dice [0.7181]
2024-12-01 06:50:44.155221: Epoch time: 88.96 s
2024-12-01 06:50:45.817171: 
2024-12-01 06:50:45.818718: Epoch 55
2024-12-01 06:50:45.819616: Current learning rate: 0.0095
2024-12-01 06:52:14.759945: Validation loss did not improve from -0.56527. Patience: 29/50
2024-12-01 06:52:14.760960: train_loss -0.6677
2024-12-01 06:52:14.762065: val_loss -0.4808
2024-12-01 06:52:14.762874: Pseudo dice [0.7274]
2024-12-01 06:52:14.763757: Epoch time: 88.94 s
2024-12-01 06:52:16.053233: 
2024-12-01 06:52:16.055010: Epoch 56
2024-12-01 06:52:16.055961: Current learning rate: 0.00949
2024-12-01 06:53:44.993954: Validation loss did not improve from -0.56527. Patience: 30/50
2024-12-01 06:53:44.995073: train_loss -0.6686
2024-12-01 06:53:44.995945: val_loss -0.5172
2024-12-01 06:53:44.996743: Pseudo dice [0.738]
2024-12-01 06:53:44.997484: Epoch time: 88.94 s
2024-12-01 06:53:46.276018: 
2024-12-01 06:53:46.277448: Epoch 57
2024-12-01 06:53:46.278174: Current learning rate: 0.00949
2024-12-01 06:55:15.190413: Validation loss did not improve from -0.56527. Patience: 31/50
2024-12-01 06:55:15.191843: train_loss -0.6799
2024-12-01 06:55:15.192983: val_loss -0.5434
2024-12-01 06:55:15.193935: Pseudo dice [0.7618]
2024-12-01 06:55:15.194766: Epoch time: 88.92 s
2024-12-01 06:55:16.483726: 
2024-12-01 06:55:16.485585: Epoch 58
2024-12-01 06:55:16.486454: Current learning rate: 0.00948
2024-12-01 06:56:45.417859: Validation loss did not improve from -0.56527. Patience: 32/50
2024-12-01 06:56:45.418989: train_loss -0.6769
2024-12-01 06:56:45.419807: val_loss -0.5412
2024-12-01 06:56:45.420483: Pseudo dice [0.7604]
2024-12-01 06:56:45.421240: Epoch time: 88.94 s
2024-12-01 06:56:46.719116: 
2024-12-01 06:56:46.720993: Epoch 59
2024-12-01 06:56:46.721740: Current learning rate: 0.00947
2024-12-01 06:58:15.622706: Validation loss did not improve from -0.56527. Patience: 33/50
2024-12-01 06:58:15.623352: train_loss -0.6829
2024-12-01 06:58:15.624283: val_loss -0.5441
2024-12-01 06:58:15.625152: Pseudo dice [0.7633]
2024-12-01 06:58:15.626021: Epoch time: 88.91 s
2024-12-01 06:58:17.832653: 
2024-12-01 06:58:17.834403: Epoch 60
2024-12-01 06:58:17.835262: Current learning rate: 0.00946
2024-12-01 06:59:46.671449: Validation loss did not improve from -0.56527. Patience: 34/50
2024-12-01 06:59:46.672508: train_loss -0.6828
2024-12-01 06:59:46.673188: val_loss -0.5333
2024-12-01 06:59:46.673830: Pseudo dice [0.7511]
2024-12-01 06:59:46.674485: Epoch time: 88.84 s
2024-12-01 06:59:47.991055: 
2024-12-01 06:59:47.992683: Epoch 61
2024-12-01 06:59:47.993483: Current learning rate: 0.00945
2024-12-01 07:01:16.898630: Validation loss did not improve from -0.56527. Patience: 35/50
2024-12-01 07:01:16.899592: train_loss -0.6808
2024-12-01 07:01:16.900829: val_loss -0.5163
2024-12-01 07:01:16.901667: Pseudo dice [0.7441]
2024-12-01 07:01:16.902630: Epoch time: 88.91 s
2024-12-01 07:01:18.202126: 
2024-12-01 07:01:18.203423: Epoch 62
2024-12-01 07:01:18.204160: Current learning rate: 0.00944
2024-12-01 07:02:47.068619: Validation loss did not improve from -0.56527. Patience: 36/50
2024-12-01 07:02:47.069559: train_loss -0.6837
2024-12-01 07:02:47.070441: val_loss -0.5593
2024-12-01 07:02:47.071118: Pseudo dice [0.7697]
2024-12-01 07:02:47.071905: Epoch time: 88.87 s
2024-12-01 07:02:47.072570: Yayy! New best EMA pseudo Dice: 0.7512
2024-12-01 07:02:48.791630: 
2024-12-01 07:02:48.793345: Epoch 63
2024-12-01 07:02:48.794137: Current learning rate: 0.00943
2024-12-01 07:04:17.675776: Validation loss did not improve from -0.56527. Patience: 37/50
2024-12-01 07:04:17.676552: train_loss -0.6925
2024-12-01 07:04:17.677305: val_loss -0.5397
2024-12-01 07:04:17.677926: Pseudo dice [0.75]
2024-12-01 07:04:17.678586: Epoch time: 88.89 s
2024-12-01 07:04:18.971724: 
2024-12-01 07:04:18.973191: Epoch 64
2024-12-01 07:04:18.973978: Current learning rate: 0.00942
2024-12-01 07:05:47.935311: Validation loss did not improve from -0.56527. Patience: 38/50
2024-12-01 07:05:47.936201: train_loss -0.6837
2024-12-01 07:05:47.937160: val_loss -0.5459
2024-12-01 07:05:47.937960: Pseudo dice [0.7553]
2024-12-01 07:05:47.938711: Epoch time: 88.97 s
2024-12-01 07:05:48.308376: Yayy! New best EMA pseudo Dice: 0.7515
2024-12-01 07:05:50.020981: 
2024-12-01 07:05:50.022553: Epoch 65
2024-12-01 07:05:50.023393: Current learning rate: 0.00941
2024-12-01 07:07:18.878502: Validation loss did not improve from -0.56527. Patience: 39/50
2024-12-01 07:07:18.879245: train_loss -0.688
2024-12-01 07:07:18.880291: val_loss -0.4824
2024-12-01 07:07:18.881016: Pseudo dice [0.7233]
2024-12-01 07:07:18.881943: Epoch time: 88.86 s
2024-12-01 07:07:20.207406: 
2024-12-01 07:07:20.208870: Epoch 66
2024-12-01 07:07:20.210023: Current learning rate: 0.0094
2024-12-01 07:08:49.136134: Validation loss did not improve from -0.56527. Patience: 40/50
2024-12-01 07:08:49.137246: train_loss -0.6869
2024-12-01 07:08:49.138106: val_loss -0.4742
2024-12-01 07:08:49.138746: Pseudo dice [0.7201]
2024-12-01 07:08:49.139322: Epoch time: 88.93 s
2024-12-01 07:08:50.469733: 
2024-12-01 07:08:50.471023: Epoch 67
2024-12-01 07:08:50.471657: Current learning rate: 0.00939
2024-12-01 07:10:19.391623: Validation loss did not improve from -0.56527. Patience: 41/50
2024-12-01 07:10:19.392697: train_loss -0.685
2024-12-01 07:10:19.393828: val_loss -0.5017
2024-12-01 07:10:19.394706: Pseudo dice [0.7486]
2024-12-01 07:10:19.395590: Epoch time: 88.92 s
2024-12-01 07:10:20.711508: 
2024-12-01 07:10:20.713376: Epoch 68
2024-12-01 07:10:20.714250: Current learning rate: 0.00939
2024-12-01 07:11:49.549233: Validation loss improved from -0.56527 to -0.56911! Patience: 41/50
2024-12-01 07:11:49.550835: train_loss -0.6888
2024-12-01 07:11:49.551832: val_loss -0.5691
2024-12-01 07:11:49.552803: Pseudo dice [0.7764]
2024-12-01 07:11:49.553785: Epoch time: 88.84 s
2024-12-01 07:11:50.854588: 
2024-12-01 07:11:50.856002: Epoch 69
2024-12-01 07:11:50.856576: Current learning rate: 0.00938
2024-12-01 07:13:19.664454: Validation loss did not improve from -0.56911. Patience: 1/50
2024-12-01 07:13:19.665782: train_loss -0.6867
2024-12-01 07:13:19.666951: val_loss -0.5231
2024-12-01 07:13:19.667876: Pseudo dice [0.7461]
2024-12-01 07:13:19.668611: Epoch time: 88.81 s
2024-12-01 07:13:21.373985: 
2024-12-01 07:13:21.375392: Epoch 70
2024-12-01 07:13:21.376247: Current learning rate: 0.00937
2024-12-01 07:14:50.397707: Validation loss did not improve from -0.56911. Patience: 2/50
2024-12-01 07:14:50.398875: train_loss -0.6776
2024-12-01 07:14:50.399626: val_loss -0.5455
2024-12-01 07:14:50.400397: Pseudo dice [0.7722]
2024-12-01 07:14:50.401017: Epoch time: 89.03 s
2024-12-01 07:14:52.280621: 
2024-12-01 07:14:52.282065: Epoch 71
2024-12-01 07:14:52.282864: Current learning rate: 0.00936
2024-12-01 07:16:21.072034: Validation loss did not improve from -0.56911. Patience: 3/50
2024-12-01 07:16:21.073558: train_loss -0.6914
2024-12-01 07:16:21.074433: val_loss -0.5445
2024-12-01 07:16:21.075183: Pseudo dice [0.7587]
2024-12-01 07:16:21.075799: Epoch time: 88.79 s
2024-12-01 07:16:21.076405: Yayy! New best EMA pseudo Dice: 0.7519
2024-12-01 07:16:22.783686: 
2024-12-01 07:16:22.785283: Epoch 72
2024-12-01 07:16:22.786298: Current learning rate: 0.00935
2024-12-01 07:17:51.749199: Validation loss did not improve from -0.56911. Patience: 4/50
2024-12-01 07:17:51.749950: train_loss -0.68
2024-12-01 07:17:51.750820: val_loss -0.4997
2024-12-01 07:17:51.751572: Pseudo dice [0.7428]
2024-12-01 07:17:51.752467: Epoch time: 88.97 s
2024-12-01 07:17:53.130851: 
2024-12-01 07:17:53.132426: Epoch 73
2024-12-01 07:17:53.133046: Current learning rate: 0.00934
2024-12-01 07:19:22.094319: Validation loss did not improve from -0.56911. Patience: 5/50
2024-12-01 07:19:22.095072: train_loss -0.6912
2024-12-01 07:19:22.095914: val_loss -0.5399
2024-12-01 07:19:22.096542: Pseudo dice [0.7672]
2024-12-01 07:19:22.097563: Epoch time: 88.97 s
2024-12-01 07:19:22.098481: Yayy! New best EMA pseudo Dice: 0.7526
2024-12-01 07:19:23.849691: 
2024-12-01 07:19:23.851163: Epoch 74
2024-12-01 07:19:23.852086: Current learning rate: 0.00933
2024-12-01 07:20:52.736770: Validation loss did not improve from -0.56911. Patience: 6/50
2024-12-01 07:20:52.737951: train_loss -0.6936
2024-12-01 07:20:52.738890: val_loss -0.4886
2024-12-01 07:20:52.739746: Pseudo dice [0.744]
2024-12-01 07:20:52.740515: Epoch time: 88.89 s
2024-12-01 07:20:54.457231: 
2024-12-01 07:20:54.458785: Epoch 75
2024-12-01 07:20:54.459651: Current learning rate: 0.00932
2024-12-01 07:22:23.329747: Validation loss did not improve from -0.56911. Patience: 7/50
2024-12-01 07:22:23.330614: train_loss -0.6914
2024-12-01 07:22:23.331304: val_loss -0.5305
2024-12-01 07:22:23.331956: Pseudo dice [0.7528]
2024-12-01 07:22:23.332574: Epoch time: 88.87 s
2024-12-01 07:22:24.674459: 
2024-12-01 07:22:24.675504: Epoch 76
2024-12-01 07:22:24.676320: Current learning rate: 0.00931
2024-12-01 07:23:53.546430: Validation loss did not improve from -0.56911. Patience: 8/50
2024-12-01 07:23:53.547327: train_loss -0.6977
2024-12-01 07:23:53.548111: val_loss -0.5372
2024-12-01 07:23:53.548775: Pseudo dice [0.7548]
2024-12-01 07:23:53.549669: Epoch time: 88.87 s
2024-12-01 07:23:54.912804: 
2024-12-01 07:23:54.914477: Epoch 77
2024-12-01 07:23:54.915381: Current learning rate: 0.0093
2024-12-01 07:25:23.835276: Validation loss did not improve from -0.56911. Patience: 9/50
2024-12-01 07:25:23.836307: train_loss -0.6969
2024-12-01 07:25:23.837201: val_loss -0.5105
2024-12-01 07:25:23.838015: Pseudo dice [0.7322]
2024-12-01 07:25:23.838698: Epoch time: 88.92 s
2024-12-01 07:25:25.188330: 
2024-12-01 07:25:25.189709: Epoch 78
2024-12-01 07:25:25.190561: Current learning rate: 0.0093
2024-12-01 07:26:54.363500: Validation loss did not improve from -0.56911. Patience: 10/50
2024-12-01 07:26:54.364656: train_loss -0.6912
2024-12-01 07:26:54.365901: val_loss -0.5282
2024-12-01 07:26:54.366751: Pseudo dice [0.7576]
2024-12-01 07:26:54.367556: Epoch time: 89.18 s
2024-12-01 07:26:55.777254: 
2024-12-01 07:26:55.778682: Epoch 79
2024-12-01 07:26:55.779617: Current learning rate: 0.00929
2024-12-01 07:28:25.085667: Validation loss did not improve from -0.56911. Patience: 11/50
2024-12-01 07:28:25.086550: train_loss -0.6932
2024-12-01 07:28:25.087533: val_loss -0.511
2024-12-01 07:28:25.088264: Pseudo dice [0.753]
2024-12-01 07:28:25.089036: Epoch time: 89.31 s
2024-12-01 07:28:26.821656: 
2024-12-01 07:28:26.823293: Epoch 80
2024-12-01 07:28:26.824147: Current learning rate: 0.00928
2024-12-01 07:29:55.996201: Validation loss did not improve from -0.56911. Patience: 12/50
2024-12-01 07:29:55.996928: train_loss -0.6964
2024-12-01 07:29:55.998096: val_loss -0.5603
2024-12-01 07:29:55.999042: Pseudo dice [0.7649]
2024-12-01 07:29:56.000014: Epoch time: 89.18 s
2024-12-01 07:29:57.797718: 
2024-12-01 07:29:57.800265: Epoch 81
2024-12-01 07:29:57.801823: Current learning rate: 0.00927
2024-12-01 07:31:26.758918: Validation loss did not improve from -0.56911. Patience: 13/50
2024-12-01 07:31:26.760272: train_loss -0.696
2024-12-01 07:31:26.761168: val_loss -0.5513
2024-12-01 07:31:26.761818: Pseudo dice [0.7696]
2024-12-01 07:31:26.762485: Epoch time: 88.96 s
2024-12-01 07:31:26.763002: Yayy! New best EMA pseudo Dice: 0.7542
2024-12-01 07:31:28.503100: 
2024-12-01 07:31:28.504908: Epoch 82
2024-12-01 07:31:28.505701: Current learning rate: 0.00926
2024-12-01 07:32:57.450603: Validation loss did not improve from -0.56911. Patience: 14/50
2024-12-01 07:32:57.451689: train_loss -0.7052
2024-12-01 07:32:57.452465: val_loss -0.5321
2024-12-01 07:32:57.453079: Pseudo dice [0.7529]
2024-12-01 07:32:57.453766: Epoch time: 88.95 s
2024-12-01 07:32:58.713778: 
2024-12-01 07:32:58.715228: Epoch 83
2024-12-01 07:32:58.715935: Current learning rate: 0.00925
2024-12-01 07:34:27.663743: Validation loss did not improve from -0.56911. Patience: 15/50
2024-12-01 07:34:27.664888: train_loss -0.7082
2024-12-01 07:34:27.665876: val_loss -0.5028
2024-12-01 07:34:27.666492: Pseudo dice [0.7413]
2024-12-01 07:34:27.667086: Epoch time: 88.95 s
2024-12-01 07:34:28.960800: 
2024-12-01 07:34:28.962760: Epoch 84
2024-12-01 07:34:28.963553: Current learning rate: 0.00924
2024-12-01 07:35:58.046011: Validation loss did not improve from -0.56911. Patience: 16/50
2024-12-01 07:35:58.048462: train_loss -0.7096
2024-12-01 07:35:58.049405: val_loss -0.5424
2024-12-01 07:35:58.050046: Pseudo dice [0.7582]
2024-12-01 07:35:58.050669: Epoch time: 89.09 s
2024-12-01 07:35:59.741552: 
2024-12-01 07:35:59.743174: Epoch 85
2024-12-01 07:35:59.743968: Current learning rate: 0.00923
2024-12-01 07:37:28.553010: Validation loss did not improve from -0.56911. Patience: 17/50
2024-12-01 07:37:28.554209: train_loss -0.7134
2024-12-01 07:37:28.555011: val_loss -0.5241
2024-12-01 07:37:28.555795: Pseudo dice [0.7534]
2024-12-01 07:37:28.556529: Epoch time: 88.81 s
2024-12-01 07:37:29.807166: 
2024-12-01 07:37:29.808808: Epoch 86
2024-12-01 07:37:29.809820: Current learning rate: 0.00922
2024-12-01 07:38:58.577698: Validation loss did not improve from -0.56911. Patience: 18/50
2024-12-01 07:38:58.578799: train_loss -0.708
2024-12-01 07:38:58.579721: val_loss -0.5068
2024-12-01 07:38:58.580311: Pseudo dice [0.7328]
2024-12-01 07:38:58.580983: Epoch time: 88.77 s
2024-12-01 07:38:59.832550: 
2024-12-01 07:38:59.834152: Epoch 87
2024-12-01 07:38:59.834934: Current learning rate: 0.00921
2024-12-01 07:40:28.617329: Validation loss did not improve from -0.56911. Patience: 19/50
2024-12-01 07:40:28.618740: train_loss -0.7063
2024-12-01 07:40:28.619891: val_loss -0.5442
2024-12-01 07:40:28.620612: Pseudo dice [0.7637]
2024-12-01 07:40:28.621232: Epoch time: 88.79 s
2024-12-01 07:40:29.888326: 
2024-12-01 07:40:29.889039: Epoch 88
2024-12-01 07:40:29.889810: Current learning rate: 0.0092
2024-12-01 07:41:58.661783: Validation loss did not improve from -0.56911. Patience: 20/50
2024-12-01 07:41:58.663134: train_loss -0.7016
2024-12-01 07:41:58.664105: val_loss -0.5442
2024-12-01 07:41:58.664856: Pseudo dice [0.7653]
2024-12-01 07:41:58.665531: Epoch time: 88.78 s
2024-12-01 07:41:59.946347: 
2024-12-01 07:41:59.947763: Epoch 89
2024-12-01 07:41:59.948526: Current learning rate: 0.0092
2024-12-01 07:43:28.859370: Validation loss did not improve from -0.56911. Patience: 21/50
2024-12-01 07:43:28.860633: train_loss -0.7067
2024-12-01 07:43:28.861709: val_loss -0.5519
2024-12-01 07:43:28.862303: Pseudo dice [0.7703]
2024-12-01 07:43:28.862849: Epoch time: 88.92 s
2024-12-01 07:43:29.241509: Yayy! New best EMA pseudo Dice: 0.7555
2024-12-01 07:43:30.869183: 
2024-12-01 07:43:30.870699: Epoch 90
2024-12-01 07:43:30.871389: Current learning rate: 0.00919
2024-12-01 07:44:59.695158: Validation loss did not improve from -0.56911. Patience: 22/50
2024-12-01 07:44:59.696181: train_loss -0.7122
2024-12-01 07:44:59.697066: val_loss -0.5316
2024-12-01 07:44:59.697747: Pseudo dice [0.7531]
2024-12-01 07:44:59.698351: Epoch time: 88.83 s
2024-12-01 07:45:00.981993: 
2024-12-01 07:45:00.983341: Epoch 91
2024-12-01 07:45:00.983974: Current learning rate: 0.00918
2024-12-01 07:46:29.746156: Validation loss did not improve from -0.56911. Patience: 23/50
2024-12-01 07:46:29.747310: train_loss -0.7055
2024-12-01 07:46:29.748248: val_loss -0.5096
2024-12-01 07:46:29.748850: Pseudo dice [0.7509]
2024-12-01 07:46:29.749424: Epoch time: 88.77 s
2024-12-01 07:46:31.442187: 
2024-12-01 07:46:31.443594: Epoch 92
2024-12-01 07:46:31.444316: Current learning rate: 0.00917
2024-12-01 07:47:59.961400: Validation loss did not improve from -0.56911. Patience: 24/50
2024-12-01 07:47:59.962598: train_loss -0.7021
2024-12-01 07:47:59.963802: val_loss -0.5348
2024-12-01 07:47:59.965236: Pseudo dice [0.7562]
2024-12-01 07:47:59.966260: Epoch time: 88.52 s
2024-12-01 07:48:01.224483: 
2024-12-01 07:48:01.226086: Epoch 93
2024-12-01 07:48:01.227038: Current learning rate: 0.00916
2024-12-01 07:49:29.840539: Validation loss did not improve from -0.56911. Patience: 25/50
2024-12-01 07:49:29.841729: train_loss -0.7005
2024-12-01 07:49:29.842930: val_loss -0.495
2024-12-01 07:49:29.843768: Pseudo dice [0.7391]
2024-12-01 07:49:29.844506: Epoch time: 88.62 s
2024-12-01 07:49:31.118376: 
2024-12-01 07:49:31.119835: Epoch 94
2024-12-01 07:49:31.120687: Current learning rate: 0.00915
2024-12-01 07:50:59.713320: Validation loss did not improve from -0.56911. Patience: 26/50
2024-12-01 07:50:59.714138: train_loss -0.7102
2024-12-01 07:50:59.715296: val_loss -0.4584
2024-12-01 07:50:59.716074: Pseudo dice [0.7157]
2024-12-01 07:50:59.716878: Epoch time: 88.6 s
2024-12-01 07:51:01.337529: 
2024-12-01 07:51:01.339148: Epoch 95
2024-12-01 07:51:01.340106: Current learning rate: 0.00914
2024-12-01 07:52:29.894760: Validation loss did not improve from -0.56911. Patience: 27/50
2024-12-01 07:52:29.896058: train_loss -0.7027
2024-12-01 07:52:29.896800: val_loss -0.5664
2024-12-01 07:52:29.897550: Pseudo dice [0.7767]
2024-12-01 07:52:29.898259: Epoch time: 88.56 s
2024-12-01 07:52:31.170309: 
2024-12-01 07:52:31.172033: Epoch 96
2024-12-01 07:52:31.172924: Current learning rate: 0.00913
2024-12-01 07:53:59.895778: Validation loss did not improve from -0.56911. Patience: 28/50
2024-12-01 07:53:59.896724: train_loss -0.7124
2024-12-01 07:53:59.897637: val_loss -0.5609
2024-12-01 07:53:59.898423: Pseudo dice [0.7721]
2024-12-01 07:53:59.899089: Epoch time: 88.73 s
2024-12-01 07:54:01.195986: 
2024-12-01 07:54:01.197554: Epoch 97
2024-12-01 07:54:01.198494: Current learning rate: 0.00912
2024-12-01 07:55:29.861602: Validation loss did not improve from -0.56911. Patience: 29/50
2024-12-01 07:55:29.862991: train_loss -0.7206
2024-12-01 07:55:29.864032: val_loss -0.5281
2024-12-01 07:55:29.865003: Pseudo dice [0.7595]
2024-12-01 07:55:29.865851: Epoch time: 88.67 s
2024-12-01 07:55:31.148170: 
2024-12-01 07:55:31.149665: Epoch 98
2024-12-01 07:55:31.150584: Current learning rate: 0.00911
2024-12-01 07:56:59.741788: Validation loss did not improve from -0.56911. Patience: 30/50
2024-12-01 07:56:59.742566: train_loss -0.7169
2024-12-01 07:56:59.743489: val_loss -0.5519
2024-12-01 07:56:59.744251: Pseudo dice [0.7687]
2024-12-01 07:56:59.744960: Epoch time: 88.6 s
2024-12-01 07:56:59.745705: Yayy! New best EMA pseudo Dice: 0.7562
2024-12-01 07:57:01.455300: 
2024-12-01 07:57:01.456672: Epoch 99
2024-12-01 07:57:01.457413: Current learning rate: 0.0091
2024-12-01 07:58:30.188875: Validation loss did not improve from -0.56911. Patience: 31/50
2024-12-01 07:58:30.189951: train_loss -0.7114
2024-12-01 07:58:30.190949: val_loss -0.5379
2024-12-01 07:58:30.191545: Pseudo dice [0.7632]
2024-12-01 07:58:30.192385: Epoch time: 88.74 s
2024-12-01 07:58:30.602687: Yayy! New best EMA pseudo Dice: 0.7569
2024-12-01 07:58:32.294091: 
2024-12-01 07:58:32.295647: Epoch 100
2024-12-01 07:58:32.296772: Current learning rate: 0.0091
2024-12-01 08:00:01.089008: Validation loss did not improve from -0.56911. Patience: 32/50
2024-12-01 08:00:01.089810: train_loss -0.7235
2024-12-01 08:00:01.090910: val_loss -0.5323
2024-12-01 08:00:01.091787: Pseudo dice [0.7611]
2024-12-01 08:00:01.092519: Epoch time: 88.8 s
2024-12-01 08:00:01.093233: Yayy! New best EMA pseudo Dice: 0.7573
2024-12-01 08:00:02.765628: 
2024-12-01 08:00:02.766989: Epoch 101
2024-12-01 08:00:02.768064: Current learning rate: 0.00909
2024-12-01 08:01:31.399653: Validation loss did not improve from -0.56911. Patience: 33/50
2024-12-01 08:01:31.400715: train_loss -0.7143
2024-12-01 08:01:31.401617: val_loss -0.4752
2024-12-01 08:01:31.402358: Pseudo dice [0.7364]
2024-12-01 08:01:31.403254: Epoch time: 88.64 s
2024-12-01 08:01:32.690806: 
2024-12-01 08:01:32.692249: Epoch 102
2024-12-01 08:01:32.692924: Current learning rate: 0.00908
2024-12-01 08:03:01.373231: Validation loss did not improve from -0.56911. Patience: 34/50
2024-12-01 08:03:01.374521: train_loss -0.7148
2024-12-01 08:03:01.375369: val_loss -0.5267
2024-12-01 08:03:01.376409: Pseudo dice [0.7549]
2024-12-01 08:03:01.377196: Epoch time: 88.68 s
2024-12-01 08:03:03.090708: 
2024-12-01 08:03:03.092380: Epoch 103
2024-12-01 08:03:03.093172: Current learning rate: 0.00907
2024-12-01 08:04:31.721115: Validation loss did not improve from -0.56911. Patience: 35/50
2024-12-01 08:04:31.722183: train_loss -0.7251
2024-12-01 08:04:31.723083: val_loss -0.557
2024-12-01 08:04:31.723720: Pseudo dice [0.7681]
2024-12-01 08:04:31.724337: Epoch time: 88.63 s
2024-12-01 08:04:33.010096: 
2024-12-01 08:04:33.011790: Epoch 104
2024-12-01 08:04:33.012630: Current learning rate: 0.00906
2024-12-01 08:06:01.627282: Validation loss did not improve from -0.56911. Patience: 36/50
2024-12-01 08:06:01.628322: train_loss -0.7259
2024-12-01 08:06:01.629172: val_loss -0.5579
2024-12-01 08:06:01.629894: Pseudo dice [0.7762]
2024-12-01 08:06:01.630519: Epoch time: 88.62 s
2024-12-01 08:06:02.014723: Yayy! New best EMA pseudo Dice: 0.7585
2024-12-01 08:06:03.628109: 
2024-12-01 08:06:03.629423: Epoch 105
2024-12-01 08:06:03.630209: Current learning rate: 0.00905
2024-12-01 08:07:32.305475: Validation loss did not improve from -0.56911. Patience: 37/50
2024-12-01 08:07:32.306716: train_loss -0.7232
2024-12-01 08:07:32.307539: val_loss -0.5555
2024-12-01 08:07:32.308500: Pseudo dice [0.7691]
2024-12-01 08:07:32.309246: Epoch time: 88.68 s
2024-12-01 08:07:32.310102: Yayy! New best EMA pseudo Dice: 0.7595
2024-12-01 08:07:33.978021: 
2024-12-01 08:07:33.979419: Epoch 106
2024-12-01 08:07:33.980115: Current learning rate: 0.00904
2024-12-01 08:09:02.636312: Validation loss did not improve from -0.56911. Patience: 38/50
2024-12-01 08:09:02.637577: train_loss -0.7243
2024-12-01 08:09:02.638782: val_loss -0.527
2024-12-01 08:09:02.639735: Pseudo dice [0.7468]
2024-12-01 08:09:02.640594: Epoch time: 88.66 s
2024-12-01 08:09:03.911002: 
2024-12-01 08:09:03.912798: Epoch 107
2024-12-01 08:09:03.913591: Current learning rate: 0.00903
2024-12-01 08:10:32.530091: Validation loss did not improve from -0.56911. Patience: 39/50
2024-12-01 08:10:32.531212: train_loss -0.7285
2024-12-01 08:10:32.532075: val_loss -0.5396
2024-12-01 08:10:32.532697: Pseudo dice [0.765]
2024-12-01 08:10:32.533297: Epoch time: 88.62 s
2024-12-01 08:10:33.832843: 
2024-12-01 08:10:33.834300: Epoch 108
2024-12-01 08:10:33.835251: Current learning rate: 0.00902
2024-12-01 08:12:02.570117: Validation loss did not improve from -0.56911. Patience: 40/50
2024-12-01 08:12:02.571280: train_loss -0.7147
2024-12-01 08:12:02.572229: val_loss -0.5303
2024-12-01 08:12:02.572858: Pseudo dice [0.762]
2024-12-01 08:12:02.573574: Epoch time: 88.74 s
2024-12-01 08:12:03.884480: 
2024-12-01 08:12:03.886191: Epoch 109
2024-12-01 08:12:03.886863: Current learning rate: 0.00901
2024-12-01 08:13:32.718875: Validation loss did not improve from -0.56911. Patience: 41/50
2024-12-01 08:13:32.720011: train_loss -0.7196
2024-12-01 08:13:32.720836: val_loss -0.4863
2024-12-01 08:13:32.721643: Pseudo dice [0.7322]
2024-12-01 08:13:32.722571: Epoch time: 88.84 s
2024-12-01 08:13:34.403279: 
2024-12-01 08:13:34.404946: Epoch 110
2024-12-01 08:13:34.405848: Current learning rate: 0.009
2024-12-01 08:15:03.277799: Validation loss did not improve from -0.56911. Patience: 42/50
2024-12-01 08:15:03.278917: train_loss -0.7229
2024-12-01 08:15:03.279902: val_loss -0.5645
2024-12-01 08:15:03.280846: Pseudo dice [0.7718]
2024-12-01 08:15:03.281732: Epoch time: 88.88 s
2024-12-01 08:15:04.570539: 
2024-12-01 08:15:04.572110: Epoch 111
2024-12-01 08:15:04.572896: Current learning rate: 0.009
2024-12-01 08:16:33.489491: Validation loss did not improve from -0.56911. Patience: 43/50
2024-12-01 08:16:33.490701: train_loss -0.7218
2024-12-01 08:16:33.491864: val_loss -0.5374
2024-12-01 08:16:33.492686: Pseudo dice [0.7645]
2024-12-01 08:16:33.493384: Epoch time: 88.92 s
2024-12-01 08:16:34.834111: 
2024-12-01 08:16:34.836006: Epoch 112
2024-12-01 08:16:34.836849: Current learning rate: 0.00899
2024-12-01 08:18:03.661375: Validation loss did not improve from -0.56911. Patience: 44/50
2024-12-01 08:18:03.662532: train_loss -0.7246
2024-12-01 08:18:03.663455: val_loss -0.5118
2024-12-01 08:18:03.664181: Pseudo dice [0.756]
2024-12-01 08:18:03.664918: Epoch time: 88.83 s
2024-12-01 08:18:04.944515: 
2024-12-01 08:18:04.946081: Epoch 113
2024-12-01 08:18:04.946858: Current learning rate: 0.00898
2024-12-01 08:19:33.744569: Validation loss did not improve from -0.56911. Patience: 45/50
2024-12-01 08:19:33.745759: train_loss -0.7291
2024-12-01 08:19:33.746617: val_loss -0.5436
2024-12-01 08:19:33.747381: Pseudo dice [0.7669]
2024-12-01 08:19:33.748162: Epoch time: 88.8 s
2024-12-01 08:19:35.463677: 
2024-12-01 08:19:35.465246: Epoch 114
2024-12-01 08:19:35.465991: Current learning rate: 0.00897
2024-12-01 08:21:04.180040: Validation loss did not improve from -0.56911. Patience: 46/50
2024-12-01 08:21:04.181588: train_loss -0.7327
2024-12-01 08:21:04.182789: val_loss -0.5056
2024-12-01 08:21:04.183431: Pseudo dice [0.7474]
2024-12-01 08:21:04.183997: Epoch time: 88.72 s
2024-12-01 08:21:05.899214: 
2024-12-01 08:21:05.900202: Epoch 115
2024-12-01 08:21:05.901035: Current learning rate: 0.00896
2024-12-01 08:22:34.584678: Validation loss did not improve from -0.56911. Patience: 47/50
2024-12-01 08:22:34.585955: train_loss -0.7345
2024-12-01 08:22:34.586734: val_loss -0.5294
2024-12-01 08:22:34.587411: Pseudo dice [0.7654]
2024-12-01 08:22:34.587980: Epoch time: 88.69 s
2024-12-01 08:22:35.917519: 
2024-12-01 08:22:35.919267: Epoch 116
2024-12-01 08:22:35.920135: Current learning rate: 0.00895
2024-12-01 08:24:04.667344: Validation loss did not improve from -0.56911. Patience: 48/50
2024-12-01 08:24:04.668312: train_loss -0.7297
2024-12-01 08:24:04.669298: val_loss -0.5465
2024-12-01 08:24:04.670209: Pseudo dice [0.7633]
2024-12-01 08:24:04.671124: Epoch time: 88.75 s
2024-12-01 08:24:06.000347: 
2024-12-01 08:24:06.001798: Epoch 117
2024-12-01 08:24:06.002746: Current learning rate: 0.00894
2024-12-01 08:25:34.768667: Validation loss did not improve from -0.56911. Patience: 49/50
2024-12-01 08:25:34.769481: train_loss -0.7278
2024-12-01 08:25:34.770545: val_loss -0.5234
2024-12-01 08:25:34.771389: Pseudo dice [0.7553]
2024-12-01 08:25:34.772302: Epoch time: 88.77 s
2024-12-01 08:25:36.079539: 
2024-12-01 08:25:36.081157: Epoch 118
2024-12-01 08:25:36.081991: Current learning rate: 0.00893
2024-12-01 08:27:04.728276: Validation loss did not improve from -0.56911. Patience: 50/50
2024-12-01 08:27:04.729337: train_loss -0.737
2024-12-01 08:27:04.730214: val_loss -0.5538
2024-12-01 08:27:04.731015: Pseudo dice [0.7598]
2024-12-01 08:27:04.731626: Epoch time: 88.65 s
2024-12-01 08:27:06.044798: Patience reached. Stopping training.
2024-12-01 08:27:06.519321: Training done.
2024-12-01 08:27:07.028087: Using splits from existing split file: /home/gridsan/nchutisilp/datasets/nnUNet_Datasets/nnUNet_preprocessed/Dataset306_Sohee_Ajay_Calcium_OCT/splits_final.json
2024-12-01 08:27:07.030712: The split file contains 5 splits.
2024-12-01 08:27:07.031653: Desired fold for training: 0
2024-12-01 08:27:07.032475: This split has 10 training and 3 validation cases.
2024-12-01 08:27:07.033387: predicting 02008Pre
2024-12-01 08:27:07.076079: 02008Pre, shape torch.Size([1, 171, 498, 498]), rank 0
2024-12-01 08:28:10.900937: predicting 106-002
2024-12-01 08:28:10.919779: 106-002, shape torch.Size([1, 539, 498, 498]), rank 0
2024-12-01 08:30:13.833046: predicting 701-013
2024-12-01 08:30:13.856460: 701-013, shape torch.Size([1, 375, 498, 498]), rank 0
2024-12-01 08:32:05.218304: Validation complete
2024-12-01 08:32:05.219530: Mean Validation Dice:  0.7665713893623484
2024-12-01 05:25:54.299014: unpacking done...
2024-12-01 05:25:54.408775: Unable to plot network architecture: nnUNet_compile is enabled!
2024-12-01 05:25:54.850237: 
2024-12-01 05:25:54.851106: Epoch 0
2024-12-01 05:25:54.851818: Current learning rate: 0.01
2024-12-01 05:29:30.701202: Validation loss improved from 1000.00000 to -0.35802! Patience: 0/50
2024-12-01 05:29:30.703051: train_loss -0.2699
2024-12-01 05:29:30.704648: val_loss -0.358
2024-12-01 05:29:30.705559: Pseudo dice [0.6255]
2024-12-01 05:29:30.706324: Epoch time: 215.85 s
2024-12-01 05:29:30.707147: Yayy! New best EMA pseudo Dice: 0.6255
2024-12-01 05:29:33.475698: 
2024-12-01 05:29:33.477184: Epoch 1
2024-12-01 05:29:33.478239: Current learning rate: 0.00999
2024-12-01 05:31:00.637980: Validation loss improved from -0.35802 to -0.44365! Patience: 0/50
2024-12-01 05:31:00.639078: train_loss -0.4371
2024-12-01 05:31:00.639858: val_loss -0.4436
2024-12-01 05:31:00.640759: Pseudo dice [0.6714]
2024-12-01 05:31:00.641716: Epoch time: 87.16 s
2024-12-01 05:31:00.642621: Yayy! New best EMA pseudo Dice: 0.63
2024-12-01 05:31:02.239958: 
2024-12-01 05:31:02.241526: Epoch 2
2024-12-01 05:31:02.242504: Current learning rate: 0.00998
2024-12-01 05:32:29.913856: Validation loss did not improve from -0.44365. Patience: 1/50
2024-12-01 05:32:29.914870: train_loss -0.462
2024-12-01 05:32:29.915736: val_loss -0.4349
2024-12-01 05:32:29.916492: Pseudo dice [0.6742]
2024-12-01 05:32:29.917166: Epoch time: 87.68 s
2024-12-01 05:32:29.918015: Yayy! New best EMA pseudo Dice: 0.6345
2024-12-01 05:32:31.576387: 
2024-12-01 05:32:31.577876: Epoch 3
2024-12-01 05:32:31.578635: Current learning rate: 0.00997
2024-12-01 05:33:58.920027: Validation loss improved from -0.44365 to -0.47555! Patience: 1/50
2024-12-01 05:33:58.920916: train_loss -0.4774
2024-12-01 05:33:58.921633: val_loss -0.4756
2024-12-01 05:33:58.922223: Pseudo dice [0.6915]
2024-12-01 05:33:58.922765: Epoch time: 87.35 s
2024-12-01 05:33:58.923354: Yayy! New best EMA pseudo Dice: 0.6402
2024-12-01 05:34:00.476437: 
2024-12-01 05:34:00.478049: Epoch 4
2024-12-01 05:34:00.479123: Current learning rate: 0.00996
2024-12-01 05:35:26.942077: Validation loss did not improve from -0.47555. Patience: 1/50
2024-12-01 05:35:26.942974: train_loss -0.5109
2024-12-01 05:35:26.944112: val_loss -0.4671
2024-12-01 05:35:26.945174: Pseudo dice [0.6931]
2024-12-01 05:35:26.946306: Epoch time: 86.47 s
2024-12-01 05:35:27.269482: Yayy! New best EMA pseudo Dice: 0.6455
2024-12-01 05:35:28.945847: 
2024-12-01 05:35:28.947463: Epoch 5
2024-12-01 05:35:28.948456: Current learning rate: 0.00995
2024-12-01 05:36:56.420956: Validation loss improved from -0.47555 to -0.48168! Patience: 1/50
2024-12-01 05:36:56.421738: train_loss -0.5199
2024-12-01 05:36:56.422588: val_loss -0.4817
2024-12-01 05:36:56.423492: Pseudo dice [0.6961]
2024-12-01 05:36:56.424289: Epoch time: 87.48 s
2024-12-01 05:36:56.425074: Yayy! New best EMA pseudo Dice: 0.6505
2024-12-01 05:36:58.032556: 
2024-12-01 05:36:58.034368: Epoch 6
2024-12-01 05:36:58.035249: Current learning rate: 0.00995
2024-12-01 05:38:24.574532: Validation loss did not improve from -0.48168. Patience: 1/50
2024-12-01 05:38:24.575614: train_loss -0.5294
2024-12-01 05:38:24.576570: val_loss -0.4176
2024-12-01 05:38:24.577394: Pseudo dice [0.6633]
2024-12-01 05:38:24.578145: Epoch time: 86.54 s
2024-12-01 05:38:24.579027: Yayy! New best EMA pseudo Dice: 0.6518
2024-12-01 05:38:26.197175: 
2024-12-01 05:38:26.198753: Epoch 7
2024-12-01 05:38:26.199608: Current learning rate: 0.00994
2024-12-01 05:39:53.739092: Validation loss improved from -0.48168 to -0.49996! Patience: 1/50
2024-12-01 05:39:53.740295: train_loss -0.5306
2024-12-01 05:39:53.741029: val_loss -0.5
2024-12-01 05:39:53.741982: Pseudo dice [0.7013]
2024-12-01 05:39:53.742750: Epoch time: 87.54 s
2024-12-01 05:39:53.743498: Yayy! New best EMA pseudo Dice: 0.6568
2024-12-01 05:39:55.962769: 
2024-12-01 05:39:55.964268: Epoch 8
2024-12-01 05:39:55.965294: Current learning rate: 0.00993
2024-12-01 05:41:23.758763: Validation loss improved from -0.49996 to -0.51947! Patience: 0/50
2024-12-01 05:41:23.759994: train_loss -0.5381
2024-12-01 05:41:23.761205: val_loss -0.5195
2024-12-01 05:41:23.762112: Pseudo dice [0.7141]
2024-12-01 05:41:23.763108: Epoch time: 87.8 s
2024-12-01 05:41:23.763998: Yayy! New best EMA pseudo Dice: 0.6625
2024-12-01 05:41:25.426763: 
2024-12-01 05:41:25.428223: Epoch 9
2024-12-01 05:41:25.429070: Current learning rate: 0.00992
2024-12-01 05:42:52.154575: Validation loss did not improve from -0.51947. Patience: 1/50
2024-12-01 05:42:52.155385: train_loss -0.5487
2024-12-01 05:42:52.156312: val_loss -0.4801
2024-12-01 05:42:52.157000: Pseudo dice [0.6952]
2024-12-01 05:42:52.157728: Epoch time: 86.73 s
2024-12-01 05:42:52.524560: Yayy! New best EMA pseudo Dice: 0.6658
2024-12-01 05:42:54.084673: 
2024-12-01 05:42:54.086402: Epoch 10
2024-12-01 05:42:54.087712: Current learning rate: 0.00991
2024-12-01 05:44:21.426349: Validation loss did not improve from -0.51947. Patience: 2/50
2024-12-01 05:44:21.427320: train_loss -0.569
2024-12-01 05:44:21.428026: val_loss -0.4932
2024-12-01 05:44:21.428637: Pseudo dice [0.6995]
2024-12-01 05:44:21.429272: Epoch time: 87.34 s
2024-12-01 05:44:21.429955: Yayy! New best EMA pseudo Dice: 0.6691
2024-12-01 05:44:22.990240: 
2024-12-01 05:44:22.991936: Epoch 11
2024-12-01 05:44:22.992946: Current learning rate: 0.0099
2024-12-01 05:45:49.615211: Validation loss did not improve from -0.51947. Patience: 3/50
2024-12-01 05:45:49.616384: train_loss -0.5721
2024-12-01 05:45:49.617280: val_loss -0.4845
2024-12-01 05:45:49.618030: Pseudo dice [0.6999]
2024-12-01 05:45:49.618703: Epoch time: 86.63 s
2024-12-01 05:45:49.619296: Yayy! New best EMA pseudo Dice: 0.6722
2024-12-01 05:45:51.210790: 
2024-12-01 05:45:51.212477: Epoch 12
2024-12-01 05:45:51.213360: Current learning rate: 0.00989
2024-12-01 05:47:17.984237: Validation loss improved from -0.51947 to -0.53541! Patience: 3/50
2024-12-01 05:47:17.985119: train_loss -0.5534
2024-12-01 05:47:17.986125: val_loss -0.5354
2024-12-01 05:47:17.987013: Pseudo dice [0.7208]
2024-12-01 05:47:17.987923: Epoch time: 86.78 s
2024-12-01 05:47:17.988626: Yayy! New best EMA pseudo Dice: 0.6771
2024-12-01 05:47:19.622740: 
2024-12-01 05:47:19.624147: Epoch 13
2024-12-01 05:47:19.625172: Current learning rate: 0.00988
2024-12-01 05:48:46.326217: Validation loss did not improve from -0.53541. Patience: 1/50
2024-12-01 05:48:46.327330: train_loss -0.5616
2024-12-01 05:48:46.328245: val_loss -0.5268
2024-12-01 05:48:46.328973: Pseudo dice [0.7207]
2024-12-01 05:48:46.329825: Epoch time: 86.71 s
2024-12-01 05:48:46.330531: Yayy! New best EMA pseudo Dice: 0.6814
2024-12-01 05:48:47.985162: 
2024-12-01 05:48:47.986936: Epoch 14
2024-12-01 05:48:47.988182: Current learning rate: 0.00987
2024-12-01 05:50:14.785768: Validation loss did not improve from -0.53541. Patience: 2/50
2024-12-01 05:50:14.786664: train_loss -0.5768
2024-12-01 05:50:14.787476: val_loss -0.4471
2024-12-01 05:50:14.788230: Pseudo dice [0.6843]
2024-12-01 05:50:14.789152: Epoch time: 86.8 s
2024-12-01 05:50:15.136319: Yayy! New best EMA pseudo Dice: 0.6817
2024-12-01 05:50:16.731317: 
2024-12-01 05:50:16.732674: Epoch 15
2024-12-01 05:50:16.733458: Current learning rate: 0.00986
2024-12-01 05:51:44.102313: Validation loss did not improve from -0.53541. Patience: 3/50
2024-12-01 05:51:44.103276: train_loss -0.5845
2024-12-01 05:51:44.104083: val_loss -0.5254
2024-12-01 05:51:44.104722: Pseudo dice [0.7191]
2024-12-01 05:51:44.105575: Epoch time: 87.37 s
2024-12-01 05:51:44.106186: Yayy! New best EMA pseudo Dice: 0.6855
2024-12-01 05:51:45.796021: 
2024-12-01 05:51:45.797487: Epoch 16
2024-12-01 05:51:45.798557: Current learning rate: 0.00986
2024-12-01 05:53:12.820507: Validation loss did not improve from -0.53541. Patience: 4/50
2024-12-01 05:53:12.821426: train_loss -0.594
2024-12-01 05:53:12.822418: val_loss -0.5348
2024-12-01 05:53:12.823235: Pseudo dice [0.7212]
2024-12-01 05:53:12.824032: Epoch time: 87.03 s
2024-12-01 05:53:12.824735: Yayy! New best EMA pseudo Dice: 0.689
2024-12-01 05:53:14.508732: 
2024-12-01 05:53:14.510336: Epoch 17
2024-12-01 05:53:14.511422: Current learning rate: 0.00985
2024-12-01 05:54:41.499027: Validation loss did not improve from -0.53541. Patience: 5/50
2024-12-01 05:54:41.499936: train_loss -0.6011
2024-12-01 05:54:41.500785: val_loss -0.5114
2024-12-01 05:54:41.501510: Pseudo dice [0.7131]
2024-12-01 05:54:41.502139: Epoch time: 86.99 s
2024-12-01 05:54:41.502776: Yayy! New best EMA pseudo Dice: 0.6914
2024-12-01 05:54:43.199577: 
2024-12-01 05:54:43.201001: Epoch 18
2024-12-01 05:54:43.201803: Current learning rate: 0.00984
2024-12-01 05:56:10.159651: Validation loss did not improve from -0.53541. Patience: 6/50
2024-12-01 05:56:10.160683: train_loss -0.6008
2024-12-01 05:56:10.161552: val_loss -0.5173
2024-12-01 05:56:10.162173: Pseudo dice [0.7198]
2024-12-01 05:56:10.162837: Epoch time: 86.96 s
2024-12-01 05:56:10.163462: Yayy! New best EMA pseudo Dice: 0.6943
2024-12-01 05:56:12.177658: 
2024-12-01 05:56:12.178867: Epoch 19
2024-12-01 05:56:12.179730: Current learning rate: 0.00983
2024-12-01 05:57:39.191170: Validation loss did not improve from -0.53541. Patience: 7/50
2024-12-01 05:57:39.192646: train_loss -0.5937
2024-12-01 05:57:39.193377: val_loss -0.5309
2024-12-01 05:57:39.194068: Pseudo dice [0.7235]
2024-12-01 05:57:39.194700: Epoch time: 87.02 s
2024-12-01 05:57:39.563154: Yayy! New best EMA pseudo Dice: 0.6972
2024-12-01 05:57:41.209374: 
2024-12-01 05:57:41.210985: Epoch 20
2024-12-01 05:57:41.212001: Current learning rate: 0.00982
2024-12-01 05:59:08.196869: Validation loss improved from -0.53541 to -0.55515! Patience: 7/50
2024-12-01 05:59:08.197966: train_loss -0.6097
2024-12-01 05:59:08.199293: val_loss -0.5552
2024-12-01 05:59:08.200299: Pseudo dice [0.7326]
2024-12-01 05:59:08.201069: Epoch time: 86.99 s
2024-12-01 05:59:08.201810: Yayy! New best EMA pseudo Dice: 0.7007
2024-12-01 05:59:09.876101: 
2024-12-01 05:59:09.877580: Epoch 21
2024-12-01 05:59:09.878349: Current learning rate: 0.00981
2024-12-01 06:00:36.945124: Validation loss did not improve from -0.55515. Patience: 1/50
2024-12-01 06:00:36.946306: train_loss -0.6076
2024-12-01 06:00:36.947125: val_loss -0.5364
2024-12-01 06:00:36.947986: Pseudo dice [0.7269]
2024-12-01 06:00:36.948866: Epoch time: 87.07 s
2024-12-01 06:00:36.949824: Yayy! New best EMA pseudo Dice: 0.7034
2024-12-01 06:00:38.552909: 
2024-12-01 06:00:38.554435: Epoch 22
2024-12-01 06:00:38.555562: Current learning rate: 0.0098
2024-12-01 06:02:05.769604: Validation loss improved from -0.55515 to -0.55623! Patience: 1/50
2024-12-01 06:02:05.770631: train_loss -0.5966
2024-12-01 06:02:05.771310: val_loss -0.5562
2024-12-01 06:02:05.771907: Pseudo dice [0.7337]
2024-12-01 06:02:05.772527: Epoch time: 87.22 s
2024-12-01 06:02:05.773157: Yayy! New best EMA pseudo Dice: 0.7064
2024-12-01 06:02:07.385955: 
2024-12-01 06:02:07.387354: Epoch 23
2024-12-01 06:02:07.388018: Current learning rate: 0.00979
2024-12-01 06:03:34.495159: Validation loss did not improve from -0.55623. Patience: 1/50
2024-12-01 06:03:34.495875: train_loss -0.607
2024-12-01 06:03:34.496816: val_loss -0.5339
2024-12-01 06:03:34.497555: Pseudo dice [0.7167]
2024-12-01 06:03:34.498289: Epoch time: 87.11 s
2024-12-01 06:03:34.498956: Yayy! New best EMA pseudo Dice: 0.7074
2024-12-01 06:03:36.109694: 
2024-12-01 06:03:36.111162: Epoch 24
2024-12-01 06:03:36.111903: Current learning rate: 0.00978
2024-12-01 06:05:03.604202: Validation loss did not improve from -0.55623. Patience: 2/50
2024-12-01 06:05:03.605199: train_loss -0.6062
2024-12-01 06:05:03.606019: val_loss -0.5167
2024-12-01 06:05:03.606698: Pseudo dice [0.7191]
2024-12-01 06:05:03.607352: Epoch time: 87.5 s
2024-12-01 06:05:03.971805: Yayy! New best EMA pseudo Dice: 0.7086
2024-12-01 06:05:05.533732: 
2024-12-01 06:05:05.535298: Epoch 25
2024-12-01 06:05:05.536349: Current learning rate: 0.00977
2024-12-01 06:06:32.915812: Validation loss did not improve from -0.55623. Patience: 3/50
2024-12-01 06:06:32.917133: train_loss -0.6219
2024-12-01 06:06:32.918321: val_loss -0.5511
2024-12-01 06:06:32.919220: Pseudo dice [0.7355]
2024-12-01 06:06:32.920024: Epoch time: 87.38 s
2024-12-01 06:06:32.921104: Yayy! New best EMA pseudo Dice: 0.7113
2024-12-01 06:06:34.509558: 
2024-12-01 06:06:34.511528: Epoch 26
2024-12-01 06:06:34.512586: Current learning rate: 0.00977
2024-12-01 06:08:01.966856: Validation loss did not improve from -0.55623. Patience: 4/50
2024-12-01 06:08:01.967781: train_loss -0.6215
2024-12-01 06:08:01.968641: val_loss -0.5471
2024-12-01 06:08:01.969354: Pseudo dice [0.7324]
2024-12-01 06:08:01.969971: Epoch time: 87.46 s
2024-12-01 06:08:01.970558: Yayy! New best EMA pseudo Dice: 0.7134
2024-12-01 06:08:03.572560: 
2024-12-01 06:08:03.573958: Epoch 27
2024-12-01 06:08:03.574893: Current learning rate: 0.00976
2024-12-01 06:09:30.942609: Validation loss improved from -0.55623 to -0.55713! Patience: 4/50
2024-12-01 06:09:30.943415: train_loss -0.6212
2024-12-01 06:09:30.944221: val_loss -0.5571
2024-12-01 06:09:30.945028: Pseudo dice [0.7374]
2024-12-01 06:09:30.945721: Epoch time: 87.37 s
2024-12-01 06:09:30.946732: Yayy! New best EMA pseudo Dice: 0.7158
2024-12-01 06:09:32.521962: 
2024-12-01 06:09:32.523304: Epoch 28
2024-12-01 06:09:32.524205: Current learning rate: 0.00975
2024-12-01 06:10:59.998327: Validation loss improved from -0.55713 to -0.56700! Patience: 0/50
2024-12-01 06:10:59.999566: train_loss -0.6316
2024-12-01 06:11:00.000694: val_loss -0.567
2024-12-01 06:11:00.001518: Pseudo dice [0.7396]
2024-12-01 06:11:00.002169: Epoch time: 87.48 s
2024-12-01 06:11:00.002851: Yayy! New best EMA pseudo Dice: 0.7182
2024-12-01 06:11:01.966260: 
2024-12-01 06:11:01.967637: Epoch 29
2024-12-01 06:11:01.968300: Current learning rate: 0.00974
2024-12-01 06:12:29.368319: Validation loss did not improve from -0.56700. Patience: 1/50
2024-12-01 06:12:29.369264: train_loss -0.6341
2024-12-01 06:12:29.370287: val_loss -0.534
2024-12-01 06:12:29.371004: Pseudo dice [0.7218]
2024-12-01 06:12:29.371727: Epoch time: 87.4 s
2024-12-01 06:12:29.748277: Yayy! New best EMA pseudo Dice: 0.7185
2024-12-01 06:12:31.363635: 
2024-12-01 06:12:31.365367: Epoch 30
2024-12-01 06:12:31.366176: Current learning rate: 0.00973
2024-12-01 06:13:58.737920: Validation loss did not improve from -0.56700. Patience: 2/50
2024-12-01 06:13:58.739230: train_loss -0.626
2024-12-01 06:13:58.740245: val_loss -0.5645
2024-12-01 06:13:58.741235: Pseudo dice [0.7371]
2024-12-01 06:13:58.742344: Epoch time: 87.38 s
2024-12-01 06:13:58.742998: Yayy! New best EMA pseudo Dice: 0.7204
2024-12-01 06:14:00.346715: 
2024-12-01 06:14:00.348139: Epoch 31
2024-12-01 06:14:00.348914: Current learning rate: 0.00972
2024-12-01 06:15:27.762269: Validation loss did not improve from -0.56700. Patience: 3/50
2024-12-01 06:15:27.763458: train_loss -0.6327
2024-12-01 06:15:27.764637: val_loss -0.5609
2024-12-01 06:15:27.765315: Pseudo dice [0.7429]
2024-12-01 06:15:27.766116: Epoch time: 87.42 s
2024-12-01 06:15:27.766706: Yayy! New best EMA pseudo Dice: 0.7226
2024-12-01 06:15:29.362951: 
2024-12-01 06:15:29.364923: Epoch 32
2024-12-01 06:15:29.365971: Current learning rate: 0.00971
2024-12-01 06:16:57.008368: Validation loss did not improve from -0.56700. Patience: 4/50
2024-12-01 06:16:57.009498: train_loss -0.6388
2024-12-01 06:16:57.010508: val_loss -0.5521
2024-12-01 06:16:57.011504: Pseudo dice [0.7306]
2024-12-01 06:16:57.012303: Epoch time: 87.65 s
2024-12-01 06:16:57.013143: Yayy! New best EMA pseudo Dice: 0.7234
2024-12-01 06:16:58.665980: 
2024-12-01 06:16:58.667050: Epoch 33
2024-12-01 06:16:58.667871: Current learning rate: 0.0097
2024-12-01 06:18:26.052908: Validation loss did not improve from -0.56700. Patience: 5/50
2024-12-01 06:18:26.053862: train_loss -0.629
2024-12-01 06:18:26.054652: val_loss -0.5506
2024-12-01 06:18:26.055309: Pseudo dice [0.731]
2024-12-01 06:18:26.055986: Epoch time: 87.39 s
2024-12-01 06:18:26.056641: Yayy! New best EMA pseudo Dice: 0.7242
2024-12-01 06:18:27.679943: 
2024-12-01 06:18:27.681259: Epoch 34
2024-12-01 06:18:27.682028: Current learning rate: 0.00969
2024-12-01 06:19:55.196173: Validation loss did not improve from -0.56700. Patience: 6/50
2024-12-01 06:19:55.197240: train_loss -0.63
2024-12-01 06:19:55.198022: val_loss -0.5375
2024-12-01 06:19:55.198801: Pseudo dice [0.7226]
2024-12-01 06:19:55.199456: Epoch time: 87.52 s
2024-12-01 06:19:56.859992: 
2024-12-01 06:19:56.861687: Epoch 35
2024-12-01 06:19:56.862377: Current learning rate: 0.00968
2024-12-01 06:21:24.223679: Validation loss did not improve from -0.56700. Patience: 7/50
2024-12-01 06:21:24.224599: train_loss -0.6404
2024-12-01 06:21:24.225663: val_loss -0.5436
2024-12-01 06:21:24.226287: Pseudo dice [0.7384]
2024-12-01 06:21:24.226976: Epoch time: 87.37 s
2024-12-01 06:21:24.227592: Yayy! New best EMA pseudo Dice: 0.7255
2024-12-01 06:21:25.864098: 
2024-12-01 06:21:25.864932: Epoch 36
2024-12-01 06:21:25.865566: Current learning rate: 0.00968
2024-12-01 06:22:53.214598: Validation loss did not improve from -0.56700. Patience: 8/50
2024-12-01 06:22:53.215745: train_loss -0.6435
2024-12-01 06:22:53.216610: val_loss -0.5634
2024-12-01 06:22:53.217493: Pseudo dice [0.7483]
2024-12-01 06:22:53.218462: Epoch time: 87.35 s
2024-12-01 06:22:53.219378: Yayy! New best EMA pseudo Dice: 0.7278
2024-12-01 06:22:54.941416: 
2024-12-01 06:22:54.943213: Epoch 37
2024-12-01 06:22:54.944073: Current learning rate: 0.00967
2024-12-01 06:24:22.316921: Validation loss improved from -0.56700 to -0.57347! Patience: 8/50
2024-12-01 06:24:22.317867: train_loss -0.6331
2024-12-01 06:24:22.318733: val_loss -0.5735
2024-12-01 06:24:22.319381: Pseudo dice [0.75]
2024-12-01 06:24:22.320121: Epoch time: 87.38 s
2024-12-01 06:24:22.320734: Yayy! New best EMA pseudo Dice: 0.73
2024-12-01 06:24:24.001040: 
2024-12-01 06:24:24.002737: Epoch 38
2024-12-01 06:24:24.003563: Current learning rate: 0.00966
2024-12-01 06:25:51.324261: Validation loss did not improve from -0.57347. Patience: 1/50
2024-12-01 06:25:51.325643: train_loss -0.6481
2024-12-01 06:25:51.326471: val_loss -0.5527
2024-12-01 06:25:51.327157: Pseudo dice [0.7348]
2024-12-01 06:25:51.327936: Epoch time: 87.33 s
2024-12-01 06:25:51.328597: Yayy! New best EMA pseudo Dice: 0.7305
2024-12-01 06:25:53.307312: 
2024-12-01 06:25:53.308777: Epoch 39
2024-12-01 06:25:53.309458: Current learning rate: 0.00965
2024-12-01 06:27:20.668805: Validation loss did not improve from -0.57347. Patience: 2/50
2024-12-01 06:27:20.669810: train_loss -0.6548
2024-12-01 06:27:20.670774: val_loss -0.5303
2024-12-01 06:27:20.671486: Pseudo dice [0.7216]
2024-12-01 06:27:20.672121: Epoch time: 87.36 s
2024-12-01 06:27:22.363809: 
2024-12-01 06:27:22.365410: Epoch 40
2024-12-01 06:27:22.366162: Current learning rate: 0.00964
2024-12-01 06:28:49.716397: Validation loss did not improve from -0.57347. Patience: 3/50
2024-12-01 06:28:49.717614: train_loss -0.6531
2024-12-01 06:28:49.718383: val_loss -0.5607
2024-12-01 06:28:49.719173: Pseudo dice [0.7399]
2024-12-01 06:28:49.719926: Epoch time: 87.35 s
2024-12-01 06:28:49.720562: Yayy! New best EMA pseudo Dice: 0.7306
2024-12-01 06:28:51.426009: 
2024-12-01 06:28:51.427531: Epoch 41
2024-12-01 06:28:51.428166: Current learning rate: 0.00963
2024-12-01 06:30:19.011190: Validation loss did not improve from -0.57347. Patience: 4/50
2024-12-01 06:30:19.013185: train_loss -0.6415
2024-12-01 06:30:19.014486: val_loss -0.5552
2024-12-01 06:30:19.015388: Pseudo dice [0.7336]
2024-12-01 06:30:19.016086: Epoch time: 87.59 s
2024-12-01 06:30:19.016826: Yayy! New best EMA pseudo Dice: 0.7309
2024-12-01 06:30:20.604760: 
2024-12-01 06:30:20.605702: Epoch 42
2024-12-01 06:30:20.606532: Current learning rate: 0.00962
2024-12-01 06:31:48.061109: Validation loss did not improve from -0.57347. Patience: 5/50
2024-12-01 06:31:48.062016: train_loss -0.642
2024-12-01 06:31:48.062779: val_loss -0.5727
2024-12-01 06:31:48.063753: Pseudo dice [0.7445]
2024-12-01 06:31:48.064395: Epoch time: 87.46 s
2024-12-01 06:31:48.065145: Yayy! New best EMA pseudo Dice: 0.7323
2024-12-01 06:31:49.691949: 
2024-12-01 06:31:49.693282: Epoch 43
2024-12-01 06:31:49.694001: Current learning rate: 0.00961
2024-12-01 06:33:17.093509: Validation loss did not improve from -0.57347. Patience: 6/50
2024-12-01 06:33:17.094206: train_loss -0.6478
2024-12-01 06:33:17.095012: val_loss -0.5581
2024-12-01 06:33:17.095776: Pseudo dice [0.7427]
2024-12-01 06:33:17.096439: Epoch time: 87.4 s
2024-12-01 06:33:17.097149: Yayy! New best EMA pseudo Dice: 0.7333
2024-12-01 06:33:18.740239: 
2024-12-01 06:33:18.741899: Epoch 44
2024-12-01 06:33:18.742649: Current learning rate: 0.0096
2024-12-01 06:34:46.267326: Validation loss did not improve from -0.57347. Patience: 7/50
2024-12-01 06:34:46.270015: train_loss -0.65
2024-12-01 06:34:46.272195: val_loss -0.5627
2024-12-01 06:34:46.273010: Pseudo dice [0.7488]
2024-12-01 06:34:46.273886: Epoch time: 87.53 s
2024-12-01 06:34:46.652182: Yayy! New best EMA pseudo Dice: 0.7349
2024-12-01 06:34:48.339123: 
2024-12-01 06:34:48.340852: Epoch 45
2024-12-01 06:34:48.341523: Current learning rate: 0.00959
2024-12-01 06:36:15.716813: Validation loss did not improve from -0.57347. Patience: 8/50
2024-12-01 06:36:15.717880: train_loss -0.6585
2024-12-01 06:36:15.718566: val_loss -0.5324
2024-12-01 06:36:15.719228: Pseudo dice [0.7256]
2024-12-01 06:36:15.720115: Epoch time: 87.38 s
2024-12-01 06:36:16.980853: 
2024-12-01 06:36:16.982545: Epoch 46
2024-12-01 06:36:16.983372: Current learning rate: 0.00959
2024-12-01 06:37:44.350413: Validation loss improved from -0.57347 to -0.58832! Patience: 8/50
2024-12-01 06:37:44.351185: train_loss -0.6469
2024-12-01 06:37:44.352171: val_loss -0.5883
2024-12-01 06:37:44.352912: Pseudo dice [0.7594]
2024-12-01 06:37:44.353579: Epoch time: 87.37 s
2024-12-01 06:37:44.354312: Yayy! New best EMA pseudo Dice: 0.7365
2024-12-01 06:37:46.007495: 
2024-12-01 06:37:46.008569: Epoch 47
2024-12-01 06:37:46.009393: Current learning rate: 0.00958
2024-12-01 06:39:13.521215: Validation loss did not improve from -0.58832. Patience: 1/50
2024-12-01 06:39:13.522271: train_loss -0.6441
2024-12-01 06:39:13.523218: val_loss -0.5313
2024-12-01 06:39:13.524089: Pseudo dice [0.7317]
2024-12-01 06:39:13.524951: Epoch time: 87.52 s
2024-12-01 06:39:14.743238: 
2024-12-01 06:39:14.745224: Epoch 48
2024-12-01 06:39:14.746054: Current learning rate: 0.00957
2024-12-01 06:40:42.096856: Validation loss did not improve from -0.58832. Patience: 2/50
2024-12-01 06:40:42.097914: train_loss -0.663
2024-12-01 06:40:42.098877: val_loss -0.5681
2024-12-01 06:40:42.099639: Pseudo dice [0.7435]
2024-12-01 06:40:42.100420: Epoch time: 87.36 s
2024-12-01 06:40:42.101218: Yayy! New best EMA pseudo Dice: 0.7367
2024-12-01 06:40:44.246691: 
2024-12-01 06:40:44.248488: Epoch 49
2024-12-01 06:40:44.249439: Current learning rate: 0.00956
2024-12-01 06:42:11.675483: Validation loss did not improve from -0.58832. Patience: 3/50
2024-12-01 06:42:11.676566: train_loss -0.6628
2024-12-01 06:42:11.677418: val_loss -0.5457
2024-12-01 06:42:11.678492: Pseudo dice [0.7369]
2024-12-01 06:42:11.679639: Epoch time: 87.43 s
2024-12-01 06:42:12.043047: Yayy! New best EMA pseudo Dice: 0.7368
2024-12-01 06:42:13.627955: 
2024-12-01 06:42:13.629560: Epoch 50
2024-12-01 06:42:13.630398: Current learning rate: 0.00955
2024-12-01 06:43:41.268646: Validation loss did not improve from -0.58832. Patience: 4/50
2024-12-01 06:43:41.269766: train_loss -0.6579
2024-12-01 06:43:41.270908: val_loss -0.574
2024-12-01 06:43:41.272035: Pseudo dice [0.753]
2024-12-01 06:43:41.273148: Epoch time: 87.64 s
2024-12-01 06:43:41.274116: Yayy! New best EMA pseudo Dice: 0.7384
2024-12-01 06:43:42.899992: 
2024-12-01 06:43:42.901495: Epoch 51
2024-12-01 06:43:42.902529: Current learning rate: 0.00954
2024-12-01 06:45:10.454053: Validation loss did not improve from -0.58832. Patience: 5/50
2024-12-01 06:45:10.455135: train_loss -0.6652
2024-12-01 06:45:10.456365: val_loss -0.5527
2024-12-01 06:45:10.457767: Pseudo dice [0.7301]
2024-12-01 06:45:10.458848: Epoch time: 87.56 s
2024-12-01 06:45:11.781193: 
2024-12-01 06:45:11.782644: Epoch 52
2024-12-01 06:45:11.783443: Current learning rate: 0.00953
2024-12-01 06:46:39.257725: Validation loss did not improve from -0.58832. Patience: 6/50
2024-12-01 06:46:39.259098: train_loss -0.668
2024-12-01 06:46:39.260189: val_loss -0.5871
2024-12-01 06:46:39.260887: Pseudo dice [0.7603]
2024-12-01 06:46:39.261660: Epoch time: 87.48 s
2024-12-01 06:46:39.262317: Yayy! New best EMA pseudo Dice: 0.7398
2024-12-01 06:46:40.931326: 
2024-12-01 06:46:40.933365: Epoch 53
2024-12-01 06:46:40.934425: Current learning rate: 0.00952
2024-12-01 06:48:08.317189: Validation loss did not improve from -0.58832. Patience: 7/50
2024-12-01 06:48:08.318148: train_loss -0.6636
2024-12-01 06:48:08.318974: val_loss -0.562
2024-12-01 06:48:08.319712: Pseudo dice [0.7516]
2024-12-01 06:48:08.320483: Epoch time: 87.39 s
2024-12-01 06:48:08.321269: Yayy! New best EMA pseudo Dice: 0.741
2024-12-01 06:48:09.950552: 
2024-12-01 06:48:09.951861: Epoch 54
2024-12-01 06:48:09.952541: Current learning rate: 0.00951
2024-12-01 06:49:37.440905: Validation loss did not improve from -0.58832. Patience: 8/50
2024-12-01 06:49:37.441978: train_loss -0.6664
2024-12-01 06:49:37.442908: val_loss -0.5781
2024-12-01 06:49:37.443741: Pseudo dice [0.751]
2024-12-01 06:49:37.444534: Epoch time: 87.49 s
2024-12-01 06:49:37.824707: Yayy! New best EMA pseudo Dice: 0.742
2024-12-01 06:49:39.477155: 
2024-12-01 06:49:39.478907: Epoch 55
2024-12-01 06:49:39.479970: Current learning rate: 0.0095
2024-12-01 06:51:07.150255: Validation loss did not improve from -0.58832. Patience: 9/50
2024-12-01 06:51:07.151431: train_loss -0.6818
2024-12-01 06:51:07.152632: val_loss -0.5732
2024-12-01 06:51:07.153526: Pseudo dice [0.7493]
2024-12-01 06:51:07.154513: Epoch time: 87.68 s
2024-12-01 06:51:07.155440: Yayy! New best EMA pseudo Dice: 0.7427
2024-12-01 06:51:08.811829: 
2024-12-01 06:51:08.813973: Epoch 56
2024-12-01 06:51:08.815000: Current learning rate: 0.00949
2024-12-01 06:52:36.169752: Validation loss did not improve from -0.58832. Patience: 10/50
2024-12-01 06:52:36.170694: train_loss -0.6628
2024-12-01 06:52:36.171538: val_loss -0.5468
2024-12-01 06:52:36.172525: Pseudo dice [0.7265]
2024-12-01 06:52:36.173247: Epoch time: 87.36 s
2024-12-01 06:52:37.422767: 
2024-12-01 06:52:37.424405: Epoch 57
2024-12-01 06:52:37.425256: Current learning rate: 0.00949
2024-12-01 06:54:04.821790: Validation loss did not improve from -0.58832. Patience: 11/50
2024-12-01 06:54:04.823012: train_loss -0.6754
2024-12-01 06:54:04.824179: val_loss -0.5652
2024-12-01 06:54:04.825035: Pseudo dice [0.7437]
2024-12-01 06:54:04.826107: Epoch time: 87.4 s
2024-12-01 06:54:06.070127: 
2024-12-01 06:54:06.071556: Epoch 58
2024-12-01 06:54:06.072160: Current learning rate: 0.00948
2024-12-01 06:55:33.137312: Validation loss did not improve from -0.58832. Patience: 12/50
2024-12-01 06:55:33.138613: train_loss -0.6763
2024-12-01 06:55:33.139772: val_loss -0.5768
2024-12-01 06:55:33.140452: Pseudo dice [0.7492]
2024-12-01 06:55:33.141169: Epoch time: 87.07 s
2024-12-01 06:55:34.413328: 
2024-12-01 06:55:34.414811: Epoch 59
2024-12-01 06:55:34.415467: Current learning rate: 0.00947
2024-12-01 06:57:01.494093: Validation loss did not improve from -0.58832. Patience: 13/50
2024-12-01 06:57:01.495218: train_loss -0.6719
2024-12-01 06:57:01.496169: val_loss -0.5219
2024-12-01 06:57:01.496849: Pseudo dice [0.7266]
2024-12-01 06:57:01.497709: Epoch time: 87.08 s
2024-12-01 06:57:03.495134: 
2024-12-01 06:57:03.496509: Epoch 60
2024-12-01 06:57:03.497268: Current learning rate: 0.00946
2024-12-01 06:58:30.577967: Validation loss did not improve from -0.58832. Patience: 14/50
2024-12-01 06:58:30.578695: train_loss -0.6707
2024-12-01 06:58:30.579717: val_loss -0.5412
2024-12-01 06:58:30.580663: Pseudo dice [0.7279]
2024-12-01 06:58:30.581664: Epoch time: 87.08 s
2024-12-01 06:58:31.883755: 
2024-12-01 06:58:31.885556: Epoch 61
2024-12-01 06:58:31.886281: Current learning rate: 0.00945
2024-12-01 06:59:58.939278: Validation loss did not improve from -0.58832. Patience: 15/50
2024-12-01 06:59:58.940211: train_loss -0.6707
2024-12-01 06:59:58.941094: val_loss -0.5874
2024-12-01 06:59:58.941959: Pseudo dice [0.7605]
2024-12-01 06:59:58.943042: Epoch time: 87.06 s
2024-12-01 07:00:00.216695: 
2024-12-01 07:00:00.218281: Epoch 62
2024-12-01 07:00:00.219048: Current learning rate: 0.00944
2024-12-01 07:01:27.370450: Validation loss did not improve from -0.58832. Patience: 16/50
2024-12-01 07:01:27.371620: train_loss -0.6791
2024-12-01 07:01:27.372364: val_loss -0.5674
2024-12-01 07:01:27.373164: Pseudo dice [0.7419]
2024-12-01 07:01:27.373848: Epoch time: 87.16 s
2024-12-01 07:01:28.697784: 
2024-12-01 07:01:28.699312: Epoch 63
2024-12-01 07:01:28.700144: Current learning rate: 0.00943
2024-12-01 07:02:55.782138: Validation loss did not improve from -0.58832. Patience: 17/50
2024-12-01 07:02:55.783095: train_loss -0.6813
2024-12-01 07:02:55.783868: val_loss -0.542
2024-12-01 07:02:55.784767: Pseudo dice [0.7337]
2024-12-01 07:02:55.785613: Epoch time: 87.09 s
2024-12-01 07:02:57.060323: 
2024-12-01 07:02:57.062002: Epoch 64
2024-12-01 07:02:57.062706: Current learning rate: 0.00942
2024-12-01 07:04:24.116123: Validation loss did not improve from -0.58832. Patience: 18/50
2024-12-01 07:04:24.116942: train_loss -0.6815
2024-12-01 07:04:24.117800: val_loss -0.5821
2024-12-01 07:04:24.118497: Pseudo dice [0.754]
2024-12-01 07:04:24.119079: Epoch time: 87.06 s
2024-12-01 07:04:25.750190: 
2024-12-01 07:04:25.751553: Epoch 65
2024-12-01 07:04:25.752193: Current learning rate: 0.00941
2024-12-01 07:05:52.888428: Validation loss did not improve from -0.58832. Patience: 19/50
2024-12-01 07:05:52.889481: train_loss -0.6794
2024-12-01 07:05:52.890414: val_loss -0.571
2024-12-01 07:05:52.891227: Pseudo dice [0.7526]
2024-12-01 07:05:52.891881: Epoch time: 87.14 s
2024-12-01 07:05:52.892565: Yayy! New best EMA pseudo Dice: 0.7431
2024-12-01 07:05:54.603226: 
2024-12-01 07:05:54.604782: Epoch 66
2024-12-01 07:05:54.605846: Current learning rate: 0.0094
2024-12-01 07:07:21.940901: Validation loss improved from -0.58832 to -0.59444! Patience: 19/50
2024-12-01 07:07:21.942050: train_loss -0.6869
2024-12-01 07:07:21.942880: val_loss -0.5944
2024-12-01 07:07:21.943632: Pseudo dice [0.7647]
2024-12-01 07:07:21.944225: Epoch time: 87.34 s
2024-12-01 07:07:21.944938: Yayy! New best EMA pseudo Dice: 0.7452
2024-12-01 07:07:23.594922: 
2024-12-01 07:07:23.596404: Epoch 67
2024-12-01 07:07:23.597372: Current learning rate: 0.00939
2024-12-01 07:08:50.855112: Validation loss improved from -0.59444 to -0.59518! Patience: 0/50
2024-12-01 07:08:50.856064: train_loss -0.678
2024-12-01 07:08:50.856847: val_loss -0.5952
2024-12-01 07:08:50.857646: Pseudo dice [0.759]
2024-12-01 07:08:50.858287: Epoch time: 87.26 s
2024-12-01 07:08:50.858992: Yayy! New best EMA pseudo Dice: 0.7466
2024-12-01 07:08:52.561400: 
2024-12-01 07:08:52.562784: Epoch 68
2024-12-01 07:08:52.563694: Current learning rate: 0.00939
2024-12-01 07:10:19.751671: Validation loss did not improve from -0.59518. Patience: 1/50
2024-12-01 07:10:19.752702: train_loss -0.6751
2024-12-01 07:10:19.753786: val_loss -0.5381
2024-12-01 07:10:19.754674: Pseudo dice [0.7288]
2024-12-01 07:10:19.755595: Epoch time: 87.19 s
2024-12-01 07:10:21.030586: 
2024-12-01 07:10:21.032308: Epoch 69
2024-12-01 07:10:21.033195: Current learning rate: 0.00938
2024-12-01 07:11:48.346390: Validation loss did not improve from -0.59518. Patience: 2/50
2024-12-01 07:11:48.347504: train_loss -0.6712
2024-12-01 07:11:48.348460: val_loss -0.5489
2024-12-01 07:11:48.349210: Pseudo dice [0.7325]
2024-12-01 07:11:48.349991: Epoch time: 87.32 s
2024-12-01 07:11:50.034705: 
2024-12-01 07:11:50.036161: Epoch 70
2024-12-01 07:11:50.036981: Current learning rate: 0.00937
2024-12-01 07:13:17.356865: Validation loss did not improve from -0.59518. Patience: 3/50
2024-12-01 07:13:17.357801: train_loss -0.671
2024-12-01 07:13:17.358685: val_loss -0.5267
2024-12-01 07:13:17.359309: Pseudo dice [0.727]
2024-12-01 07:13:17.359958: Epoch time: 87.32 s
2024-12-01 07:13:19.114275: 
2024-12-01 07:13:19.115489: Epoch 71
2024-12-01 07:13:19.116274: Current learning rate: 0.00936
2024-12-01 07:14:46.515337: Validation loss did not improve from -0.59518. Patience: 4/50
2024-12-01 07:14:46.516217: train_loss -0.6785
2024-12-01 07:14:46.516914: val_loss -0.5608
2024-12-01 07:14:46.517485: Pseudo dice [0.7463]
2024-12-01 07:14:46.518109: Epoch time: 87.4 s
2024-12-01 07:14:47.832860: 
2024-12-01 07:14:47.834328: Epoch 72
2024-12-01 07:14:47.835014: Current learning rate: 0.00935
2024-12-01 07:16:15.524100: Validation loss did not improve from -0.59518. Patience: 5/50
2024-12-01 07:16:15.525104: train_loss -0.684
2024-12-01 07:16:15.526471: val_loss -0.5746
2024-12-01 07:16:15.527255: Pseudo dice [0.7497]
2024-12-01 07:16:15.528058: Epoch time: 87.69 s
2024-12-01 07:16:16.866371: 
2024-12-01 07:16:16.867800: Epoch 73
2024-12-01 07:16:16.868768: Current learning rate: 0.00934
2024-12-01 07:17:44.389807: Validation loss did not improve from -0.59518. Patience: 6/50
2024-12-01 07:17:44.390806: train_loss -0.6903
2024-12-01 07:17:44.391789: val_loss -0.5437
2024-12-01 07:17:44.392677: Pseudo dice [0.7356]
2024-12-01 07:17:44.393578: Epoch time: 87.53 s
2024-12-01 07:17:45.697596: 
2024-12-01 07:17:45.698977: Epoch 74
2024-12-01 07:17:45.699899: Current learning rate: 0.00933
2024-12-01 07:19:13.244558: Validation loss did not improve from -0.59518. Patience: 7/50
2024-12-01 07:19:13.245627: train_loss -0.6907
2024-12-01 07:19:13.246287: val_loss -0.5555
2024-12-01 07:19:13.246978: Pseudo dice [0.7356]
2024-12-01 07:19:13.247609: Epoch time: 87.55 s
2024-12-01 07:19:14.905944: 
2024-12-01 07:19:14.907119: Epoch 75
2024-12-01 07:19:14.907813: Current learning rate: 0.00932
2024-12-01 07:20:42.395144: Validation loss did not improve from -0.59518. Patience: 8/50
2024-12-01 07:20:42.396111: train_loss -0.6949
2024-12-01 07:20:42.397067: val_loss -0.5416
2024-12-01 07:20:42.397735: Pseudo dice [0.7365]
2024-12-01 07:20:42.398376: Epoch time: 87.49 s
2024-12-01 07:20:43.708503: 
2024-12-01 07:20:43.710021: Epoch 76
2024-12-01 07:20:43.710822: Current learning rate: 0.00931
2024-12-01 07:22:11.094329: Validation loss did not improve from -0.59518. Patience: 9/50
2024-12-01 07:22:11.095222: train_loss -0.6951
2024-12-01 07:22:11.096080: val_loss -0.5571
2024-12-01 07:22:11.096751: Pseudo dice [0.7334]
2024-12-01 07:22:11.097518: Epoch time: 87.39 s
2024-12-01 07:22:12.427236: 
2024-12-01 07:22:12.428690: Epoch 77
2024-12-01 07:22:12.429313: Current learning rate: 0.0093
2024-12-01 07:23:39.770195: Validation loss improved from -0.59518 to -0.60760! Patience: 9/50
2024-12-01 07:23:39.771417: train_loss -0.6945
2024-12-01 07:23:39.772298: val_loss -0.6076
2024-12-01 07:23:39.772977: Pseudo dice [0.7654]
2024-12-01 07:23:39.773613: Epoch time: 87.35 s
2024-12-01 07:23:41.092944: 
2024-12-01 07:23:41.094460: Epoch 78
2024-12-01 07:23:41.095277: Current learning rate: 0.0093
2024-12-01 07:25:08.513700: Validation loss did not improve from -0.60760. Patience: 1/50
2024-12-01 07:25:08.514872: train_loss -0.6903
2024-12-01 07:25:08.515594: val_loss -0.5695
2024-12-01 07:25:08.516284: Pseudo dice [0.7416]
2024-12-01 07:25:08.517023: Epoch time: 87.42 s
2024-12-01 07:25:09.899723: 
2024-12-01 07:25:09.901503: Epoch 79
2024-12-01 07:25:09.902306: Current learning rate: 0.00929
2024-12-01 07:26:37.365153: Validation loss did not improve from -0.60760. Patience: 2/50
2024-12-01 07:26:37.366105: train_loss -0.6925
2024-12-01 07:26:37.366954: val_loss -0.5633
2024-12-01 07:26:37.367626: Pseudo dice [0.7465]
2024-12-01 07:26:37.368326: Epoch time: 87.47 s
2024-12-01 07:26:39.025052: 
2024-12-01 07:26:39.026713: Epoch 80
2024-12-01 07:26:39.027592: Current learning rate: 0.00928
2024-12-01 07:28:06.362682: Validation loss did not improve from -0.60760. Patience: 3/50
2024-12-01 07:28:06.363644: train_loss -0.6927
2024-12-01 07:28:06.364507: val_loss -0.5752
2024-12-01 07:28:06.365241: Pseudo dice [0.7506]
2024-12-01 07:28:06.365896: Epoch time: 87.34 s
2024-12-01 07:28:08.068634: 
2024-12-01 07:28:08.069893: Epoch 81
2024-12-01 07:28:08.070780: Current learning rate: 0.00927
2024-12-01 07:29:35.513731: Validation loss did not improve from -0.60760. Patience: 4/50
2024-12-01 07:29:35.514752: train_loss -0.6917
2024-12-01 07:29:35.515764: val_loss -0.5442
2024-12-01 07:29:35.516548: Pseudo dice [0.7362]
2024-12-01 07:29:35.517145: Epoch time: 87.45 s
2024-12-01 07:29:36.820800: 
2024-12-01 07:29:36.822249: Epoch 82
2024-12-01 07:29:36.823024: Current learning rate: 0.00926
2024-12-01 07:31:03.997773: Validation loss did not improve from -0.60760. Patience: 5/50
2024-12-01 07:31:03.998968: train_loss -0.6952
2024-12-01 07:31:03.999708: val_loss -0.5856
2024-12-01 07:31:04.000494: Pseudo dice [0.7533]
2024-12-01 07:31:04.001131: Epoch time: 87.18 s
2024-12-01 07:31:05.244211: 
2024-12-01 07:31:05.245880: Epoch 83
2024-12-01 07:31:05.246617: Current learning rate: 0.00925
2024-12-01 07:32:32.415487: Validation loss did not improve from -0.60760. Patience: 6/50
2024-12-01 07:32:32.416514: train_loss -0.697
2024-12-01 07:32:32.417617: val_loss -0.5471
2024-12-01 07:32:32.418471: Pseudo dice [0.7331]
2024-12-01 07:32:32.419422: Epoch time: 87.17 s
2024-12-01 07:32:33.660846: 
2024-12-01 07:32:33.662330: Epoch 84
2024-12-01 07:32:33.663211: Current learning rate: 0.00924
2024-12-01 07:34:00.885959: Validation loss did not improve from -0.60760. Patience: 7/50
2024-12-01 07:34:00.886748: train_loss -0.6951
2024-12-01 07:34:00.887537: val_loss -0.5768
2024-12-01 07:34:00.888317: Pseudo dice [0.7437]
2024-12-01 07:34:00.889079: Epoch time: 87.23 s
2024-12-01 07:34:02.510486: 
2024-12-01 07:34:02.512002: Epoch 85
2024-12-01 07:34:02.512672: Current learning rate: 0.00923
2024-12-01 07:35:29.838444: Validation loss did not improve from -0.60760. Patience: 8/50
2024-12-01 07:35:29.844988: train_loss -0.6957
2024-12-01 07:35:29.846120: val_loss -0.5726
2024-12-01 07:35:29.846799: Pseudo dice [0.7575]
2024-12-01 07:35:29.847960: Epoch time: 87.33 s
2024-12-01 07:35:31.183611: 
2024-12-01 07:35:31.185006: Epoch 86
2024-12-01 07:35:31.185810: Current learning rate: 0.00922
2024-12-01 07:36:58.233525: Validation loss did not improve from -0.60760. Patience: 9/50
2024-12-01 07:36:58.234737: train_loss -0.6943
2024-12-01 07:36:58.236816: val_loss -0.5621
2024-12-01 07:36:58.237798: Pseudo dice [0.7414]
2024-12-01 07:36:58.238904: Epoch time: 87.05 s
2024-12-01 07:36:59.522996: 
2024-12-01 07:36:59.524942: Epoch 87
2024-12-01 07:36:59.525842: Current learning rate: 0.00921
2024-12-01 07:38:26.552672: Validation loss did not improve from -0.60760. Patience: 10/50
2024-12-01 07:38:26.553842: train_loss -0.7003
2024-12-01 07:38:26.554730: val_loss -0.5762
2024-12-01 07:38:26.555346: Pseudo dice [0.7513]
2024-12-01 07:38:26.555991: Epoch time: 87.03 s
2024-12-01 07:38:27.801561: 
2024-12-01 07:38:27.803408: Epoch 88
2024-12-01 07:38:27.804286: Current learning rate: 0.0092
2024-12-01 07:39:54.823450: Validation loss did not improve from -0.60760. Patience: 11/50
2024-12-01 07:39:54.824521: train_loss -0.6984
2024-12-01 07:39:54.825309: val_loss -0.5664
2024-12-01 07:39:54.825966: Pseudo dice [0.749]
2024-12-01 07:39:54.826583: Epoch time: 87.02 s
2024-12-01 07:39:56.151437: 
2024-12-01 07:39:56.153022: Epoch 89
2024-12-01 07:39:56.153946: Current learning rate: 0.0092
2024-12-01 07:41:23.160246: Validation loss did not improve from -0.60760. Patience: 12/50
2024-12-01 07:41:23.161557: train_loss -0.6936
2024-12-01 07:41:23.162525: val_loss -0.557
2024-12-01 07:41:23.163280: Pseudo dice [0.744]
2024-12-01 07:41:23.164033: Epoch time: 87.01 s
2024-12-01 07:41:24.753518: 
2024-12-01 07:41:24.754826: Epoch 90
2024-12-01 07:41:24.755654: Current learning rate: 0.00919
2024-12-01 07:42:53.390839: Validation loss did not improve from -0.60760. Patience: 13/50
2024-12-01 07:42:53.391823: train_loss -0.698
2024-12-01 07:42:53.392881: val_loss -0.5597
2024-12-01 07:42:53.393827: Pseudo dice [0.7443]
2024-12-01 07:42:53.394714: Epoch time: 88.64 s
2024-12-01 07:42:54.635304: 
2024-12-01 07:42:54.636757: Epoch 91
2024-12-01 07:42:54.637950: Current learning rate: 0.00918
2024-12-01 07:44:23.272467: Validation loss did not improve from -0.60760. Patience: 14/50
2024-12-01 07:44:23.273766: train_loss -0.6928
2024-12-01 07:44:23.274722: val_loss -0.5083
2024-12-01 07:44:23.275589: Pseudo dice [0.7174]
2024-12-01 07:44:23.276526: Epoch time: 88.64 s
2024-12-01 07:44:24.560048: 
2024-12-01 07:44:24.561521: Epoch 92
2024-12-01 07:44:24.562404: Current learning rate: 0.00917
2024-12-01 07:45:53.118481: Validation loss did not improve from -0.60760. Patience: 15/50
2024-12-01 07:45:53.120481: train_loss -0.69
2024-12-01 07:45:53.121804: val_loss -0.5444
2024-12-01 07:45:53.122729: Pseudo dice [0.739]
2024-12-01 07:45:53.123697: Epoch time: 88.56 s
2024-12-01 07:45:55.070471: 
2024-12-01 07:45:55.072175: Epoch 93
2024-12-01 07:45:55.072997: Current learning rate: 0.00916
2024-12-01 07:47:23.595851: Validation loss did not improve from -0.60760. Patience: 16/50
2024-12-01 07:47:23.596877: train_loss -0.6872
2024-12-01 07:47:23.597579: val_loss -0.5537
2024-12-01 07:47:23.598188: Pseudo dice [0.7387]
2024-12-01 07:47:23.599001: Epoch time: 88.53 s
2024-12-01 07:47:24.804468: 
2024-12-01 07:47:24.806162: Epoch 94
2024-12-01 07:47:24.806897: Current learning rate: 0.00915
2024-12-01 07:48:53.355723: Validation loss did not improve from -0.60760. Patience: 17/50
2024-12-01 07:48:53.356436: train_loss -0.698
2024-12-01 07:48:53.357264: val_loss -0.5216
2024-12-01 07:48:53.357926: Pseudo dice [0.7231]
2024-12-01 07:48:53.358531: Epoch time: 88.55 s
2024-12-01 07:48:54.954188: 
2024-12-01 07:48:54.955582: Epoch 95
2024-12-01 07:48:54.956354: Current learning rate: 0.00914
2024-12-01 07:50:23.549112: Validation loss did not improve from -0.60760. Patience: 18/50
2024-12-01 07:50:23.550096: train_loss -0.6978
2024-12-01 07:50:23.551130: val_loss -0.5473
2024-12-01 07:50:23.551782: Pseudo dice [0.7359]
2024-12-01 07:50:23.552665: Epoch time: 88.6 s
2024-12-01 07:50:24.741096: 
2024-12-01 07:50:24.742268: Epoch 96
2024-12-01 07:50:24.743051: Current learning rate: 0.00913
2024-12-01 07:51:53.312817: Validation loss did not improve from -0.60760. Patience: 19/50
2024-12-01 07:51:53.313859: train_loss -0.6973
2024-12-01 07:51:53.314728: val_loss -0.5676
2024-12-01 07:51:53.315600: Pseudo dice [0.7521]
2024-12-01 07:51:53.316363: Epoch time: 88.57 s
2024-12-01 07:51:54.560864: 
2024-12-01 07:51:54.562547: Epoch 97
2024-12-01 07:51:54.563406: Current learning rate: 0.00912
2024-12-01 07:53:23.020005: Validation loss did not improve from -0.60760. Patience: 20/50
2024-12-01 07:53:23.021103: train_loss -0.7037
2024-12-01 07:53:23.022026: val_loss -0.5598
2024-12-01 07:53:23.022641: Pseudo dice [0.7416]
2024-12-01 07:53:23.023305: Epoch time: 88.46 s
2024-12-01 07:53:24.290287: 
2024-12-01 07:53:24.291517: Epoch 98
2024-12-01 07:53:24.292377: Current learning rate: 0.00911
2024-12-01 07:54:52.585898: Validation loss did not improve from -0.60760. Patience: 21/50
2024-12-01 07:54:52.586998: train_loss -0.7108
2024-12-01 07:54:52.587868: val_loss -0.5542
2024-12-01 07:54:52.588621: Pseudo dice [0.7416]
2024-12-01 07:54:52.589387: Epoch time: 88.3 s
2024-12-01 07:54:53.839365: 
2024-12-01 07:54:53.841100: Epoch 99
2024-12-01 07:54:53.842098: Current learning rate: 0.0091
2024-12-01 07:56:22.144216: Validation loss did not improve from -0.60760. Patience: 22/50
2024-12-01 07:56:22.145412: train_loss -0.707
2024-12-01 07:56:22.146327: val_loss -0.5445
2024-12-01 07:56:22.146979: Pseudo dice [0.7366]
2024-12-01 07:56:22.147609: Epoch time: 88.31 s
2024-12-01 07:56:23.763087: 
2024-12-01 07:56:23.764864: Epoch 100
2024-12-01 07:56:23.765758: Current learning rate: 0.0091
2024-12-01 07:57:52.107249: Validation loss did not improve from -0.60760. Patience: 23/50
2024-12-01 07:57:52.108329: train_loss -0.7068
2024-12-01 07:57:52.109264: val_loss -0.5664
2024-12-01 07:57:52.109925: Pseudo dice [0.7503]
2024-12-01 07:57:52.110688: Epoch time: 88.35 s
2024-12-01 07:57:53.343682: 
2024-12-01 07:57:53.345328: Epoch 101
2024-12-01 07:57:53.346273: Current learning rate: 0.00909
2024-12-01 07:59:21.701997: Validation loss did not improve from -0.60760. Patience: 24/50
2024-12-01 07:59:21.703323: train_loss -0.7064
2024-12-01 07:59:21.704560: val_loss -0.5643
2024-12-01 07:59:21.705280: Pseudo dice [0.7491]
2024-12-01 07:59:21.706019: Epoch time: 88.36 s
2024-12-01 07:59:22.990692: 
2024-12-01 07:59:22.992414: Epoch 102
2024-12-01 07:59:22.993476: Current learning rate: 0.00908
2024-12-01 08:00:51.321452: Validation loss did not improve from -0.60760. Patience: 25/50
2024-12-01 08:00:51.322642: train_loss -0.7114
2024-12-01 08:00:51.323549: val_loss -0.5667
2024-12-01 08:00:51.324308: Pseudo dice [0.7365]
2024-12-01 08:00:51.324934: Epoch time: 88.33 s
2024-12-01 08:00:52.568090: 
2024-12-01 08:00:52.569487: Epoch 103
2024-12-01 08:00:52.570289: Current learning rate: 0.00907
2024-12-01 08:02:20.888841: Validation loss did not improve from -0.60760. Patience: 26/50
2024-12-01 08:02:20.889896: train_loss -0.7103
2024-12-01 08:02:20.890736: val_loss -0.5685
2024-12-01 08:02:20.891475: Pseudo dice [0.7533]
2024-12-01 08:02:20.892181: Epoch time: 88.32 s
2024-12-01 08:02:22.433135: 
2024-12-01 08:02:22.434521: Epoch 104
2024-12-01 08:02:22.435156: Current learning rate: 0.00906
2024-12-01 08:03:50.755885: Validation loss did not improve from -0.60760. Patience: 27/50
2024-12-01 08:03:50.756913: train_loss -0.7174
2024-12-01 08:03:50.757954: val_loss -0.5625
2024-12-01 08:03:50.758795: Pseudo dice [0.7423]
2024-12-01 08:03:50.759794: Epoch time: 88.32 s
2024-12-01 08:03:52.389295: 
2024-12-01 08:03:52.390830: Epoch 105
2024-12-01 08:03:52.391792: Current learning rate: 0.00905
2024-12-01 08:05:20.765284: Validation loss did not improve from -0.60760. Patience: 28/50
2024-12-01 08:05:20.766125: train_loss -0.7151
2024-12-01 08:05:20.766822: val_loss -0.5617
2024-12-01 08:05:20.767440: Pseudo dice [0.7352]
2024-12-01 08:05:20.768092: Epoch time: 88.38 s
2024-12-01 08:05:21.997117: 
2024-12-01 08:05:21.998503: Epoch 106
2024-12-01 08:05:21.999151: Current learning rate: 0.00904
2024-12-01 08:06:50.524037: Validation loss did not improve from -0.60760. Patience: 29/50
2024-12-01 08:06:50.525321: train_loss -0.717
2024-12-01 08:06:50.526283: val_loss -0.5367
2024-12-01 08:06:50.527276: Pseudo dice [0.7408]
2024-12-01 08:06:50.527927: Epoch time: 88.53 s
2024-12-01 08:06:51.808617: 
2024-12-01 08:06:51.810296: Epoch 107
2024-12-01 08:06:51.811065: Current learning rate: 0.00903
2024-12-01 08:08:20.458748: Validation loss did not improve from -0.60760. Patience: 30/50
2024-12-01 08:08:20.459629: train_loss -0.7174
2024-12-01 08:08:20.460620: val_loss -0.5786
2024-12-01 08:08:20.461312: Pseudo dice [0.7548]
2024-12-01 08:08:20.462059: Epoch time: 88.65 s
2024-12-01 08:08:21.732391: 
2024-12-01 08:08:21.733710: Epoch 108
2024-12-01 08:08:21.734368: Current learning rate: 0.00902
2024-12-01 08:09:50.257312: Validation loss did not improve from -0.60760. Patience: 31/50
2024-12-01 08:09:50.258020: train_loss -0.7184
2024-12-01 08:09:50.258726: val_loss -0.5795
2024-12-01 08:09:50.259621: Pseudo dice [0.7492]
2024-12-01 08:09:50.260371: Epoch time: 88.53 s
2024-12-01 08:09:51.518045: 
2024-12-01 08:09:51.519659: Epoch 109
2024-12-01 08:09:51.520701: Current learning rate: 0.00901
2024-12-01 08:11:20.123533: Validation loss did not improve from -0.60760. Patience: 32/50
2024-12-01 08:11:20.124338: train_loss -0.7136
2024-12-01 08:11:20.125144: val_loss -0.5611
2024-12-01 08:11:20.125843: Pseudo dice [0.7399]
2024-12-01 08:11:20.126457: Epoch time: 88.61 s
2024-12-01 08:11:21.736813: 
2024-12-01 08:11:21.737938: Epoch 110
2024-12-01 08:11:21.738663: Current learning rate: 0.009
2024-12-01 08:12:50.381393: Validation loss did not improve from -0.60760. Patience: 33/50
2024-12-01 08:12:50.382660: train_loss -0.7096
2024-12-01 08:12:50.383605: val_loss -0.5469
2024-12-01 08:12:50.384414: Pseudo dice [0.7389]
2024-12-01 08:12:50.385095: Epoch time: 88.65 s
2024-12-01 08:12:51.621274: 
2024-12-01 08:12:51.623046: Epoch 111
2024-12-01 08:12:51.623769: Current learning rate: 0.009
2024-12-01 08:14:20.151151: Validation loss did not improve from -0.60760. Patience: 34/50
2024-12-01 08:14:20.151839: train_loss -0.7227
2024-12-01 08:14:20.152843: val_loss -0.5635
2024-12-01 08:14:20.153603: Pseudo dice [0.7455]
2024-12-01 08:14:20.154377: Epoch time: 88.53 s
2024-12-01 08:14:21.414779: 
2024-12-01 08:14:21.416577: Epoch 112
2024-12-01 08:14:21.417296: Current learning rate: 0.00899
2024-12-01 08:15:50.019312: Validation loss did not improve from -0.60760. Patience: 35/50
2024-12-01 08:15:50.020382: train_loss -0.718
2024-12-01 08:15:50.021141: val_loss -0.5853
2024-12-01 08:15:50.021780: Pseudo dice [0.7559]
2024-12-01 08:15:50.022438: Epoch time: 88.61 s
2024-12-01 08:15:51.288165: 
2024-12-01 08:15:51.289634: Epoch 113
2024-12-01 08:15:51.290458: Current learning rate: 0.00898
2024-12-01 08:17:20.008863: Validation loss did not improve from -0.60760. Patience: 36/50
2024-12-01 08:17:20.010085: train_loss -0.7094
2024-12-01 08:17:20.011270: val_loss -0.5741
2024-12-01 08:17:20.011978: Pseudo dice [0.7523]
2024-12-01 08:17:20.012662: Epoch time: 88.72 s
2024-12-01 08:17:21.261743: 
2024-12-01 08:17:21.263070: Epoch 114
2024-12-01 08:17:21.263849: Current learning rate: 0.00897
2024-12-01 08:18:48.658942: Validation loss did not improve from -0.60760. Patience: 37/50
2024-12-01 08:18:48.660278: train_loss -0.7173
2024-12-01 08:18:48.661333: val_loss -0.5429
2024-12-01 08:18:48.662110: Pseudo dice [0.7348]
2024-12-01 08:18:48.662805: Epoch time: 87.4 s
2024-12-01 08:18:50.786188: 
2024-12-01 08:18:50.787420: Epoch 115
2024-12-01 08:18:50.788181: Current learning rate: 0.00896
2024-12-01 08:20:19.616440: Validation loss did not improve from -0.60760. Patience: 38/50
2024-12-01 08:20:19.617294: train_loss -0.72
2024-12-01 08:20:19.618192: val_loss -0.5653
2024-12-01 08:20:19.618827: Pseudo dice [0.7499]
2024-12-01 08:20:19.619565: Epoch time: 88.83 s
2024-12-01 08:20:20.925882: 
2024-12-01 08:20:20.927170: Epoch 116
2024-12-01 08:20:20.928153: Current learning rate: 0.00895
2024-12-01 08:21:48.342731: Validation loss did not improve from -0.60760. Patience: 39/50
2024-12-01 08:21:48.343596: train_loss -0.7256
2024-12-01 08:21:48.344410: val_loss -0.5557
2024-12-01 08:21:48.345057: Pseudo dice [0.7359]
2024-12-01 08:21:48.345702: Epoch time: 87.42 s
2024-12-01 08:21:49.629260: 
2024-12-01 08:21:49.630657: Epoch 117
2024-12-01 08:21:49.631457: Current learning rate: 0.00894
2024-12-01 08:23:17.117149: Validation loss did not improve from -0.60760. Patience: 40/50
2024-12-01 08:23:17.118135: train_loss -0.7141
2024-12-01 08:23:17.119179: val_loss -0.5653
2024-12-01 08:23:17.119849: Pseudo dice [0.7485]
2024-12-01 08:23:17.120621: Epoch time: 87.49 s
2024-12-01 08:23:18.398488: 
2024-12-01 08:23:18.399836: Epoch 118
2024-12-01 08:23:18.400597: Current learning rate: 0.00893
2024-12-01 08:24:45.924202: Validation loss did not improve from -0.60760. Patience: 41/50
2024-12-01 08:24:45.925243: train_loss -0.7184
2024-12-01 08:24:45.926214: val_loss -0.5414
2024-12-01 08:24:45.926920: Pseudo dice [0.7385]
2024-12-01 08:24:45.927683: Epoch time: 87.53 s
2024-12-01 08:24:47.209648: 
2024-12-01 08:24:47.211120: Epoch 119
2024-12-01 08:24:47.212221: Current learning rate: 0.00892
2024-12-01 08:26:14.691015: Validation loss did not improve from -0.60760. Patience: 42/50
2024-12-01 08:26:14.692097: train_loss -0.7187
2024-12-01 08:26:14.692980: val_loss -0.5796
2024-12-01 08:26:14.693598: Pseudo dice [0.7486]
2024-12-01 08:26:14.694210: Epoch time: 87.48 s
2024-12-01 08:26:16.414388: 
2024-12-01 08:26:16.416102: Epoch 120
2024-12-01 08:26:16.416941: Current learning rate: 0.00891
2024-12-01 08:27:44.497220: Validation loss did not improve from -0.60760. Patience: 43/50
2024-12-01 08:27:44.498406: train_loss -0.7185
2024-12-01 08:27:44.500128: val_loss -0.5695
2024-12-01 08:27:44.500992: Pseudo dice [0.756]
2024-12-01 08:27:44.501904: Epoch time: 88.09 s
2024-12-01 08:27:45.783056: 
2024-12-01 08:27:45.784759: Epoch 121
2024-12-01 08:27:45.785735: Current learning rate: 0.0089
2024-12-01 08:29:12.961286: Validation loss did not improve from -0.60760. Patience: 44/50
2024-12-01 08:29:12.962361: train_loss -0.7171
2024-12-01 08:29:12.963365: val_loss -0.5686
2024-12-01 08:29:12.964374: Pseudo dice [0.7553]
2024-12-01 08:29:12.965374: Epoch time: 87.18 s
2024-12-01 08:29:14.225255: 
2024-12-01 08:29:14.227116: Epoch 122
2024-12-01 08:29:14.228085: Current learning rate: 0.00889
2024-12-01 08:30:41.135440: Validation loss did not improve from -0.60760. Patience: 45/50
2024-12-01 08:30:41.136324: train_loss -0.7228
2024-12-01 08:30:41.137202: val_loss -0.5967
2024-12-01 08:30:41.138015: Pseudo dice [0.7632]
2024-12-01 08:30:41.138847: Epoch time: 86.91 s
2024-12-01 08:30:41.139644: Yayy! New best EMA pseudo Dice: 0.7481
2024-12-01 08:30:42.757496: 
2024-12-01 08:30:42.759631: Epoch 123
2024-12-01 08:30:42.760677: Current learning rate: 0.00889
2024-12-01 08:32:09.708440: Validation loss did not improve from -0.60760. Patience: 46/50
2024-12-01 08:32:09.709522: train_loss -0.7256
2024-12-01 08:32:09.710766: val_loss -0.5772
2024-12-01 08:32:09.711940: Pseudo dice [0.7517]
2024-12-01 08:32:09.713099: Epoch time: 86.95 s
2024-12-01 08:32:09.714113: Yayy! New best EMA pseudo Dice: 0.7485
2024-12-01 08:32:11.292004: 
2024-12-01 08:32:11.294269: Epoch 124
2024-12-01 08:32:11.295567: Current learning rate: 0.00888
2024-12-01 08:33:38.697774: Validation loss did not improve from -0.60760. Patience: 47/50
2024-12-01 08:33:38.699085: train_loss -0.7201
2024-12-01 08:33:38.700394: val_loss -0.5697
2024-12-01 08:33:38.701751: Pseudo dice [0.7533]
2024-12-01 08:33:38.702561: Epoch time: 87.41 s
2024-12-01 08:33:39.062381: Yayy! New best EMA pseudo Dice: 0.7489
2024-12-01 08:33:40.570803: 
2024-12-01 08:33:40.573109: Epoch 125
2024-12-01 08:33:40.574461: Current learning rate: 0.00887
2024-12-01 08:35:08.872424: Validation loss did not improve from -0.60760. Patience: 48/50
2024-12-01 08:35:08.873724: train_loss -0.7243
2024-12-01 08:35:08.874906: val_loss -0.5576
2024-12-01 08:35:08.876049: Pseudo dice [0.7409]
2024-12-01 08:35:08.877031: Epoch time: 88.3 s
2024-12-01 08:35:10.428426: 
2024-12-01 08:35:10.430919: Epoch 126
2024-12-01 08:35:10.431776: Current learning rate: 0.00886
2024-12-01 08:36:38.666726: Validation loss did not improve from -0.60760. Patience: 49/50
2024-12-01 08:36:38.667928: train_loss -0.7251
2024-12-01 08:36:38.669186: val_loss -0.5516
2024-12-01 08:36:38.670019: Pseudo dice [0.7427]
2024-12-01 08:36:38.670856: Epoch time: 88.24 s
2024-12-01 08:36:39.882775: 
2024-12-01 08:36:39.884912: Epoch 127
2024-12-01 08:36:39.885975: Current learning rate: 0.00885
2024-12-01 08:38:08.138594: Validation loss did not improve from -0.60760. Patience: 50/50
2024-12-01 08:38:08.139709: train_loss -0.7262
2024-12-01 08:38:08.140379: val_loss -0.5717
2024-12-01 08:38:08.141003: Pseudo dice [0.7443]
2024-12-01 08:38:08.141703: Epoch time: 88.26 s
2024-12-01 08:38:09.323004: Patience reached. Stopping training.
2024-12-01 08:38:09.826792: Training done.
2024-12-01 08:38:09.964536: Using splits from existing split file: /home/gridsan/nchutisilp/datasets/nnUNet_Datasets/nnUNet_preprocessed/Dataset306_Sohee_Ajay_Calcium_OCT/splits_final.json
2024-12-01 08:38:09.966305: The split file contains 5 splits.
2024-12-01 08:38:09.967123: Desired fold for training: 1
2024-12-01 08:38:09.967787: This split has 10 training and 3 validation cases.
2024-12-01 08:38:09.968521: predicting 101-019
2024-12-01 08:38:09.979475: 101-019, shape torch.Size([1, 375, 498, 498]), rank 0
2024-12-01 08:39:47.420154: predicting 101009Pre
2024-12-01 08:39:47.440561: 101009Pre, shape torch.Size([1, 230, 498, 498]), rank 0
2024-12-01 08:40:40.465843: predicting 704-003
2024-12-01 08:40:40.482843: 704-003, shape torch.Size([1, 375, 498, 498]), rank 0
2024-12-01 08:42:29.344484: Validation complete
2024-12-01 08:42:29.345555: Mean Validation Dice:  0.751370815161234

############################
INFO: You are using the old nnU-Net default plans. We have updated our recommendations. Please consider using those instead! Read more here: https://github.com/MIC-DKFZ/nnUNet/blob/master/documentation/resenc_presets.md
############################

Using device: cuda:0

#######################################################################
Please cite the following paper when using nnU-Net:
Isensee, F., Jaeger, P. F., Kohl, S. A., Petersen, J., & Maier-Hein, K. H. (2021). nnU-Net: a self-configuring method for deep learning-based biomedical image segmentation. Nature methods, 18(2), 203-211.
#######################################################################

2024-12-01 08:42:36.000053: Using torch.compile...

############################
INFO: You are using the old nnU-Net default plans. We have updated our recommendations. Please consider using those instead! Read more here: https://github.com/MIC-DKFZ/nnUNet/blob/master/documentation/resenc_presets.md
############################

Using device: cuda:0

#######################################################################
Please cite the following paper when using nnU-Net:
Isensee, F., Jaeger, P. F., Kohl, S. A., Petersen, J., & Maier-Hein, K. H. (2021). nnU-Net: a self-configuring method for deep learning-based biomedical image segmentation. Nature methods, 18(2), 203-211.
#######################################################################

2024-12-01 08:42:36.001781: Using torch.compile...
/home/gridsan/nchutisilp/.local/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn("The verbose parameter is deprecated. Please use get_last_lr() "
/home/gridsan/nchutisilp/.local/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn("The verbose parameter is deprecated. Please use get_last_lr() "
################### Loading pretrained weights from file  /home/gridsan/nchutisilp/datasets/nnUNet_Datasets/nnUNet_results/Dataset300_Lumen_and_Wall_OCT/nnUNetTrainer__nnUNetPreprocessPlans__3d_fullres/fold_all/checkpoint_best.pth ###################
Below is the list of overlapping blocks in pretrained model and nnUNet architecture:
encoder.stages.0.0.convs.0.conv.weight shape torch.Size([32, 1, 3, 3, 3])
encoder.stages.0.0.convs.0.conv.bias shape torch.Size([32])
encoder.stages.0.0.convs.0.norm.weight shape torch.Size([32])
encoder.stages.0.0.convs.0.norm.bias shape torch.Size([32])
encoder.stages.0.0.convs.0.all_modules.0.weight shape torch.Size([32, 1, 3, 3, 3])
encoder.stages.0.0.convs.0.all_modules.0.bias shape torch.Size([32])
encoder.stages.0.0.convs.0.all_modules.1.weight shape torch.Size([32])
encoder.stages.0.0.convs.0.all_modules.1.bias shape torch.Size([32])
encoder.stages.0.0.convs.1.conv.weight shape torch.Size([32, 32, 3, 3, 3])
encoder.stages.0.0.convs.1.conv.bias shape torch.Size([32])
encoder.stages.0.0.convs.1.norm.weight shape torch.Size([32])
encoder.stages.0.0.convs.1.norm.bias shape torch.Size([32])
encoder.stages.0.0.convs.1.all_modules.0.weight shape torch.Size([32, 32, 3, 3, 3])
encoder.stages.0.0.convs.1.all_modules.0.bias shape torch.Size([32])
encoder.stages.0.0.convs.1.all_modules.1.weight shape torch.Size([32])
encoder.stages.0.0.convs.1.all_modules.1.bias shape torch.Size([32])
encoder.stages.1.0.convs.0.conv.weight shape torch.Size([64, 32, 3, 3, 3])
encoder.stages.1.0.convs.0.conv.bias shape torch.Size([64])
encoder.stages.1.0.convs.0.norm.weight shape torch.Size([64])
encoder.stages.1.0.convs.0.norm.bias shape torch.Size([64])
encoder.stages.1.0.convs.0.all_modules.0.weight shape torch.Size([64, 32, 3, 3, 3])
encoder.stages.1.0.convs.0.all_modules.0.bias shape torch.Size([64])
encoder.stages.1.0.convs.0.all_modules.1.weight shape torch.Size([64])
encoder.stages.1.0.convs.0.all_modules.1.bias shape torch.Size([64])
encoder.stages.1.0.convs.1.conv.weight shape torch.Size([64, 64, 3, 3, 3])
encoder.stages.1.0.convs.1.conv.bias shape torch.Size([64])
encoder.stages.1.0.convs.1.norm.weight shape torch.Size([64])
encoder.stages.1.0.convs.1.norm.bias shape torch.Size([64])
encoder.stages.1.0.convs.1.all_modules.0.weight shape torch.Size([64, 64, 3, 3, 3])
encoder.stages.1.0.convs.1.all_modules.0.bias shape torch.Size([64])
encoder.stages.1.0.convs.1.all_modules.1.weight shape torch.Size([64])
encoder.stages.1.0.convs.1.all_modules.1.bias shape torch.Size([64])
encoder.stages.2.0.convs.0.conv.weight shape torch.Size([128, 64, 3, 3, 3])
encoder.stages.2.0.convs.0.conv.bias shape torch.Size([128])
encoder.stages.2.0.convs.0.norm.weight shape torch.Size([128])
encoder.stages.2.0.convs.0.norm.bias shape torch.Size([128])
encoder.stages.2.0.convs.0.all_modules.0.weight shape torch.Size([128, 64, 3, 3, 3])
encoder.stages.2.0.convs.0.all_modules.0.bias shape torch.Size([128])
encoder.stages.2.0.convs.0.all_modules.1.weight shape torch.Size([128])
encoder.stages.2.0.convs.0.all_modules.1.bias shape torch.Size([128])
encoder.stages.2.0.convs.1.conv.weight shape torch.Size([128, 128, 3, 3, 3])
encoder.stages.2.0.convs.1.conv.bias shape torch.Size([128])
encoder.stages.2.0.convs.1.norm.weight shape torch.Size([128])
encoder.stages.2.0.convs.1.norm.bias shape torch.Size([128])
encoder.stages.2.0.convs.1.all_modules.0.weight shape torch.Size([128, 128, 3, 3, 3])
encoder.stages.2.0.convs.1.all_modules.0.bias shape torch.Size([128])
encoder.stages.2.0.convs.1.all_modules.1.weight shape torch.Size([128])
encoder.stages.2.0.convs.1.all_modules.1.bias shape torch.Size([128])
encoder.stages.3.0.convs.0.conv.weight shape torch.Size([256, 128, 3, 3, 3])
encoder.stages.3.0.convs.0.conv.bias shape torch.Size([256])
encoder.stages.3.0.convs.0.norm.weight shape torch.Size([256])
encoder.stages.3.0.convs.0.norm.bias shape torch.Size([256])
encoder.stages.3.0.convs.0.all_modules.0.weight shape torch.Size([256, 128, 3, 3, 3])
encoder.stages.3.0.convs.0.all_modules.0.bias shape torch.Size([256])
encoder.stages.3.0.convs.0.all_modules.1.weight shape torch.Size([256])
encoder.stages.3.0.convs.0.all_modules.1.bias shape torch.Size([256])
encoder.stages.3.0.convs.1.conv.weight shape torch.Size([256, 256, 3, 3, 3])
encoder.stages.3.0.convs.1.conv.bias shape torch.Size([256])
encoder.stages.3.0.convs.1.norm.weight shape torch.Size([256])
encoder.stages.3.0.convs.1.norm.bias shape torch.Size([256])
encoder.stages.3.0.convs.1.all_modules.0.weight shape torch.Size([256, 256, 3, 3, 3])
encoder.stages.3.0.convs.1.all_modules.0.bias shape torch.Size([256])
encoder.stages.3.0.convs.1.all_modules.1.weight shape torch.Size([256])
encoder.stages.3.0.convs.1.all_modules.1.bias shape torch.Size([256])
encoder.stages.4.0.convs.0.conv.weight shape torch.Size([320, 256, 3, 3, 3])
encoder.stages.4.0.convs.0.conv.bias shape torch.Size([320])
encoder.stages.4.0.convs.0.norm.weight shape torch.Size([320])
encoder.stages.4.0.convs.0.norm.bias shape torch.Size([320])
encoder.stages.4.0.convs.0.all_modules.0.weight shape torch.Size([320, 256, 3, 3, 3])
encoder.stages.4.0.convs.0.all_modules.0.bias shape torch.Size([320])
encoder.stages.4.0.convs.0.all_modules.1.weight shape torch.Size([320])
encoder.stages.4.0.convs.0.all_modules.1.bias shape torch.Size([320])
encoder.stages.4.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.4.0.convs.1.conv.bias shape torch.Size([320])
encoder.stages.4.0.convs.1.norm.weight shape torch.Size([320])
encoder.stages.4.0.convs.1.norm.bias shape torch.Size([320])
encoder.stages.4.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.4.0.convs.1.all_modules.0.bias shape torch.Size([320])
encoder.stages.4.0.convs.1.all_modules.1.weight shape torch.Size([320])
encoder.stages.4.0.convs.1.all_modules.1.bias shape torch.Size([320])
encoder.stages.5.0.convs.0.conv.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.5.0.convs.0.conv.bias shape torch.Size([320])
encoder.stages.5.0.convs.0.norm.weight shape torch.Size([320])
encoder.stages.5.0.convs.0.norm.bias shape torch.Size([320])
encoder.stages.5.0.convs.0.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.5.0.convs.0.all_modules.0.bias shape torch.Size([320])
encoder.stages.5.0.convs.0.all_modules.1.weight shape torch.Size([320])
encoder.stages.5.0.convs.0.all_modules.1.bias shape torch.Size([320])
encoder.stages.5.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.5.0.convs.1.conv.bias shape torch.Size([320])
encoder.stages.5.0.convs.1.norm.weight shape torch.Size([320])
encoder.stages.5.0.convs.1.norm.bias shape torch.Size([320])
encoder.stages.5.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.5.0.convs.1.all_modules.0.bias shape torch.Size([320])
encoder.stages.5.0.convs.1.all_modules.1.weight shape torch.Size([320])
encoder.stages.5.0.convs.1.all_modules.1.bias shape torch.Size([320])
decoder.encoder.stages.0.0.convs.0.conv.weight shape torch.Size([32, 1, 3, 3, 3])
decoder.encoder.stages.0.0.convs.0.conv.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.norm.weight shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.norm.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.all_modules.0.weight shape torch.Size([32, 1, 3, 3, 3])
decoder.encoder.stages.0.0.convs.0.all_modules.0.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.all_modules.1.weight shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.all_modules.1.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.conv.weight shape torch.Size([32, 32, 3, 3, 3])
decoder.encoder.stages.0.0.convs.1.conv.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.norm.weight shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.norm.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.all_modules.0.weight shape torch.Size([32, 32, 3, 3, 3])
decoder.encoder.stages.0.0.convs.1.all_modules.0.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.all_modules.1.weight shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.all_modules.1.bias shape torch.Size([32])
decoder.encoder.stages.1.0.convs.0.conv.weight shape torch.Size([64, 32, 3, 3, 3])
decoder.encoder.stages.1.0.convs.0.conv.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.norm.weight shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.norm.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.all_modules.0.weight shape torch.Size([64, 32, 3, 3, 3])
decoder.encoder.stages.1.0.convs.0.all_modules.0.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.all_modules.1.weight shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.all_modules.1.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.conv.weight shape torch.Size([64, 64, 3, 3, 3])
decoder.encoder.stages.1.0.convs.1.conv.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.norm.weight shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.norm.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.all_modules.0.weight shape torch.Size([64, 64, 3, 3, 3])
decoder.encoder.stages.1.0.convs.1.all_modules.0.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.all_modules.1.weight shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.all_modules.1.bias shape torch.Size([64])
decoder.encoder.stages.2.0.convs.0.conv.weight shape torch.Size([128, 64, 3, 3, 3])
decoder.encoder.stages.2.0.convs.0.conv.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.norm.weight shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.norm.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.all_modules.0.weight shape torch.Size([128, 64, 3, 3, 3])
decoder.encoder.stages.2.0.convs.0.all_modules.0.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.all_modules.1.weight shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.all_modules.1.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.conv.weight shape torch.Size([128, 128, 3, 3, 3])
decoder.encoder.stages.2.0.convs.1.conv.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.norm.weight shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.norm.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.all_modules.0.weight shape torch.Size([128, 128, 3, 3, 3])
decoder.encoder.stages.2.0.convs.1.all_modules.0.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.all_modules.1.weight shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.all_modules.1.bias shape torch.Size([128])
decoder.encoder.stages.3.0.convs.0.conv.weight shape torch.Size([256, 128, 3, 3, 3])
decoder.encoder.stages.3.0.convs.0.conv.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.norm.weight shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.norm.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.all_modules.0.weight shape torch.Size([256, 128, 3, 3, 3])
decoder.encoder.stages.3.0.convs.0.all_modules.0.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.all_modules.1.weight shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.all_modules.1.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.conv.weight shape torch.Size([256, 256, 3, 3, 3])
decoder.encoder.stages.3.0.convs.1.conv.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.norm.weight shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.norm.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.all_modules.0.weight shape torch.Size([256, 256, 3, 3, 3])
decoder.encoder.stages.3.0.convs.1.all_modules.0.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.all_modules.1.weight shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.all_modules.1.bias shape torch.Size([256])
decoder.encoder.stages.4.0.convs.0.conv.weight shape torch.Size([320, 256, 3, 3, 3])
decoder.encoder.stages.4.0.convs.0.conv.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.norm.weight shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.norm.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.all_modules.0.weight shape torch.Size([320, 256, 3, 3, 3])
decoder.encoder.stages.4.0.convs.0.all_modules.0.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.all_modules.1.weight shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.all_modules.1.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.4.0.convs.1.conv.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.norm.weight shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.norm.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.4.0.convs.1.all_modules.0.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.all_modules.1.weight shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.all_modules.1.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.conv.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.5.0.convs.0.conv.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.norm.weight shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.norm.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.5.0.convs.0.all_modules.0.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.all_modules.1.weight shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.all_modules.1.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.5.0.convs.1.conv.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.norm.weight shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.norm.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.5.0.convs.1.all_modules.0.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.all_modules.1.weight shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.all_modules.1.bias shape torch.Size([320])
decoder.stages.0.convs.0.conv.weight shape torch.Size([320, 640, 3, 3, 3])
decoder.stages.0.convs.0.conv.bias shape torch.Size([320])
decoder.stages.0.convs.0.norm.weight shape torch.Size([320])
decoder.stages.0.convs.0.norm.bias shape torch.Size([320])
decoder.stages.0.convs.0.all_modules.0.weight shape torch.Size([320, 640, 3, 3, 3])
decoder.stages.0.convs.0.all_modules.0.bias shape torch.Size([320])
decoder.stages.0.convs.0.all_modules.1.weight shape torch.Size([320])
decoder.stages.0.convs.0.all_modules.1.bias shape torch.Size([320])
decoder.stages.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.stages.0.convs.1.conv.bias shape torch.Size([320])
decoder.stages.0.convs.1.norm.weight shape torch.Size([320])
decoder.stages.0.convs.1.norm.bias shape torch.Size([320])
decoder.stages.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.stages.0.convs.1.all_modules.0.bias shape torch.Size([320])
decoder.stages.0.convs.1.all_modules.1.weight shape torch.Size([320])
decoder.stages.0.convs.1.all_modules.1.bias shape torch.Size([320])
decoder.stages.1.convs.0.conv.weight shape torch.Size([256, 512, 3, 3, 3])
decoder.stages.1.convs.0.conv.bias shape torch.Size([256])
decoder.stages.1.convs.0.norm.weight shape torch.Size([256])
decoder.stages.1.convs.0.norm.bias shape torch.Size([256])
decoder.stages.1.convs.0.all_modules.0.weight shape torch.Size([256, 512, 3, 3, 3])
decoder.stages.1.convs.0.all_modules.0.bias shape torch.Size([256])
decoder.stages.1.convs.0.all_modules.1.weight shape torch.Size([256])
decoder.stages.1.convs.0.all_modules.1.bias shape torch.Size([256])
decoder.stages.1.convs.1.conv.weight shape torch.Size([256, 256, 3, 3, 3])
decoder.stages.1.convs.1.conv.bias shape torch.Size([256])
decoder.stages.1.convs.1.norm.weight shape torch.Size([256])
decoder.stages.1.convs.1.norm.bias shape torch.Size([256])
decoder.stages.1.convs.1.all_modules.0.weight shape torch.Size([256, 256, 3, 3, 3])
decoder.stages.1.convs.1.all_modules.0.bias shape torch.Size([256])
decoder.stages.1.convs.1.all_modules.1.weight shape torch.Size([256])
decoder.stages.1.convs.1.all_modules.1.bias shape torch.Size([256])
decoder.stages.2.convs.0.conv.weight shape torch.Size([128, 256, 3, 3, 3])
decoder.stages.2.convs.0.conv.bias shape torch.Size([128])
decoder.stages.2.convs.0.norm.weight shape torch.Size([128])
decoder.stages.2.convs.0.norm.bias shape torch.Size([128])
decoder.stages.2.convs.0.all_modules.0.weight shape torch.Size([128, 256, 3, 3, 3])
decoder.stages.2.convs.0.all_modules.0.bias shape torch.Size([128])
decoder.stages.2.convs.0.all_modules.1.weight shape torch.Size([128])
decoder.stages.2.convs.0.all_modules.1.bias shape torch.Size([128])
decoder.stages.2.convs.1.conv.weight shape torch.Size([128, 128, 3, 3, 3])
decoder.stages.2.convs.1.conv.bias shape torch.Size([128])
decoder.stages.2.convs.1.norm.weight shape torch.Size([128])
decoder.stages.2.convs.1.norm.bias shape torch.Size([128])
decoder.stages.2.convs.1.all_modules.0.weight shape torch.Size([128, 128, 3, 3, 3])
decoder.stages.2.convs.1.all_modules.0.bias shape torch.Size([128])
decoder.stages.2.convs.1.all_modules.1.weight shape torch.Size([128])
decoder.stages.2.convs.1.all_modules.1.bias shape torch.Size([128])
decoder.stages.3.convs.0.conv.weight shape torch.Size([64, 128, 3, 3, 3])
decoder.stages.3.convs.0.conv.bias shape torch.Size([64])
decoder.stages.3.convs.0.norm.weight shape torch.Size([64])
decoder.stages.3.convs.0.norm.bias shape torch.Size([64])
decoder.stages.3.convs.0.all_modules.0.weight shape torch.Size([64, 128, 3, 3, 3])
decoder.stages.3.convs.0.all_modules.0.bias shape torch.Size([64])
decoder.stages.3.convs.0.all_modules.1.weight shape torch.Size([64])
decoder.stages.3.convs.0.all_modules.1.bias shape torch.Size([64])
decoder.stages.3.convs.1.conv.weight shape torch.Size([64, 64, 3, 3, 3])
decoder.stages.3.convs.1.conv.bias shape torch.Size([64])
decoder.stages.3.convs.1.norm.weight shape torch.Size([64])
decoder.stages.3.convs.1.norm.bias shape torch.Size([64])
decoder.stages.3.convs.1.all_modules.0.weight shape torch.Size([64, 64, 3, 3, 3])
decoder.stages.3.convs.1.all_modules.0.bias shape torch.Size([64])
decoder.stages.3.convs.1.all_modules.1.weight shape torch.Size([64])
decoder.stages.3.convs.1.all_modules.1.bias shape torch.Size([64])
decoder.stages.4.convs.0.conv.weight shape torch.Size([32, 64, 3, 3, 3])
decoder.stages.4.convs.0.conv.bias shape torch.Size([32])
decoder.stages.4.convs.0.norm.weight shape torch.Size([32])
decoder.stages.4.convs.0.norm.bias shape torch.Size([32])
decoder.stages.4.convs.0.all_modules.0.weight shape torch.Size([32, 64, 3, 3, 3])
decoder.stages.4.convs.0.all_modules.0.bias shape torch.Size([32])
decoder.stages.4.convs.0.all_modules.1.weight shape torch.Size([32])
decoder.stages.4.convs.0.all_modules.1.bias shape torch.Size([32])
decoder.stages.4.convs.1.conv.weight shape torch.Size([32, 32, 3, 3, 3])
decoder.stages.4.convs.1.conv.bias shape torch.Size([32])
decoder.stages.4.convs.1.norm.weight shape torch.Size([32])
decoder.stages.4.convs.1.norm.bias shape torch.Size([32])
decoder.stages.4.convs.1.all_modules.0.weight shape torch.Size([32, 32, 3, 3, 3])
decoder.stages.4.convs.1.all_modules.0.bias shape torch.Size([32])
decoder.stages.4.convs.1.all_modules.1.weight shape torch.Size([32])
decoder.stages.4.convs.1.all_modules.1.bias shape torch.Size([32])
decoder.transpconvs.0.weight shape torch.Size([320, 320, 1, 2, 2])
decoder.transpconvs.0.bias shape torch.Size([320])
decoder.transpconvs.1.weight shape torch.Size([320, 256, 2, 2, 2])
decoder.transpconvs.1.bias shape torch.Size([256])
decoder.transpconvs.2.weight shape torch.Size([256, 128, 2, 2, 2])
decoder.transpconvs.2.bias shape torch.Size([128])
decoder.transpconvs.3.weight shape torch.Size([128, 64, 2, 2, 2])
decoder.transpconvs.3.bias shape torch.Size([64])
decoder.transpconvs.4.weight shape torch.Size([64, 32, 2, 2, 2])
decoder.transpconvs.4.bias shape torch.Size([32])
################### Done ###################
2024-12-01 08:42:50.557255: do_dummy_2d_data_aug: True
2024-12-01 08:42:50.559714: Using splits from existing split file: /home/gridsan/nchutisilp/datasets/nnUNet_Datasets/nnUNet_preprocessed/Dataset306_Sohee_Ajay_Calcium_OCT/splits_final.json
2024-12-01 08:42:50.561885: The split file contains 5 splits.
2024-12-01 08:42:50.563090: Desired fold for training: 2
2024-12-01 08:42:50.564248: This split has 10 training and 3 validation cases.
################### Loading pretrained weights from file  /home/gridsan/nchutisilp/datasets/nnUNet_Datasets/nnUNet_results/Dataset300_Lumen_and_Wall_OCT/nnUNetTrainer__nnUNetPreprocessPlans__3d_fullres/fold_all/checkpoint_best.pth ###################
Below is the list of overlapping blocks in pretrained model and nnUNet architecture:
encoder.stages.0.0.convs.0.conv.weight shape torch.Size([32, 1, 3, 3, 3])
encoder.stages.0.0.convs.0.conv.bias shape torch.Size([32])
encoder.stages.0.0.convs.0.norm.weight shape torch.Size([32])
encoder.stages.0.0.convs.0.norm.bias shape torch.Size([32])
encoder.stages.0.0.convs.0.all_modules.0.weight shape torch.Size([32, 1, 3, 3, 3])
encoder.stages.0.0.convs.0.all_modules.0.bias shape torch.Size([32])
encoder.stages.0.0.convs.0.all_modules.1.weight shape torch.Size([32])
encoder.stages.0.0.convs.0.all_modules.1.bias shape torch.Size([32])
encoder.stages.0.0.convs.1.conv.weight shape torch.Size([32, 32, 3, 3, 3])
encoder.stages.0.0.convs.1.conv.bias shape torch.Size([32])
encoder.stages.0.0.convs.1.norm.weight shape torch.Size([32])
encoder.stages.0.0.convs.1.norm.bias shape torch.Size([32])
encoder.stages.0.0.convs.1.all_modules.0.weight shape torch.Size([32, 32, 3, 3, 3])
encoder.stages.0.0.convs.1.all_modules.0.bias shape torch.Size([32])
encoder.stages.0.0.convs.1.all_modules.1.weight shape torch.Size([32])
encoder.stages.0.0.convs.1.all_modules.1.bias shape torch.Size([32])
encoder.stages.1.0.convs.0.conv.weight shape torch.Size([64, 32, 3, 3, 3])
encoder.stages.1.0.convs.0.conv.bias shape torch.Size([64])
encoder.stages.1.0.convs.0.norm.weight shape torch.Size([64])
encoder.stages.1.0.convs.0.norm.bias shape torch.Size([64])
encoder.stages.1.0.convs.0.all_modules.0.weight shape torch.Size([64, 32, 3, 3, 3])
encoder.stages.1.0.convs.0.all_modules.0.bias shape torch.Size([64])
encoder.stages.1.0.convs.0.all_modules.1.weight shape torch.Size([64])
encoder.stages.1.0.convs.0.all_modules.1.bias shape torch.Size([64])
encoder.stages.1.0.convs.1.conv.weight shape torch.Size([64, 64, 3, 3, 3])
encoder.stages.1.0.convs.1.conv.bias shape torch.Size([64])
encoder.stages.1.0.convs.1.norm.weight shape torch.Size([64])
encoder.stages.1.0.convs.1.norm.bias shape torch.Size([64])
encoder.stages.1.0.convs.1.all_modules.0.weight shape torch.Size([64, 64, 3, 3, 3])
encoder.stages.1.0.convs.1.all_modules.0.bias shape torch.Size([64])
encoder.stages.1.0.convs.1.all_modules.1.weight shape torch.Size([64])
encoder.stages.1.0.convs.1.all_modules.1.bias shape torch.Size([64])
encoder.stages.2.0.convs.0.conv.weight shape torch.Size([128, 64, 3, 3, 3])
encoder.stages.2.0.convs.0.conv.bias shape torch.Size([128])
encoder.stages.2.0.convs.0.norm.weight shape torch.Size([128])
encoder.stages.2.0.convs.0.norm.bias shape torch.Size([128])
encoder.stages.2.0.convs.0.all_modules.0.weight shape torch.Size([128, 64, 3, 3, 3])
encoder.stages.2.0.convs.0.all_modules.0.bias shape torch.Size([128])
encoder.stages.2.0.convs.0.all_modules.1.weight shape torch.Size([128])
encoder.stages.2.0.convs.0.all_modules.1.bias shape torch.Size([128])
encoder.stages.2.0.convs.1.conv.weight shape torch.Size([128, 128, 3, 3, 3])
encoder.stages.2.0.convs.1.conv.bias shape torch.Size([128])
encoder.stages.2.0.convs.1.norm.weight shape torch.Size([128])
encoder.stages.2.0.convs.1.norm.bias shape torch.Size([128])
encoder.stages.2.0.convs.1.all_modules.0.weight shape torch.Size([128, 128, 3, 3, 3])
encoder.stages.2.0.convs.1.all_modules.0.bias shape torch.Size([128])
encoder.stages.2.0.convs.1.all_modules.1.weight shape torch.Size([128])
encoder.stages.2.0.convs.1.all_modules.1.bias shape torch.Size([128])
encoder.stages.3.0.convs.0.conv.weight shape torch.Size([256, 128, 3, 3, 3])
encoder.stages.3.0.convs.0.conv.bias shape torch.Size([256])
encoder.stages.3.0.convs.0.norm.weight shape torch.Size([256])
encoder.stages.3.0.convs.0.norm.bias shape torch.Size([256])
encoder.stages.3.0.convs.0.all_modules.0.weight shape torch.Size([256, 128, 3, 3, 3])
encoder.stages.3.0.convs.0.all_modules.0.bias shape torch.Size([256])
encoder.stages.3.0.convs.0.all_modules.1.weight shape torch.Size([256])
encoder.stages.3.0.convs.0.all_modules.1.bias shape torch.Size([256])
encoder.stages.3.0.convs.1.conv.weight shape torch.Size([256, 256, 3, 3, 3])
encoder.stages.3.0.convs.1.conv.bias shape torch.Size([256])
encoder.stages.3.0.convs.1.norm.weight shape torch.Size([256])
encoder.stages.3.0.convs.1.norm.bias shape torch.Size([256])
encoder.stages.3.0.convs.1.all_modules.0.weight shape torch.Size([256, 256, 3, 3, 3])
encoder.stages.3.0.convs.1.all_modules.0.bias shape torch.Size([256])
encoder.stages.3.0.convs.1.all_modules.1.weight shape torch.Size([256])
encoder.stages.3.0.convs.1.all_modules.1.bias shape torch.Size([256])
encoder.stages.4.0.convs.0.conv.weight shape torch.Size([320, 256, 3, 3, 3])
encoder.stages.4.0.convs.0.conv.bias shape torch.Size([320])
encoder.stages.4.0.convs.0.norm.weight shape torch.Size([320])
encoder.stages.4.0.convs.0.norm.bias shape torch.Size([320])
encoder.stages.4.0.convs.0.all_modules.0.weight shape torch.Size([320, 256, 3, 3, 3])
encoder.stages.4.0.convs.0.all_modules.0.bias shape torch.Size([320])
encoder.stages.4.0.convs.0.all_modules.1.weight shape torch.Size([320])
encoder.stages.4.0.convs.0.all_modules.1.bias shape torch.Size([320])
encoder.stages.4.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.4.0.convs.1.conv.bias shape torch.Size([320])
encoder.stages.4.0.convs.1.norm.weight shape torch.Size([320])
encoder.stages.4.0.convs.1.norm.bias shape torch.Size([320])
encoder.stages.4.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.4.0.convs.1.all_modules.0.bias shape torch.Size([320])
encoder.stages.4.0.convs.1.all_modules.1.weight shape torch.Size([320])
encoder.stages.4.0.convs.1.all_modules.1.bias shape torch.Size([320])
encoder.stages.5.0.convs.0.conv.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.5.0.convs.0.conv.bias shape torch.Size([320])
encoder.stages.5.0.convs.0.norm.weight shape torch.Size([320])
encoder.stages.5.0.convs.0.norm.bias shape torch.Size([320])
encoder.stages.5.0.convs.0.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.5.0.convs.0.all_modules.0.bias shape torch.Size([320])
encoder.stages.5.0.convs.0.all_modules.1.weight shape torch.Size([320])
encoder.stages.5.0.convs.0.all_modules.1.bias shape torch.Size([320])
encoder.stages.5.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.5.0.convs.1.conv.bias shape torch.Size([320])
encoder.stages.5.0.convs.1.norm.weight shape torch.Size([320])
encoder.stages.5.0.convs.1.norm.bias shape torch.Size([320])
encoder.stages.5.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.5.0.convs.1.all_modules.0.bias shape torch.Size([320])
encoder.stages.5.0.convs.1.all_modules.1.weight shape torch.Size([320])
encoder.stages.5.0.convs.1.all_modules.1.bias shape torch.Size([320])
decoder.encoder.stages.0.0.convs.0.conv.weight shape torch.Size([32, 1, 3, 3, 3])
decoder.encoder.stages.0.0.convs.0.conv.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.norm.weight shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.norm.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.all_modules.0.weight shape torch.Size([32, 1, 3, 3, 3])
decoder.encoder.stages.0.0.convs.0.all_modules.0.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.all_modules.1.weight shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.all_modules.1.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.conv.weight shape torch.Size([32, 32, 3, 3, 3])
decoder.encoder.stages.0.0.convs.1.conv.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.norm.weight shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.norm.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.all_modules.0.weight shape torch.Size([32, 32, 3, 3, 3])
decoder.encoder.stages.0.0.convs.1.all_modules.0.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.all_modules.1.weight shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.all_modules.1.bias shape torch.Size([32])
decoder.encoder.stages.1.0.convs.0.conv.weight shape torch.Size([64, 32, 3, 3, 3])
decoder.encoder.stages.1.0.convs.0.conv.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.norm.weight shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.norm.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.all_modules.0.weight shape torch.Size([64, 32, 3, 3, 3])
decoder.encoder.stages.1.0.convs.0.all_modules.0.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.all_modules.1.weight shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.all_modules.1.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.conv.weight shape torch.Size([64, 64, 3, 3, 3])
decoder.encoder.stages.1.0.convs.1.conv.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.norm.weight shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.norm.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.all_modules.0.weight shape torch.Size([64, 64, 3, 3, 3])
decoder.encoder.stages.1.0.convs.1.all_modules.0.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.all_modules.1.weight shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.all_modules.1.bias shape torch.Size([64])
decoder.encoder.stages.2.0.convs.0.conv.weight shape torch.Size([128, 64, 3, 3, 3])
decoder.encoder.stages.2.0.convs.0.conv.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.norm.weight shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.norm.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.all_modules.0.weight shape torch.Size([128, 64, 3, 3, 3])
decoder.encoder.stages.2.0.convs.0.all_modules.0.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.all_modules.1.weight shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.all_modules.1.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.conv.weight shape torch.Size([128, 128, 3, 3, 3])
decoder.encoder.stages.2.0.convs.1.conv.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.norm.weight shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.norm.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.all_modules.0.weight shape torch.Size([128, 128, 3, 3, 3])
decoder.encoder.stages.2.0.convs.1.all_modules.0.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.all_modules.1.weight shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.all_modules.1.bias shape torch.Size([128])
decoder.encoder.stages.3.0.convs.0.conv.weight shape torch.Size([256, 128, 3, 3, 3])
decoder.encoder.stages.3.0.convs.0.conv.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.norm.weight shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.norm.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.all_modules.0.weight shape torch.Size([256, 128, 3, 3, 3])
decoder.encoder.stages.3.0.convs.0.all_modules.0.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.all_modules.1.weight shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.all_modules.1.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.conv.weight shape torch.Size([256, 256, 3, 3, 3])
decoder.encoder.stages.3.0.convs.1.conv.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.norm.weight shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.norm.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.all_modules.0.weight shape torch.Size([256, 256, 3, 3, 3])
decoder.encoder.stages.3.0.convs.1.all_modules.0.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.all_modules.1.weight shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.all_modules.1.bias shape torch.Size([256])
decoder.encoder.stages.4.0.convs.0.conv.weight shape torch.Size([320, 256, 3, 3, 3])
decoder.encoder.stages.4.0.convs.0.conv.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.norm.weight shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.norm.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.all_modules.0.weight shape torch.Size([320, 256, 3, 3, 3])
decoder.encoder.stages.4.0.convs.0.all_modules.0.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.all_modules.1.weight shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.all_modules.1.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.4.0.convs.1.conv.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.norm.weight shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.norm.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.4.0.convs.1.all_modules.0.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.all_modules.1.weight shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.all_modules.1.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.conv.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.5.0.convs.0.conv.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.norm.weight shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.norm.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.5.0.convs.0.all_modules.0.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.all_modules.1.weight shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.all_modules.1.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.5.0.convs.1.conv.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.norm.weight shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.norm.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.5.0.convs.1.all_modules.0.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.all_modules.1.weight shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.all_modules.1.bias shape torch.Size([320])
decoder.stages.0.convs.0.conv.weight shape torch.Size([320, 640, 3, 3, 3])
decoder.stages.0.convs.0.conv.bias shape torch.Size([320])
decoder.stages.0.convs.0.norm.weight shape torch.Size([320])
decoder.stages.0.convs.0.norm.bias shape torch.Size([320])
decoder.stages.0.convs.0.all_modules.0.weight shape torch.Size([320, 640, 3, 3, 3])
decoder.stages.0.convs.0.all_modules.0.bias shape torch.Size([320])
decoder.stages.0.convs.0.all_modules.1.weight shape torch.Size([320])
decoder.stages.0.convs.0.all_modules.1.bias shape torch.Size([320])
decoder.stages.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.stages.0.convs.1.conv.bias shape torch.Size([320])
decoder.stages.0.convs.1.norm.weight shape torch.Size([320])
decoder.stages.0.convs.1.norm.bias shape torch.Size([320])
decoder.stages.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.stages.0.convs.1.all_modules.0.bias shape torch.Size([320])
decoder.stages.0.convs.1.all_modules.1.weight shape torch.Size([320])
decoder.stages.0.convs.1.all_modules.1.bias shape torch.Size([320])
decoder.stages.1.convs.0.conv.weight shape torch.Size([256, 512, 3, 3, 3])
decoder.stages.1.convs.0.conv.bias shape torch.Size([256])
decoder.stages.1.convs.0.norm.weight shape torch.Size([256])
decoder.stages.1.convs.0.norm.bias shape torch.Size([256])
decoder.stages.1.convs.0.all_modules.0.weight shape torch.Size([256, 512, 3, 3, 3])
decoder.stages.1.convs.0.all_modules.0.bias shape torch.Size([256])
decoder.stages.1.convs.0.all_modules.1.weight shape torch.Size([256])
decoder.stages.1.convs.0.all_modules.1.bias shape torch.Size([256])
decoder.stages.1.convs.1.conv.weight shape torch.Size([256, 256, 3, 3, 3])
decoder.stages.1.convs.1.conv.bias shape torch.Size([256])
decoder.stages.1.convs.1.norm.weight shape torch.Size([256])
decoder.stages.1.convs.1.norm.bias shape torch.Size([256])
decoder.stages.1.convs.1.all_modules.0.weight shape torch.Size([256, 256, 3, 3, 3])
decoder.stages.1.convs.1.all_modules.0.bias shape torch.Size([256])
decoder.stages.1.convs.1.all_modules.1.weight shape torch.Size([256])
decoder.stages.1.convs.1.all_modules.1.bias shape torch.Size([256])
decoder.stages.2.convs.0.conv.weight shape torch.Size([128, 256, 3, 3, 3])
decoder.stages.2.convs.0.conv.bias shape torch.Size([128])
decoder.stages.2.convs.0.norm.weight shape torch.Size([128])
decoder.stages.2.convs.0.norm.bias shape torch.Size([128])
decoder.stages.2.convs.0.all_modules.0.weight shape torch.Size([128, 256, 3, 3, 3])
decoder.stages.2.convs.0.all_modules.0.bias shape torch.Size([128])
decoder.stages.2.convs.0.all_modules.1.weight shape torch.Size([128])
decoder.stages.2.convs.0.all_modules.1.bias shape torch.Size([128])
decoder.stages.2.convs.1.conv.weight shape torch.Size([128, 128, 3, 3, 3])
decoder.stages.2.convs.1.conv.bias shape torch.Size([128])
decoder.stages.2.convs.1.norm.weight shape torch.Size([128])
decoder.stages.2.convs.1.norm.bias shape torch.Size([128])
decoder.stages.2.convs.1.all_modules.0.weight shape torch.Size([128, 128, 3, 3, 3])
decoder.stages.2.convs.1.all_modules.0.bias shape torch.Size([128])
decoder.stages.2.convs.1.all_modules.1.weight shape torch.Size([128])
decoder.stages.2.convs.1.all_modules.1.bias shape torch.Size([128])
decoder.stages.3.convs.0.conv.weight shape torch.Size([64, 128, 3, 3, 3])
decoder.stages.3.convs.0.conv.bias shape torch.Size([64])
decoder.stages.3.convs.0.norm.weight shape torch.Size([64])
decoder.stages.3.convs.0.norm.bias shape torch.Size([64])
decoder.stages.3.convs.0.all_modules.0.weight shape torch.Size([64, 128, 3, 3, 3])
decoder.stages.3.convs.0.all_modules.0.bias shape torch.Size([64])
decoder.stages.3.convs.0.all_modules.1.weight shape torch.Size([64])
decoder.stages.3.convs.0.all_modules.1.bias shape torch.Size([64])
decoder.stages.3.convs.1.conv.weight shape torch.Size([64, 64, 3, 3, 3])
decoder.stages.3.convs.1.conv.bias shape torch.Size([64])
decoder.stages.3.convs.1.norm.weight shape torch.Size([64])
decoder.stages.3.convs.1.norm.bias shape torch.Size([64])
decoder.stages.3.convs.1.all_modules.0.weight shape torch.Size([64, 64, 3, 3, 3])
decoder.stages.3.convs.1.all_modules.0.bias shape torch.Size([64])
decoder.stages.3.convs.1.all_modules.1.weight shape torch.Size([64])
decoder.stages.3.convs.1.all_modules.1.bias shape torch.Size([64])
decoder.stages.4.convs.0.conv.weight shape torch.Size([32, 64, 3, 3, 3])
decoder.stages.4.convs.0.conv.bias shape torch.Size([32])
decoder.stages.4.convs.0.norm.weight shape torch.Size([32])
decoder.stages.4.convs.0.norm.bias shape torch.Size([32])
decoder.stages.4.convs.0.all_modules.0.weight shape torch.Size([32, 64, 3, 3, 3])
decoder.stages.4.convs.0.all_modules.0.bias shape torch.Size([32])
decoder.stages.4.convs.0.all_modules.1.weight shape torch.Size([32])
decoder.stages.4.convs.0.all_modules.1.bias shape torch.Size([32])
decoder.stages.4.convs.1.conv.weight shape torch.Size([32, 32, 3, 3, 3])
decoder.stages.4.convs.1.conv.bias shape torch.Size([32])
decoder.stages.4.convs.1.norm.weight shape torch.Size([32])
decoder.stages.4.convs.1.norm.bias shape torch.Size([32])
decoder.stages.4.convs.1.all_modules.0.weight shape torch.Size([32, 32, 3, 3, 3])
decoder.stages.4.convs.1.all_modules.0.bias shape torch.Size([32])
decoder.stages.4.convs.1.all_modules.1.weight shape torch.Size([32])
decoder.stages.4.convs.1.all_modules.1.bias shape torch.Size([32])
decoder.transpconvs.0.weight shape torch.Size([320, 320, 1, 2, 2])
decoder.transpconvs.0.bias shape torch.Size([320])
decoder.transpconvs.1.weight shape torch.Size([320, 256, 2, 2, 2])
decoder.transpconvs.1.bias shape torch.Size([256])
decoder.transpconvs.2.weight shape torch.Size([256, 128, 2, 2, 2])
decoder.transpconvs.2.bias shape torch.Size([128])
decoder.transpconvs.3.weight shape torch.Size([128, 64, 2, 2, 2])
decoder.transpconvs.3.bias shape torch.Size([64])
decoder.transpconvs.4.weight shape torch.Size([64, 32, 2, 2, 2])
decoder.transpconvs.4.bias shape torch.Size([32])
################### Done ###################
2024-12-01 08:42:50.557515: do_dummy_2d_data_aug: True
2024-12-01 08:42:50.559919: Using splits from existing split file: /home/gridsan/nchutisilp/datasets/nnUNet_Datasets/nnUNet_preprocessed/Dataset306_Sohee_Ajay_Calcium_OCT/splits_final.json
2024-12-01 08:42:50.562077: The split file contains 5 splits.
2024-12-01 08:42:50.563229: Desired fold for training: 3
2024-12-01 08:42:50.564447: This split has 11 training and 2 validation cases.
using pin_memory on device 0
using pin_memory on device 0
using pin_memory on device 0

This is the configuration used by this training:
Configuration name: 3d_32x160x128_b10
 {'data_identifier': 'nnUNetPlans_3d_fullres', 'preprocessor_name': 'DefaultPreprocessor', 'batch_size': 10, 'patch_size': [32, 160, 128], 'median_image_size_in_voxels': [375.0, 498.0, 498.0], 'spacing': [1.0, 1.0, 1.0], 'normalization_schemes': ['ZScoreNormalization'], 'use_mask_for_norm': [False], 'resampling_fn_data': 'resample_data_or_seg_to_shape', 'resampling_fn_seg': 'resample_data_or_seg_to_shape', 'resampling_fn_data_kwargs': {'is_seg': False, 'order': 3, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_seg_kwargs': {'is_seg': True, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_probabilities': 'resample_data_or_seg_to_shape', 'resampling_fn_probabilities_kwargs': {'is_seg': False, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'architecture': {'network_class_name': 'dynamic_network_architectures.architectures.unet.PlainConvUNet', 'arch_kwargs': {'n_stages': 6, 'features_per_stage': [32, 64, 128, 256, 320, 320], 'conv_op': 'torch.nn.modules.conv.Conv3d', 'kernel_sizes': [[3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3]], 'strides': [[1, 1, 1], [2, 2, 2], [2, 2, 2], [2, 2, 2], [2, 2, 2], [1, 2, 2]], 'n_conv_per_stage': [2, 2, 2, 2, 2, 2], 'n_conv_per_stage_decoder': [2, 2, 2, 2, 2], 'conv_bias': True, 'norm_op': 'torch.nn.modules.instancenorm.InstanceNorm3d', 'norm_op_kwargs': {'eps': 1e-05, 'affine': True}, 'dropout_op': None, 'dropout_op_kwargs': None, 'nonlin': 'torch.nn.LeakyReLU', 'nonlin_kwargs': {'inplace': True}, 'deep_supervision': True}, '_kw_requires_import': ['conv_op', 'norm_op', 'dropout_op', 'nonlin']}, 'batch_dice': True, 'inherits_from': '3d_fullres'} 

These are the global plan.json settings:
 {'dataset_name': 'Dataset306_Sohee_Ajay_Calcium_OCT', 'plans_name': 'nnUNetPlans', 'original_median_spacing_after_transp': [1.0, 1.0, 1.0], 'original_median_shape_after_transp': [375, 498, 498], 'image_reader_writer': 'SimpleITKIO', 'transpose_forward': [0, 1, 2], 'transpose_backward': [0, 1, 2], 'experiment_planner_used': 'ExperimentPlanner', 'label_manager': 'LabelManager', 'foreground_intensity_properties_per_channel': {'0': {'max': 0.49803921580314636, 'mean': 0.14296366274356842, 'median': 0.10980392247438431, 'min': 0.0, 'percentile_00_5': 0.01558823511004448, 'percentile_99_5': 0.49803921580314636, 'std': 0.11562220752239227}}} 

2024-12-01 08:42:53.170163: unpacking dataset...
using pin_memory on device 0

This is the configuration used by this training:
Configuration name: 3d_32x160x128_b10
 {'data_identifier': 'nnUNetPlans_3d_fullres', 'preprocessor_name': 'DefaultPreprocessor', 'batch_size': 10, 'patch_size': [32, 160, 128], 'median_image_size_in_voxels': [375.0, 498.0, 498.0], 'spacing': [1.0, 1.0, 1.0], 'normalization_schemes': ['ZScoreNormalization'], 'use_mask_for_norm': [False], 'resampling_fn_data': 'resample_data_or_seg_to_shape', 'resampling_fn_seg': 'resample_data_or_seg_to_shape', 'resampling_fn_data_kwargs': {'is_seg': False, 'order': 3, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_seg_kwargs': {'is_seg': True, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_probabilities': 'resample_data_or_seg_to_shape', 'resampling_fn_probabilities_kwargs': {'is_seg': False, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'architecture': {'network_class_name': 'dynamic_network_architectures.architectures.unet.PlainConvUNet', 'arch_kwargs': {'n_stages': 6, 'features_per_stage': [32, 64, 128, 256, 320, 320], 'conv_op': 'torch.nn.modules.conv.Conv3d', 'kernel_sizes': [[3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3]], 'strides': [[1, 1, 1], [2, 2, 2], [2, 2, 2], [2, 2, 2], [2, 2, 2], [1, 2, 2]], 'n_conv_per_stage': [2, 2, 2, 2, 2, 2], 'n_conv_per_stage_decoder': [2, 2, 2, 2, 2], 'conv_bias': True, 'norm_op': 'torch.nn.modules.instancenorm.InstanceNorm3d', 'norm_op_kwargs': {'eps': 1e-05, 'affine': True}, 'dropout_op': None, 'dropout_op_kwargs': None, 'nonlin': 'torch.nn.LeakyReLU', 'nonlin_kwargs': {'inplace': True}, 'deep_supervision': True}, '_kw_requires_import': ['conv_op', 'norm_op', 'dropout_op', 'nonlin']}, 'batch_dice': True, 'inherits_from': '3d_fullres'} 

These are the global plan.json settings:
 {'dataset_name': 'Dataset306_Sohee_Ajay_Calcium_OCT', 'plans_name': 'nnUNetPlans', 'original_median_spacing_after_transp': [1.0, 1.0, 1.0], 'original_median_shape_after_transp': [375, 498, 498], 'image_reader_writer': 'SimpleITKIO', 'transpose_forward': [0, 1, 2], 'transpose_backward': [0, 1, 2], 'experiment_planner_used': 'ExperimentPlanner', 'label_manager': 'LabelManager', 'foreground_intensity_properties_per_channel': {'0': {'max': 0.49803921580314636, 'mean': 0.14296366274356842, 'median': 0.10980392247438431, 'min': 0.0, 'percentile_00_5': 0.01558823511004448, 'percentile_99_5': 0.49803921580314636, 'std': 0.11562220752239227}}} 

2024-12-01 08:42:53.464490: unpacking dataset...
2024-12-01 08:42:57.693808: unpacking done...
2024-12-01 08:42:57.886566: Unable to plot network architecture: nnUNet_compile is enabled!
2024-12-01 08:42:57.968065: 
2024-12-01 08:42:57.969459: Epoch 0
2024-12-01 08:42:57.970963: Current learning rate: 0.01
2024-12-01 08:45:23.909086: Validation loss improved from 1000.00000 to -0.35523! Patience: 0/50
2024-12-01 08:45:23.910287: train_loss -0.2744
2024-12-01 08:45:23.911164: val_loss -0.3552
2024-12-01 08:45:23.912040: Pseudo dice [0.6172]
2024-12-01 08:45:23.912925: Epoch time: 145.94 s
2024-12-01 08:45:23.913724: Yayy! New best EMA pseudo Dice: 0.6172
2024-12-01 08:45:25.771014: 
2024-12-01 08:45:25.772505: Epoch 1
2024-12-01 08:45:25.773594: Current learning rate: 0.00999
2024-12-01 08:46:52.358399: Validation loss improved from -0.35523 to -0.39289! Patience: 0/50
2024-12-01 08:46:52.359472: train_loss -0.4555
2024-12-01 08:46:52.360201: val_loss -0.3929
2024-12-01 08:46:52.360857: Pseudo dice [0.644]
2024-12-01 08:46:52.361633: Epoch time: 86.59 s
2024-12-01 08:46:52.362338: Yayy! New best EMA pseudo Dice: 0.6199
2024-12-01 08:46:53.999306: 
2024-12-01 08:46:54.000875: Epoch 2
2024-12-01 08:46:54.001697: Current learning rate: 0.00998
2024-12-01 08:48:20.768024: Validation loss improved from -0.39289 to -0.40599! Patience: 0/50
2024-12-01 08:48:20.768975: train_loss -0.506
2024-12-01 08:48:20.770252: val_loss -0.406
2024-12-01 08:48:20.771428: Pseudo dice [0.6487]
2024-12-01 08:48:20.772519: Epoch time: 86.77 s
2024-12-01 08:48:20.773526: Yayy! New best EMA pseudo Dice: 0.6228
2024-12-01 08:48:22.421462: 
2024-12-01 08:48:22.422421: Epoch 3
2024-12-01 08:48:22.423206: Current learning rate: 0.00997
2024-12-01 08:49:49.472210: Validation loss improved from -0.40599 to -0.42540! Patience: 0/50
2024-12-01 08:49:49.473178: train_loss -0.5094
2024-12-01 08:49:49.474130: val_loss -0.4254
2024-12-01 08:49:49.474870: Pseudo dice [0.6632]
2024-12-01 08:49:49.475519: Epoch time: 87.05 s
2024-12-01 08:49:49.476275: Yayy! New best EMA pseudo Dice: 0.6268
2024-12-01 08:49:51.115429: 
2024-12-01 08:49:51.116549: Epoch 4
2024-12-01 08:49:51.117294: Current learning rate: 0.00996
2024-12-01 08:51:18.481633: Validation loss did not improve from -0.42540. Patience: 1/50
2024-12-01 08:51:18.482705: train_loss -0.5319
2024-12-01 08:51:18.483469: val_loss -0.3984
2024-12-01 08:51:18.484100: Pseudo dice [0.6499]
2024-12-01 08:51:18.484753: Epoch time: 87.37 s
2024-12-01 08:51:18.859804: Yayy! New best EMA pseudo Dice: 0.6291
2024-12-01 08:51:20.554289: 
2024-12-01 08:51:20.555831: Epoch 5
2024-12-01 08:51:20.556517: Current learning rate: 0.00995
2024-12-01 08:52:47.680579: Validation loss did not improve from -0.42540. Patience: 2/50
2024-12-01 08:52:47.681790: train_loss -0.5264
2024-12-01 08:52:47.682790: val_loss -0.4112
2024-12-01 08:52:47.683522: Pseudo dice [0.6498]
2024-12-01 08:52:47.684293: Epoch time: 87.13 s
2024-12-01 08:52:47.684936: Yayy! New best EMA pseudo Dice: 0.6312
2024-12-01 08:52:49.340700: 
2024-12-01 08:52:49.342265: Epoch 6
2024-12-01 08:52:49.343029: Current learning rate: 0.00995
2024-12-01 08:54:16.727574: Validation loss improved from -0.42540 to -0.43232! Patience: 2/50
2024-12-01 08:54:16.728802: train_loss -0.54
2024-12-01 08:54:16.729772: val_loss -0.4323
2024-12-01 08:54:16.730438: Pseudo dice [0.67]
2024-12-01 08:54:16.731020: Epoch time: 87.39 s
2024-12-01 08:54:16.731617: Yayy! New best EMA pseudo Dice: 0.6351
2024-12-01 08:54:18.443722: 
2024-12-01 08:54:18.445014: Epoch 7
2024-12-01 08:54:18.446087: Current learning rate: 0.00994
2024-12-01 08:55:45.709581: Validation loss improved from -0.43232 to -0.43838! Patience: 0/50
2024-12-01 08:55:45.710613: train_loss -0.5611
2024-12-01 08:55:45.711680: val_loss -0.4384
2024-12-01 08:55:45.712619: Pseudo dice [0.6768]
2024-12-01 08:55:45.713428: Epoch time: 87.27 s
2024-12-01 08:55:45.714223: Yayy! New best EMA pseudo Dice: 0.6392
2024-12-01 08:55:47.860238: 
2024-12-01 08:55:47.862172: Epoch 8
2024-12-01 08:55:47.863144: Current learning rate: 0.00993
2024-12-01 08:57:15.506972: Validation loss did not improve from -0.43838. Patience: 1/50
2024-12-01 08:57:15.507946: train_loss -0.5604
2024-12-01 08:57:15.508829: val_loss -0.4292
2024-12-01 08:57:15.509664: Pseudo dice [0.667]
2024-12-01 08:57:15.510478: Epoch time: 87.65 s
2024-12-01 08:57:15.511170: Yayy! New best EMA pseudo Dice: 0.642
2024-12-01 08:57:17.165132: 
2024-12-01 08:57:17.166931: Epoch 9
2024-12-01 08:57:17.167942: Current learning rate: 0.00992
2024-12-01 08:58:44.890567: Validation loss did not improve from -0.43838. Patience: 2/50
2024-12-01 08:58:44.891879: train_loss -0.5692
2024-12-01 08:58:44.892972: val_loss -0.3851
2024-12-01 08:58:44.893854: Pseudo dice [0.6378]
2024-12-01 08:58:44.894621: Epoch time: 87.73 s
2024-12-01 08:58:46.553474: 
2024-12-01 08:58:46.554909: Epoch 10
2024-12-01 08:58:46.555589: Current learning rate: 0.00991
2024-12-01 09:00:13.819749: Validation loss did not improve from -0.43838. Patience: 3/50
2024-12-01 09:00:13.820563: train_loss -0.5856
2024-12-01 09:00:13.821373: val_loss -0.4315
2024-12-01 09:00:13.822151: Pseudo dice [0.6679]
2024-12-01 09:00:13.822833: Epoch time: 87.27 s
2024-12-01 09:00:13.823482: Yayy! New best EMA pseudo Dice: 0.6442
2024-12-01 09:00:15.383330: 
2024-12-01 09:00:15.384728: Epoch 11
2024-12-01 09:00:15.385354: Current learning rate: 0.0099
2024-12-01 09:01:42.602753: Validation loss improved from -0.43838 to -0.44650! Patience: 3/50
2024-12-01 09:01:42.603973: train_loss -0.5833
2024-12-01 09:01:42.605077: val_loss -0.4465
2024-12-01 09:01:42.605804: Pseudo dice [0.6846]
2024-12-01 09:01:42.606612: Epoch time: 87.22 s
2024-12-01 09:01:42.607389: Yayy! New best EMA pseudo Dice: 0.6483
2024-12-01 09:01:44.264466: 
2024-12-01 09:01:44.266169: Epoch 12
2024-12-01 09:01:44.267156: Current learning rate: 0.00989
2024-12-01 09:03:11.496010: Validation loss did not improve from -0.44650. Patience: 1/50
2024-12-01 09:03:11.497809: train_loss -0.5846
2024-12-01 09:03:11.499007: val_loss -0.4235
2024-12-01 09:03:11.499733: Pseudo dice [0.666]
2024-12-01 09:03:11.500642: Epoch time: 87.23 s
2024-12-01 09:03:11.501367: Yayy! New best EMA pseudo Dice: 0.65
2024-12-01 09:03:13.262211: 
2024-12-01 09:03:13.263367: Epoch 13
2024-12-01 09:03:13.264298: Current learning rate: 0.00988
2024-12-01 09:04:40.328333: Validation loss did not improve from -0.44650. Patience: 2/50
2024-12-01 09:04:40.329019: train_loss -0.5991
2024-12-01 09:04:40.329826: val_loss -0.4232
2024-12-01 09:04:40.330714: Pseudo dice [0.657]
2024-12-01 09:04:40.331587: Epoch time: 87.07 s
2024-12-01 09:04:40.332655: Yayy! New best EMA pseudo Dice: 0.6507
2024-12-01 09:04:41.965220: 
2024-12-01 09:04:41.966355: Epoch 14
2024-12-01 09:04:41.967153: Current learning rate: 0.00987
2024-12-01 09:06:08.968846: Validation loss improved from -0.44650 to -0.44715! Patience: 2/50
2024-12-01 09:06:08.970035: train_loss -0.6014
2024-12-01 09:06:08.970979: val_loss -0.4471
2024-12-01 09:06:08.971673: Pseudo dice [0.6802]
2024-12-01 09:06:08.972456: Epoch time: 87.01 s
2024-12-01 09:06:09.337525: Yayy! New best EMA pseudo Dice: 0.6537
2024-12-01 09:06:10.937655: 
2024-12-01 09:06:10.939244: Epoch 15
2024-12-01 09:06:10.940233: Current learning rate: 0.00986
2024-12-01 09:07:38.047104: Validation loss improved from -0.44715 to -0.48030! Patience: 0/50
2024-12-01 09:07:38.048248: train_loss -0.5967
2024-12-01 09:07:38.049042: val_loss -0.4803
2024-12-01 09:07:38.049726: Pseudo dice [0.7011]
2024-12-01 09:07:38.050567: Epoch time: 87.11 s
2024-12-01 09:07:38.051409: Yayy! New best EMA pseudo Dice: 0.6584
2024-12-01 09:07:39.696400: 
2024-12-01 09:07:39.697821: Epoch 16
2024-12-01 09:07:39.698716: Current learning rate: 0.00986
2024-12-01 09:09:06.808584: Validation loss did not improve from -0.48030. Patience: 1/50
2024-12-01 09:09:06.809791: train_loss -0.6107
2024-12-01 09:09:06.810610: val_loss -0.478
2024-12-01 09:09:06.811229: Pseudo dice [0.7007]
2024-12-01 09:09:06.811847: Epoch time: 87.11 s
2024-12-01 09:09:06.812547: Yayy! New best EMA pseudo Dice: 0.6626
2024-12-01 09:09:08.501758: 
2024-12-01 09:09:08.502902: Epoch 17
2024-12-01 09:09:08.503675: Current learning rate: 0.00985
2024-12-01 09:10:35.567620: Validation loss did not improve from -0.48030. Patience: 2/50
2024-12-01 09:10:35.568424: train_loss -0.6084
2024-12-01 09:10:35.569135: val_loss -0.4638
2024-12-01 09:10:35.569825: Pseudo dice [0.6903]
2024-12-01 09:10:35.570561: Epoch time: 87.07 s
2024-12-01 09:10:35.571164: Yayy! New best EMA pseudo Dice: 0.6654
2024-12-01 09:10:37.253209: 
2024-12-01 09:10:37.254880: Epoch 18
2024-12-01 09:10:37.255642: Current learning rate: 0.00984
2024-12-01 09:12:04.633335: Validation loss did not improve from -0.48030. Patience: 3/50
2024-12-01 09:12:04.634054: train_loss -0.6181
2024-12-01 09:12:04.634795: val_loss -0.4414
2024-12-01 09:12:04.635386: Pseudo dice [0.6761]
2024-12-01 09:12:04.636012: Epoch time: 87.38 s
2024-12-01 09:12:04.636665: Yayy! New best EMA pseudo Dice: 0.6665
2024-12-01 09:12:06.701158: 
2024-12-01 09:12:06.702817: Epoch 19
2024-12-01 09:12:06.703455: Current learning rate: 0.00983
2024-12-01 09:13:34.275726: Validation loss did not improve from -0.48030. Patience: 4/50
2024-12-01 09:13:34.276570: train_loss -0.6233
2024-12-01 09:13:34.277490: val_loss -0.4656
2024-12-01 09:13:34.278261: Pseudo dice [0.6998]
2024-12-01 09:13:34.278962: Epoch time: 87.58 s
2024-12-01 09:13:34.655420: Yayy! New best EMA pseudo Dice: 0.6698
2024-12-01 09:13:36.288828: 
2024-12-01 09:13:36.290376: Epoch 20
2024-12-01 09:13:36.291307: Current learning rate: 0.00982
2024-12-01 09:15:03.785680: Validation loss did not improve from -0.48030. Patience: 5/50
2024-12-01 09:15:03.786468: train_loss -0.6273
2024-12-01 09:15:03.787582: val_loss -0.4625
2024-12-01 09:15:03.788491: Pseudo dice [0.6983]
2024-12-01 09:15:03.789314: Epoch time: 87.5 s
2024-12-01 09:15:03.790059: Yayy! New best EMA pseudo Dice: 0.6727
2024-12-01 09:15:05.480134: 
2024-12-01 09:15:05.481107: Epoch 21
2024-12-01 09:15:05.481912: Current learning rate: 0.00981
2024-12-01 09:16:33.049230: Validation loss did not improve from -0.48030. Patience: 6/50
2024-12-01 09:16:33.050163: train_loss -0.6065
2024-12-01 09:16:33.051131: val_loss -0.467
2024-12-01 09:16:33.051890: Pseudo dice [0.6877]
2024-12-01 09:16:33.052608: Epoch time: 87.57 s
2024-12-01 09:16:33.053258: Yayy! New best EMA pseudo Dice: 0.6742
2024-12-01 09:16:34.664975: 
2024-12-01 09:16:34.666152: Epoch 22
2024-12-01 09:16:34.666810: Current learning rate: 0.0098
2024-12-01 09:18:02.361148: Validation loss did not improve from -0.48030. Patience: 7/50
2024-12-01 09:18:02.362118: train_loss -0.6278
2024-12-01 09:18:02.362911: val_loss -0.4737
2024-12-01 09:18:02.363667: Pseudo dice [0.6968]
2024-12-01 09:18:02.364319: Epoch time: 87.7 s
2024-12-01 09:18:02.365108: Yayy! New best EMA pseudo Dice: 0.6764
2024-12-01 09:18:03.921545: 
2024-12-01 09:18:03.922997: Epoch 23
2024-12-01 09:18:03.923649: Current learning rate: 0.00979
2024-12-01 09:19:31.456895: Validation loss did not improve from -0.48030. Patience: 8/50
2024-12-01 09:19:31.458032: train_loss -0.6337
2024-12-01 09:19:31.458953: val_loss -0.4608
2024-12-01 09:19:31.459619: Pseudo dice [0.6897]
2024-12-01 09:19:31.460299: Epoch time: 87.54 s
2024-12-01 09:19:31.460841: Yayy! New best EMA pseudo Dice: 0.6777
2024-12-01 09:19:33.026411: 
2024-12-01 09:19:33.027764: Epoch 24
2024-12-01 09:19:33.028533: Current learning rate: 0.00978
2024-12-01 09:21:00.712936: Validation loss improved from -0.48030 to -0.50920! Patience: 8/50
2024-12-01 09:21:00.715832: train_loss -0.6264
2024-12-01 09:21:00.716901: val_loss -0.5092
2024-12-01 09:21:00.717655: Pseudo dice [0.724]
2024-12-01 09:21:00.718510: Epoch time: 87.69 s
2024-12-01 09:21:01.101627: Yayy! New best EMA pseudo Dice: 0.6824
2024-12-01 09:21:02.708908: 
2024-12-01 09:21:02.710426: Epoch 25
2024-12-01 09:21:02.711035: Current learning rate: 0.00977
2024-12-01 09:22:30.678180: Validation loss did not improve from -0.50920. Patience: 1/50
2024-12-01 09:22:30.679076: train_loss -0.6399
2024-12-01 09:22:30.679817: val_loss -0.4381
2024-12-01 09:22:30.680545: Pseudo dice [0.6779]
2024-12-01 09:22:30.681307: Epoch time: 87.97 s
2024-12-01 09:22:31.947587: 
2024-12-01 09:22:31.948792: Epoch 26
2024-12-01 09:22:31.949688: Current learning rate: 0.00977
2024-12-01 09:24:00.239741: Validation loss did not improve from -0.50920. Patience: 2/50
2024-12-01 09:24:00.241781: train_loss -0.632
2024-12-01 09:24:00.242934: val_loss -0.4604
2024-12-01 09:24:00.243701: Pseudo dice [0.6871]
2024-12-01 09:24:00.244571: Epoch time: 88.3 s
2024-12-01 09:24:00.245334: Yayy! New best EMA pseudo Dice: 0.6824
2024-12-01 09:24:01.843659: 
2024-12-01 09:24:01.845262: Epoch 27
2024-12-01 09:24:01.846128: Current learning rate: 0.00976
2024-12-01 09:25:30.048849: Validation loss did not improve from -0.50920. Patience: 3/50
2024-12-01 09:25:30.050038: train_loss -0.6384
2024-12-01 09:25:30.050899: val_loss -0.4537
2024-12-01 09:25:30.051670: Pseudo dice [0.6781]
2024-12-01 09:25:30.052530: Epoch time: 88.21 s
2024-12-01 09:25:31.294858: 
2024-12-01 09:25:31.296194: Epoch 28
2024-12-01 09:25:31.296913: Current learning rate: 0.00975
2024-12-01 09:26:59.671065: Validation loss did not improve from -0.50920. Patience: 4/50
2024-12-01 09:26:59.672098: train_loss -0.6443
2024-12-01 09:26:59.673138: val_loss -0.5014
2024-12-01 09:26:59.673925: Pseudo dice [0.7169]
2024-12-01 09:26:59.674641: Epoch time: 88.38 s
2024-12-01 09:26:59.675336: Yayy! New best EMA pseudo Dice: 0.6855
2024-12-01 09:27:01.293698: 
2024-12-01 09:27:01.294935: Epoch 29
2024-12-01 09:27:01.295602: Current learning rate: 0.00974
2024-12-01 09:28:29.675782: Validation loss did not improve from -0.50920. Patience: 5/50
2024-12-01 09:28:29.676732: train_loss -0.6499
2024-12-01 09:28:29.677642: val_loss -0.4836
2024-12-01 09:28:29.678380: Pseudo dice [0.7085]
2024-12-01 09:28:29.679136: Epoch time: 88.38 s
2024-12-01 09:28:30.468741: Yayy! New best EMA pseudo Dice: 0.6878
2024-12-01 09:28:32.140379: 
2024-12-01 09:28:32.141758: Epoch 30
2024-12-01 09:28:32.142480: Current learning rate: 0.00973
2024-12-01 09:30:00.436217: Validation loss did not improve from -0.50920. Patience: 6/50
2024-12-01 09:30:00.436962: train_loss -0.6495
2024-12-01 09:30:00.437648: val_loss -0.4395
2024-12-01 09:30:00.438377: Pseudo dice [0.6737]
2024-12-01 09:30:00.439066: Epoch time: 88.3 s
2024-12-01 09:30:01.736057: 
2024-12-01 09:30:01.737130: Epoch 31
2024-12-01 09:30:01.737758: Current learning rate: 0.00972
2024-12-01 09:31:30.115965: Validation loss did not improve from -0.50920. Patience: 7/50
2024-12-01 09:31:30.117166: train_loss -0.6535
2024-12-01 09:31:30.118056: val_loss -0.4501
2024-12-01 09:31:30.118850: Pseudo dice [0.683]
2024-12-01 09:31:30.119512: Epoch time: 88.38 s
2024-12-01 09:31:31.397539: 
2024-12-01 09:31:31.399099: Epoch 32
2024-12-01 09:31:31.399945: Current learning rate: 0.00971
2024-12-01 09:32:59.840471: Validation loss did not improve from -0.50920. Patience: 8/50
2024-12-01 09:32:59.841873: train_loss -0.6603
2024-12-01 09:32:59.842915: val_loss -0.502
2024-12-01 09:32:59.843726: Pseudo dice [0.7248]
2024-12-01 09:32:59.844893: Epoch time: 88.45 s
2024-12-01 09:32:59.845921: Yayy! New best EMA pseudo Dice: 0.6899
2024-12-01 09:33:01.493943: 
2024-12-01 09:33:01.495970: Epoch 33
2024-12-01 09:33:01.496795: Current learning rate: 0.0097
2024-12-01 09:34:29.941256: Validation loss did not improve from -0.50920. Patience: 9/50
2024-12-01 09:34:29.942499: train_loss -0.6427
2024-12-01 09:34:29.943306: val_loss -0.4538
2024-12-01 09:34:29.944020: Pseudo dice [0.6812]
2024-12-01 09:34:29.944626: Epoch time: 88.45 s
2024-12-01 09:34:31.227284: 
2024-12-01 09:34:31.229045: Epoch 34
2024-12-01 09:34:31.230063: Current learning rate: 0.00969
2024-12-01 09:35:59.930724: Validation loss did not improve from -0.50920. Patience: 10/50
2024-12-01 09:35:59.931807: train_loss -0.6502
2024-12-01 09:35:59.932927: val_loss -0.4008
2024-12-01 09:35:59.933989: Pseudo dice [0.6463]
2024-12-01 09:35:59.934917: Epoch time: 88.71 s
2024-12-01 09:36:01.621085: 
2024-12-01 09:36:01.622577: Epoch 35
2024-12-01 09:36:01.623613: Current learning rate: 0.00968
2024-12-01 09:37:30.407161: Validation loss did not improve from -0.50920. Patience: 11/50
2024-12-01 09:37:30.408302: train_loss -0.6413
2024-12-01 09:37:30.409333: val_loss -0.4685
2024-12-01 09:37:30.410234: Pseudo dice [0.6943]
2024-12-01 09:37:30.411129: Epoch time: 88.79 s
2024-12-01 09:37:31.735871: 
2024-12-01 09:37:31.737360: Epoch 36
2024-12-01 09:37:31.738306: Current learning rate: 0.00968
2024-12-01 09:39:00.420727: Validation loss did not improve from -0.50920. Patience: 12/50
2024-12-01 09:39:00.421994: train_loss -0.6623
2024-12-01 09:39:00.423062: val_loss -0.4762
2024-12-01 09:39:00.423957: Pseudo dice [0.6974]
2024-12-01 09:39:00.424754: Epoch time: 88.69 s
2024-12-01 09:39:01.749559: 
2024-12-01 09:39:01.751493: Epoch 37
2024-12-01 09:39:01.752238: Current learning rate: 0.00967
2024-12-01 09:40:30.461918: Validation loss did not improve from -0.50920. Patience: 13/50
2024-12-01 09:40:30.463070: train_loss -0.653
2024-12-01 09:40:30.464050: val_loss -0.4404
2024-12-01 09:40:30.464757: Pseudo dice [0.6633]
2024-12-01 09:40:30.465481: Epoch time: 88.72 s
2024-12-01 09:40:31.750441: 
2024-12-01 09:40:31.752325: Epoch 38
2024-12-01 09:40:31.753184: Current learning rate: 0.00966
2024-12-01 09:42:00.400655: Validation loss did not improve from -0.50920. Patience: 14/50
2024-12-01 09:42:00.401620: train_loss -0.6672
2024-12-01 09:42:00.402526: val_loss -0.5014
2024-12-01 09:42:00.403244: Pseudo dice [0.7066]
2024-12-01 09:42:00.404114: Epoch time: 88.65 s
2024-12-01 09:42:01.733682: 
2024-12-01 09:42:01.735323: Epoch 39
2024-12-01 09:42:01.736137: Current learning rate: 0.00965
2024-12-01 09:43:30.466170: Validation loss did not improve from -0.50920. Patience: 15/50
2024-12-01 09:43:30.467113: train_loss -0.6551
2024-12-01 09:43:30.468189: val_loss -0.4981
2024-12-01 09:43:30.468954: Pseudo dice [0.7147]
2024-12-01 09:43:30.469620: Epoch time: 88.73 s
2024-12-01 09:43:32.556809: 
2024-12-01 09:43:32.558423: Epoch 40
2024-12-01 09:43:32.559345: Current learning rate: 0.00964
2024-12-01 09:45:01.209272: Validation loss did not improve from -0.50920. Patience: 16/50
2024-12-01 09:45:01.210315: train_loss -0.6607
2024-12-01 09:45:01.211202: val_loss -0.4225
2024-12-01 09:45:01.212017: Pseudo dice [0.6637]
2024-12-01 09:45:01.212789: Epoch time: 88.65 s
2024-12-01 09:45:02.573882: 
2024-12-01 09:45:02.575242: Epoch 41
2024-12-01 09:45:02.576118: Current learning rate: 0.00963
2024-12-01 09:46:31.225802: Validation loss did not improve from -0.50920. Patience: 17/50
2024-12-01 09:46:31.226745: train_loss -0.6719
2024-12-01 09:46:31.227504: val_loss -0.4509
2024-12-01 09:46:31.228160: Pseudo dice [0.6842]
2024-12-01 09:46:31.228931: Epoch time: 88.65 s
2024-12-01 09:46:32.470420: 
2024-12-01 09:46:32.471884: Epoch 42
2024-12-01 09:46:32.472546: Current learning rate: 0.00962
2024-12-01 09:48:01.353063: Validation loss did not improve from -0.50920. Patience: 18/50
2024-12-01 09:48:01.355874: train_loss -0.6771
2024-12-01 09:48:01.356923: val_loss -0.4231
2024-12-01 09:48:01.357715: Pseudo dice [0.6583]
2024-12-01 09:48:01.358484: Epoch time: 88.89 s
2024-12-01 09:48:02.577083: 
2024-12-01 09:48:02.578502: Epoch 43
2024-12-01 09:48:02.579323: Current learning rate: 0.00961
2024-12-01 09:49:31.584414: Validation loss did not improve from -0.50920. Patience: 19/50
2024-12-01 09:49:31.586127: train_loss -0.6686
2024-12-01 09:49:31.588312: val_loss -0.4754
2024-12-01 09:49:31.589301: Pseudo dice [0.6916]
2024-12-01 09:49:31.590617: Epoch time: 89.01 s
2024-12-01 09:49:32.863609: 
2024-12-01 09:49:32.864792: Epoch 44
2024-12-01 09:49:32.865548: Current learning rate: 0.0096
2024-12-01 09:51:01.579849: Validation loss did not improve from -0.50920. Patience: 20/50
2024-12-01 09:51:01.580627: train_loss -0.6748
2024-12-01 09:51:01.581541: val_loss -0.4964
2024-12-01 09:51:01.582134: Pseudo dice [0.716]
2024-12-01 09:51:01.582797: Epoch time: 88.72 s
2024-12-01 09:51:03.180811: 
2024-12-01 09:51:03.182314: Epoch 45
2024-12-01 09:51:03.183258: Current learning rate: 0.00959
2024-12-01 09:52:31.746153: Validation loss did not improve from -0.50920. Patience: 21/50
2024-12-01 09:52:31.747278: train_loss -0.6719
2024-12-01 09:52:31.748260: val_loss -0.4961
2024-12-01 09:52:31.748938: Pseudo dice [0.7176]
2024-12-01 09:52:31.749657: Epoch time: 88.57 s
2024-12-01 09:52:31.750378: Yayy! New best EMA pseudo Dice: 0.6907
2024-12-01 09:52:33.332531: 
2024-12-01 09:52:33.334235: Epoch 46
2024-12-01 09:52:33.335260: Current learning rate: 0.00959
2024-12-01 09:54:01.950338: Validation loss did not improve from -0.50920. Patience: 22/50
2024-12-01 09:54:01.951374: train_loss -0.675
2024-12-01 09:54:01.952449: val_loss -0.5001
2024-12-01 09:54:01.953044: Pseudo dice [0.7061]
2024-12-01 09:54:01.953624: Epoch time: 88.62 s
2024-12-01 09:54:01.954243: Yayy! New best EMA pseudo Dice: 0.6923
2024-12-01 09:54:03.526659: 
2024-12-01 09:54:03.527978: Epoch 47
2024-12-01 09:54:03.528616: Current learning rate: 0.00958
2024-12-01 09:55:32.133075: Validation loss did not improve from -0.50920. Patience: 23/50
2024-12-01 09:55:32.134257: train_loss -0.673
2024-12-01 09:55:32.135324: val_loss -0.4603
2024-12-01 09:55:32.136075: Pseudo dice [0.6857]
2024-12-01 09:55:32.136750: Epoch time: 88.61 s
2024-12-01 09:55:33.420573: 
2024-12-01 09:55:33.422247: Epoch 48
2024-12-01 09:55:33.422963: Current learning rate: 0.00957
2024-12-01 09:57:02.024582: Validation loss did not improve from -0.50920. Patience: 24/50
2024-12-01 09:57:02.025478: train_loss -0.6845
2024-12-01 09:57:02.026285: val_loss -0.4957
2024-12-01 09:57:02.027143: Pseudo dice [0.7157]
2024-12-01 09:57:02.027957: Epoch time: 88.61 s
2024-12-01 09:57:02.028652: Yayy! New best EMA pseudo Dice: 0.694
2024-12-01 09:57:03.585957: 
2024-12-01 09:57:03.586995: Epoch 49
2024-12-01 09:57:03.587645: Current learning rate: 0.00956
2024-12-01 09:58:32.207626: Validation loss did not improve from -0.50920. Patience: 25/50
2024-12-01 09:58:32.208770: train_loss -0.6781
2024-12-01 09:58:32.209774: val_loss -0.5072
2024-12-01 09:58:32.210676: Pseudo dice [0.7265]
2024-12-01 09:58:32.211516: Epoch time: 88.62 s
2024-12-01 09:58:32.597382: Yayy! New best EMA pseudo Dice: 0.6973
2024-12-01 09:58:34.877656: 
2024-12-01 09:58:34.879180: Epoch 50
2024-12-01 09:58:34.880189: Current learning rate: 0.00955
2024-12-01 10:00:03.511786: Validation loss did not improve from -0.50920. Patience: 26/50
2024-12-01 10:00:03.512990: train_loss -0.6603
2024-12-01 10:00:03.514139: val_loss -0.4728
2024-12-01 10:00:03.514857: Pseudo dice [0.6946]
2024-12-01 10:00:03.515570: Epoch time: 88.64 s
2024-12-01 10:00:04.828634: 
2024-12-01 10:00:04.830342: Epoch 51
2024-12-01 10:00:04.831130: Current learning rate: 0.00954
2024-12-01 10:01:33.517322: Validation loss did not improve from -0.50920. Patience: 27/50
2024-12-01 10:01:33.518539: train_loss -0.6792
2024-12-01 10:01:33.519486: val_loss -0.4969
2024-12-01 10:01:33.520323: Pseudo dice [0.7037]
2024-12-01 10:01:33.521150: Epoch time: 88.69 s
2024-12-01 10:01:33.522047: Yayy! New best EMA pseudo Dice: 0.6977
2024-12-01 10:01:35.129125: 
2024-12-01 10:01:35.130437: Epoch 52
2024-12-01 10:01:35.131296: Current learning rate: 0.00953
2024-12-01 10:03:04.049821: Validation loss did not improve from -0.50920. Patience: 28/50
2024-12-01 10:03:04.051341: train_loss -0.6848
2024-12-01 10:03:04.052606: val_loss -0.468
2024-12-01 10:03:04.053217: Pseudo dice [0.6928]
2024-12-01 10:03:04.053838: Epoch time: 88.92 s
2024-12-01 10:03:05.328098: 
2024-12-01 10:03:05.329624: Epoch 53
2024-12-01 10:03:05.330500: Current learning rate: 0.00952
2024-12-01 10:04:34.113216: Validation loss did not improve from -0.50920. Patience: 29/50
2024-12-01 10:04:34.114536: train_loss -0.6838
2024-12-01 10:04:34.115420: val_loss -0.4539
2024-12-01 10:04:34.116248: Pseudo dice [0.6905]
2024-12-01 10:04:34.117111: Epoch time: 88.79 s
2024-12-01 10:04:35.391258: 
2024-12-01 10:04:35.392538: Epoch 54
2024-12-01 10:04:35.393715: Current learning rate: 0.00951
2024-12-01 10:06:03.896787: Validation loss did not improve from -0.50920. Patience: 30/50
2024-12-01 10:06:03.897752: train_loss -0.6795
2024-12-01 10:06:03.898654: val_loss -0.4392
2024-12-01 10:06:03.899376: Pseudo dice [0.6851]
2024-12-01 10:06:03.900204: Epoch time: 88.51 s
2024-12-01 10:06:05.546309: 
2024-12-01 10:06:05.548152: Epoch 55
2024-12-01 10:06:05.548988: Current learning rate: 0.0095
2024-12-01 10:07:33.990110: Validation loss did not improve from -0.50920. Patience: 31/50
2024-12-01 10:07:33.991556: train_loss -0.6799
2024-12-01 10:07:33.992597: val_loss -0.4437
2024-12-01 10:07:33.993314: Pseudo dice [0.687]
2024-12-01 10:07:33.994092: Epoch time: 88.45 s
2024-12-01 10:07:35.282675: 
2024-12-01 10:07:35.284120: Epoch 56
2024-12-01 10:07:35.284807: Current learning rate: 0.00949
2024-12-01 10:09:03.783159: Validation loss did not improve from -0.50920. Patience: 32/50
2024-12-01 10:09:03.784167: train_loss -0.687
2024-12-01 10:09:03.784910: val_loss -0.4656
2024-12-01 10:09:03.785464: Pseudo dice [0.676]
2024-12-01 10:09:03.786144: Epoch time: 88.5 s
2024-12-01 10:09:05.002327: 
2024-12-01 10:09:05.003773: Epoch 57
2024-12-01 10:09:05.004460: Current learning rate: 0.00949
2024-12-01 10:10:33.444795: Validation loss did not improve from -0.50920. Patience: 33/50
2024-12-01 10:10:33.445965: train_loss -0.692
2024-12-01 10:10:33.446840: val_loss -0.4784
2024-12-01 10:10:33.447596: Pseudo dice [0.7055]
2024-12-01 10:10:33.448241: Epoch time: 88.44 s
2024-12-01 10:10:34.672925: 
2024-12-01 10:10:34.674165: Epoch 58
2024-12-01 10:10:34.674862: Current learning rate: 0.00948
2024-12-01 10:12:03.269837: Validation loss did not improve from -0.50920. Patience: 34/50
2024-12-01 10:12:03.270960: train_loss -0.69
2024-12-01 10:12:03.271944: val_loss -0.4825
2024-12-01 10:12:03.272747: Pseudo dice [0.706]
2024-12-01 10:12:03.273351: Epoch time: 88.6 s
2024-12-01 10:12:04.550199: 
2024-12-01 10:12:04.551805: Epoch 59
2024-12-01 10:12:04.552563: Current learning rate: 0.00947
2024-12-01 10:13:33.033497: Validation loss did not improve from -0.50920. Patience: 35/50
2024-12-01 10:13:33.034581: train_loss -0.6863
2024-12-01 10:13:33.035403: val_loss -0.4617
2024-12-01 10:13:33.036115: Pseudo dice [0.6823]
2024-12-01 10:13:33.036773: Epoch time: 88.49 s
2024-12-01 10:13:34.707267: 
2024-12-01 10:13:34.708946: Epoch 60
2024-12-01 10:13:34.709658: Current learning rate: 0.00946
2024-12-01 10:15:03.018536: Validation loss did not improve from -0.50920. Patience: 36/50
2024-12-01 10:15:03.019363: train_loss -0.6931
2024-12-01 10:15:03.020197: val_loss -0.5045
2024-12-01 10:15:03.020830: Pseudo dice [0.7202]
2024-12-01 10:15:03.021555: Epoch time: 88.31 s
2024-12-01 10:15:04.320694: 
2024-12-01 10:15:04.321845: Epoch 61
2024-12-01 10:15:04.322511: Current learning rate: 0.00945
2024-12-01 10:16:32.835355: Validation loss did not improve from -0.50920. Patience: 37/50
2024-12-01 10:16:32.836260: train_loss -0.6893
2024-12-01 10:16:32.836985: val_loss -0.5051
2024-12-01 10:16:32.837593: Pseudo dice [0.7202]
2024-12-01 10:16:32.838296: Epoch time: 88.52 s
2024-12-01 10:16:32.839083: Yayy! New best EMA pseudo Dice: 0.6989
2024-12-01 10:16:34.921020: 
2024-12-01 10:16:34.922575: Epoch 62
2024-12-01 10:16:34.923546: Current learning rate: 0.00944
2024-12-01 10:18:03.391339: Validation loss did not improve from -0.50920. Patience: 38/50
2024-12-01 10:18:03.392614: train_loss -0.6952
2024-12-01 10:18:03.393506: val_loss -0.4943
2024-12-01 10:18:03.394140: Pseudo dice [0.7246]
2024-12-01 10:18:03.394868: Epoch time: 88.47 s
2024-12-01 10:18:03.395481: Yayy! New best EMA pseudo Dice: 0.7014
2024-12-01 10:18:05.025615: 
2024-12-01 10:18:05.027099: Epoch 63
2024-12-01 10:18:05.027895: Current learning rate: 0.00943
2024-12-01 10:19:33.409582: Validation loss did not improve from -0.50920. Patience: 39/50
2024-12-01 10:19:33.410325: train_loss -0.698
2024-12-01 10:19:33.411211: val_loss -0.4275
2024-12-01 10:19:33.412110: Pseudo dice [0.6693]
2024-12-01 10:19:33.412935: Epoch time: 88.39 s
2024-12-01 10:19:34.703573: 
2024-12-01 10:19:34.705107: Epoch 64
2024-12-01 10:19:34.705877: Current learning rate: 0.00942
2024-12-01 10:21:03.297190: Validation loss did not improve from -0.50920. Patience: 40/50
2024-12-01 10:21:03.297904: train_loss -0.6927
2024-12-01 10:21:03.298702: val_loss -0.469
2024-12-01 10:21:03.299299: Pseudo dice [0.6853]
2024-12-01 10:21:03.299945: Epoch time: 88.6 s
2024-12-01 10:21:04.949950: 
2024-12-01 10:21:04.951355: Epoch 65
2024-12-01 10:21:04.952060: Current learning rate: 0.00941
2024-12-01 10:22:33.406621: Validation loss did not improve from -0.50920. Patience: 41/50
2024-12-01 10:22:33.407699: train_loss -0.6993
2024-12-01 10:22:33.408572: val_loss -0.4633
2024-12-01 10:22:33.409452: Pseudo dice [0.6959]
2024-12-01 10:22:33.410236: Epoch time: 88.46 s
2024-12-01 10:22:34.658127: 
2024-12-01 10:22:34.659731: Epoch 66
2024-12-01 10:22:34.660714: Current learning rate: 0.0094
2024-12-01 10:24:03.109970: Validation loss did not improve from -0.50920. Patience: 42/50
2024-12-01 10:24:03.111223: train_loss -0.6953
2024-12-01 10:24:03.112074: val_loss -0.4245
2024-12-01 10:24:03.112863: Pseudo dice [0.6675]
2024-12-01 10:24:03.113538: Epoch time: 88.45 s
2024-12-01 10:24:04.372077: 
2024-12-01 10:24:04.373844: Epoch 67
2024-12-01 10:24:04.374587: Current learning rate: 0.00939
2024-12-01 10:25:32.847529: Validation loss did not improve from -0.50920. Patience: 43/50
2024-12-01 10:25:32.848535: train_loss -0.6909
2024-12-01 10:25:32.849380: val_loss -0.4641
2024-12-01 10:25:32.850055: Pseudo dice [0.7004]
2024-12-01 10:25:32.850823: Epoch time: 88.48 s
2024-12-01 10:25:34.127311: 
2024-12-01 10:25:34.128966: Epoch 68
2024-12-01 10:25:34.129736: Current learning rate: 0.00939
2024-12-01 10:27:02.652260: Validation loss did not improve from -0.50920. Patience: 44/50
2024-12-01 10:27:02.652951: train_loss -0.6835
2024-12-01 10:27:02.654047: val_loss -0.5073
2024-12-01 10:27:02.654994: Pseudo dice [0.7263]
2024-12-01 10:27:02.655853: Epoch time: 88.53 s
2024-12-01 10:27:03.967416: 
2024-12-01 10:27:03.968741: Epoch 69
2024-12-01 10:27:03.969495: Current learning rate: 0.00938
2024-12-01 10:28:32.311843: Validation loss did not improve from -0.50920. Patience: 45/50
2024-12-01 10:28:32.312801: train_loss -0.6903
2024-12-01 10:28:32.313909: val_loss -0.3933
2024-12-01 10:28:32.314821: Pseudo dice [0.6613]
2024-12-01 10:28:32.315746: Epoch time: 88.35 s
2024-12-01 10:28:33.978065: 
2024-12-01 10:28:33.979646: Epoch 70
2024-12-01 10:28:33.980607: Current learning rate: 0.00937
2024-12-01 10:30:02.467319: Validation loss did not improve from -0.50920. Patience: 46/50
2024-12-01 10:30:02.468412: train_loss -0.6865
2024-12-01 10:30:02.469280: val_loss -0.4757
2024-12-01 10:30:02.470200: Pseudo dice [0.6983]
2024-12-01 10:30:02.470949: Epoch time: 88.49 s
2024-12-01 10:30:03.767480: 
2024-12-01 10:30:03.768813: Epoch 71
2024-12-01 10:30:03.769655: Current learning rate: 0.00936
2024-12-01 10:31:32.508422: Validation loss did not improve from -0.50920. Patience: 47/50
2024-12-01 10:31:32.509519: train_loss -0.692
2024-12-01 10:31:32.510247: val_loss -0.4846
2024-12-01 10:31:32.511029: Pseudo dice [0.7041]
2024-12-01 10:31:32.511787: Epoch time: 88.74 s
2024-12-01 10:31:34.254971: 
2024-12-01 10:31:34.256262: Epoch 72
2024-12-01 10:31:34.257070: Current learning rate: 0.00935
2024-12-01 10:33:02.819979: Validation loss did not improve from -0.50920. Patience: 48/50
2024-12-01 10:33:02.821238: train_loss -0.6979
2024-12-01 10:33:02.822482: val_loss -0.4741
2024-12-01 10:33:02.823461: Pseudo dice [0.6986]
2024-12-01 10:33:02.824224: Epoch time: 88.57 s
2024-12-01 10:33:04.171811: 
2024-12-01 10:33:04.173531: Epoch 73
2024-12-01 10:33:04.174499: Current learning rate: 0.00934
2024-12-01 10:34:32.690582: Validation loss did not improve from -0.50920. Patience: 49/50
2024-12-01 10:34:32.691622: train_loss -0.6971
2024-12-01 10:34:32.692580: val_loss -0.4381
2024-12-01 10:34:32.693278: Pseudo dice [0.6851]
2024-12-01 10:34:32.693896: Epoch time: 88.52 s
2024-12-01 10:34:34.033632: 
2024-12-01 10:34:34.034820: Epoch 74
2024-12-01 10:34:34.035634: Current learning rate: 0.00933
2024-12-01 10:36:02.537823: Validation loss did not improve from -0.50920. Patience: 50/50
2024-12-01 10:36:02.538826: train_loss -0.695
2024-12-01 10:36:02.539680: val_loss -0.4917
2024-12-01 10:36:02.540435: Pseudo dice [0.7133]
2024-12-01 10:36:02.541114: Epoch time: 88.51 s
2024-12-01 10:36:04.264989: Patience reached. Stopping training.
2024-12-01 10:36:04.672014: Training done.
2024-12-01 10:36:04.869348: Using splits from existing split file: /home/gridsan/nchutisilp/datasets/nnUNet_Datasets/nnUNet_preprocessed/Dataset306_Sohee_Ajay_Calcium_OCT/splits_final.json
2024-12-01 10:36:04.951645: The split file contains 5 splits.
2024-12-01 10:36:04.953396: Desired fold for training: 2
2024-12-01 10:36:04.954528: This split has 10 training and 3 validation cases.
2024-12-01 10:36:04.955910: predicting 401-004
2024-12-01 10:36:04.962282: 401-004, shape torch.Size([1, 375, 498, 498]), rank 0
2024-12-01 10:37:43.658464: predicting 706-005
2024-12-01 10:37:43.682552: 706-005, shape torch.Size([1, 375, 498, 498]), rank 0
2024-12-01 10:39:10.012922: predicting 708006Pre
2024-12-01 10:39:10.037419: 708006Pre, shape torch.Size([1, 253, 498, 498]), rank 0
2024-12-01 10:40:24.564519: Validation complete
2024-12-01 10:40:24.564965: Mean Validation Dice:  0.6832816419135099
2024-12-01 08:42:57.866741: unpacking done...
2024-12-01 08:42:57.886280: Unable to plot network architecture: nnUNet_compile is enabled!
2024-12-01 08:42:57.954234: 
2024-12-01 08:42:57.955991: Epoch 0
2024-12-01 08:42:57.957146: Current learning rate: 0.01
2024-12-01 08:45:24.943954: Validation loss improved from 1000.00000 to -0.49444! Patience: 0/50
2024-12-01 08:45:24.945215: train_loss -0.2641
2024-12-01 08:45:24.946485: val_loss -0.4944
2024-12-01 08:45:24.947242: Pseudo dice [0.6963]
2024-12-01 08:45:24.948143: Epoch time: 146.99 s
2024-12-01 08:45:24.948912: Yayy! New best EMA pseudo Dice: 0.6963
2024-12-01 08:45:26.469544: 
2024-12-01 08:45:26.471065: Epoch 1
2024-12-01 08:45:26.472116: Current learning rate: 0.00999
2024-12-01 08:46:54.139616: Validation loss improved from -0.49444 to -0.56359! Patience: 0/50
2024-12-01 08:46:54.140662: train_loss -0.4378
2024-12-01 08:46:54.141545: val_loss -0.5636
2024-12-01 08:46:54.142303: Pseudo dice [0.7384]
2024-12-01 08:46:54.143033: Epoch time: 87.67 s
2024-12-01 08:46:54.143697: Yayy! New best EMA pseudo Dice: 0.7005
2024-12-01 08:46:55.772079: 
2024-12-01 08:46:55.773347: Epoch 2
2024-12-01 08:46:55.774153: Current learning rate: 0.00998
2024-12-01 08:48:23.788176: Validation loss improved from -0.56359 to -0.60101! Patience: 0/50
2024-12-01 08:48:23.789162: train_loss -0.4571
2024-12-01 08:48:23.790101: val_loss -0.601
2024-12-01 08:48:23.790703: Pseudo dice [0.7714]
2024-12-01 08:48:23.791397: Epoch time: 88.02 s
2024-12-01 08:48:23.792169: Yayy! New best EMA pseudo Dice: 0.7076
2024-12-01 08:48:25.512980: 
2024-12-01 08:48:25.514687: Epoch 3
2024-12-01 08:48:25.515632: Current learning rate: 0.00997
2024-12-01 08:49:53.728807: Validation loss did not improve from -0.60101. Patience: 1/50
2024-12-01 08:49:53.729878: train_loss -0.473
2024-12-01 08:49:53.730675: val_loss -0.571
2024-12-01 08:49:53.731492: Pseudo dice [0.7477]
2024-12-01 08:49:53.732193: Epoch time: 88.22 s
2024-12-01 08:49:53.732807: Yayy! New best EMA pseudo Dice: 0.7116
2024-12-01 08:49:55.414213: 
2024-12-01 08:49:55.415545: Epoch 4
2024-12-01 08:49:55.416259: Current learning rate: 0.00996
2024-12-01 08:51:23.564145: Validation loss did not improve from -0.60101. Patience: 2/50
2024-12-01 08:51:23.565176: train_loss -0.4974
2024-12-01 08:51:23.565940: val_loss -0.5949
2024-12-01 08:51:23.566614: Pseudo dice [0.7559]
2024-12-01 08:51:23.567248: Epoch time: 88.15 s
2024-12-01 08:51:23.935757: Yayy! New best EMA pseudo Dice: 0.7161
2024-12-01 08:51:25.666798: 
2024-12-01 08:51:25.668369: Epoch 5
2024-12-01 08:51:25.669024: Current learning rate: 0.00995
2024-12-01 08:52:53.900918: Validation loss did not improve from -0.60101. Patience: 3/50
2024-12-01 08:52:53.902181: train_loss -0.4959
2024-12-01 08:52:53.903096: val_loss -0.531
2024-12-01 08:52:53.903827: Pseudo dice [0.6933]
2024-12-01 08:52:53.904439: Epoch time: 88.24 s
2024-12-01 08:52:55.175828: 
2024-12-01 08:52:55.177078: Epoch 6
2024-12-01 08:52:55.177856: Current learning rate: 0.00995
2024-12-01 08:54:23.383028: Validation loss did not improve from -0.60101. Patience: 4/50
2024-12-01 08:54:23.384130: train_loss -0.5015
2024-12-01 08:54:23.385157: val_loss -0.5502
2024-12-01 08:54:23.385721: Pseudo dice [0.7203]
2024-12-01 08:54:23.386269: Epoch time: 88.21 s
2024-12-01 08:54:24.674370: 
2024-12-01 08:54:24.675841: Epoch 7
2024-12-01 08:54:24.676686: Current learning rate: 0.00994
2024-12-01 08:55:52.869805: Validation loss did not improve from -0.60101. Patience: 5/50
2024-12-01 08:55:52.870828: train_loss -0.5219
2024-12-01 08:55:52.871603: val_loss -0.5633
2024-12-01 08:55:52.872277: Pseudo dice [0.7282]
2024-12-01 08:55:52.872947: Epoch time: 88.2 s
2024-12-01 08:55:54.720198: 
2024-12-01 08:55:54.721740: Epoch 8
2024-12-01 08:55:54.722379: Current learning rate: 0.00993
2024-12-01 08:57:22.720039: Validation loss improved from -0.60101 to -0.63801! Patience: 5/50
2024-12-01 08:57:22.721032: train_loss -0.526
2024-12-01 08:57:22.722044: val_loss -0.638
2024-12-01 08:57:22.722647: Pseudo dice [0.7779]
2024-12-01 08:57:22.723351: Epoch time: 88.0 s
2024-12-01 08:57:22.723886: Yayy! New best EMA pseudo Dice: 0.722
2024-12-01 08:57:24.381502: 
2024-12-01 08:57:24.383004: Epoch 9
2024-12-01 08:57:24.383737: Current learning rate: 0.00992
2024-12-01 08:58:52.586337: Validation loss did not improve from -0.63801. Patience: 1/50
2024-12-01 08:58:52.587567: train_loss -0.5429
2024-12-01 08:58:52.588637: val_loss -0.5709
2024-12-01 08:58:52.589487: Pseudo dice [0.725]
2024-12-01 08:58:52.590216: Epoch time: 88.21 s
2024-12-01 08:58:52.964302: Yayy! New best EMA pseudo Dice: 0.7223
2024-12-01 08:58:54.581703: 
2024-12-01 08:58:54.583213: Epoch 10
2024-12-01 08:58:54.584162: Current learning rate: 0.00991
2024-12-01 09:00:23.003444: Validation loss did not improve from -0.63801. Patience: 2/50
2024-12-01 09:00:23.004556: train_loss -0.5313
2024-12-01 09:00:23.005431: val_loss -0.614
2024-12-01 09:00:23.006076: Pseudo dice [0.7703]
2024-12-01 09:00:23.006667: Epoch time: 88.42 s
2024-12-01 09:00:23.007233: Yayy! New best EMA pseudo Dice: 0.7271
2024-12-01 09:00:24.578576: 
2024-12-01 09:00:24.580097: Epoch 11
2024-12-01 09:00:24.580902: Current learning rate: 0.0099
2024-12-01 09:01:53.076607: Validation loss did not improve from -0.63801. Patience: 3/50
2024-12-01 09:01:53.077864: train_loss -0.5484
2024-12-01 09:01:53.078907: val_loss -0.6156
2024-12-01 09:01:53.079612: Pseudo dice [0.768]
2024-12-01 09:01:53.080354: Epoch time: 88.5 s
2024-12-01 09:01:53.081064: Yayy! New best EMA pseudo Dice: 0.7312
2024-12-01 09:01:54.700473: 
2024-12-01 09:01:54.701802: Epoch 12
2024-12-01 09:01:54.702699: Current learning rate: 0.00989
2024-12-01 09:03:23.203535: Validation loss improved from -0.63801 to -0.65148! Patience: 3/50
2024-12-01 09:03:23.204567: train_loss -0.5432
2024-12-01 09:03:23.205415: val_loss -0.6515
2024-12-01 09:03:23.206017: Pseudo dice [0.7872]
2024-12-01 09:03:23.206594: Epoch time: 88.5 s
2024-12-01 09:03:23.207294: Yayy! New best EMA pseudo Dice: 0.7368
2024-12-01 09:03:24.811042: 
2024-12-01 09:03:24.812416: Epoch 13
2024-12-01 09:03:24.813145: Current learning rate: 0.00988
2024-12-01 09:04:53.234148: Validation loss did not improve from -0.65148. Patience: 1/50
2024-12-01 09:04:53.235260: train_loss -0.557
2024-12-01 09:04:53.235972: val_loss -0.6316
2024-12-01 09:04:53.236626: Pseudo dice [0.7716]
2024-12-01 09:04:53.237383: Epoch time: 88.43 s
2024-12-01 09:04:53.237988: Yayy! New best EMA pseudo Dice: 0.7403
2024-12-01 09:04:54.907768: 
2024-12-01 09:04:54.909407: Epoch 14
2024-12-01 09:04:54.910344: Current learning rate: 0.00987
2024-12-01 09:06:23.766429: Validation loss did not improve from -0.65148. Patience: 2/50
2024-12-01 09:06:23.767548: train_loss -0.5531
2024-12-01 09:06:23.768318: val_loss -0.5349
2024-12-01 09:06:23.768892: Pseudo dice [0.7119]
2024-12-01 09:06:23.769587: Epoch time: 88.86 s
2024-12-01 09:06:25.482780: 
2024-12-01 09:06:25.484538: Epoch 15
2024-12-01 09:06:25.485304: Current learning rate: 0.00986
2024-12-01 09:07:53.960099: Validation loss did not improve from -0.65148. Patience: 3/50
2024-12-01 09:07:53.961024: train_loss -0.5713
2024-12-01 09:07:53.961803: val_loss -0.6449
2024-12-01 09:07:53.962462: Pseudo dice [0.7851]
2024-12-01 09:07:53.963159: Epoch time: 88.48 s
2024-12-01 09:07:53.963903: Yayy! New best EMA pseudo Dice: 0.7422
2024-12-01 09:07:55.701069: 
2024-12-01 09:07:55.702724: Epoch 16
2024-12-01 09:07:55.703538: Current learning rate: 0.00986
2024-12-01 09:09:24.101831: Validation loss did not improve from -0.65148. Patience: 4/50
2024-12-01 09:09:24.103166: train_loss -0.5724
2024-12-01 09:09:24.104136: val_loss -0.5812
2024-12-01 09:09:24.104920: Pseudo dice [0.7475]
2024-12-01 09:09:24.105710: Epoch time: 88.4 s
2024-12-01 09:09:24.106488: Yayy! New best EMA pseudo Dice: 0.7427
2024-12-01 09:09:25.885378: 
2024-12-01 09:09:25.886750: Epoch 17
2024-12-01 09:09:25.887737: Current learning rate: 0.00985
2024-12-01 09:10:54.607168: Validation loss improved from -0.65148 to -0.66242! Patience: 4/50
2024-12-01 09:10:54.608177: train_loss -0.5647
2024-12-01 09:10:54.608868: val_loss -0.6624
2024-12-01 09:10:54.609466: Pseudo dice [0.7988]
2024-12-01 09:10:54.610261: Epoch time: 88.72 s
2024-12-01 09:10:54.610908: Yayy! New best EMA pseudo Dice: 0.7484
2024-12-01 09:10:56.310959: 
2024-12-01 09:10:56.312215: Epoch 18
2024-12-01 09:10:56.312934: Current learning rate: 0.00984
2024-12-01 09:12:25.146185: Validation loss did not improve from -0.66242. Patience: 1/50
2024-12-01 09:12:25.146966: train_loss -0.5752
2024-12-01 09:12:25.147682: val_loss -0.6194
2024-12-01 09:12:25.148407: Pseudo dice [0.7561]
2024-12-01 09:12:25.149019: Epoch time: 88.84 s
2024-12-01 09:12:25.149705: Yayy! New best EMA pseudo Dice: 0.7491
2024-12-01 09:12:27.263339: 
2024-12-01 09:12:27.265043: Epoch 19
2024-12-01 09:12:27.265780: Current learning rate: 0.00983
2024-12-01 09:13:56.040738: Validation loss did not improve from -0.66242. Patience: 2/50
2024-12-01 09:13:56.041703: train_loss -0.5842
2024-12-01 09:13:56.042456: val_loss -0.5739
2024-12-01 09:13:56.043137: Pseudo dice [0.7299]
2024-12-01 09:13:56.043775: Epoch time: 88.78 s
2024-12-01 09:13:57.869817: 
2024-12-01 09:13:57.871328: Epoch 20
2024-12-01 09:13:57.872074: Current learning rate: 0.00982
2024-12-01 09:15:26.807648: Validation loss did not improve from -0.66242. Patience: 3/50
2024-12-01 09:15:26.808797: train_loss -0.5801
2024-12-01 09:15:26.809512: val_loss -0.6291
2024-12-01 09:15:26.810213: Pseudo dice [0.7763]
2024-12-01 09:15:26.810864: Epoch time: 88.94 s
2024-12-01 09:15:26.811442: Yayy! New best EMA pseudo Dice: 0.7501
2024-12-01 09:15:28.512741: 
2024-12-01 09:15:28.513972: Epoch 21
2024-12-01 09:15:28.514556: Current learning rate: 0.00981
2024-12-01 09:16:57.366619: Validation loss did not improve from -0.66242. Patience: 4/50
2024-12-01 09:16:57.367628: train_loss -0.5823
2024-12-01 09:16:57.368345: val_loss -0.5904
2024-12-01 09:16:57.368966: Pseudo dice [0.74]
2024-12-01 09:16:57.369669: Epoch time: 88.86 s
2024-12-01 09:16:58.633009: 
2024-12-01 09:16:58.634618: Epoch 22
2024-12-01 09:16:58.635343: Current learning rate: 0.0098
2024-12-01 09:18:26.345663: Validation loss did not improve from -0.66242. Patience: 5/50
2024-12-01 09:18:26.346418: train_loss -0.5742
2024-12-01 09:18:26.347185: val_loss -0.5821
2024-12-01 09:18:26.347838: Pseudo dice [0.7439]
2024-12-01 09:18:26.348446: Epoch time: 87.71 s
2024-12-01 09:18:27.662658: 
2024-12-01 09:18:27.663990: Epoch 23
2024-12-01 09:18:27.664625: Current learning rate: 0.00979
2024-12-01 09:19:55.397049: Validation loss did not improve from -0.66242. Patience: 6/50
2024-12-01 09:19:55.398170: train_loss -0.5843
2024-12-01 09:19:55.399050: val_loss -0.6423
2024-12-01 09:19:55.399631: Pseudo dice [0.7863]
2024-12-01 09:19:55.400360: Epoch time: 87.74 s
2024-12-01 09:19:55.400974: Yayy! New best EMA pseudo Dice: 0.7524
2024-12-01 09:19:57.033981: 
2024-12-01 09:19:57.035370: Epoch 24
2024-12-01 09:19:57.036140: Current learning rate: 0.00978
2024-12-01 09:21:25.315253: Validation loss did not improve from -0.66242. Patience: 7/50
2024-12-01 09:21:25.316195: train_loss -0.5906
2024-12-01 09:21:25.316848: val_loss -0.571
2024-12-01 09:21:25.317425: Pseudo dice [0.7337]
2024-12-01 09:21:25.318032: Epoch time: 88.28 s
2024-12-01 09:21:26.984675: 
2024-12-01 09:21:26.986238: Epoch 25
2024-12-01 09:21:26.986883: Current learning rate: 0.00977
2024-12-01 09:22:54.946689: Validation loss did not improve from -0.66242. Patience: 8/50
2024-12-01 09:22:54.947857: train_loss -0.5977
2024-12-01 09:22:54.948810: val_loss -0.5484
2024-12-01 09:22:54.949511: Pseudo dice [0.6951]
2024-12-01 09:22:54.950186: Epoch time: 87.96 s
2024-12-01 09:22:56.269754: 
2024-12-01 09:22:56.270839: Epoch 26
2024-12-01 09:22:56.271652: Current learning rate: 0.00977
2024-12-01 09:24:24.409608: Validation loss improved from -0.66242 to -0.66561! Patience: 8/50
2024-12-01 09:24:24.410985: train_loss -0.6
2024-12-01 09:24:24.412025: val_loss -0.6656
2024-12-01 09:24:24.412949: Pseudo dice [0.799]
2024-12-01 09:24:24.413878: Epoch time: 88.14 s
2024-12-01 09:24:25.670857: 
2024-12-01 09:24:25.672752: Epoch 27
2024-12-01 09:24:25.673683: Current learning rate: 0.00976
2024-12-01 09:25:53.665737: Validation loss did not improve from -0.66561. Patience: 1/50
2024-12-01 09:25:53.666806: train_loss -0.6088
2024-12-01 09:25:53.667610: val_loss -0.6012
2024-12-01 09:25:53.668348: Pseudo dice [0.7575]
2024-12-01 09:25:53.668973: Epoch time: 88.0 s
2024-12-01 09:25:54.942692: 
2024-12-01 09:25:54.944427: Epoch 28
2024-12-01 09:25:54.945093: Current learning rate: 0.00975
2024-12-01 09:27:23.033563: Validation loss did not improve from -0.66561. Patience: 2/50
2024-12-01 09:27:23.034208: train_loss -0.5987
2024-12-01 09:27:23.034896: val_loss -0.604
2024-12-01 09:27:23.035595: Pseudo dice [0.7622]
2024-12-01 09:27:23.036230: Epoch time: 88.09 s
2024-12-01 09:27:24.780067: 
2024-12-01 09:27:24.781585: Epoch 29
2024-12-01 09:27:24.782439: Current learning rate: 0.00974
2024-12-01 09:28:52.969044: Validation loss did not improve from -0.66561. Patience: 3/50
2024-12-01 09:28:52.970169: train_loss -0.6084
2024-12-01 09:28:52.971082: val_loss -0.6003
2024-12-01 09:28:52.971725: Pseudo dice [0.751]
2024-12-01 09:28:52.972862: Epoch time: 88.19 s
2024-12-01 09:28:54.680616: 
2024-12-01 09:28:54.682016: Epoch 30
2024-12-01 09:28:54.682706: Current learning rate: 0.00973
2024-12-01 09:30:22.800748: Validation loss did not improve from -0.66561. Patience: 4/50
2024-12-01 09:30:22.801745: train_loss -0.6195
2024-12-01 09:30:22.802628: val_loss -0.6347
2024-12-01 09:30:22.803306: Pseudo dice [0.7858]
2024-12-01 09:30:22.804030: Epoch time: 88.12 s
2024-12-01 09:30:22.804700: Yayy! New best EMA pseudo Dice: 0.7554
2024-12-01 09:30:24.502404: 
2024-12-01 09:30:24.503710: Epoch 31
2024-12-01 09:30:24.504499: Current learning rate: 0.00972
2024-12-01 09:31:52.640086: Validation loss did not improve from -0.66561. Patience: 5/50
2024-12-01 09:31:52.641272: train_loss -0.6111
2024-12-01 09:31:52.642000: val_loss -0.6362
2024-12-01 09:31:52.642686: Pseudo dice [0.7764]
2024-12-01 09:31:52.643317: Epoch time: 88.14 s
2024-12-01 09:31:52.643862: Yayy! New best EMA pseudo Dice: 0.7575
2024-12-01 09:31:54.329687: 
2024-12-01 09:31:54.331180: Epoch 32
2024-12-01 09:31:54.331779: Current learning rate: 0.00971
2024-12-01 09:33:22.724381: Validation loss did not improve from -0.66561. Patience: 6/50
2024-12-01 09:33:22.725530: train_loss -0.6136
2024-12-01 09:33:22.726426: val_loss -0.6355
2024-12-01 09:33:22.727090: Pseudo dice [0.7734]
2024-12-01 09:33:22.727816: Epoch time: 88.4 s
2024-12-01 09:33:22.728427: Yayy! New best EMA pseudo Dice: 0.7591
2024-12-01 09:33:24.414705: 
2024-12-01 09:33:24.416217: Epoch 33
2024-12-01 09:33:24.416975: Current learning rate: 0.0097
2024-12-01 09:34:52.755477: Validation loss did not improve from -0.66561. Patience: 7/50
2024-12-01 09:34:52.756519: train_loss -0.6208
2024-12-01 09:34:52.757334: val_loss -0.61
2024-12-01 09:34:52.757957: Pseudo dice [0.7679]
2024-12-01 09:34:52.758553: Epoch time: 88.34 s
2024-12-01 09:34:52.759246: Yayy! New best EMA pseudo Dice: 0.76
2024-12-01 09:34:54.458587: 
2024-12-01 09:34:54.460279: Epoch 34
2024-12-01 09:34:54.461282: Current learning rate: 0.00969
2024-12-01 09:36:22.914518: Validation loss did not improve from -0.66561. Patience: 8/50
2024-12-01 09:36:22.915648: train_loss -0.618
2024-12-01 09:36:22.916405: val_loss -0.6054
2024-12-01 09:36:22.916908: Pseudo dice [0.7609]
2024-12-01 09:36:22.917428: Epoch time: 88.46 s
2024-12-01 09:36:23.305586: Yayy! New best EMA pseudo Dice: 0.7601
2024-12-01 09:36:25.017463: 
2024-12-01 09:36:25.019060: Epoch 35
2024-12-01 09:36:25.019804: Current learning rate: 0.00968
2024-12-01 09:37:53.479346: Validation loss improved from -0.66561 to -0.66575! Patience: 8/50
2024-12-01 09:37:53.480395: train_loss -0.6273
2024-12-01 09:37:53.481188: val_loss -0.6658
2024-12-01 09:37:53.481950: Pseudo dice [0.8074]
2024-12-01 09:37:53.482580: Epoch time: 88.46 s
2024-12-01 09:37:53.483144: Yayy! New best EMA pseudo Dice: 0.7648
2024-12-01 09:37:55.208063: 
2024-12-01 09:37:55.209809: Epoch 36
2024-12-01 09:37:55.210834: Current learning rate: 0.00968
2024-12-01 09:39:23.704888: Validation loss did not improve from -0.66575. Patience: 1/50
2024-12-01 09:39:23.705937: train_loss -0.617
2024-12-01 09:39:23.706802: val_loss -0.5729
2024-12-01 09:39:23.707420: Pseudo dice [0.7364]
2024-12-01 09:39:23.708033: Epoch time: 88.5 s
2024-12-01 09:39:25.052172: 
2024-12-01 09:39:25.053775: Epoch 37
2024-12-01 09:39:25.054611: Current learning rate: 0.00967
2024-12-01 09:40:53.410831: Validation loss did not improve from -0.66575. Patience: 2/50
2024-12-01 09:40:53.412248: train_loss -0.6223
2024-12-01 09:40:53.413097: val_loss -0.6536
2024-12-01 09:40:53.413682: Pseudo dice [0.7948]
2024-12-01 09:40:53.414274: Epoch time: 88.36 s
2024-12-01 09:40:53.414806: Yayy! New best EMA pseudo Dice: 0.7652
2024-12-01 09:40:55.158532: 
2024-12-01 09:40:55.160533: Epoch 38
2024-12-01 09:40:55.161588: Current learning rate: 0.00966
2024-12-01 09:42:23.687990: Validation loss improved from -0.66575 to -0.66853! Patience: 2/50
2024-12-01 09:42:23.689030: train_loss -0.619
2024-12-01 09:42:23.689750: val_loss -0.6685
2024-12-01 09:42:23.690430: Pseudo dice [0.8039]
2024-12-01 09:42:23.691028: Epoch time: 88.53 s
2024-12-01 09:42:23.691721: Yayy! New best EMA pseudo Dice: 0.7691
2024-12-01 09:42:25.394582: 
2024-12-01 09:42:25.396086: Epoch 39
2024-12-01 09:42:25.396837: Current learning rate: 0.00965
2024-12-01 09:43:54.076356: Validation loss did not improve from -0.66853. Patience: 1/50
2024-12-01 09:43:54.077430: train_loss -0.6278
2024-12-01 09:43:54.078564: val_loss -0.6474
2024-12-01 09:43:54.079178: Pseudo dice [0.7879]
2024-12-01 09:43:54.079855: Epoch time: 88.68 s
2024-12-01 09:43:54.460579: Yayy! New best EMA pseudo Dice: 0.771
2024-12-01 09:43:56.259237: 
2024-12-01 09:43:56.260698: Epoch 40
2024-12-01 09:43:56.261324: Current learning rate: 0.00964
2024-12-01 09:45:24.705271: Validation loss improved from -0.66853 to -0.67192! Patience: 1/50
2024-12-01 09:45:24.705977: train_loss -0.6302
2024-12-01 09:45:24.706638: val_loss -0.6719
2024-12-01 09:45:24.707461: Pseudo dice [0.8019]
2024-12-01 09:45:24.708223: Epoch time: 88.45 s
2024-12-01 09:45:24.708916: Yayy! New best EMA pseudo Dice: 0.7741
2024-12-01 09:45:26.415240: 
2024-12-01 09:45:26.416496: Epoch 41
2024-12-01 09:45:26.417155: Current learning rate: 0.00963
2024-12-01 09:46:54.987968: Validation loss did not improve from -0.67192. Patience: 1/50
2024-12-01 09:46:54.989065: train_loss -0.6298
2024-12-01 09:46:54.989851: val_loss -0.5986
2024-12-01 09:46:54.990532: Pseudo dice [0.757]
2024-12-01 09:46:54.991251: Epoch time: 88.57 s
2024-12-01 09:46:56.308435: 
2024-12-01 09:46:56.309869: Epoch 42
2024-12-01 09:46:56.310592: Current learning rate: 0.00962
2024-12-01 09:48:24.797190: Validation loss did not improve from -0.67192. Patience: 2/50
2024-12-01 09:48:24.798254: train_loss -0.639
2024-12-01 09:48:24.799012: val_loss -0.6484
2024-12-01 09:48:24.799854: Pseudo dice [0.7888]
2024-12-01 09:48:24.800498: Epoch time: 88.49 s
2024-12-01 09:48:26.096729: 
2024-12-01 09:48:26.098365: Epoch 43
2024-12-01 09:48:26.099118: Current learning rate: 0.00961
2024-12-01 09:49:54.625246: Validation loss did not improve from -0.67192. Patience: 3/50
2024-12-01 09:49:54.625906: train_loss -0.6261
2024-12-01 09:49:54.626572: val_loss -0.6556
2024-12-01 09:49:54.627238: Pseudo dice [0.7963]
2024-12-01 09:49:54.627867: Epoch time: 88.53 s
2024-12-01 09:49:54.628534: Yayy! New best EMA pseudo Dice: 0.7762
2024-12-01 09:49:56.283429: 
2024-12-01 09:49:56.284915: Epoch 44
2024-12-01 09:49:56.285686: Current learning rate: 0.0096
2024-12-01 09:51:24.700577: Validation loss did not improve from -0.67192. Patience: 4/50
2024-12-01 09:51:24.701683: train_loss -0.6352
2024-12-01 09:51:24.702648: val_loss -0.6519
2024-12-01 09:51:24.703479: Pseudo dice [0.7914]
2024-12-01 09:51:24.704123: Epoch time: 88.42 s
2024-12-01 09:51:25.079292: Yayy! New best EMA pseudo Dice: 0.7778
2024-12-01 09:51:26.729115: 
2024-12-01 09:51:26.730481: Epoch 45
2024-12-01 09:51:26.731191: Current learning rate: 0.00959
2024-12-01 09:52:55.121651: Validation loss did not improve from -0.67192. Patience: 5/50
2024-12-01 09:52:55.122705: train_loss -0.6256
2024-12-01 09:52:55.123615: val_loss -0.6368
2024-12-01 09:52:55.124347: Pseudo dice [0.7772]
2024-12-01 09:52:55.124988: Epoch time: 88.39 s
2024-12-01 09:52:56.454821: 
2024-12-01 09:52:56.456233: Epoch 46
2024-12-01 09:52:56.456945: Current learning rate: 0.00959
2024-12-01 09:54:25.000049: Validation loss improved from -0.67192 to -0.67576! Patience: 5/50
2024-12-01 09:54:25.001296: train_loss -0.6451
2024-12-01 09:54:25.002260: val_loss -0.6758
2024-12-01 09:54:25.003040: Pseudo dice [0.8116]
2024-12-01 09:54:25.003756: Epoch time: 88.55 s
2024-12-01 09:54:25.004774: Yayy! New best EMA pseudo Dice: 0.7811
2024-12-01 09:54:26.652163: 
2024-12-01 09:54:26.653798: Epoch 47
2024-12-01 09:54:26.654716: Current learning rate: 0.00958
2024-12-01 09:55:55.101870: Validation loss did not improve from -0.67576. Patience: 1/50
2024-12-01 09:55:55.103067: train_loss -0.6349
2024-12-01 09:55:55.103804: val_loss -0.5829
2024-12-01 09:55:55.104456: Pseudo dice [0.7492]
2024-12-01 09:55:55.105050: Epoch time: 88.45 s
2024-12-01 09:55:56.378925: 
2024-12-01 09:55:56.380327: Epoch 48
2024-12-01 09:55:56.380999: Current learning rate: 0.00957
2024-12-01 09:57:24.764704: Validation loss did not improve from -0.67576. Patience: 2/50
2024-12-01 09:57:24.765883: train_loss -0.6404
2024-12-01 09:57:24.766671: val_loss -0.6662
2024-12-01 09:57:24.767344: Pseudo dice [0.7947]
2024-12-01 09:57:24.767994: Epoch time: 88.39 s
2024-12-01 09:57:26.076992: 
2024-12-01 09:57:26.078387: Epoch 49
2024-12-01 09:57:26.079118: Current learning rate: 0.00956
2024-12-01 09:58:54.519312: Validation loss did not improve from -0.67576. Patience: 3/50
2024-12-01 09:58:54.520236: train_loss -0.6338
2024-12-01 09:58:54.521021: val_loss -0.6228
2024-12-01 09:58:54.521643: Pseudo dice [0.7778]
2024-12-01 09:58:54.522202: Epoch time: 88.44 s
2024-12-01 09:58:56.549974: 
2024-12-01 09:58:56.551441: Epoch 50
2024-12-01 09:58:56.552168: Current learning rate: 0.00955
2024-12-01 10:00:25.227141: Validation loss did not improve from -0.67576. Patience: 4/50
2024-12-01 10:00:25.228354: train_loss -0.6412
2024-12-01 10:00:25.229336: val_loss -0.6638
2024-12-01 10:00:25.230247: Pseudo dice [0.8034]
2024-12-01 10:00:25.231129: Epoch time: 88.68 s
2024-12-01 10:00:25.231956: Yayy! New best EMA pseudo Dice: 0.7818
2024-12-01 10:00:26.932949: 
2024-12-01 10:00:26.934310: Epoch 51
2024-12-01 10:00:26.935119: Current learning rate: 0.00954
2024-12-01 10:01:55.589128: Validation loss did not improve from -0.67576. Patience: 5/50
2024-12-01 10:01:55.590100: train_loss -0.6491
2024-12-01 10:01:55.591052: val_loss -0.5872
2024-12-01 10:01:55.591781: Pseudo dice [0.7332]
2024-12-01 10:01:55.592604: Epoch time: 88.66 s
2024-12-01 10:01:56.924035: 
2024-12-01 10:01:56.925421: Epoch 52
2024-12-01 10:01:56.926192: Current learning rate: 0.00953
2024-12-01 10:03:25.351842: Validation loss did not improve from -0.67576. Patience: 6/50
2024-12-01 10:03:25.352904: train_loss -0.6483
2024-12-01 10:03:25.353760: val_loss -0.6578
2024-12-01 10:03:25.354373: Pseudo dice [0.7944]
2024-12-01 10:03:25.355188: Epoch time: 88.43 s
2024-12-01 10:03:26.603071: 
2024-12-01 10:03:26.604651: Epoch 53
2024-12-01 10:03:26.605390: Current learning rate: 0.00952
2024-12-01 10:04:55.197758: Validation loss did not improve from -0.67576. Patience: 7/50
2024-12-01 10:04:55.198965: train_loss -0.6453
2024-12-01 10:04:55.199760: val_loss -0.6423
2024-12-01 10:04:55.200490: Pseudo dice [0.7876]
2024-12-01 10:04:55.201141: Epoch time: 88.6 s
2024-12-01 10:04:56.492583: 
2024-12-01 10:04:56.494014: Epoch 54
2024-12-01 10:04:56.494665: Current learning rate: 0.00951
2024-12-01 10:06:25.027205: Validation loss did not improve from -0.67576. Patience: 8/50
2024-12-01 10:06:25.028111: train_loss -0.6506
2024-12-01 10:06:25.028790: val_loss -0.6296
2024-12-01 10:06:25.029390: Pseudo dice [0.7743]
2024-12-01 10:06:25.029938: Epoch time: 88.54 s
2024-12-01 10:06:26.682171: 
2024-12-01 10:06:26.683710: Epoch 55
2024-12-01 10:06:26.684476: Current learning rate: 0.0095
2024-12-01 10:07:55.356973: Validation loss did not improve from -0.67576. Patience: 9/50
2024-12-01 10:07:55.357944: train_loss -0.6532
2024-12-01 10:07:55.358838: val_loss -0.6009
2024-12-01 10:07:55.359568: Pseudo dice [0.7605]
2024-12-01 10:07:55.360276: Epoch time: 88.68 s
2024-12-01 10:07:56.695661: 
2024-12-01 10:07:56.697016: Epoch 56
2024-12-01 10:07:56.698015: Current learning rate: 0.00949
2024-12-01 10:09:25.253396: Validation loss did not improve from -0.67576. Patience: 10/50
2024-12-01 10:09:25.254410: train_loss -0.654
2024-12-01 10:09:25.255255: val_loss -0.6423
2024-12-01 10:09:25.255901: Pseudo dice [0.7903]
2024-12-01 10:09:25.256494: Epoch time: 88.56 s
2024-12-01 10:09:26.567197: 
2024-12-01 10:09:26.568896: Epoch 57
2024-12-01 10:09:26.569588: Current learning rate: 0.00949
2024-12-01 10:10:55.026192: Validation loss did not improve from -0.67576. Patience: 11/50
2024-12-01 10:10:55.026998: train_loss -0.6515
2024-12-01 10:10:55.027633: val_loss -0.6345
2024-12-01 10:10:55.028221: Pseudo dice [0.7878]
2024-12-01 10:10:55.028769: Epoch time: 88.46 s
2024-12-01 10:10:56.342865: 
2024-12-01 10:10:56.344152: Epoch 58
2024-12-01 10:10:56.344816: Current learning rate: 0.00948
2024-12-01 10:12:24.769615: Validation loss improved from -0.67576 to -0.69011! Patience: 11/50
2024-12-01 10:12:24.770831: train_loss -0.6442
2024-12-01 10:12:24.771679: val_loss -0.6901
2024-12-01 10:12:24.772399: Pseudo dice [0.8176]
2024-12-01 10:12:24.773034: Epoch time: 88.43 s
2024-12-01 10:12:24.773680: Yayy! New best EMA pseudo Dice: 0.7833
2024-12-01 10:12:26.433713: 
2024-12-01 10:12:26.435825: Epoch 59
2024-12-01 10:12:26.436538: Current learning rate: 0.00947
2024-12-01 10:13:54.884010: Validation loss did not improve from -0.69011. Patience: 1/50
2024-12-01 10:13:54.885111: train_loss -0.6524
2024-12-01 10:13:54.885975: val_loss -0.6661
2024-12-01 10:13:54.886680: Pseudo dice [0.803]
2024-12-01 10:13:54.887373: Epoch time: 88.45 s
2024-12-01 10:13:55.301249: Yayy! New best EMA pseudo Dice: 0.7852
2024-12-01 10:13:57.039526: 
2024-12-01 10:13:57.041128: Epoch 60
2024-12-01 10:13:57.042222: Current learning rate: 0.00946
2024-12-01 10:15:25.499591: Validation loss did not improve from -0.69011. Patience: 2/50
2024-12-01 10:15:25.500901: train_loss -0.6526
2024-12-01 10:15:25.502040: val_loss -0.6718
2024-12-01 10:15:25.502842: Pseudo dice [0.8032]
2024-12-01 10:15:25.503600: Epoch time: 88.46 s
2024-12-01 10:15:25.504344: Yayy! New best EMA pseudo Dice: 0.787
2024-12-01 10:15:27.216851: 
2024-12-01 10:15:27.218596: Epoch 61
2024-12-01 10:15:27.219548: Current learning rate: 0.00945
2024-12-01 10:16:55.609992: Validation loss did not improve from -0.69011. Patience: 3/50
2024-12-01 10:16:55.610975: train_loss -0.6587
2024-12-01 10:16:55.611694: val_loss -0.6727
2024-12-01 10:16:55.612343: Pseudo dice [0.8037]
2024-12-01 10:16:55.612961: Epoch time: 88.4 s
2024-12-01 10:16:55.613478: Yayy! New best EMA pseudo Dice: 0.7887
2024-12-01 10:16:57.742947: 
2024-12-01 10:16:57.744395: Epoch 62
2024-12-01 10:16:57.745092: Current learning rate: 0.00944
2024-12-01 10:18:26.108442: Validation loss improved from -0.69011 to -0.71251! Patience: 3/50
2024-12-01 10:18:26.109738: train_loss -0.6651
2024-12-01 10:18:26.110521: val_loss -0.7125
2024-12-01 10:18:26.111284: Pseudo dice [0.8277]
2024-12-01 10:18:26.111856: Epoch time: 88.37 s
2024-12-01 10:18:26.112468: Yayy! New best EMA pseudo Dice: 0.7926
2024-12-01 10:18:27.845687: 
2024-12-01 10:18:27.847015: Epoch 63
2024-12-01 10:18:27.847735: Current learning rate: 0.00943
2024-12-01 10:19:56.097987: Validation loss did not improve from -0.71251. Patience: 1/50
2024-12-01 10:19:56.098751: train_loss -0.6615
2024-12-01 10:19:56.099515: val_loss -0.6311
2024-12-01 10:19:56.100226: Pseudo dice [0.7748]
2024-12-01 10:19:56.100934: Epoch time: 88.25 s
2024-12-01 10:19:57.404377: 
2024-12-01 10:19:57.405703: Epoch 64
2024-12-01 10:19:57.406459: Current learning rate: 0.00942
2024-12-01 10:21:25.739872: Validation loss did not improve from -0.71251. Patience: 2/50
2024-12-01 10:21:25.740937: train_loss -0.6561
2024-12-01 10:21:25.741717: val_loss -0.6816
2024-12-01 10:21:25.742345: Pseudo dice [0.8072]
2024-12-01 10:21:25.742940: Epoch time: 88.34 s
2024-12-01 10:21:27.480437: 
2024-12-01 10:21:27.482203: Epoch 65
2024-12-01 10:21:27.483073: Current learning rate: 0.00941
2024-12-01 10:22:55.716714: Validation loss did not improve from -0.71251. Patience: 3/50
2024-12-01 10:22:55.717900: train_loss -0.6591
2024-12-01 10:22:55.718635: val_loss -0.679
2024-12-01 10:22:55.719273: Pseudo dice [0.8084]
2024-12-01 10:22:55.719979: Epoch time: 88.24 s
2024-12-01 10:22:55.720820: Yayy! New best EMA pseudo Dice: 0.7941
2024-12-01 10:22:57.414000: 
2024-12-01 10:22:57.415501: Epoch 66
2024-12-01 10:22:57.416259: Current learning rate: 0.0094
2024-12-01 10:24:25.591426: Validation loss did not improve from -0.71251. Patience: 4/50
2024-12-01 10:24:25.592576: train_loss -0.6573
2024-12-01 10:24:25.593518: val_loss -0.661
2024-12-01 10:24:25.594124: Pseudo dice [0.799]
2024-12-01 10:24:25.594878: Epoch time: 88.18 s
2024-12-01 10:24:25.595455: Yayy! New best EMA pseudo Dice: 0.7945
2024-12-01 10:24:27.288915: 
2024-12-01 10:24:27.290400: Epoch 67
2024-12-01 10:24:27.291106: Current learning rate: 0.00939
2024-12-01 10:25:55.432951: Validation loss did not improve from -0.71251. Patience: 5/50
2024-12-01 10:25:55.433633: train_loss -0.6656
2024-12-01 10:25:55.434225: val_loss -0.6352
2024-12-01 10:25:55.434789: Pseudo dice [0.7824]
2024-12-01 10:25:55.435445: Epoch time: 88.15 s
2024-12-01 10:25:56.744781: 
2024-12-01 10:25:56.746133: Epoch 68
2024-12-01 10:25:56.746817: Current learning rate: 0.00939
2024-12-01 10:27:25.012719: Validation loss did not improve from -0.71251. Patience: 6/50
2024-12-01 10:27:25.013686: train_loss -0.6554
2024-12-01 10:27:25.014705: val_loss -0.6071
2024-12-01 10:27:25.015462: Pseudo dice [0.7603]
2024-12-01 10:27:25.016142: Epoch time: 88.27 s
2024-12-01 10:27:26.326101: 
2024-12-01 10:27:26.327404: Epoch 69
2024-12-01 10:27:26.328311: Current learning rate: 0.00938
2024-12-01 10:28:54.613019: Validation loss did not improve from -0.71251. Patience: 7/50
2024-12-01 10:28:54.614185: train_loss -0.6701
2024-12-01 10:28:54.615125: val_loss -0.6822
2024-12-01 10:28:54.615861: Pseudo dice [0.806]
2024-12-01 10:28:54.616445: Epoch time: 88.29 s
2024-12-01 10:28:56.310450: 
2024-12-01 10:28:56.311934: Epoch 70
2024-12-01 10:28:56.312765: Current learning rate: 0.00937
2024-12-01 10:30:24.618634: Validation loss did not improve from -0.71251. Patience: 8/50
2024-12-01 10:30:24.619716: train_loss -0.6642
2024-12-01 10:30:24.620662: val_loss -0.6619
2024-12-01 10:30:24.621605: Pseudo dice [0.7976]
2024-12-01 10:30:24.622450: Epoch time: 88.31 s
2024-12-01 10:30:25.916786: 
2024-12-01 10:30:25.919049: Epoch 71
2024-12-01 10:30:25.920070: Current learning rate: 0.00936
2024-12-01 10:31:54.401029: Validation loss did not improve from -0.71251. Patience: 9/50
2024-12-01 10:31:54.401927: train_loss -0.6692
2024-12-01 10:31:54.402993: val_loss -0.6387
2024-12-01 10:31:54.403605: Pseudo dice [0.7816]
2024-12-01 10:31:54.404294: Epoch time: 88.49 s
2024-12-01 10:31:56.215740: 
2024-12-01 10:31:56.216636: Epoch 72
2024-12-01 10:31:56.217383: Current learning rate: 0.00935
2024-12-01 10:33:24.725609: Validation loss did not improve from -0.71251. Patience: 10/50
2024-12-01 10:33:24.726751: train_loss -0.6757
2024-12-01 10:33:24.727634: val_loss -0.596
2024-12-01 10:33:24.728264: Pseudo dice [0.7459]
2024-12-01 10:33:24.728889: Epoch time: 88.51 s
2024-12-01 10:33:26.034098: 
2024-12-01 10:33:26.035920: Epoch 73
2024-12-01 10:33:26.036702: Current learning rate: 0.00934
2024-12-01 10:34:54.494191: Validation loss did not improve from -0.71251. Patience: 11/50
2024-12-01 10:34:54.495361: train_loss -0.669
2024-12-01 10:34:54.496375: val_loss -0.6222
2024-12-01 10:34:54.497050: Pseudo dice [0.7732]
2024-12-01 10:34:54.497640: Epoch time: 88.46 s
2024-12-01 10:34:55.803099: 
2024-12-01 10:34:55.804652: Epoch 74
2024-12-01 10:34:55.805330: Current learning rate: 0.00933
2024-12-01 10:36:24.303432: Validation loss did not improve from -0.71251. Patience: 12/50
2024-12-01 10:36:24.304628: train_loss -0.6716
2024-12-01 10:36:24.305684: val_loss -0.6748
2024-12-01 10:36:24.306571: Pseudo dice [0.8044]
2024-12-01 10:36:24.307382: Epoch time: 88.5 s
2024-12-01 10:36:26.024024: 
2024-12-01 10:36:26.025131: Epoch 75
2024-12-01 10:36:26.025907: Current learning rate: 0.00932
2024-12-01 10:37:54.250553: Validation loss did not improve from -0.71251. Patience: 13/50
2024-12-01 10:37:54.251633: train_loss -0.6752
2024-12-01 10:37:54.252496: val_loss -0.6392
2024-12-01 10:37:54.253164: Pseudo dice [0.7838]
2024-12-01 10:37:54.253799: Epoch time: 88.23 s
2024-12-01 10:37:55.604777: 
2024-12-01 10:37:55.606184: Epoch 76
2024-12-01 10:37:55.606922: Current learning rate: 0.00931
2024-12-01 10:39:23.895440: Validation loss did not improve from -0.71251. Patience: 14/50
2024-12-01 10:39:23.896326: train_loss -0.6693
2024-12-01 10:39:23.897322: val_loss -0.652
2024-12-01 10:39:23.898554: Pseudo dice [0.7908]
2024-12-01 10:39:23.899575: Epoch time: 88.29 s
2024-12-01 10:39:25.201838: 
2024-12-01 10:39:25.203455: Epoch 77
2024-12-01 10:39:25.204409: Current learning rate: 0.0093
2024-12-01 10:40:53.395173: Validation loss did not improve from -0.71251. Patience: 15/50
2024-12-01 10:40:53.396668: train_loss -0.6768
2024-12-01 10:40:53.397848: val_loss -0.6434
2024-12-01 10:40:53.398787: Pseudo dice [0.7876]
2024-12-01 10:40:53.399460: Epoch time: 88.2 s
2024-12-01 10:40:54.666023: 
2024-12-01 10:40:54.667821: Epoch 78
2024-12-01 10:40:54.668915: Current learning rate: 0.0093
2024-12-01 10:42:22.949386: Validation loss did not improve from -0.71251. Patience: 16/50
2024-12-01 10:42:22.950577: train_loss -0.6738
2024-12-01 10:42:22.951995: val_loss -0.6781
2024-12-01 10:42:22.953036: Pseudo dice [0.8088]
2024-12-01 10:42:22.953835: Epoch time: 88.29 s
2024-12-01 10:42:24.216929: 
2024-12-01 10:42:24.218770: Epoch 79
2024-12-01 10:42:24.219900: Current learning rate: 0.00929
2024-12-01 10:43:52.587266: Validation loss did not improve from -0.71251. Patience: 17/50
2024-12-01 10:43:52.588561: train_loss -0.6737
2024-12-01 10:43:52.589739: val_loss -0.6676
2024-12-01 10:43:52.590712: Pseudo dice [0.7982]
2024-12-01 10:43:52.591764: Epoch time: 88.37 s
2024-12-01 10:43:54.192803: 
2024-12-01 10:43:54.194666: Epoch 80
2024-12-01 10:43:54.195542: Current learning rate: 0.00928
2024-12-01 10:45:22.391835: Validation loss did not improve from -0.71251. Patience: 18/50
2024-12-01 10:45:22.392884: train_loss -0.6717
2024-12-01 10:45:22.393877: val_loss -0.6822
2024-12-01 10:45:22.394682: Pseudo dice [0.8089]
2024-12-01 10:45:22.395638: Epoch time: 88.2 s
2024-12-01 10:45:23.630505: 
2024-12-01 10:45:23.632214: Epoch 81
2024-12-01 10:45:23.632806: Current learning rate: 0.00927
2024-12-01 10:46:51.896881: Validation loss did not improve from -0.71251. Patience: 19/50
2024-12-01 10:46:51.897941: train_loss -0.6636
2024-12-01 10:46:51.898994: val_loss -0.6411
2024-12-01 10:46:51.899726: Pseudo dice [0.7875]
2024-12-01 10:46:51.900846: Epoch time: 88.27 s
2024-12-01 10:46:53.139922: 
2024-12-01 10:46:53.141806: Epoch 82
2024-12-01 10:46:53.142741: Current learning rate: 0.00926
2024-12-01 10:48:21.291149: Validation loss did not improve from -0.71251. Patience: 20/50
2024-12-01 10:48:21.292382: train_loss -0.6683
2024-12-01 10:48:21.293730: val_loss -0.6533
2024-12-01 10:48:21.294326: Pseudo dice [0.7896]
2024-12-01 10:48:21.294940: Epoch time: 88.15 s
2024-12-01 10:48:22.766313: 
2024-12-01 10:48:22.768124: Epoch 83
2024-12-01 10:48:22.769100: Current learning rate: 0.00925
2024-12-01 10:49:50.940799: Validation loss did not improve from -0.71251. Patience: 21/50
2024-12-01 10:49:50.941934: train_loss -0.6722
2024-12-01 10:49:50.943128: val_loss -0.6792
2024-12-01 10:49:50.943981: Pseudo dice [0.8126]
2024-12-01 10:49:50.944789: Epoch time: 88.18 s
2024-12-01 10:49:52.117136: 
2024-12-01 10:49:52.119080: Epoch 84
2024-12-01 10:49:52.120223: Current learning rate: 0.00924
2024-12-01 10:51:20.234134: Validation loss did not improve from -0.71251. Patience: 22/50
2024-12-01 10:51:20.234911: train_loss -0.6735
2024-12-01 10:51:20.235699: val_loss -0.6279
2024-12-01 10:51:20.236440: Pseudo dice [0.7813]
2024-12-01 10:51:20.237134: Epoch time: 88.12 s
2024-12-01 10:51:21.781471: 
2024-12-01 10:51:21.783856: Epoch 85
2024-12-01 10:51:21.784767: Current learning rate: 0.00923
2024-12-01 10:52:49.961885: Validation loss did not improve from -0.71251. Patience: 23/50
2024-12-01 10:52:49.962952: train_loss -0.671
2024-12-01 10:52:49.963986: val_loss -0.6506
2024-12-01 10:52:49.965143: Pseudo dice [0.7891]
2024-12-01 10:52:49.966052: Epoch time: 88.18 s
2024-12-01 10:52:51.172034: 
2024-12-01 10:52:51.174074: Epoch 86
2024-12-01 10:52:51.175246: Current learning rate: 0.00922
2024-12-01 10:54:19.354386: Validation loss did not improve from -0.71251. Patience: 24/50
2024-12-01 10:54:19.355336: train_loss -0.6755
2024-12-01 10:54:19.356493: val_loss -0.6878
2024-12-01 10:54:19.357558: Pseudo dice [0.8085]
2024-12-01 10:54:19.358770: Epoch time: 88.18 s
2024-12-01 10:54:20.532555: 
2024-12-01 10:54:20.534691: Epoch 87
2024-12-01 10:54:20.535534: Current learning rate: 0.00921
2024-12-01 10:55:48.354272: Validation loss did not improve from -0.71251. Patience: 25/50
2024-12-01 10:55:48.355377: train_loss -0.6763
2024-12-01 10:55:48.356138: val_loss -0.6388
2024-12-01 10:55:48.356934: Pseudo dice [0.7839]
2024-12-01 10:55:48.357740: Epoch time: 87.82 s
2024-12-01 10:55:49.545237: 
2024-12-01 10:55:49.546986: Epoch 88
2024-12-01 10:55:49.548006: Current learning rate: 0.0092
2024-12-01 10:57:17.395641: Validation loss did not improve from -0.71251. Patience: 26/50
2024-12-01 10:57:17.396638: train_loss -0.6818
2024-12-01 10:57:17.397551: val_loss -0.6448
2024-12-01 10:57:17.398223: Pseudo dice [0.7774]
2024-12-01 10:57:17.399269: Epoch time: 87.85 s
2024-12-01 10:57:18.586753: 
2024-12-01 10:57:18.588552: Epoch 89
2024-12-01 10:57:18.589207: Current learning rate: 0.0092
2024-12-01 10:58:46.446426: Validation loss did not improve from -0.71251. Patience: 27/50
2024-12-01 10:58:46.447767: train_loss -0.6791
2024-12-01 10:58:46.449044: val_loss -0.5834
2024-12-01 10:58:46.449776: Pseudo dice [0.7486]
2024-12-01 10:58:46.450499: Epoch time: 87.86 s
2024-12-01 10:58:47.971365: 
2024-12-01 10:58:47.973435: Epoch 90
2024-12-01 10:58:47.974509: Current learning rate: 0.00919
2024-12-01 11:00:15.755711: Validation loss did not improve from -0.71251. Patience: 28/50
2024-12-01 11:00:15.756578: train_loss -0.6794
2024-12-01 11:00:15.757360: val_loss -0.6157
2024-12-01 11:00:15.757955: Pseudo dice [0.7664]
2024-12-01 11:00:15.758537: Epoch time: 87.79 s
2024-12-01 11:00:16.927353: 
2024-12-01 11:00:16.929252: Epoch 91
2024-12-01 11:00:16.929889: Current learning rate: 0.00918
2024-12-01 11:01:44.800630: Validation loss did not improve from -0.71251. Patience: 29/50
2024-12-01 11:01:44.801550: train_loss -0.6838
2024-12-01 11:01:44.802537: val_loss -0.5942
2024-12-01 11:01:44.803558: Pseudo dice [0.7604]
2024-12-01 11:01:44.804427: Epoch time: 87.88 s
2024-12-01 11:01:45.996563: 
2024-12-01 11:01:45.998817: Epoch 92
2024-12-01 11:01:45.999844: Current learning rate: 0.00917
2024-12-01 11:03:13.785107: Validation loss did not improve from -0.71251. Patience: 30/50
2024-12-01 11:03:13.785914: train_loss -0.6909
2024-12-01 11:03:13.786693: val_loss -0.6257
2024-12-01 11:03:13.787931: Pseudo dice [0.7794]
2024-12-01 11:03:13.788955: Epoch time: 87.79 s
2024-12-01 11:03:14.963710: 
2024-12-01 11:03:14.965726: Epoch 93
2024-12-01 11:03:14.966536: Current learning rate: 0.00916
2024-12-01 11:04:42.865535: Validation loss did not improve from -0.71251. Patience: 31/50
2024-12-01 11:04:42.866950: train_loss -0.6743
2024-12-01 11:04:42.868125: val_loss -0.6225
2024-12-01 11:04:42.869003: Pseudo dice [0.7716]
2024-12-01 11:04:42.869654: Epoch time: 87.9 s
2024-12-01 11:04:44.350513: 
2024-12-01 11:04:44.352485: Epoch 94
2024-12-01 11:04:44.353545: Current learning rate: 0.00915
2024-12-01 11:06:12.189287: Validation loss did not improve from -0.71251. Patience: 32/50
2024-12-01 11:06:12.190485: train_loss -0.6717
2024-12-01 11:06:12.192059: val_loss -0.6526
2024-12-01 11:06:12.193281: Pseudo dice [0.7865]
2024-12-01 11:06:12.194482: Epoch time: 87.84 s
2024-12-01 11:06:13.732257: 
2024-12-01 11:06:13.734548: Epoch 95
2024-12-01 11:06:13.735292: Current learning rate: 0.00914
2024-12-01 11:07:41.973804: Validation loss did not improve from -0.71251. Patience: 33/50
2024-12-01 11:07:41.974549: train_loss -0.6816
2024-12-01 11:07:41.975370: val_loss -0.6858
2024-12-01 11:07:41.976048: Pseudo dice [0.814]
2024-12-01 11:07:41.976710: Epoch time: 88.24 s
2024-12-01 11:07:43.149519: 
2024-12-01 11:07:43.151252: Epoch 96
2024-12-01 11:07:43.152239: Current learning rate: 0.00913
2024-12-01 11:09:11.318506: Validation loss did not improve from -0.71251. Patience: 34/50
2024-12-01 11:09:11.319810: train_loss -0.6783
2024-12-01 11:09:11.320932: val_loss -0.6627
2024-12-01 11:09:11.321468: Pseudo dice [0.7962]
2024-12-01 11:09:11.322045: Epoch time: 88.17 s
2024-12-01 11:09:12.522440: 
2024-12-01 11:09:12.524338: Epoch 97
2024-12-01 11:09:12.525304: Current learning rate: 0.00912
2024-12-01 11:10:40.530406: Validation loss did not improve from -0.71251. Patience: 35/50
2024-12-01 11:10:40.531483: train_loss -0.6751
2024-12-01 11:10:40.532659: val_loss -0.6814
2024-12-01 11:10:40.533471: Pseudo dice [0.812]
2024-12-01 11:10:40.534240: Epoch time: 88.01 s
2024-12-01 11:10:41.724256: 
2024-12-01 11:10:41.726183: Epoch 98
2024-12-01 11:10:41.727257: Current learning rate: 0.00911
2024-12-01 11:12:09.848020: Validation loss did not improve from -0.71251. Patience: 36/50
2024-12-01 11:12:09.849288: train_loss -0.6868
2024-12-01 11:12:09.850165: val_loss -0.6476
2024-12-01 11:12:09.851196: Pseudo dice [0.7845]
2024-12-01 11:12:09.852184: Epoch time: 88.13 s
2024-12-01 11:12:11.051965: 
2024-12-01 11:12:11.053777: Epoch 99
2024-12-01 11:12:11.054819: Current learning rate: 0.0091
2024-12-01 11:13:39.173098: Validation loss did not improve from -0.71251. Patience: 37/50
2024-12-01 11:13:39.174573: train_loss -0.6896
2024-12-01 11:13:39.175912: val_loss -0.6794
2024-12-01 11:13:39.176832: Pseudo dice [0.8096]
2024-12-01 11:13:39.177557: Epoch time: 88.12 s
2024-12-01 11:13:40.732148: 
2024-12-01 11:13:40.733916: Epoch 100
2024-12-01 11:13:40.734775: Current learning rate: 0.0091
2024-12-01 11:15:08.873508: Validation loss did not improve from -0.71251. Patience: 38/50
2024-12-01 11:15:08.874598: train_loss -0.6878
2024-12-01 11:15:08.875429: val_loss -0.6901
2024-12-01 11:15:08.876289: Pseudo dice [0.8135]
2024-12-01 11:15:08.877097: Epoch time: 88.14 s
2024-12-01 11:15:10.069802: 
2024-12-01 11:15:10.071660: Epoch 101
2024-12-01 11:15:10.072833: Current learning rate: 0.00909
2024-12-01 11:16:38.220842: Validation loss did not improve from -0.71251. Patience: 39/50
2024-12-01 11:16:38.221830: train_loss -0.6889
2024-12-01 11:16:38.222763: val_loss -0.6346
2024-12-01 11:16:38.223586: Pseudo dice [0.768]
2024-12-01 11:16:38.224349: Epoch time: 88.15 s
2024-12-01 11:16:39.407569: 
2024-12-01 11:16:39.409509: Epoch 102
2024-12-01 11:16:39.410434: Current learning rate: 0.00908
2024-12-01 11:18:07.534546: Validation loss did not improve from -0.71251. Patience: 40/50
2024-12-01 11:18:07.535557: train_loss -0.6998
2024-12-01 11:18:07.536365: val_loss -0.6374
2024-12-01 11:18:07.536946: Pseudo dice [0.7758]
2024-12-01 11:18:07.537640: Epoch time: 88.13 s
2024-12-01 11:18:08.736981: 
2024-12-01 11:18:08.738907: Epoch 103
2024-12-01 11:18:08.739985: Current learning rate: 0.00907
2024-12-01 11:19:37.043620: Validation loss did not improve from -0.71251. Patience: 41/50
2024-12-01 11:19:37.044510: train_loss -0.7017
2024-12-01 11:19:37.045263: val_loss -0.6493
2024-12-01 11:19:37.046152: Pseudo dice [0.7822]
2024-12-01 11:19:37.046951: Epoch time: 88.31 s
2024-12-01 11:19:38.249100: 
2024-12-01 11:19:38.251142: Epoch 104
2024-12-01 11:19:38.252076: Current learning rate: 0.00906
2024-12-01 11:21:06.615503: Validation loss did not improve from -0.71251. Patience: 42/50
2024-12-01 11:21:06.617087: train_loss -0.6973
2024-12-01 11:21:06.618679: val_loss -0.6566
2024-12-01 11:21:06.619852: Pseudo dice [0.7901]
2024-12-01 11:21:06.620662: Epoch time: 88.37 s
2024-12-01 11:21:08.483149: 
2024-12-01 11:21:08.484562: Epoch 105
2024-12-01 11:21:08.485358: Current learning rate: 0.00905
2024-12-01 11:22:36.672931: Validation loss did not improve from -0.71251. Patience: 43/50
2024-12-01 11:22:36.674397: train_loss -0.7034
2024-12-01 11:22:36.675409: val_loss -0.622
2024-12-01 11:22:36.676324: Pseudo dice [0.7683]
2024-12-01 11:22:36.676925: Epoch time: 88.19 s
2024-12-01 11:22:37.860988: 
2024-12-01 11:22:37.862634: Epoch 106
2024-12-01 11:22:37.863803: Current learning rate: 0.00904
2024-12-01 11:24:05.998672: Validation loss did not improve from -0.71251. Patience: 44/50
2024-12-01 11:24:05.999497: train_loss -0.6895
2024-12-01 11:24:06.000501: val_loss -0.6267
2024-12-01 11:24:06.001335: Pseudo dice [0.7709]
2024-12-01 11:24:06.002051: Epoch time: 88.14 s
2024-12-01 11:24:07.199195: 
2024-12-01 11:24:07.201084: Epoch 107
2024-12-01 11:24:07.202243: Current learning rate: 0.00903
2024-12-01 11:25:35.317128: Validation loss did not improve from -0.71251. Patience: 45/50
2024-12-01 11:25:35.317913: train_loss -0.695
2024-12-01 11:25:35.318833: val_loss -0.6922
2024-12-01 11:25:35.319614: Pseudo dice [0.8176]
2024-12-01 11:25:35.320400: Epoch time: 88.12 s
2024-12-01 11:25:36.516987: 
2024-12-01 11:25:36.518851: Epoch 108
2024-12-01 11:25:36.519712: Current learning rate: 0.00902
2024-12-01 11:27:04.601531: Validation loss did not improve from -0.71251. Patience: 46/50
2024-12-01 11:27:04.602843: train_loss -0.6917
2024-12-01 11:27:04.604070: val_loss -0.606
2024-12-01 11:27:04.605128: Pseudo dice [0.7582]
2024-12-01 11:27:04.606253: Epoch time: 88.09 s
2024-12-01 11:27:05.808455: 
2024-12-01 11:27:05.810401: Epoch 109
2024-12-01 11:27:05.811592: Current learning rate: 0.00901
2024-12-01 11:28:33.942295: Validation loss did not improve from -0.71251. Patience: 47/50
2024-12-01 11:28:33.943164: train_loss -0.6964
2024-12-01 11:28:33.944102: val_loss -0.6442
2024-12-01 11:28:33.945319: Pseudo dice [0.7768]
2024-12-01 11:28:33.946475: Epoch time: 88.14 s
2024-12-01 11:28:35.463469: 
2024-12-01 11:28:35.464861: Epoch 110
2024-12-01 11:28:35.465909: Current learning rate: 0.009
2024-12-01 11:30:03.605988: Validation loss did not improve from -0.71251. Patience: 48/50
2024-12-01 11:30:03.606842: train_loss -0.6959
2024-12-01 11:30:03.607702: val_loss -0.6615
2024-12-01 11:30:03.608595: Pseudo dice [0.7997]
2024-12-01 11:30:03.609798: Epoch time: 88.14 s
2024-12-01 11:30:04.815744: 
2024-12-01 11:30:04.817180: Epoch 111
2024-12-01 11:30:04.817932: Current learning rate: 0.009
2024-12-01 11:31:32.926982: Validation loss did not improve from -0.71251. Patience: 49/50
2024-12-01 11:31:32.928122: train_loss -0.6981
2024-12-01 11:31:32.929149: val_loss -0.6904
2024-12-01 11:31:32.930055: Pseudo dice [0.8189]
2024-12-01 11:31:32.931222: Epoch time: 88.11 s
2024-12-01 11:31:34.125123: 
2024-12-01 11:31:34.126694: Epoch 112
2024-12-01 11:31:34.127627: Current learning rate: 0.00899
2024-12-01 11:33:02.088312: Validation loss did not improve from -0.71251. Patience: 50/50
2024-12-01 11:33:02.089426: train_loss -0.695
2024-12-01 11:33:02.090532: val_loss -0.6162
2024-12-01 11:33:02.091257: Pseudo dice [0.7642]
2024-12-01 11:33:02.092215: Epoch time: 87.97 s
2024-12-01 11:33:03.286141: Patience reached. Stopping training.
2024-12-01 11:33:03.714082: Training done.
2024-12-01 11:33:03.876999: Using splits from existing split file: /home/gridsan/nchutisilp/datasets/nnUNet_Datasets/nnUNet_preprocessed/Dataset306_Sohee_Ajay_Calcium_OCT/splits_final.json
2024-12-01 11:33:03.879925: The split file contains 5 splits.
2024-12-01 11:33:03.880787: Desired fold for training: 3
2024-12-01 11:33:03.881487: This split has 11 training and 2 validation cases.
2024-12-01 11:33:03.882370: predicting 03009Pre
2024-12-01 11:33:03.890826: 03009Pre, shape torch.Size([1, 400, 498, 498]), rank 0
2024-12-01 11:34:45.332488: predicting 101-044
2024-12-01 11:34:45.356477: 101-044, shape torch.Size([1, 404, 498, 498]), rank 0
2024-12-01 11:36:43.632096: Validation complete
2024-12-01 11:36:43.633339: Mean Validation Dice:  0.7735279989753419

############################
INFO: You are using the old nnU-Net default plans. We have updated our recommendations. Please consider using those instead! Read more here: https://github.com/MIC-DKFZ/nnUNet/blob/master/documentation/resenc_presets.md
############################

Using device: cuda:0

#######################################################################
Please cite the following paper when using nnU-Net:
Isensee, F., Jaeger, P. F., Kohl, S. A., Petersen, J., & Maier-Hein, K. H. (2021). nnU-Net: a self-configuring method for deep learning-based biomedical image segmentation. Nature methods, 18(2), 203-211.
#######################################################################

2024-12-01 11:36:52.079426: Using torch.compile...
/home/gridsan/nchutisilp/.local/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn("The verbose parameter is deprecated. Please use get_last_lr() "
################### Loading pretrained weights from file  /home/gridsan/nchutisilp/datasets/nnUNet_Datasets/nnUNet_results/Dataset300_Lumen_and_Wall_OCT/nnUNetTrainer__nnUNetPreprocessPlans__3d_fullres/fold_all/checkpoint_best.pth ###################
Below is the list of overlapping blocks in pretrained model and nnUNet architecture:
encoder.stages.0.0.convs.0.conv.weight shape torch.Size([32, 1, 3, 3, 3])
encoder.stages.0.0.convs.0.conv.bias shape torch.Size([32])
encoder.stages.0.0.convs.0.norm.weight shape torch.Size([32])
encoder.stages.0.0.convs.0.norm.bias shape torch.Size([32])
encoder.stages.0.0.convs.0.all_modules.0.weight shape torch.Size([32, 1, 3, 3, 3])
encoder.stages.0.0.convs.0.all_modules.0.bias shape torch.Size([32])
encoder.stages.0.0.convs.0.all_modules.1.weight shape torch.Size([32])
encoder.stages.0.0.convs.0.all_modules.1.bias shape torch.Size([32])
encoder.stages.0.0.convs.1.conv.weight shape torch.Size([32, 32, 3, 3, 3])
encoder.stages.0.0.convs.1.conv.bias shape torch.Size([32])
encoder.stages.0.0.convs.1.norm.weight shape torch.Size([32])
encoder.stages.0.0.convs.1.norm.bias shape torch.Size([32])
encoder.stages.0.0.convs.1.all_modules.0.weight shape torch.Size([32, 32, 3, 3, 3])
encoder.stages.0.0.convs.1.all_modules.0.bias shape torch.Size([32])
encoder.stages.0.0.convs.1.all_modules.1.weight shape torch.Size([32])
encoder.stages.0.0.convs.1.all_modules.1.bias shape torch.Size([32])
encoder.stages.1.0.convs.0.conv.weight shape torch.Size([64, 32, 3, 3, 3])
encoder.stages.1.0.convs.0.conv.bias shape torch.Size([64])
encoder.stages.1.0.convs.0.norm.weight shape torch.Size([64])
encoder.stages.1.0.convs.0.norm.bias shape torch.Size([64])
encoder.stages.1.0.convs.0.all_modules.0.weight shape torch.Size([64, 32, 3, 3, 3])
encoder.stages.1.0.convs.0.all_modules.0.bias shape torch.Size([64])
encoder.stages.1.0.convs.0.all_modules.1.weight shape torch.Size([64])
encoder.stages.1.0.convs.0.all_modules.1.bias shape torch.Size([64])
encoder.stages.1.0.convs.1.conv.weight shape torch.Size([64, 64, 3, 3, 3])
encoder.stages.1.0.convs.1.conv.bias shape torch.Size([64])
encoder.stages.1.0.convs.1.norm.weight shape torch.Size([64])
encoder.stages.1.0.convs.1.norm.bias shape torch.Size([64])
encoder.stages.1.0.convs.1.all_modules.0.weight shape torch.Size([64, 64, 3, 3, 3])
encoder.stages.1.0.convs.1.all_modules.0.bias shape torch.Size([64])
encoder.stages.1.0.convs.1.all_modules.1.weight shape torch.Size([64])
encoder.stages.1.0.convs.1.all_modules.1.bias shape torch.Size([64])
encoder.stages.2.0.convs.0.conv.weight shape torch.Size([128, 64, 3, 3, 3])
encoder.stages.2.0.convs.0.conv.bias shape torch.Size([128])
encoder.stages.2.0.convs.0.norm.weight shape torch.Size([128])
encoder.stages.2.0.convs.0.norm.bias shape torch.Size([128])
encoder.stages.2.0.convs.0.all_modules.0.weight shape torch.Size([128, 64, 3, 3, 3])
encoder.stages.2.0.convs.0.all_modules.0.bias shape torch.Size([128])
encoder.stages.2.0.convs.0.all_modules.1.weight shape torch.Size([128])
encoder.stages.2.0.convs.0.all_modules.1.bias shape torch.Size([128])
encoder.stages.2.0.convs.1.conv.weight shape torch.Size([128, 128, 3, 3, 3])
encoder.stages.2.0.convs.1.conv.bias shape torch.Size([128])
encoder.stages.2.0.convs.1.norm.weight shape torch.Size([128])
encoder.stages.2.0.convs.1.norm.bias shape torch.Size([128])
encoder.stages.2.0.convs.1.all_modules.0.weight shape torch.Size([128, 128, 3, 3, 3])
encoder.stages.2.0.convs.1.all_modules.0.bias shape torch.Size([128])
encoder.stages.2.0.convs.1.all_modules.1.weight shape torch.Size([128])
encoder.stages.2.0.convs.1.all_modules.1.bias shape torch.Size([128])
encoder.stages.3.0.convs.0.conv.weight shape torch.Size([256, 128, 3, 3, 3])
encoder.stages.3.0.convs.0.conv.bias shape torch.Size([256])
encoder.stages.3.0.convs.0.norm.weight shape torch.Size([256])
encoder.stages.3.0.convs.0.norm.bias shape torch.Size([256])
encoder.stages.3.0.convs.0.all_modules.0.weight shape torch.Size([256, 128, 3, 3, 3])
encoder.stages.3.0.convs.0.all_modules.0.bias shape torch.Size([256])
encoder.stages.3.0.convs.0.all_modules.1.weight shape torch.Size([256])
encoder.stages.3.0.convs.0.all_modules.1.bias shape torch.Size([256])
encoder.stages.3.0.convs.1.conv.weight shape torch.Size([256, 256, 3, 3, 3])
encoder.stages.3.0.convs.1.conv.bias shape torch.Size([256])
encoder.stages.3.0.convs.1.norm.weight shape torch.Size([256])
encoder.stages.3.0.convs.1.norm.bias shape torch.Size([256])
encoder.stages.3.0.convs.1.all_modules.0.weight shape torch.Size([256, 256, 3, 3, 3])
encoder.stages.3.0.convs.1.all_modules.0.bias shape torch.Size([256])
encoder.stages.3.0.convs.1.all_modules.1.weight shape torch.Size([256])
encoder.stages.3.0.convs.1.all_modules.1.bias shape torch.Size([256])
encoder.stages.4.0.convs.0.conv.weight shape torch.Size([320, 256, 3, 3, 3])
encoder.stages.4.0.convs.0.conv.bias shape torch.Size([320])
encoder.stages.4.0.convs.0.norm.weight shape torch.Size([320])
encoder.stages.4.0.convs.0.norm.bias shape torch.Size([320])
encoder.stages.4.0.convs.0.all_modules.0.weight shape torch.Size([320, 256, 3, 3, 3])
encoder.stages.4.0.convs.0.all_modules.0.bias shape torch.Size([320])
encoder.stages.4.0.convs.0.all_modules.1.weight shape torch.Size([320])
encoder.stages.4.0.convs.0.all_modules.1.bias shape torch.Size([320])
encoder.stages.4.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.4.0.convs.1.conv.bias shape torch.Size([320])
encoder.stages.4.0.convs.1.norm.weight shape torch.Size([320])
encoder.stages.4.0.convs.1.norm.bias shape torch.Size([320])
encoder.stages.4.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.4.0.convs.1.all_modules.0.bias shape torch.Size([320])
encoder.stages.4.0.convs.1.all_modules.1.weight shape torch.Size([320])
encoder.stages.4.0.convs.1.all_modules.1.bias shape torch.Size([320])
encoder.stages.5.0.convs.0.conv.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.5.0.convs.0.conv.bias shape torch.Size([320])
encoder.stages.5.0.convs.0.norm.weight shape torch.Size([320])
encoder.stages.5.0.convs.0.norm.bias shape torch.Size([320])
encoder.stages.5.0.convs.0.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.5.0.convs.0.all_modules.0.bias shape torch.Size([320])
encoder.stages.5.0.convs.0.all_modules.1.weight shape torch.Size([320])
encoder.stages.5.0.convs.0.all_modules.1.bias shape torch.Size([320])
encoder.stages.5.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.5.0.convs.1.conv.bias shape torch.Size([320])
encoder.stages.5.0.convs.1.norm.weight shape torch.Size([320])
encoder.stages.5.0.convs.1.norm.bias shape torch.Size([320])
encoder.stages.5.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.5.0.convs.1.all_modules.0.bias shape torch.Size([320])
encoder.stages.5.0.convs.1.all_modules.1.weight shape torch.Size([320])
encoder.stages.5.0.convs.1.all_modules.1.bias shape torch.Size([320])
decoder.encoder.stages.0.0.convs.0.conv.weight shape torch.Size([32, 1, 3, 3, 3])
decoder.encoder.stages.0.0.convs.0.conv.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.norm.weight shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.norm.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.all_modules.0.weight shape torch.Size([32, 1, 3, 3, 3])
decoder.encoder.stages.0.0.convs.0.all_modules.0.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.all_modules.1.weight shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.all_modules.1.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.conv.weight shape torch.Size([32, 32, 3, 3, 3])
decoder.encoder.stages.0.0.convs.1.conv.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.norm.weight shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.norm.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.all_modules.0.weight shape torch.Size([32, 32, 3, 3, 3])
decoder.encoder.stages.0.0.convs.1.all_modules.0.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.all_modules.1.weight shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.all_modules.1.bias shape torch.Size([32])
decoder.encoder.stages.1.0.convs.0.conv.weight shape torch.Size([64, 32, 3, 3, 3])
decoder.encoder.stages.1.0.convs.0.conv.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.norm.weight shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.norm.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.all_modules.0.weight shape torch.Size([64, 32, 3, 3, 3])
decoder.encoder.stages.1.0.convs.0.all_modules.0.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.all_modules.1.weight shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.all_modules.1.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.conv.weight shape torch.Size([64, 64, 3, 3, 3])
decoder.encoder.stages.1.0.convs.1.conv.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.norm.weight shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.norm.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.all_modules.0.weight shape torch.Size([64, 64, 3, 3, 3])
decoder.encoder.stages.1.0.convs.1.all_modules.0.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.all_modules.1.weight shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.all_modules.1.bias shape torch.Size([64])
decoder.encoder.stages.2.0.convs.0.conv.weight shape torch.Size([128, 64, 3, 3, 3])
decoder.encoder.stages.2.0.convs.0.conv.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.norm.weight shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.norm.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.all_modules.0.weight shape torch.Size([128, 64, 3, 3, 3])
decoder.encoder.stages.2.0.convs.0.all_modules.0.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.all_modules.1.weight shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.all_modules.1.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.conv.weight shape torch.Size([128, 128, 3, 3, 3])
decoder.encoder.stages.2.0.convs.1.conv.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.norm.weight shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.norm.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.all_modules.0.weight shape torch.Size([128, 128, 3, 3, 3])
decoder.encoder.stages.2.0.convs.1.all_modules.0.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.all_modules.1.weight shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.all_modules.1.bias shape torch.Size([128])
decoder.encoder.stages.3.0.convs.0.conv.weight shape torch.Size([256, 128, 3, 3, 3])
decoder.encoder.stages.3.0.convs.0.conv.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.norm.weight shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.norm.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.all_modules.0.weight shape torch.Size([256, 128, 3, 3, 3])
decoder.encoder.stages.3.0.convs.0.all_modules.0.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.all_modules.1.weight shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.all_modules.1.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.conv.weight shape torch.Size([256, 256, 3, 3, 3])
decoder.encoder.stages.3.0.convs.1.conv.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.norm.weight shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.norm.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.all_modules.0.weight shape torch.Size([256, 256, 3, 3, 3])
decoder.encoder.stages.3.0.convs.1.all_modules.0.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.all_modules.1.weight shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.all_modules.1.bias shape torch.Size([256])
decoder.encoder.stages.4.0.convs.0.conv.weight shape torch.Size([320, 256, 3, 3, 3])
decoder.encoder.stages.4.0.convs.0.conv.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.norm.weight shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.norm.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.all_modules.0.weight shape torch.Size([320, 256, 3, 3, 3])
decoder.encoder.stages.4.0.convs.0.all_modules.0.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.all_modules.1.weight shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.all_modules.1.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.4.0.convs.1.conv.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.norm.weight shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.norm.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.4.0.convs.1.all_modules.0.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.all_modules.1.weight shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.all_modules.1.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.conv.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.5.0.convs.0.conv.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.norm.weight shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.norm.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.5.0.convs.0.all_modules.0.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.all_modules.1.weight shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.all_modules.1.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.5.0.convs.1.conv.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.norm.weight shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.norm.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.5.0.convs.1.all_modules.0.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.all_modules.1.weight shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.all_modules.1.bias shape torch.Size([320])
decoder.stages.0.convs.0.conv.weight shape torch.Size([320, 640, 3, 3, 3])
decoder.stages.0.convs.0.conv.bias shape torch.Size([320])
decoder.stages.0.convs.0.norm.weight shape torch.Size([320])
decoder.stages.0.convs.0.norm.bias shape torch.Size([320])
decoder.stages.0.convs.0.all_modules.0.weight shape torch.Size([320, 640, 3, 3, 3])
decoder.stages.0.convs.0.all_modules.0.bias shape torch.Size([320])
decoder.stages.0.convs.0.all_modules.1.weight shape torch.Size([320])
decoder.stages.0.convs.0.all_modules.1.bias shape torch.Size([320])
decoder.stages.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.stages.0.convs.1.conv.bias shape torch.Size([320])
decoder.stages.0.convs.1.norm.weight shape torch.Size([320])
decoder.stages.0.convs.1.norm.bias shape torch.Size([320])
decoder.stages.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.stages.0.convs.1.all_modules.0.bias shape torch.Size([320])
decoder.stages.0.convs.1.all_modules.1.weight shape torch.Size([320])
decoder.stages.0.convs.1.all_modules.1.bias shape torch.Size([320])
decoder.stages.1.convs.0.conv.weight shape torch.Size([256, 512, 3, 3, 3])
decoder.stages.1.convs.0.conv.bias shape torch.Size([256])
decoder.stages.1.convs.0.norm.weight shape torch.Size([256])
decoder.stages.1.convs.0.norm.bias shape torch.Size([256])
decoder.stages.1.convs.0.all_modules.0.weight shape torch.Size([256, 512, 3, 3, 3])
decoder.stages.1.convs.0.all_modules.0.bias shape torch.Size([256])
decoder.stages.1.convs.0.all_modules.1.weight shape torch.Size([256])
decoder.stages.1.convs.0.all_modules.1.bias shape torch.Size([256])
decoder.stages.1.convs.1.conv.weight shape torch.Size([256, 256, 3, 3, 3])
decoder.stages.1.convs.1.conv.bias shape torch.Size([256])
decoder.stages.1.convs.1.norm.weight shape torch.Size([256])
decoder.stages.1.convs.1.norm.bias shape torch.Size([256])
decoder.stages.1.convs.1.all_modules.0.weight shape torch.Size([256, 256, 3, 3, 3])
decoder.stages.1.convs.1.all_modules.0.bias shape torch.Size([256])
decoder.stages.1.convs.1.all_modules.1.weight shape torch.Size([256])
decoder.stages.1.convs.1.all_modules.1.bias shape torch.Size([256])
decoder.stages.2.convs.0.conv.weight shape torch.Size([128, 256, 3, 3, 3])
decoder.stages.2.convs.0.conv.bias shape torch.Size([128])
decoder.stages.2.convs.0.norm.weight shape torch.Size([128])
decoder.stages.2.convs.0.norm.bias shape torch.Size([128])
decoder.stages.2.convs.0.all_modules.0.weight shape torch.Size([128, 256, 3, 3, 3])
decoder.stages.2.convs.0.all_modules.0.bias shape torch.Size([128])
decoder.stages.2.convs.0.all_modules.1.weight shape torch.Size([128])
decoder.stages.2.convs.0.all_modules.1.bias shape torch.Size([128])
decoder.stages.2.convs.1.conv.weight shape torch.Size([128, 128, 3, 3, 3])
decoder.stages.2.convs.1.conv.bias shape torch.Size([128])
decoder.stages.2.convs.1.norm.weight shape torch.Size([128])
decoder.stages.2.convs.1.norm.bias shape torch.Size([128])
decoder.stages.2.convs.1.all_modules.0.weight shape torch.Size([128, 128, 3, 3, 3])
decoder.stages.2.convs.1.all_modules.0.bias shape torch.Size([128])
decoder.stages.2.convs.1.all_modules.1.weight shape torch.Size([128])
decoder.stages.2.convs.1.all_modules.1.bias shape torch.Size([128])
decoder.stages.3.convs.0.conv.weight shape torch.Size([64, 128, 3, 3, 3])
decoder.stages.3.convs.0.conv.bias shape torch.Size([64])
decoder.stages.3.convs.0.norm.weight shape torch.Size([64])
decoder.stages.3.convs.0.norm.bias shape torch.Size([64])
decoder.stages.3.convs.0.all_modules.0.weight shape torch.Size([64, 128, 3, 3, 3])
decoder.stages.3.convs.0.all_modules.0.bias shape torch.Size([64])
decoder.stages.3.convs.0.all_modules.1.weight shape torch.Size([64])
decoder.stages.3.convs.0.all_modules.1.bias shape torch.Size([64])
decoder.stages.3.convs.1.conv.weight shape torch.Size([64, 64, 3, 3, 3])
decoder.stages.3.convs.1.conv.bias shape torch.Size([64])
decoder.stages.3.convs.1.norm.weight shape torch.Size([64])
decoder.stages.3.convs.1.norm.bias shape torch.Size([64])
decoder.stages.3.convs.1.all_modules.0.weight shape torch.Size([64, 64, 3, 3, 3])
decoder.stages.3.convs.1.all_modules.0.bias shape torch.Size([64])
decoder.stages.3.convs.1.all_modules.1.weight shape torch.Size([64])
decoder.stages.3.convs.1.all_modules.1.bias shape torch.Size([64])
decoder.stages.4.convs.0.conv.weight shape torch.Size([32, 64, 3, 3, 3])
decoder.stages.4.convs.0.conv.bias shape torch.Size([32])
decoder.stages.4.convs.0.norm.weight shape torch.Size([32])
decoder.stages.4.convs.0.norm.bias shape torch.Size([32])
decoder.stages.4.convs.0.all_modules.0.weight shape torch.Size([32, 64, 3, 3, 3])
decoder.stages.4.convs.0.all_modules.0.bias shape torch.Size([32])
decoder.stages.4.convs.0.all_modules.1.weight shape torch.Size([32])
decoder.stages.4.convs.0.all_modules.1.bias shape torch.Size([32])
decoder.stages.4.convs.1.conv.weight shape torch.Size([32, 32, 3, 3, 3])
decoder.stages.4.convs.1.conv.bias shape torch.Size([32])
decoder.stages.4.convs.1.norm.weight shape torch.Size([32])
decoder.stages.4.convs.1.norm.bias shape torch.Size([32])
decoder.stages.4.convs.1.all_modules.0.weight shape torch.Size([32, 32, 3, 3, 3])
decoder.stages.4.convs.1.all_modules.0.bias shape torch.Size([32])
decoder.stages.4.convs.1.all_modules.1.weight shape torch.Size([32])
decoder.stages.4.convs.1.all_modules.1.bias shape torch.Size([32])
decoder.transpconvs.0.weight shape torch.Size([320, 320, 1, 2, 2])
decoder.transpconvs.0.bias shape torch.Size([320])
decoder.transpconvs.1.weight shape torch.Size([320, 256, 2, 2, 2])
decoder.transpconvs.1.bias shape torch.Size([256])
decoder.transpconvs.2.weight shape torch.Size([256, 128, 2, 2, 2])
decoder.transpconvs.2.bias shape torch.Size([128])
decoder.transpconvs.3.weight shape torch.Size([128, 64, 2, 2, 2])
decoder.transpconvs.3.bias shape torch.Size([64])
decoder.transpconvs.4.weight shape torch.Size([64, 32, 2, 2, 2])
decoder.transpconvs.4.bias shape torch.Size([32])
################### Done ###################
2024-12-01 11:37:10.156696: do_dummy_2d_data_aug: True
2024-12-01 11:37:10.158959: Using splits from existing split file: /home/gridsan/nchutisilp/datasets/nnUNet_Datasets/nnUNet_preprocessed/Dataset306_Sohee_Ajay_Calcium_OCT/splits_final.json
2024-12-01 11:37:10.160776: The split file contains 5 splits.
2024-12-01 11:37:10.161447: Desired fold for training: 4
2024-12-01 11:37:10.162025: This split has 11 training and 2 validation cases.
using pin_memory on device 0
using pin_memory on device 0

This is the configuration used by this training:
Configuration name: 3d_32x160x128_b10
 {'data_identifier': 'nnUNetPlans_3d_fullres', 'preprocessor_name': 'DefaultPreprocessor', 'batch_size': 10, 'patch_size': [32, 160, 128], 'median_image_size_in_voxels': [375.0, 498.0, 498.0], 'spacing': [1.0, 1.0, 1.0], 'normalization_schemes': ['ZScoreNormalization'], 'use_mask_for_norm': [False], 'resampling_fn_data': 'resample_data_or_seg_to_shape', 'resampling_fn_seg': 'resample_data_or_seg_to_shape', 'resampling_fn_data_kwargs': {'is_seg': False, 'order': 3, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_seg_kwargs': {'is_seg': True, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_probabilities': 'resample_data_or_seg_to_shape', 'resampling_fn_probabilities_kwargs': {'is_seg': False, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'architecture': {'network_class_name': 'dynamic_network_architectures.architectures.unet.PlainConvUNet', 'arch_kwargs': {'n_stages': 6, 'features_per_stage': [32, 64, 128, 256, 320, 320], 'conv_op': 'torch.nn.modules.conv.Conv3d', 'kernel_sizes': [[3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3]], 'strides': [[1, 1, 1], [2, 2, 2], [2, 2, 2], [2, 2, 2], [2, 2, 2], [1, 2, 2]], 'n_conv_per_stage': [2, 2, 2, 2, 2, 2], 'n_conv_per_stage_decoder': [2, 2, 2, 2, 2], 'conv_bias': True, 'norm_op': 'torch.nn.modules.instancenorm.InstanceNorm3d', 'norm_op_kwargs': {'eps': 1e-05, 'affine': True}, 'dropout_op': None, 'dropout_op_kwargs': None, 'nonlin': 'torch.nn.LeakyReLU', 'nonlin_kwargs': {'inplace': True}, 'deep_supervision': True}, '_kw_requires_import': ['conv_op', 'norm_op', 'dropout_op', 'nonlin']}, 'batch_dice': True, 'inherits_from': '3d_fullres'} 

These are the global plan.json settings:
 {'dataset_name': 'Dataset306_Sohee_Ajay_Calcium_OCT', 'plans_name': 'nnUNetPlans', 'original_median_spacing_after_transp': [1.0, 1.0, 1.0], 'original_median_shape_after_transp': [375, 498, 498], 'image_reader_writer': 'SimpleITKIO', 'transpose_forward': [0, 1, 2], 'transpose_backward': [0, 1, 2], 'experiment_planner_used': 'ExperimentPlanner', 'label_manager': 'LabelManager', 'foreground_intensity_properties_per_channel': {'0': {'max': 0.49803921580314636, 'mean': 0.14296366274356842, 'median': 0.10980392247438431, 'min': 0.0, 'percentile_00_5': 0.01558823511004448, 'percentile_99_5': 0.49803921580314636, 'std': 0.11562220752239227}}} 

2024-12-01 11:37:12.385661: unpacking dataset...
2024-12-01 11:37:16.139989: unpacking done...
2024-12-01 11:37:16.380352: Unable to plot network architecture: nnUNet_compile is enabled!
2024-12-01 11:37:16.468733: 
2024-12-01 11:37:16.471009: Epoch 0
2024-12-01 11:37:16.472497: Current learning rate: 0.01
2024-12-01 11:39:42.947332: Validation loss improved from 1000.00000 to -0.28661! Patience: 0/50
2024-12-01 11:39:42.948760: train_loss -0.2509
2024-12-01 11:39:42.950098: val_loss -0.2866
2024-12-01 11:39:42.950994: Pseudo dice [0.5568]
2024-12-01 11:39:42.951989: Epoch time: 146.48 s
2024-12-01 11:39:42.952877: Yayy! New best EMA pseudo Dice: 0.5568
2024-12-01 11:39:45.546178: 
2024-12-01 11:39:45.548307: Epoch 1
2024-12-01 11:39:45.549533: Current learning rate: 0.00999
2024-12-01 11:41:11.719559: Validation loss did not improve from -0.28661. Patience: 1/50
2024-12-01 11:41:11.720871: train_loss -0.4577
2024-12-01 11:41:11.721916: val_loss -0.2479
2024-12-01 11:41:11.722951: Pseudo dice [0.537]
2024-12-01 11:41:11.724009: Epoch time: 86.18 s
2024-12-01 11:41:12.923463: 
2024-12-01 11:41:12.925095: Epoch 2
2024-12-01 11:41:12.926141: Current learning rate: 0.00998
2024-12-01 11:42:39.392890: Validation loss improved from -0.28661 to -0.31420! Patience: 1/50
2024-12-01 11:42:39.394293: train_loss -0.4937
2024-12-01 11:42:39.396050: val_loss -0.3142
2024-12-01 11:42:39.396804: Pseudo dice [0.5656]
2024-12-01 11:42:39.397571: Epoch time: 86.47 s
2024-12-01 11:42:40.652358: 
2024-12-01 11:42:40.654579: Epoch 3
2024-12-01 11:42:40.655559: Current learning rate: 0.00997
2024-12-01 11:44:07.156768: Validation loss improved from -0.31420 to -0.35964! Patience: 0/50
2024-12-01 11:44:07.158180: train_loss -0.5143
2024-12-01 11:44:07.159012: val_loss -0.3596
2024-12-01 11:44:07.159984: Pseudo dice [0.5936]
2024-12-01 11:44:07.161080: Epoch time: 86.51 s
2024-12-01 11:44:07.162133: Yayy! New best EMA pseudo Dice: 0.5597
2024-12-01 11:44:08.698662: 
2024-12-01 11:44:08.700975: Epoch 4
2024-12-01 11:44:08.702169: Current learning rate: 0.00996
2024-12-01 11:45:35.195912: Validation loss improved from -0.35964 to -0.36054! Patience: 0/50
2024-12-01 11:45:35.196854: train_loss -0.5357
2024-12-01 11:45:35.197646: val_loss -0.3605
2024-12-01 11:45:35.198726: Pseudo dice [0.6247]
2024-12-01 11:45:35.199672: Epoch time: 86.5 s
2024-12-01 11:45:35.624331: Yayy! New best EMA pseudo Dice: 0.5662
2024-12-01 11:45:37.194194: 
2024-12-01 11:45:37.196103: Epoch 5
2024-12-01 11:45:37.196788: Current learning rate: 0.00995
2024-12-01 11:47:03.643013: Validation loss improved from -0.36054 to -0.38738! Patience: 0/50
2024-12-01 11:47:03.643817: train_loss -0.5434
2024-12-01 11:47:03.644946: val_loss -0.3874
2024-12-01 11:47:03.645885: Pseudo dice [0.6233]
2024-12-01 11:47:03.646852: Epoch time: 86.45 s
2024-12-01 11:47:03.647374: Yayy! New best EMA pseudo Dice: 0.5719
2024-12-01 11:47:05.099532: 
2024-12-01 11:47:05.101372: Epoch 6
2024-12-01 11:47:05.102502: Current learning rate: 0.00995
2024-12-01 11:48:31.361924: Validation loss improved from -0.38738 to -0.40378! Patience: 0/50
2024-12-01 11:48:31.362967: train_loss -0.5358
2024-12-01 11:48:31.363817: val_loss -0.4038
2024-12-01 11:48:31.364504: Pseudo dice [0.624]
2024-12-01 11:48:31.365130: Epoch time: 86.26 s
2024-12-01 11:48:31.365724: Yayy! New best EMA pseudo Dice: 0.5771
2024-12-01 11:48:32.871905: 
2024-12-01 11:48:32.874014: Epoch 7
2024-12-01 11:48:32.874766: Current learning rate: 0.00994
2024-12-01 11:49:59.220187: Validation loss did not improve from -0.40378. Patience: 1/50
2024-12-01 11:49:59.221649: train_loss -0.5572
2024-12-01 11:49:59.222996: val_loss -0.3927
2024-12-01 11:49:59.224077: Pseudo dice [0.6199]
2024-12-01 11:49:59.225304: Epoch time: 86.35 s
2024-12-01 11:49:59.226482: Yayy! New best EMA pseudo Dice: 0.5814
2024-12-01 11:50:01.157335: 
2024-12-01 11:50:01.159754: Epoch 8
2024-12-01 11:50:01.160497: Current learning rate: 0.00993
2024-12-01 11:51:28.046404: Validation loss improved from -0.40378 to -0.41167! Patience: 1/50
2024-12-01 11:51:28.047926: train_loss -0.573
2024-12-01 11:51:28.049264: val_loss -0.4117
2024-12-01 11:51:28.049819: Pseudo dice [0.6296]
2024-12-01 11:51:28.050357: Epoch time: 86.89 s
2024-12-01 11:51:28.050870: Yayy! New best EMA pseudo Dice: 0.5862
2024-12-01 11:51:29.631162: 
2024-12-01 11:51:29.632968: Epoch 9
2024-12-01 11:51:29.634083: Current learning rate: 0.00992
2024-12-01 11:52:56.521092: Validation loss did not improve from -0.41167. Patience: 1/50
2024-12-01 11:52:56.522224: train_loss -0.5717
2024-12-01 11:52:56.522880: val_loss -0.3863
2024-12-01 11:52:56.523408: Pseudo dice [0.6331]
2024-12-01 11:52:56.523937: Epoch time: 86.89 s
2024-12-01 11:52:56.912916: Yayy! New best EMA pseudo Dice: 0.5909
2024-12-01 11:52:58.449069: 
2024-12-01 11:52:58.450956: Epoch 10
2024-12-01 11:52:58.452091: Current learning rate: 0.00991
2024-12-01 11:54:25.246231: Validation loss did not improve from -0.41167. Patience: 2/50
2024-12-01 11:54:25.247027: train_loss -0.579
2024-12-01 11:54:25.247784: val_loss -0.4086
2024-12-01 11:54:25.248500: Pseudo dice [0.6337]
2024-12-01 11:54:25.249269: Epoch time: 86.8 s
2024-12-01 11:54:25.249806: Yayy! New best EMA pseudo Dice: 0.5952
2024-12-01 11:54:26.739339: 
2024-12-01 11:54:26.741999: Epoch 11
2024-12-01 11:54:26.742858: Current learning rate: 0.0099
2024-12-01 11:55:53.888544: Validation loss did not improve from -0.41167. Patience: 3/50
2024-12-01 11:55:53.889944: train_loss -0.5828
2024-12-01 11:55:53.891293: val_loss -0.3975
2024-12-01 11:55:53.892221: Pseudo dice [0.6162]
2024-12-01 11:55:53.893375: Epoch time: 87.15 s
2024-12-01 11:55:53.894340: Yayy! New best EMA pseudo Dice: 0.5973
2024-12-01 11:55:55.354413: 
2024-12-01 11:55:55.356244: Epoch 12
2024-12-01 11:55:55.357427: Current learning rate: 0.00989
2024-12-01 11:57:22.301361: Validation loss improved from -0.41167 to -0.41416! Patience: 3/50
2024-12-01 11:57:22.302439: train_loss -0.5925
2024-12-01 11:57:22.303388: val_loss -0.4142
2024-12-01 11:57:22.304150: Pseudo dice [0.6315]
2024-12-01 11:57:22.304655: Epoch time: 86.95 s
2024-12-01 11:57:22.305212: Yayy! New best EMA pseudo Dice: 0.6007
2024-12-01 11:57:23.814177: 
2024-12-01 11:57:23.816229: Epoch 13
2024-12-01 11:57:23.817330: Current learning rate: 0.00988
2024-12-01 11:58:50.639405: Validation loss did not improve from -0.41416. Patience: 1/50
2024-12-01 11:58:50.640605: train_loss -0.5882
2024-12-01 11:58:50.641588: val_loss -0.3925
2024-12-01 11:58:50.642484: Pseudo dice [0.6151]
2024-12-01 11:58:50.643500: Epoch time: 86.83 s
2024-12-01 11:58:50.644233: Yayy! New best EMA pseudo Dice: 0.6021
2024-12-01 11:58:52.190454: 
2024-12-01 11:58:52.192648: Epoch 14
2024-12-01 11:58:52.193797: Current learning rate: 0.00987
2024-12-01 12:00:18.966703: Validation loss did not improve from -0.41416. Patience: 2/50
2024-12-01 12:00:18.968218: train_loss -0.6033
2024-12-01 12:00:18.969190: val_loss -0.4141
2024-12-01 12:00:18.969781: Pseudo dice [0.6464]
2024-12-01 12:00:18.970596: Epoch time: 86.78 s
2024-12-01 12:00:19.317267: Yayy! New best EMA pseudo Dice: 0.6066
2024-12-01 12:00:20.845757: 
2024-12-01 12:00:20.847848: Epoch 15
2024-12-01 12:00:20.848773: Current learning rate: 0.00986
2024-12-01 12:01:47.644216: Validation loss did not improve from -0.41416. Patience: 3/50
2024-12-01 12:01:47.645661: train_loss -0.607
2024-12-01 12:01:47.646921: val_loss -0.4123
2024-12-01 12:01:47.647890: Pseudo dice [0.6418]
2024-12-01 12:01:47.648756: Epoch time: 86.8 s
2024-12-01 12:01:47.649771: Yayy! New best EMA pseudo Dice: 0.6101
2024-12-01 12:01:49.175944: 
2024-12-01 12:01:49.178222: Epoch 16
2024-12-01 12:01:49.179047: Current learning rate: 0.00986
2024-12-01 12:03:16.418675: Validation loss improved from -0.41416 to -0.42172! Patience: 3/50
2024-12-01 12:03:16.419390: train_loss -0.6182
2024-12-01 12:03:16.420360: val_loss -0.4217
2024-12-01 12:03:16.420933: Pseudo dice [0.6468]
2024-12-01 12:03:16.421801: Epoch time: 87.24 s
2024-12-01 12:03:16.422444: Yayy! New best EMA pseudo Dice: 0.6138
2024-12-01 12:03:17.951583: 
2024-12-01 12:03:17.953183: Epoch 17
2024-12-01 12:03:17.954512: Current learning rate: 0.00985
2024-12-01 12:04:45.367313: Validation loss improved from -0.42172 to -0.45949! Patience: 0/50
2024-12-01 12:04:45.368940: train_loss -0.6084
2024-12-01 12:04:45.369979: val_loss -0.4595
2024-12-01 12:04:45.371057: Pseudo dice [0.6617]
2024-12-01 12:04:45.372375: Epoch time: 87.42 s
2024-12-01 12:04:45.372897: Yayy! New best EMA pseudo Dice: 0.6186
2024-12-01 12:04:47.288924: 
2024-12-01 12:04:47.291285: Epoch 18
2024-12-01 12:04:47.292022: Current learning rate: 0.00984
2024-12-01 12:06:14.645568: Validation loss did not improve from -0.45949. Patience: 1/50
2024-12-01 12:06:14.646509: train_loss -0.6151
2024-12-01 12:06:14.647732: val_loss -0.4051
2024-12-01 12:06:14.648736: Pseudo dice [0.6327]
2024-12-01 12:06:14.649673: Epoch time: 87.36 s
2024-12-01 12:06:14.650942: Yayy! New best EMA pseudo Dice: 0.62
2024-12-01 12:06:16.217587: 
2024-12-01 12:06:16.219287: Epoch 19
2024-12-01 12:06:16.220470: Current learning rate: 0.00983
2024-12-01 12:07:43.484247: Validation loss did not improve from -0.45949. Patience: 2/50
2024-12-01 12:07:43.485179: train_loss -0.6228
2024-12-01 12:07:43.486606: val_loss -0.4177
2024-12-01 12:07:43.487652: Pseudo dice [0.6359]
2024-12-01 12:07:43.488894: Epoch time: 87.27 s
2024-12-01 12:07:43.839250: Yayy! New best EMA pseudo Dice: 0.6216
2024-12-01 12:07:45.390994: 
2024-12-01 12:07:45.393335: Epoch 20
2024-12-01 12:07:45.394271: Current learning rate: 0.00982
2024-12-01 12:09:12.643082: Validation loss did not improve from -0.45949. Patience: 3/50
2024-12-01 12:09:12.644499: train_loss -0.6106
2024-12-01 12:09:12.645488: val_loss -0.3753
2024-12-01 12:09:12.646609: Pseudo dice [0.6154]
2024-12-01 12:09:12.647423: Epoch time: 87.25 s
2024-12-01 12:09:13.877835: 
2024-12-01 12:09:13.879638: Epoch 21
2024-12-01 12:09:13.880426: Current learning rate: 0.00981
2024-12-01 12:10:41.180563: Validation loss did not improve from -0.45949. Patience: 4/50
2024-12-01 12:10:41.181979: train_loss -0.6159
2024-12-01 12:10:41.183220: val_loss -0.3955
2024-12-01 12:10:41.183878: Pseudo dice [0.6206]
2024-12-01 12:10:41.184849: Epoch time: 87.31 s
2024-12-01 12:10:42.294395: 
2024-12-01 12:10:42.296524: Epoch 22
2024-12-01 12:10:42.297427: Current learning rate: 0.0098
2024-12-01 12:12:09.538398: Validation loss did not improve from -0.45949. Patience: 5/50
2024-12-01 12:12:09.539800: train_loss -0.6247
2024-12-01 12:12:09.541529: val_loss -0.3945
2024-12-01 12:12:09.542439: Pseudo dice [0.6281]
2024-12-01 12:12:09.543269: Epoch time: 87.25 s
2024-12-01 12:12:09.544197: Yayy! New best EMA pseudo Dice: 0.6216
2024-12-01 12:12:11.074504: 
2024-12-01 12:12:11.076259: Epoch 23
2024-12-01 12:12:11.077309: Current learning rate: 0.00979
2024-12-01 12:13:38.182620: Validation loss did not improve from -0.45949. Patience: 6/50
2024-12-01 12:13:38.183820: train_loss -0.6261
2024-12-01 12:13:38.184710: val_loss -0.4336
2024-12-01 12:13:38.185333: Pseudo dice [0.634]
2024-12-01 12:13:38.186020: Epoch time: 87.11 s
2024-12-01 12:13:38.187056: Yayy! New best EMA pseudo Dice: 0.6229
2024-12-01 12:13:39.623703: 
2024-12-01 12:13:39.625866: Epoch 24
2024-12-01 12:13:39.626751: Current learning rate: 0.00978
2024-12-01 12:15:07.302704: Validation loss did not improve from -0.45949. Patience: 7/50
2024-12-01 12:15:07.303768: train_loss -0.6378
2024-12-01 12:15:07.304767: val_loss -0.4205
2024-12-01 12:15:07.305607: Pseudo dice [0.6375]
2024-12-01 12:15:07.306388: Epoch time: 87.68 s
2024-12-01 12:15:07.679312: Yayy! New best EMA pseudo Dice: 0.6243
2024-12-01 12:15:09.198334: 
2024-12-01 12:15:09.200425: Epoch 25
2024-12-01 12:15:09.201848: Current learning rate: 0.00977
2024-12-01 12:16:36.536513: Validation loss did not improve from -0.45949. Patience: 8/50
2024-12-01 12:16:36.537260: train_loss -0.6392
2024-12-01 12:16:36.537921: val_loss -0.3691
2024-12-01 12:16:36.538786: Pseudo dice [0.6178]
2024-12-01 12:16:36.539844: Epoch time: 87.34 s
2024-12-01 12:16:37.710744: 
2024-12-01 12:16:37.712664: Epoch 26
2024-12-01 12:16:37.713882: Current learning rate: 0.00977
2024-12-01 12:18:04.803029: Validation loss did not improve from -0.45949. Patience: 9/50
2024-12-01 12:18:04.803999: train_loss -0.6402
2024-12-01 12:18:04.804741: val_loss -0.3356
2024-12-01 12:18:04.805885: Pseudo dice [0.5924]
2024-12-01 12:18:04.806507: Epoch time: 87.09 s
2024-12-01 12:18:05.988941: 
2024-12-01 12:18:05.990897: Epoch 27
2024-12-01 12:18:05.992006: Current learning rate: 0.00976
2024-12-01 12:19:33.192054: Validation loss did not improve from -0.45949. Patience: 10/50
2024-12-01 12:19:33.193711: train_loss -0.6444
2024-12-01 12:19:33.195011: val_loss -0.4028
2024-12-01 12:19:33.196064: Pseudo dice [0.6311]
2024-12-01 12:19:33.196667: Epoch time: 87.21 s
2024-12-01 12:19:34.399086: 
2024-12-01 12:19:34.401156: Epoch 28
2024-12-01 12:19:34.402582: Current learning rate: 0.00975
2024-12-01 12:21:01.592609: Validation loss did not improve from -0.45949. Patience: 11/50
2024-12-01 12:21:01.593471: train_loss -0.6425
2024-12-01 12:21:01.594429: val_loss -0.4231
2024-12-01 12:21:01.595288: Pseudo dice [0.6276]
2024-12-01 12:21:01.595954: Epoch time: 87.2 s
2024-12-01 12:21:03.096203: 
2024-12-01 12:21:03.098128: Epoch 29
2024-12-01 12:21:03.099272: Current learning rate: 0.00974
2024-12-01 12:22:30.162183: Validation loss did not improve from -0.45949. Patience: 12/50
2024-12-01 12:22:30.163650: train_loss -0.659
2024-12-01 12:22:30.165073: val_loss -0.4279
2024-12-01 12:22:30.166229: Pseudo dice [0.6443]
2024-12-01 12:22:30.167593: Epoch time: 87.07 s
2024-12-01 12:22:30.546688: Yayy! New best EMA pseudo Dice: 0.6244
2024-12-01 12:22:32.022793: 
2024-12-01 12:22:32.025281: Epoch 30
2024-12-01 12:22:32.026365: Current learning rate: 0.00973
2024-12-01 12:23:59.086682: Validation loss did not improve from -0.45949. Patience: 13/50
2024-12-01 12:23:59.087312: train_loss -0.6539
2024-12-01 12:23:59.088286: val_loss -0.3885
2024-12-01 12:23:59.089459: Pseudo dice [0.622]
2024-12-01 12:23:59.090015: Epoch time: 87.07 s
2024-12-01 12:24:00.256950: 
2024-12-01 12:24:00.258972: Epoch 31
2024-12-01 12:24:00.259657: Current learning rate: 0.00972
2024-12-01 12:25:27.467240: Validation loss did not improve from -0.45949. Patience: 14/50
2024-12-01 12:25:27.468434: train_loss -0.6543
2024-12-01 12:25:27.469581: val_loss -0.4069
2024-12-01 12:25:27.470437: Pseudo dice [0.6234]
2024-12-01 12:25:27.471251: Epoch time: 87.21 s
2024-12-01 12:25:28.637503: 
2024-12-01 12:25:28.639779: Epoch 32
2024-12-01 12:25:28.640676: Current learning rate: 0.00971
2024-12-01 12:26:55.797078: Validation loss did not improve from -0.45949. Patience: 15/50
2024-12-01 12:26:55.798166: train_loss -0.6532
2024-12-01 12:26:55.798905: val_loss -0.3406
2024-12-01 12:26:55.799928: Pseudo dice [0.6052]
2024-12-01 12:26:55.801062: Epoch time: 87.16 s
2024-12-01 12:26:56.976625: 
2024-12-01 12:26:56.978796: Epoch 33
2024-12-01 12:26:56.979779: Current learning rate: 0.0097
2024-12-01 12:28:24.509074: Validation loss did not improve from -0.45949. Patience: 16/50
2024-12-01 12:28:24.510185: train_loss -0.6604
2024-12-01 12:28:24.511215: val_loss -0.4178
2024-12-01 12:28:24.512082: Pseudo dice [0.6525]
2024-12-01 12:28:24.513106: Epoch time: 87.53 s
2024-12-01 12:28:24.514181: Yayy! New best EMA pseudo Dice: 0.6252
2024-12-01 12:28:26.086735: 
2024-12-01 12:28:26.088689: Epoch 34
2024-12-01 12:28:26.089958: Current learning rate: 0.00969
2024-12-01 12:29:53.735268: Validation loss did not improve from -0.45949. Patience: 17/50
2024-12-01 12:29:53.736481: train_loss -0.653
2024-12-01 12:29:53.737350: val_loss -0.4023
2024-12-01 12:29:53.738410: Pseudo dice [0.6194]
2024-12-01 12:29:53.739408: Epoch time: 87.65 s
2024-12-01 12:29:55.310424: 
2024-12-01 12:29:55.312117: Epoch 35
2024-12-01 12:29:55.312726: Current learning rate: 0.00968
2024-12-01 12:31:22.914087: Validation loss did not improve from -0.45949. Patience: 18/50
2024-12-01 12:31:22.915031: train_loss -0.6467
2024-12-01 12:31:22.916448: val_loss -0.4238
2024-12-01 12:31:22.917713: Pseudo dice [0.6382]
2024-12-01 12:31:22.919053: Epoch time: 87.61 s
2024-12-01 12:31:22.920191: Yayy! New best EMA pseudo Dice: 0.626
2024-12-01 12:31:24.471411: 
2024-12-01 12:31:24.473324: Epoch 36
2024-12-01 12:31:24.474740: Current learning rate: 0.00968
2024-12-01 12:32:52.081258: Validation loss did not improve from -0.45949. Patience: 19/50
2024-12-01 12:32:52.082107: train_loss -0.6563
2024-12-01 12:32:52.083339: val_loss -0.3778
2024-12-01 12:32:52.084044: Pseudo dice [0.6254]
2024-12-01 12:32:52.084932: Epoch time: 87.61 s
2024-12-01 12:32:53.287199: 
2024-12-01 12:32:53.289449: Epoch 37
2024-12-01 12:32:53.290546: Current learning rate: 0.00967
2024-12-01 12:34:20.899109: Validation loss did not improve from -0.45949. Patience: 20/50
2024-12-01 12:34:20.900500: train_loss -0.6566
2024-12-01 12:34:20.901911: val_loss -0.3887
2024-12-01 12:34:20.902907: Pseudo dice [0.6243]
2024-12-01 12:34:20.903835: Epoch time: 87.61 s
2024-12-01 12:34:22.158681: 
2024-12-01 12:34:22.160710: Epoch 38
2024-12-01 12:34:22.161773: Current learning rate: 0.00966
2024-12-01 12:35:49.689693: Validation loss did not improve from -0.45949. Patience: 21/50
2024-12-01 12:35:49.691321: train_loss -0.6677
2024-12-01 12:35:49.692425: val_loss -0.4112
2024-12-01 12:35:49.692965: Pseudo dice [0.6372]
2024-12-01 12:35:49.693474: Epoch time: 87.53 s
2024-12-01 12:35:49.693960: Yayy! New best EMA pseudo Dice: 0.6269
2024-12-01 12:35:51.248506: 
2024-12-01 12:35:51.250777: Epoch 39
2024-12-01 12:35:51.251935: Current learning rate: 0.00965
2024-12-01 12:37:18.690826: Validation loss did not improve from -0.45949. Patience: 22/50
2024-12-01 12:37:18.692253: train_loss -0.6703
2024-12-01 12:37:18.693506: val_loss -0.4289
2024-12-01 12:37:18.694409: Pseudo dice [0.6409]
2024-12-01 12:37:18.695234: Epoch time: 87.44 s
2024-12-01 12:37:19.045106: Yayy! New best EMA pseudo Dice: 0.6283
2024-12-01 12:37:20.958071: 
2024-12-01 12:37:20.960488: Epoch 40
2024-12-01 12:37:20.961586: Current learning rate: 0.00964
2024-12-01 12:38:48.661471: Validation loss did not improve from -0.45949. Patience: 23/50
2024-12-01 12:38:48.662608: train_loss -0.6602
2024-12-01 12:38:48.663358: val_loss -0.4453
2024-12-01 12:38:48.664007: Pseudo dice [0.6557]
2024-12-01 12:38:48.664715: Epoch time: 87.71 s
2024-12-01 12:38:48.665884: Yayy! New best EMA pseudo Dice: 0.631
2024-12-01 12:38:50.225367: 
2024-12-01 12:38:50.227189: Epoch 41
2024-12-01 12:38:50.228296: Current learning rate: 0.00963
2024-12-01 12:40:17.862019: Validation loss did not improve from -0.45949. Patience: 24/50
2024-12-01 12:40:17.863558: train_loss -0.6636
2024-12-01 12:40:17.864786: val_loss -0.3553
2024-12-01 12:40:17.865750: Pseudo dice [0.6047]
2024-12-01 12:40:17.866814: Epoch time: 87.64 s
2024-12-01 12:40:19.049078: 
2024-12-01 12:40:19.051048: Epoch 42
2024-12-01 12:40:19.052226: Current learning rate: 0.00962
2024-12-01 12:41:47.052250: Validation loss did not improve from -0.45949. Patience: 25/50
2024-12-01 12:41:47.053518: train_loss -0.6677
2024-12-01 12:41:47.054976: val_loss -0.4242
2024-12-01 12:41:47.055679: Pseudo dice [0.6346]
2024-12-01 12:41:47.056657: Epoch time: 88.01 s
2024-12-01 12:41:48.225128: 
2024-12-01 12:41:48.227323: Epoch 43
2024-12-01 12:41:48.228572: Current learning rate: 0.00961
2024-12-01 12:43:23.110906: Validation loss did not improve from -0.45949. Patience: 26/50
2024-12-01 12:43:23.124119: train_loss -0.6746
2024-12-01 12:43:23.188676: val_loss -0.3917
2024-12-01 12:43:23.190470: Pseudo dice [0.6206]
2024-12-01 12:43:23.194353: Epoch time: 94.89 s
2024-12-01 12:43:25.309688: 
2024-12-01 12:43:25.311467: Epoch 44
2024-12-01 12:43:25.312225: Current learning rate: 0.0096
2024-12-01 12:44:52.877205: Validation loss did not improve from -0.45949. Patience: 27/50
2024-12-01 12:44:52.878450: train_loss -0.6665
2024-12-01 12:44:52.879536: val_loss -0.4322
2024-12-01 12:44:52.880371: Pseudo dice [0.6533]
2024-12-01 12:44:52.881349: Epoch time: 87.57 s
2024-12-01 12:44:55.036508: 
2024-12-01 12:44:55.038390: Epoch 45
2024-12-01 12:44:55.039519: Current learning rate: 0.00959
2024-12-01 12:46:22.719644: Validation loss did not improve from -0.45949. Patience: 28/50
2024-12-01 12:46:22.720705: train_loss -0.6765
2024-12-01 12:46:22.721755: val_loss -0.4148
2024-12-01 12:46:22.722580: Pseudo dice [0.6465]
2024-12-01 12:46:22.723441: Epoch time: 87.69 s
2024-12-01 12:46:22.724120: Yayy! New best EMA pseudo Dice: 0.6323
2024-12-01 12:46:24.206180: 
2024-12-01 12:46:24.208014: Epoch 46
2024-12-01 12:46:24.209290: Current learning rate: 0.00959
2024-12-01 12:47:51.760878: Validation loss did not improve from -0.45949. Patience: 29/50
2024-12-01 12:47:51.761925: train_loss -0.6845
2024-12-01 12:47:51.762721: val_loss -0.4321
2024-12-01 12:47:51.763645: Pseudo dice [0.6452]
2024-12-01 12:47:51.764682: Epoch time: 87.56 s
2024-12-01 12:47:51.765310: Yayy! New best EMA pseudo Dice: 0.6336
2024-12-01 12:47:53.280105: 
2024-12-01 12:47:53.282130: Epoch 47
2024-12-01 12:47:53.283068: Current learning rate: 0.00958
2024-12-01 12:49:20.710066: Validation loss did not improve from -0.45949. Patience: 30/50
2024-12-01 12:49:20.711537: train_loss -0.6781
2024-12-01 12:49:20.713433: val_loss -0.4445
2024-12-01 12:49:20.714187: Pseudo dice [0.6563]
2024-12-01 12:49:20.714993: Epoch time: 87.43 s
2024-12-01 12:49:20.715726: Yayy! New best EMA pseudo Dice: 0.6358
2024-12-01 12:49:22.251890: 
2024-12-01 12:49:22.253659: Epoch 48
2024-12-01 12:49:22.254349: Current learning rate: 0.00957
2024-12-01 12:50:49.702771: Validation loss did not improve from -0.45949. Patience: 31/50
2024-12-01 12:50:49.703940: train_loss -0.6795
2024-12-01 12:50:49.704878: val_loss -0.377
2024-12-01 12:50:49.705488: Pseudo dice [0.6226]
2024-12-01 12:50:49.706347: Epoch time: 87.45 s
2024-12-01 12:50:50.865723: 
2024-12-01 12:50:50.867810: Epoch 49
2024-12-01 12:50:50.868654: Current learning rate: 0.00956
2024-12-01 12:52:18.511188: Validation loss did not improve from -0.45949. Patience: 32/50
2024-12-01 12:52:18.512250: train_loss -0.6801
2024-12-01 12:52:18.513389: val_loss -0.4005
2024-12-01 12:52:18.514557: Pseudo dice [0.6164]
2024-12-01 12:52:18.515472: Epoch time: 87.65 s
2024-12-01 12:52:22.481286: 
2024-12-01 12:52:22.483813: Epoch 50
2024-12-01 12:52:22.485062: Current learning rate: 0.00955
2024-12-01 12:53:49.969698: Validation loss did not improve from -0.45949. Patience: 33/50
2024-12-01 12:53:49.971291: train_loss -0.6686
2024-12-01 12:53:49.972203: val_loss -0.3976
2024-12-01 12:53:49.972959: Pseudo dice [0.6217]
2024-12-01 12:53:49.973804: Epoch time: 87.49 s
2024-12-01 12:53:51.155131: 
2024-12-01 12:53:51.157190: Epoch 51
2024-12-01 12:53:51.158144: Current learning rate: 0.00954
2024-12-01 12:55:18.783183: Validation loss did not improve from -0.45949. Patience: 34/50
2024-12-01 12:55:18.784281: train_loss -0.6803
2024-12-01 12:55:18.785719: val_loss -0.4357
2024-12-01 12:55:18.786947: Pseudo dice [0.6381]
2024-12-01 12:55:18.788027: Epoch time: 87.63 s
2024-12-01 12:55:19.949490: 
2024-12-01 12:55:19.951110: Epoch 52
2024-12-01 12:55:19.952373: Current learning rate: 0.00953
2024-12-01 12:56:47.723951: Validation loss did not improve from -0.45949. Patience: 35/50
2024-12-01 12:56:47.725123: train_loss -0.6732
2024-12-01 12:56:47.726254: val_loss -0.4352
2024-12-01 12:56:47.727408: Pseudo dice [0.6414]
2024-12-01 12:56:47.728284: Epoch time: 87.78 s
2024-12-01 12:56:48.900011: 
2024-12-01 12:56:48.902233: Epoch 53
2024-12-01 12:56:48.902868: Current learning rate: 0.00952
2024-12-01 12:58:16.643077: Validation loss did not improve from -0.45949. Patience: 36/50
2024-12-01 12:58:16.644073: train_loss -0.6829
2024-12-01 12:58:16.645345: val_loss -0.3671
2024-12-01 12:58:16.646748: Pseudo dice [0.6125]
2024-12-01 12:58:16.648046: Epoch time: 87.75 s
2024-12-01 12:58:17.804804: 
2024-12-01 12:58:17.806385: Epoch 54
2024-12-01 12:58:17.807540: Current learning rate: 0.00951
2024-12-01 12:59:45.580897: Validation loss did not improve from -0.45949. Patience: 37/50
2024-12-01 12:59:45.582230: train_loss -0.6907
2024-12-01 12:59:45.583054: val_loss -0.3754
2024-12-01 12:59:45.584209: Pseudo dice [0.6229]
2024-12-01 12:59:45.585145: Epoch time: 87.78 s
2024-12-01 12:59:47.196409: 
2024-12-01 12:59:47.198483: Epoch 55
2024-12-01 12:59:47.199492: Current learning rate: 0.0095
2024-12-01 13:01:15.199077: Validation loss did not improve from -0.45949. Patience: 38/50
2024-12-01 13:01:15.200571: train_loss -0.684
2024-12-01 13:01:15.201378: val_loss -0.3816
2024-12-01 13:01:15.202024: Pseudo dice [0.6072]
2024-12-01 13:01:15.202631: Epoch time: 88.01 s
2024-12-01 13:01:16.384798: 
2024-12-01 13:01:16.386670: Epoch 56
2024-12-01 13:01:16.387499: Current learning rate: 0.00949
2024-12-01 13:02:44.242542: Validation loss improved from -0.45949 to -0.46027! Patience: 38/50
2024-12-01 13:02:44.243410: train_loss -0.6874
2024-12-01 13:02:44.244315: val_loss -0.4603
2024-12-01 13:02:44.245430: Pseudo dice [0.664]
2024-12-01 13:02:44.246261: Epoch time: 87.86 s
2024-12-01 13:02:45.422016: 
2024-12-01 13:02:45.424218: Epoch 57
2024-12-01 13:02:45.425414: Current learning rate: 0.00949
2024-12-01 13:04:12.972394: Validation loss did not improve from -0.46027. Patience: 1/50
2024-12-01 13:04:12.973299: train_loss -0.6844
2024-12-01 13:04:12.974810: val_loss -0.4051
2024-12-01 13:04:12.975846: Pseudo dice [0.6199]
2024-12-01 13:04:12.976456: Epoch time: 87.55 s
2024-12-01 13:04:14.137884: 
2024-12-01 13:04:14.139848: Epoch 58
2024-12-01 13:04:14.141153: Current learning rate: 0.00948
2024-12-01 13:05:41.779302: Validation loss did not improve from -0.46027. Patience: 2/50
2024-12-01 13:05:41.780006: train_loss -0.6802
2024-12-01 13:05:41.780676: val_loss -0.4127
2024-12-01 13:05:41.781283: Pseudo dice [0.6376]
2024-12-01 13:05:41.781954: Epoch time: 87.64 s
2024-12-01 13:05:42.974612: 
2024-12-01 13:05:42.976521: Epoch 59
2024-12-01 13:05:42.977807: Current learning rate: 0.00947
2024-12-01 13:07:10.513603: Validation loss did not improve from -0.46027. Patience: 3/50
2024-12-01 13:07:10.514502: train_loss -0.6874
2024-12-01 13:07:10.515468: val_loss -0.4267
2024-12-01 13:07:10.516426: Pseudo dice [0.6519]
2024-12-01 13:07:10.517188: Epoch time: 87.54 s
2024-12-01 13:07:12.037851: 
2024-12-01 13:07:12.040179: Epoch 60
2024-12-01 13:07:12.041404: Current learning rate: 0.00946
2024-12-01 13:08:39.534178: Validation loss did not improve from -0.46027. Patience: 4/50
2024-12-01 13:08:39.535450: train_loss -0.6831
2024-12-01 13:08:39.536319: val_loss -0.4439
2024-12-01 13:08:39.536947: Pseudo dice [0.6489]
2024-12-01 13:08:39.537637: Epoch time: 87.5 s
2024-12-01 13:08:40.701111: 
2024-12-01 13:08:40.702991: Epoch 61
2024-12-01 13:08:40.704126: Current learning rate: 0.00945
2024-12-01 13:10:08.252622: Validation loss did not improve from -0.46027. Patience: 5/50
2024-12-01 13:10:08.253689: train_loss -0.6913
2024-12-01 13:10:08.254776: val_loss -0.3985
2024-12-01 13:10:08.255466: Pseudo dice [0.6312]
2024-12-01 13:10:08.256366: Epoch time: 87.55 s
2024-12-01 13:10:09.789529: 
2024-12-01 13:10:09.791118: Epoch 62
2024-12-01 13:10:09.791774: Current learning rate: 0.00944
2024-12-01 13:11:37.344502: Validation loss did not improve from -0.46027. Patience: 6/50
2024-12-01 13:11:37.345226: train_loss -0.6811
2024-12-01 13:11:37.345893: val_loss -0.3736
2024-12-01 13:11:37.346693: Pseudo dice [0.618]
2024-12-01 13:11:37.347291: Epoch time: 87.56 s
2024-12-01 13:11:38.545182: 
2024-12-01 13:11:38.547272: Epoch 63
2024-12-01 13:11:38.548347: Current learning rate: 0.00943
2024-12-01 13:13:06.048701: Validation loss did not improve from -0.46027. Patience: 7/50
2024-12-01 13:13:06.050019: train_loss -0.6911
2024-12-01 13:13:06.051586: val_loss -0.4318
2024-12-01 13:13:06.052462: Pseudo dice [0.6588]
2024-12-01 13:13:06.053450: Epoch time: 87.51 s
2024-12-01 13:13:07.223887: 
2024-12-01 13:13:07.226229: Epoch 64
2024-12-01 13:13:07.227125: Current learning rate: 0.00942
2024-12-01 13:14:34.978477: Validation loss did not improve from -0.46027. Patience: 8/50
2024-12-01 13:14:34.979662: train_loss -0.6936
2024-12-01 13:14:34.980661: val_loss -0.4154
2024-12-01 13:14:34.981682: Pseudo dice [0.6382]
2024-12-01 13:14:34.982553: Epoch time: 87.76 s
2024-12-01 13:14:36.548464: 
2024-12-01 13:14:36.550422: Epoch 65
2024-12-01 13:14:36.551749: Current learning rate: 0.00941
2024-12-01 13:16:04.429682: Validation loss improved from -0.46027 to -0.46175! Patience: 8/50
2024-12-01 13:16:04.430708: train_loss -0.6928
2024-12-01 13:16:04.431686: val_loss -0.4617
2024-12-01 13:16:04.432620: Pseudo dice [0.6653]
2024-12-01 13:16:04.433408: Epoch time: 87.88 s
2024-12-01 13:16:04.434284: Yayy! New best EMA pseudo Dice: 0.6386
2024-12-01 13:16:05.992969: 
2024-12-01 13:16:05.994475: Epoch 66
2024-12-01 13:16:05.995384: Current learning rate: 0.0094
2024-12-01 13:17:33.836921: Validation loss did not improve from -0.46175. Patience: 1/50
2024-12-01 13:17:33.837962: train_loss -0.6948
2024-12-01 13:17:33.838925: val_loss -0.3758
2024-12-01 13:17:33.839715: Pseudo dice [0.6056]
2024-12-01 13:17:33.840498: Epoch time: 87.85 s
2024-12-01 13:17:35.046750: 
2024-12-01 13:17:35.048402: Epoch 67
2024-12-01 13:17:35.049147: Current learning rate: 0.00939
2024-12-01 13:19:03.047835: Validation loss did not improve from -0.46175. Patience: 2/50
2024-12-01 13:19:03.049144: train_loss -0.6953
2024-12-01 13:19:03.050244: val_loss -0.3961
2024-12-01 13:19:03.051089: Pseudo dice [0.6245]
2024-12-01 13:19:03.052348: Epoch time: 88.0 s
2024-12-01 13:19:04.259550: 
2024-12-01 13:19:04.260877: Epoch 68
2024-12-01 13:19:04.261768: Current learning rate: 0.00939
2024-12-01 13:20:32.321063: Validation loss did not improve from -0.46175. Patience: 3/50
2024-12-01 13:20:32.322559: train_loss -0.6941
2024-12-01 13:20:32.324172: val_loss -0.3971
2024-12-01 13:20:32.325070: Pseudo dice [0.6411]
2024-12-01 13:20:32.326539: Epoch time: 88.06 s
2024-12-01 13:20:33.546544: 
2024-12-01 13:20:33.548244: Epoch 69
2024-12-01 13:20:33.549046: Current learning rate: 0.00938
2024-12-01 13:22:01.335630: Validation loss did not improve from -0.46175. Patience: 4/50
2024-12-01 13:22:01.337272: train_loss -0.6858
2024-12-01 13:22:01.339105: val_loss -0.3744
2024-12-01 13:22:01.339931: Pseudo dice [0.6235]
2024-12-01 13:22:01.340641: Epoch time: 87.79 s
2024-12-01 13:22:02.896287: 
2024-12-01 13:22:02.898232: Epoch 70
2024-12-01 13:22:02.898911: Current learning rate: 0.00937
2024-12-01 13:23:30.544863: Validation loss did not improve from -0.46175. Patience: 5/50
2024-12-01 13:23:30.546274: train_loss -0.7011
2024-12-01 13:23:30.546931: val_loss -0.4172
2024-12-01 13:23:30.547683: Pseudo dice [0.636]
2024-12-01 13:23:30.548585: Epoch time: 87.65 s
2024-12-01 13:23:31.747772: 
2024-12-01 13:23:31.749634: Epoch 71
2024-12-01 13:23:31.750474: Current learning rate: 0.00936
2024-12-01 13:24:59.443297: Validation loss did not improve from -0.46175. Patience: 6/50
2024-12-01 13:24:59.444608: train_loss -0.6884
2024-12-01 13:24:59.445915: val_loss -0.4386
2024-12-01 13:24:59.446565: Pseudo dice [0.6583]
2024-12-01 13:24:59.447179: Epoch time: 87.7 s
2024-12-01 13:25:00.991305: 
2024-12-01 13:25:00.993642: Epoch 72
2024-12-01 13:25:00.994591: Current learning rate: 0.00935
2024-12-01 13:26:28.471431: Validation loss did not improve from -0.46175. Patience: 7/50
2024-12-01 13:26:28.472877: train_loss -0.6993
2024-12-01 13:26:28.474249: val_loss -0.4078
2024-12-01 13:26:28.475335: Pseudo dice [0.6182]
2024-12-01 13:26:28.476342: Epoch time: 87.48 s
2024-12-01 13:26:29.671521: 
2024-12-01 13:26:29.673348: Epoch 73
2024-12-01 13:26:29.674582: Current learning rate: 0.00934
2024-12-01 13:27:57.287209: Validation loss did not improve from -0.46175. Patience: 8/50
2024-12-01 13:27:57.288317: train_loss -0.7014
2024-12-01 13:27:57.289353: val_loss -0.394
2024-12-01 13:27:57.290439: Pseudo dice [0.6309]
2024-12-01 13:27:57.291327: Epoch time: 87.62 s
2024-12-01 13:27:58.494571: 
2024-12-01 13:27:58.496793: Epoch 74
2024-12-01 13:27:58.498232: Current learning rate: 0.00933
2024-12-01 13:29:26.089951: Validation loss did not improve from -0.46175. Patience: 9/50
2024-12-01 13:29:26.091396: train_loss -0.697
2024-12-01 13:29:26.092583: val_loss -0.4162
2024-12-01 13:29:26.093527: Pseudo dice [0.6374]
2024-12-01 13:29:26.094248: Epoch time: 87.6 s
2024-12-01 13:29:27.667796: 
2024-12-01 13:29:27.669799: Epoch 75
2024-12-01 13:29:27.670844: Current learning rate: 0.00932
2024-12-01 13:30:55.257940: Validation loss did not improve from -0.46175. Patience: 10/50
2024-12-01 13:30:55.259274: train_loss -0.7056
2024-12-01 13:30:55.260402: val_loss -0.4069
2024-12-01 13:30:55.261476: Pseudo dice [0.6367]
2024-12-01 13:30:55.262419: Epoch time: 87.59 s
2024-12-01 13:30:56.474837: 
2024-12-01 13:30:56.476310: Epoch 76
2024-12-01 13:30:56.477489: Current learning rate: 0.00931
2024-12-01 13:32:24.012115: Validation loss did not improve from -0.46175. Patience: 11/50
2024-12-01 13:32:24.012987: train_loss -0.7105
2024-12-01 13:32:24.013684: val_loss -0.4339
2024-12-01 13:32:24.014293: Pseudo dice [0.6492]
2024-12-01 13:32:24.015233: Epoch time: 87.54 s
2024-12-01 13:32:25.225233: 
2024-12-01 13:32:25.226926: Epoch 77
2024-12-01 13:32:25.228115: Current learning rate: 0.0093
2024-12-01 13:33:53.126225: Validation loss did not improve from -0.46175. Patience: 12/50
2024-12-01 13:33:53.127395: train_loss -0.7022
2024-12-01 13:33:53.128300: val_loss -0.3953
2024-12-01 13:33:53.129400: Pseudo dice [0.6348]
2024-12-01 13:33:53.130573: Epoch time: 87.9 s
2024-12-01 13:33:54.372490: 
2024-12-01 13:33:54.374321: Epoch 78
2024-12-01 13:33:54.375260: Current learning rate: 0.0093
2024-12-01 13:35:22.267446: Validation loss did not improve from -0.46175. Patience: 13/50
2024-12-01 13:35:22.268670: train_loss -0.7012
2024-12-01 13:35:22.269419: val_loss -0.3837
2024-12-01 13:35:22.270088: Pseudo dice [0.6338]
2024-12-01 13:35:22.271030: Epoch time: 87.9 s
2024-12-01 13:35:23.496324: 
2024-12-01 13:35:23.498137: Epoch 79
2024-12-01 13:35:23.498786: Current learning rate: 0.00929
2024-12-01 13:36:51.411343: Validation loss did not improve from -0.46175. Patience: 14/50
2024-12-01 13:36:51.412324: train_loss -0.6981
2024-12-01 13:36:51.413113: val_loss -0.4389
2024-12-01 13:36:51.414169: Pseudo dice [0.6554]
2024-12-01 13:36:51.414892: Epoch time: 87.92 s
2024-12-01 13:36:52.973338: 
2024-12-01 13:36:52.975265: Epoch 80
2024-12-01 13:36:52.976101: Current learning rate: 0.00928
2024-12-01 13:38:20.671304: Validation loss did not improve from -0.46175. Patience: 15/50
2024-12-01 13:38:20.672650: train_loss -0.7049
2024-12-01 13:38:20.674122: val_loss -0.3552
2024-12-01 13:38:20.675363: Pseudo dice [0.5886]
2024-12-01 13:38:20.676038: Epoch time: 87.7 s
2024-12-01 13:38:21.907657: 
2024-12-01 13:38:21.909945: Epoch 81
2024-12-01 13:38:21.910613: Current learning rate: 0.00927
2024-12-01 13:39:49.641117: Validation loss did not improve from -0.46175. Patience: 16/50
2024-12-01 13:39:49.642209: train_loss -0.7008
2024-12-01 13:39:49.643019: val_loss -0.4367
2024-12-01 13:39:49.643670: Pseudo dice [0.6445]
2024-12-01 13:39:49.644430: Epoch time: 87.74 s
2024-12-01 13:39:51.246792: 
2024-12-01 13:39:51.248505: Epoch 82
2024-12-01 13:39:51.249655: Current learning rate: 0.00926
2024-12-01 13:41:18.737340: Validation loss did not improve from -0.46175. Patience: 17/50
2024-12-01 13:41:18.738511: train_loss -0.7052
2024-12-01 13:41:18.739552: val_loss -0.4184
2024-12-01 13:41:18.740355: Pseudo dice [0.628]
2024-12-01 13:41:18.741149: Epoch time: 87.49 s
2024-12-01 13:41:19.870729: 
2024-12-01 13:41:19.872740: Epoch 83
2024-12-01 13:41:19.873900: Current learning rate: 0.00925
2024-12-01 13:42:47.464850: Validation loss did not improve from -0.46175. Patience: 18/50
2024-12-01 13:42:47.466089: train_loss -0.7126
2024-12-01 13:42:47.467363: val_loss -0.4141
2024-12-01 13:42:47.468434: Pseudo dice [0.6405]
2024-12-01 13:42:47.469650: Epoch time: 87.6 s
2024-12-01 13:42:48.635247: 
2024-12-01 13:42:48.637003: Epoch 84
2024-12-01 13:42:48.638094: Current learning rate: 0.00924
2024-12-01 13:44:16.257760: Validation loss did not improve from -0.46175. Patience: 19/50
2024-12-01 13:44:16.258822: train_loss -0.7083
2024-12-01 13:44:16.259764: val_loss -0.3631
2024-12-01 13:44:16.260623: Pseudo dice [0.6106]
2024-12-01 13:44:16.261496: Epoch time: 87.62 s
2024-12-01 13:44:17.716125: 
2024-12-01 13:44:17.717510: Epoch 85
2024-12-01 13:44:17.718299: Current learning rate: 0.00923
2024-12-01 13:45:45.222664: Validation loss did not improve from -0.46175. Patience: 20/50
2024-12-01 13:45:45.223579: train_loss -0.7005
2024-12-01 13:45:45.224560: val_loss -0.3809
2024-12-01 13:45:45.225234: Pseudo dice [0.6177]
2024-12-01 13:45:45.225827: Epoch time: 87.51 s
2024-12-01 13:45:46.395373: 
2024-12-01 13:45:46.397231: Epoch 86
2024-12-01 13:45:46.398351: Current learning rate: 0.00922
2024-12-01 13:47:14.468964: Validation loss did not improve from -0.46175. Patience: 21/50
2024-12-01 13:47:14.472343: train_loss -0.7076
2024-12-01 13:47:14.473540: val_loss -0.3925
2024-12-01 13:47:14.474200: Pseudo dice [0.6229]
2024-12-01 13:47:14.475491: Epoch time: 88.08 s
2024-12-01 13:47:15.770275: 
2024-12-01 13:47:15.785154: Epoch 87
2024-12-01 13:47:15.785957: Current learning rate: 0.00921
2024-12-01 13:48:43.335940: Validation loss did not improve from -0.46175. Patience: 22/50
2024-12-01 13:48:43.337642: train_loss -0.7078
2024-12-01 13:48:43.340088: val_loss -0.3669
2024-12-01 13:48:43.340979: Pseudo dice [0.6073]
2024-12-01 13:48:43.342429: Epoch time: 87.57 s
2024-12-01 13:48:44.549032: 
2024-12-01 13:48:44.550867: Epoch 88
2024-12-01 13:48:44.552231: Current learning rate: 0.0092
2024-12-01 13:50:11.809706: Validation loss did not improve from -0.46175. Patience: 23/50
2024-12-01 13:50:11.810624: train_loss -0.7057
2024-12-01 13:50:11.811332: val_loss -0.4343
2024-12-01 13:50:11.812133: Pseudo dice [0.6324]
2024-12-01 13:50:11.813250: Epoch time: 87.26 s
2024-12-01 13:50:12.975839: 
2024-12-01 13:50:12.977131: Epoch 89
2024-12-01 13:50:12.977947: Current learning rate: 0.0092
2024-12-01 13:51:40.349242: Validation loss did not improve from -0.46175. Patience: 24/50
2024-12-01 13:51:40.350134: train_loss -0.7205
2024-12-01 13:51:40.351281: val_loss -0.4004
2024-12-01 13:51:40.352439: Pseudo dice [0.6481]
2024-12-01 13:51:40.353223: Epoch time: 87.38 s
2024-12-01 13:51:41.872698: 
2024-12-01 13:51:41.874409: Epoch 90
2024-12-01 13:51:41.875269: Current learning rate: 0.00919
2024-12-01 13:53:09.099641: Validation loss did not improve from -0.46175. Patience: 25/50
2024-12-01 13:53:09.100558: train_loss -0.7079
2024-12-01 13:53:09.101218: val_loss -0.4056
2024-12-01 13:53:09.101792: Pseudo dice [0.6452]
2024-12-01 13:53:09.102325: Epoch time: 87.23 s
2024-12-01 13:53:10.261321: 
2024-12-01 13:53:10.263176: Epoch 91
2024-12-01 13:53:10.263815: Current learning rate: 0.00918
2024-12-01 13:54:37.486489: Validation loss did not improve from -0.46175. Patience: 26/50
2024-12-01 13:54:37.487475: train_loss -0.7069
2024-12-01 13:54:37.488231: val_loss -0.4154
2024-12-01 13:54:37.489258: Pseudo dice [0.6278]
2024-12-01 13:54:37.490199: Epoch time: 87.23 s
2024-12-01 13:54:38.640172: 
2024-12-01 13:54:38.641749: Epoch 92
2024-12-01 13:54:38.642394: Current learning rate: 0.00917
2024-12-01 13:56:05.976555: Validation loss did not improve from -0.46175. Patience: 27/50
2024-12-01 13:56:05.977938: train_loss -0.7145
2024-12-01 13:56:05.979007: val_loss -0.3995
2024-12-01 13:56:05.979755: Pseudo dice [0.6266]
2024-12-01 13:56:05.980873: Epoch time: 87.34 s
2024-12-01 13:56:07.140739: 
2024-12-01 13:56:07.142516: Epoch 93
2024-12-01 13:56:07.143252: Current learning rate: 0.00916
2024-12-01 13:57:34.496090: Validation loss did not improve from -0.46175. Patience: 28/50
2024-12-01 13:57:34.496767: train_loss -0.7146
2024-12-01 13:57:34.497556: val_loss -0.3986
2024-12-01 13:57:34.498413: Pseudo dice [0.6356]
2024-12-01 13:57:34.499062: Epoch time: 87.36 s
2024-12-01 13:57:36.442529: 
2024-12-01 13:57:36.443714: Epoch 94
2024-12-01 13:57:36.444924: Current learning rate: 0.00915
2024-12-01 13:59:03.657415: Validation loss did not improve from -0.46175. Patience: 29/50
2024-12-01 13:59:03.658224: train_loss -0.7124
2024-12-01 13:59:03.658986: val_loss -0.3984
2024-12-01 13:59:03.659587: Pseudo dice [0.6339]
2024-12-01 13:59:03.660489: Epoch time: 87.22 s
2024-12-01 13:59:05.133095: 
2024-12-01 13:59:05.135176: Epoch 95
2024-12-01 13:59:05.137726: Current learning rate: 0.00914
2024-12-01 14:00:32.692012: Validation loss did not improve from -0.46175. Patience: 30/50
2024-12-01 14:00:32.693326: train_loss -0.711
2024-12-01 14:00:32.694241: val_loss -0.4397
2024-12-01 14:00:32.694762: Pseudo dice [0.6422]
2024-12-01 14:00:32.695406: Epoch time: 87.56 s
2024-12-01 14:00:33.848138: 
2024-12-01 14:00:33.849829: Epoch 96
2024-12-01 14:00:33.850800: Current learning rate: 0.00913
2024-12-01 14:02:01.377361: Validation loss did not improve from -0.46175. Patience: 31/50
2024-12-01 14:02:01.378169: train_loss -0.7244
2024-12-01 14:02:01.379122: val_loss -0.4164
2024-12-01 14:02:01.379888: Pseudo dice [0.6332]
2024-12-01 14:02:01.380773: Epoch time: 87.53 s
2024-12-01 14:02:02.547611: 
2024-12-01 14:02:02.549110: Epoch 97
2024-12-01 14:02:02.549735: Current learning rate: 0.00912
2024-12-01 14:03:30.251102: Validation loss did not improve from -0.46175. Patience: 32/50
2024-12-01 14:03:30.251969: train_loss -0.7172
2024-12-01 14:03:30.253168: val_loss -0.4162
2024-12-01 14:03:30.254248: Pseudo dice [0.6356]
2024-12-01 14:03:30.254923: Epoch time: 87.71 s
2024-12-01 14:03:31.417233: 
2024-12-01 14:03:31.418620: Epoch 98
2024-12-01 14:03:31.419330: Current learning rate: 0.00911
2024-12-01 14:04:59.045031: Validation loss did not improve from -0.46175. Patience: 33/50
2024-12-01 14:04:59.046241: train_loss -0.7146
2024-12-01 14:04:59.047200: val_loss -0.4109
2024-12-01 14:04:59.047986: Pseudo dice [0.6351]
2024-12-01 14:04:59.048955: Epoch time: 87.63 s
2024-12-01 14:05:00.236849: 
2024-12-01 14:05:00.238843: Epoch 99
2024-12-01 14:05:00.239653: Current learning rate: 0.0091
2024-12-01 14:06:27.928077: Validation loss did not improve from -0.46175. Patience: 34/50
2024-12-01 14:06:27.928983: train_loss -0.718
2024-12-01 14:06:27.929784: val_loss -0.3598
2024-12-01 14:06:27.930420: Pseudo dice [0.6096]
2024-12-01 14:06:27.931113: Epoch time: 87.69 s
2024-12-01 14:06:29.463710: 
2024-12-01 14:06:29.465552: Epoch 100
2024-12-01 14:06:29.466784: Current learning rate: 0.0091
2024-12-01 14:07:57.030972: Validation loss improved from -0.46175 to -0.47307! Patience: 34/50
2024-12-01 14:07:57.031960: train_loss -0.7172
2024-12-01 14:07:57.032890: val_loss -0.4731
2024-12-01 14:07:57.033831: Pseudo dice [0.6788]
2024-12-01 14:07:57.034494: Epoch time: 87.57 s
2024-12-01 14:07:58.206724: 
2024-12-01 14:07:58.208730: Epoch 101
2024-12-01 14:07:58.210095: Current learning rate: 0.00909
2024-12-01 14:09:25.541285: Validation loss did not improve from -0.47307. Patience: 1/50
2024-12-01 14:09:25.542354: train_loss -0.7193
2024-12-01 14:09:25.543219: val_loss -0.3916
2024-12-01 14:09:25.543855: Pseudo dice [0.6099]
2024-12-01 14:09:25.544436: Epoch time: 87.34 s
2024-12-01 14:09:26.727207: 
2024-12-01 14:09:26.728697: Epoch 102
2024-12-01 14:09:26.729402: Current learning rate: 0.00908
2024-12-01 14:10:54.086238: Validation loss did not improve from -0.47307. Patience: 2/50
2024-12-01 14:10:54.087613: train_loss -0.7186
2024-12-01 14:10:54.089031: val_loss -0.4312
2024-12-01 14:10:54.090061: Pseudo dice [0.6521]
2024-12-01 14:10:54.090817: Epoch time: 87.36 s
2024-12-01 14:10:55.262515: 
2024-12-01 14:10:55.264618: Epoch 103
2024-12-01 14:10:55.265393: Current learning rate: 0.00907
2024-12-01 14:12:22.433238: Validation loss did not improve from -0.47307. Patience: 3/50
2024-12-01 14:12:22.434451: train_loss -0.7248
2024-12-01 14:12:22.435292: val_loss -0.4229
2024-12-01 14:12:22.435925: Pseudo dice [0.6359]
2024-12-01 14:12:22.436855: Epoch time: 87.17 s
2024-12-01 14:12:23.618536: 
2024-12-01 14:12:23.620291: Epoch 104
2024-12-01 14:12:23.621043: Current learning rate: 0.00906
2024-12-01 14:13:50.917484: Validation loss did not improve from -0.47307. Patience: 4/50
2024-12-01 14:13:50.918641: train_loss -0.7268
2024-12-01 14:13:50.919501: val_loss -0.4185
2024-12-01 14:13:50.920270: Pseudo dice [0.6314]
2024-12-01 14:13:50.920918: Epoch time: 87.3 s
2024-12-01 14:13:52.782260: 
2024-12-01 14:13:52.784311: Epoch 105
2024-12-01 14:13:52.785388: Current learning rate: 0.00905
2024-12-01 14:15:20.068619: Validation loss did not improve from -0.47307. Patience: 5/50
2024-12-01 14:15:20.069777: train_loss -0.7241
2024-12-01 14:15:20.071034: val_loss -0.3984
2024-12-01 14:15:20.072043: Pseudo dice [0.6269]
2024-12-01 14:15:20.073321: Epoch time: 87.29 s
2024-12-01 14:15:21.272130: 
2024-12-01 14:15:21.274080: Epoch 106
2024-12-01 14:15:21.275308: Current learning rate: 0.00904
2024-12-01 14:16:48.558543: Validation loss did not improve from -0.47307. Patience: 6/50
2024-12-01 14:16:48.559785: train_loss -0.7278
2024-12-01 14:16:48.560901: val_loss -0.4211
2024-12-01 14:16:48.561554: Pseudo dice [0.6357]
2024-12-01 14:16:48.562295: Epoch time: 87.29 s
2024-12-01 14:16:49.750786: 
2024-12-01 14:16:49.752906: Epoch 107
2024-12-01 14:16:49.753545: Current learning rate: 0.00903
2024-12-01 14:18:16.932954: Validation loss did not improve from -0.47307. Patience: 7/50
2024-12-01 14:18:16.934584: train_loss -0.728
2024-12-01 14:18:16.935506: val_loss -0.4213
2024-12-01 14:18:16.936560: Pseudo dice [0.647]
2024-12-01 14:18:16.937564: Epoch time: 87.18 s
2024-12-01 14:18:18.111113: 
2024-12-01 14:18:18.112564: Epoch 108
2024-12-01 14:18:18.113273: Current learning rate: 0.00902
2024-12-01 14:19:45.388298: Validation loss did not improve from -0.47307. Patience: 8/50
2024-12-01 14:19:45.389263: train_loss -0.7218
2024-12-01 14:19:45.390334: val_loss -0.4425
2024-12-01 14:19:45.391261: Pseudo dice [0.6619]
2024-12-01 14:19:45.391934: Epoch time: 87.28 s
2024-12-01 14:19:46.570608: 
2024-12-01 14:19:46.572506: Epoch 109
2024-12-01 14:19:46.573746: Current learning rate: 0.00901
2024-12-01 14:21:14.011693: Validation loss did not improve from -0.47307. Patience: 9/50
2024-12-01 14:21:14.013094: train_loss -0.7275
2024-12-01 14:21:14.013998: val_loss -0.4379
2024-12-01 14:21:14.014722: Pseudo dice [0.6547]
2024-12-01 14:21:14.015356: Epoch time: 87.44 s
2024-12-01 14:21:14.375929: Yayy! New best EMA pseudo Dice: 0.6397
2024-12-01 14:21:15.855613: 
2024-12-01 14:21:15.857120: Epoch 110
2024-12-01 14:21:15.857884: Current learning rate: 0.009
2024-12-01 14:22:43.473955: Validation loss did not improve from -0.47307. Patience: 10/50
2024-12-01 14:22:43.475239: train_loss -0.7213
2024-12-01 14:22:43.476522: val_loss -0.3982
2024-12-01 14:22:43.477725: Pseudo dice [0.6327]
2024-12-01 14:22:43.478686: Epoch time: 87.62 s
2024-12-01 14:22:44.665333: 
2024-12-01 14:22:44.667011: Epoch 111
2024-12-01 14:22:44.667947: Current learning rate: 0.009
2024-12-01 14:24:12.406723: Validation loss did not improve from -0.47307. Patience: 11/50
2024-12-01 14:24:12.408017: train_loss -0.729
2024-12-01 14:24:12.409443: val_loss -0.3843
2024-12-01 14:24:12.410127: Pseudo dice [0.6209]
2024-12-01 14:24:12.410702: Epoch time: 87.74 s
2024-12-01 14:24:13.579653: 
2024-12-01 14:24:13.581391: Epoch 112
2024-12-01 14:24:13.582081: Current learning rate: 0.00899
2024-12-01 14:25:41.299454: Validation loss did not improve from -0.47307. Patience: 12/50
2024-12-01 14:25:41.300760: train_loss -0.7143
2024-12-01 14:25:41.301583: val_loss -0.4098
2024-12-01 14:25:41.302302: Pseudo dice [0.639]
2024-12-01 14:25:41.302890: Epoch time: 87.72 s
2024-12-01 14:25:42.484141: 
2024-12-01 14:25:42.486259: Epoch 113
2024-12-01 14:25:42.487147: Current learning rate: 0.00898
2024-12-01 14:27:10.099461: Validation loss did not improve from -0.47307. Patience: 13/50
2024-12-01 14:27:10.100256: train_loss -0.7241
2024-12-01 14:27:10.101283: val_loss -0.4325
2024-12-01 14:27:10.102131: Pseudo dice [0.6388]
2024-12-01 14:27:10.102808: Epoch time: 87.62 s
2024-12-01 14:27:11.288035: 
2024-12-01 14:27:11.289907: Epoch 114
2024-12-01 14:27:11.290633: Current learning rate: 0.00897
2024-12-01 14:28:39.042148: Validation loss did not improve from -0.47307. Patience: 14/50
2024-12-01 14:28:39.043537: train_loss -0.7275
2024-12-01 14:28:39.045177: val_loss -0.4083
2024-12-01 14:28:39.046325: Pseudo dice [0.6308]
2024-12-01 14:28:39.047596: Epoch time: 87.76 s
2024-12-01 14:28:40.515625: 
2024-12-01 14:28:40.517880: Epoch 115
2024-12-01 14:28:40.518877: Current learning rate: 0.00896
2024-12-01 14:30:08.161506: Validation loss did not improve from -0.47307. Patience: 15/50
2024-12-01 14:30:08.162981: train_loss -0.7311
2024-12-01 14:30:08.164518: val_loss -0.3995
2024-12-01 14:30:08.165591: Pseudo dice [0.6337]
2024-12-01 14:30:08.166626: Epoch time: 87.65 s
2024-12-01 14:30:09.710055: 
2024-12-01 14:30:09.711588: Epoch 116
2024-12-01 14:30:09.712751: Current learning rate: 0.00895
2024-12-01 14:31:37.430016: Validation loss did not improve from -0.47307. Patience: 16/50
2024-12-01 14:31:37.431080: train_loss -0.7254
2024-12-01 14:31:37.431864: val_loss -0.4386
2024-12-01 14:31:37.432590: Pseudo dice [0.6466]
2024-12-01 14:31:37.433183: Epoch time: 87.72 s
2024-12-01 14:31:38.636864: 
2024-12-01 14:31:38.638802: Epoch 117
2024-12-01 14:31:38.639769: Current learning rate: 0.00894
2024-12-01 14:33:06.527034: Validation loss did not improve from -0.47307. Patience: 17/50
2024-12-01 14:33:06.528192: train_loss -0.7278
2024-12-01 14:33:06.528964: val_loss -0.4045
2024-12-01 14:33:06.530017: Pseudo dice [0.6215]
2024-12-01 14:33:06.531174: Epoch time: 87.89 s
2024-12-01 14:33:07.730842: 
2024-12-01 14:33:07.733392: Epoch 118
2024-12-01 14:33:07.734668: Current learning rate: 0.00893
2024-12-01 14:34:35.581557: Validation loss did not improve from -0.47307. Patience: 18/50
2024-12-01 14:34:35.582587: train_loss -0.7336
2024-12-01 14:34:35.583872: val_loss -0.4472
2024-12-01 14:34:35.585095: Pseudo dice [0.6547]
2024-12-01 14:34:35.585880: Epoch time: 87.85 s
2024-12-01 14:34:36.797778: 
2024-12-01 14:34:36.800020: Epoch 119
2024-12-01 14:34:36.801430: Current learning rate: 0.00892
2024-12-01 14:36:04.523353: Validation loss did not improve from -0.47307. Patience: 19/50
2024-12-01 14:36:04.524928: train_loss -0.7341
2024-12-01 14:36:04.525817: val_loss -0.4239
2024-12-01 14:36:04.526476: Pseudo dice [0.643]
2024-12-01 14:36:04.527293: Epoch time: 87.73 s
2024-12-01 14:36:06.063456: 
2024-12-01 14:36:06.065714: Epoch 120
2024-12-01 14:36:06.066592: Current learning rate: 0.00891
2024-12-01 14:37:33.848543: Validation loss did not improve from -0.47307. Patience: 20/50
2024-12-01 14:37:33.849926: train_loss -0.7286
2024-12-01 14:37:33.851070: val_loss -0.3922
2024-12-01 14:37:33.852422: Pseudo dice [0.6102]
2024-12-01 14:37:33.853584: Epoch time: 87.79 s
2024-12-01 14:37:35.063436: 
2024-12-01 14:37:35.065658: Epoch 121
2024-12-01 14:37:35.066364: Current learning rate: 0.0089
2024-12-01 14:39:02.820569: Validation loss did not improve from -0.47307. Patience: 21/50
2024-12-01 14:39:02.821810: train_loss -0.7331
2024-12-01 14:39:02.822522: val_loss -0.4181
2024-12-01 14:39:02.823470: Pseudo dice [0.6281]
2024-12-01 14:39:02.824558: Epoch time: 87.76 s
2024-12-01 14:39:04.017301: 
2024-12-01 14:39:04.019734: Epoch 122
2024-12-01 14:39:04.020730: Current learning rate: 0.00889
2024-12-01 14:40:31.731369: Validation loss did not improve from -0.47307. Patience: 22/50
2024-12-01 14:40:31.732630: train_loss -0.7275
2024-12-01 14:40:31.733494: val_loss -0.3859
2024-12-01 14:40:31.734554: Pseudo dice [0.6087]
2024-12-01 14:40:31.735247: Epoch time: 87.72 s
2024-12-01 14:40:32.949947: 
2024-12-01 14:40:32.952339: Epoch 123
2024-12-01 14:40:32.953458: Current learning rate: 0.00889
2024-12-01 14:42:00.653604: Validation loss did not improve from -0.47307. Patience: 23/50
2024-12-01 14:42:00.654679: train_loss -0.7323
2024-12-01 14:42:00.655682: val_loss -0.3979
2024-12-01 14:42:00.656627: Pseudo dice [0.624]
2024-12-01 14:42:00.657493: Epoch time: 87.71 s
2024-12-01 14:42:01.866691: 
2024-12-01 14:42:01.869184: Epoch 124
2024-12-01 14:42:01.870307: Current learning rate: 0.00888
2024-12-01 14:43:29.718259: Validation loss did not improve from -0.47307. Patience: 24/50
2024-12-01 14:43:29.719481: train_loss -0.7326
2024-12-01 14:43:29.720744: val_loss -0.4225
2024-12-01 14:43:29.721860: Pseudo dice [0.6354]
2024-12-01 14:43:29.722624: Epoch time: 87.85 s
2024-12-01 14:43:31.331579: 
2024-12-01 14:43:31.333653: Epoch 125
2024-12-01 14:43:31.334810: Current learning rate: 0.00887
2024-12-01 14:44:59.036264: Validation loss did not improve from -0.47307. Patience: 25/50
2024-12-01 14:44:59.037179: train_loss -0.7285
2024-12-01 14:44:59.038031: val_loss -0.4244
2024-12-01 14:44:59.038591: Pseudo dice [0.6385]
2024-12-01 14:44:59.039311: Epoch time: 87.71 s
2024-12-01 14:45:00.243719: 
2024-12-01 14:45:00.245380: Epoch 126
2024-12-01 14:45:00.246508: Current learning rate: 0.00886
2024-12-01 14:46:28.005874: Validation loss did not improve from -0.47307. Patience: 26/50
2024-12-01 14:46:28.007452: train_loss -0.7305
2024-12-01 14:46:28.008942: val_loss -0.4235
2024-12-01 14:46:28.009910: Pseudo dice [0.6355]
2024-12-01 14:46:28.010842: Epoch time: 87.76 s
2024-12-01 14:46:29.562112: 
2024-12-01 14:46:29.564224: Epoch 127
2024-12-01 14:46:29.565050: Current learning rate: 0.00885
2024-12-01 14:47:57.342695: Validation loss did not improve from -0.47307. Patience: 27/50
2024-12-01 14:47:57.343782: train_loss -0.7373
2024-12-01 14:47:57.345028: val_loss -0.4024
2024-12-01 14:47:57.346038: Pseudo dice [0.6471]
2024-12-01 14:47:57.346667: Epoch time: 87.78 s
2024-12-01 14:47:58.543484: 
2024-12-01 14:47:58.545422: Epoch 128
2024-12-01 14:47:58.546578: Current learning rate: 0.00884
2024-12-01 14:49:26.209507: Validation loss did not improve from -0.47307. Patience: 28/50
2024-12-01 14:49:26.210819: train_loss -0.7425
2024-12-01 14:49:26.211880: val_loss -0.379
2024-12-01 14:49:26.212766: Pseudo dice [0.6192]
2024-12-01 14:49:26.213727: Epoch time: 87.67 s
2024-12-01 14:49:27.408378: 
2024-12-01 14:49:27.410323: Epoch 129
2024-12-01 14:49:27.411798: Current learning rate: 0.00883
2024-12-01 14:50:55.285449: Validation loss did not improve from -0.47307. Patience: 29/50
2024-12-01 14:50:55.301012: train_loss -0.7354
2024-12-01 14:50:55.302644: val_loss -0.3997
2024-12-01 14:50:55.303211: Pseudo dice [0.6265]
2024-12-01 14:50:55.303771: Epoch time: 87.89 s
2024-12-01 14:50:56.954316: 
2024-12-01 14:50:56.956680: Epoch 130
2024-12-01 14:50:56.958094: Current learning rate: 0.00882
2024-12-01 14:52:24.927788: Validation loss did not improve from -0.47307. Patience: 30/50
2024-12-01 14:52:24.932462: train_loss -0.7398
2024-12-01 14:52:24.933540: val_loss -0.3793
2024-12-01 14:52:24.934335: Pseudo dice [0.636]
2024-12-01 14:52:24.935223: Epoch time: 87.98 s
2024-12-01 14:52:26.170252: 
2024-12-01 14:52:26.172303: Epoch 131
2024-12-01 14:52:26.173325: Current learning rate: 0.00881
2024-12-01 14:53:53.892075: Validation loss did not improve from -0.47307. Patience: 31/50
2024-12-01 14:53:53.893082: train_loss -0.7408
2024-12-01 14:53:53.895047: val_loss -0.4303
2024-12-01 14:53:53.895767: Pseudo dice [0.6365]
2024-12-01 14:53:53.896811: Epoch time: 87.72 s
2024-12-01 14:53:55.122886: 
2024-12-01 14:53:55.124673: Epoch 132
2024-12-01 14:53:55.125792: Current learning rate: 0.0088
2024-12-01 14:55:22.812828: Validation loss did not improve from -0.47307. Patience: 32/50
2024-12-01 14:55:22.813613: train_loss -0.7404
2024-12-01 14:55:22.814311: val_loss -0.4462
2024-12-01 14:55:22.814924: Pseudo dice [0.6646]
2024-12-01 14:55:22.815736: Epoch time: 87.69 s
2024-12-01 14:55:24.097462: 
2024-12-01 14:55:24.099389: Epoch 133
2024-12-01 14:55:24.100462: Current learning rate: 0.00879
2024-12-01 14:56:51.874408: Validation loss did not improve from -0.47307. Patience: 33/50
2024-12-01 14:56:51.875579: train_loss -0.7452
2024-12-01 14:56:51.876277: val_loss -0.4194
2024-12-01 14:56:51.876884: Pseudo dice [0.6412]
2024-12-01 14:56:51.877683: Epoch time: 87.78 s
2024-12-01 14:56:53.085329: 
2024-12-01 14:56:53.086697: Epoch 134
2024-12-01 14:56:53.087376: Current learning rate: 0.00879
2024-12-01 14:58:20.835069: Validation loss did not improve from -0.47307. Patience: 34/50
2024-12-01 14:58:20.835938: train_loss -0.7388
2024-12-01 14:58:20.836577: val_loss -0.4254
2024-12-01 14:58:20.837354: Pseudo dice [0.6471]
2024-12-01 14:58:20.838055: Epoch time: 87.75 s
2024-12-01 14:58:22.376306: 
2024-12-01 14:58:22.378027: Epoch 135
2024-12-01 14:58:22.379167: Current learning rate: 0.00878
2024-12-01 14:59:50.199991: Validation loss did not improve from -0.47307. Patience: 35/50
2024-12-01 14:59:50.200911: train_loss -0.7311
2024-12-01 14:59:50.201733: val_loss -0.4226
2024-12-01 14:59:50.202429: Pseudo dice [0.656]
2024-12-01 14:59:50.203136: Epoch time: 87.83 s
2024-12-01 14:59:51.429424: 
2024-12-01 14:59:51.431298: Epoch 136
2024-12-01 14:59:51.432232: Current learning rate: 0.00877
2024-12-01 15:01:19.176777: Validation loss did not improve from -0.47307. Patience: 36/50
2024-12-01 15:01:19.177717: train_loss -0.7355
2024-12-01 15:01:19.178721: val_loss -0.4238
2024-12-01 15:01:19.179861: Pseudo dice [0.6383]
2024-12-01 15:01:19.180577: Epoch time: 87.75 s
2024-12-01 15:01:20.414647: 
2024-12-01 15:01:20.416731: Epoch 137
2024-12-01 15:01:20.417553: Current learning rate: 0.00876
2024-12-01 15:02:48.031197: Validation loss did not improve from -0.47307. Patience: 37/50
2024-12-01 15:02:48.032373: train_loss -0.7322
2024-12-01 15:02:48.033292: val_loss -0.3921
2024-12-01 15:02:48.034148: Pseudo dice [0.6211]
2024-12-01 15:02:48.034908: Epoch time: 87.62 s
2024-12-01 15:02:51.199144: 
2024-12-01 15:02:51.201001: Epoch 138
2024-12-01 15:02:51.202237: Current learning rate: 0.00875
2024-12-01 15:04:18.464262: Validation loss did not improve from -0.47307. Patience: 38/50
2024-12-01 15:04:18.465675: train_loss -0.7284
2024-12-01 15:04:18.466608: val_loss -0.4475
2024-12-01 15:04:18.467623: Pseudo dice [0.6503]
2024-12-01 15:04:18.468531: Epoch time: 87.27 s
2024-12-01 15:04:19.716334: 
2024-12-01 15:04:19.718466: Epoch 139
2024-12-01 15:04:19.719259: Current learning rate: 0.00874
2024-12-01 15:05:47.242946: Validation loss did not improve from -0.47307. Patience: 39/50
2024-12-01 15:05:47.243695: train_loss -0.7413
2024-12-01 15:05:47.244556: val_loss -0.4439
2024-12-01 15:05:47.245144: Pseudo dice [0.6628]
2024-12-01 15:05:47.245793: Epoch time: 87.53 s
2024-12-01 15:05:47.623689: Yayy! New best EMA pseudo Dice: 0.6412
2024-12-01 15:05:49.166399: 
2024-12-01 15:05:49.168283: Epoch 140
2024-12-01 15:05:49.169154: Current learning rate: 0.00873
2024-12-01 15:07:16.467557: Validation loss did not improve from -0.47307. Patience: 40/50
2024-12-01 15:07:16.468649: train_loss -0.7442
2024-12-01 15:07:16.469325: val_loss -0.4212
2024-12-01 15:07:16.469854: Pseudo dice [0.6385]
2024-12-01 15:07:16.470490: Epoch time: 87.3 s
2024-12-01 15:07:17.723474: 
2024-12-01 15:07:17.725563: Epoch 141
2024-12-01 15:07:17.726429: Current learning rate: 0.00872
2024-12-01 15:08:44.908804: Validation loss did not improve from -0.47307. Patience: 41/50
2024-12-01 15:08:44.909607: train_loss -0.7404
2024-12-01 15:08:44.910401: val_loss -0.4106
2024-12-01 15:08:44.911113: Pseudo dice [0.6335]
2024-12-01 15:08:44.911917: Epoch time: 87.19 s
2024-12-01 15:08:46.155967: 
2024-12-01 15:08:46.157598: Epoch 142
2024-12-01 15:08:46.158574: Current learning rate: 0.00871
2024-12-01 15:10:13.288608: Validation loss did not improve from -0.47307. Patience: 42/50
2024-12-01 15:10:13.289799: train_loss -0.7477
2024-12-01 15:10:13.290881: val_loss -0.4155
2024-12-01 15:10:13.291674: Pseudo dice [0.6289]
2024-12-01 15:10:13.292337: Epoch time: 87.13 s
2024-12-01 15:10:14.533978: 
2024-12-01 15:10:14.535935: Epoch 143
2024-12-01 15:10:14.536818: Current learning rate: 0.0087
2024-12-01 15:11:41.630783: Validation loss did not improve from -0.47307. Patience: 43/50
2024-12-01 15:11:41.631703: train_loss -0.7473
2024-12-01 15:11:41.632907: val_loss -0.3365
2024-12-01 15:11:41.634073: Pseudo dice [0.5915]
2024-12-01 15:11:41.635332: Epoch time: 87.1 s
2024-12-01 15:11:42.862758: 
2024-12-01 15:11:42.864619: Epoch 144
2024-12-01 15:11:42.865693: Current learning rate: 0.00869
2024-12-01 15:13:10.129968: Validation loss did not improve from -0.47307. Patience: 44/50
2024-12-01 15:13:10.130701: train_loss -0.7424
2024-12-01 15:13:10.131438: val_loss -0.3707
2024-12-01 15:13:10.132652: Pseudo dice [0.6225]
2024-12-01 15:13:10.133548: Epoch time: 87.27 s
2024-12-01 15:13:11.721217: 
2024-12-01 15:13:11.722666: Epoch 145
2024-12-01 15:13:11.723568: Current learning rate: 0.00868
2024-12-01 15:14:38.911626: Validation loss did not improve from -0.47307. Patience: 45/50
2024-12-01 15:14:38.912964: train_loss -0.7426
2024-12-01 15:14:38.913956: val_loss -0.4463
2024-12-01 15:14:38.915063: Pseudo dice [0.6562]
2024-12-01 15:14:38.915973: Epoch time: 87.19 s
2024-12-01 15:14:40.147382: 
2024-12-01 15:14:40.149520: Epoch 146
2024-12-01 15:14:40.150702: Current learning rate: 0.00868
2024-12-01 15:16:07.314472: Validation loss did not improve from -0.47307. Patience: 46/50
2024-12-01 15:16:07.315253: train_loss -0.7302
2024-12-01 15:16:07.316394: val_loss -0.4286
2024-12-01 15:16:07.317459: Pseudo dice [0.6472]
2024-12-01 15:16:07.318165: Epoch time: 87.17 s
2024-12-01 15:16:08.539283: 
2024-12-01 15:16:08.541146: Epoch 147
2024-12-01 15:16:08.542521: Current learning rate: 0.00867
2024-12-01 15:17:35.718970: Validation loss did not improve from -0.47307. Patience: 47/50
2024-12-01 15:17:35.720007: train_loss -0.7306
2024-12-01 15:17:35.721798: val_loss -0.4328
2024-12-01 15:17:35.722932: Pseudo dice [0.6522]
2024-12-01 15:17:35.723707: Epoch time: 87.18 s
2024-12-01 15:17:37.273671: 
2024-12-01 15:17:37.275595: Epoch 148
2024-12-01 15:17:37.276333: Current learning rate: 0.00866
2024-12-01 15:19:04.449824: Validation loss did not improve from -0.47307. Patience: 48/50
2024-12-01 15:19:04.451052: train_loss -0.7359
2024-12-01 15:19:04.452148: val_loss -0.3934
2024-12-01 15:19:04.453107: Pseudo dice [0.6326]
2024-12-01 15:19:04.453843: Epoch time: 87.18 s
2024-12-01 15:19:05.715647: 
2024-12-01 15:19:05.717306: Epoch 149
2024-12-01 15:19:05.718448: Current learning rate: 0.00865
2024-12-01 15:20:33.089852: Validation loss did not improve from -0.47307. Patience: 49/50
2024-12-01 15:20:33.090575: train_loss -0.7383
2024-12-01 15:20:33.091304: val_loss -0.3971
2024-12-01 15:20:33.091839: Pseudo dice [0.6242]
2024-12-01 15:20:33.092722: Epoch time: 87.38 s
2024-12-01 15:20:34.721938: 
2024-12-01 15:20:34.723628: Epoch 150
2024-12-01 15:20:34.725074: Current learning rate: 0.00864
2024-12-01 15:22:02.261781: Validation loss did not improve from -0.47307. Patience: 50/50
2024-12-01 15:22:02.262776: train_loss -0.7426
2024-12-01 15:22:02.264411: val_loss -0.3935
2024-12-01 15:22:02.265317: Pseudo dice [0.6271]
2024-12-01 15:22:02.266310: Epoch time: 87.54 s
2024-12-01 15:22:03.486748: Patience reached. Stopping training.
2024-12-01 15:22:03.955308: Training done.
2024-12-01 15:22:04.431354: Using splits from existing split file: /home/gridsan/nchutisilp/datasets/nnUNet_Datasets/nnUNet_preprocessed/Dataset306_Sohee_Ajay_Calcium_OCT/splits_final.json
2024-12-01 15:22:04.445703: The split file contains 5 splits.
2024-12-01 15:22:04.447346: Desired fold for training: 4
2024-12-01 15:22:04.448486: This split has 11 training and 2 validation cases.
2024-12-01 15:22:04.449506: predicting 04010Pre
2024-12-01 15:22:04.519218: 04010Pre, shape torch.Size([1, 248, 498, 498]), rank 0
2024-12-01 15:23:41.644244: predicting 101-045
2024-12-01 15:23:41.664677: 101-045, shape torch.Size([1, 375, 498, 498]), rank 0
2024-12-01 15:26:27.819951: Validation complete
2024-12-01 15:26:27.820586: Mean Validation Dice:  0.5489758115198313
