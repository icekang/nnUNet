/home/gridsan/nchutisilp/.conda/envs/nnunet/bin/python

############################
INFO: You are using the old nnU-Net default plans. We have updated our recommendations. Please consider using those instead! Read more here: https://github.com/MIC-DKFZ/nnUNet/blob/master/documentation/resenc_presets.md
############################

Using device: cuda:0

#######################################################################
Please cite the following paper when using nnU-Net:
Isensee, F., Jaeger, P. F., Kohl, S. A., Petersen, J., & Maier-Hein, K. H. (2021). nnU-Net: a self-configuring method for deep learning-based biomedical image segmentation. Nature methods, 18(2), 203-211.
#######################################################################

2024-11-27 18:25:12.990925: Using torch.compile...
/home/gridsan/nchutisilp/.local/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn("The verbose parameter is deprecated. Please use get_last_lr() "
2024-11-27 18:25:19.430467: do_dummy_2d_data_aug: False
2024-11-27 18:25:19.434657: Using splits from existing split file: /home/gridsan/nchutisilp/datasets/nnUNet_Datasets/nnUNet_preprocessed/Dataset306_Sohee_Ajay_Calcium_OCT/splits_final.json
2024-11-27 18:25:19.438029: The split file contains 5 splits.
2024-11-27 18:25:19.439490: Desired fold for training: 2
2024-11-27 18:25:19.440482: This split has 10 training and 3 validation cases.
using pin_memory on device 0
using pin_memory on device 0

This is the configuration used by this training:
Configuration name: 3d_fullres
 {'data_identifier': 'nnUNetPlans_3d_fullres', 'preprocessor_name': 'DefaultPreprocessor', 'batch_size': 2, 'patch_size': [112, 160, 128], 'median_image_size_in_voxels': [375.0, 498.0, 498.0], 'spacing': [1.0, 1.0, 1.0], 'normalization_schemes': ['ZScoreNormalization'], 'use_mask_for_norm': [False], 'resampling_fn_data': 'resample_data_or_seg_to_shape', 'resampling_fn_seg': 'resample_data_or_seg_to_shape', 'resampling_fn_data_kwargs': {'is_seg': False, 'order': 3, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_seg_kwargs': {'is_seg': True, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_probabilities': 'resample_data_or_seg_to_shape', 'resampling_fn_probabilities_kwargs': {'is_seg': False, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'architecture': {'network_class_name': 'dynamic_network_architectures.architectures.unet.PlainConvUNet', 'arch_kwargs': {'n_stages': 6, 'features_per_stage': [32, 64, 128, 256, 320, 320], 'conv_op': 'torch.nn.modules.conv.Conv3d', 'kernel_sizes': [[3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3]], 'strides': [[1, 1, 1], [2, 2, 2], [2, 2, 2], [2, 2, 2], [2, 2, 2], [1, 2, 2]], 'n_conv_per_stage': [2, 2, 2, 2, 2, 2], 'n_conv_per_stage_decoder': [2, 2, 2, 2, 2], 'conv_bias': True, 'norm_op': 'torch.nn.modules.instancenorm.InstanceNorm3d', 'norm_op_kwargs': {'eps': 1e-05, 'affine': True}, 'dropout_op': None, 'dropout_op_kwargs': None, 'nonlin': 'torch.nn.LeakyReLU', 'nonlin_kwargs': {'inplace': True}, 'deep_supervision': True}, '_kw_requires_import': ['conv_op', 'norm_op', 'dropout_op', 'nonlin']}, 'batch_dice': True} 

These are the global plan.json settings:
 {'dataset_name': 'Dataset306_Sohee_Ajay_Calcium_OCT', 'plans_name': 'nnUNetPlans', 'original_median_spacing_after_transp': [1.0, 1.0, 1.0], 'original_median_shape_after_transp': [375, 498, 498], 'image_reader_writer': 'SimpleITKIO', 'transpose_forward': [0, 1, 2], 'transpose_backward': [0, 1, 2], 'experiment_planner_used': 'ExperimentPlanner', 'label_manager': 'LabelManager', 'foreground_intensity_properties_per_channel': {'0': {'max': 0.49803921580314636, 'mean': 0.14296366274356842, 'median': 0.10980392247438431, 'min': 0.0, 'percentile_00_5': 0.01558823511004448, 'percentile_99_5': 0.49803921580314636, 'std': 0.11562220752239227}}} 

2024-11-27 18:25:21.587437: unpacking dataset...
2024-11-27 18:25:26.881682: unpacking done...
2024-11-27 18:25:26.900310: Unable to plot network architecture: nnUNet_compile is enabled!
2024-11-27 18:25:26.973009: 
2024-11-27 18:25:26.974959: Epoch 900
2024-11-27 18:25:26.976346: Current learning rate: 0.00126
2024-11-27 18:28:08.391313: train_loss -0.8381
2024-11-27 18:28:08.393775: val_loss -0.4037
2024-11-27 18:28:08.395317: Pseudo dice [0.7015]
2024-11-27 18:28:08.396088: Epoch time: 161.42 s
2024-11-27 18:28:10.049582: 
2024-11-27 18:28:10.050874: Epoch 901
2024-11-27 18:28:10.051706: Current learning rate: 0.00125
2024-11-27 18:30:36.873889: train_loss -0.8303
2024-11-27 18:30:36.875244: val_loss -0.341
2024-11-27 18:30:36.876138: Pseudo dice [0.6596]
2024-11-27 18:30:36.877051: Epoch time: 146.83 s
2024-11-27 18:30:38.363425: 
2024-11-27 18:30:38.364761: Epoch 902
2024-11-27 18:30:38.365625: Current learning rate: 0.00124
2024-11-27 18:33:40.290515: train_loss -0.8356
2024-11-27 18:33:40.291720: val_loss -0.3843
2024-11-27 18:33:40.292764: Pseudo dice [0.6679]
2024-11-27 18:33:40.293532: Epoch time: 181.93 s
2024-11-27 18:33:41.854036: 
2024-11-27 18:33:41.856454: Epoch 903
2024-11-27 18:33:41.857779: Current learning rate: 0.00122
2024-11-27 18:37:12.956972: train_loss -0.8378
2024-11-27 18:37:12.958056: val_loss -0.433
2024-11-27 18:37:12.959017: Pseudo dice [0.7052]
2024-11-27 18:37:12.959884: Epoch time: 211.11 s
2024-11-27 18:37:14.479736: 
2024-11-27 18:37:14.481150: Epoch 904
2024-11-27 18:37:14.482176: Current learning rate: 0.00121
2024-11-27 18:40:53.291653: train_loss -0.8257
2024-11-27 18:40:53.292640: val_loss -0.3758
2024-11-27 18:40:53.293394: Pseudo dice [0.6753]
2024-11-27 18:40:53.294168: Epoch time: 218.81 s
2024-11-27 18:40:54.811752: 
2024-11-27 18:40:54.813142: Epoch 905
2024-11-27 18:40:54.813930: Current learning rate: 0.0012
2024-11-27 18:44:59.870399: train_loss -0.8257
2024-11-27 18:44:59.871608: val_loss -0.4026
2024-11-27 18:44:59.872398: Pseudo dice [0.6982]
2024-11-27 18:44:59.873126: Epoch time: 245.06 s
2024-11-27 18:45:02.120081: 
2024-11-27 18:45:02.121731: Epoch 906
2024-11-27 18:45:02.122705: Current learning rate: 0.00119
2024-11-27 18:49:05.835445: train_loss -0.8251
2024-11-27 18:49:05.836326: val_loss -0.3888
2024-11-27 18:49:05.837318: Pseudo dice [0.7055]
2024-11-27 18:49:05.838229: Epoch time: 243.72 s
2024-11-27 18:49:07.317264: 
2024-11-27 18:49:07.318994: Epoch 907
2024-11-27 18:49:07.319816: Current learning rate: 0.00118
2024-11-27 18:53:33.334492: train_loss -0.8368
2024-11-27 18:53:33.335560: val_loss -0.4473
2024-11-27 18:53:33.336427: Pseudo dice [0.7124]
2024-11-27 18:53:33.337125: Epoch time: 266.02 s
2024-11-27 18:53:34.842043: 
2024-11-27 18:53:34.843877: Epoch 908
2024-11-27 18:53:34.844806: Current learning rate: 0.00117
2024-11-27 18:57:45.456755: train_loss -0.8352
2024-11-27 18:57:45.457839: val_loss -0.3276
2024-11-27 18:57:45.458748: Pseudo dice [0.6533]
2024-11-27 18:57:45.459583: Epoch time: 250.62 s
2024-11-27 18:57:46.958939: 
2024-11-27 18:57:46.960780: Epoch 909
2024-11-27 18:57:46.961642: Current learning rate: 0.00116
2024-11-27 19:02:17.016649: train_loss -0.8348
2024-11-27 19:02:17.017606: val_loss -0.3705
2024-11-27 19:02:17.018651: Pseudo dice [0.6943]
2024-11-27 19:02:17.019516: Epoch time: 270.06 s
2024-11-27 19:02:18.486108: 
2024-11-27 19:02:18.487580: Epoch 910
2024-11-27 19:02:18.488331: Current learning rate: 0.00115
2024-11-27 19:06:47.179173: train_loss -0.8367
2024-11-27 19:06:47.180163: val_loss -0.4325
2024-11-27 19:06:47.180947: Pseudo dice [0.7204]
2024-11-27 19:06:47.181775: Epoch time: 268.69 s
2024-11-27 19:06:48.651735: 
2024-11-27 19:06:48.653277: Epoch 911
2024-11-27 19:06:48.654280: Current learning rate: 0.00113
2024-11-27 19:11:18.911869: train_loss -0.8344
2024-11-27 19:11:18.912931: val_loss -0.3944
2024-11-27 19:11:18.913822: Pseudo dice [0.6818]
2024-11-27 19:11:18.914709: Epoch time: 270.26 s
2024-11-27 19:11:20.509386: 
2024-11-27 19:11:20.511125: Epoch 912
2024-11-27 19:11:20.512032: Current learning rate: 0.00112
2024-11-27 19:15:48.581321: train_loss -0.8374
2024-11-27 19:15:48.582441: val_loss -0.4133
2024-11-27 19:15:48.583665: Pseudo dice [0.7036]
2024-11-27 19:15:48.584756: Epoch time: 268.07 s
2024-11-27 19:15:50.068873: 
2024-11-27 19:15:50.070453: Epoch 913
2024-11-27 19:15:50.071592: Current learning rate: 0.00111
2024-11-27 19:20:36.930628: train_loss -0.8313
2024-11-27 19:20:36.931670: val_loss -0.4025
2024-11-27 19:20:36.932650: Pseudo dice [0.7092]
2024-11-27 19:20:36.933499: Epoch time: 286.86 s
2024-11-27 19:20:38.411104: 
2024-11-27 19:20:38.412622: Epoch 914
2024-11-27 19:20:38.413654: Current learning rate: 0.0011
2024-11-27 19:25:20.324037: train_loss -0.8393
2024-11-27 19:25:20.325121: val_loss -0.3086
2024-11-27 19:25:20.326621: Pseudo dice [0.6661]
2024-11-27 19:25:20.327589: Epoch time: 281.91 s
2024-11-27 19:25:21.847371: 
2024-11-27 19:25:21.849127: Epoch 915
2024-11-27 19:25:21.850359: Current learning rate: 0.00109
2024-11-27 19:30:08.304271: train_loss -0.8323
2024-11-27 19:30:08.305257: val_loss -0.3647
2024-11-27 19:30:08.306048: Pseudo dice [0.6925]
2024-11-27 19:30:08.306945: Epoch time: 286.46 s
2024-11-27 19:30:09.765944: 
2024-11-27 19:30:09.767292: Epoch 916
2024-11-27 19:30:09.768239: Current learning rate: 0.00108
2024-11-27 19:34:45.975705: train_loss -0.8258
2024-11-27 19:34:45.991175: val_loss -0.325
2024-11-27 19:34:45.992352: Pseudo dice [0.6674]
2024-11-27 19:34:45.994457: Epoch time: 276.21 s
2024-11-27 19:34:47.560332: 
2024-11-27 19:34:47.562047: Epoch 917
2024-11-27 19:34:47.563008: Current learning rate: 0.00106
2024-11-27 19:39:30.954782: train_loss -0.8371
2024-11-27 19:39:30.955786: val_loss -0.4305
2024-11-27 19:39:30.956697: Pseudo dice [0.722]
2024-11-27 19:39:30.957625: Epoch time: 283.4 s
2024-11-27 19:39:33.338318: 
2024-11-27 19:39:33.339876: Epoch 918
2024-11-27 19:39:33.340852: Current learning rate: 0.00105
2024-11-27 19:44:27.665389: train_loss -0.8323
2024-11-27 19:44:27.666368: val_loss -0.3698
2024-11-27 19:44:27.667360: Pseudo dice [0.7016]
2024-11-27 19:44:27.668145: Epoch time: 294.33 s
2024-11-27 19:44:29.120372: 
2024-11-27 19:44:29.121793: Epoch 919
2024-11-27 19:44:29.122629: Current learning rate: 0.00104
2024-11-27 19:49:33.300419: train_loss -0.8311
2024-11-27 19:49:33.301537: val_loss -0.385
2024-11-27 19:49:33.302559: Pseudo dice [0.7198]
2024-11-27 19:49:33.303389: Epoch time: 304.18 s
2024-11-27 19:49:34.811067: 
2024-11-27 19:49:34.812463: Epoch 920
2024-11-27 19:49:34.813197: Current learning rate: 0.00103
2024-11-27 19:54:36.182897: train_loss -0.8308
2024-11-27 19:54:36.183856: val_loss -0.3327
2024-11-27 19:54:36.184639: Pseudo dice [0.6759]
2024-11-27 19:54:36.185334: Epoch time: 301.37 s
2024-11-27 19:54:37.657597: 
2024-11-27 19:54:37.658903: Epoch 921
2024-11-27 19:54:37.659814: Current learning rate: 0.00102
2024-11-27 19:59:22.192900: train_loss -0.8359
2024-11-27 19:59:22.193989: val_loss -0.3781
2024-11-27 19:59:22.195212: Pseudo dice [0.6901]
2024-11-27 19:59:22.196314: Epoch time: 284.54 s
2024-11-27 19:59:23.687414: 
2024-11-27 19:59:23.688961: Epoch 922
2024-11-27 19:59:23.690026: Current learning rate: 0.00101
2024-11-27 20:04:25.203237: train_loss -0.8395
2024-11-27 20:04:25.204243: val_loss -0.372
2024-11-27 20:04:25.204991: Pseudo dice [0.6683]
2024-11-27 20:04:25.205785: Epoch time: 301.52 s
2024-11-27 20:04:26.718867: 
2024-11-27 20:04:26.720249: Epoch 923
2024-11-27 20:04:26.721032: Current learning rate: 0.001
2024-11-27 20:09:12.797308: train_loss -0.838
2024-11-27 20:09:12.798298: val_loss -0.3628
2024-11-27 20:09:12.799310: Pseudo dice [0.6816]
2024-11-27 20:09:12.800150: Epoch time: 286.08 s
2024-11-27 20:09:14.263772: 
2024-11-27 20:09:14.265169: Epoch 924
2024-11-27 20:09:14.266089: Current learning rate: 0.00098
2024-11-27 20:14:01.164243: train_loss -0.8365
2024-11-27 20:14:01.165416: val_loss -0.3612
2024-11-27 20:14:01.166422: Pseudo dice [0.6954]
2024-11-27 20:14:01.167141: Epoch time: 286.9 s
2024-11-27 20:14:02.648239: 
2024-11-27 20:14:02.649617: Epoch 925
2024-11-27 20:14:02.650419: Current learning rate: 0.00097
2024-11-27 20:19:08.457892: train_loss -0.8426
2024-11-27 20:19:08.459295: val_loss -0.3904
2024-11-27 20:19:08.460199: Pseudo dice [0.6897]
2024-11-27 20:19:08.461015: Epoch time: 305.81 s
2024-11-27 20:19:09.986484: 
2024-11-27 20:19:09.987782: Epoch 926
2024-11-27 20:19:09.988560: Current learning rate: 0.00096
2024-11-27 20:24:14.565722: train_loss -0.8373
2024-11-27 20:24:14.566719: val_loss -0.38
2024-11-27 20:24:14.567790: Pseudo dice [0.686]
2024-11-27 20:24:14.568774: Epoch time: 304.58 s
2024-11-27 20:24:16.076521: 
2024-11-27 20:24:16.078140: Epoch 927
2024-11-27 20:24:16.079146: Current learning rate: 0.00095
2024-11-27 20:29:35.889136: train_loss -0.8359
2024-11-27 20:29:35.890121: val_loss -0.3798
2024-11-27 20:29:35.891019: Pseudo dice [0.689]
2024-11-27 20:29:35.891748: Epoch time: 319.81 s
2024-11-27 20:29:37.381040: 
2024-11-27 20:29:37.382489: Epoch 928
2024-11-27 20:29:37.383226: Current learning rate: 0.00094
2024-11-27 20:34:35.187193: train_loss -0.8362
2024-11-27 20:34:35.188235: val_loss -0.3704
2024-11-27 20:34:35.188952: Pseudo dice [0.6824]
2024-11-27 20:34:35.189692: Epoch time: 297.81 s
2024-11-27 20:34:36.627573: 
2024-11-27 20:34:36.629253: Epoch 929
2024-11-27 20:34:36.630088: Current learning rate: 0.00092
2024-11-27 20:39:49.370664: train_loss -0.8387
2024-11-27 20:39:49.375784: val_loss -0.3515
2024-11-27 20:39:49.376829: Pseudo dice [0.6879]
2024-11-27 20:39:49.378801: Epoch time: 312.74 s
2024-11-27 20:39:51.544176: 
2024-11-27 20:39:51.545998: Epoch 930
2024-11-27 20:39:51.546972: Current learning rate: 0.00091
2024-11-27 20:44:53.652225: train_loss -0.8379
2024-11-27 20:44:53.654189: val_loss -0.3735
2024-11-27 20:44:53.655568: Pseudo dice [0.6806]
2024-11-27 20:44:53.656549: Epoch time: 302.11 s
2024-11-27 20:44:55.164429: 
2024-11-27 20:44:55.166058: Epoch 931
2024-11-27 20:44:55.167017: Current learning rate: 0.0009
2024-11-27 20:49:55.981482: train_loss -0.8408
2024-11-27 20:49:55.982644: val_loss -0.4187
2024-11-27 20:49:55.983465: Pseudo dice [0.6986]
2024-11-27 20:49:55.984284: Epoch time: 300.82 s
2024-11-27 20:49:57.460319: 
2024-11-27 20:49:57.461735: Epoch 932
2024-11-27 20:49:57.462631: Current learning rate: 0.00089
2024-11-27 20:54:57.318792: train_loss -0.8318
2024-11-27 20:54:57.320835: val_loss -0.4829
2024-11-27 20:54:57.322001: Pseudo dice [0.7372]
2024-11-27 20:54:57.322999: Epoch time: 299.86 s
2024-11-27 20:54:58.825457: 
2024-11-27 20:54:58.827096: Epoch 933
2024-11-27 20:54:58.828269: Current learning rate: 0.00088
2024-11-27 21:00:05.479498: train_loss -0.8387
2024-11-27 21:00:05.480686: val_loss -0.4158
2024-11-27 21:00:05.481549: Pseudo dice [0.7023]
2024-11-27 21:00:05.482334: Epoch time: 306.66 s
2024-11-27 21:00:06.922806: 
2024-11-27 21:00:06.924407: Epoch 934
2024-11-27 21:00:06.925357: Current learning rate: 0.00087
2024-11-27 21:05:29.458652: train_loss -0.8368
2024-11-27 21:05:29.459908: val_loss -0.3905
2024-11-27 21:05:29.460723: Pseudo dice [0.6964]
2024-11-27 21:05:29.461589: Epoch time: 322.54 s
2024-11-27 21:05:30.893046: 
2024-11-27 21:05:30.894457: Epoch 935
2024-11-27 21:05:30.895322: Current learning rate: 0.00085
2024-11-27 21:10:30.780344: train_loss -0.8363
2024-11-27 21:10:30.781685: val_loss -0.4066
2024-11-27 21:10:30.782573: Pseudo dice [0.704]
2024-11-27 21:10:30.783472: Epoch time: 299.89 s
2024-11-27 21:10:32.350799: 
2024-11-27 21:10:32.352574: Epoch 936
2024-11-27 21:10:32.353530: Current learning rate: 0.00084
2024-11-27 21:15:30.307166: train_loss -0.8391
2024-11-27 21:15:30.308943: val_loss -0.3233
2024-11-27 21:15:30.310205: Pseudo dice [0.6256]
2024-11-27 21:15:30.311295: Epoch time: 297.96 s
2024-11-27 21:15:31.862911: 
2024-11-27 21:15:31.864671: Epoch 937
2024-11-27 21:15:31.865649: Current learning rate: 0.00083
2024-11-27 21:21:10.443292: train_loss -0.8398
2024-11-27 21:21:10.444613: val_loss -0.4107
2024-11-27 21:21:10.445601: Pseudo dice [0.6988]
2024-11-27 21:21:10.446634: Epoch time: 338.58 s
2024-11-27 21:21:12.037333: 
2024-11-27 21:21:12.039103: Epoch 938
2024-11-27 21:21:12.040204: Current learning rate: 0.00082
2024-11-27 21:26:29.773494: train_loss -0.8371
2024-11-27 21:26:29.774712: val_loss -0.3685
2024-11-27 21:26:29.775839: Pseudo dice [0.7078]
2024-11-27 21:26:29.776849: Epoch time: 317.74 s
2024-11-27 21:26:31.279903: 
2024-11-27 21:26:31.281480: Epoch 939
2024-11-27 21:26:31.282537: Current learning rate: 0.00081
2024-11-27 21:31:32.896468: train_loss -0.841
2024-11-27 21:31:32.897688: val_loss -0.385
2024-11-27 21:31:32.898768: Pseudo dice [0.7061]
2024-11-27 21:31:32.899720: Epoch time: 301.62 s
2024-11-27 21:31:34.373285: 
2024-11-27 21:31:34.374737: Epoch 940
2024-11-27 21:31:34.375709: Current learning rate: 0.00079
2024-11-27 21:36:29.682329: train_loss -0.8339
2024-11-27 21:36:29.683645: val_loss -0.3744
2024-11-27 21:36:29.684812: Pseudo dice [0.6959]
2024-11-27 21:36:29.685879: Epoch time: 295.31 s
2024-11-27 21:36:31.258423: 
2024-11-27 21:36:31.260129: Epoch 941
2024-11-27 21:36:31.261136: Current learning rate: 0.00078
2024-11-27 21:41:30.714824: train_loss -0.8375
2024-11-27 21:41:30.718349: val_loss -0.3517
2024-11-27 21:41:30.719508: Pseudo dice [0.6802]
2024-11-27 21:41:30.721174: Epoch time: 299.46 s
2024-11-27 21:41:33.295262: 
2024-11-27 21:41:33.296921: Epoch 942
2024-11-27 21:41:33.297816: Current learning rate: 0.00077
2024-11-27 21:46:29.031349: train_loss -0.8293
2024-11-27 21:46:29.033288: val_loss -0.3716
2024-11-27 21:46:29.034420: Pseudo dice [0.6797]
2024-11-27 21:46:29.035917: Epoch time: 295.74 s
2024-11-27 21:46:30.516471: 
2024-11-27 21:46:30.518191: Epoch 943
2024-11-27 21:46:30.519400: Current learning rate: 0.00076
2024-11-27 21:51:59.257954: train_loss -0.8337
2024-11-27 21:51:59.259219: val_loss -0.3598
2024-11-27 21:51:59.260250: Pseudo dice [0.68]
2024-11-27 21:51:59.261194: Epoch time: 328.74 s
2024-11-27 21:52:00.904563: 
2024-11-27 21:52:00.906079: Epoch 944
2024-11-27 21:52:00.907073: Current learning rate: 0.00075
2024-11-27 21:57:20.698722: train_loss -0.8378
2024-11-27 21:57:20.700124: val_loss -0.3628
2024-11-27 21:57:20.701155: Pseudo dice [0.6854]
2024-11-27 21:57:20.702154: Epoch time: 319.8 s
2024-11-27 21:57:22.258068: 
2024-11-27 21:57:22.259853: Epoch 945
2024-11-27 21:57:22.261184: Current learning rate: 0.00074
2024-11-27 22:02:40.290478: train_loss -0.8345
2024-11-27 22:02:40.291626: val_loss -0.4512
2024-11-27 22:02:40.292817: Pseudo dice [0.7273]
2024-11-27 22:02:40.293875: Epoch time: 318.03 s
2024-11-27 22:02:41.767687: 
2024-11-27 22:02:41.769286: Epoch 946
2024-11-27 22:02:41.770435: Current learning rate: 0.00072
2024-11-27 22:07:47.975616: train_loss -0.8359
2024-11-27 22:07:47.976890: val_loss -0.4052
2024-11-27 22:07:47.977692: Pseudo dice [0.694]
2024-11-27 22:07:47.978409: Epoch time: 306.21 s
2024-11-27 22:07:49.433179: 
2024-11-27 22:07:49.435367: Epoch 947
2024-11-27 22:07:49.436314: Current learning rate: 0.00071
2024-11-27 22:12:45.152074: train_loss -0.8366
2024-11-27 22:12:45.153699: val_loss -0.3789
2024-11-27 22:12:45.154765: Pseudo dice [0.7025]
2024-11-27 22:12:45.155747: Epoch time: 295.72 s
2024-11-27 22:12:46.787412: 
2024-11-27 22:12:46.789354: Epoch 948
2024-11-27 22:12:46.790563: Current learning rate: 0.0007
2024-11-27 22:17:41.288957: train_loss -0.8392
2024-11-27 22:17:41.290472: val_loss -0.434
2024-11-27 22:17:41.291513: Pseudo dice [0.7098]
2024-11-27 22:17:41.292634: Epoch time: 294.5 s
2024-11-27 22:17:42.913865: 
2024-11-27 22:17:42.915650: Epoch 949
2024-11-27 22:17:42.916821: Current learning rate: 0.00069
2024-11-27 22:22:56.037336: train_loss -0.8408
2024-11-27 22:22:56.038460: val_loss -0.407
2024-11-27 22:22:56.039483: Pseudo dice [0.708]
2024-11-27 22:22:56.040378: Epoch time: 313.13 s
2024-11-27 22:22:58.131646: 
2024-11-27 22:22:58.133230: Epoch 950
2024-11-27 22:22:58.134214: Current learning rate: 0.00067
2024-11-27 22:28:22.226131: train_loss -0.8435
2024-11-27 22:28:22.227451: val_loss -0.364
2024-11-27 22:28:22.228345: Pseudo dice [0.6777]
2024-11-27 22:28:22.229238: Epoch time: 324.1 s
2024-11-27 22:28:23.710485: 
2024-11-27 22:28:23.711959: Epoch 951
2024-11-27 22:28:23.712891: Current learning rate: 0.00066
2024-11-27 22:33:35.920470: train_loss -0.8339
2024-11-27 22:33:35.922022: val_loss -0.382
2024-11-27 22:33:35.922948: Pseudo dice [0.6929]
2024-11-27 22:33:35.923875: Epoch time: 312.21 s
2024-11-27 22:33:37.379679: 
2024-11-27 22:33:37.381174: Epoch 952
2024-11-27 22:33:37.382287: Current learning rate: 0.00065
2024-11-27 22:38:43.551979: train_loss -0.8393
2024-11-27 22:38:43.553295: val_loss -0.4075
2024-11-27 22:38:43.554127: Pseudo dice [0.6932]
2024-11-27 22:38:43.554948: Epoch time: 306.17 s
2024-11-27 22:38:45.029217: 
2024-11-27 22:38:45.031135: Epoch 953
2024-11-27 22:38:45.032303: Current learning rate: 0.00064
2024-11-27 22:44:08.428444: train_loss -0.8363
2024-11-27 22:44:08.429789: val_loss -0.4062
2024-11-27 22:44:08.431003: Pseudo dice [0.7141]
2024-11-27 22:44:08.432060: Epoch time: 323.4 s
2024-11-27 22:44:09.955461: 
2024-11-27 22:44:09.957188: Epoch 954
2024-11-27 22:44:09.958284: Current learning rate: 0.00063
2024-11-27 22:49:39.242945: train_loss -0.8364
2024-11-27 22:49:39.247694: val_loss -0.4011
2024-11-27 22:49:39.249008: Pseudo dice [0.692]
2024-11-27 22:49:39.250339: Epoch time: 329.29 s
2024-11-27 22:49:40.795230: 
2024-11-27 22:49:40.796762: Epoch 955
2024-11-27 22:49:40.797892: Current learning rate: 0.00061
2024-11-27 22:55:02.564733: train_loss -0.8381
2024-11-27 22:55:02.566509: val_loss -0.4168
2024-11-27 22:55:02.567645: Pseudo dice [0.7042]
2024-11-27 22:55:02.568983: Epoch time: 321.77 s
2024-11-27 22:55:04.107814: 
2024-11-27 22:55:04.109339: Epoch 956
2024-11-27 22:55:04.110519: Current learning rate: 0.0006
2024-11-27 23:00:05.169269: train_loss -0.8396
2024-11-27 23:00:05.170429: val_loss -0.3375
2024-11-27 23:00:05.171709: Pseudo dice [0.6622]
2024-11-27 23:00:05.172817: Epoch time: 301.06 s
2024-11-27 23:00:06.659315: 
2024-11-27 23:00:06.660862: Epoch 957
2024-11-27 23:00:06.661725: Current learning rate: 0.00059
2024-11-27 23:05:33.432373: train_loss -0.8353
2024-11-27 23:05:33.433693: val_loss -0.3953
2024-11-27 23:05:33.434990: Pseudo dice [0.7065]
2024-11-27 23:05:33.435973: Epoch time: 326.77 s
2024-11-27 23:05:34.912452: 
2024-11-27 23:05:34.914756: Epoch 958
2024-11-27 23:05:34.915844: Current learning rate: 0.00058
2024-11-27 23:10:37.749352: train_loss -0.8347
2024-11-27 23:10:37.750662: val_loss -0.4287
2024-11-27 23:10:37.751630: Pseudo dice [0.709]
2024-11-27 23:10:37.752628: Epoch time: 302.84 s
2024-11-27 23:10:39.228226: 
2024-11-27 23:10:39.229651: Epoch 959
2024-11-27 23:10:39.230454: Current learning rate: 0.00056
2024-11-27 23:15:43.797647: train_loss -0.8356
2024-11-27 23:15:43.798882: val_loss -0.4257
2024-11-27 23:15:43.799802: Pseudo dice [0.7213]
2024-11-27 23:15:43.800710: Epoch time: 304.57 s
2024-11-27 23:15:45.287161: 
2024-11-27 23:15:45.288579: Epoch 960
2024-11-27 23:15:45.289504: Current learning rate: 0.00055
2024-11-27 23:20:57.591112: train_loss -0.8416
2024-11-27 23:20:57.592496: val_loss -0.3572
2024-11-27 23:20:57.593419: Pseudo dice [0.6986]
2024-11-27 23:20:57.594348: Epoch time: 312.31 s
2024-11-27 23:20:59.139576: 
2024-11-27 23:20:59.141412: Epoch 961
2024-11-27 23:20:59.142463: Current learning rate: 0.00054
2024-11-27 23:26:04.593673: train_loss -0.8468
2024-11-27 23:26:04.595271: val_loss -0.4029
2024-11-27 23:26:04.596424: Pseudo dice [0.6973]
2024-11-27 23:26:04.597846: Epoch time: 305.46 s
2024-11-27 23:26:06.130716: 
2024-11-27 23:26:06.132250: Epoch 962
2024-11-27 23:26:06.133364: Current learning rate: 0.00053
2024-11-27 23:31:26.502587: train_loss -0.8372
2024-11-27 23:31:26.503758: val_loss -0.4
2024-11-27 23:31:26.504661: Pseudo dice [0.6972]
2024-11-27 23:31:26.505540: Epoch time: 320.37 s
2024-11-27 23:31:27.999031: 
2024-11-27 23:31:28.000632: Epoch 963
2024-11-27 23:31:28.001606: Current learning rate: 0.00051
2024-11-27 23:36:51.108000: train_loss -0.8342
2024-11-27 23:36:51.109117: val_loss -0.3519
2024-11-27 23:36:51.110006: Pseudo dice [0.6716]
2024-11-27 23:36:51.110826: Epoch time: 323.11 s
2024-11-27 23:36:52.672075: 
2024-11-27 23:36:52.673686: Epoch 964
2024-11-27 23:36:52.674617: Current learning rate: 0.0005
2024-11-27 23:41:50.434207: train_loss -0.8352
2024-11-27 23:41:50.435483: val_loss -0.4395
2024-11-27 23:41:50.436654: Pseudo dice [0.7411]
2024-11-27 23:41:50.437609: Epoch time: 297.76 s
2024-11-27 23:41:50.438521: Yayy! New best EMA pseudo Dice: 0.7003
2024-11-27 23:41:52.977236: 
2024-11-27 23:41:52.978653: Epoch 965
2024-11-27 23:41:52.979737: Current learning rate: 0.00049
2024-11-27 23:46:56.225417: train_loss -0.8411
2024-11-27 23:46:56.226780: val_loss -0.4042
2024-11-27 23:46:56.227858: Pseudo dice [0.6942]
2024-11-27 23:46:56.228728: Epoch time: 303.25 s
2024-11-27 23:46:57.821868: 
2024-11-27 23:46:57.823551: Epoch 966
2024-11-27 23:46:57.824606: Current learning rate: 0.00048
2024-11-27 23:52:18.500878: train_loss -0.8437
2024-11-27 23:52:18.505948: val_loss -0.3671
2024-11-27 23:52:18.507183: Pseudo dice [0.6832]
2024-11-27 23:52:18.509069: Epoch time: 320.68 s
2024-11-27 23:52:20.061403: 
2024-11-27 23:52:20.063127: Epoch 967
2024-11-27 23:52:20.064031: Current learning rate: 0.00046
2024-11-27 23:58:00.271960: train_loss -0.8384
2024-11-27 23:58:00.274504: val_loss -0.4135
2024-11-27 23:58:00.275546: Pseudo dice [0.7022]
2024-11-27 23:58:00.276698: Epoch time: 340.21 s
2024-11-27 23:58:01.814518: 
2024-11-27 23:58:01.816150: Epoch 968
2024-11-27 23:58:01.816982: Current learning rate: 0.00045
2024-11-28 00:03:15.107049: train_loss -0.8426
2024-11-28 00:03:15.108255: val_loss -0.4217
2024-11-28 00:03:15.109169: Pseudo dice [0.7088]
2024-11-28 00:03:15.110069: Epoch time: 313.29 s
2024-11-28 00:03:16.726331: 
2024-11-28 00:03:16.728067: Epoch 969
2024-11-28 00:03:16.729136: Current learning rate: 0.00044
2024-11-28 00:08:32.829729: train_loss -0.8395
2024-11-28 00:08:32.830896: val_loss -0.3731
2024-11-28 00:08:32.832012: Pseudo dice [0.6759]
2024-11-28 00:08:32.832959: Epoch time: 316.1 s
2024-11-28 00:08:34.351570: 
2024-11-28 00:08:34.353052: Epoch 970
2024-11-28 00:08:34.354198: Current learning rate: 0.00043
2024-11-28 00:13:56.290235: train_loss -0.8479
2024-11-28 00:13:56.291427: val_loss -0.4018
2024-11-28 00:13:56.292365: Pseudo dice [0.705]
2024-11-28 00:13:56.293314: Epoch time: 321.94 s
2024-11-28 00:13:57.868934: 
2024-11-28 00:13:57.870625: Epoch 971
2024-11-28 00:13:57.871560: Current learning rate: 0.00041
2024-11-28 00:19:31.017134: train_loss -0.832
2024-11-28 00:19:31.018355: val_loss -0.4299
2024-11-28 00:19:31.019393: Pseudo dice [0.7223]
2024-11-28 00:19:31.020347: Epoch time: 333.15 s
2024-11-28 00:19:31.021279: Yayy! New best EMA pseudo Dice: 0.7003
2024-11-28 00:19:33.013516: 
2024-11-28 00:19:33.014823: Epoch 972
2024-11-28 00:19:33.015820: Current learning rate: 0.0004
2024-11-28 00:24:35.479435: train_loss -0.8393
2024-11-28 00:24:35.480765: val_loss -0.43
2024-11-28 00:24:35.481610: Pseudo dice [0.719]
2024-11-28 00:24:35.482641: Epoch time: 302.47 s
2024-11-28 00:24:35.483615: Yayy! New best EMA pseudo Dice: 0.7022
2024-11-28 00:24:37.505492: 
2024-11-28 00:24:37.507099: Epoch 973
2024-11-28 00:24:37.508173: Current learning rate: 0.00039
2024-11-28 00:29:38.302596: train_loss -0.8429
2024-11-28 00:29:38.303827: val_loss -0.3738
2024-11-28 00:29:38.305034: Pseudo dice [0.6971]
2024-11-28 00:29:38.306106: Epoch time: 300.8 s
2024-11-28 00:29:39.922222: 
2024-11-28 00:29:39.923918: Epoch 974
2024-11-28 00:29:39.925294: Current learning rate: 0.00037
2024-11-28 00:34:40.406381: train_loss -0.8416
2024-11-28 00:34:40.407482: val_loss -0.3793
2024-11-28 00:34:40.408605: Pseudo dice [0.6819]
2024-11-28 00:34:40.409748: Epoch time: 300.49 s
2024-11-28 00:34:42.021936: 
2024-11-28 00:34:42.023657: Epoch 975
2024-11-28 00:34:42.024815: Current learning rate: 0.00036
2024-11-28 00:39:49.162656: train_loss -0.8387
2024-11-28 00:39:49.163751: val_loss -0.4056
2024-11-28 00:39:49.164813: Pseudo dice [0.6973]
2024-11-28 00:39:49.165651: Epoch time: 307.14 s
2024-11-28 00:39:51.306265: 
2024-11-28 00:39:51.307939: Epoch 976
2024-11-28 00:39:51.309045: Current learning rate: 0.00035
2024-11-28 00:45:00.364315: train_loss -0.8507
2024-11-28 00:45:00.365708: val_loss -0.4216
2024-11-28 00:45:00.366871: Pseudo dice [0.7061]
2024-11-28 00:45:00.368239: Epoch time: 309.06 s
2024-11-28 00:45:01.930994: 
2024-11-28 00:45:01.932768: Epoch 977
2024-11-28 00:45:01.934002: Current learning rate: 0.00034
2024-11-28 00:50:20.671721: train_loss -0.8443
2024-11-28 00:50:20.672862: val_loss -0.3758
2024-11-28 00:50:20.673834: Pseudo dice [0.677]
2024-11-28 00:50:20.674640: Epoch time: 318.74 s
2024-11-28 00:50:22.177636: 
2024-11-28 00:50:22.179279: Epoch 978
2024-11-28 00:50:22.180153: Current learning rate: 0.00032
2024-11-28 00:55:46.018585: train_loss -0.8451
2024-11-28 00:55:46.020693: val_loss -0.4266
2024-11-28 00:55:46.021666: Pseudo dice [0.7235]
2024-11-28 00:55:46.022885: Epoch time: 323.84 s
2024-11-28 00:55:47.562644: 
2024-11-28 00:55:47.563942: Epoch 979
2024-11-28 00:55:47.565158: Current learning rate: 0.00031
2024-11-28 01:00:48.669384: train_loss -0.8411
2024-11-28 01:00:48.671913: val_loss -0.3491
2024-11-28 01:00:48.673349: Pseudo dice [0.6726]
2024-11-28 01:00:48.674990: Epoch time: 301.11 s
2024-11-28 01:00:50.227627: 
2024-11-28 01:00:50.229527: Epoch 980
2024-11-28 01:00:50.230717: Current learning rate: 0.0003
2024-11-28 01:06:19.530994: train_loss -0.8452
2024-11-28 01:06:19.534418: val_loss -0.4092
2024-11-28 01:06:19.535474: Pseudo dice [0.7011]
2024-11-28 01:06:19.536612: Epoch time: 329.3 s
2024-11-28 01:06:21.089062: 
2024-11-28 01:06:21.090444: Epoch 981
2024-11-28 01:06:21.091357: Current learning rate: 0.00028
2024-11-28 01:11:28.354906: train_loss -0.8435
2024-11-28 01:11:28.356233: val_loss -0.3888
2024-11-28 01:11:28.357537: Pseudo dice [0.7117]
2024-11-28 01:11:28.358541: Epoch time: 307.27 s
2024-11-28 01:11:29.842473: 
2024-11-28 01:11:29.844362: Epoch 982
2024-11-28 01:11:29.845505: Current learning rate: 0.00027
2024-11-28 01:16:28.232809: train_loss -0.84
2024-11-28 01:16:28.234055: val_loss -0.392
2024-11-28 01:16:28.235023: Pseudo dice [0.6856]
2024-11-28 01:16:28.235952: Epoch time: 298.39 s
2024-11-28 01:16:29.715127: 
2024-11-28 01:16:29.716723: Epoch 983
2024-11-28 01:16:29.717833: Current learning rate: 0.00026
2024-11-28 01:22:00.130133: train_loss -0.8369
2024-11-28 01:22:00.131510: val_loss -0.369
2024-11-28 01:22:00.132595: Pseudo dice [0.6955]
2024-11-28 01:22:00.133559: Epoch time: 330.42 s
2024-11-28 01:22:01.658742: 
2024-11-28 01:22:01.660406: Epoch 984
2024-11-28 01:22:01.661507: Current learning rate: 0.00024
2024-11-28 01:27:24.316506: train_loss -0.8515
2024-11-28 01:27:24.317768: val_loss -0.4508
2024-11-28 01:27:24.318778: Pseudo dice [0.7212]
2024-11-28 01:27:24.319651: Epoch time: 322.66 s
2024-11-28 01:27:25.973383: 
2024-11-28 01:27:25.974925: Epoch 985
2024-11-28 01:27:25.976078: Current learning rate: 0.00023
2024-11-28 01:32:43.346189: train_loss -0.8426
2024-11-28 01:32:43.347430: val_loss -0.4013
2024-11-28 01:32:43.348522: Pseudo dice [0.6932]
2024-11-28 01:32:43.349405: Epoch time: 317.37 s
2024-11-28 01:32:44.923332: 
2024-11-28 01:32:44.925006: Epoch 986
2024-11-28 01:32:44.925971: Current learning rate: 0.00021
2024-11-28 01:38:03.816912: train_loss -0.8466
2024-11-28 01:38:03.817929: val_loss -0.3839
2024-11-28 01:38:03.819129: Pseudo dice [0.6916]
2024-11-28 01:38:03.820043: Epoch time: 318.89 s
2024-11-28 01:38:05.378386: 
2024-11-28 01:38:05.380112: Epoch 987
2024-11-28 01:38:05.381137: Current learning rate: 0.0002
2024-11-28 01:43:14.170576: train_loss -0.8442
2024-11-28 01:43:14.171787: val_loss -0.3454
2024-11-28 01:43:14.172694: Pseudo dice [0.6718]
2024-11-28 01:43:14.173576: Epoch time: 308.79 s
2024-11-28 01:43:16.298504: 
2024-11-28 01:43:16.300231: Epoch 988
2024-11-28 01:43:16.301187: Current learning rate: 0.00019
2024-11-28 01:48:09.802847: train_loss -0.853
2024-11-28 01:48:09.805177: val_loss -0.3881
2024-11-28 01:48:09.806123: Pseudo dice [0.7011]
2024-11-28 01:48:09.807205: Epoch time: 293.51 s
2024-11-28 01:48:11.339537: 
2024-11-28 01:48:11.341027: Epoch 989
2024-11-28 01:48:11.342192: Current learning rate: 0.00017
2024-11-28 01:53:22.144462: train_loss -0.8478
2024-11-28 01:53:22.145801: val_loss -0.377
2024-11-28 01:53:22.146864: Pseudo dice [0.6885]
2024-11-28 01:53:22.147681: Epoch time: 310.81 s
2024-11-28 01:53:23.732641: 
2024-11-28 01:53:23.734066: Epoch 990
2024-11-28 01:53:23.734973: Current learning rate: 0.00016
2024-11-28 01:58:30.351363: train_loss -0.844
2024-11-28 01:58:30.352815: val_loss -0.3528
2024-11-28 01:58:30.354245: Pseudo dice [0.6739]
2024-11-28 01:58:30.355314: Epoch time: 306.62 s
2024-11-28 01:58:31.982885: 
2024-11-28 01:58:31.984428: Epoch 991
2024-11-28 01:58:31.985586: Current learning rate: 0.00014
2024-11-28 02:03:53.430209: train_loss -0.8463
2024-11-28 02:03:53.433179: val_loss -0.3995
2024-11-28 02:03:53.434245: Pseudo dice [0.6987]
2024-11-28 02:03:53.435731: Epoch time: 321.45 s
2024-11-28 02:03:55.047126: 
2024-11-28 02:03:55.048643: Epoch 992
2024-11-28 02:03:55.049651: Current learning rate: 0.00013
2024-11-28 02:09:18.359596: train_loss -0.835
2024-11-28 02:09:18.361384: val_loss -0.3583
2024-11-28 02:09:18.362300: Pseudo dice [0.6883]
2024-11-28 02:09:18.363524: Epoch time: 323.31 s
2024-11-28 02:09:19.944314: 
2024-11-28 02:09:19.945921: Epoch 993
2024-11-28 02:09:19.947107: Current learning rate: 0.00011
2024-11-28 02:14:45.492640: train_loss -0.8526
2024-11-28 02:14:45.494101: val_loss -0.4104
2024-11-28 02:14:45.495225: Pseudo dice [0.7073]
2024-11-28 02:14:45.496396: Epoch time: 325.55 s
2024-11-28 02:14:47.038467: 
2024-11-28 02:14:47.040167: Epoch 994
2024-11-28 02:14:47.041365: Current learning rate: 0.0001
2024-11-28 02:20:18.244115: train_loss -0.8447
2024-11-28 02:20:18.245447: val_loss -0.4077
2024-11-28 02:20:18.246527: Pseudo dice [0.6993]
2024-11-28 02:20:18.247594: Epoch time: 331.21 s
2024-11-28 02:20:19.783718: 
2024-11-28 02:20:19.785568: Epoch 995
2024-11-28 02:20:19.786860: Current learning rate: 8e-05
2024-11-28 02:25:56.618221: train_loss -0.8417
2024-11-28 02:25:56.619519: val_loss -0.34
2024-11-28 02:25:56.620625: Pseudo dice [0.6483]
2024-11-28 02:25:56.621551: Epoch time: 336.84 s
2024-11-28 02:25:58.171997: 
2024-11-28 02:25:58.173767: Epoch 996
2024-11-28 02:25:58.175048: Current learning rate: 7e-05
2024-11-28 02:31:41.386204: train_loss -0.8412
2024-11-28 02:31:41.387580: val_loss -0.3987
2024-11-28 02:31:41.388643: Pseudo dice [0.7128]
2024-11-28 02:31:41.389562: Epoch time: 343.22 s
2024-11-28 02:31:42.912900: 
2024-11-28 02:31:42.914390: Epoch 997
2024-11-28 02:31:42.915318: Current learning rate: 5e-05
2024-11-28 02:37:00.316558: train_loss -0.8452
2024-11-28 02:37:00.318087: val_loss -0.3871
2024-11-28 02:37:00.319058: Pseudo dice [0.6874]
2024-11-28 02:37:00.319989: Epoch time: 317.4 s
2024-11-28 02:37:01.846213: 
2024-11-28 02:37:01.848380: Epoch 998
2024-11-28 02:37:01.849131: Current learning rate: 4e-05
2024-11-28 02:42:38.331768: train_loss -0.8389
2024-11-28 02:42:38.333385: val_loss -0.3383
2024-11-28 02:42:38.334302: Pseudo dice [0.6589]
2024-11-28 02:42:38.335175: Epoch time: 336.49 s
2024-11-28 02:42:39.801038: 
2024-11-28 02:42:39.802523: Epoch 999
2024-11-28 02:42:39.803611: Current learning rate: 2e-05
2024-11-28 02:48:13.883330: train_loss -0.8387
2024-11-28 02:48:13.884444: val_loss -0.4085
2024-11-28 02:48:13.885339: Pseudo dice [0.6952]
2024-11-28 02:48:13.886193: Epoch time: 334.08 s
2024-11-28 02:48:17.471032: Training done.
2024-11-28 02:48:17.880805: Using splits from existing split file: /home/gridsan/nchutisilp/datasets/nnUNet_Datasets/nnUNet_preprocessed/Dataset306_Sohee_Ajay_Calcium_OCT/splits_final.json
2024-11-28 02:48:17.897034: The split file contains 5 splits.
2024-11-28 02:48:17.898312: Desired fold for training: 2
2024-11-28 02:48:17.899310: This split has 10 training and 3 validation cases.
2024-11-28 02:48:17.900528: predicting 401-004
2024-11-28 02:48:17.931636: 401-004, shape torch.Size([1, 375, 498, 498]), rank 0
2024-11-28 02:50:50.397143: predicting 706-005
2024-11-28 02:50:50.411123: 706-005, shape torch.Size([1, 375, 498, 498]), rank 0
2024-11-28 02:52:43.838803: predicting 708006Pre
2024-11-28 02:52:43.856278: 708006Pre, shape torch.Size([1, 253, 498, 498]), rank 0
2024-11-28 02:54:24.318362: Validation complete
2024-11-28 02:54:24.319679: Mean Validation Dice:  0.647543976707154
