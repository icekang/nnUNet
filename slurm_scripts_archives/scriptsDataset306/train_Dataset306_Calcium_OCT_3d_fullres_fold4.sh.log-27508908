/home/gridsan/nchutisilp/.conda/envs/nnunet/bin/python

############################
INFO: You are using the old nnU-Net default plans. We have updated our recommendations. Please consider using those instead! Read more here: https://github.com/MIC-DKFZ/nnUNet/blob/master/documentation/resenc_presets.md
############################

Using device: cuda:0

#######################################################################
Please cite the following paper when using nnU-Net:
Isensee, F., Jaeger, P. F., Kohl, S. A., Petersen, J., & Maier-Hein, K. H. (2021). nnU-Net: a self-configuring method for deep learning-based biomedical image segmentation. Nature methods, 18(2), 203-211.
#######################################################################

2024-11-27 18:23:04.782962: Using torch.compile...
/home/gridsan/nchutisilp/.local/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn("The verbose parameter is deprecated. Please use get_last_lr() "
2024-11-27 18:23:23.052688: do_dummy_2d_data_aug: False
2024-11-27 18:23:23.056316: Using splits from existing split file: /home/gridsan/nchutisilp/datasets/nnUNet_Datasets/nnUNet_preprocessed/Dataset306_Sohee_Ajay_Calcium_OCT/splits_final.json
2024-11-27 18:23:23.058618: The split file contains 5 splits.
2024-11-27 18:23:23.059595: Desired fold for training: 4
2024-11-27 18:23:23.060440: This split has 11 training and 2 validation cases.
using pin_memory on device 0
using pin_memory on device 0

This is the configuration used by this training:
Configuration name: 3d_fullres
 {'data_identifier': 'nnUNetPlans_3d_fullres', 'preprocessor_name': 'DefaultPreprocessor', 'batch_size': 2, 'patch_size': [112, 160, 128], 'median_image_size_in_voxels': [375.0, 498.0, 498.0], 'spacing': [1.0, 1.0, 1.0], 'normalization_schemes': ['ZScoreNormalization'], 'use_mask_for_norm': [False], 'resampling_fn_data': 'resample_data_or_seg_to_shape', 'resampling_fn_seg': 'resample_data_or_seg_to_shape', 'resampling_fn_data_kwargs': {'is_seg': False, 'order': 3, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_seg_kwargs': {'is_seg': True, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_probabilities': 'resample_data_or_seg_to_shape', 'resampling_fn_probabilities_kwargs': {'is_seg': False, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'architecture': {'network_class_name': 'dynamic_network_architectures.architectures.unet.PlainConvUNet', 'arch_kwargs': {'n_stages': 6, 'features_per_stage': [32, 64, 128, 256, 320, 320], 'conv_op': 'torch.nn.modules.conv.Conv3d', 'kernel_sizes': [[3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3]], 'strides': [[1, 1, 1], [2, 2, 2], [2, 2, 2], [2, 2, 2], [2, 2, 2], [1, 2, 2]], 'n_conv_per_stage': [2, 2, 2, 2, 2, 2], 'n_conv_per_stage_decoder': [2, 2, 2, 2, 2], 'conv_bias': True, 'norm_op': 'torch.nn.modules.instancenorm.InstanceNorm3d', 'norm_op_kwargs': {'eps': 1e-05, 'affine': True}, 'dropout_op': None, 'dropout_op_kwargs': None, 'nonlin': 'torch.nn.LeakyReLU', 'nonlin_kwargs': {'inplace': True}, 'deep_supervision': True}, '_kw_requires_import': ['conv_op', 'norm_op', 'dropout_op', 'nonlin']}, 'batch_dice': True} 

These are the global plan.json settings:
 {'dataset_name': 'Dataset306_Sohee_Ajay_Calcium_OCT', 'plans_name': 'nnUNetPlans', 'original_median_spacing_after_transp': [1.0, 1.0, 1.0], 'original_median_shape_after_transp': [375, 498, 498], 'image_reader_writer': 'SimpleITKIO', 'transpose_forward': [0, 1, 2], 'transpose_backward': [0, 1, 2], 'experiment_planner_used': 'ExperimentPlanner', 'label_manager': 'LabelManager', 'foreground_intensity_properties_per_channel': {'0': {'max': 0.49803921580314636, 'mean': 0.14296366274356842, 'median': 0.10980392247438431, 'min': 0.0, 'percentile_00_5': 0.01558823511004448, 'percentile_99_5': 0.49803921580314636, 'std': 0.11562220752239227}}} 

2024-11-27 18:23:32.823361: unpacking dataset...
2024-11-27 18:23:38.090040: unpacking done...
2024-11-27 18:23:38.106318: Unable to plot network architecture: nnUNet_compile is enabled!
2024-11-27 18:23:38.240053: 
2024-11-27 18:23:38.241100: Epoch 900
2024-11-27 18:23:38.241770: Current learning rate: 0.00126
2024-11-27 18:26:50.278037: train_loss -0.8385
2024-11-27 18:26:50.280097: val_loss -0.3027
2024-11-27 18:26:50.281312: Pseudo dice [0.6213]
2024-11-27 18:26:50.281954: Epoch time: 192.04 s
2024-11-27 18:26:51.769585: 
2024-11-27 18:26:51.771660: Epoch 901
2024-11-27 18:26:51.772624: Current learning rate: 0.00125
2024-11-27 18:28:50.499852: train_loss -0.8302
2024-11-27 18:28:50.501274: val_loss -0.3027
2024-11-27 18:28:50.502597: Pseudo dice [0.607]
2024-11-27 18:28:50.503891: Epoch time: 118.73 s
2024-11-27 18:28:52.153069: 
2024-11-27 18:28:52.154757: Epoch 902
2024-11-27 18:28:52.155859: Current learning rate: 0.00124
2024-11-27 18:31:53.315314: train_loss -0.8374
2024-11-27 18:31:53.316346: val_loss -0.3713
2024-11-27 18:31:53.317030: Pseudo dice [0.6615]
2024-11-27 18:31:53.317698: Epoch time: 181.16 s
2024-11-27 18:31:54.945812: 
2024-11-27 18:31:54.947308: Epoch 903
2024-11-27 18:31:54.948074: Current learning rate: 0.00122
2024-11-27 18:35:22.794706: train_loss -0.8368
2024-11-27 18:35:22.795658: val_loss -0.3216
2024-11-27 18:35:22.796456: Pseudo dice [0.6047]
2024-11-27 18:35:22.797156: Epoch time: 207.85 s
2024-11-27 18:35:24.271907: 
2024-11-27 18:35:24.273774: Epoch 904
2024-11-27 18:35:24.274647: Current learning rate: 0.00121
2024-11-27 18:39:16.435665: train_loss -0.8359
2024-11-27 18:39:16.436646: val_loss -0.3627
2024-11-27 18:39:16.437785: Pseudo dice [0.6439]
2024-11-27 18:39:16.438842: Epoch time: 232.17 s
2024-11-27 18:39:17.992454: 
2024-11-27 18:39:17.993768: Epoch 905
2024-11-27 18:39:17.994533: Current learning rate: 0.0012
2024-11-27 18:43:08.081166: train_loss -0.8403
2024-11-27 18:43:08.082390: val_loss -0.3235
2024-11-27 18:43:08.083507: Pseudo dice [0.6305]
2024-11-27 18:43:08.084513: Epoch time: 230.09 s
2024-11-27 18:43:10.118829: 
2024-11-27 18:43:10.120371: Epoch 906
2024-11-27 18:43:10.121511: Current learning rate: 0.00119
2024-11-27 18:47:41.422279: train_loss -0.8344
2024-11-27 18:47:41.423423: val_loss -0.2847
2024-11-27 18:47:41.424437: Pseudo dice [0.6057]
2024-11-27 18:47:41.425628: Epoch time: 271.3 s
2024-11-27 18:47:42.944029: 
2024-11-27 18:47:42.946255: Epoch 907
2024-11-27 18:47:42.947383: Current learning rate: 0.00118
2024-11-27 18:52:14.363537: train_loss -0.8398
2024-11-27 18:52:14.364748: val_loss -0.3065
2024-11-27 18:52:14.365778: Pseudo dice [0.5955]
2024-11-27 18:52:14.366848: Epoch time: 271.42 s
2024-11-27 18:52:16.033722: 
2024-11-27 18:52:16.034998: Epoch 908
2024-11-27 18:52:16.036056: Current learning rate: 0.00117
2024-11-27 18:56:48.658155: train_loss -0.8366
2024-11-27 18:56:48.659200: val_loss -0.2965
2024-11-27 18:56:48.660033: Pseudo dice [0.6125]
2024-11-27 18:56:48.660794: Epoch time: 272.63 s
2024-11-27 18:56:50.199338: 
2024-11-27 18:56:50.200941: Epoch 909
2024-11-27 18:56:50.201892: Current learning rate: 0.00116
2024-11-27 19:01:23.515594: train_loss -0.8313
2024-11-27 19:01:23.516558: val_loss -0.309
2024-11-27 19:01:23.517485: Pseudo dice [0.6161]
2024-11-27 19:01:23.518180: Epoch time: 273.32 s
2024-11-27 19:01:25.021463: 
2024-11-27 19:01:25.022903: Epoch 910
2024-11-27 19:01:25.023763: Current learning rate: 0.00115
2024-11-27 19:06:07.448208: train_loss -0.837
2024-11-27 19:06:07.449205: val_loss -0.3106
2024-11-27 19:06:07.450022: Pseudo dice [0.6127]
2024-11-27 19:06:07.451697: Epoch time: 282.43 s
2024-11-27 19:06:08.919533: 
2024-11-27 19:06:08.921158: Epoch 911
2024-11-27 19:06:08.922498: Current learning rate: 0.00113
2024-11-27 19:10:44.382649: train_loss -0.8344
2024-11-27 19:10:44.383804: val_loss -0.3396
2024-11-27 19:10:44.385391: Pseudo dice [0.641]
2024-11-27 19:10:44.386573: Epoch time: 275.46 s
2024-11-27 19:10:45.857241: 
2024-11-27 19:10:45.858884: Epoch 912
2024-11-27 19:10:45.860361: Current learning rate: 0.00112
2024-11-27 19:15:40.336472: train_loss -0.8391
2024-11-27 19:15:40.354363: val_loss -0.2494
2024-11-27 19:15:40.355700: Pseudo dice [0.5532]
2024-11-27 19:15:40.357294: Epoch time: 294.48 s
2024-11-27 19:15:41.982056: 
2024-11-27 19:15:41.983586: Epoch 913
2024-11-27 19:15:41.984823: Current learning rate: 0.00111
2024-11-27 19:20:23.029161: train_loss -0.8338
2024-11-27 19:20:23.030055: val_loss -0.3408
2024-11-27 19:20:23.030919: Pseudo dice [0.6185]
2024-11-27 19:20:23.031581: Epoch time: 281.05 s
2024-11-27 19:20:24.539181: 
2024-11-27 19:20:24.540505: Epoch 914
2024-11-27 19:20:24.541294: Current learning rate: 0.0011
2024-11-27 19:25:11.555488: train_loss -0.8374
2024-11-27 19:25:11.556261: val_loss -0.3626
2024-11-27 19:25:11.557138: Pseudo dice [0.6331]
2024-11-27 19:25:11.557898: Epoch time: 287.02 s
2024-11-27 19:25:13.040343: 
2024-11-27 19:25:13.042280: Epoch 915
2024-11-27 19:25:13.044015: Current learning rate: 0.00109
2024-11-27 19:30:09.907928: train_loss -0.8352
2024-11-27 19:30:09.909964: val_loss -0.2711
2024-11-27 19:30:09.910948: Pseudo dice [0.5918]
2024-11-27 19:30:09.912278: Epoch time: 296.87 s
2024-11-27 19:30:11.401095: 
2024-11-27 19:30:11.402348: Epoch 916
2024-11-27 19:30:11.403092: Current learning rate: 0.00108
2024-11-27 19:35:27.553061: train_loss -0.8392
2024-11-27 19:35:27.554193: val_loss -0.3121
2024-11-27 19:35:27.555430: Pseudo dice [0.5763]
2024-11-27 19:35:27.556823: Epoch time: 316.15 s
2024-11-27 19:35:29.199610: 
2024-11-27 19:35:29.207721: Epoch 917
2024-11-27 19:35:29.209538: Current learning rate: 0.00106
2024-11-27 19:40:41.974292: train_loss -0.837
2024-11-27 19:40:41.975413: val_loss -0.3129
2024-11-27 19:40:41.976128: Pseudo dice [0.6085]
2024-11-27 19:40:41.977073: Epoch time: 312.78 s
2024-11-27 19:40:43.929909: 
2024-11-27 19:40:43.931071: Epoch 918
2024-11-27 19:40:43.932223: Current learning rate: 0.00105
2024-11-27 19:45:46.238505: train_loss -0.8402
2024-11-27 19:45:46.239247: val_loss -0.3127
2024-11-27 19:45:46.239933: Pseudo dice [0.6192]
2024-11-27 19:45:46.240706: Epoch time: 302.31 s
2024-11-27 19:45:47.716518: 
2024-11-27 19:45:47.717855: Epoch 919
2024-11-27 19:45:47.718682: Current learning rate: 0.00104
2024-11-27 19:51:00.350681: train_loss -0.8365
2024-11-27 19:51:00.351827: val_loss -0.3044
2024-11-27 19:51:00.352726: Pseudo dice [0.6131]
2024-11-27 19:51:00.353447: Epoch time: 312.64 s
2024-11-27 19:51:01.844522: 
2024-11-27 19:51:01.845852: Epoch 920
2024-11-27 19:51:01.846684: Current learning rate: 0.00103
2024-11-27 19:56:14.818186: train_loss -0.8402
2024-11-27 19:56:14.819622: val_loss -0.3388
2024-11-27 19:56:14.820481: Pseudo dice [0.6202]
2024-11-27 19:56:14.821345: Epoch time: 312.97 s
2024-11-27 19:56:16.288692: 
2024-11-27 19:56:16.290267: Epoch 921
2024-11-27 19:56:16.291322: Current learning rate: 0.00102
2024-11-27 20:01:30.173562: train_loss -0.8394
2024-11-27 20:01:30.174446: val_loss -0.2842
2024-11-27 20:01:30.175148: Pseudo dice [0.5994]
2024-11-27 20:01:30.175964: Epoch time: 313.89 s
2024-11-27 20:01:31.659840: 
2024-11-27 20:01:31.661257: Epoch 922
2024-11-27 20:01:31.662078: Current learning rate: 0.00101
2024-11-27 20:06:55.951418: train_loss -0.8368
2024-11-27 20:06:55.952456: val_loss -0.3279
2024-11-27 20:06:55.953428: Pseudo dice [0.6096]
2024-11-27 20:06:55.954260: Epoch time: 324.29 s
2024-11-27 20:06:57.498217: 
2024-11-27 20:06:57.499619: Epoch 923
2024-11-27 20:06:57.500537: Current learning rate: 0.001
2024-11-27 20:11:49.138283: train_loss -0.8429
2024-11-27 20:11:49.139323: val_loss -0.3514
2024-11-27 20:11:49.140155: Pseudo dice [0.6461]
2024-11-27 20:11:49.141146: Epoch time: 291.64 s
2024-11-27 20:11:50.763711: 
2024-11-27 20:11:50.764969: Epoch 924
2024-11-27 20:11:50.765774: Current learning rate: 0.00098
2024-11-27 20:16:55.833898: train_loss -0.8376
2024-11-27 20:16:55.834687: val_loss -0.3831
2024-11-27 20:16:55.835637: Pseudo dice [0.6529]
2024-11-27 20:16:55.836732: Epoch time: 305.07 s
2024-11-27 20:16:57.364979: 
2024-11-27 20:16:57.366540: Epoch 925
2024-11-27 20:16:57.367506: Current learning rate: 0.00097
2024-11-27 20:22:32.670436: train_loss -0.8378
2024-11-27 20:22:32.675120: val_loss -0.3161
2024-11-27 20:22:32.677050: Pseudo dice [0.6287]
2024-11-27 20:22:32.678633: Epoch time: 335.31 s
2024-11-27 20:22:34.341683: 
2024-11-27 20:22:34.343069: Epoch 926
2024-11-27 20:22:34.343957: Current learning rate: 0.00096
2024-11-27 20:27:56.534532: train_loss -0.8427
2024-11-27 20:27:56.535464: val_loss -0.3032
2024-11-27 20:27:56.536682: Pseudo dice [0.5984]
2024-11-27 20:27:56.537629: Epoch time: 322.19 s
2024-11-27 20:27:58.064355: 
2024-11-27 20:27:58.065739: Epoch 927
2024-11-27 20:27:58.067243: Current learning rate: 0.00095
2024-11-27 20:33:32.433029: train_loss -0.8406
2024-11-27 20:33:32.433957: val_loss -0.344
2024-11-27 20:33:32.434749: Pseudo dice [0.6295]
2024-11-27 20:33:32.435677: Epoch time: 334.37 s
2024-11-27 20:33:33.877368: 
2024-11-27 20:33:33.889155: Epoch 928
2024-11-27 20:33:33.890140: Current learning rate: 0.00094
2024-11-27 20:39:16.155272: train_loss -0.8341
2024-11-27 20:39:16.157504: val_loss -0.2571
2024-11-27 20:39:16.158371: Pseudo dice [0.5831]
2024-11-27 20:39:16.159167: Epoch time: 342.28 s
2024-11-27 20:39:17.681562: 
2024-11-27 20:39:17.682879: Epoch 929
2024-11-27 20:39:17.683653: Current learning rate: 0.00092
2024-11-27 20:44:50.844488: train_loss -0.8368
2024-11-27 20:44:50.845620: val_loss -0.3205
2024-11-27 20:44:50.846536: Pseudo dice [0.6135]
2024-11-27 20:44:50.847420: Epoch time: 333.16 s
2024-11-27 20:44:52.913953: 
2024-11-27 20:44:52.916410: Epoch 930
2024-11-27 20:44:52.918331: Current learning rate: 0.00091
2024-11-27 20:49:59.195232: train_loss -0.8379
2024-11-27 20:49:59.196151: val_loss -0.3049
2024-11-27 20:49:59.196998: Pseudo dice [0.5959]
2024-11-27 20:49:59.197755: Epoch time: 306.28 s
2024-11-27 20:50:00.701293: 
2024-11-27 20:50:00.702713: Epoch 931
2024-11-27 20:50:00.703413: Current learning rate: 0.0009
2024-11-27 20:55:06.727855: train_loss -0.836
2024-11-27 20:55:06.728803: val_loss -0.304
2024-11-27 20:55:06.729552: Pseudo dice [0.5745]
2024-11-27 20:55:06.730254: Epoch time: 306.03 s
2024-11-27 20:55:08.261749: 
2024-11-27 20:55:08.263125: Epoch 932
2024-11-27 20:55:08.264042: Current learning rate: 0.00089
2024-11-27 21:00:28.958326: train_loss -0.8402
2024-11-27 21:00:28.959316: val_loss -0.3226
2024-11-27 21:00:28.960345: Pseudo dice [0.6321]
2024-11-27 21:00:28.961383: Epoch time: 320.7 s
2024-11-27 21:00:30.699106: 
2024-11-27 21:00:30.700589: Epoch 933
2024-11-27 21:00:30.701362: Current learning rate: 0.00088
2024-11-27 21:05:51.273409: train_loss -0.8416
2024-11-27 21:05:51.275308: val_loss -0.3262
2024-11-27 21:05:51.276642: Pseudo dice [0.6324]
2024-11-27 21:05:51.277755: Epoch time: 320.58 s
2024-11-27 21:05:52.836783: 
2024-11-27 21:05:52.838382: Epoch 934
2024-11-27 21:05:52.839463: Current learning rate: 0.00087
2024-11-27 21:11:13.716932: train_loss -0.8418
2024-11-27 21:11:13.718014: val_loss -0.3652
2024-11-27 21:11:13.718853: Pseudo dice [0.6528]
2024-11-27 21:11:13.719576: Epoch time: 320.88 s
2024-11-27 21:11:15.163779: 
2024-11-27 21:11:15.165135: Epoch 935
2024-11-27 21:11:15.165834: Current learning rate: 0.00085
2024-11-27 21:16:40.775982: train_loss -0.8384
2024-11-27 21:16:40.776946: val_loss -0.3423
2024-11-27 21:16:40.777834: Pseudo dice [0.6258]
2024-11-27 21:16:40.778439: Epoch time: 325.61 s
2024-11-27 21:16:42.262225: 
2024-11-27 21:16:42.263405: Epoch 936
2024-11-27 21:16:42.264202: Current learning rate: 0.00084
2024-11-27 21:22:25.804621: train_loss -0.8437
2024-11-27 21:22:25.805902: val_loss -0.3198
2024-11-27 21:22:25.806959: Pseudo dice [0.6205]
2024-11-27 21:22:25.808025: Epoch time: 343.54 s
2024-11-27 21:22:27.266627: 
2024-11-27 21:22:27.268058: Epoch 937
2024-11-27 21:22:27.269006: Current learning rate: 0.00083
2024-11-27 21:27:56.098411: train_loss -0.8419
2024-11-27 21:27:56.099492: val_loss -0.3278
2024-11-27 21:27:56.100399: Pseudo dice [0.604]
2024-11-27 21:27:56.101241: Epoch time: 328.83 s
2024-11-27 21:27:57.613158: 
2024-11-27 21:27:57.614872: Epoch 938
2024-11-27 21:27:57.615832: Current learning rate: 0.00082
2024-11-27 21:33:19.823984: train_loss -0.8454
2024-11-27 21:33:19.824988: val_loss -0.3548
2024-11-27 21:33:19.825872: Pseudo dice [0.6452]
2024-11-27 21:33:19.827010: Epoch time: 322.21 s
2024-11-27 21:33:21.324636: 
2024-11-27 21:33:21.325872: Epoch 939
2024-11-27 21:33:21.326808: Current learning rate: 0.00081
2024-11-27 21:38:55.341735: train_loss -0.8412
2024-11-27 21:38:55.342662: val_loss -0.2867
2024-11-27 21:38:55.343590: Pseudo dice [0.583]
2024-11-27 21:38:55.344372: Epoch time: 334.02 s
2024-11-27 21:38:56.780413: 
2024-11-27 21:38:56.781532: Epoch 940
2024-11-27 21:38:56.782249: Current learning rate: 0.00079
2024-11-27 21:44:17.490277: train_loss -0.8418
2024-11-27 21:44:17.492501: val_loss -0.3284
2024-11-27 21:44:17.493230: Pseudo dice [0.6233]
2024-11-27 21:44:17.493874: Epoch time: 320.71 s
2024-11-27 21:44:19.052091: 
2024-11-27 21:44:19.053134: Epoch 941
2024-11-27 21:44:19.053811: Current learning rate: 0.00078
2024-11-27 21:50:10.657881: train_loss -0.8412
2024-11-27 21:50:10.660829: val_loss -0.3148
2024-11-27 21:50:10.661765: Pseudo dice [0.62]
2024-11-27 21:50:10.663222: Epoch time: 351.61 s
2024-11-27 21:50:12.669973: 
2024-11-27 21:50:12.671453: Epoch 942
2024-11-27 21:50:12.672256: Current learning rate: 0.00077
2024-11-27 21:55:46.117573: train_loss -0.8416
2024-11-27 21:55:46.118387: val_loss -0.3337
2024-11-27 21:55:46.119138: Pseudo dice [0.6291]
2024-11-27 21:55:46.119957: Epoch time: 333.45 s
2024-11-27 21:55:47.653507: 
2024-11-27 21:55:47.654530: Epoch 943
2024-11-27 21:55:47.655287: Current learning rate: 0.00076
2024-11-27 22:01:19.287658: train_loss -0.8397
2024-11-27 22:01:19.288575: val_loss -0.321
2024-11-27 22:01:19.289428: Pseudo dice [0.6117]
2024-11-27 22:01:19.290342: Epoch time: 331.64 s
2024-11-27 22:01:20.790901: 
2024-11-27 22:01:20.792206: Epoch 944
2024-11-27 22:01:20.793017: Current learning rate: 0.00075
2024-11-27 22:06:37.893909: train_loss -0.8411
2024-11-27 22:06:37.895093: val_loss -0.3646
2024-11-27 22:06:37.896610: Pseudo dice [0.6577]
2024-11-27 22:06:37.898148: Epoch time: 317.1 s
2024-11-27 22:06:39.354429: 
2024-11-27 22:06:39.355819: Epoch 945
2024-11-27 22:06:39.356627: Current learning rate: 0.00074
2024-11-27 22:12:30.329651: train_loss -0.8423
2024-11-27 22:12:30.330730: val_loss -0.2981
2024-11-27 22:12:30.331577: Pseudo dice [0.5913]
2024-11-27 22:12:30.332394: Epoch time: 350.98 s
2024-11-27 22:12:31.864081: 
2024-11-27 22:12:31.865458: Epoch 946
2024-11-27 22:12:31.866303: Current learning rate: 0.00072
2024-11-27 22:18:13.208070: train_loss -0.8416
2024-11-27 22:18:13.209003: val_loss -0.3657
2024-11-27 22:18:13.209700: Pseudo dice [0.6542]
2024-11-27 22:18:13.210445: Epoch time: 341.35 s
2024-11-27 22:18:14.714583: 
2024-11-27 22:18:14.716011: Epoch 947
2024-11-27 22:18:14.717038: Current learning rate: 0.00071
2024-11-27 22:23:23.821796: train_loss -0.8424
2024-11-27 22:23:23.822893: val_loss -0.3128
2024-11-27 22:23:23.823702: Pseudo dice [0.6199]
2024-11-27 22:23:23.824425: Epoch time: 309.11 s
2024-11-27 22:23:25.334812: 
2024-11-27 22:23:25.336196: Epoch 948
2024-11-27 22:23:25.336878: Current learning rate: 0.0007
2024-11-27 22:28:24.150298: train_loss -0.8422
2024-11-27 22:28:24.151414: val_loss -0.3522
2024-11-27 22:28:24.152118: Pseudo dice [0.6431]
2024-11-27 22:28:24.152761: Epoch time: 298.82 s
2024-11-27 22:28:25.639535: 
2024-11-27 22:28:25.641328: Epoch 949
2024-11-27 22:28:25.642345: Current learning rate: 0.00069
2024-11-27 22:34:08.296736: train_loss -0.8477
2024-11-27 22:34:08.297766: val_loss -0.3462
2024-11-27 22:34:08.298775: Pseudo dice [0.6395]
2024-11-27 22:34:08.299627: Epoch time: 342.66 s
2024-11-27 22:34:08.959854: Yayy! New best EMA pseudo Dice: 0.6255
2024-11-27 22:34:11.292155: 
2024-11-27 22:34:11.293566: Epoch 950
2024-11-27 22:34:11.294310: Current learning rate: 0.00067
2024-11-27 22:40:04.155766: train_loss -0.8388
2024-11-27 22:40:04.156710: val_loss -0.3104
2024-11-27 22:40:04.157772: Pseudo dice [0.6268]
2024-11-27 22:40:04.158807: Epoch time: 352.86 s
2024-11-27 22:40:04.159742: Yayy! New best EMA pseudo Dice: 0.6257
2024-11-27 22:40:06.155114: 
2024-11-27 22:40:06.156733: Epoch 951
2024-11-27 22:40:06.157830: Current learning rate: 0.00066
2024-11-27 22:45:22.572333: train_loss -0.8467
2024-11-27 22:45:22.573377: val_loss -0.3047
2024-11-27 22:45:22.574882: Pseudo dice [0.6062]
2024-11-27 22:45:22.576330: Epoch time: 316.42 s
2024-11-27 22:45:24.034259: 
2024-11-27 22:45:24.035641: Epoch 952
2024-11-27 22:45:24.036960: Current learning rate: 0.00065
2024-11-27 22:50:34.494738: train_loss -0.8381
2024-11-27 22:50:34.497246: val_loss -0.3531
2024-11-27 22:50:34.498027: Pseudo dice [0.648]
2024-11-27 22:50:34.498800: Epoch time: 310.46 s
2024-11-27 22:50:34.499500: Yayy! New best EMA pseudo Dice: 0.6261
2024-11-27 22:50:37.017130: 
2024-11-27 22:50:37.018317: Epoch 953
2024-11-27 22:50:37.019047: Current learning rate: 0.00064
2024-11-27 22:56:17.287366: train_loss -0.839
2024-11-27 22:56:17.289714: val_loss -0.3328
2024-11-27 22:56:17.290821: Pseudo dice [0.6091]
2024-11-27 22:56:17.291941: Epoch time: 340.27 s
2024-11-27 22:56:18.943528: 
2024-11-27 22:56:18.945146: Epoch 954
2024-11-27 22:56:18.946246: Current learning rate: 0.00063
2024-11-27 23:02:12.314378: train_loss -0.8457
2024-11-27 23:02:12.315397: val_loss -0.3289
2024-11-27 23:02:12.316260: Pseudo dice [0.6199]
2024-11-27 23:02:12.316986: Epoch time: 353.37 s
2024-11-27 23:02:13.824879: 
2024-11-27 23:02:13.826257: Epoch 955
2024-11-27 23:02:13.826933: Current learning rate: 0.00061
2024-11-27 23:07:38.534902: train_loss -0.8408
2024-11-27 23:07:38.535963: val_loss -0.2963
2024-11-27 23:07:38.537307: Pseudo dice [0.6292]
2024-11-27 23:07:38.538379: Epoch time: 324.71 s
2024-11-27 23:07:40.044519: 
2024-11-27 23:07:40.045825: Epoch 956
2024-11-27 23:07:40.046668: Current learning rate: 0.0006
2024-11-27 23:13:20.252779: train_loss -0.8386
2024-11-27 23:13:20.253690: val_loss -0.3074
2024-11-27 23:13:20.254428: Pseudo dice [0.5994]
2024-11-27 23:13:20.255087: Epoch time: 340.21 s
2024-11-27 23:13:21.800948: 
2024-11-27 23:13:21.802264: Epoch 957
2024-11-27 23:13:21.802996: Current learning rate: 0.00059
2024-11-27 23:19:12.823921: train_loss -0.8486
2024-11-27 23:19:12.825007: val_loss -0.2867
2024-11-27 23:19:12.825857: Pseudo dice [0.6047]
2024-11-27 23:19:12.826717: Epoch time: 351.02 s
2024-11-27 23:19:14.363283: 
2024-11-27 23:19:14.364636: Epoch 958
2024-11-27 23:19:14.365637: Current learning rate: 0.00058
2024-11-27 23:25:03.479654: train_loss -0.8494
2024-11-27 23:25:03.480516: val_loss -0.2683
2024-11-27 23:25:03.481216: Pseudo dice [0.5558]
2024-11-27 23:25:03.481952: Epoch time: 349.12 s
2024-11-27 23:25:04.984829: 
2024-11-27 23:25:04.986288: Epoch 959
2024-11-27 23:25:04.986959: Current learning rate: 0.00056
2024-11-27 23:30:42.452434: train_loss -0.8445
2024-11-27 23:30:42.453324: val_loss -0.3255
2024-11-27 23:30:42.454085: Pseudo dice [0.6195]
2024-11-27 23:30:42.454750: Epoch time: 337.47 s
2024-11-27 23:30:43.941285: 
2024-11-27 23:30:43.942768: Epoch 960
2024-11-27 23:30:43.943564: Current learning rate: 0.00055
2024-11-27 23:36:36.147132: train_loss -0.84
2024-11-27 23:36:36.150987: val_loss -0.3787
2024-11-27 23:36:36.152053: Pseudo dice [0.6717]
2024-11-27 23:36:36.153673: Epoch time: 352.21 s
2024-11-27 23:36:37.756183: 
2024-11-27 23:36:37.757461: Epoch 961
2024-11-27 23:36:37.758210: Current learning rate: 0.00054
2024-11-27 23:42:35.448422: train_loss -0.8417
2024-11-27 23:42:35.449609: val_loss -0.3182
2024-11-27 23:42:35.450679: Pseudo dice [0.6181]
2024-11-27 23:42:35.451588: Epoch time: 357.69 s
2024-11-27 23:42:37.279636: 
2024-11-27 23:42:37.282149: Epoch 962
2024-11-27 23:42:37.283250: Current learning rate: 0.00053
2024-11-27 23:48:18.871786: train_loss -0.8438
2024-11-27 23:48:18.872896: val_loss -0.3211
2024-11-27 23:48:18.873763: Pseudo dice [0.6087]
2024-11-27 23:48:18.874489: Epoch time: 341.59 s
2024-11-27 23:48:20.381187: 
2024-11-27 23:48:20.384084: Epoch 963
2024-11-27 23:48:20.384931: Current learning rate: 0.00051
2024-11-27 23:53:45.519003: train_loss -0.8436
2024-11-27 23:53:45.520000: val_loss -0.3083
2024-11-27 23:53:45.520758: Pseudo dice [0.6215]
2024-11-27 23:53:45.521436: Epoch time: 325.14 s
2024-11-27 23:53:47.402028: 
2024-11-27 23:53:47.403260: Epoch 964
2024-11-27 23:53:47.404141: Current learning rate: 0.0005
2024-11-27 23:59:29.892802: train_loss -0.8483
2024-11-27 23:59:29.894063: val_loss -0.3267
2024-11-27 23:59:29.895011: Pseudo dice [0.6223]
2024-11-27 23:59:29.895890: Epoch time: 342.49 s
2024-11-27 23:59:31.574979: 
2024-11-27 23:59:31.577320: Epoch 965
2024-11-27 23:59:31.578565: Current learning rate: 0.00049
2024-11-28 00:05:28.035743: train_loss -0.846
2024-11-28 00:05:28.037044: val_loss -0.3285
2024-11-28 00:05:28.037953: Pseudo dice [0.627]
2024-11-28 00:05:28.038681: Epoch time: 356.46 s
2024-11-28 00:05:29.656838: 
2024-11-28 00:05:29.659421: Epoch 966
2024-11-28 00:05:29.660742: Current learning rate: 0.00048
2024-11-28 00:11:04.158105: train_loss -0.8424
2024-11-28 00:11:04.159274: val_loss -0.3274
2024-11-28 00:11:04.160114: Pseudo dice [0.6406]
2024-11-28 00:11:04.160842: Epoch time: 334.5 s
2024-11-28 00:11:05.725369: 
2024-11-28 00:11:05.726926: Epoch 967
2024-11-28 00:11:05.727710: Current learning rate: 0.00046
2024-11-28 00:16:44.797698: train_loss -0.8417
2024-11-28 00:16:44.798753: val_loss -0.3464
2024-11-28 00:16:44.799796: Pseudo dice [0.6388]
2024-11-28 00:16:44.800501: Epoch time: 339.07 s
2024-11-28 00:16:46.276845: 
2024-11-28 00:16:46.278025: Epoch 968
2024-11-28 00:16:46.279002: Current learning rate: 0.00045
2024-11-28 00:22:17.229510: train_loss -0.8424
2024-11-28 00:22:17.230730: val_loss -0.3633
2024-11-28 00:22:17.232049: Pseudo dice [0.6556]
2024-11-28 00:22:17.233144: Epoch time: 330.95 s
2024-11-28 00:22:17.234565: Yayy! New best EMA pseudo Dice: 0.627
2024-11-28 00:22:19.374060: 
2024-11-28 00:22:19.375621: Epoch 969
2024-11-28 00:22:19.377369: Current learning rate: 0.00044
2024-11-28 00:28:12.537824: train_loss -0.8511
2024-11-28 00:28:12.539046: val_loss -0.3441
2024-11-28 00:28:12.540397: Pseudo dice [0.6394]
2024-11-28 00:28:12.541509: Epoch time: 353.16 s
2024-11-28 00:28:12.542533: Yayy! New best EMA pseudo Dice: 0.6283
2024-11-28 00:28:14.767394: 
2024-11-28 00:28:14.768778: Epoch 970
2024-11-28 00:28:14.770742: Current learning rate: 0.00043
2024-11-28 00:33:47.112054: train_loss -0.8445
2024-11-28 00:33:47.144685: val_loss -0.3164
2024-11-28 00:33:47.146109: Pseudo dice [0.635]
2024-11-28 00:33:47.148843: Epoch time: 332.35 s
2024-11-28 00:33:47.150804: Yayy! New best EMA pseudo Dice: 0.6289
2024-11-28 00:33:49.449566: 
2024-11-28 00:33:49.451547: Epoch 971
2024-11-28 00:33:49.452612: Current learning rate: 0.00041
2024-11-28 00:39:41.461447: train_loss -0.8443
2024-11-28 00:39:41.462435: val_loss -0.2944
2024-11-28 00:39:41.463202: Pseudo dice [0.601]
2024-11-28 00:39:41.463909: Epoch time: 352.01 s
2024-11-28 00:39:42.972205: 
2024-11-28 00:39:42.973887: Epoch 972
2024-11-28 00:39:42.975332: Current learning rate: 0.0004
2024-11-28 00:45:33.900722: train_loss -0.8424
2024-11-28 00:45:33.902092: val_loss -0.3176
2024-11-28 00:45:33.903558: Pseudo dice [0.6127]
2024-11-28 00:45:33.905243: Epoch time: 350.93 s
2024-11-28 00:45:35.572840: 
2024-11-28 00:45:35.574739: Epoch 973
2024-11-28 00:45:35.575661: Current learning rate: 0.00039
2024-11-28 00:51:29.321044: train_loss -0.846
2024-11-28 00:51:29.322018: val_loss -0.3164
2024-11-28 00:51:29.322761: Pseudo dice [0.6054]
2024-11-28 00:51:29.323512: Epoch time: 353.75 s
2024-11-28 00:51:30.815203: 
2024-11-28 00:51:30.817097: Epoch 974
2024-11-28 00:51:30.818288: Current learning rate: 0.00037
2024-11-28 00:57:17.098029: train_loss -0.846
2024-11-28 00:57:17.099386: val_loss -0.3119
2024-11-28 00:57:17.100380: Pseudo dice [0.6042]
2024-11-28 00:57:17.101044: Epoch time: 346.29 s
2024-11-28 00:57:18.971812: 
2024-11-28 00:57:18.973062: Epoch 975
2024-11-28 00:57:18.973731: Current learning rate: 0.00036
2024-11-28 01:02:47.094925: train_loss -0.8458
2024-11-28 01:02:47.095643: val_loss -0.3304
2024-11-28 01:02:47.096320: Pseudo dice [0.6256]
2024-11-28 01:02:47.097006: Epoch time: 328.12 s
2024-11-28 01:02:48.607228: 
2024-11-28 01:02:48.608703: Epoch 976
2024-11-28 01:02:48.609866: Current learning rate: 0.00035
2024-11-28 01:08:25.519684: train_loss -0.848
2024-11-28 01:08:25.520773: val_loss -0.2956
2024-11-28 01:08:25.521543: Pseudo dice [0.6037]
2024-11-28 01:08:25.522337: Epoch time: 336.91 s
2024-11-28 01:08:27.048017: 
2024-11-28 01:08:27.049426: Epoch 977
2024-11-28 01:08:27.050172: Current learning rate: 0.00034
2024-11-28 01:14:24.864629: train_loss -0.845
2024-11-28 01:14:24.868649: val_loss -0.299
2024-11-28 01:14:24.869509: Pseudo dice [0.5823]
2024-11-28 01:14:24.870809: Epoch time: 357.82 s
2024-11-28 01:14:26.453085: 
2024-11-28 01:14:26.454014: Epoch 978
2024-11-28 01:14:26.454941: Current learning rate: 0.00032
2024-11-28 01:20:10.523083: train_loss -0.8398
2024-11-28 01:20:10.523948: val_loss -0.269
2024-11-28 01:20:10.524654: Pseudo dice [0.5784]
2024-11-28 01:20:10.525428: Epoch time: 344.07 s
2024-11-28 01:20:12.032196: 
2024-11-28 01:20:12.033465: Epoch 979
2024-11-28 01:20:12.034251: Current learning rate: 0.00031
2024-11-28 01:25:30.170102: train_loss -0.8517
2024-11-28 01:25:30.171245: val_loss -0.3885
2024-11-28 01:25:30.171991: Pseudo dice [0.6461]
2024-11-28 01:25:30.172676: Epoch time: 318.14 s
2024-11-28 01:25:31.706513: 
2024-11-28 01:25:31.708019: Epoch 980
2024-11-28 01:25:31.708844: Current learning rate: 0.0003
2024-11-28 01:31:12.503994: train_loss -0.8482
2024-11-28 01:31:12.505061: val_loss -0.324
2024-11-28 01:31:12.505904: Pseudo dice [0.6175]
2024-11-28 01:31:12.506728: Epoch time: 340.8 s
2024-11-28 01:31:14.252917: 
2024-11-28 01:31:14.254784: Epoch 981
2024-11-28 01:31:14.256575: Current learning rate: 0.00028
2024-11-28 01:37:13.568825: train_loss -0.8459
2024-11-28 01:37:13.569718: val_loss -0.3316
2024-11-28 01:37:13.570609: Pseudo dice [0.639]
2024-11-28 01:37:13.571386: Epoch time: 359.32 s
2024-11-28 01:37:15.120752: 
2024-11-28 01:37:15.122157: Epoch 982
2024-11-28 01:37:15.122948: Current learning rate: 0.00027
2024-11-28 01:42:39.858973: train_loss -0.8397
2024-11-28 01:42:39.860232: val_loss -0.3476
2024-11-28 01:42:39.861759: Pseudo dice [0.6471]
2024-11-28 01:42:39.863321: Epoch time: 324.74 s
2024-11-28 01:42:41.352626: 
2024-11-28 01:42:41.354076: Epoch 983
2024-11-28 01:42:41.354811: Current learning rate: 0.00026
2024-11-28 01:48:31.209513: train_loss -0.8525
2024-11-28 01:48:31.210550: val_loss -0.2558
2024-11-28 01:48:31.211416: Pseudo dice [0.5647]
2024-11-28 01:48:31.212262: Epoch time: 349.86 s
2024-11-28 01:48:32.705584: 
2024-11-28 01:48:32.706711: Epoch 984
2024-11-28 01:48:32.707566: Current learning rate: 0.00024
2024-11-28 01:54:08.129024: train_loss -0.8438
2024-11-28 01:54:08.130013: val_loss -0.307
2024-11-28 01:54:08.130828: Pseudo dice [0.6001]
2024-11-28 01:54:08.131615: Epoch time: 335.42 s
2024-11-28 01:54:09.683357: 
2024-11-28 01:54:09.685074: Epoch 985
2024-11-28 01:54:09.685807: Current learning rate: 0.00023
2024-11-28 02:00:08.448097: train_loss -0.8451
2024-11-28 02:00:08.448957: val_loss -0.3118
2024-11-28 02:00:08.449630: Pseudo dice [0.6173]
2024-11-28 02:00:08.450240: Epoch time: 358.77 s
2024-11-28 02:00:09.940835: 
2024-11-28 02:00:09.942351: Epoch 986
2024-11-28 02:00:09.943082: Current learning rate: 0.00021
2024-11-28 02:05:59.013678: train_loss -0.8488
2024-11-28 02:05:59.014804: val_loss -0.3035
2024-11-28 02:05:59.015687: Pseudo dice [0.5995]
2024-11-28 02:05:59.016405: Epoch time: 349.07 s
2024-11-28 02:06:00.988318: 
2024-11-28 02:06:00.990664: Epoch 987
2024-11-28 02:06:00.992455: Current learning rate: 0.0002
2024-11-28 02:11:35.750846: train_loss -0.8467
2024-11-28 02:11:35.751786: val_loss -0.3696
2024-11-28 02:11:35.752610: Pseudo dice [0.648]
2024-11-28 02:11:35.753598: Epoch time: 334.76 s
2024-11-28 02:11:37.278769: 
2024-11-28 02:11:37.280235: Epoch 988
2024-11-28 02:11:37.281270: Current learning rate: 0.00019
2024-11-28 02:17:37.584840: train_loss -0.8464
2024-11-28 02:17:37.585662: val_loss -0.3335
2024-11-28 02:17:37.586468: Pseudo dice [0.6401]
2024-11-28 02:17:37.587330: Epoch time: 360.31 s
2024-11-28 02:17:39.438898: 
2024-11-28 02:17:39.440665: Epoch 989
2024-11-28 02:17:39.441620: Current learning rate: 0.00017
2024-11-28 02:23:27.027053: train_loss -0.8505
2024-11-28 02:23:27.039659: val_loss -0.3171
2024-11-28 02:23:27.040541: Pseudo dice [0.64]
2024-11-28 02:23:27.042298: Epoch time: 347.59 s
2024-11-28 02:23:28.618163: 
2024-11-28 02:23:28.619646: Epoch 990
2024-11-28 02:23:28.620700: Current learning rate: 0.00016
2024-11-28 02:29:18.491629: train_loss -0.8451
2024-11-28 02:29:18.493192: val_loss -0.3234
2024-11-28 02:29:18.494310: Pseudo dice [0.6077]
2024-11-28 02:29:18.495487: Epoch time: 349.87 s
2024-11-28 02:29:20.046904: 
2024-11-28 02:29:20.049427: Epoch 991
2024-11-28 02:29:20.050756: Current learning rate: 0.00014
2024-11-28 02:34:41.866126: train_loss -0.847
2024-11-28 02:34:41.868074: val_loss -0.3072
2024-11-28 02:34:41.869262: Pseudo dice [0.6062]
2024-11-28 02:34:41.870125: Epoch time: 321.82 s
2024-11-28 02:34:43.645552: 
2024-11-28 02:34:43.647775: Epoch 992
2024-11-28 02:34:43.649431: Current learning rate: 0.00013
2024-11-28 02:40:04.202130: train_loss -0.8479
2024-11-28 02:40:04.204407: val_loss -0.328
2024-11-28 02:40:04.205934: Pseudo dice [0.6254]
2024-11-28 02:40:04.207117: Epoch time: 320.56 s
2024-11-28 02:40:06.022225: 
2024-11-28 02:40:06.024548: Epoch 993
2024-11-28 02:40:06.025919: Current learning rate: 0.00011
2024-11-28 02:46:12.782592: train_loss -0.8517
2024-11-28 02:46:12.783469: val_loss -0.3545
2024-11-28 02:46:12.784381: Pseudo dice [0.6582]
2024-11-28 02:46:12.785178: Epoch time: 366.76 s
2024-11-28 02:46:14.267741: 
2024-11-28 02:46:14.269409: Epoch 994
2024-11-28 02:46:14.270207: Current learning rate: 0.0001
2024-11-28 02:50:43.929267: train_loss -0.8457
2024-11-28 02:50:43.930066: val_loss -0.3047
2024-11-28 02:50:43.930760: Pseudo dice [0.5983]
2024-11-28 02:50:43.931533: Epoch time: 269.66 s
2024-11-28 02:50:45.447268: 
2024-11-28 02:50:45.449066: Epoch 995
2024-11-28 02:50:45.449860: Current learning rate: 8e-05
2024-11-28 02:55:04.664206: train_loss -0.8427
2024-11-28 02:55:04.665023: val_loss -0.3757
2024-11-28 02:55:04.666114: Pseudo dice [0.6459]
2024-11-28 02:55:04.667025: Epoch time: 259.22 s
2024-11-28 02:55:06.161704: 
2024-11-28 02:55:06.163005: Epoch 996
2024-11-28 02:55:06.163851: Current learning rate: 7e-05
2024-11-28 03:00:15.613911: train_loss -0.8481
2024-11-28 03:00:15.614815: val_loss -0.3652
2024-11-28 03:00:15.615863: Pseudo dice [0.6482]
2024-11-28 03:00:15.616767: Epoch time: 309.45 s
2024-11-28 03:00:17.072769: 
2024-11-28 03:00:17.074322: Epoch 997
2024-11-28 03:00:17.075283: Current learning rate: 5e-05
2024-11-28 03:06:21.606714: train_loss -0.8483
2024-11-28 03:06:21.607667: val_loss -0.3337
2024-11-28 03:06:21.608541: Pseudo dice [0.6146]
2024-11-28 03:06:21.609542: Epoch time: 364.54 s
2024-11-28 03:06:23.090344: 
2024-11-28 03:06:23.091636: Epoch 998
2024-11-28 03:06:23.092670: Current learning rate: 4e-05
2024-11-28 03:12:25.121363: train_loss -0.8448
2024-11-28 03:12:25.122377: val_loss -0.3449
2024-11-28 03:12:25.123206: Pseudo dice [0.6312]
2024-11-28 03:12:25.124300: Epoch time: 362.03 s
2024-11-28 03:12:26.966097: 
2024-11-28 03:12:26.967936: Epoch 999
2024-11-28 03:12:26.969016: Current learning rate: 2e-05
2024-11-28 03:18:03.145916: train_loss -0.8477
2024-11-28 03:18:03.146706: val_loss -0.3289
2024-11-28 03:18:03.147498: Pseudo dice [0.6193]
2024-11-28 03:18:03.148323: Epoch time: 336.18 s
2024-11-28 03:18:05.253333: Training done.
2024-11-28 03:18:05.470241: Using splits from existing split file: /home/gridsan/nchutisilp/datasets/nnUNet_Datasets/nnUNet_preprocessed/Dataset306_Sohee_Ajay_Calcium_OCT/splits_final.json
2024-11-28 03:18:05.471963: The split file contains 5 splits.
2024-11-28 03:18:05.472778: Desired fold for training: 4
2024-11-28 03:18:05.473930: This split has 11 training and 2 validation cases.
2024-11-28 03:18:05.475520: predicting 04010Pre
2024-11-28 03:18:05.511510: 04010Pre, shape torch.Size([1, 248, 498, 498]), rank 0
2024-11-28 03:19:31.694066: predicting 101-045
2024-11-28 03:19:31.712200: 101-045, shape torch.Size([1, 375, 498, 498]), rank 0
2024-11-28 03:21:43.083338: Validation complete
2024-11-28 03:21:43.083869: Mean Validation Dice:  0.5464291476335137
