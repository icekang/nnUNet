/home/gridsan/nchutisilp/.conda/envs/nnunet/bin/python

############################
INFO: You are using the old nnU-Net default plans. We have updated our recommendations. Please consider using those instead! Read more here: https://github.com/MIC-DKFZ/nnUNet/blob/master/documentation/resenc_presets.md
############################

Using device: cuda:0

#######################################################################
Please cite the following paper when using nnU-Net:
Isensee, F., Jaeger, P. F., Kohl, S. A., Petersen, J., & Maier-Hein, K. H. (2021). nnU-Net: a self-configuring method for deep learning-based biomedical image segmentation. Nature methods, 18(2), 203-211.
#######################################################################

2024-11-28 02:55:03.202645: Using torch.compile...
/home/gridsan/nchutisilp/.local/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn("The verbose parameter is deprecated. Please use get_last_lr() "
2024-11-28 02:55:28.937171: do_dummy_2d_data_aug: False
2024-11-28 02:55:28.940698: Using splits from existing split file: /home/gridsan/nchutisilp/datasets/nnUNet_Datasets/nnUNet_preprocessed/Dataset306_Sohee_Ajay_Calcium_OCT/splits_final.json
2024-11-28 02:55:28.943451: The split file contains 5 splits.
2024-11-28 02:55:28.944794: Desired fold for training: 1
2024-11-28 02:55:28.946090: This split has 10 training and 3 validation cases.
using pin_memory on device 0
using pin_memory on device 0

This is the configuration used by this training:
Configuration name: 3d_fullres
 {'data_identifier': 'nnUNetPlans_3d_fullres', 'preprocessor_name': 'DefaultPreprocessor', 'batch_size': 2, 'patch_size': [112, 160, 128], 'median_image_size_in_voxels': [375.0, 498.0, 498.0], 'spacing': [1.0, 1.0, 1.0], 'normalization_schemes': ['ZScoreNormalization'], 'use_mask_for_norm': [False], 'resampling_fn_data': 'resample_data_or_seg_to_shape', 'resampling_fn_seg': 'resample_data_or_seg_to_shape', 'resampling_fn_data_kwargs': {'is_seg': False, 'order': 3, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_seg_kwargs': {'is_seg': True, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_probabilities': 'resample_data_or_seg_to_shape', 'resampling_fn_probabilities_kwargs': {'is_seg': False, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'architecture': {'network_class_name': 'dynamic_network_architectures.architectures.unet.PlainConvUNet', 'arch_kwargs': {'n_stages': 6, 'features_per_stage': [32, 64, 128, 256, 320, 320], 'conv_op': 'torch.nn.modules.conv.Conv3d', 'kernel_sizes': [[3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3]], 'strides': [[1, 1, 1], [2, 2, 2], [2, 2, 2], [2, 2, 2], [2, 2, 2], [1, 2, 2]], 'n_conv_per_stage': [2, 2, 2, 2, 2, 2], 'n_conv_per_stage_decoder': [2, 2, 2, 2, 2], 'conv_bias': True, 'norm_op': 'torch.nn.modules.instancenorm.InstanceNorm3d', 'norm_op_kwargs': {'eps': 1e-05, 'affine': True}, 'dropout_op': None, 'dropout_op_kwargs': None, 'nonlin': 'torch.nn.LeakyReLU', 'nonlin_kwargs': {'inplace': True}, 'deep_supervision': True}, '_kw_requires_import': ['conv_op', 'norm_op', 'dropout_op', 'nonlin']}, 'batch_dice': True} 

These are the global plan.json settings:
 {'dataset_name': 'Dataset306_Sohee_Ajay_Calcium_OCT', 'plans_name': 'nnUNetPlans', 'original_median_spacing_after_transp': [1.0, 1.0, 1.0], 'original_median_shape_after_transp': [375, 498, 498], 'image_reader_writer': 'SimpleITKIO', 'transpose_forward': [0, 1, 2], 'transpose_backward': [0, 1, 2], 'experiment_planner_used': 'ExperimentPlanner', 'label_manager': 'LabelManager', 'foreground_intensity_properties_per_channel': {'0': {'max': 0.49803921580314636, 'mean': 0.14296366274356842, 'median': 0.10980392247438431, 'min': 0.0, 'percentile_00_5': 0.01558823511004448, 'percentile_99_5': 0.49803921580314636, 'std': 0.11562220752239227}}} 

2024-11-28 02:55:48.317086: unpacking dataset...
2024-11-28 02:55:52.928706: unpacking done...
2024-11-28 02:55:53.225063: Unable to plot network architecture: nnUNet_compile is enabled!
2024-11-28 02:55:53.509224: 
2024-11-28 02:55:53.510430: Epoch 850
2024-11-28 02:55:53.511335: Current learning rate: 0.00181
2024-11-28 03:03:25.175140: train_loss -0.8312
2024-11-28 03:03:25.178389: val_loss -0.4324
2024-11-28 03:03:25.179533: Pseudo dice [0.7005]
2024-11-28 03:03:25.180457: Epoch time: 451.67 s
2024-11-28 03:03:27.368495: 
2024-11-28 03:03:27.369997: Epoch 851
2024-11-28 03:03:27.371006: Current learning rate: 0.0018
2024-11-28 03:08:52.556980: train_loss -0.8222
2024-11-28 03:08:52.558173: val_loss -0.4416
2024-11-28 03:08:52.559092: Pseudo dice [0.7104]
2024-11-28 03:08:52.559968: Epoch time: 325.19 s
2024-11-28 03:08:54.159692: 
2024-11-28 03:08:54.161186: Epoch 852
2024-11-28 03:08:54.161987: Current learning rate: 0.00179
2024-11-28 03:14:20.278341: train_loss -0.8243
2024-11-28 03:14:20.279377: val_loss -0.4572
2024-11-28 03:14:20.280144: Pseudo dice [0.712]
2024-11-28 03:14:20.280858: Epoch time: 326.12 s
2024-11-28 03:14:21.788679: 
2024-11-28 03:14:21.789868: Epoch 853
2024-11-28 03:14:21.790653: Current learning rate: 0.00178
2024-11-28 03:19:13.891721: train_loss -0.8172
2024-11-28 03:19:13.892773: val_loss -0.3921
2024-11-28 03:19:13.893790: Pseudo dice [0.6868]
2024-11-28 03:19:13.894765: Epoch time: 292.1 s
2024-11-28 03:19:15.482894: 
2024-11-28 03:19:15.484418: Epoch 854
2024-11-28 03:19:15.485425: Current learning rate: 0.00177
2024-11-28 03:22:47.603057: train_loss -0.8182
2024-11-28 03:22:47.603994: val_loss -0.4256
2024-11-28 03:22:47.604908: Pseudo dice [0.6895]
2024-11-28 03:22:47.605793: Epoch time: 212.12 s
2024-11-28 03:22:49.059216: 
2024-11-28 03:22:49.060773: Epoch 855
2024-11-28 03:22:49.061843: Current learning rate: 0.00176
2024-11-28 03:27:02.973964: train_loss -0.8181
2024-11-28 03:27:02.974858: val_loss -0.46
2024-11-28 03:27:02.975642: Pseudo dice [0.7208]
2024-11-28 03:27:02.976423: Epoch time: 253.92 s
2024-11-28 03:27:04.994829: 
2024-11-28 03:27:04.996324: Epoch 856
2024-11-28 03:27:04.997196: Current learning rate: 0.00175
2024-11-28 03:30:46.338357: train_loss -0.8267
2024-11-28 03:30:46.339283: val_loss -0.469
2024-11-28 03:30:46.340183: Pseudo dice [0.7222]
2024-11-28 03:30:46.340929: Epoch time: 221.34 s
2024-11-28 03:30:47.931170: 
2024-11-28 03:30:47.932666: Epoch 857
2024-11-28 03:30:47.933645: Current learning rate: 0.00174
2024-11-28 03:34:39.943284: train_loss -0.8228
2024-11-28 03:34:39.944323: val_loss -0.4374
2024-11-28 03:34:39.945147: Pseudo dice [0.7017]
2024-11-28 03:34:39.946068: Epoch time: 232.01 s
2024-11-28 03:34:41.505183: 
2024-11-28 03:34:41.506549: Epoch 858
2024-11-28 03:34:41.507353: Current learning rate: 0.00173
2024-11-28 03:38:41.350109: train_loss -0.8195
2024-11-28 03:38:41.351068: val_loss -0.4703
2024-11-28 03:38:41.351959: Pseudo dice [0.7202]
2024-11-28 03:38:41.353083: Epoch time: 239.85 s
2024-11-28 03:38:42.918727: 
2024-11-28 03:38:42.920172: Epoch 859
2024-11-28 03:38:42.921021: Current learning rate: 0.00172
2024-11-28 03:42:17.139806: train_loss -0.8211
2024-11-28 03:42:17.140705: val_loss -0.399
2024-11-28 03:42:17.141601: Pseudo dice [0.6863]
2024-11-28 03:42:17.142454: Epoch time: 214.22 s
2024-11-28 03:42:18.599167: 
2024-11-28 03:42:18.600494: Epoch 860
2024-11-28 03:42:18.601196: Current learning rate: 0.0017
2024-11-28 03:46:35.136862: train_loss -0.822
2024-11-28 03:46:35.137890: val_loss -0.4487
2024-11-28 03:46:35.138803: Pseudo dice [0.7134]
2024-11-28 03:46:35.139601: Epoch time: 256.54 s
2024-11-28 03:46:36.670147: 
2024-11-28 03:46:36.672007: Epoch 861
2024-11-28 03:46:36.672884: Current learning rate: 0.00169
2024-11-28 03:50:49.670208: train_loss -0.8255
2024-11-28 03:50:49.671165: val_loss -0.4327
2024-11-28 03:50:49.672220: Pseudo dice [0.7051]
2024-11-28 03:50:49.673257: Epoch time: 253.0 s
2024-11-28 03:50:51.186591: 
2024-11-28 03:50:51.188475: Epoch 862
2024-11-28 03:50:51.189807: Current learning rate: 0.00168
2024-11-28 03:54:48.894428: train_loss -0.827
2024-11-28 03:54:48.895529: val_loss -0.4613
2024-11-28 03:54:48.896455: Pseudo dice [0.7107]
2024-11-28 03:54:48.897313: Epoch time: 237.71 s
2024-11-28 03:54:50.424490: 
2024-11-28 03:54:50.425858: Epoch 863
2024-11-28 03:54:50.426653: Current learning rate: 0.00167
2024-11-28 03:58:48.859698: train_loss -0.8219
2024-11-28 03:58:48.860807: val_loss -0.4784
2024-11-28 03:58:48.861695: Pseudo dice [0.724]
2024-11-28 03:58:48.862520: Epoch time: 238.44 s
2024-11-28 03:58:50.359011: 
2024-11-28 03:58:50.360519: Epoch 864
2024-11-28 03:58:50.361238: Current learning rate: 0.00166
2024-11-28 04:02:52.102865: train_loss -0.8236
2024-11-28 04:02:52.105111: val_loss -0.4719
2024-11-28 04:02:52.105955: Pseudo dice [0.7203]
2024-11-28 04:02:52.106664: Epoch time: 241.74 s
2024-11-28 04:02:53.617697: 
2024-11-28 04:02:53.619093: Epoch 865
2024-11-28 04:02:53.619938: Current learning rate: 0.00165
2024-11-28 04:06:53.047982: train_loss -0.8188
2024-11-28 04:06:53.048887: val_loss -0.426
2024-11-28 04:06:53.049782: Pseudo dice [0.6925]
2024-11-28 04:06:53.050606: Epoch time: 239.43 s
2024-11-28 04:06:54.580830: 
2024-11-28 04:06:54.582226: Epoch 866
2024-11-28 04:06:54.583054: Current learning rate: 0.00164
2024-11-28 04:10:43.431215: train_loss -0.8212
2024-11-28 04:10:43.432855: val_loss -0.404
2024-11-28 04:10:43.433778: Pseudo dice [0.6951]
2024-11-28 04:10:43.434731: Epoch time: 228.85 s
2024-11-28 04:10:44.942179: 
2024-11-28 04:10:44.943695: Epoch 867
2024-11-28 04:10:44.944604: Current learning rate: 0.00163
2024-11-28 04:14:29.311245: train_loss -0.8269
2024-11-28 04:14:29.312182: val_loss -0.4683
2024-11-28 04:14:29.312898: Pseudo dice [0.718]
2024-11-28 04:14:29.313662: Epoch time: 224.37 s
2024-11-28 04:14:31.287528: 
2024-11-28 04:14:31.288941: Epoch 868
2024-11-28 04:14:31.289778: Current learning rate: 0.00162
2024-11-28 04:18:34.764573: train_loss -0.8314
2024-11-28 04:18:34.765620: val_loss -0.4624
2024-11-28 04:18:34.766627: Pseudo dice [0.7256]
2024-11-28 04:18:34.767647: Epoch time: 243.48 s
2024-11-28 04:18:36.374237: 
2024-11-28 04:18:36.375787: Epoch 869
2024-11-28 04:18:36.376769: Current learning rate: 0.00161
2024-11-28 04:22:55.001039: train_loss -0.8252
2024-11-28 04:22:55.002096: val_loss -0.4346
2024-11-28 04:22:55.002927: Pseudo dice [0.7186]
2024-11-28 04:22:55.003885: Epoch time: 258.63 s
2024-11-28 04:22:56.466864: 
2024-11-28 04:22:56.468167: Epoch 870
2024-11-28 04:22:56.469032: Current learning rate: 0.00159
2024-11-28 04:26:55.830816: train_loss -0.8223
2024-11-28 04:26:55.831989: val_loss -0.5078
2024-11-28 04:26:55.832810: Pseudo dice [0.7422]
2024-11-28 04:26:55.833507: Epoch time: 239.37 s
2024-11-28 04:26:57.362179: 
2024-11-28 04:26:57.363467: Epoch 871
2024-11-28 04:26:57.364165: Current learning rate: 0.00158
2024-11-28 04:31:16.463498: train_loss -0.8228
2024-11-28 04:31:16.464423: val_loss -0.445
2024-11-28 04:31:16.465402: Pseudo dice [0.6985]
2024-11-28 04:31:16.466172: Epoch time: 259.1 s
2024-11-28 04:31:17.989796: 
2024-11-28 04:31:17.991292: Epoch 872
2024-11-28 04:31:17.992324: Current learning rate: 0.00157
2024-11-28 04:35:05.342822: train_loss -0.8247
2024-11-28 04:35:05.343761: val_loss -0.4547
2024-11-28 04:35:05.344531: Pseudo dice [0.7056]
2024-11-28 04:35:05.345356: Epoch time: 227.35 s
2024-11-28 04:35:06.835280: 
2024-11-28 04:35:06.836913: Epoch 873
2024-11-28 04:35:06.837930: Current learning rate: 0.00156
2024-11-28 04:39:20.963239: train_loss -0.828
2024-11-28 04:39:20.964202: val_loss -0.4627
2024-11-28 04:39:20.965101: Pseudo dice [0.7137]
2024-11-28 04:39:20.965922: Epoch time: 254.13 s
2024-11-28 04:39:22.470664: 
2024-11-28 04:39:22.472012: Epoch 874
2024-11-28 04:39:22.472877: Current learning rate: 0.00155
2024-11-28 04:43:22.249977: train_loss -0.8295
2024-11-28 04:43:22.250897: val_loss -0.4857
2024-11-28 04:43:22.251614: Pseudo dice [0.7403]
2024-11-28 04:43:22.252361: Epoch time: 239.78 s
2024-11-28 04:43:23.698030: 
2024-11-28 04:43:23.699298: Epoch 875
2024-11-28 04:43:23.700059: Current learning rate: 0.00154
2024-11-28 04:47:11.546546: train_loss -0.8198
2024-11-28 04:47:11.547755: val_loss -0.423
2024-11-28 04:47:11.548733: Pseudo dice [0.7003]
2024-11-28 04:47:11.549594: Epoch time: 227.85 s
2024-11-28 04:47:13.038881: 
2024-11-28 04:47:13.040306: Epoch 876
2024-11-28 04:47:13.041062: Current learning rate: 0.00153
2024-11-28 04:51:23.706907: train_loss -0.8145
2024-11-28 04:51:23.707867: val_loss -0.4566
2024-11-28 04:51:23.708595: Pseudo dice [0.7093]
2024-11-28 04:51:23.709283: Epoch time: 250.67 s
2024-11-28 04:51:25.251148: 
2024-11-28 04:51:25.254004: Epoch 877
2024-11-28 04:51:25.255107: Current learning rate: 0.00152
2024-11-28 04:55:22.602834: train_loss -0.8185
2024-11-28 04:55:22.603855: val_loss -0.4138
2024-11-28 04:55:22.604632: Pseudo dice [0.6896]
2024-11-28 04:55:22.605326: Epoch time: 237.35 s
2024-11-28 04:55:24.118057: 
2024-11-28 04:55:24.119462: Epoch 878
2024-11-28 04:55:24.120354: Current learning rate: 0.00151
2024-11-28 04:59:09.732244: train_loss -0.8183
2024-11-28 04:59:09.733290: val_loss -0.4583
2024-11-28 04:59:09.734215: Pseudo dice [0.7075]
2024-11-28 04:59:09.734998: Epoch time: 225.62 s
2024-11-28 04:59:11.621453: 
2024-11-28 04:59:11.622847: Epoch 879
2024-11-28 04:59:11.623732: Current learning rate: 0.00149
2024-11-28 05:03:18.654652: train_loss -0.8234
2024-11-28 05:03:18.656106: val_loss -0.4607
2024-11-28 05:03:18.657044: Pseudo dice [0.723]
2024-11-28 05:03:18.658016: Epoch time: 247.03 s
2024-11-28 05:03:20.145004: 
2024-11-28 05:03:20.146314: Epoch 880
2024-11-28 05:03:20.147074: Current learning rate: 0.00148
2024-11-28 05:07:22.928472: train_loss -0.8228
2024-11-28 05:07:22.932851: val_loss -0.4461
2024-11-28 05:07:22.933765: Pseudo dice [0.7055]
2024-11-28 05:07:22.934970: Epoch time: 242.78 s
2024-11-28 05:07:24.545783: 
2024-11-28 05:07:24.547225: Epoch 881
2024-11-28 05:07:24.548038: Current learning rate: 0.00147
2024-11-28 05:11:40.060680: train_loss -0.829
2024-11-28 05:11:40.061919: val_loss -0.4565
2024-11-28 05:11:40.062918: Pseudo dice [0.7119]
2024-11-28 05:11:40.063740: Epoch time: 255.52 s
2024-11-28 05:11:41.577287: 
2024-11-28 05:11:41.578703: Epoch 882
2024-11-28 05:11:41.579645: Current learning rate: 0.00146
2024-11-28 05:15:58.159089: train_loss -0.8272
2024-11-28 05:15:58.160269: val_loss -0.4705
2024-11-28 05:15:58.161222: Pseudo dice [0.7119]
2024-11-28 05:15:58.162160: Epoch time: 256.58 s
2024-11-28 05:15:59.752236: 
2024-11-28 05:15:59.753998: Epoch 883
2024-11-28 05:15:59.755095: Current learning rate: 0.00145
2024-11-28 05:19:48.072784: train_loss -0.8315
2024-11-28 05:19:48.074765: val_loss -0.4791
2024-11-28 05:19:48.075870: Pseudo dice [0.7208]
2024-11-28 05:19:48.077392: Epoch time: 228.32 s
2024-11-28 05:19:49.668872: 
2024-11-28 05:19:49.670317: Epoch 884
2024-11-28 05:19:49.671264: Current learning rate: 0.00144
2024-11-28 05:23:19.554395: train_loss -0.8273
2024-11-28 05:23:19.555279: val_loss -0.4496
2024-11-28 05:23:19.556044: Pseudo dice [0.7044]
2024-11-28 05:23:19.556773: Epoch time: 209.89 s
2024-11-28 05:23:21.140393: 
2024-11-28 05:23:21.141853: Epoch 885
2024-11-28 05:23:21.142745: Current learning rate: 0.00143
2024-11-28 05:27:21.785090: train_loss -0.8222
2024-11-28 05:27:21.786085: val_loss -0.4871
2024-11-28 05:27:21.787044: Pseudo dice [0.735]
2024-11-28 05:27:21.787997: Epoch time: 240.65 s
2024-11-28 05:27:23.347755: 
2024-11-28 05:27:23.349226: Epoch 886
2024-11-28 05:27:23.350216: Current learning rate: 0.00142
2024-11-28 05:31:30.958028: train_loss -0.8238
2024-11-28 05:31:30.959136: val_loss -0.4214
2024-11-28 05:31:30.960078: Pseudo dice [0.6869]
2024-11-28 05:31:30.960876: Epoch time: 247.61 s
2024-11-28 05:31:32.577287: 
2024-11-28 05:31:32.578913: Epoch 887
2024-11-28 05:31:32.580061: Current learning rate: 0.00141
2024-11-28 05:35:14.371878: train_loss -0.829
2024-11-28 05:35:14.372983: val_loss -0.5105
2024-11-28 05:35:14.374123: Pseudo dice [0.7408]
2024-11-28 05:35:14.375088: Epoch time: 221.8 s
2024-11-28 05:35:15.946499: 
2024-11-28 05:35:15.947945: Epoch 888
2024-11-28 05:35:15.948945: Current learning rate: 0.00139
2024-11-28 05:39:25.303024: train_loss -0.831
2024-11-28 05:39:25.303943: val_loss -0.4675
2024-11-28 05:39:25.304837: Pseudo dice [0.7176]
2024-11-28 05:39:25.305497: Epoch time: 249.36 s
2024-11-28 05:39:26.828864: 
2024-11-28 05:39:26.829841: Epoch 889
2024-11-28 05:39:26.830619: Current learning rate: 0.00138
2024-11-28 05:44:06.453718: train_loss -0.8234
2024-11-28 05:44:06.454755: val_loss -0.4717
2024-11-28 05:44:06.455523: Pseudo dice [0.722]
2024-11-28 05:44:06.456264: Epoch time: 279.63 s
2024-11-28 05:44:07.901270: 
2024-11-28 05:44:07.902441: Epoch 890
2024-11-28 05:44:07.903147: Current learning rate: 0.00137
2024-11-28 05:48:01.453585: train_loss -0.8267
2024-11-28 05:48:01.454647: val_loss -0.4493
2024-11-28 05:48:01.455634: Pseudo dice [0.7325]
2024-11-28 05:48:01.456604: Epoch time: 233.55 s
2024-11-28 05:48:04.500789: 
2024-11-28 05:48:04.502376: Epoch 891
2024-11-28 05:48:04.503395: Current learning rate: 0.00136
2024-11-28 05:51:59.280066: train_loss -0.8269
2024-11-28 05:51:59.281031: val_loss -0.4533
2024-11-28 05:51:59.282020: Pseudo dice [0.7141]
2024-11-28 05:51:59.282916: Epoch time: 234.78 s
2024-11-28 05:52:00.798336: 
2024-11-28 05:52:00.799762: Epoch 892
2024-11-28 05:52:00.800594: Current learning rate: 0.00135
2024-11-28 05:55:41.548139: train_loss -0.8255
2024-11-28 05:55:41.549274: val_loss -0.4716
2024-11-28 05:55:41.550126: Pseudo dice [0.7345]
2024-11-28 05:55:41.550850: Epoch time: 220.75 s
2024-11-28 05:55:43.114759: 
2024-11-28 05:55:43.116093: Epoch 893
2024-11-28 05:55:43.116922: Current learning rate: 0.00134
2024-11-28 05:59:18.338319: train_loss -0.8247
2024-11-28 05:59:18.339234: val_loss -0.4997
2024-11-28 05:59:18.340093: Pseudo dice [0.736]
2024-11-28 05:59:18.340892: Epoch time: 215.22 s
2024-11-28 05:59:19.840813: 
2024-11-28 05:59:19.842339: Epoch 894
2024-11-28 05:59:19.843383: Current learning rate: 0.00133
2024-11-28 06:02:56.499176: train_loss -0.833
2024-11-28 06:02:56.500013: val_loss -0.4692
2024-11-28 06:02:56.500935: Pseudo dice [0.7235]
2024-11-28 06:02:56.501712: Epoch time: 216.66 s
2024-11-28 06:02:58.197969: 
2024-11-28 06:02:58.199704: Epoch 895
2024-11-28 06:02:58.200854: Current learning rate: 0.00132
2024-11-28 06:07:23.864372: train_loss -0.8325
2024-11-28 06:07:23.865963: val_loss -0.4221
2024-11-28 06:07:23.866923: Pseudo dice [0.6822]
2024-11-28 06:07:23.867935: Epoch time: 265.67 s
2024-11-28 06:07:25.375031: 
2024-11-28 06:07:25.376666: Epoch 896
2024-11-28 06:07:25.377653: Current learning rate: 0.0013
2024-11-28 06:11:21.378419: train_loss -0.8296
2024-11-28 06:11:21.380157: val_loss -0.4462
2024-11-28 06:11:21.381102: Pseudo dice [0.705]
2024-11-28 06:11:21.382126: Epoch time: 236.0 s
2024-11-28 06:11:22.934855: 
2024-11-28 06:11:22.936934: Epoch 897
2024-11-28 06:11:22.937863: Current learning rate: 0.00129
2024-11-28 06:15:10.632702: train_loss -0.8306
2024-11-28 06:15:10.634369: val_loss -0.4271
2024-11-28 06:15:10.635263: Pseudo dice [0.7005]
2024-11-28 06:15:10.636499: Epoch time: 227.7 s
2024-11-28 06:15:12.157008: 
2024-11-28 06:15:12.158457: Epoch 898
2024-11-28 06:15:12.159235: Current learning rate: 0.00128
2024-11-28 06:19:25.354899: train_loss -0.8311
2024-11-28 06:19:25.355978: val_loss -0.4647
2024-11-28 06:19:25.356944: Pseudo dice [0.7178]
2024-11-28 06:19:25.357895: Epoch time: 253.2 s
2024-11-28 06:19:26.872149: 
2024-11-28 06:19:26.873724: Epoch 899
2024-11-28 06:19:26.874707: Current learning rate: 0.00127
2024-11-28 06:23:14.570913: train_loss -0.8258
2024-11-28 06:23:14.571874: val_loss -0.505
2024-11-28 06:23:14.572683: Pseudo dice [0.7453]
2024-11-28 06:23:14.573447: Epoch time: 227.7 s
2024-11-28 06:23:16.772009: 
2024-11-28 06:23:16.773340: Epoch 900
2024-11-28 06:23:16.774173: Current learning rate: 0.00126
2024-11-28 06:27:18.040290: train_loss -0.8285
2024-11-28 06:27:18.041641: val_loss -0.4435
2024-11-28 06:27:18.042668: Pseudo dice [0.7027]
2024-11-28 06:27:18.043659: Epoch time: 241.27 s
2024-11-28 06:27:19.519662: 
2024-11-28 06:27:19.520999: Epoch 901
2024-11-28 06:27:19.521831: Current learning rate: 0.00125
2024-11-28 06:31:21.973562: train_loss -0.8291
2024-11-28 06:31:21.974798: val_loss -0.4935
2024-11-28 06:31:21.975814: Pseudo dice [0.7213]
2024-11-28 06:31:21.976606: Epoch time: 242.46 s
2024-11-28 06:31:23.578535: 
2024-11-28 06:31:23.579985: Epoch 902
2024-11-28 06:31:23.580774: Current learning rate: 0.00124
2024-11-28 06:35:59.585539: train_loss -0.8244
2024-11-28 06:35:59.586501: val_loss -0.4709
2024-11-28 06:35:59.587301: Pseudo dice [0.7212]
2024-11-28 06:35:59.588089: Epoch time: 276.01 s
2024-11-28 06:36:01.470645: 
2024-11-28 06:36:01.472046: Epoch 903
2024-11-28 06:36:01.472826: Current learning rate: 0.00122
2024-11-28 06:40:03.000608: train_loss -0.8345
2024-11-28 06:40:03.001700: val_loss -0.444
2024-11-28 06:40:03.002553: Pseudo dice [0.705]
2024-11-28 06:40:03.003321: Epoch time: 241.53 s
2024-11-28 06:40:04.448944: 
2024-11-28 06:40:04.450530: Epoch 904
2024-11-28 06:40:04.451389: Current learning rate: 0.00121
2024-11-28 06:44:14.050906: train_loss -0.8298
2024-11-28 06:44:14.051963: val_loss -0.4684
2024-11-28 06:44:14.052887: Pseudo dice [0.7217]
2024-11-28 06:44:14.053943: Epoch time: 249.6 s
2024-11-28 06:44:15.562470: 
2024-11-28 06:44:15.563924: Epoch 905
2024-11-28 06:44:15.564979: Current learning rate: 0.0012
2024-11-28 06:48:19.100297: train_loss -0.8338
2024-11-28 06:48:19.101462: val_loss -0.4
2024-11-28 06:48:19.102530: Pseudo dice [0.6786]
2024-11-28 06:48:19.103390: Epoch time: 243.54 s
2024-11-28 06:48:20.677025: 
2024-11-28 06:48:20.678501: Epoch 906
2024-11-28 06:48:20.679482: Current learning rate: 0.00119
2024-11-28 06:52:17.081832: train_loss -0.8272
2024-11-28 06:52:17.082911: val_loss -0.4677
2024-11-28 06:52:17.083661: Pseudo dice [0.7379]
2024-11-28 06:52:17.084365: Epoch time: 236.41 s
2024-11-28 06:52:18.603697: 
2024-11-28 06:52:18.605180: Epoch 907
2024-11-28 06:52:18.605923: Current learning rate: 0.00118
2024-11-28 06:56:25.552796: train_loss -0.8294
2024-11-28 06:56:25.553834: val_loss -0.433
2024-11-28 06:56:25.554656: Pseudo dice [0.6948]
2024-11-28 06:56:25.555488: Epoch time: 246.95 s
2024-11-28 06:56:27.095870: 
2024-11-28 06:56:27.097240: Epoch 908
2024-11-28 06:56:27.098034: Current learning rate: 0.00117
2024-11-28 07:00:09.209479: train_loss -0.8253
2024-11-28 07:00:09.210581: val_loss -0.4625
2024-11-28 07:00:09.211711: Pseudo dice [0.723]
2024-11-28 07:00:09.212742: Epoch time: 222.12 s
2024-11-28 07:00:10.703405: 
2024-11-28 07:00:10.704872: Epoch 909
2024-11-28 07:00:10.705916: Current learning rate: 0.00116
2024-11-28 07:04:16.529703: train_loss -0.8316
2024-11-28 07:04:16.530404: val_loss -0.4584
2024-11-28 07:04:16.531195: Pseudo dice [0.7098]
2024-11-28 07:04:16.532047: Epoch time: 245.83 s
2024-11-28 07:04:18.195163: 
2024-11-28 07:04:18.196309: Epoch 910
2024-11-28 07:04:18.197112: Current learning rate: 0.00115
2024-11-28 07:07:44.193075: train_loss -0.832
2024-11-28 07:07:44.194109: val_loss -0.4436
2024-11-28 07:07:44.194959: Pseudo dice [0.7106]
2024-11-28 07:07:44.195904: Epoch time: 206.0 s
2024-11-28 07:07:45.666771: 
2024-11-28 07:07:45.668190: Epoch 911
2024-11-28 07:07:45.669086: Current learning rate: 0.00113
2024-11-28 07:11:38.172647: train_loss -0.8298
2024-11-28 07:11:38.173712: val_loss -0.4635
2024-11-28 07:11:38.174477: Pseudo dice [0.7103]
2024-11-28 07:11:38.175229: Epoch time: 232.51 s
2024-11-28 07:11:39.721875: 
2024-11-28 07:11:39.723361: Epoch 912
2024-11-28 07:11:39.724272: Current learning rate: 0.00112
2024-11-28 07:15:49.171366: train_loss -0.8316
2024-11-28 07:15:49.174803: val_loss -0.4286
2024-11-28 07:15:49.175725: Pseudo dice [0.6922]
2024-11-28 07:15:49.176513: Epoch time: 249.45 s
2024-11-28 07:15:50.704003: 
2024-11-28 07:15:50.705563: Epoch 913
2024-11-28 07:15:50.706929: Current learning rate: 0.00111
2024-11-28 07:19:59.372333: train_loss -0.8263
2024-11-28 07:19:59.374443: val_loss -0.4048
2024-11-28 07:19:59.375465: Pseudo dice [0.6924]
2024-11-28 07:19:59.376657: Epoch time: 248.67 s
2024-11-28 07:20:01.352437: 
2024-11-28 07:20:01.353804: Epoch 914
2024-11-28 07:20:01.354634: Current learning rate: 0.0011
2024-11-28 07:24:07.552653: train_loss -0.8299
2024-11-28 07:24:07.553674: val_loss -0.4116
2024-11-28 07:24:07.554594: Pseudo dice [0.696]
2024-11-28 07:24:07.555364: Epoch time: 246.2 s
2024-11-28 07:24:09.024627: 
2024-11-28 07:24:09.026212: Epoch 915
2024-11-28 07:24:09.027217: Current learning rate: 0.00109
2024-11-28 07:28:32.401537: train_loss -0.8287
2024-11-28 07:28:32.402677: val_loss -0.4379
2024-11-28 07:28:32.403658: Pseudo dice [0.7213]
2024-11-28 07:28:32.404585: Epoch time: 263.38 s
2024-11-28 07:28:34.009650: 
2024-11-28 07:28:34.011315: Epoch 916
2024-11-28 07:28:34.012329: Current learning rate: 0.00108
2024-11-28 07:32:33.703269: train_loss -0.8343
2024-11-28 07:32:33.705228: val_loss -0.4147
2024-11-28 07:32:33.706097: Pseudo dice [0.6965]
2024-11-28 07:32:33.707047: Epoch time: 239.69 s
2024-11-28 07:32:35.205244: 
2024-11-28 07:32:35.206711: Epoch 917
2024-11-28 07:32:35.207612: Current learning rate: 0.00106
2024-11-28 07:37:03.704608: train_loss -0.8342
2024-11-28 07:37:03.705620: val_loss -0.4518
2024-11-28 07:37:03.706472: Pseudo dice [0.7245]
2024-11-28 07:37:03.707214: Epoch time: 268.5 s
2024-11-28 07:37:05.296055: 
2024-11-28 07:37:05.297389: Epoch 918
2024-11-28 07:37:05.298263: Current learning rate: 0.00105
2024-11-28 07:41:04.370439: train_loss -0.8302
2024-11-28 07:41:04.371758: val_loss -0.4324
2024-11-28 07:41:04.372657: Pseudo dice [0.6926]
2024-11-28 07:41:04.373909: Epoch time: 239.08 s
2024-11-28 07:41:05.832078: 
2024-11-28 07:41:05.834100: Epoch 919
2024-11-28 07:41:05.834880: Current learning rate: 0.00104
2024-11-28 07:45:11.055216: train_loss -0.8243
2024-11-28 07:45:11.056285: val_loss -0.4551
2024-11-28 07:45:11.057299: Pseudo dice [0.7034]
2024-11-28 07:45:11.058383: Epoch time: 245.22 s
2024-11-28 07:45:12.526016: 
2024-11-28 07:45:12.527589: Epoch 920
2024-11-28 07:45:12.528607: Current learning rate: 0.00103
2024-11-28 07:49:08.983295: train_loss -0.8272
2024-11-28 07:49:08.984336: val_loss -0.457
2024-11-28 07:49:08.985251: Pseudo dice [0.7154]
2024-11-28 07:49:08.986203: Epoch time: 236.46 s
2024-11-28 07:49:10.431507: 
2024-11-28 07:49:10.432733: Epoch 921
2024-11-28 07:49:10.433655: Current learning rate: 0.00102
2024-11-28 07:53:26.128008: train_loss -0.8213
2024-11-28 07:53:26.128810: val_loss -0.4856
2024-11-28 07:53:26.129576: Pseudo dice [0.7224]
2024-11-28 07:53:26.130397: Epoch time: 255.7 s
2024-11-28 07:53:27.571391: 
2024-11-28 07:53:27.572738: Epoch 922
2024-11-28 07:53:27.573455: Current learning rate: 0.00101
2024-11-28 07:57:26.786204: train_loss -0.8255
2024-11-28 07:57:26.787186: val_loss -0.4847
2024-11-28 07:57:26.787943: Pseudo dice [0.727]
2024-11-28 07:57:26.788614: Epoch time: 239.22 s
2024-11-28 07:57:28.225464: 
2024-11-28 07:57:28.226770: Epoch 923
2024-11-28 07:57:28.227530: Current learning rate: 0.001
2024-11-28 08:01:34.120672: train_loss -0.836
2024-11-28 08:01:34.121735: val_loss -0.4679
2024-11-28 08:01:34.122587: Pseudo dice [0.7321]
2024-11-28 08:01:34.123325: Epoch time: 245.9 s
2024-11-28 08:01:35.552476: 
2024-11-28 08:01:35.553847: Epoch 924
2024-11-28 08:01:35.554582: Current learning rate: 0.00098
2024-11-28 08:06:10.333573: train_loss -0.823
2024-11-28 08:06:10.334573: val_loss -0.4886
2024-11-28 08:06:10.335635: Pseudo dice [0.724]
2024-11-28 08:06:10.336431: Epoch time: 274.78 s
2024-11-28 08:06:11.765795: 
2024-11-28 08:06:11.767146: Epoch 925
2024-11-28 08:06:11.767878: Current learning rate: 0.00097
2024-11-28 08:10:32.481657: train_loss -0.8365
2024-11-28 08:10:32.482678: val_loss -0.4438
2024-11-28 08:10:32.483445: Pseudo dice [0.7002]
2024-11-28 08:10:32.484118: Epoch time: 260.72 s
2024-11-28 08:10:34.523437: 
2024-11-28 08:10:34.524707: Epoch 926
2024-11-28 08:10:34.525531: Current learning rate: 0.00096
2024-11-28 08:14:27.357477: train_loss -0.832
2024-11-28 08:14:27.358429: val_loss -0.4992
2024-11-28 08:14:27.359153: Pseudo dice [0.7293]
2024-11-28 08:14:27.359943: Epoch time: 232.84 s
2024-11-28 08:14:28.837349: 
2024-11-28 08:14:28.838553: Epoch 927
2024-11-28 08:14:28.839279: Current learning rate: 0.00095
2024-11-28 08:19:25.973611: train_loss -0.8306
2024-11-28 08:19:25.975698: val_loss -0.4622
2024-11-28 08:19:25.976852: Pseudo dice [0.7143]
2024-11-28 08:19:25.977637: Epoch time: 297.14 s
2024-11-28 08:19:27.437561: 
2024-11-28 08:19:27.438680: Epoch 928
2024-11-28 08:19:27.439625: Current learning rate: 0.00094
2024-11-28 08:23:27.006749: train_loss -0.8341
2024-11-28 08:23:27.009696: val_loss -0.455
2024-11-28 08:23:27.010652: Pseudo dice [0.7221]
2024-11-28 08:23:27.011688: Epoch time: 239.57 s
2024-11-28 08:23:28.449284: 
2024-11-28 08:23:28.450661: Epoch 929
2024-11-28 08:23:28.451511: Current learning rate: 0.00092
2024-11-28 08:27:38.178631: train_loss -0.835
2024-11-28 08:27:38.179662: val_loss -0.4232
2024-11-28 08:27:38.180564: Pseudo dice [0.7084]
2024-11-28 08:27:38.181404: Epoch time: 249.73 s
2024-11-28 08:27:39.619703: 
2024-11-28 08:27:39.621142: Epoch 930
2024-11-28 08:27:39.621987: Current learning rate: 0.00091
2024-11-28 08:32:25.310391: train_loss -0.831
2024-11-28 08:32:25.311220: val_loss -0.4969
2024-11-28 08:32:25.312019: Pseudo dice [0.7256]
2024-11-28 08:32:25.312812: Epoch time: 285.69 s
2024-11-28 08:32:26.762071: 
2024-11-28 08:32:26.763434: Epoch 931
2024-11-28 08:32:26.764220: Current learning rate: 0.0009
2024-11-28 08:36:37.892618: train_loss -0.8405
2024-11-28 08:36:37.894885: val_loss -0.4425
2024-11-28 08:36:37.895763: Pseudo dice [0.7236]
2024-11-28 08:36:37.896734: Epoch time: 251.13 s
2024-11-28 08:36:39.332521: 
2024-11-28 08:36:39.333731: Epoch 932
2024-11-28 08:36:39.334463: Current learning rate: 0.00089
2024-11-28 08:40:33.432422: train_loss -0.8301
2024-11-28 08:40:33.433478: val_loss -0.4529
2024-11-28 08:40:33.434217: Pseudo dice [0.7211]
2024-11-28 08:40:33.434950: Epoch time: 234.1 s
2024-11-28 08:40:34.867779: 
2024-11-28 08:40:34.869242: Epoch 933
2024-11-28 08:40:34.869987: Current learning rate: 0.00088
2024-11-28 08:44:51.524017: train_loss -0.8355
2024-11-28 08:44:51.525148: val_loss -0.4604
2024-11-28 08:44:51.525836: Pseudo dice [0.7101]
2024-11-28 08:44:51.526501: Epoch time: 256.66 s
2024-11-28 08:44:52.965902: 
2024-11-28 08:44:52.967223: Epoch 934
2024-11-28 08:44:52.967891: Current learning rate: 0.00087
2024-11-28 08:49:11.772665: train_loss -0.8342
2024-11-28 08:49:11.773651: val_loss -0.4624
2024-11-28 08:49:11.774465: Pseudo dice [0.7177]
2024-11-28 08:49:11.775448: Epoch time: 258.81 s
2024-11-28 08:49:13.208239: 
2024-11-28 08:49:13.209755: Epoch 935
2024-11-28 08:49:13.210788: Current learning rate: 0.00085
2024-11-28 08:53:35.873680: train_loss -0.831
2024-11-28 08:53:35.875252: val_loss -0.4509
2024-11-28 08:53:35.876366: Pseudo dice [0.7104]
2024-11-28 08:53:35.877340: Epoch time: 262.67 s
2024-11-28 08:53:37.330563: 
2024-11-28 08:53:37.332123: Epoch 936
2024-11-28 08:53:37.333292: Current learning rate: 0.00084
2024-11-28 08:57:14.081690: train_loss -0.825
2024-11-28 08:57:14.082747: val_loss -0.4905
2024-11-28 08:57:14.083715: Pseudo dice [0.7354]
2024-11-28 08:57:14.084589: Epoch time: 216.75 s
2024-11-28 08:57:15.552611: 
2024-11-28 08:57:15.554364: Epoch 937
2024-11-28 08:57:15.555617: Current learning rate: 0.00083
2024-11-28 09:01:09.424535: train_loss -0.8327
2024-11-28 09:01:09.425590: val_loss -0.429
2024-11-28 09:01:09.426420: Pseudo dice [0.6935]
2024-11-28 09:01:09.427148: Epoch time: 233.88 s
2024-11-28 09:01:10.880119: 
2024-11-28 09:01:10.881551: Epoch 938
2024-11-28 09:01:10.882402: Current learning rate: 0.00082
2024-11-28 09:05:18.240972: train_loss -0.8312
2024-11-28 09:05:18.242019: val_loss -0.452
2024-11-28 09:05:18.242821: Pseudo dice [0.7084]
2024-11-28 09:05:18.243654: Epoch time: 247.36 s
2024-11-28 09:05:19.687187: 
2024-11-28 09:05:19.688704: Epoch 939
2024-11-28 09:05:19.689700: Current learning rate: 0.00081
2024-11-28 09:09:26.306404: train_loss -0.8313
2024-11-28 09:09:26.307409: val_loss -0.4972
2024-11-28 09:09:26.308100: Pseudo dice [0.7283]
2024-11-28 09:09:26.308937: Epoch time: 246.62 s
2024-11-28 09:09:27.800700: 
2024-11-28 09:09:27.802105: Epoch 940
2024-11-28 09:09:27.802951: Current learning rate: 0.00079
2024-11-28 09:13:15.469356: train_loss -0.8348
2024-11-28 09:13:15.470281: val_loss -0.477
2024-11-28 09:13:15.471020: Pseudo dice [0.7208]
2024-11-28 09:13:15.471730: Epoch time: 227.67 s
2024-11-28 09:13:16.931827: 
2024-11-28 09:13:16.933163: Epoch 941
2024-11-28 09:13:16.933880: Current learning rate: 0.00078
2024-11-28 09:16:29.985358: train_loss -0.834
2024-11-28 09:16:29.986319: val_loss -0.4384
2024-11-28 09:16:29.987563: Pseudo dice [0.7062]
2024-11-28 09:16:29.988560: Epoch time: 193.05 s
2024-11-28 09:16:31.389740: 
2024-11-28 09:16:31.391389: Epoch 942
2024-11-28 09:16:31.392553: Current learning rate: 0.00077
2024-11-28 09:17:35.929408: train_loss -0.8248
2024-11-28 09:17:35.930339: val_loss -0.4227
2024-11-28 09:17:35.931106: Pseudo dice [0.7148]
2024-11-28 09:17:35.931858: Epoch time: 64.54 s
2024-11-28 09:17:37.229889: 
2024-11-28 09:17:37.231881: Epoch 943
2024-11-28 09:17:37.232647: Current learning rate: 0.00076
2024-11-28 09:18:42.719822: train_loss -0.832
2024-11-28 09:18:42.720878: val_loss -0.4134
2024-11-28 09:18:42.721579: Pseudo dice [0.6877]
2024-11-28 09:18:42.722266: Epoch time: 65.49 s
2024-11-28 09:18:44.026543: 
2024-11-28 09:18:44.028349: Epoch 944
2024-11-28 09:18:44.029402: Current learning rate: 0.00075
2024-11-28 09:19:49.555853: train_loss -0.8328
2024-11-28 09:19:49.556999: val_loss -0.437
2024-11-28 09:19:49.557868: Pseudo dice [0.7121]
2024-11-28 09:19:49.558682: Epoch time: 65.53 s
2024-11-28 09:19:50.819549: 
2024-11-28 09:19:50.821869: Epoch 945
2024-11-28 09:19:50.823068: Current learning rate: 0.00074
2024-11-28 09:20:56.206254: train_loss -0.8383
2024-11-28 09:20:56.207480: val_loss -0.4508
2024-11-28 09:20:56.208219: Pseudo dice [0.7018]
2024-11-28 09:20:56.209152: Epoch time: 65.39 s
2024-11-28 09:20:57.474652: 
2024-11-28 09:20:57.476292: Epoch 946
2024-11-28 09:20:57.477247: Current learning rate: 0.00072
2024-11-28 09:22:02.721385: train_loss -0.8337
2024-11-28 09:22:02.723996: val_loss -0.4809
2024-11-28 09:22:02.725449: Pseudo dice [0.7153]
2024-11-28 09:22:02.726335: Epoch time: 65.25 s
2024-11-28 09:22:04.031757: 
2024-11-28 09:22:04.034907: Epoch 947
2024-11-28 09:22:04.036310: Current learning rate: 0.00071
2024-11-28 09:23:09.323575: train_loss -0.8366
2024-11-28 09:23:09.324617: val_loss -0.4377
2024-11-28 09:23:09.325502: Pseudo dice [0.7144]
2024-11-28 09:23:09.326268: Epoch time: 65.29 s
2024-11-28 09:23:10.600152: 
2024-11-28 09:23:10.602482: Epoch 948
2024-11-28 09:23:10.603483: Current learning rate: 0.0007
2024-11-28 09:24:16.123866: train_loss -0.8301
2024-11-28 09:24:16.128151: val_loss -0.4578
2024-11-28 09:24:16.129166: Pseudo dice [0.7138]
2024-11-28 09:24:16.130104: Epoch time: 65.52 s
2024-11-28 09:24:17.849742: 
2024-11-28 09:24:17.852053: Epoch 949
2024-11-28 09:24:17.853213: Current learning rate: 0.00069
2024-11-28 09:25:23.393717: train_loss -0.8369
2024-11-28 09:25:23.395020: val_loss -0.4728
2024-11-28 09:25:23.395942: Pseudo dice [0.7358]
2024-11-28 09:25:23.396741: Epoch time: 65.54 s
2024-11-28 09:25:24.998232: 
2024-11-28 09:25:25.000804: Epoch 950
2024-11-28 09:25:25.001770: Current learning rate: 0.00067
2024-11-28 09:26:30.659030: train_loss -0.8353
2024-11-28 09:26:30.660505: val_loss -0.447
2024-11-28 09:26:30.661952: Pseudo dice [0.7071]
2024-11-28 09:26:30.662636: Epoch time: 65.66 s
2024-11-28 09:26:31.941653: 
2024-11-28 09:26:31.943423: Epoch 951
2024-11-28 09:26:31.944572: Current learning rate: 0.00066
2024-11-28 09:27:37.248930: train_loss -0.8317
2024-11-28 09:27:37.253025: val_loss -0.463
2024-11-28 09:27:37.253946: Pseudo dice [0.7164]
2024-11-28 09:27:37.255735: Epoch time: 65.31 s
2024-11-28 09:27:38.526387: 
2024-11-28 09:27:38.528769: Epoch 952
2024-11-28 09:27:38.530478: Current learning rate: 0.00065
2024-11-28 09:28:44.038166: train_loss -0.8338
2024-11-28 09:28:44.041567: val_loss -0.4599
2024-11-28 09:28:44.042336: Pseudo dice [0.7287]
2024-11-28 09:28:44.043030: Epoch time: 65.51 s
2024-11-28 09:28:45.311874: 
2024-11-28 09:28:45.313886: Epoch 953
2024-11-28 09:28:45.314680: Current learning rate: 0.00064
2024-11-28 09:29:50.832341: train_loss -0.833
2024-11-28 09:29:50.833638: val_loss -0.4759
2024-11-28 09:29:50.834726: Pseudo dice [0.7379]
2024-11-28 09:29:50.835423: Epoch time: 65.52 s
2024-11-28 09:29:52.099328: 
2024-11-28 09:29:52.101526: Epoch 954
2024-11-28 09:29:52.102555: Current learning rate: 0.00063
2024-11-28 09:30:57.618968: train_loss -0.8348
2024-11-28 09:30:57.620161: val_loss -0.4754
2024-11-28 09:30:57.621094: Pseudo dice [0.7183]
2024-11-28 09:30:57.622046: Epoch time: 65.52 s
2024-11-28 09:30:58.898425: 
2024-11-28 09:30:58.900408: Epoch 955
2024-11-28 09:30:58.901530: Current learning rate: 0.00061
2024-11-28 09:32:04.601735: train_loss -0.8349
2024-11-28 09:32:04.602991: val_loss -0.4708
2024-11-28 09:32:04.604004: Pseudo dice [0.7136]
2024-11-28 09:32:04.604843: Epoch time: 65.7 s
2024-11-28 09:32:05.869250: 
2024-11-28 09:32:05.871615: Epoch 956
2024-11-28 09:32:05.872885: Current learning rate: 0.0006
2024-11-28 09:33:11.441380: train_loss -0.836
2024-11-28 09:33:11.442754: val_loss -0.4561
2024-11-28 09:33:11.443824: Pseudo dice [0.7244]
2024-11-28 09:33:11.444503: Epoch time: 65.57 s
2024-11-28 09:33:12.734632: 
2024-11-28 09:33:12.736652: Epoch 957
2024-11-28 09:33:12.737567: Current learning rate: 0.00059
2024-11-28 09:34:18.321139: train_loss -0.8301
2024-11-28 09:34:18.322487: val_loss -0.4428
2024-11-28 09:34:18.323864: Pseudo dice [0.7089]
2024-11-28 09:34:18.325322: Epoch time: 65.59 s
2024-11-28 09:34:19.608833: 
2024-11-28 09:34:19.611344: Epoch 958
2024-11-28 09:34:19.612231: Current learning rate: 0.00058
2024-11-28 09:35:25.091686: train_loss -0.838
2024-11-28 09:35:25.093164: val_loss -0.4602
2024-11-28 09:35:25.094590: Pseudo dice [0.715]
2024-11-28 09:35:25.095297: Epoch time: 65.48 s
2024-11-28 09:35:26.363627: 
2024-11-28 09:35:26.365625: Epoch 959
2024-11-28 09:35:26.366677: Current learning rate: 0.00056
2024-11-28 09:36:32.096132: train_loss -0.8343
2024-11-28 09:36:32.097396: val_loss -0.4628
2024-11-28 09:36:32.098509: Pseudo dice [0.7144]
2024-11-28 09:36:32.099552: Epoch time: 65.73 s
2024-11-28 09:36:33.773388: 
2024-11-28 09:36:33.775475: Epoch 960
2024-11-28 09:36:33.776871: Current learning rate: 0.00055
2024-11-28 09:37:39.336678: train_loss -0.8272
2024-11-28 09:37:39.337763: val_loss -0.4392
2024-11-28 09:37:39.339242: Pseudo dice [0.7087]
2024-11-28 09:37:39.340581: Epoch time: 65.56 s
2024-11-28 09:37:40.620832: 
2024-11-28 09:37:40.622947: Epoch 961
2024-11-28 09:37:40.624539: Current learning rate: 0.00054
2024-11-28 09:38:46.277967: train_loss -0.8321
2024-11-28 09:38:46.279185: val_loss -0.4677
2024-11-28 09:38:46.280184: Pseudo dice [0.7184]
2024-11-28 09:38:46.281115: Epoch time: 65.66 s
2024-11-28 09:38:47.565983: 
2024-11-28 09:38:47.568369: Epoch 962
2024-11-28 09:38:47.569155: Current learning rate: 0.00053
2024-11-28 09:39:53.186125: train_loss -0.8337
2024-11-28 09:39:53.187072: val_loss -0.4474
2024-11-28 09:39:53.188303: Pseudo dice [0.7155]
2024-11-28 09:39:53.189013: Epoch time: 65.62 s
2024-11-28 09:39:54.447031: 
2024-11-28 09:39:54.449007: Epoch 963
2024-11-28 09:39:54.449756: Current learning rate: 0.00051
2024-11-28 09:40:59.939898: train_loss -0.8324
2024-11-28 09:40:59.941864: val_loss -0.4678
2024-11-28 09:40:59.943330: Pseudo dice [0.7249]
2024-11-28 09:40:59.944642: Epoch time: 65.49 s
2024-11-28 09:41:01.233912: 
2024-11-28 09:41:01.236191: Epoch 964
2024-11-28 09:41:01.237567: Current learning rate: 0.0005
2024-11-28 09:42:06.788601: train_loss -0.8404
2024-11-28 09:42:06.789858: val_loss -0.4708
2024-11-28 09:42:06.790906: Pseudo dice [0.7214]
2024-11-28 09:42:06.791581: Epoch time: 65.56 s
2024-11-28 09:42:08.085680: 
2024-11-28 09:42:08.088052: Epoch 965
2024-11-28 09:42:08.089125: Current learning rate: 0.00049
2024-11-28 09:43:13.608004: train_loss -0.8369
2024-11-28 09:43:13.609254: val_loss -0.4888
2024-11-28 09:43:13.610386: Pseudo dice [0.7257]
2024-11-28 09:43:13.611383: Epoch time: 65.52 s
2024-11-28 09:43:14.887454: 
2024-11-28 09:43:14.889228: Epoch 966
2024-11-28 09:43:14.890231: Current learning rate: 0.00048
2024-11-28 09:44:20.511492: train_loss -0.8348
2024-11-28 09:44:20.512789: val_loss -0.4551
2024-11-28 09:44:20.513709: Pseudo dice [0.7118]
2024-11-28 09:44:20.514788: Epoch time: 65.62 s
2024-11-28 09:44:21.792826: 
2024-11-28 09:44:21.795259: Epoch 967
2024-11-28 09:44:21.796344: Current learning rate: 0.00046
2024-11-28 09:45:27.429120: train_loss -0.8411
2024-11-28 09:45:27.430368: val_loss -0.4531
2024-11-28 09:45:27.431336: Pseudo dice [0.7172]
2024-11-28 09:45:27.432125: Epoch time: 65.64 s
2024-11-28 09:45:28.717252: 
2024-11-28 09:45:28.719261: Epoch 968
2024-11-28 09:45:28.720408: Current learning rate: 0.00045
2024-11-28 09:46:34.204362: train_loss -0.84
2024-11-28 09:46:34.205590: val_loss -0.4548
2024-11-28 09:46:34.206408: Pseudo dice [0.732]
2024-11-28 09:46:34.207782: Epoch time: 65.49 s
2024-11-28 09:46:35.503800: 
2024-11-28 09:46:35.505545: Epoch 969
2024-11-28 09:46:35.506414: Current learning rate: 0.00044
2024-11-28 09:47:41.077666: train_loss -0.8387
2024-11-28 09:47:41.079089: val_loss -0.4308
2024-11-28 09:47:41.080122: Pseudo dice [0.7136]
2024-11-28 09:47:41.081290: Epoch time: 65.57 s
2024-11-28 09:47:42.381462: 
2024-11-28 09:47:42.383540: Epoch 970
2024-11-28 09:47:42.384799: Current learning rate: 0.00043
2024-11-28 09:48:48.124671: train_loss -0.8386
2024-11-28 09:48:48.125708: val_loss -0.4662
2024-11-28 09:48:48.126625: Pseudo dice [0.6988]
2024-11-28 09:48:48.127609: Epoch time: 65.74 s
2024-11-28 09:48:49.764902: 
2024-11-28 09:48:49.766945: Epoch 971
2024-11-28 09:48:49.767715: Current learning rate: 0.00041
2024-11-28 09:49:55.511779: train_loss -0.835
2024-11-28 09:49:55.513036: val_loss -0.4605
2024-11-28 09:49:55.514007: Pseudo dice [0.719]
2024-11-28 09:49:55.514834: Epoch time: 65.75 s
2024-11-28 09:49:56.804843: 
2024-11-28 09:49:56.806629: Epoch 972
2024-11-28 09:49:56.807697: Current learning rate: 0.0004
2024-11-28 09:51:02.338685: train_loss -0.8455
2024-11-28 09:51:02.339714: val_loss -0.4549
2024-11-28 09:51:02.340689: Pseudo dice [0.7127]
2024-11-28 09:51:02.341719: Epoch time: 65.53 s
2024-11-28 09:51:03.616305: 
2024-11-28 09:51:03.618232: Epoch 973
2024-11-28 09:51:03.619500: Current learning rate: 0.00039
2024-11-28 09:52:09.271812: train_loss -0.8327
2024-11-28 09:52:09.272918: val_loss -0.4587
2024-11-28 09:52:09.273941: Pseudo dice [0.7215]
2024-11-28 09:52:09.274628: Epoch time: 65.66 s
2024-11-28 09:52:10.552801: 
2024-11-28 09:52:10.554618: Epoch 974
2024-11-28 09:52:10.555665: Current learning rate: 0.00037
2024-11-28 09:53:16.126160: train_loss -0.8356
2024-11-28 09:53:16.127418: val_loss -0.5113
2024-11-28 09:53:16.128170: Pseudo dice [0.7264]
2024-11-28 09:53:16.128936: Epoch time: 65.57 s
2024-11-28 09:53:17.424317: 
2024-11-28 09:53:17.426104: Epoch 975
2024-11-28 09:53:17.427062: Current learning rate: 0.00036
2024-11-28 09:54:23.429555: train_loss -0.8378
2024-11-28 09:54:23.430864: val_loss -0.5017
2024-11-28 09:54:23.432034: Pseudo dice [0.729]
2024-11-28 09:54:23.432890: Epoch time: 66.01 s
2024-11-28 09:54:24.703548: 
2024-11-28 09:54:24.706059: Epoch 976
2024-11-28 09:54:24.707475: Current learning rate: 0.00035
2024-11-28 09:55:30.313971: train_loss -0.8366
2024-11-28 09:55:30.315376: val_loss -0.4725
2024-11-28 09:55:30.316548: Pseudo dice [0.7131]
2024-11-28 09:55:30.317441: Epoch time: 65.61 s
2024-11-28 09:55:31.576665: 
2024-11-28 09:55:31.578866: Epoch 977
2024-11-28 09:55:31.580047: Current learning rate: 0.00034
2024-11-28 09:56:37.144284: train_loss -0.8345
2024-11-28 09:56:37.145367: val_loss -0.4707
2024-11-28 09:56:37.146357: Pseudo dice [0.7198]
2024-11-28 09:56:37.147168: Epoch time: 65.57 s
2024-11-28 09:56:38.421650: 
2024-11-28 09:56:38.423878: Epoch 978
2024-11-28 09:56:38.425184: Current learning rate: 0.00032
2024-11-28 09:57:44.181087: train_loss -0.8394
2024-11-28 09:57:44.182299: val_loss -0.4937
2024-11-28 09:57:44.183170: Pseudo dice [0.7474]
2024-11-28 09:57:44.183964: Epoch time: 65.76 s
2024-11-28 09:57:44.184699: Yayy! New best EMA pseudo Dice: 0.7214
2024-11-28 09:57:46.118082: 
2024-11-28 09:57:46.120288: Epoch 979
2024-11-28 09:57:46.120983: Current learning rate: 0.00031
2024-11-28 09:58:51.707691: train_loss -0.8345
2024-11-28 09:58:51.708977: val_loss -0.4609
2024-11-28 09:58:51.710661: Pseudo dice [0.7207]
2024-11-28 09:58:51.711582: Epoch time: 65.59 s
2024-11-28 09:58:53.000142: 
2024-11-28 09:58:53.002770: Epoch 980
2024-11-28 09:58:53.004065: Current learning rate: 0.0003
2024-11-28 09:59:58.453377: train_loss -0.8414
2024-11-28 09:59:58.454613: val_loss -0.4791
2024-11-28 09:59:58.455660: Pseudo dice [0.7267]
2024-11-28 09:59:58.456550: Epoch time: 65.45 s
2024-11-28 09:59:58.457370: Yayy! New best EMA pseudo Dice: 0.7219
2024-11-28 10:00:00.112079: 
2024-11-28 10:00:00.114663: Epoch 981
2024-11-28 10:00:00.115603: Current learning rate: 0.00028
2024-11-28 10:01:05.635116: train_loss -0.8363
2024-11-28 10:01:05.636426: val_loss -0.4767
2024-11-28 10:01:05.637216: Pseudo dice [0.7229]
2024-11-28 10:01:05.637877: Epoch time: 65.52 s
2024-11-28 10:01:05.638599: Yayy! New best EMA pseudo Dice: 0.722
2024-11-28 10:01:07.287442: 
2024-11-28 10:01:07.289531: Epoch 982
2024-11-28 10:01:07.290672: Current learning rate: 0.00027
2024-11-28 10:02:12.888086: train_loss -0.835
2024-11-28 10:02:12.889404: val_loss -0.4707
2024-11-28 10:02:12.890644: Pseudo dice [0.7246]
2024-11-28 10:02:12.891375: Epoch time: 65.6 s
2024-11-28 10:02:12.892068: Yayy! New best EMA pseudo Dice: 0.7222
2024-11-28 10:02:14.866746: 
2024-11-28 10:02:14.869160: Epoch 983
2024-11-28 10:02:14.870602: Current learning rate: 0.00026
2024-11-28 10:03:20.453246: train_loss -0.8382
2024-11-28 10:03:20.454360: val_loss -0.4155
2024-11-28 10:03:20.455282: Pseudo dice [0.7041]
2024-11-28 10:03:20.456241: Epoch time: 65.59 s
2024-11-28 10:03:21.718679: 
2024-11-28 10:03:21.721553: Epoch 984
2024-11-28 10:03:21.722859: Current learning rate: 0.00024
2024-11-28 10:04:27.289902: train_loss -0.8358
2024-11-28 10:04:27.291345: val_loss -0.4509
2024-11-28 10:04:27.292839: Pseudo dice [0.6978]
2024-11-28 10:04:27.294219: Epoch time: 65.57 s
2024-11-28 10:04:28.581464: 
2024-11-28 10:04:28.583261: Epoch 985
2024-11-28 10:04:28.584432: Current learning rate: 0.00023
2024-11-28 10:05:34.192571: train_loss -0.8417
2024-11-28 10:05:34.193750: val_loss -0.4507
2024-11-28 10:05:34.194780: Pseudo dice [0.7209]
2024-11-28 10:05:34.195747: Epoch time: 65.61 s
2024-11-28 10:05:35.472493: 
2024-11-28 10:05:35.474805: Epoch 986
2024-11-28 10:05:35.476156: Current learning rate: 0.00021
2024-11-28 10:06:41.088142: train_loss -0.8317
2024-11-28 10:06:41.089474: val_loss -0.4684
2024-11-28 10:06:41.090520: Pseudo dice [0.7205]
2024-11-28 10:06:41.091225: Epoch time: 65.62 s
2024-11-28 10:06:42.357730: 
2024-11-28 10:06:42.359761: Epoch 987
2024-11-28 10:06:42.360550: Current learning rate: 0.0002
2024-11-28 10:07:47.869239: train_loss -0.8379
2024-11-28 10:07:47.870843: val_loss -0.4574
2024-11-28 10:07:47.872302: Pseudo dice [0.723]
2024-11-28 10:07:47.873149: Epoch time: 65.51 s
2024-11-28 10:07:49.138909: 
2024-11-28 10:07:49.141338: Epoch 988
2024-11-28 10:07:49.142457: Current learning rate: 0.00019
2024-11-28 10:08:54.357764: train_loss -0.839
2024-11-28 10:08:54.359076: val_loss -0.4486
2024-11-28 10:08:54.359953: Pseudo dice [0.7145]
2024-11-28 10:08:54.360600: Epoch time: 65.22 s
2024-11-28 10:08:55.634875: 
2024-11-28 10:08:55.636738: Epoch 989
2024-11-28 10:08:55.637811: Current learning rate: 0.00017
2024-11-28 10:10:01.039392: train_loss -0.8383
2024-11-28 10:10:01.040450: val_loss -0.435
2024-11-28 10:10:01.041234: Pseudo dice [0.7065]
2024-11-28 10:10:01.041977: Epoch time: 65.41 s
2024-11-28 10:10:02.305290: 
2024-11-28 10:10:02.307136: Epoch 990
2024-11-28 10:10:02.308018: Current learning rate: 0.00016
2024-11-28 10:11:07.574313: train_loss -0.8399
2024-11-28 10:11:07.575413: val_loss -0.4465
2024-11-28 10:11:07.576458: Pseudo dice [0.7117]
2024-11-28 10:11:07.577343: Epoch time: 65.27 s
2024-11-28 10:11:08.847814: 
2024-11-28 10:11:08.849725: Epoch 991
2024-11-28 10:11:08.850835: Current learning rate: 0.00014
2024-11-28 10:12:14.118178: train_loss -0.8364
2024-11-28 10:12:14.119348: val_loss -0.446
2024-11-28 10:12:14.120308: Pseudo dice [0.7224]
2024-11-28 10:12:14.121005: Epoch time: 65.27 s
2024-11-28 10:12:15.401084: 
2024-11-28 10:12:15.402814: Epoch 992
2024-11-28 10:12:15.404104: Current learning rate: 0.00013
2024-11-28 10:13:20.887996: train_loss -0.8381
2024-11-28 10:13:20.889667: val_loss -0.4526
2024-11-28 10:13:20.890978: Pseudo dice [0.7174]
2024-11-28 10:13:20.891850: Epoch time: 65.49 s
2024-11-28 10:13:22.164272: 
2024-11-28 10:13:22.166808: Epoch 993
2024-11-28 10:13:22.167862: Current learning rate: 0.00011
2024-11-28 10:14:27.333787: train_loss -0.8323
2024-11-28 10:14:27.335100: val_loss -0.4454
2024-11-28 10:14:27.335966: Pseudo dice [0.7253]
2024-11-28 10:14:27.336983: Epoch time: 65.17 s
2024-11-28 10:14:28.950463: 
2024-11-28 10:14:28.952714: Epoch 994
2024-11-28 10:14:28.953912: Current learning rate: 0.0001
2024-11-28 10:15:34.429934: train_loss -0.8376
2024-11-28 10:15:34.431060: val_loss -0.4693
2024-11-28 10:15:34.432094: Pseudo dice [0.7264]
2024-11-28 10:15:34.433023: Epoch time: 65.48 s
2024-11-28 10:15:35.685564: 
2024-11-28 10:15:35.687847: Epoch 995
2024-11-28 10:15:35.688723: Current learning rate: 8e-05
2024-11-28 10:16:40.963265: train_loss -0.8402
2024-11-28 10:16:40.964369: val_loss -0.4747
2024-11-28 10:16:40.965336: Pseudo dice [0.7193]
2024-11-28 10:16:40.966014: Epoch time: 65.28 s
2024-11-28 10:16:42.234553: 
2024-11-28 10:16:42.236566: Epoch 996
2024-11-28 10:16:42.237501: Current learning rate: 7e-05
2024-11-28 10:17:47.761858: train_loss -0.8421
2024-11-28 10:17:47.763049: val_loss -0.4588
2024-11-28 10:17:47.763825: Pseudo dice [0.7285]
2024-11-28 10:17:47.764526: Epoch time: 65.53 s
2024-11-28 10:17:49.003764: 
2024-11-28 10:17:49.006147: Epoch 997
2024-11-28 10:17:49.007220: Current learning rate: 5e-05
2024-11-28 10:18:54.417157: train_loss -0.8381
2024-11-28 10:18:54.418448: val_loss -0.4871
2024-11-28 10:18:54.419356: Pseudo dice [0.741]
2024-11-28 10:18:54.420215: Epoch time: 65.41 s
2024-11-28 10:18:55.674844: 
2024-11-28 10:18:55.677748: Epoch 998
2024-11-28 10:18:55.679166: Current learning rate: 4e-05
2024-11-28 10:20:01.416383: train_loss -0.8337
2024-11-28 10:20:01.417785: val_loss -0.4804
2024-11-28 10:20:01.418745: Pseudo dice [0.736]
2024-11-28 10:20:01.419522: Epoch time: 65.74 s
2024-11-28 10:20:01.420360: Yayy! New best EMA pseudo Dice: 0.7235
2024-11-28 10:20:03.056674: 
2024-11-28 10:20:03.058944: Epoch 999
2024-11-28 10:20:03.060399: Current learning rate: 2e-05
2024-11-28 10:21:08.762784: train_loss -0.8423
2024-11-28 10:21:08.764029: val_loss -0.442
2024-11-28 10:21:08.764972: Pseudo dice [0.7079]
2024-11-28 10:21:08.766002: Epoch time: 65.71 s
2024-11-28 10:21:10.448671: Training done.
2024-11-28 10:21:10.625184: Using splits from existing split file: /home/gridsan/nchutisilp/datasets/nnUNet_Datasets/nnUNet_preprocessed/Dataset306_Sohee_Ajay_Calcium_OCT/splits_final.json
2024-11-28 10:21:10.627695: The split file contains 5 splits.
2024-11-28 10:21:10.628530: Desired fold for training: 1
2024-11-28 10:21:10.629413: This split has 10 training and 3 validation cases.
2024-11-28 10:21:10.630302: predicting 101-019
2024-11-28 10:21:10.698754: 101-019, shape torch.Size([1, 375, 498, 498]), rank 0
2024-11-28 10:22:43.235448: predicting 101009Pre
2024-11-28 10:22:43.278319: 101009Pre, shape torch.Size([1, 230, 498, 498]), rank 0
2024-11-28 10:23:38.077517: predicting 704-003
2024-11-28 10:23:38.107792: 704-003, shape torch.Size([1, 375, 498, 498]), rank 0
2024-11-28 10:25:23.600487: Validation complete
2024-11-28 10:25:23.602037: Mean Validation Dice:  0.6925077931150034
