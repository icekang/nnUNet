/home/gridsan/nchutisilp/.conda/envs/nnunet/bin/python
wandb: Tracking run with wandb version 0.16.6
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.

############################
INFO: You are using the old nnU-Net default plans. We have updated our recommendations. Please consider using those instead! Read more here: https://github.com/MIC-DKFZ/nnUNet/blob/master/documentation/resenc_presets.md
############################

Using device: cuda:0

#######################################################################
Please cite the following paper when using nnU-Net:
Isensee, F., Jaeger, P. F., Kohl, S. A., Petersen, J., & Maier-Hein, K. H. (2021). nnU-Net: a self-configuring method for deep learning-based biomedical image segmentation. Nature methods, 18(2), 203-211.
#######################################################################

2024-05-06 18:45:33.130478: Using torch.compile...
/home/gridsan/nchutisilp/.local/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn("The verbose parameter is deprecated. Please use get_last_lr() "
################### Loading pretrained weights from file  /home/gridsan/nchutisilp/datasets/nnUNet_Datasets/nnUNet_results/Dataset300_Lumen_and_Wall_OCT/nnUNetTrainer__nnUNetPreprocessPlans__2d/fold_all/checkpoint_best.pth ###################
Below is the list of overlapping blocks in pretrained model and nnUNet architecture:
encoder.stages.0.0.convs.0.conv.weight shape torch.Size([32, 1, 3, 3])
encoder.stages.0.0.convs.0.conv.bias shape torch.Size([32])
encoder.stages.0.0.convs.0.norm.weight shape torch.Size([32])
encoder.stages.0.0.convs.0.norm.bias shape torch.Size([32])
encoder.stages.0.0.convs.0.all_modules.0.weight shape torch.Size([32, 1, 3, 3])
encoder.stages.0.0.convs.0.all_modules.0.bias shape torch.Size([32])
encoder.stages.0.0.convs.0.all_modules.1.weight shape torch.Size([32])
encoder.stages.0.0.convs.0.all_modules.1.bias shape torch.Size([32])
encoder.stages.0.0.convs.1.conv.weight shape torch.Size([32, 32, 3, 3])
encoder.stages.0.0.convs.1.conv.bias shape torch.Size([32])
encoder.stages.0.0.convs.1.norm.weight shape torch.Size([32])
encoder.stages.0.0.convs.1.norm.bias shape torch.Size([32])
encoder.stages.0.0.convs.1.all_modules.0.weight shape torch.Size([32, 32, 3, 3])
encoder.stages.0.0.convs.1.all_modules.0.bias shape torch.Size([32])
encoder.stages.0.0.convs.1.all_modules.1.weight shape torch.Size([32])
encoder.stages.0.0.convs.1.all_modules.1.bias shape torch.Size([32])
encoder.stages.1.0.convs.0.conv.weight shape torch.Size([64, 32, 3, 3])
encoder.stages.1.0.convs.0.conv.bias shape torch.Size([64])
encoder.stages.1.0.convs.0.norm.weight shape torch.Size([64])
encoder.stages.1.0.convs.0.norm.bias shape torch.Size([64])
encoder.stages.1.0.convs.0.all_modules.0.weight shape torch.Size([64, 32, 3, 3])
encoder.stages.1.0.convs.0.all_modules.0.bias shape torch.Size([64])
encoder.stages.1.0.convs.0.all_modules.1.weight shape torch.Size([64])
encoder.stages.1.0.convs.0.all_modules.1.bias shape torch.Size([64])
encoder.stages.1.0.convs.1.conv.weight shape torch.Size([64, 64, 3, 3])
encoder.stages.1.0.convs.1.conv.bias shape torch.Size([64])
encoder.stages.1.0.convs.1.norm.weight shape torch.Size([64])
encoder.stages.1.0.convs.1.norm.bias shape torch.Size([64])
encoder.stages.1.0.convs.1.all_modules.0.weight shape torch.Size([64, 64, 3, 3])
encoder.stages.1.0.convs.1.all_modules.0.bias shape torch.Size([64])
encoder.stages.1.0.convs.1.all_modules.1.weight shape torch.Size([64])
encoder.stages.1.0.convs.1.all_modules.1.bias shape torch.Size([64])
encoder.stages.2.0.convs.0.conv.weight shape torch.Size([128, 64, 3, 3])
encoder.stages.2.0.convs.0.conv.bias shape torch.Size([128])
encoder.stages.2.0.convs.0.norm.weight shape torch.Size([128])
encoder.stages.2.0.convs.0.norm.bias shape torch.Size([128])
encoder.stages.2.0.convs.0.all_modules.0.weight shape torch.Size([128, 64, 3, 3])
encoder.stages.2.0.convs.0.all_modules.0.bias shape torch.Size([128])
encoder.stages.2.0.convs.0.all_modules.1.weight shape torch.Size([128])
encoder.stages.2.0.convs.0.all_modules.1.bias shape torch.Size([128])
encoder.stages.2.0.convs.1.conv.weight shape torch.Size([128, 128, 3, 3])
encoder.stages.2.0.convs.1.conv.bias shape torch.Size([128])
encoder.stages.2.0.convs.1.norm.weight shape torch.Size([128])
encoder.stages.2.0.convs.1.norm.bias shape torch.Size([128])
encoder.stages.2.0.convs.1.all_modules.0.weight shape torch.Size([128, 128, 3, 3])
encoder.stages.2.0.convs.1.all_modules.0.bias shape torch.Size([128])
encoder.stages.2.0.convs.1.all_modules.1.weight shape torch.Size([128])
encoder.stages.2.0.convs.1.all_modules.1.bias shape torch.Size([128])
encoder.stages.3.0.convs.0.conv.weight shape torch.Size([256, 128, 3, 3])
encoder.stages.3.0.convs.0.conv.bias shape torch.Size([256])
encoder.stages.3.0.convs.0.norm.weight shape torch.Size([256])
encoder.stages.3.0.convs.0.norm.bias shape torch.Size([256])
encoder.stages.3.0.convs.0.all_modules.0.weight shape torch.Size([256, 128, 3, 3])
encoder.stages.3.0.convs.0.all_modules.0.bias shape torch.Size([256])
encoder.stages.3.0.convs.0.all_modules.1.weight shape torch.Size([256])
encoder.stages.3.0.convs.0.all_modules.1.bias shape torch.Size([256])
encoder.stages.3.0.convs.1.conv.weight shape torch.Size([256, 256, 3, 3])
encoder.stages.3.0.convs.1.conv.bias shape torch.Size([256])
encoder.stages.3.0.convs.1.norm.weight shape torch.Size([256])
encoder.stages.3.0.convs.1.norm.bias shape torch.Size([256])
encoder.stages.3.0.convs.1.all_modules.0.weight shape torch.Size([256, 256, 3, 3])
encoder.stages.3.0.convs.1.all_modules.0.bias shape torch.Size([256])
encoder.stages.3.0.convs.1.all_modules.1.weight shape torch.Size([256])
encoder.stages.3.0.convs.1.all_modules.1.bias shape torch.Size([256])
encoder.stages.4.0.convs.0.conv.weight shape torch.Size([512, 256, 3, 3])
encoder.stages.4.0.convs.0.conv.bias shape torch.Size([512])
encoder.stages.4.0.convs.0.norm.weight shape torch.Size([512])
encoder.stages.4.0.convs.0.norm.bias shape torch.Size([512])
encoder.stages.4.0.convs.0.all_modules.0.weight shape torch.Size([512, 256, 3, 3])
encoder.stages.4.0.convs.0.all_modules.0.bias shape torch.Size([512])
encoder.stages.4.0.convs.0.all_modules.1.weight shape torch.Size([512])
encoder.stages.4.0.convs.0.all_modules.1.bias shape torch.Size([512])
encoder.stages.4.0.convs.1.conv.weight shape torch.Size([512, 512, 3, 3])
encoder.stages.4.0.convs.1.conv.bias shape torch.Size([512])
encoder.stages.4.0.convs.1.norm.weight shape torch.Size([512])
encoder.stages.4.0.convs.1.norm.bias shape torch.Size([512])
encoder.stages.4.0.convs.1.all_modules.0.weight shape torch.Size([512, 512, 3, 3])
encoder.stages.4.0.convs.1.all_modules.0.bias shape torch.Size([512])
encoder.stages.4.0.convs.1.all_modules.1.weight shape torch.Size([512])
encoder.stages.4.0.convs.1.all_modules.1.bias shape torch.Size([512])
encoder.stages.5.0.convs.0.conv.weight shape torch.Size([512, 512, 3, 3])
encoder.stages.5.0.convs.0.conv.bias shape torch.Size([512])
encoder.stages.5.0.convs.0.norm.weight shape torch.Size([512])
encoder.stages.5.0.convs.0.norm.bias shape torch.Size([512])
encoder.stages.5.0.convs.0.all_modules.0.weight shape torch.Size([512, 512, 3, 3])
encoder.stages.5.0.convs.0.all_modules.0.bias shape torch.Size([512])
encoder.stages.5.0.convs.0.all_modules.1.weight shape torch.Size([512])
encoder.stages.5.0.convs.0.all_modules.1.bias shape torch.Size([512])
encoder.stages.5.0.convs.1.conv.weight shape torch.Size([512, 512, 3, 3])
encoder.stages.5.0.convs.1.conv.bias shape torch.Size([512])
encoder.stages.5.0.convs.1.norm.weight shape torch.Size([512])
encoder.stages.5.0.convs.1.norm.bias shape torch.Size([512])
encoder.stages.5.0.convs.1.all_modules.0.weight shape torch.Size([512, 512, 3, 3])
encoder.stages.5.0.convs.1.all_modules.0.bias shape torch.Size([512])
encoder.stages.5.0.convs.1.all_modules.1.weight shape torch.Size([512])
encoder.stages.5.0.convs.1.all_modules.1.bias shape torch.Size([512])
encoder.stages.6.0.convs.0.conv.weight shape torch.Size([512, 512, 3, 3])
encoder.stages.6.0.convs.0.conv.bias shape torch.Size([512])
encoder.stages.6.0.convs.0.norm.weight shape torch.Size([512])
encoder.stages.6.0.convs.0.norm.bias shape torch.Size([512])
encoder.stages.6.0.convs.0.all_modules.0.weight shape torch.Size([512, 512, 3, 3])
encoder.stages.6.0.convs.0.all_modules.0.bias shape torch.Size([512])
encoder.stages.6.0.convs.0.all_modules.1.weight shape torch.Size([512])
encoder.stages.6.0.convs.0.all_modules.1.bias shape torch.Size([512])
encoder.stages.6.0.convs.1.conv.weight shape torch.Size([512, 512, 3, 3])
encoder.stages.6.0.convs.1.conv.bias shape torch.Size([512])
encoder.stages.6.0.convs.1.norm.weight shape torch.Size([512])
encoder.stages.6.0.convs.1.norm.bias shape torch.Size([512])
encoder.stages.6.0.convs.1.all_modules.0.weight shape torch.Size([512, 512, 3, 3])
encoder.stages.6.0.convs.1.all_modules.0.bias shape torch.Size([512])
encoder.stages.6.0.convs.1.all_modules.1.weight shape torch.Size([512])
encoder.stages.6.0.convs.1.all_modules.1.bias shape torch.Size([512])
encoder.stages.7.0.convs.0.conv.weight shape torch.Size([512, 512, 3, 3])
encoder.stages.7.0.convs.0.conv.bias shape torch.Size([512])
encoder.stages.7.0.convs.0.norm.weight shape torch.Size([512])
encoder.stages.7.0.convs.0.norm.bias shape torch.Size([512])
encoder.stages.7.0.convs.0.all_modules.0.weight shape torch.Size([512, 512, 3, 3])
encoder.stages.7.0.convs.0.all_modules.0.bias shape torch.Size([512])
encoder.stages.7.0.convs.0.all_modules.1.weight shape torch.Size([512])
encoder.stages.7.0.convs.0.all_modules.1.bias shape torch.Size([512])
encoder.stages.7.0.convs.1.conv.weight shape torch.Size([512, 512, 3, 3])
encoder.stages.7.0.convs.1.conv.bias shape torch.Size([512])
encoder.stages.7.0.convs.1.norm.weight shape torch.Size([512])
encoder.stages.7.0.convs.1.norm.bias shape torch.Size([512])
encoder.stages.7.0.convs.1.all_modules.0.weight shape torch.Size([512, 512, 3, 3])
encoder.stages.7.0.convs.1.all_modules.0.bias shape torch.Size([512])
encoder.stages.7.0.convs.1.all_modules.1.weight shape torch.Size([512])
encoder.stages.7.0.convs.1.all_modules.1.bias shape torch.Size([512])
decoder.encoder.stages.0.0.convs.0.conv.weight shape torch.Size([32, 1, 3, 3])
decoder.encoder.stages.0.0.convs.0.conv.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.norm.weight shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.norm.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.all_modules.0.weight shape torch.Size([32, 1, 3, 3])
decoder.encoder.stages.0.0.convs.0.all_modules.0.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.all_modules.1.weight shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.all_modules.1.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.conv.weight shape torch.Size([32, 32, 3, 3])
decoder.encoder.stages.0.0.convs.1.conv.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.norm.weight shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.norm.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.all_modules.0.weight shape torch.Size([32, 32, 3, 3])
decoder.encoder.stages.0.0.convs.1.all_modules.0.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.all_modules.1.weight shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.all_modules.1.bias shape torch.Size([32])
decoder.encoder.stages.1.0.convs.0.conv.weight shape torch.Size([64, 32, 3, 3])
decoder.encoder.stages.1.0.convs.0.conv.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.norm.weight shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.norm.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.all_modules.0.weight shape torch.Size([64, 32, 3, 3])
decoder.encoder.stages.1.0.convs.0.all_modules.0.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.all_modules.1.weight shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.all_modules.1.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.conv.weight shape torch.Size([64, 64, 3, 3])
decoder.encoder.stages.1.0.convs.1.conv.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.norm.weight shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.norm.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.all_modules.0.weight shape torch.Size([64, 64, 3, 3])
decoder.encoder.stages.1.0.convs.1.all_modules.0.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.all_modules.1.weight shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.all_modules.1.bias shape torch.Size([64])
decoder.encoder.stages.2.0.convs.0.conv.weight shape torch.Size([128, 64, 3, 3])
decoder.encoder.stages.2.0.convs.0.conv.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.norm.weight shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.norm.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.all_modules.0.weight shape torch.Size([128, 64, 3, 3])
decoder.encoder.stages.2.0.convs.0.all_modules.0.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.all_modules.1.weight shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.all_modules.1.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.conv.weight shape torch.Size([128, 128, 3, 3])
decoder.encoder.stages.2.0.convs.1.conv.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.norm.weight shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.norm.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.all_modules.0.weight shape torch.Size([128, 128, 3, 3])
decoder.encoder.stages.2.0.convs.1.all_modules.0.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.all_modules.1.weight shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.all_modules.1.bias shape torch.Size([128])
decoder.encoder.stages.3.0.convs.0.conv.weight shape torch.Size([256, 128, 3, 3])
decoder.encoder.stages.3.0.convs.0.conv.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.norm.weight shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.norm.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.all_modules.0.weight shape torch.Size([256, 128, 3, 3])
decoder.encoder.stages.3.0.convs.0.all_modules.0.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.all_modules.1.weight shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.all_modules.1.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.conv.weight shape torch.Size([256, 256, 3, 3])
decoder.encoder.stages.3.0.convs.1.conv.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.norm.weight shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.norm.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.all_modules.0.weight shape torch.Size([256, 256, 3, 3])
decoder.encoder.stages.3.0.convs.1.all_modules.0.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.all_modules.1.weight shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.all_modules.1.bias shape torch.Size([256])
decoder.encoder.stages.4.0.convs.0.conv.weight shape torch.Size([512, 256, 3, 3])
decoder.encoder.stages.4.0.convs.0.conv.bias shape torch.Size([512])
decoder.encoder.stages.4.0.convs.0.norm.weight shape torch.Size([512])
decoder.encoder.stages.4.0.convs.0.norm.bias shape torch.Size([512])
decoder.encoder.stages.4.0.convs.0.all_modules.0.weight shape torch.Size([512, 256, 3, 3])
decoder.encoder.stages.4.0.convs.0.all_modules.0.bias shape torch.Size([512])
decoder.encoder.stages.4.0.convs.0.all_modules.1.weight shape torch.Size([512])
decoder.encoder.stages.4.0.convs.0.all_modules.1.bias shape torch.Size([512])
decoder.encoder.stages.4.0.convs.1.conv.weight shape torch.Size([512, 512, 3, 3])
decoder.encoder.stages.4.0.convs.1.conv.bias shape torch.Size([512])
decoder.encoder.stages.4.0.convs.1.norm.weight shape torch.Size([512])
decoder.encoder.stages.4.0.convs.1.norm.bias shape torch.Size([512])
decoder.encoder.stages.4.0.convs.1.all_modules.0.weight shape torch.Size([512, 512, 3, 3])
decoder.encoder.stages.4.0.convs.1.all_modules.0.bias shape torch.Size([512])
decoder.encoder.stages.4.0.convs.1.all_modules.1.weight shape torch.Size([512])
decoder.encoder.stages.4.0.convs.1.all_modules.1.bias shape torch.Size([512])
decoder.encoder.stages.5.0.convs.0.conv.weight shape torch.Size([512, 512, 3, 3])
decoder.encoder.stages.5.0.convs.0.conv.bias shape torch.Size([512])
decoder.encoder.stages.5.0.convs.0.norm.weight shape torch.Size([512])
decoder.encoder.stages.5.0.convs.0.norm.bias shape torch.Size([512])
decoder.encoder.stages.5.0.convs.0.all_modules.0.weight shape torch.Size([512, 512, 3, 3])
decoder.encoder.stages.5.0.convs.0.all_modules.0.bias shape torch.Size([512])
decoder.encoder.stages.5.0.convs.0.all_modules.1.weight shape torch.Size([512])
decoder.encoder.stages.5.0.convs.0.all_modules.1.bias shape torch.Size([512])
decoder.encoder.stages.5.0.convs.1.conv.weight shape torch.Size([512, 512, 3, 3])
decoder.encoder.stages.5.0.convs.1.conv.bias shape torch.Size([512])
decoder.encoder.stages.5.0.convs.1.norm.weight shape torch.Size([512])
decoder.encoder.stages.5.0.convs.1.norm.bias shape torch.Size([512])
decoder.encoder.stages.5.0.convs.1.all_modules.0.weight shape torch.Size([512, 512, 3, 3])
decoder.encoder.stages.5.0.convs.1.all_modules.0.bias shape torch.Size([512])
decoder.encoder.stages.5.0.convs.1.all_modules.1.weight shape torch.Size([512])
decoder.encoder.stages.5.0.convs.1.all_modules.1.bias shape torch.Size([512])
decoder.encoder.stages.6.0.convs.0.conv.weight shape torch.Size([512, 512, 3, 3])
decoder.encoder.stages.6.0.convs.0.conv.bias shape torch.Size([512])
decoder.encoder.stages.6.0.convs.0.norm.weight shape torch.Size([512])
decoder.encoder.stages.6.0.convs.0.norm.bias shape torch.Size([512])
decoder.encoder.stages.6.0.convs.0.all_modules.0.weight shape torch.Size([512, 512, 3, 3])
decoder.encoder.stages.6.0.convs.0.all_modules.0.bias shape torch.Size([512])
decoder.encoder.stages.6.0.convs.0.all_modules.1.weight shape torch.Size([512])
decoder.encoder.stages.6.0.convs.0.all_modules.1.bias shape torch.Size([512])
decoder.encoder.stages.6.0.convs.1.conv.weight shape torch.Size([512, 512, 3, 3])
decoder.encoder.stages.6.0.convs.1.conv.bias shape torch.Size([512])
decoder.encoder.stages.6.0.convs.1.norm.weight shape torch.Size([512])
decoder.encoder.stages.6.0.convs.1.norm.bias shape torch.Size([512])
decoder.encoder.stages.6.0.convs.1.all_modules.0.weight shape torch.Size([512, 512, 3, 3])
decoder.encoder.stages.6.0.convs.1.all_modules.0.bias shape torch.Size([512])
decoder.encoder.stages.6.0.convs.1.all_modules.1.weight shape torch.Size([512])
decoder.encoder.stages.6.0.convs.1.all_modules.1.bias shape torch.Size([512])
decoder.encoder.stages.7.0.convs.0.conv.weight shape torch.Size([512, 512, 3, 3])
decoder.encoder.stages.7.0.convs.0.conv.bias shape torch.Size([512])
decoder.encoder.stages.7.0.convs.0.norm.weight shape torch.Size([512])
decoder.encoder.stages.7.0.convs.0.norm.bias shape torch.Size([512])
decoder.encoder.stages.7.0.convs.0.all_modules.0.weight shape torch.Size([512, 512, 3, 3])
decoder.encoder.stages.7.0.convs.0.all_modules.0.bias shape torch.Size([512])
decoder.encoder.stages.7.0.convs.0.all_modules.1.weight shape torch.Size([512])
decoder.encoder.stages.7.0.convs.0.all_modules.1.bias shape torch.Size([512])
decoder.encoder.stages.7.0.convs.1.conv.weight shape torch.Size([512, 512, 3, 3])
decoder.encoder.stages.7.0.convs.1.conv.bias shape torch.Size([512])
decoder.encoder.stages.7.0.convs.1.norm.weight shape torch.Size([512])
decoder.encoder.stages.7.0.convs.1.norm.bias shape torch.Size([512])
decoder.encoder.stages.7.0.convs.1.all_modules.0.weight shape torch.Size([512, 512, 3, 3])
decoder.encoder.stages.7.0.convs.1.all_modules.0.bias shape torch.Size([512])
decoder.encoder.stages.7.0.convs.1.all_modules.1.weight shape torch.Size([512])
decoder.encoder.stages.7.0.convs.1.all_modules.1.bias shape torch.Size([512])
decoder.stages.0.convs.0.conv.weight shape torch.Size([512, 1024, 3, 3])
decoder.stages.0.convs.0.conv.bias shape torch.Size([512])
decoder.stages.0.convs.0.norm.weight shape torch.Size([512])
decoder.stages.0.convs.0.norm.bias shape torch.Size([512])
decoder.stages.0.convs.0.all_modules.0.weight shape torch.Size([512, 1024, 3, 3])
decoder.stages.0.convs.0.all_modules.0.bias shape torch.Size([512])
decoder.stages.0.convs.0.all_modules.1.weight shape torch.Size([512])
decoder.stages.0.convs.0.all_modules.1.bias shape torch.Size([512])
decoder.stages.0.convs.1.conv.weight shape torch.Size([512, 512, 3, 3])
decoder.stages.0.convs.1.conv.bias shape torch.Size([512])
decoder.stages.0.convs.1.norm.weight shape torch.Size([512])
decoder.stages.0.convs.1.norm.bias shape torch.Size([512])
decoder.stages.0.convs.1.all_modules.0.weight shape torch.Size([512, 512, 3, 3])
decoder.stages.0.convs.1.all_modules.0.bias shape torch.Size([512])
decoder.stages.0.convs.1.all_modules.1.weight shape torch.Size([512])
decoder.stages.0.convs.1.all_modules.1.bias shape torch.Size([512])
decoder.stages.1.convs.0.conv.weight shape torch.Size([512, 1024, 3, 3])
decoder.stages.1.convs.0.conv.bias shape torch.Size([512])
decoder.stages.1.convs.0.norm.weight shape torch.Size([512])
decoder.stages.1.convs.0.norm.bias shape torch.Size([512])
decoder.stages.1.convs.0.all_modules.0.weight shape torch.Size([512, 1024, 3, 3])
decoder.stages.1.convs.0.all_modules.0.bias shape torch.Size([512])
decoder.stages.1.convs.0.all_modules.1.weight shape torch.Size([512])
decoder.stages.1.convs.0.all_modules.1.bias shape torch.Size([512])
decoder.stages.1.convs.1.conv.weight shape torch.Size([512, 512, 3, 3])
decoder.stages.1.convs.1.conv.bias shape torch.Size([512])
decoder.stages.1.convs.1.norm.weight shape torch.Size([512])
decoder.stages.1.convs.1.norm.bias shape torch.Size([512])
decoder.stages.1.convs.1.all_modules.0.weight shape torch.Size([512, 512, 3, 3])
decoder.stages.1.convs.1.all_modules.0.bias shape torch.Size([512])
decoder.stages.1.convs.1.all_modules.1.weight shape torch.Size([512])
decoder.stages.1.convs.1.all_modules.1.bias shape torch.Size([512])
decoder.stages.2.convs.0.conv.weight shape torch.Size([512, 1024, 3, 3])
decoder.stages.2.convs.0.conv.bias shape torch.Size([512])
decoder.stages.2.convs.0.norm.weight shape torch.Size([512])
decoder.stages.2.convs.0.norm.bias shape torch.Size([512])
decoder.stages.2.convs.0.all_modules.0.weight shape torch.Size([512, 1024, 3, 3])
decoder.stages.2.convs.0.all_modules.0.bias shape torch.Size([512])
decoder.stages.2.convs.0.all_modules.1.weight shape torch.Size([512])
decoder.stages.2.convs.0.all_modules.1.bias shape torch.Size([512])
decoder.stages.2.convs.1.conv.weight shape torch.Size([512, 512, 3, 3])
decoder.stages.2.convs.1.conv.bias shape torch.Size([512])
decoder.stages.2.convs.1.norm.weight shape torch.Size([512])
decoder.stages.2.convs.1.norm.bias shape torch.Size([512])
decoder.stages.2.convs.1.all_modules.0.weight shape torch.Size([512, 512, 3, 3])
decoder.stages.2.convs.1.all_modules.0.bias shape torch.Size([512])
decoder.stages.2.convs.1.all_modules.1.weight shape torch.Size([512])
decoder.stages.2.convs.1.all_modules.1.bias shape torch.Size([512])
decoder.stages.3.convs.0.conv.weight shape torch.Size([256, 512, 3, 3])
decoder.stages.3.convs.0.conv.bias shape torch.Size([256])
decoder.stages.3.convs.0.norm.weight shape torch.Size([256])
decoder.stages.3.convs.0.norm.bias shape torch.Size([256])
decoder.stages.3.convs.0.all_modules.0.weight shape torch.Size([256, 512, 3, 3])
decoder.stages.3.convs.0.all_modules.0.bias shape torch.Size([256])
decoder.stages.3.convs.0.all_modules.1.weight shape torch.Size([256])
decoder.stages.3.convs.0.all_modules.1.bias shape torch.Size([256])
decoder.stages.3.convs.1.conv.weight shape torch.Size([256, 256, 3, 3])
decoder.stages.3.convs.1.conv.bias shape torch.Size([256])
decoder.stages.3.convs.1.norm.weight shape torch.Size([256])
decoder.stages.3.convs.1.norm.bias shape torch.Size([256])
decoder.stages.3.convs.1.all_modules.0.weight shape torch.Size([256, 256, 3, 3])
decoder.stages.3.convs.1.all_modules.0.bias shape torch.Size([256])
decoder.stages.3.convs.1.all_modules.1.weight shape torch.Size([256])
decoder.stages.3.convs.1.all_modules.1.bias shape torch.Size([256])
decoder.stages.4.convs.0.conv.weight shape torch.Size([128, 256, 3, 3])
decoder.stages.4.convs.0.conv.bias shape torch.Size([128])
decoder.stages.4.convs.0.norm.weight shape torch.Size([128])
decoder.stages.4.convs.0.norm.bias shape torch.Size([128])
decoder.stages.4.convs.0.all_modules.0.weight shape torch.Size([128, 256, 3, 3])
decoder.stages.4.convs.0.all_modules.0.bias shape torch.Size([128])
decoder.stages.4.convs.0.all_modules.1.weight shape torch.Size([128])
decoder.stages.4.convs.0.all_modules.1.bias shape torch.Size([128])
decoder.stages.4.convs.1.conv.weight shape torch.Size([128, 128, 3, 3])
decoder.stages.4.convs.1.conv.bias shape torch.Size([128])
decoder.stages.4.convs.1.norm.weight shape torch.Size([128])
decoder.stages.4.convs.1.norm.bias shape torch.Size([128])
decoder.stages.4.convs.1.all_modules.0.weight shape torch.Size([128, 128, 3, 3])
decoder.stages.4.convs.1.all_modules.0.bias shape torch.Size([128])
decoder.stages.4.convs.1.all_modules.1.weight shape torch.Size([128])
decoder.stages.4.convs.1.all_modules.1.bias shape torch.Size([128])
decoder.stages.5.convs.0.conv.weight shape torch.Size([64, 128, 3, 3])
decoder.stages.5.convs.0.conv.bias shape torch.Size([64])
decoder.stages.5.convs.0.norm.weight shape torch.Size([64])
decoder.stages.5.convs.0.norm.bias shape torch.Size([64])
decoder.stages.5.convs.0.all_modules.0.weight shape torch.Size([64, 128, 3, 3])
decoder.stages.5.convs.0.all_modules.0.bias shape torch.Size([64])
decoder.stages.5.convs.0.all_modules.1.weight shape torch.Size([64])
decoder.stages.5.convs.0.all_modules.1.bias shape torch.Size([64])
decoder.stages.5.convs.1.conv.weight shape torch.Size([64, 64, 3, 3])
decoder.stages.5.convs.1.conv.bias shape torch.Size([64])
decoder.stages.5.convs.1.norm.weight shape torch.Size([64])
decoder.stages.5.convs.1.norm.bias shape torch.Size([64])
decoder.stages.5.convs.1.all_modules.0.weight shape torch.Size([64, 64, 3, 3])
decoder.stages.5.convs.1.all_modules.0.bias shape torch.Size([64])
decoder.stages.5.convs.1.all_modules.1.weight shape torch.Size([64])
decoder.stages.5.convs.1.all_modules.1.bias shape torch.Size([64])
decoder.stages.6.convs.0.conv.weight shape torch.Size([32, 64, 3, 3])
decoder.stages.6.convs.0.conv.bias shape torch.Size([32])
decoder.stages.6.convs.0.norm.weight shape torch.Size([32])
decoder.stages.6.convs.0.norm.bias shape torch.Size([32])
decoder.stages.6.convs.0.all_modules.0.weight shape torch.Size([32, 64, 3, 3])
decoder.stages.6.convs.0.all_modules.0.bias shape torch.Size([32])
decoder.stages.6.convs.0.all_modules.1.weight shape torch.Size([32])
decoder.stages.6.convs.0.all_modules.1.bias shape torch.Size([32])
decoder.stages.6.convs.1.conv.weight shape torch.Size([32, 32, 3, 3])
decoder.stages.6.convs.1.conv.bias shape torch.Size([32])
decoder.stages.6.convs.1.norm.weight shape torch.Size([32])
decoder.stages.6.convs.1.norm.bias shape torch.Size([32])
decoder.stages.6.convs.1.all_modules.0.weight shape torch.Size([32, 32, 3, 3])
decoder.stages.6.convs.1.all_modules.0.bias shape torch.Size([32])
decoder.stages.6.convs.1.all_modules.1.weight shape torch.Size([32])
decoder.stages.6.convs.1.all_modules.1.bias shape torch.Size([32])
decoder.transpconvs.0.weight shape torch.Size([512, 512, 2, 2])
decoder.transpconvs.0.bias shape torch.Size([512])
decoder.transpconvs.1.weight shape torch.Size([512, 512, 2, 2])
decoder.transpconvs.1.bias shape torch.Size([512])
decoder.transpconvs.2.weight shape torch.Size([512, 512, 2, 2])
decoder.transpconvs.2.bias shape torch.Size([512])
decoder.transpconvs.3.weight shape torch.Size([512, 256, 2, 2])
decoder.transpconvs.3.bias shape torch.Size([256])
decoder.transpconvs.4.weight shape torch.Size([256, 128, 2, 2])
decoder.transpconvs.4.bias shape torch.Size([128])
decoder.transpconvs.5.weight shape torch.Size([128, 64, 2, 2])
decoder.transpconvs.5.bias shape torch.Size([64])
decoder.transpconvs.6.weight shape torch.Size([64, 32, 2, 2])
decoder.transpconvs.6.bias shape torch.Size([32])
################### Done ###################
2024-05-06 18:45:43.687733: do_dummy_2d_data_aug: False
2024-05-06 18:45:43.690741: Using splits from existing split file: /home/gridsan/nchutisilp/datasets/nnUNet_Datasets/nnUNet_preprocessed/Dataset302_Calcium_OCTv2/splits_final.json
2024-05-06 18:45:43.693238: The split file contains 3 splits.
2024-05-06 18:45:43.694610: Desired fold for training: 1
2024-05-06 18:45:43.695433: This split has 4 training and 2 validation cases.
using pin_memory on device 0
using pin_memory on device 0

This is the configuration used by this training:
Configuration name: 2d
 {'data_identifier': 'nnUNetPlans_2d', 'preprocessor_name': 'DefaultPreprocessor', 'batch_size': 12, 'patch_size': [512, 512], 'median_image_size_in_voxels': [498.0, 498.0], 'spacing': [1.0, 1.0], 'normalization_schemes': ['ZScoreNormalization'], 'use_mask_for_norm': [False], 'resampling_fn_data': 'resample_data_or_seg_to_shape', 'resampling_fn_seg': 'resample_data_or_seg_to_shape', 'resampling_fn_data_kwargs': {'is_seg': False, 'order': 3, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_seg_kwargs': {'is_seg': True, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_probabilities': 'resample_data_or_seg_to_shape', 'resampling_fn_probabilities_kwargs': {'is_seg': False, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'architecture': {'network_class_name': 'dynamic_network_architectures.architectures.unet.PlainConvUNet', 'arch_kwargs': {'n_stages': 8, 'features_per_stage': [32, 64, 128, 256, 512, 512, 512, 512], 'conv_op': 'torch.nn.modules.conv.Conv2d', 'kernel_sizes': [[3, 3], [3, 3], [3, 3], [3, 3], [3, 3], [3, 3], [3, 3], [3, 3]], 'strides': [[1, 1], [2, 2], [2, 2], [2, 2], [2, 2], [2, 2], [2, 2], [2, 2]], 'n_conv_per_stage': [2, 2, 2, 2, 2, 2, 2, 2], 'n_conv_per_stage_decoder': [2, 2, 2, 2, 2, 2, 2], 'conv_bias': True, 'norm_op': 'torch.nn.modules.instancenorm.InstanceNorm2d', 'norm_op_kwargs': {'eps': 1e-05, 'affine': True}, 'dropout_op': None, 'dropout_op_kwargs': None, 'nonlin': 'torch.nn.LeakyReLU', 'nonlin_kwargs': {'inplace': True}, 'deep_supervision': True}, '_kw_requires_import': ['conv_op', 'norm_op', 'dropout_op', 'nonlin']}, 'batch_dice': True} 

These are the global plan.json settings:
 {'dataset_name': 'Dataset302_Calcium_OCTv2', 'plans_name': 'nnUNetPlans', 'original_median_spacing_after_transp': [1.0, 1.0, 1.0], 'original_median_shape_after_transp': [375, 498, 498], 'image_reader_writer': 'SimpleITKIO', 'transpose_forward': [0, 1, 2], 'transpose_backward': [0, 1, 2], 'experiment_planner_used': 'ExperimentPlanner', 'label_manager': 'LabelManager', 'foreground_intensity_properties_per_channel': {'0': {'max': 0.4977976381778717, 'mean': 0.13332499563694, 'median': 0.08871842920780182, 'min': 0.0, 'percentile_00_5': 0.014754901640117168, 'percentile_99_5': 0.4977976381778717, 'std': 0.12022686749696732}}} 

2024-05-06 18:45:53.708553: unpacking dataset...
2024-05-06 18:45:59.689321: unpacking done...
2024-05-06 18:45:59.801327: Unable to plot network architecture: nnUNet_compile is enabled!
2024-05-06 18:45:59.909727: 
2024-05-06 18:45:59.911372: Epoch 0
2024-05-06 18:45:59.913250: Current learning rate: 0.01
2024-05-06 18:48:40.571859: Validation loss improved from 1000.00000 to -0.58283! Patience: 0/50
2024-05-06 18:48:40.573813: train_loss -0.5561
2024-05-06 18:48:40.575581: val_loss -0.5828
2024-05-06 18:48:40.576867: Pseudo dice [0.7128]
2024-05-06 18:48:40.578041: Epoch time: 160.67 s
2024-05-06 18:48:40.578969: Yayy! New best EMA pseudo Dice: 0.7128
2024-05-06 18:48:42.385117: 
2024-05-06 18:48:42.387115: Epoch 1
2024-05-06 18:48:42.388885: Current learning rate: 0.00999
2024-05-06 18:49:29.764033: Validation loss did not improve from -0.58283. Patience: 1/50
2024-05-06 18:49:29.766531: train_loss -0.6885
2024-05-06 18:49:29.767825: val_loss -0.5448
2024-05-06 18:49:29.768985: Pseudo dice [0.6901]
2024-05-06 18:49:29.770108: Epoch time: 47.38 s
2024-05-06 18:49:31.059240: 
2024-05-06 18:49:31.060980: Epoch 2
2024-05-06 18:49:31.062220: Current learning rate: 0.00998
2024-05-06 18:50:18.466493: Validation loss improved from -0.58283 to -0.58953! Patience: 1/50
2024-05-06 18:50:18.468791: train_loss -0.7258
2024-05-06 18:50:18.469960: val_loss -0.5895
2024-05-06 18:50:18.470871: Pseudo dice [0.7092]
2024-05-06 18:50:18.471816: Epoch time: 47.41 s
2024-05-06 18:50:19.807161: 
2024-05-06 18:50:19.809050: Epoch 3
2024-05-06 18:50:19.810417: Current learning rate: 0.00997
2024-05-06 18:51:07.208684: Validation loss improved from -0.58953 to -0.60605! Patience: 0/50
2024-05-06 18:51:07.210951: train_loss -0.7465
2024-05-06 18:51:07.212253: val_loss -0.6061
2024-05-06 18:51:07.213483: Pseudo dice [0.7281]
2024-05-06 18:51:07.214706: Epoch time: 47.41 s
2024-05-06 18:51:08.495315: 
2024-05-06 18:51:08.498415: Epoch 4
2024-05-06 18:51:08.500199: Current learning rate: 0.00996
2024-05-06 18:51:55.911410: Validation loss did not improve from -0.60605. Patience: 1/50
2024-05-06 18:51:55.913306: train_loss -0.7576
2024-05-06 18:51:55.914675: val_loss -0.6034
2024-05-06 18:51:55.915798: Pseudo dice [0.7227]
2024-05-06 18:51:55.916842: Epoch time: 47.42 s
2024-05-06 18:51:56.366972: Yayy! New best EMA pseudo Dice: 0.7132
2024-05-06 18:51:58.173664: 
2024-05-06 18:51:58.175488: Epoch 5
2024-05-06 18:51:58.176645: Current learning rate: 0.00995
2024-05-06 18:52:45.601899: Validation loss improved from -0.60605 to -0.61467! Patience: 1/50
2024-05-06 18:52:45.603449: train_loss -0.7619
2024-05-06 18:52:45.604490: val_loss -0.6147
2024-05-06 18:52:45.605502: Pseudo dice [0.7291]
2024-05-06 18:52:45.606447: Epoch time: 47.43 s
2024-05-06 18:52:45.607359: Yayy! New best EMA pseudo Dice: 0.7148
2024-05-06 18:52:47.352076: 
2024-05-06 18:52:47.354527: Epoch 6
2024-05-06 18:52:47.356199: Current learning rate: 0.00995
2024-05-06 18:53:34.808960: Validation loss did not improve from -0.61467. Patience: 1/50
2024-05-06 18:53:34.810883: train_loss -0.7697
2024-05-06 18:53:34.811933: val_loss -0.6123
2024-05-06 18:53:34.812952: Pseudo dice [0.7226]
2024-05-06 18:53:34.813962: Epoch time: 47.46 s
2024-05-06 18:53:34.814865: Yayy! New best EMA pseudo Dice: 0.7156
2024-05-06 18:53:36.537770: 
2024-05-06 18:53:36.539505: Epoch 7
2024-05-06 18:53:36.540832: Current learning rate: 0.00994
2024-05-06 18:54:23.973329: Validation loss improved from -0.61467 to -0.62825! Patience: 1/50
2024-05-06 18:54:23.974965: train_loss -0.7819
2024-05-06 18:54:23.975920: val_loss -0.6282
2024-05-06 18:54:23.976840: Pseudo dice [0.7437]
2024-05-06 18:54:23.977841: Epoch time: 47.44 s
2024-05-06 18:54:23.978698: Yayy! New best EMA pseudo Dice: 0.7184
2024-05-06 18:54:25.764444: 
2024-05-06 18:54:25.766610: Epoch 8
2024-05-06 18:54:25.768303: Current learning rate: 0.00993
2024-05-06 18:55:13.283229: Validation loss improved from -0.62825 to -0.65619! Patience: 0/50
2024-05-06 18:55:13.285124: train_loss -0.7896
2024-05-06 18:55:13.286377: val_loss -0.6562
2024-05-06 18:55:13.287582: Pseudo dice [0.7563]
2024-05-06 18:55:13.288671: Epoch time: 47.52 s
2024-05-06 18:55:13.289665: Yayy! New best EMA pseudo Dice: 0.7222
2024-05-06 18:55:15.468309: 
2024-05-06 18:55:15.470845: Epoch 9
2024-05-06 18:55:15.472136: Current learning rate: 0.00992
2024-05-06 18:56:02.940261: Validation loss did not improve from -0.65619. Patience: 1/50
2024-05-06 18:56:02.941962: train_loss -0.7959
2024-05-06 18:56:02.943201: val_loss -0.6559
2024-05-06 18:56:02.944280: Pseudo dice [0.7612]
2024-05-06 18:56:02.945337: Epoch time: 47.47 s
2024-05-06 18:56:03.449634: Yayy! New best EMA pseudo Dice: 0.7261
2024-05-06 18:56:05.150960: 
2024-05-06 18:56:05.153087: Epoch 10
2024-05-06 18:56:05.154370: Current learning rate: 0.00991
2024-05-06 18:56:52.656866: Validation loss did not improve from -0.65619. Patience: 2/50
2024-05-06 18:56:52.658948: train_loss -0.7939
2024-05-06 18:56:52.660208: val_loss -0.6392
2024-05-06 18:56:52.661430: Pseudo dice [0.7507]
2024-05-06 18:56:52.662644: Epoch time: 47.51 s
2024-05-06 18:56:52.663827: Yayy! New best EMA pseudo Dice: 0.7285
2024-05-06 18:56:54.415915: 
2024-05-06 18:56:54.418114: Epoch 11
2024-05-06 18:56:54.419582: Current learning rate: 0.0099
2024-05-06 18:57:41.905578: Validation loss did not improve from -0.65619. Patience: 3/50
2024-05-06 18:57:41.907487: train_loss -0.801
2024-05-06 18:57:41.908537: val_loss -0.5893
2024-05-06 18:57:41.909557: Pseudo dice [0.7163]
2024-05-06 18:57:41.910541: Epoch time: 47.49 s
2024-05-06 18:57:43.195133: 
2024-05-06 18:57:43.196980: Epoch 12
2024-05-06 18:57:43.198442: Current learning rate: 0.00989
2024-05-06 18:58:30.916708: Validation loss did not improve from -0.65619. Patience: 4/50
2024-05-06 18:58:30.918363: train_loss -0.7998
2024-05-06 18:58:30.919425: val_loss -0.6477
2024-05-06 18:58:30.920382: Pseudo dice [0.7555]
2024-05-06 18:58:30.921476: Epoch time: 47.72 s
2024-05-06 18:58:30.922762: Yayy! New best EMA pseudo Dice: 0.7301
2024-05-06 18:58:32.731034: 
2024-05-06 18:58:32.732581: Epoch 13
2024-05-06 18:58:32.736697: Current learning rate: 0.00988
2024-05-06 18:59:20.255492: Validation loss did not improve from -0.65619. Patience: 5/50
2024-05-06 18:59:20.257333: train_loss -0.8082
2024-05-06 18:59:20.258451: val_loss -0.6441
2024-05-06 18:59:20.259463: Pseudo dice [0.759]
2024-05-06 18:59:20.260469: Epoch time: 47.53 s
2024-05-06 18:59:20.261367: Yayy! New best EMA pseudo Dice: 0.733
2024-05-06 18:59:22.021612: 
2024-05-06 18:59:22.024168: Epoch 14
2024-05-06 18:59:22.025357: Current learning rate: 0.00987
2024-05-06 19:00:09.502537: Validation loss improved from -0.65619 to -0.68423! Patience: 5/50
2024-05-06 19:00:09.504311: train_loss -0.8108
2024-05-06 19:00:09.505576: val_loss -0.6842
2024-05-06 19:00:09.506671: Pseudo dice [0.7819]
2024-05-06 19:00:09.507704: Epoch time: 47.48 s
2024-05-06 19:00:10.016194: Yayy! New best EMA pseudo Dice: 0.7379
2024-05-06 19:00:11.758954: 
2024-05-06 19:00:11.760929: Epoch 15
2024-05-06 19:00:11.762565: Current learning rate: 0.00986
2024-05-06 19:00:59.238444: Validation loss did not improve from -0.68423. Patience: 1/50
2024-05-06 19:00:59.240557: train_loss -0.8142
2024-05-06 19:00:59.241706: val_loss -0.6808
2024-05-06 19:00:59.243053: Pseudo dice [0.7783]
2024-05-06 19:00:59.244265: Epoch time: 47.48 s
2024-05-06 19:00:59.245296: Yayy! New best EMA pseudo Dice: 0.742
2024-05-06 19:01:01.044583: 
2024-05-06 19:01:01.046597: Epoch 16
2024-05-06 19:01:01.048198: Current learning rate: 0.00986
2024-05-06 19:01:48.651315: Validation loss did not improve from -0.68423. Patience: 2/50
2024-05-06 19:01:48.652885: train_loss -0.8191
2024-05-06 19:01:48.654045: val_loss -0.6525
2024-05-06 19:01:48.654952: Pseudo dice [0.7588]
2024-05-06 19:01:48.655951: Epoch time: 47.61 s
2024-05-06 19:01:48.656749: Yayy! New best EMA pseudo Dice: 0.7436
2024-05-06 19:01:50.502260: 
2024-05-06 19:01:50.504377: Epoch 17
2024-05-06 19:01:50.505703: Current learning rate: 0.00985
2024-05-06 19:02:38.147537: Validation loss did not improve from -0.68423. Patience: 3/50
2024-05-06 19:02:38.149420: train_loss -0.8182
2024-05-06 19:02:38.150452: val_loss -0.6683
2024-05-06 19:02:38.151615: Pseudo dice [0.7708]
2024-05-06 19:02:38.152604: Epoch time: 47.65 s
2024-05-06 19:02:38.153494: Yayy! New best EMA pseudo Dice: 0.7464
2024-05-06 19:02:39.961630: 
2024-05-06 19:02:39.964279: Epoch 18
2024-05-06 19:02:39.965469: Current learning rate: 0.00984
2024-05-06 19:03:27.520363: Validation loss improved from -0.68423 to -0.69124! Patience: 3/50
2024-05-06 19:03:27.522402: train_loss -0.826
2024-05-06 19:03:27.533523: val_loss -0.6912
2024-05-06 19:03:27.534769: Pseudo dice [0.7811]
2024-05-06 19:03:27.535753: Epoch time: 47.56 s
2024-05-06 19:03:27.536823: Yayy! New best EMA pseudo Dice: 0.7498
2024-05-06 19:03:29.368320: 
2024-05-06 19:03:29.370186: Epoch 19
2024-05-06 19:03:29.371508: Current learning rate: 0.00983
2024-05-06 19:04:16.926461: Validation loss did not improve from -0.69124. Patience: 1/50
2024-05-06 19:04:16.927958: train_loss -0.8236
2024-05-06 19:04:16.929008: val_loss -0.6634
2024-05-06 19:04:16.930099: Pseudo dice [0.7626]
2024-05-06 19:04:16.931066: Epoch time: 47.56 s
2024-05-06 19:04:17.450423: Yayy! New best EMA pseudo Dice: 0.7511
2024-05-06 19:04:19.279346: 
2024-05-06 19:04:19.281431: Epoch 20
2024-05-06 19:04:19.283046: Current learning rate: 0.00982
2024-05-06 19:05:06.867642: Validation loss did not improve from -0.69124. Patience: 2/50
2024-05-06 19:05:06.869616: train_loss -0.8281
2024-05-06 19:05:06.870703: val_loss -0.6726
2024-05-06 19:05:06.871666: Pseudo dice [0.7718]
2024-05-06 19:05:06.872599: Epoch time: 47.59 s
2024-05-06 19:05:06.873636: Yayy! New best EMA pseudo Dice: 0.7532
2024-05-06 19:05:09.090571: 
2024-05-06 19:05:09.092843: Epoch 21
2024-05-06 19:05:09.094778: Current learning rate: 0.00981
2024-05-06 19:05:56.668267: Validation loss did not improve from -0.69124. Patience: 3/50
2024-05-06 19:05:56.669648: train_loss -0.8285
2024-05-06 19:05:56.670661: val_loss -0.6278
2024-05-06 19:05:56.671534: Pseudo dice [0.7408]
2024-05-06 19:05:56.672498: Epoch time: 47.58 s
2024-05-06 19:05:58.137886: 
2024-05-06 19:05:58.139807: Epoch 22
2024-05-06 19:05:58.141110: Current learning rate: 0.0098
2024-05-06 19:06:45.698509: Validation loss did not improve from -0.69124. Patience: 4/50
2024-05-06 19:06:45.700651: train_loss -0.8333
2024-05-06 19:06:45.701621: val_loss -0.6456
2024-05-06 19:06:45.702511: Pseudo dice [0.7503]
2024-05-06 19:06:45.703532: Epoch time: 47.56 s
2024-05-06 19:06:46.961220: 
2024-05-06 19:06:46.963339: Epoch 23
2024-05-06 19:06:46.964435: Current learning rate: 0.00979
2024-05-06 19:07:34.539786: Validation loss did not improve from -0.69124. Patience: 5/50
2024-05-06 19:07:34.541453: train_loss -0.8398
2024-05-06 19:07:34.542782: val_loss -0.6489
2024-05-06 19:07:34.543987: Pseudo dice [0.7621]
2024-05-06 19:07:34.545207: Epoch time: 47.58 s
2024-05-06 19:07:35.795778: 
2024-05-06 19:07:35.798937: Epoch 24
2024-05-06 19:07:35.800542: Current learning rate: 0.00978
2024-05-06 19:08:23.454110: Validation loss did not improve from -0.69124. Patience: 6/50
2024-05-06 19:08:23.455855: train_loss -0.8331
2024-05-06 19:08:23.457002: val_loss -0.6263
2024-05-06 19:08:23.458070: Pseudo dice [0.7476]
2024-05-06 19:08:23.459076: Epoch time: 47.66 s
2024-05-06 19:08:25.226214: 
2024-05-06 19:08:25.228616: Epoch 25
2024-05-06 19:08:25.230231: Current learning rate: 0.00977
2024-05-06 19:09:12.891747: Validation loss did not improve from -0.69124. Patience: 7/50
2024-05-06 19:09:12.893550: train_loss -0.8392
2024-05-06 19:09:12.894583: val_loss -0.6271
2024-05-06 19:09:12.895547: Pseudo dice [0.7495]
2024-05-06 19:09:12.896478: Epoch time: 47.67 s
2024-05-06 19:09:14.135405: 
2024-05-06 19:09:14.137070: Epoch 26
2024-05-06 19:09:14.138253: Current learning rate: 0.00977
2024-05-06 19:10:01.791074: Validation loss did not improve from -0.69124. Patience: 8/50
2024-05-06 19:10:01.794019: train_loss -0.8375
2024-05-06 19:10:01.795566: val_loss -0.6666
2024-05-06 19:10:01.796909: Pseudo dice [0.7686]
2024-05-06 19:10:01.798178: Epoch time: 47.66 s
2024-05-06 19:10:01.799425: Yayy! New best EMA pseudo Dice: 0.7537
2024-05-06 19:10:03.558101: 
2024-05-06 19:10:03.560063: Epoch 27
2024-05-06 19:10:03.561205: Current learning rate: 0.00976
2024-05-06 19:10:51.207388: Validation loss did not improve from -0.69124. Patience: 9/50
2024-05-06 19:10:51.210046: train_loss -0.8402
2024-05-06 19:10:51.211556: val_loss -0.6868
2024-05-06 19:10:51.212929: Pseudo dice [0.7821]
2024-05-06 19:10:51.214585: Epoch time: 47.65 s
2024-05-06 19:10:51.215948: Yayy! New best EMA pseudo Dice: 0.7565
2024-05-06 19:10:53.023603: 
2024-05-06 19:10:53.025353: Epoch 28
2024-05-06 19:10:53.026544: Current learning rate: 0.00975
2024-05-06 19:11:40.693068: Validation loss did not improve from -0.69124. Patience: 10/50
2024-05-06 19:11:40.695441: train_loss -0.8443
2024-05-06 19:11:40.696667: val_loss -0.6692
2024-05-06 19:11:40.697682: Pseudo dice [0.7703]
2024-05-06 19:11:40.698661: Epoch time: 47.67 s
2024-05-06 19:11:40.699602: Yayy! New best EMA pseudo Dice: 0.7579
2024-05-06 19:11:42.468700: 
2024-05-06 19:11:42.470641: Epoch 29
2024-05-06 19:11:42.471766: Current learning rate: 0.00974
2024-05-06 19:12:30.162720: Validation loss did not improve from -0.69124. Patience: 11/50
2024-05-06 19:12:30.164677: train_loss -0.8459
2024-05-06 19:12:30.165989: val_loss -0.6521
2024-05-06 19:12:30.167095: Pseudo dice [0.7629]
2024-05-06 19:12:30.168128: Epoch time: 47.7 s
2024-05-06 19:12:30.706845: Yayy! New best EMA pseudo Dice: 0.7584
2024-05-06 19:12:32.471498: 
2024-05-06 19:12:32.473711: Epoch 30
2024-05-06 19:12:32.475191: Current learning rate: 0.00973
2024-05-06 19:13:20.143719: Validation loss did not improve from -0.69124. Patience: 12/50
2024-05-06 19:13:20.145493: train_loss -0.8399
2024-05-06 19:13:20.146635: val_loss -0.6594
2024-05-06 19:13:20.147738: Pseudo dice [0.7678]
2024-05-06 19:13:20.148848: Epoch time: 47.68 s
2024-05-06 19:13:20.149830: Yayy! New best EMA pseudo Dice: 0.7593
2024-05-06 19:13:21.948832: 
2024-05-06 19:13:21.950754: Epoch 31
2024-05-06 19:13:21.951873: Current learning rate: 0.00972
2024-05-06 19:14:09.594214: Validation loss did not improve from -0.69124. Patience: 13/50
2024-05-06 19:14:09.596095: train_loss -0.8424
2024-05-06 19:14:09.597413: val_loss -0.617
2024-05-06 19:14:09.598515: Pseudo dice [0.7337]
2024-05-06 19:14:09.599535: Epoch time: 47.65 s
2024-05-06 19:14:10.846807: 
2024-05-06 19:14:10.848336: Epoch 32
2024-05-06 19:14:10.849578: Current learning rate: 0.00971
2024-05-06 19:14:58.977418: Validation loss did not improve from -0.69124. Patience: 14/50
2024-05-06 19:14:58.979023: train_loss -0.8458
2024-05-06 19:14:58.980172: val_loss -0.6851
2024-05-06 19:14:58.981447: Pseudo dice [0.7813]
2024-05-06 19:14:58.982458: Epoch time: 48.13 s
2024-05-06 19:15:00.266440: 
2024-05-06 19:15:00.268537: Epoch 33
2024-05-06 19:15:00.270335: Current learning rate: 0.0097
2024-05-06 19:15:47.980758: Validation loss did not improve from -0.69124. Patience: 15/50
2024-05-06 19:15:47.982283: train_loss -0.8492
2024-05-06 19:15:47.983502: val_loss -0.6267
2024-05-06 19:15:47.984420: Pseudo dice [0.7455]
2024-05-06 19:15:47.985523: Epoch time: 47.72 s
2024-05-06 19:15:49.298331: 
2024-05-06 19:15:49.300633: Epoch 34
2024-05-06 19:15:49.302175: Current learning rate: 0.00969
2024-05-06 19:16:36.984448: Validation loss did not improve from -0.69124. Patience: 16/50
2024-05-06 19:16:36.986253: train_loss -0.8501
2024-05-06 19:16:36.987785: val_loss -0.6801
2024-05-06 19:16:36.988856: Pseudo dice [0.7798]
2024-05-06 19:16:36.990047: Epoch time: 47.69 s
2024-05-06 19:16:37.519004: Yayy! New best EMA pseudo Dice: 0.7601
2024-05-06 19:16:39.308795: 
2024-05-06 19:16:39.310762: Epoch 35
2024-05-06 19:16:39.311810: Current learning rate: 0.00968
2024-05-06 19:17:26.919051: Validation loss did not improve from -0.69124. Patience: 17/50
2024-05-06 19:17:26.920696: train_loss -0.8456
2024-05-06 19:17:26.921689: val_loss -0.6608
2024-05-06 19:17:26.922620: Pseudo dice [0.766]
2024-05-06 19:17:26.923609: Epoch time: 47.61 s
2024-05-06 19:17:26.924546: Yayy! New best EMA pseudo Dice: 0.7606
2024-05-06 19:17:28.755070: 
2024-05-06 19:17:28.756865: Epoch 36
2024-05-06 19:17:28.758445: Current learning rate: 0.00968
2024-05-06 19:18:16.394783: Validation loss did not improve from -0.69124. Patience: 18/50
2024-05-06 19:18:16.396658: train_loss -0.8526
2024-05-06 19:18:16.397974: val_loss -0.6449
2024-05-06 19:18:16.399122: Pseudo dice [0.7612]
2024-05-06 19:18:16.400012: Epoch time: 47.64 s
2024-05-06 19:18:16.400833: Yayy! New best EMA pseudo Dice: 0.7607
2024-05-06 19:18:18.236087: 
2024-05-06 19:18:18.238287: Epoch 37
2024-05-06 19:18:18.239989: Current learning rate: 0.00967
2024-05-06 19:19:06.109632: Validation loss did not improve from -0.69124. Patience: 19/50
2024-05-06 19:19:06.111419: train_loss -0.8565
2024-05-06 19:19:06.112728: val_loss -0.6187
2024-05-06 19:19:06.113785: Pseudo dice [0.7491]
2024-05-06 19:19:06.114783: Epoch time: 47.88 s
2024-05-06 19:19:07.458412: 
2024-05-06 19:19:07.460490: Epoch 38
2024-05-06 19:19:07.461418: Current learning rate: 0.00966
2024-05-06 19:19:55.127524: Validation loss did not improve from -0.69124. Patience: 20/50
2024-05-06 19:19:55.129519: train_loss -0.8561
2024-05-06 19:19:55.130702: val_loss -0.6671
2024-05-06 19:19:55.131764: Pseudo dice [0.7688]
2024-05-06 19:19:55.132851: Epoch time: 47.67 s
2024-05-06 19:19:56.420905: 
2024-05-06 19:19:56.423609: Epoch 39
2024-05-06 19:19:56.425607: Current learning rate: 0.00965
2024-05-06 19:20:45.492026: Validation loss improved from -0.69124 to -0.69615! Patience: 20/50
2024-05-06 19:20:45.493865: train_loss -0.8547
2024-05-06 19:20:45.495059: val_loss -0.6961
2024-05-06 19:20:45.496234: Pseudo dice [0.7892]
2024-05-06 19:20:45.497356: Epoch time: 49.07 s
2024-05-06 19:20:46.038448: Yayy! New best EMA pseudo Dice: 0.7633
2024-05-06 19:20:47.890471: 
2024-05-06 19:20:47.892427: Epoch 40
2024-05-06 19:20:47.894063: Current learning rate: 0.00964
2024-05-06 19:21:35.475697: Validation loss did not improve from -0.69615. Patience: 1/50
2024-05-06 19:21:35.478350: train_loss -0.8577
2024-05-06 19:21:35.480302: val_loss -0.6363
2024-05-06 19:21:35.481569: Pseudo dice [0.752]
2024-05-06 19:21:35.482555: Epoch time: 47.59 s
2024-05-06 19:21:36.800697: 
2024-05-06 19:21:36.803063: Epoch 41
2024-05-06 19:21:36.805039: Current learning rate: 0.00963
2024-05-06 19:22:24.399365: Validation loss did not improve from -0.69615. Patience: 2/50
2024-05-06 19:22:24.401647: train_loss -0.8628
2024-05-06 19:22:24.403139: val_loss -0.6369
2024-05-06 19:22:24.404193: Pseudo dice [0.7599]
2024-05-06 19:22:24.405234: Epoch time: 47.6 s
2024-05-06 19:22:25.663354: 
2024-05-06 19:22:25.665310: Epoch 42
2024-05-06 19:22:25.666689: Current learning rate: 0.00962
2024-05-06 19:23:13.321586: Validation loss did not improve from -0.69615. Patience: 3/50
2024-05-06 19:23:13.323363: train_loss -0.8618
2024-05-06 19:23:13.324795: val_loss -0.6467
2024-05-06 19:23:13.326011: Pseudo dice [0.7646]
2024-05-06 19:23:13.327208: Epoch time: 47.66 s
2024-05-06 19:23:14.572669: 
2024-05-06 19:23:14.574527: Epoch 43
2024-05-06 19:23:14.575970: Current learning rate: 0.00961
2024-05-06 19:24:02.268040: Validation loss did not improve from -0.69615. Patience: 4/50
2024-05-06 19:24:02.270806: train_loss -0.8589
2024-05-06 19:24:02.271794: val_loss -0.6674
2024-05-06 19:24:02.272804: Pseudo dice [0.7714]
2024-05-06 19:24:02.273672: Epoch time: 47.7 s
2024-05-06 19:24:03.919657: 
2024-05-06 19:24:03.922374: Epoch 44
2024-05-06 19:24:03.923393: Current learning rate: 0.0096
2024-05-06 19:24:51.573882: Validation loss did not improve from -0.69615. Patience: 5/50
2024-05-06 19:24:51.576402: train_loss -0.8604
2024-05-06 19:24:51.577639: val_loss -0.6694
2024-05-06 19:24:51.578749: Pseudo dice [0.7744]
2024-05-06 19:24:51.579874: Epoch time: 47.66 s
2024-05-06 19:24:52.044799: Yayy! New best EMA pseudo Dice: 0.7643
2024-05-06 19:24:53.697114: 
2024-05-06 19:24:53.701307: Epoch 45
2024-05-06 19:24:53.703326: Current learning rate: 0.00959
2024-05-06 19:25:41.309693: Validation loss did not improve from -0.69615. Patience: 6/50
2024-05-06 19:25:41.312921: train_loss -0.8636
2024-05-06 19:25:41.314121: val_loss -0.6548
2024-05-06 19:25:41.315336: Pseudo dice [0.7678]
2024-05-06 19:25:41.316490: Epoch time: 47.62 s
2024-05-06 19:25:41.317611: Yayy! New best EMA pseudo Dice: 0.7646
2024-05-06 19:25:43.101805: 
2024-05-06 19:25:43.105320: Epoch 46
2024-05-06 19:25:43.107476: Current learning rate: 0.00959
2024-05-06 19:26:30.717240: Validation loss did not improve from -0.69615. Patience: 7/50
2024-05-06 19:26:30.719684: train_loss -0.864
2024-05-06 19:26:30.720789: val_loss -0.6662
2024-05-06 19:26:30.721866: Pseudo dice [0.7719]
2024-05-06 19:26:30.722880: Epoch time: 47.62 s
2024-05-06 19:26:30.723810: Yayy! New best EMA pseudo Dice: 0.7654
2024-05-06 19:26:32.460842: 
2024-05-06 19:26:32.463959: Epoch 47
2024-05-06 19:26:32.465125: Current learning rate: 0.00958
2024-05-06 19:27:20.077021: Validation loss did not improve from -0.69615. Patience: 8/50
2024-05-06 19:27:20.079686: train_loss -0.8637
2024-05-06 19:27:20.080713: val_loss -0.6799
2024-05-06 19:27:20.081730: Pseudo dice [0.7803]
2024-05-06 19:27:20.082732: Epoch time: 47.62 s
2024-05-06 19:27:20.083757: Yayy! New best EMA pseudo Dice: 0.7669
2024-05-06 19:27:21.856493: 
2024-05-06 19:27:21.858991: Epoch 48
2024-05-06 19:27:21.860668: Current learning rate: 0.00957
2024-05-06 19:28:09.491578: Validation loss did not improve from -0.69615. Patience: 9/50
2024-05-06 19:28:09.494240: train_loss -0.8641
2024-05-06 19:28:09.495341: val_loss -0.6663
2024-05-06 19:28:09.496368: Pseudo dice [0.7741]
2024-05-06 19:28:09.497406: Epoch time: 47.64 s
2024-05-06 19:28:09.498422: Yayy! New best EMA pseudo Dice: 0.7676
2024-05-06 19:28:11.292335: 
2024-05-06 19:28:11.295274: Epoch 49
2024-05-06 19:28:11.296442: Current learning rate: 0.00956
2024-05-06 19:28:58.893470: Validation loss did not improve from -0.69615. Patience: 10/50
2024-05-06 19:28:58.898574: train_loss -0.8666
2024-05-06 19:28:58.899857: val_loss -0.6203
2024-05-06 19:28:58.901103: Pseudo dice [0.7439]
2024-05-06 19:28:58.902278: Epoch time: 47.61 s
2024-05-06 19:29:00.704943: 
2024-05-06 19:29:00.708499: Epoch 50
2024-05-06 19:29:00.710153: Current learning rate: 0.00955
2024-05-06 19:29:48.398290: Validation loss did not improve from -0.69615. Patience: 11/50
2024-05-06 19:29:48.401492: train_loss -0.8659
2024-05-06 19:29:48.402752: val_loss -0.6875
2024-05-06 19:29:48.403921: Pseudo dice [0.7868]
2024-05-06 19:29:48.405196: Epoch time: 47.7 s
2024-05-06 19:29:49.670144: 
2024-05-06 19:29:49.673848: Epoch 51
2024-05-06 19:29:49.675342: Current learning rate: 0.00954
2024-05-06 19:30:37.348191: Validation loss did not improve from -0.69615. Patience: 12/50
2024-05-06 19:30:37.350827: train_loss -0.8704
2024-05-06 19:30:37.351929: val_loss -0.6665
2024-05-06 19:30:37.352898: Pseudo dice [0.7692]
2024-05-06 19:30:37.353874: Epoch time: 47.68 s
2024-05-06 19:30:38.584620: 
2024-05-06 19:30:38.587682: Epoch 52
2024-05-06 19:30:38.588707: Current learning rate: 0.00953
2024-05-06 19:31:26.215379: Validation loss did not improve from -0.69615. Patience: 13/50
2024-05-06 19:31:26.218014: train_loss -0.8679
2024-05-06 19:31:26.219172: val_loss -0.6911
2024-05-06 19:31:26.220242: Pseudo dice [0.7918]
2024-05-06 19:31:26.221242: Epoch time: 47.63 s
2024-05-06 19:31:26.222308: Yayy! New best EMA pseudo Dice: 0.77
2024-05-06 19:31:27.942973: 
2024-05-06 19:31:27.946035: Epoch 53
2024-05-06 19:31:27.947845: Current learning rate: 0.00952
2024-05-06 19:32:15.578382: Validation loss did not improve from -0.69615. Patience: 14/50
2024-05-06 19:32:15.581020: train_loss -0.8716
2024-05-06 19:32:15.582260: val_loss -0.6777
2024-05-06 19:32:15.583420: Pseudo dice [0.7868]
2024-05-06 19:32:15.584636: Epoch time: 47.64 s
2024-05-06 19:32:15.585739: Yayy! New best EMA pseudo Dice: 0.7717
2024-05-06 19:32:17.378375: 
2024-05-06 19:32:17.380789: Epoch 54
2024-05-06 19:32:17.381907: Current learning rate: 0.00951
2024-05-06 19:33:04.974360: Validation loss did not improve from -0.69615. Patience: 15/50
2024-05-06 19:33:04.977169: train_loss -0.8701
2024-05-06 19:33:04.978149: val_loss -0.6836
2024-05-06 19:33:04.979084: Pseudo dice [0.7815]
2024-05-06 19:33:04.980059: Epoch time: 47.6 s
2024-05-06 19:33:05.524417: Yayy! New best EMA pseudo Dice: 0.7726
2024-05-06 19:33:07.512581: 
2024-05-06 19:33:07.515242: Epoch 55
2024-05-06 19:33:07.516333: Current learning rate: 0.0095
2024-05-06 19:33:55.069507: Validation loss did not improve from -0.69615. Patience: 16/50
2024-05-06 19:33:55.073340: train_loss -0.8683
2024-05-06 19:33:55.075389: val_loss -0.6662
2024-05-06 19:33:55.076767: Pseudo dice [0.7738]
2024-05-06 19:33:55.078220: Epoch time: 47.56 s
2024-05-06 19:33:55.079213: Yayy! New best EMA pseudo Dice: 0.7728
2024-05-06 19:33:57.254375: 
2024-05-06 19:33:57.258181: Epoch 56
2024-05-06 19:33:57.259690: Current learning rate: 0.00949
2024-05-06 19:34:44.820731: Validation loss did not improve from -0.69615. Patience: 17/50
2024-05-06 19:34:44.823295: train_loss -0.8697
2024-05-06 19:34:44.825405: val_loss -0.66
2024-05-06 19:34:44.826466: Pseudo dice [0.7683]
2024-05-06 19:34:44.827472: Epoch time: 47.57 s
2024-05-06 19:34:46.090399: 
2024-05-06 19:34:46.092954: Epoch 57
2024-05-06 19:34:46.094022: Current learning rate: 0.00949
2024-05-06 19:35:33.686249: Validation loss did not improve from -0.69615. Patience: 18/50
2024-05-06 19:35:33.689250: train_loss -0.8727
2024-05-06 19:35:33.690449: val_loss -0.6542
2024-05-06 19:35:33.691614: Pseudo dice [0.7687]
2024-05-06 19:35:33.692906: Epoch time: 47.6 s
2024-05-06 19:35:34.975245: 
2024-05-06 19:35:34.978293: Epoch 58
2024-05-06 19:35:34.979737: Current learning rate: 0.00948
2024-05-06 19:36:22.602740: Validation loss did not improve from -0.69615. Patience: 19/50
2024-05-06 19:36:22.607118: train_loss -0.8722
2024-05-06 19:36:22.609038: val_loss -0.6791
2024-05-06 19:36:22.610386: Pseudo dice [0.7803]
2024-05-06 19:36:22.611856: Epoch time: 47.63 s
2024-05-06 19:36:22.613008: Yayy! New best EMA pseudo Dice: 0.7728
2024-05-06 19:36:24.414593: 
2024-05-06 19:36:24.418358: Epoch 59
2024-05-06 19:36:24.419959: Current learning rate: 0.00947
2024-05-06 19:37:12.048731: Validation loss did not improve from -0.69615. Patience: 20/50
2024-05-06 19:37:12.051704: train_loss -0.872
2024-05-06 19:37:12.052732: val_loss -0.6882
2024-05-06 19:37:12.053860: Pseudo dice [0.7874]
2024-05-06 19:37:12.055060: Epoch time: 47.64 s
2024-05-06 19:37:12.594706: Yayy! New best EMA pseudo Dice: 0.7743
2024-05-06 19:37:14.392710: 
2024-05-06 19:37:14.396483: Epoch 60
2024-05-06 19:37:14.398438: Current learning rate: 0.00946
2024-05-06 19:38:01.972636: Validation loss did not improve from -0.69615. Patience: 21/50
2024-05-06 19:38:01.975337: train_loss -0.8751
2024-05-06 19:38:01.976390: val_loss -0.6226
2024-05-06 19:38:01.977597: Pseudo dice [0.7466]
2024-05-06 19:38:01.978609: Epoch time: 47.58 s
2024-05-06 19:38:03.268046: 
2024-05-06 19:38:03.271322: Epoch 61
2024-05-06 19:38:03.272404: Current learning rate: 0.00945
2024-05-06 19:38:50.876859: Validation loss did not improve from -0.69615. Patience: 22/50
2024-05-06 19:38:50.879894: train_loss -0.8732
2024-05-06 19:38:50.880959: val_loss -0.6854
2024-05-06 19:38:50.882125: Pseudo dice [0.7857]
2024-05-06 19:38:50.883195: Epoch time: 47.61 s
2024-05-06 19:38:52.193943: 
2024-05-06 19:38:52.196389: Epoch 62
2024-05-06 19:38:52.197599: Current learning rate: 0.00944
2024-05-06 19:39:39.869354: Validation loss did not improve from -0.69615. Patience: 23/50
2024-05-06 19:39:39.872376: train_loss -0.8726
2024-05-06 19:39:39.873615: val_loss -0.6703
2024-05-06 19:39:39.874807: Pseudo dice [0.7748]
2024-05-06 19:39:39.876001: Epoch time: 47.68 s
2024-05-06 19:39:41.178786: 
2024-05-06 19:39:41.181532: Epoch 63
2024-05-06 19:39:41.182765: Current learning rate: 0.00943
2024-05-06 19:40:28.792875: Validation loss did not improve from -0.69615. Patience: 24/50
2024-05-06 19:40:28.795584: train_loss -0.8694
2024-05-06 19:40:28.797076: val_loss -0.6754
2024-05-06 19:40:28.798191: Pseudo dice [0.7806]
2024-05-06 19:40:28.799325: Epoch time: 47.62 s
2024-05-06 19:40:30.110514: 
2024-05-06 19:40:30.114197: Epoch 64
2024-05-06 19:40:30.115638: Current learning rate: 0.00942
2024-05-06 19:41:17.688444: Validation loss did not improve from -0.69615. Patience: 25/50
2024-05-06 19:41:17.691806: train_loss -0.8778
2024-05-06 19:41:17.693350: val_loss -0.6507
2024-05-06 19:41:17.694647: Pseudo dice [0.7671]
2024-05-06 19:41:17.695745: Epoch time: 47.58 s
2024-05-06 19:41:19.472389: 
2024-05-06 19:41:19.475725: Epoch 65
2024-05-06 19:41:19.477769: Current learning rate: 0.00941
2024-05-06 19:42:06.965068: Validation loss did not improve from -0.69615. Patience: 26/50
2024-05-06 19:42:06.967827: train_loss -0.8758
2024-05-06 19:42:06.968842: val_loss -0.6505
2024-05-06 19:42:06.970072: Pseudo dice [0.7656]
2024-05-06 19:42:06.971055: Epoch time: 47.5 s
2024-05-06 19:42:08.267672: 
2024-05-06 19:42:08.271273: Epoch 66
2024-05-06 19:42:08.272438: Current learning rate: 0.0094
2024-05-06 19:42:55.847799: Validation loss did not improve from -0.69615. Patience: 27/50
2024-05-06 19:42:55.851047: train_loss -0.8749
2024-05-06 19:42:55.852270: val_loss -0.658
2024-05-06 19:42:55.853392: Pseudo dice [0.7686]
2024-05-06 19:42:55.854608: Epoch time: 47.58 s
2024-05-06 19:42:57.146198: 
2024-05-06 19:42:57.149625: Epoch 67
2024-05-06 19:42:57.151123: Current learning rate: 0.00939
2024-05-06 19:43:44.680341: Validation loss did not improve from -0.69615. Patience: 28/50
2024-05-06 19:43:44.682750: train_loss -0.8768
2024-05-06 19:43:44.683948: val_loss -0.6763
2024-05-06 19:43:44.684847: Pseudo dice [0.7723]
2024-05-06 19:43:44.685879: Epoch time: 47.54 s
2024-05-06 19:43:46.381613: 
2024-05-06 19:43:46.386799: Epoch 68
2024-05-06 19:43:46.387918: Current learning rate: 0.00939
2024-05-06 19:44:33.943501: Validation loss did not improve from -0.69615. Patience: 29/50
2024-05-06 19:44:33.946688: train_loss -0.8765
2024-05-06 19:44:33.947869: val_loss -0.6649
2024-05-06 19:44:33.948994: Pseudo dice [0.7737]
2024-05-06 19:44:33.950043: Epoch time: 47.57 s
2024-05-06 19:44:35.287620: 
2024-05-06 19:44:35.290987: Epoch 69
2024-05-06 19:44:35.292236: Current learning rate: 0.00938
2024-05-06 19:45:22.873913: Validation loss did not improve from -0.69615. Patience: 30/50
2024-05-06 19:45:22.876920: train_loss -0.8765
2024-05-06 19:45:22.878334: val_loss -0.6657
2024-05-06 19:45:22.879596: Pseudo dice [0.7757]
2024-05-06 19:45:22.880817: Epoch time: 47.59 s
2024-05-06 19:45:25.001683: 
2024-05-06 19:45:25.005111: Epoch 70
2024-05-06 19:45:25.006955: Current learning rate: 0.00937
2024-05-06 19:46:12.529900: Validation loss did not improve from -0.69615. Patience: 31/50
2024-05-06 19:46:12.532608: train_loss -0.8786
2024-05-06 19:46:12.533859: val_loss -0.6374
2024-05-06 19:46:12.534982: Pseudo dice [0.7513]
2024-05-06 19:46:12.536119: Epoch time: 47.53 s
2024-05-06 19:46:13.871704: 
2024-05-06 19:46:13.875082: Epoch 71
2024-05-06 19:46:13.876637: Current learning rate: 0.00936
2024-05-06 19:47:01.458180: Validation loss did not improve from -0.69615. Patience: 32/50
2024-05-06 19:47:01.461665: train_loss -0.8776
2024-05-06 19:47:01.463040: val_loss -0.654
2024-05-06 19:47:01.464027: Pseudo dice [0.7668]
2024-05-06 19:47:01.465023: Epoch time: 47.59 s
2024-05-06 19:47:02.768640: 
2024-05-06 19:47:02.771685: Epoch 72
2024-05-06 19:47:02.773540: Current learning rate: 0.00935
2024-05-06 19:47:50.394794: Validation loss did not improve from -0.69615. Patience: 33/50
2024-05-06 19:47:50.397486: train_loss -0.8816
2024-05-06 19:47:50.398664: val_loss -0.6466
2024-05-06 19:47:50.399918: Pseudo dice [0.762]
2024-05-06 19:47:50.400966: Epoch time: 47.63 s
2024-05-06 19:47:51.742905: 
2024-05-06 19:47:51.745996: Epoch 73
2024-05-06 19:47:51.747379: Current learning rate: 0.00934
2024-05-06 19:48:39.364632: Validation loss did not improve from -0.69615. Patience: 34/50
2024-05-06 19:48:39.367669: train_loss -0.8828
2024-05-06 19:48:39.368734: val_loss -0.662
2024-05-06 19:48:39.369793: Pseudo dice [0.7725]
2024-05-06 19:48:39.370838: Epoch time: 47.63 s
2024-05-06 19:48:40.692830: 
2024-05-06 19:48:40.695362: Epoch 74
2024-05-06 19:48:40.696977: Current learning rate: 0.00933
2024-05-06 19:49:28.344766: Validation loss did not improve from -0.69615. Patience: 35/50
2024-05-06 19:49:28.347770: train_loss -0.8845
2024-05-06 19:49:28.349126: val_loss -0.6694
2024-05-06 19:49:28.350500: Pseudo dice [0.7761]
2024-05-06 19:49:28.351628: Epoch time: 47.66 s
2024-05-06 19:49:30.205616: 
2024-05-06 19:49:30.209159: Epoch 75
2024-05-06 19:49:30.210662: Current learning rate: 0.00932
2024-05-06 19:50:17.889366: Validation loss did not improve from -0.69615. Patience: 36/50
2024-05-06 19:50:17.891868: train_loss -0.882
2024-05-06 19:50:17.893312: val_loss -0.6807
2024-05-06 19:50:17.894701: Pseudo dice [0.7816]
2024-05-06 19:50:17.896183: Epoch time: 47.69 s
2024-05-06 19:50:19.242399: 
2024-05-06 19:50:19.245740: Epoch 76
2024-05-06 19:50:19.247507: Current learning rate: 0.00931
2024-05-06 19:51:07.100364: Validation loss did not improve from -0.69615. Patience: 37/50
2024-05-06 19:51:07.104410: train_loss -0.8851
2024-05-06 19:51:07.105651: val_loss -0.6386
2024-05-06 19:51:07.106624: Pseudo dice [0.7561]
2024-05-06 19:51:07.107645: Epoch time: 47.86 s
2024-05-06 19:51:08.370384: 
2024-05-06 19:51:08.375514: Epoch 77
2024-05-06 19:51:08.376473: Current learning rate: 0.0093
2024-05-06 19:51:56.601723: Validation loss did not improve from -0.69615. Patience: 38/50
2024-05-06 19:51:56.605183: train_loss -0.8892
2024-05-06 19:51:56.606525: val_loss -0.6341
2024-05-06 19:51:56.607691: Pseudo dice [0.7515]
2024-05-06 19:51:56.608753: Epoch time: 48.24 s
2024-05-06 19:51:57.945256: 
2024-05-06 19:51:57.948367: Epoch 78
2024-05-06 19:51:57.950110: Current learning rate: 0.0093
2024-05-06 19:52:45.889841: Validation loss did not improve from -0.69615. Patience: 39/50
2024-05-06 19:52:45.892404: train_loss -0.8832
2024-05-06 19:52:45.893634: val_loss -0.6275
2024-05-06 19:52:45.894932: Pseudo dice [0.7553]
2024-05-06 19:52:45.895983: Epoch time: 47.95 s
2024-05-06 19:52:47.213098: 
2024-05-06 19:52:47.215817: Epoch 79
2024-05-06 19:52:47.216975: Current learning rate: 0.00929
2024-05-06 19:53:35.140069: Validation loss did not improve from -0.69615. Patience: 40/50
2024-05-06 19:53:35.143361: train_loss -0.8736
2024-05-06 19:53:35.145867: val_loss -0.5851
2024-05-06 19:53:35.147214: Pseudo dice [0.7235]
2024-05-06 19:53:35.148706: Epoch time: 47.93 s
2024-05-06 19:53:38.123783: 
2024-05-06 19:53:38.127147: Epoch 80
2024-05-06 19:53:38.128802: Current learning rate: 0.00928
2024-05-06 19:54:25.929889: Validation loss did not improve from -0.69615. Patience: 41/50
2024-05-06 19:54:25.932394: train_loss -0.878
2024-05-06 19:54:25.933486: val_loss -0.6222
2024-05-06 19:54:25.934674: Pseudo dice [0.7462]
2024-05-06 19:54:25.935818: Epoch time: 47.81 s
2024-05-06 19:54:27.279767: 
2024-05-06 19:54:27.282546: Epoch 81
2024-05-06 19:54:27.283927: Current learning rate: 0.00927
2024-05-06 19:55:14.991609: Validation loss did not improve from -0.69615. Patience: 42/50
2024-05-06 19:55:14.994679: train_loss -0.8827
2024-05-06 19:55:14.995999: val_loss -0.6791
2024-05-06 19:55:14.997139: Pseudo dice [0.7831]
2024-05-06 19:55:14.998309: Epoch time: 47.72 s
2024-05-06 19:55:16.345277: 
2024-05-06 19:55:16.348237: Epoch 82
2024-05-06 19:55:16.350191: Current learning rate: 0.00926
2024-05-06 19:56:04.124491: Validation loss did not improve from -0.69615. Patience: 43/50
2024-05-06 19:56:04.127372: train_loss -0.8819
2024-05-06 19:56:04.128650: val_loss -0.6329
2024-05-06 19:56:04.129742: Pseudo dice [0.749]
2024-05-06 19:56:04.130822: Epoch time: 47.78 s
2024-05-06 19:56:05.383712: 
2024-05-06 19:56:05.387206: Epoch 83
2024-05-06 19:56:05.389006: Current learning rate: 0.00925
2024-05-06 19:56:53.147193: Validation loss did not improve from -0.69615. Patience: 44/50
2024-05-06 19:56:53.150127: train_loss -0.8811
2024-05-06 19:56:53.151370: val_loss -0.6637
2024-05-06 19:56:53.152813: Pseudo dice [0.7719]
2024-05-06 19:56:53.154001: Epoch time: 47.77 s
2024-05-06 19:56:54.367337: 
2024-05-06 19:56:54.370340: Epoch 84
2024-05-06 19:56:54.371470: Current learning rate: 0.00924
2024-05-06 19:57:42.076133: Validation loss did not improve from -0.69615. Patience: 45/50
2024-05-06 19:57:42.078950: train_loss -0.888
2024-05-06 19:57:42.080128: val_loss -0.6371
2024-05-06 19:57:42.081269: Pseudo dice [0.759]
2024-05-06 19:57:42.082404: Epoch time: 47.71 s
2024-05-06 19:57:43.820087: 
2024-05-06 19:57:43.822578: Epoch 85
2024-05-06 19:57:43.823653: Current learning rate: 0.00923
2024-05-06 19:58:31.469166: Validation loss did not improve from -0.69615. Patience: 46/50
2024-05-06 19:58:31.472213: train_loss -0.8861
2024-05-06 19:58:31.473667: val_loss -0.6647
2024-05-06 19:58:31.475102: Pseudo dice [0.7782]
2024-05-06 19:58:31.476486: Epoch time: 47.65 s
2024-05-06 19:58:32.687812: 
2024-05-06 19:58:32.691210: Epoch 86
2024-05-06 19:58:32.692928: Current learning rate: 0.00922
2024-05-06 19:59:20.402327: Validation loss did not improve from -0.69615. Patience: 47/50
2024-05-06 19:59:20.406227: train_loss -0.8841
2024-05-06 19:59:20.407529: val_loss -0.6014
2024-05-06 19:59:20.409019: Pseudo dice [0.7438]
2024-05-06 19:59:20.410186: Epoch time: 47.72 s
2024-05-06 19:59:21.663994: 
2024-05-06 19:59:21.668216: Epoch 87
2024-05-06 19:59:21.669763: Current learning rate: 0.00921
2024-05-06 20:00:09.503688: Validation loss did not improve from -0.69615. Patience: 48/50
2024-05-06 20:00:09.506400: train_loss -0.8611
2024-05-06 20:00:09.507664: val_loss -0.6422
2024-05-06 20:00:09.508835: Pseudo dice [0.7622]
2024-05-06 20:00:09.509938: Epoch time: 47.84 s
2024-05-06 20:00:10.753115: 
2024-05-06 20:00:10.756157: Epoch 88
2024-05-06 20:00:10.757181: Current learning rate: 0.0092
2024-05-06 20:00:58.538256: Validation loss did not improve from -0.69615. Patience: 49/50
2024-05-06 20:00:58.543024: train_loss -0.8728
2024-05-06 20:00:58.544997: val_loss -0.6359
2024-05-06 20:00:58.546078: Pseudo dice [0.7564]
2024-05-06 20:00:58.547095: Epoch time: 47.79 s
2024-05-06 20:00:59.789854: 
2024-05-06 20:00:59.793423: Epoch 89
2024-05-06 20:00:59.794634: Current learning rate: 0.0092
2024-05-06 20:01:47.588729: Validation loss did not improve from -0.69615. Patience: 50/50
2024-05-06 20:01:47.592376: train_loss -0.8836
2024-05-06 20:01:47.593956: val_loss -0.67
2024-05-06 20:01:47.594929: Pseudo dice [0.7715]
2024-05-06 20:01:47.595959: Epoch time: 47.8 s
2024-05-06 20:01:49.296337: Patience reached. Stopping training.
2024-05-06 20:01:49.804178: Training done.
2024-05-06 20:01:50.448705: Using splits from existing split file: /home/gridsan/nchutisilp/datasets/nnUNet_Datasets/nnUNet_preprocessed/Dataset302_Calcium_OCTv2/splits_final.json
2024-05-06 20:01:50.470068: The split file contains 3 splits.
2024-05-06 20:01:50.471553: Desired fold for training: 1
2024-05-06 20:01:50.472645: This split has 4 training and 2 validation cases.
2024-05-06 20:01:50.473897: predicting 101-044
2024-05-06 20:01:50.535692: 101-044, shape torch.Size([1, 404, 498, 498]), rank 0
2024-05-06 20:02:46.974343: predicting 106-002
2024-05-06 20:02:47.011218: 106-002, shape torch.Size([1, 539, 498, 498]), rank 0
2024-05-06 20:06:09.427037: Validation complete
2024-05-06 20:06:09.429271: Mean Validation Dice:  0.7399443917630235
wandb: 
wandb: Run history:
wandb:            ema_fg_dice ▁▁▁▂▃▃▃▄▅▆▆▆▆▆▆▆▇▆▇▇▇▇▇█████████▇█▇▇▇▇▇▇
wandb:   epoch_end_timestamps ▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███
wandb: epoch_start_timestamps ▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇████
wandb:                    lrs ████▇▇▇▇▇▆▆▆▆▆▆▅▅▅▅▅▄▄▄▄▄▄▃▃▃▃▃▂▂▂▂▂▂▁▁▁
wandb:           mean_fg_dice ▁▁▂▂▅▂▅▇▇▆▄▄▇▆▃▇▅▆▅▆▆▇██▇▆█▇▇▆▆▅▆▇▅▂▄▅▄▆
wandb:           train_losses █▄▄▄▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:             val_losses ██▇▆▃█▄▂▁▂▄▅▁▄▆▂▄▃▅▃▃▂▁▁▁▃▁▁▂▃▃▄▃▂▅█▅▄▇▂
wandb: 
wandb: Run summary:
wandb:            ema_fg_dice 0.76238
wandb:   epoch_end_timestamps 1715040107.59216
wandb: epoch_start_timestamps 1715040059.78774
wandb:                    lrs 0.0092
wandb:           mean_fg_dice 0.77155
wandb:           train_losses -0.88357
wandb:             val_losses -0.66999
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/gridsan/nchutisilp/datasets/nnUNet_Datasets/nnUNet_results/Dataset302_Calcium_OCTv2/nnUNetTrainer__nnUNetPlans__2d/fold_1/wandb/offline-run-20240506_184527-m7qwv1u3
wandb: Find logs at: /home/gridsan/nchutisilp/datasets/nnUNet_Datasets/nnUNet_results/Dataset302_Calcium_OCTv2/nnUNetTrainer__nnUNetPlans__2d/fold_1/wandb/offline-run-20240506_184527-m7qwv1u3/logs
WARNING:urllib3.connectionpool:Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x7fd3e44840a0>: Failed to establish a new connection: [Errno 101] Network is unreachable')': /api/4504800232407040/envelope/
WARNING:urllib3.connectionpool:Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x7fd31daef490>: Failed to establish a new connection: [Errno 101] Network is unreachable')': /api/4504800232407040/envelope/
WARNING:urllib3.connectionpool:Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x7fd3e6ab05e0>: Failed to establish a new connection: [Errno 101] Network is unreachable')': /api/4504800232407040/envelope/
WARNING:urllib3.connectionpool:Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x7fd3b43bb1c0>: Failed to establish a new connection: [Errno 101] Network is unreachable')': /api/4504800232407040/envelope/
WARNING:urllib3.connectionpool:Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x7fd3ad2bed60>: Failed to establish a new connection: [Errno 101] Network is unreachable')': /api/4504800232407040/envelope/
WARNING:urllib3.connectionpool:Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x7fd321f3bbb0>: Failed to establish a new connection: [Errno 101] Network is unreachable')': /api/4504800232407040/envelope/
