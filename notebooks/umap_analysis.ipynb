{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6d761e6b",
   "metadata": {},
   "source": [
    "# UMAP analysis of patch embeddings\n",
    "\n",
    "This notebook computes 2D UMAP embeddings for a set of 3D patches across multiple pretrained feature models.\n",
    "\n",
    "Usage:\n",
    "- Fill `PATCH_LIST` with tuples `(case_filename, (d,h,w))` and set `PRETRAINED_MODELS` to paths or model folders to evaluate.\n",
    "- Run the cells top-to-bottom. The notebook loads each model sequentially to conserve GPU memory.\n",
    "- Outputs: one UMAP scatter per model (over patches) and a grid of patch middle-slices with a legend of model colors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe758bf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports and configuration\n",
    "import sys\n",
    "import json\n",
    "import subprocess\n",
    "import importlib\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.offsetbox import OffsetImage, AnnotationBbox\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# try import umap-learn\n",
    "try:\n",
    "    import umap\n",
    "except Exception:\n",
    "    print('umap not found, attempting to install umap-learn...')\n",
    "    subprocess.check_call([sys.executable, '-m', 'pip', 'install', 'umap-learn'])\n",
    "    importlib.invalidate_caches()\n",
    "    import umap\n",
    "\n",
    "print('torch:', torch.__version__)\n",
    "print('umap:', umap.__version__)\n",
    "\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Device:', DEVICE)\n",
    "\n",
    "# Defaults\n",
    "DEFAULT_PATCH_SIZE = (32, 160, 128)  # (D,H,W)\n",
    "ROOT_DIR = Path('/nfs/erelab001/shared/Computational_Group/Naravich')\n",
    "DATASETS_DIR = ROOT_DIR / 'datasets' / 'nnUNet_Datasets'\n",
    "IMAGES_DIR = DATASETS_DIR / 'nnUNet_preprocessed'/ 'Dataset307_Sohee_Calcium_OCT_CrossValidation' / 'nnUNetPlans_3d_fullres'\n",
    "plans_file = DATASETS_DIR / \"nnUNet_preprocessed/Dataset307_Sohee_Calcium_OCT_CrossValidation/nnUNetPlans.json\"\n",
    "dataset_file = DATASETS_DIR / \"nnUNet_preprocessed/Dataset307_Sohee_Calcium_OCT_CrossValidation/dataset.json\"\n",
    "with open(dataset_file) as f:\n",
    "    dataset_json = json.load(f)\n",
    "\n",
    "# Example: user will override these two lists when using the notebook\n",
    "PATCH_LIST = [\n",
    "    ('101-019.npy', (162,220,155)),\n",
    "    ('101-019.npy', (147,170,150)),\n",
    "    ('101-019.npy', (170,270,135)),\n",
    "    ('101-019.npy', (147,80,290)),\n",
    "    ('101-019.npy', (171,242,288)),\n",
    "    ('101-019.npy', (171,299,172)),\n",
    "    ('101-019.npy', (171,122,132)),\n",
    "    ('101-019.npy', (160,114,310)),\n",
    "    ('101-019.npy', (153-16,172,337)),\n",
    "    ('101-019.npy', (153-16,345,224)),\n",
    "    ('101-019.npy', (149-16,348,242)),\n",
    "    ('101-019.npy', (149-16,194,183)),\n",
    "    ('101-019.npy', (143-16,207,199)),\n",
    "    ('101-019.npy', (108-16,300,300)),\n",
    "    ('101-019.npy', (108-16,176,234)),\n",
    "    ('101-019.npy', (96-16,149,212)),\n",
    "    ('101-019.npy', (81-16,122,215)),\n",
    "    ('101-019.npy', (81-16,190,248)),\n",
    "\n",
    "]\n",
    "PRETRAINED_MODELS = [\n",
    "    # LaW\n",
    "    (DATASETS_DIR / \"nnUNet_results/Dataset300_Lumen_and_Wall_OCT/nnUNetTrainer__nnUNetPreprocessPlans__3d_fullres/fold_all/checkpoint_best.pth\", \"LaW\"),\n",
    "    # Genesis\n",
    "    (ROOT_DIR / \"datasets\" / \"ModelGenesisOutputs/ModelGenesisNNUNetPretrainingV2_noNorm_correct_orientation/Converted_nnUNet_Genesis_OCT_Best.pt\", \"Genesis\"),\n",
    "    # CLIP\n",
    "    (DATASETS_DIR / \"../OpenAI-CLIP-logs/shared_projector_shared_encoder/nnunet.pt\", \"CLIP\"),\n",
    "    # No Pretrain\n",
    "    (Path(\"None\"), \"No Pretrain\"),\n",
    "    # str(path_to_pretrained_checkpoint_or_model_folder),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3504f4b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions: loading image, extracting patch, and initializing predictor\n",
    "import nibabel as nib\n",
    "from nnunetv2.inference.predict_from_raw_data import nnUNetPredictor\n",
    "from nnunetv2.utilities.get_network_from_plans import get_network_from_plans\n",
    "from nnunetv2.run.load_pretrained_weights import load_pretrained_weights\n",
    "from nnunetv2.utilities.plans_handling.plans_handler import PlansManager\n",
    "from nnunetv2.utilities.label_handling.label_handling import determine_num_input_channels\n",
    "import torch\n",
    "\n",
    "def find_case_file(image_dir: Path, case_name: str):\n",
    "    p = Path(image_dir) / case_name\n",
    "    if p.exists():\n",
    "        return p\n",
    "    # try to find by pattern\n",
    "    files = list(p.parent.rglob(case_name)) if p.parent.exists() else []\n",
    "    if files:\n",
    "        return files[0]\n",
    "    raise FileNotFoundError(f'Case file {case_name} not found under {image_dir}')\n",
    "\n",
    "def load_case_array(case_file: Path):\n",
    "    if case_file.suffix == '.npz':\n",
    "        arr = np.load(case_file)\n",
    "        # heuristics for key\n",
    "        for k in ('data','x','arr_0','image','images'):\n",
    "            if k in arr:\n",
    "                return arr[k]  # may be C,D,H,W or D,H,W\n",
    "        return arr[list(arr.files)[0]]\n",
    "    elif case_file.suffix == '.npy':\n",
    "        return np.load(case_file)\n",
    "    else:\n",
    "        img = nib.load(str(case_file))\n",
    "        return img.get_fdata()\n",
    "\n",
    "def ensure_4d_CDHW(x: np.ndarray):\n",
    "    # convert to (C,D,H,W)\n",
    "    if x.ndim == 3:\n",
    "        return np.expand_dims(x, 0)\n",
    "    if x.ndim == 4:\n",
    "        # ambiguous: could be (B,C,D,H,W) or (C,D,H,W) - we assume (C,D,H,W)\n",
    "        return x\n",
    "    if x.ndim == 5:\n",
    "        return x[0]\n",
    "    raise ValueError(f'Unexpected image array shape: {x.shape}')\n",
    "\n",
    "def extract_patch_from_array(arr: np.ndarray, offset: tuple, patch_size: tuple):\n",
    "    # arr: (C,D,H,W)\n",
    "    C, D, H, W = arr.shape\n",
    "    d0 = int(np.clip(offset[0], 0, D - patch_size[0]))\n",
    "    h0 = int(np.clip(offset[1], 0, H - patch_size[1]))\n",
    "    w0 = int(np.clip(offset[2], 0, W - patch_size[2]))\n",
    "    d1, h1, w1 = d0 + patch_size[0], h0 + patch_size[1], w0 + patch_size[2]\n",
    "    patch = arr[:, d0:d1, h0:h1, w0:w1].copy()\n",
    "    return patch, (d0, h0, w0)\n",
    "\n",
    "def initialize_untrained_predictor(predictor, plans_file):\n",
    "    \"\"\"Initialize predictor with an untrained model based on plans file.\n",
    "    def initialize_untrained_predictor(predictor, plans_file):\n",
    "    Args:\n",
    "        predictor: nnUNetPredictor instance\n",
    "        plans_file: Path to the plans.json file\n",
    "    \"\"\"\n",
    "    # Load network from plans\n",
    "    plans_file = Path(plans_file)\n",
    "    if not plans_file.exists():\n",
    "        raise FileNotFoundError(f\"Plans file not found: {plans_file}\")\n",
    "    \n",
    "    # Load and parse plans\n",
    "    with open(plans_file) as f:\n",
    "        plans = json.load(f)\n",
    "    \n",
    "    # Initialize managers\n",
    "    plans_manager = PlansManager(plans_file)\n",
    "    configuration = \"3d_32x160x128_b10\"\n",
    "    if configuration not in plans['configurations']:\n",
    "        configuration = list(plans['configurations'].keys())[0]\n",
    "    \n",
    "    configuration_manager = plans_manager.get_configuration(configuration)\n",
    "    label_manager = plans_manager.get_label_manager(dataset_json)\n",
    "    num_input_channels = determine_num_input_channels(plans_manager, configuration_manager,dataset_json)\n",
    "    # Get network parameters from configuration manager\n",
    "    network = get_network_from_plans(\n",
    "        arch_class_name=configuration_manager.network_arch_class_name,\n",
    "        arch_kwargs=configuration_manager.network_arch_init_kwargs,\n",
    "        arch_kwargs_req_import=configuration_manager.network_arch_init_kwargs_req_import,\n",
    "        input_channels=num_input_channels,\n",
    "        output_channels=label_manager.num_segmentation_heads,\n",
    "        allow_init=True,\n",
    "        deep_supervision=True\n",
    "    )\n",
    "    network.to(predictor.device)\n",
    "    \n",
    "    # Initialize predictor with untrained network\n",
    "    predictor.network = network\n",
    "    predictor.plans_manager = plans_manager\n",
    "    predictor.configuration_manager = configuration_manager\n",
    "    predictor.label_manager = label_manager\n",
    "    predictor.plans = plans\n",
    "    predictor.configuration_name = configuration\n",
    "    \n",
    "    print(f\"Initialized untrained network from {plans_file}\")\n",
    "    print(f\"Using configuration: {configuration}\")\n",
    "    print(f\"Architecture: {configuration_manager.network_arch_class_name}\")\n",
    "    print(f\"Input channels: {num_input_channels}, Output channels: {label_manager.num_segmentation_heads}\")\n",
    "    return predictor\n",
    "\n",
    "def extract_activation_vector(predictor, input_tensor: torch.Tensor, target_module_name: str = None) -> np.ndarray:\n",
    "    \"\"\"Run forward and capture a single pooled feature vector for the target module.\n",
    "    We perform global average pooling over spatial dims of the activation to get a (C,) vector.\n",
    "    \"\"\"\n",
    "    net = predictor.network.to(DEVICE)\n",
    "    activations = {}\n",
    "    hooks = []\n",
    "\n",
    "    def find_module_by_name(root, name):\n",
    "        if not name:\n",
    "            return None\n",
    "        cur = root\n",
    "        for p in name.split('.'):\n",
    "            if p.isdigit():\n",
    "                cur = cur[int(p)]\n",
    "            else:\n",
    "                cur = getattr(cur, p)\n",
    "        return cur\n",
    "\n",
    "    # select module: default to last encoder stage if available\n",
    "    if target_module_name is None and hasattr(net, 'encoder'):\n",
    "        enc = net.encoder\n",
    "        if hasattr(enc, 'stages') and len(enc.stages) > 0:\n",
    "            target = enc.stages[-1]\n",
    "            target_name = 'encoder.stages.-1'\n",
    "        else:\n",
    "            target = enc\n",
    "            target_name = 'encoder'\n",
    "    elif target_module_name is not None:\n",
    "        target = find_module_by_name(net, target_module_name)\n",
    "        target_name = target_module_name\n",
    "    else:\n",
    "        target = net\n",
    "        target_name = 'network'\n",
    "\n",
    "    def hook_fn(m, inp, out):\n",
    "        activations[target_name] = out.detach().cpu()\n",
    "\n",
    "    hooks.append(target.register_forward_hook(hook_fn))\n",
    "    was_training = net.training\n",
    "    net.eval()\n",
    "    with torch.no_grad():\n",
    "        x = input_tensor.to(DEVICE).float()\n",
    "        out = net(x)\n",
    "\n",
    "    for h in hooks:\n",
    "        h.remove()\n",
    "    if was_training:\n",
    "        net.train()\n",
    "\n",
    "    act = activations[target_name]  # [B,C,(D,),H,W] or [B,C,H,W]\n",
    "    a = act.cpu().numpy()\n",
    "    # global average pool spatial dims for first batch\n",
    "    if a.ndim == 5:\n",
    "        # B,C,D,H,W -> pool D,H,W -> C vector\n",
    "        v = a[0].mean(axis=(1,2,3))\n",
    "    elif a.ndim == 4:\n",
    "        # B,C,H,W -> pool H,W\n",
    "        v = a[0].mean(axis=(1,2))\n",
    "    else:\n",
    "        raise RuntimeError('Unexpected activation rank: ' + str(a.shape))\n",
    "    return v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16c89bd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main pipeline: compute embeddings for PATCH_LIST across PRETRAINED_MODELS and visualize\n",
    "import matplotlib.patches as patches\n",
    "from matplotlib import cm\n",
    "\n",
    "def compute_embeddings_for_models(patch_list, pretrained_models, image_root, patch_size=DEFAULT_PATCH_SIZE, target_module=None):\n",
    "    \"\"\"Returns: dict model_path -> dict with keys 'embeddings' (N x C), 'coords' (N x 2 after UMAP), 'patches' (list of patch arrays)\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "    for m_idx, model in enumerate(pretrained_models):\n",
    "        pretrained_weights_file, model_name = model\n",
    "        print(f'Processing model {m_idx+1}/{len(pretrained_models)}: {model}')\n",
    "        predictor = nnUNetPredictor(tile_step_size=0.5, use_gaussian=True, use_mirroring=False, perform_everything_on_device=True, device=DEVICE)\n",
    "        # Initialize predictor\n",
    "        initialize_untrained_predictor(predictor, plans_file)\n",
    "        if pretrained_weights_file.exists():\n",
    "            print('Loading pretrained weights from', pretrained_weights_file)\n",
    "            load_pretrained_weights(predictor.network, pretrained_weights_file)\n",
    "        embeddings = []\n",
    "        patches = []\n",
    "        for case_name, offset in patch_list:\n",
    "            case_file = find_case_file(image_root, case_name)\n",
    "            arr = load_case_array(case_file)\n",
    "            arr = ensure_4d_CDHW(arr)\n",
    "            patch, actual_offset = extract_patch_from_array(arr, offset, patch_size)\n",
    "            patches.append(patch)\n",
    "            x_in = np.expand_dims(patch, 0)  # (1,C,D,H,W)\n",
    "            t = torch.from_numpy(x_in).float()\n",
    "            try:\n",
    "                v = extract_activation_vector(predictor, t, target_module_name=target_module)\n",
    "            except Exception as e:\n",
    "                print('Failed to extract activation vector for', case_name, offset, ':', e)\n",
    "                v = np.zeros(64)  # fallback vector\n",
    "            embeddings.append(v)\n",
    "        embeddings = np.stack(embeddings, axis=0)\n",
    "        # # reduce dimensionality (PCA) to speed up UMAP if necessary\n",
    "        # if embeddings.shape[1] > 64:\n",
    "        #     print(embeddings.shape[1], 'dimensions too high, reducing to 64 with PCA before UMAP', embeddings.shape)\n",
    "        #     p = PCA(n_components=min(64, embeddings.shape[1]))\n",
    "        #     emb_p = p.fit_transform(embeddings)\n",
    "        # else:\n",
    "        #     emb_p = embeddings\n",
    "        # run UMAP to 2D\n",
    "        emb_p = embeddings\n",
    "        reducer = PCA(n_components=2)\n",
    "        # reducer = umap.UMAP(n_components=2, random_state=0)\n",
    "        coords = reducer.fit_transform(emb_p)\n",
    "        results[str(model_name)] = {'embeddings': embeddings, 'coords': coords, 'patches': patches}\n",
    "        # free GPU memory explicitly\n",
    "        del predictor\n",
    "        torch.cuda.empty_cache()\n",
    "    return results\n",
    "\n",
    "def plot_results(results, patch_list, figsize=(12,10), mode_viz=None):\n",
    "    if mode_viz is not None:\n",
    "        models = [mode_viz]\n",
    "        print('Only model', models)\n",
    "    else:\n",
    "        models = list(results.keys())\n",
    "    # assign colors per model\n",
    "    # models = list(results.keys())\n",
    "    cmap = cm.get_cmap('tab10')\n",
    "    colors = {m: cmap(i % 10) for i,m in enumerate(models)}\n",
    "    \n",
    "    # scatter per model with embedded thumbnails\n",
    "    fig, ax = plt.subplots(figsize=figsize)\n",
    "    for m_idx, m in enumerate(models):\n",
    "\n",
    "        coords = results[m]['coords']\n",
    "        patches_list = results[m]['patches']\n",
    "\n",
    "        ax.scatter(coords[:,0], coords[:,1], alpha=0)  # invisible but triggers autoscale\n",
    "\n",
    "        for i, (x, y) in enumerate(coords):\n",
    "            # extract middle slice thumbnail\n",
    "            patch = patches_list[i]\n",
    "            mid = patch.shape[1] // 2\n",
    "            thumb = patch[0, mid]\n",
    "            \n",
    "            # normalize thumbnail for display\n",
    "            thumb_norm = (thumb - thumb.min()) / (thumb.max() - thumb.min() + 1e-6)\n",
    "            \n",
    "            # create RGB image with colored border\n",
    "            thumb_rgb = np.stack([thumb_norm]*3, axis=-1)\n",
    "            border_color = np.array(colors[m][:3])\n",
    "            \n",
    "            # add colored border (5 pixels)\n",
    "            border_width = 5\n",
    "            h, w = thumb_rgb.shape[:2]\n",
    "            thumb_bordered = np.ones((h + 2*border_width, w + 2*border_width, 3))\n",
    "            thumb_bordered[:] = border_color\n",
    "            thumb_bordered[border_width:-border_width, border_width:-border_width] = thumb_rgb\n",
    "            \n",
    "            # embed as annotation\n",
    "            imagebox = OffsetImage(thumb_bordered, zoom=0.3, cmap='gray')\n",
    "            ab = AnnotationBbox(imagebox, (x, y), frameon=False)\n",
    "            ax.add_artist(ab)\n",
    "    \n",
    "    # add legend\n",
    "    from matplotlib.lines import Line2D\n",
    "    legend_elements = [Line2D([0], [0], marker='s', color='w', markerfacecolor=colors[m], \n",
    "                              markersize=10, label=m) for m in models]\n",
    "    ax.legend(handles=legend_elements, fontsize=10, loc='upper right')\n",
    "    \n",
    "    ax.set_xlabel('PCA 1')\n",
    "    ax.set_ylabel('PCA 2')\n",
    "    ax.set_title('PCA embeddings with patch thumbnails (border color = model)')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "863c6d05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def plot_results(results, patch_list, figsize=(10,8)):\n",
    "#     # assign colors per model\n",
    "#     models = list(results.keys())\n",
    "#     cmap = cm.get_cmap('tab10')\n",
    "#     colors = {m: cmap(i % 10) for i,m in enumerate(models)}\n",
    "#     # scatter per model (each has N points)\n",
    "#     plt.figure(figsize=figsize)\n",
    "#     for m in models:\n",
    "#         coords = results[m]['coords']\n",
    "#         plt.scatter(coords[:,0], coords[:,1], label=m, color=colors[m], s=40)\n",
    "#         # annotate with patch index to link with thumbnails\n",
    "#         for i,(x,y) in enumerate(coords):\n",
    "#             plt.text(x, y, str(i), color='k', fontsize=8)\n",
    "#     plt.legend(fontsize=8)\n",
    "#     plt.title('UMAP embeddings (one point per patch)')\n",
    "#     plt.show()\n",
    "\n",
    "#     # show thumbnails in a grid and a color bar showing model colors\n",
    "#     N = len(patch_list)\n",
    "#     ncols = min(6, N)\n",
    "#     nrows = int(np.ceil(N / ncols))\n",
    "#     fig, axs = plt.subplots(nrows, ncols, figsize=(3*ncols, 3*nrows))\n",
    "#     axs = np.atleast_2d(axs).reshape(nrows, ncols)\n",
    "#     for i, (case_name, offset) in enumerate(patch_list):\n",
    "#         r = i // ncols\n",
    "#         c = i % ncols\n",
    "#         ax = axs[r,c]\n",
    "#         patch = results[list(results.keys())[0]]['patches'][i]  # pick first model's patch (same patches)\n",
    "#         # show middle depth slice\n",
    "#         mid = patch.shape[1] // 2\n",
    "#         ax.imshow(patch[0, mid], cmap='gray')\n",
    "#         ax.axis('off')\n",
    "#         ax.set_title(f'{i}: {case_name} offset={offset}')\n",
    "#         # draw small color legend squares for each model under the thumbnail\n",
    "#         for j,m in enumerate(models):\n",
    "#             col = colors[m]\n",
    "#             rect = patches.Rectangle((0.02 + 0.08*j, 0.02), 0.06, 0.06, transform=ax.transAxes, facecolor=col, edgecolor='k')\n",
    "#             ax.add_patch(rect)\n",
    "#     # hide empty axes\n",
    "#     for i in range(N, nrows*ncols):\n",
    "#         r = i // ncols\n",
    "#         c = i % ncols\n",
    "#         axs[r,c].axis('off')\n",
    "#     plt.suptitle('Patch middle-slices with model color keys (squares)')\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9c77cb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dc15b21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example run: user should populate PATCH_LIST and PRETRAINED_MODELS above, then run this cell.\n",
    "# Example values (uncomment and edit):\n",
    "# PATCH_LIST = [('101-044_0000.nii.gz', (140,150,128)), ('101-045_0000.nii.gz', (120,130,100))]\n",
    "# PRETRAINED_MODELS = ['/path/to/modelA', '/path/to/modelB']\n",
    "\n",
    "if len(PATCH_LIST) == 0:\n",
    "    print('No patches provided: set PATCH_LIST and rerun the cell')\n",
    "else:\n",
    "    results = compute_embeddings_for_models(PATCH_LIST, PRETRAINED_MODELS, IMAGES_DIR, patch_size=DEFAULT_PATCH_SIZE)\n",
    "    plot_results(results, PATCH_LIST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b44eaeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save results to file (optional)\n",
    "import pickle\n",
    "with open('umap_results.pkl', 'wb') as f:\n",
    "    pickle.dump(results, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96748e45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load results from file (optional)\n",
    "import pickle\n",
    "with open('umap_results.pkl', 'rb') as f:\n",
    "    results = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "110daf40",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_results(results, PATCH_LIST, mode_viz=\"No Pretrain\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e247c53",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
