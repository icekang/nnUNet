{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ffeecee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from nnunetv2.inference.predict_from_raw_data import nnUNetPredictor\n",
    "from nnunetv2.utilities.get_network_from_plans import get_network_from_plans\n",
    "from nnunetv2.run.load_pretrained_weights import load_pretrained_weights\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af8df0d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "# Define root directory and dataset paths\n",
    "ROOT_DIR = Path(\"/nfs/erelab001/shared/Computational_Group/Naravich\")\n",
    "datasets = ROOT_DIR / \"datasets\" / \"nnUNet_Datasets\"\n",
    "\n",
    "plans_file = datasets / \"nnUNet_preprocessed/Dataset307_Sohee_Calcium_OCT_CrossValidation/nnUNetPlans.json\"\n",
    "dataset_file = datasets / \"nnUNet_preprocessed/Dataset307_Sohee_Calcium_OCT_CrossValidation/dataset.json\"\n",
    "with open(dataset_file) as f:\n",
    "    dataset_json = json.load(f)\n",
    "\n",
    "# Define model path\n",
    "model_path = None # No pretrained?\n",
    "# model_path = datasets / \"nnUNet_results/Dataset300_Lumen_and_Wall_OCT/nnUNetTrainer__nnUNetPreprocessPlans__3d_fullres\"\n",
    "# model checkpoint is in fold_all/checkpoint_best.pth\n",
    "\n",
    "# LaW\n",
    "pretrained_weights_file = datasets / \"nnUNet_results/Dataset300_Lumen_and_Wall_OCT/nnUNetTrainer__nnUNetPreprocessPlans__3d_fullres/fold_all/checkpoint_best.pth\"\n",
    "# Genesis\n",
    "# pretrained_weights_file = ROOT_DIR / \"datasets\" / \"ModelGenesisOutputs/ModelGenesisNNUNetPretrainingV2_noNorm_correct_orientation/Converted_nnUNet_Genesis_OCT_Best.pt\"\n",
    "\n",
    "# Define images path\n",
    "image_path = datasets / \"nnUNet_preprocessed/Dataset307_Sohee_Calcium_OCT_CrossValidation/nnUNetPlans_3d_fullres\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab6f98fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_weights_file.exists()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a39a14a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from nnunetv2.inference.predict_from_raw_data import nnUNetPredictor\n",
    "from nnunetv2.utilities.get_network_from_plans import get_network_from_plans\n",
    "from nnunetv2.utilities.plans_handling.plans_handler import PlansManager, ConfigurationManager\n",
    "from nnunetv2.utilities.label_handling.label_handling import LabelManager, determine_num_input_channels\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "import json\n",
    "\n",
    "def initialize_untrained_predictor(predictor, plans_file):\n",
    "    \"\"\"Initialize predictor with an untrained model based on plans file.\n",
    "    \n",
    "    Args:\n",
    "        predictor: nnUNetPredictor instance\n",
    "        plans_file: Path to the plans.json file\n",
    "    \"\"\"\n",
    "    # Load network from plans\n",
    "    plans_file = Path(plans_file)\n",
    "    if not plans_file.exists():\n",
    "        raise FileNotFoundError(f\"Plans file not found: {plans_file}\")\n",
    "    \n",
    "    # Load and parse plans\n",
    "    with open(plans_file) as f:\n",
    "        plans = json.load(f)\n",
    "    \n",
    "    # Initialize managers\n",
    "    plans_manager = PlansManager(plans_file)\n",
    "    configuration = \"3d_32x160x128_b10\"\n",
    "    if configuration not in plans['configurations']:\n",
    "        configuration = list(plans['configurations'].keys())[0]\n",
    "    \n",
    "    configuration_manager = plans_manager.get_configuration(configuration)\n",
    "    label_manager = plans_manager.get_label_manager(dataset_json)\n",
    "    num_input_channels = determine_num_input_channels(plans_manager, configuration_manager,dataset_json)\n",
    "    # Get network parameters from configuration manager\n",
    "    network = get_network_from_plans(\n",
    "        arch_class_name=configuration_manager.network_arch_class_name,\n",
    "        arch_kwargs=configuration_manager.network_arch_init_kwargs,\n",
    "        arch_kwargs_req_import=configuration_manager.network_arch_init_kwargs_req_import,\n",
    "        input_channels=num_input_channels,\n",
    "        output_channels=label_manager.num_segmentation_heads,\n",
    "        allow_init=True,\n",
    "        deep_supervision=True\n",
    "    )\n",
    "    network.to(predictor.device)\n",
    "    \n",
    "    # Initialize predictor with untrained network\n",
    "    predictor.network = network\n",
    "    predictor.plans_manager = plans_manager\n",
    "    predictor.configuration_manager = configuration_manager\n",
    "    predictor.label_manager = label_manager\n",
    "    predictor.plans = plans\n",
    "    predictor.configuration_name = configuration\n",
    "    \n",
    "    print(f\"Initialized untrained network from {plans_file}\")\n",
    "    print(f\"Using configuration: {configuration}\")\n",
    "    print(f\"Architecture: {configuration_manager.network_arch_class_name}\")\n",
    "    print(f\"Input channels: {num_input_channels}, Output channels: {label_manager.num_segmentation_heads}\")\n",
    "    return predictor\n",
    "\n",
    "# Initialize predictor\n",
    "predictor = nnUNetPredictor(\n",
    "    tile_step_size=0.5,\n",
    "    use_gaussian=True,\n",
    "    use_mirroring=True,\n",
    "    perform_everything_on_device=True,\n",
    "    device=torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "607634e1",
   "metadata": {},
   "source": [
    "# nnUNet embedding visualization (PCA + UMAP)\n",
    "\n",
    "This notebook adds utilities to extract intermediate embeddings (feature maps) from a trained nnUNet model and visualize them using PCA and UMAP.\n",
    "\n",
    "How to use:\n",
    "- Ensure `predictor`, `model_path`, and `image_path` variables are set (they are defined above in this notebook).\n",
    "- Run the imports/install cell (Cell N+1).\n",
    "- Run the helper functions cell (Cell N+2).\n",
    "- Update the example run cell with a case filename if necessary and run it (Cell N+3).\n",
    "\n",
    "Notes:\n",
    "- The extraction uses a forward hook on a selected module (by default the last encoder stage / bottleneck). You can change the `target_module_name` in the example to capture other layers.\n",
    "- We sample voxels for UMAP to keep computation light; increase `n_samples` if you want more points.\n",
    "- If your data uses different key names (e.g., `.npz` with `data`/`arr_0`), adjust the loading step accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c62548f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports and installs (will try to install umap-learn if missing)\n",
    "import sys\n",
    "import subprocess\n",
    "import importlib\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Try import umap, install if missing\n",
    "try:\n",
    "    import umap\n",
    "except Exception:\n",
    "    print('umap not found, attempting to install umap-learn...')\n",
    "    subprocess.check_call([sys.executable, '-m', 'pip', 'install', 'umap-learn'])\n",
    "    importlib.invalidate_caches()\n",
    "    import umap\n",
    "\n",
    "print('torch:', torch.__version__)\n",
    "print('umap:', umap.__version__)\n",
    "print('sklearn:', importlib.import_module('sklearn').__version__)\n",
    "\n",
    "# helper - device\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Device:', DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e469bd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions: load model into predictor, extract activations via hook, flatten embeddings\n",
    "from typing import Optional, Tuple, Dict\n",
    "\n",
    "def initialize_predictor_from_folder(predictor: nnUNetPredictor, model_folder: Path, checkpoint_name: Optional[str] = None):\n",
    "    \"\"\"Try to initialize the predictor from a trained model folder.\n",
    "    Falls back to searching for checkpoint files if `initialize_from_trained_model_folder` is not available.\n",
    "    \"\"\"\n",
    "    model_folder = Path(model_folder)\n",
    "    # Prefer built-in initializer if available\n",
    "    try:\n",
    "        predictor.initialize_from_trained_model_folder(str(model_folder), use_folds=\"all\", checkpoint_name=checkpoint_name)\n",
    "        print('Initialized predictor via initialize_from_trained_model_folder')\n",
    "        return\n",
    "    except Exception as e:\n",
    "        print('initialize_from_trained_model_folder not available or failed, trying to load checkpoint directly:', e)\n",
    "\n",
    "    # Fallback: find a checkpoint file\n",
    "    candidates = list(model_folder.rglob('checkpoint*.pth')) + list(model_folder.rglob('*.pth'))\n",
    "    if not candidates:\n",
    "        raise FileNotFoundError(f'No checkpoint found under {model_folder}')\n",
    "    # choose the most recent candidate\n",
    "    ckpt = sorted(candidates)[-1]\n",
    "    ckpt = str(ckpt)\n",
    "    print('Loading checkpoint:', ckpt)\n",
    "    state = torch.load(ckpt, map_location=DEVICE)\n",
    "    # handle different save formats\n",
    "    if isinstance(state, dict) and 'state_dict' in state:\n",
    "        sd = state['state_dict']\n",
    "    elif isinstance(state, dict) and any(k.startswith('network') or k.startswith('model') for k in state.keys()):\n",
    "        # try to find direct network dict\n",
    "        sd = None\n",
    "        for k in ['network', 'model', 'state_dict']:\n",
    "            if k in state:\n",
    "                sd = state[k]\n",
    "                break\n",
    "        if sd is None:\n",
    "            # maybe the state is the state_dict itself\n",
    "            sd = state\n",
    "    else:\n",
    "        sd = state\n",
    "    try:\n",
    "        predictor.network.load_state_dict(sd)\n",
    "    except Exception as e:\n",
    "        print('Failed to load state_dict directly into predictor.network:', e)\n",
    "        # Try to handle prefixed keys\n",
    "        new_sd = {}\n",
    "        for k,v in sd.items():\n",
    "            new_key = k\n",
    "            if k.startswith('module.'):\n",
    "                new_key = k[len('module.'):]\n",
    "            new_sd[new_key] = v\n",
    "        predictor.network.load_state_dict(new_sd)\n",
    "    predictor.network.to(DEVICE)\n",
    "    predictor.device = DEVICE\n",
    "    predictor.network.eval()\n",
    "    print('Checkpoint loaded and network set to eval on', DEVICE)\n",
    "\n",
    "\n",
    "def extract_activations(predictor, input_tensor: torch.Tensor, target_module_name: Optional[str] = None) -> Tuple[Dict[str, torch.Tensor], torch.Tensor]:\n",
    "    \"\"\"Run forward and capture activations from the specified module.\n",
    "\n",
    "    Returns (activations, output_prediction)\n",
    "    - activations: dict name -> tensor (B,C,...) on CPU\n",
    "    - output_prediction: raw network output (if returned by forward)\n",
    "    \"\"\"\n",
    "    net = predictor.network\n",
    "    net = net.to(DEVICE)\n",
    "    activations = {}\n",
    "    hooks = []\n",
    "\n",
    "    # Decide which module to hook\n",
    "    def find_module_by_name(root, name):\n",
    "        # name may be like 'encoder.stages.4' or simple 'encoder'\n",
    "        if not name:\n",
    "            return None\n",
    "        parts = name.split('.')\n",
    "        cur = root\n",
    "        try:\n",
    "            for p in parts:\n",
    "                if p.isdigit():\n",
    "                    cur = cur[int(p)]\n",
    "                else:\n",
    "                    cur = getattr(cur, p)\n",
    "            return cur\n",
    "        except Exception:\n",
    "            return None\n",
    "\n",
    "    if target_module_name is None:\n",
    "        # attempt to find a bottleneck or last encoder stage\n",
    "        candidates = []\n",
    "        if hasattr(net, 'encoder'):\n",
    "            enc = net.encoder\n",
    "            # common attribute names\n",
    "            if hasattr(enc, 'stages'):\n",
    "                try:\n",
    "                    candidates.append(enc.stages[-1])\n",
    "                    target_module = enc.stages[-1]\n",
    "                    target_name = 'encoder.stages.-1'\n",
    "                except Exception:\n",
    "                    target_module = enc\n",
    "                    target_name = 'encoder'\n",
    "            else:\n",
    "                target_module = enc\n",
    "                target_name = 'encoder'\n",
    "        else:\n",
    "            # fallback to entire network\n",
    "            target_module = net\n",
    "            target_name = 'network'\n",
    "    else:\n",
    "        m = find_module_by_name(net, target_module_name)\n",
    "        if m is None:\n",
    "            raise ValueError(f'Could not find module by name {target_module_name}')\n",
    "        target_module = m\n",
    "        target_name = target_module_name\n",
    "\n",
    "    # register forward hook\n",
    "    def hook_fn(module, inp, out):\n",
    "        # detach and move to cpu\n",
    "        activations[target_name] = out.detach().cpu()\n",
    "\n",
    "    hooks.append(target_module.register_forward_hook(hook_fn))\n",
    "\n",
    "    # run forward\n",
    "    was_training = net.training\n",
    "    net.eval()\n",
    "    with torch.no_grad():\n",
    "        x = input_tensor.to(DEVICE).float()\n",
    "        # ensure batch dimension\n",
    "        if x.ndim == len(net.forward.__annotations__.get('return', [])):\n",
    "            pass\n",
    "        # we call the raw network - predictor may have helper wrappers but direct forward usually works\n",
    "        try:\n",
    "            out = net(x)\n",
    "        except Exception as e:\n",
    "            # If forward signature expects (x, do_mirroring, ...), try predictor.run_network\n",
    "            try:\n",
    "                out = predictor.network(x)\n",
    "            except Exception as e2:\n",
    "                raise RuntimeError('Failed to run network forward: ' + str(e2))\n",
    "\n",
    "    # remove hooks\n",
    "    for h in hooks:\n",
    "        h.remove()\n",
    "    if was_training:\n",
    "        net.train()\n",
    "\n",
    "    return activations, out\n",
    "\n",
    "\n",
    "def flatten_activations(activation: torch.Tensor) -> np.ndarray:\n",
    "    \"\"\"Convert activation tensor (B,C,H,W,(D)) to (N_voxels, C) numpy array (for first batch only).\"\"\"\n",
    "    # activation shape [B, C, *spatial]\n",
    "    act = activation.cpu().numpy()\n",
    "    B, C = act.shape[:2]\n",
    "    spatial = act.shape[2:]\n",
    "    N = int(np.prod(spatial))\n",
    "    # take first batch\n",
    "    arr = act[0].reshape(C, -1).T  # (N, C)\n",
    "    return arr, spatial\n",
    "\n",
    "print('Helper functions defined: initialize_predictor_from_folder, extract_activations, flatten_activations')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82bd1fa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: end-to-end extraction -> PCA -> UMAP -> visualize\n",
    "# Assumes `model_path`, `image_path`, and `predictor` exist in the notebook scope.\n",
    "\n",
    "from random import Random\n",
    "import matplotlib\n",
    "\n",
    "# Utility to find a case file in image_path (supports .npz, .npy, .nii.gz)\n",
    "def find_case_file(image_dir: Path):\n",
    "    for ext in ('*.npz', '*.npy', '*.nii.gz', '*.nii'):\n",
    "        files = list(Path(image_dir).rglob(ext))\n",
    "        if files:\n",
    "            return files[0]\n",
    "    raise FileNotFoundError(f'No case file found under {image_dir}')\n",
    "\n",
    "# Load model (safe wrapper)\n",
    "try:\n",
    "    initialize_predictor_from_folder(predictor, model_path, checkpoint_name='checkpoint_best.pth')\n",
    "except Exception as e:\n",
    "    initialize_untrained_predictor(predictor, plans_file)\n",
    "    if pretrained_weights_file.exists():\n",
    "        print('Loading pretrained weights from', pretrained_weights_file)\n",
    "        load_pretrained_weights(predictor.network, pretrained_weights_file)\n",
    "    print(\"Initialized untrained predictor. As the model path is not provided or invalid.\")\n",
    "    # print('initialize_predictor_from_folder failed; ensure predictor and model_path are correct:', e)\n",
    "\n",
    "# Pick a case file\n",
    "case_file = find_case_file(image_path)\n",
    "print('Using case file:', case_file)\n",
    "\n",
    "# Load the case - try multiple key names\n",
    "if case_file.suffix == '.npz':\n",
    "    arr = np.load(case_file)\n",
    "    # common keys: 'data', 'x', 'arr_0'\n",
    "    for k in ('data','x','arr_0','image','images'):\n",
    "        if k in arr:\n",
    "            img = arr[k]\n",
    "            break\n",
    "    else:\n",
    "        # take the first array\n",
    "        img = arr[list(arr.files)[0]]\n",
    "elif case_file.suffix == '.npy':\n",
    "    img = np.load(case_file)\n",
    "else:\n",
    "    # for .nii or .nii.gz use nibabel if available\n",
    "    try:\n",
    "        import nibabel as nib\n",
    "    except Exception:\n",
    "        raise RuntimeError('To load NiFTI files please install nibabel (pip install nibabel)')\n",
    "    img = nib.load(str(case_file)).get_fdata()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17436974",
   "metadata": {},
   "source": [
    "## Interactive patch selection\n",
    "\n",
    "Use the cell below to set a patch size (we'll use 32 x 160 x 128 by default), visualize a patch extracted at an offset you provide, and then run the patch through the network to extract embeddings. The loop lets you adjust the offset interactively until you accept the patch. No stitching is done â€” this only runs a single patch through the model (good for GPU and quick inspection)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9382ce9b",
   "metadata": {},
   "source": [
    "## Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4e7bdc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84b789b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Capture and visualize multiple stages' activations\n",
    "# Usage example:\n",
    "#   visualize_stages_for_patch(patch, module_names=None, channels_per_stage=3)\n",
    "# where patch is a numpy array (C, D, H, W) or the same `input_tensor` shape used before.\n",
    "\n",
    "from typing import List\n",
    "\n",
    "\n",
    "def find_module_by_name(root, name: str):\n",
    "    if not name:\n",
    "        return None\n",
    "    cur = root\n",
    "    for p in name.split('.'):\n",
    "        if p.isdigit():\n",
    "            cur = cur[int(p)]\n",
    "        else:\n",
    "            cur = getattr(cur, p)\n",
    "    return cur\n",
    "\n",
    "\n",
    "def default_stage_names_for(net, max_stages=4) -> List[str]:\n",
    "    # choose last `max_stages` encoder stages if available\n",
    "    if hasattr(net, 'encoder') and hasattr(net.encoder, 'stages'):\n",
    "        n = len(net.encoder.stages)\n",
    "        idxs = list(range(max(0, n - max_stages), n))\n",
    "        return [f'encoder.stages.{i}' for i in idxs]\n",
    "    # fallback heuristics\n",
    "    candidates = []\n",
    "    if hasattr(net, 'encoder'):\n",
    "        candidates.append('encoder')\n",
    "    candidates.append('network')\n",
    "    return candidates[:max_stages]\n",
    "\n",
    "\n",
    "def capture_activations_multiple(predictor, input_tensor: torch.Tensor, module_names: List[str]):\n",
    "    \"\"\"Run forward and capture activations from multiple modules. Returns dict name->tensor (on CPU) and network output.\"\"\"\n",
    "    net = predictor.network\n",
    "    net.to(DEVICE)\n",
    "    net.eval()\n",
    "    activations = {}\n",
    "    handles = []\n",
    "\n",
    "    def make_hook(name):\n",
    "        def hook(module, inp, out):\n",
    "            # store on CPU to avoid holding GPU memory across stages\n",
    "            activations[name] = out.detach().cpu()\n",
    "        return hook\n",
    "\n",
    "    for name in module_names:\n",
    "        m = find_module_by_name(net, name)\n",
    "        if m is None:\n",
    "            raise ValueError(f'Could not resolve module {name}')\n",
    "        handles.append(m.register_forward_hook(make_hook(name)))\n",
    "\n",
    "    with torch.no_grad():\n",
    "        x = input_tensor.to(DEVICE).float()\n",
    "        out = net(x)\n",
    "\n",
    "    for h in handles:\n",
    "        h.remove()\n",
    "\n",
    "    return activations, out\n",
    "\n",
    "\n",
    "def visualize_activations_grid(activations: dict, channels_per_stage: int = 3, cmap='viridis'):\n",
    "    \"\"\"Plot a grid: one row per activation (module), columns are feature channels (first channels_per_stage channels).\n",
    "    Each cell shows the central depth slice for 3D activations (or the 2D activation if 4D).\n",
    "    \"\"\"\n",
    "    names = list(activations.keys())\n",
    "    nrows = len(names)\n",
    "    # determine number of columns as min available channels and requested\n",
    "    first_act = next(iter(activations.values()))\n",
    "    if first_act.ndim < 3:\n",
    "        raise RuntimeError('Activation tensors have unexpected rank: ' + str(first_act.shape))\n",
    "\n",
    "    # For each activation determine channels\n",
    "    channels_per_stage = int(channels_per_stage)\n",
    "    fig_cols = channels_per_stage\n",
    "    fig, axes = plt.subplots(nrows, fig_cols, figsize=(4*fig_cols, 4*nrows))\n",
    "    if nrows == 1:\n",
    "        axes = np.atleast_2d(axes)\n",
    "    axes = np.array(axes).reshape(nrows, fig_cols)\n",
    "\n",
    "    for i, name in enumerate(names):\n",
    "        act = activations[name].numpy()  # [B, C, (D,), H, W] or [B, C, H, W]\n",
    "        B, C = act.shape[0], act.shape[1]\n",
    "        spatial = act.shape[2:]\n",
    "        # choose channels to display\n",
    "        nshow = min(C, channels_per_stage)\n",
    "        for j in range(fig_cols):\n",
    "            ax = axes[i, j]\n",
    "            ax.axis('off')\n",
    "            if j < nshow:\n",
    "                ch = j\n",
    "                if len(spatial) == 3:\n",
    "                    # 3D activation: (D,H,W)\n",
    "                    mid = spatial[0] // 2\n",
    "                    img = act[0, ch, mid]\n",
    "                elif len(spatial) == 2:\n",
    "                    img = act[0, ch]\n",
    "                else:\n",
    "                    # higher dims - try to collapse\n",
    "                    img = act[0, ch].squeeze()\n",
    "                im = ax.imshow(img, cmap=cmap)\n",
    "                ax.set_title(f'{name}\\nch {ch} shape {img.shape}')\n",
    "                fig.colorbar(im, ax=ax, fraction=0.046, pad=0.04)\n",
    "            else:\n",
    "                ax.set_visible(False)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def visualize_stages_for_patch(patch_np: np.ndarray, module_names: List[str] = None, channels_per_stage: int = 3):\n",
    "    \"\"\"Convenience wrapper: given patch numpy (C, D, H, W) or (1,C,D,H,W), capture activations and visualize grid.\"\"\"\n",
    "    # normalize patch shape to (B=1,C,...)\n",
    "    x = patch_np\n",
    "    if x.ndim == 4:\n",
    "        x = np.expand_dims(x, 0)\n",
    "    elif x.ndim == 5 and x.shape[0] != 1:\n",
    "        # assume already batched >1; take first\n",
    "        x = x[0:1]\n",
    "    input_tensor = torch.from_numpy(x).float()\n",
    "\n",
    "    net = predictor.network\n",
    "    if module_names is None:\n",
    "        module_names = default_stage_names_for(net, max_stages=4)\n",
    "    print('Capturing from modules:', module_names)\n",
    "    activations, _ = capture_activations_multiple(predictor, input_tensor, module_names)\n",
    "    visualize_activations_grid(activations, channels_per_stage=channels_per_stage)\n",
    "\n",
    "print('Added helpers: visualize_stages_for_patch(patch_np, module_names=None, channels_per_stage=3)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f0e6fd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# helpers for get_img_array, extract_patch_from_array and visualize_patch\n",
    "def get_img_array():\n",
    "    \"\"\"Get the image array from the case file loaded in the notebook scope.\n",
    "    Returns array of shape (C,D,H,W).\"\"\"\n",
    "    global img  # from notebook scope\n",
    "    # ensure 4D (C,D,H,W)\n",
    "    x = img\n",
    "    if x.ndim == 3:\n",
    "        x = np.expand_dims(x, 0)  # add channel dim\n",
    "    elif x.ndim == 5:\n",
    "        # assume (B,C,D,H,W), take first batch\n",
    "        x = x[0]\n",
    "    elif x.ndim != 4:\n",
    "        raise ValueError(f'Unexpected image dimensions: {x.shape}')\n",
    "    return x\n",
    "\n",
    "def extract_patch_from_array(arr: np.ndarray, offset: tuple, patch_size: tuple):\n",
    "    \"\"\"Extract a patch from array at given offset.\n",
    "    Args:\n",
    "        arr: array of shape (C,D,H,W)\n",
    "        offset: (d,h,w) offset into spatial dimensions\n",
    "        patch_size: (d,h,w) size of patch to extract\n",
    "    Returns:\n",
    "        (patch, actual_offset) where patch has shape (C,D,H,W) and actual_offset is\n",
    "        the clamped offset that was actually used.\n",
    "    \"\"\"\n",
    "    if arr.ndim != 4:\n",
    "        raise ValueError(f'Array must be 4D (C,D,H,W), got shape {arr.shape}')\n",
    "    if len(offset) != 3 or len(patch_size) != 3:\n",
    "        raise ValueError('Offset and patch_size must be 3-tuples (d,h,w)')\n",
    "    \n",
    "    C = arr.shape[0]\n",
    "    # clamp offsets to valid range\n",
    "    d0 = np.clip(offset[0], 0, arr.shape[1] - patch_size[0])\n",
    "    h0 = np.clip(offset[1], 0, arr.shape[2] - patch_size[1])\n",
    "    w0 = np.clip(offset[2], 0, arr.shape[3] - patch_size[2])\n",
    "    actual_offset = (d0, h0, w0)\n",
    "    \n",
    "    # extract patch\n",
    "    d1, h1, w1 = d0 + patch_size[0], h0 + patch_size[1], w0 + patch_size[2]\n",
    "    patch = arr[:, d0:d1, h0:h1, w0:w1].copy()\n",
    "    return patch, actual_offset\n",
    "\n",
    "def visualize_patch(patch: np.ndarray, channel: int = 0):\n",
    "    \"\"\"Visualize a patch array by showing its central depth slice.\n",
    "    Args:\n",
    "        patch: array of shape (C,D,H,W)\n",
    "        channel: which channel to display\n",
    "    \"\"\"\n",
    "    if patch.ndim != 4:\n",
    "        raise ValueError(f'Patch must be 4D (C,D,H,W), got shape {patch.shape}')\n",
    "    if channel >= patch.shape[0]:\n",
    "        raise ValueError(f'Channel {channel} out of range (0-{patch.shape[0]-1})')\n",
    "    \n",
    "    # show middle depth slice\n",
    "    mid = patch.shape[1] // 2\n",
    "    plt.figure(figsize=(8,8))\n",
    "    plt.imshow(patch[channel, mid], cmap='gray')\n",
    "    # plt.colorbar(label=f'Channel {channel}')\n",
    "    plt.title(f'Patch central slice (d={mid})\\nShape: {patch.shape}')\n",
    "    plt.show()\n",
    "\n",
    "print('Added helpers: get_img_array(), extract_patch_from_array(arr, offset, patch_size), visualize_patch(patch, channel=0)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12d2e972",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Integrated interactive patch selector + multi-stage visualizer with PCA->RGB per-stage\n",
    "# This cell reuses the helpers in the notebook:\n",
    "#   - get_img_array()\n",
    "#   - extract_patch_from_array(a, offset, patch_size)\n",
    "#   - visualize_patch(patch)\n",
    "#   - capture_activations_multiple(predictor, input_tensor, module_names)\n",
    "#   - visualize_activations_grid(activations, channels_per_stage)\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "DEFAULT_PATCH_SIZE = (32, 160, 128)\n",
    "\n",
    "print('Interactive multi-stage patch visualizer (with PCA->RGB per-stage)')\n",
    "print('Commands:')\n",
    "print(\"  - enter offsets as 'd,h,w' to preview a patch\")\n",
    "print(\"  - 'modules <comma-separated module names>' to set which modules to capture (e.g. modules encoder.stages.2,encoder.stages.3)\")\n",
    "print(\"  - 'channels N' to set number of channels per stage to display\")\n",
    "print(\"  - 'accept' or 'a' to run visualization on the current patch (shows grid + PCA-RGB per stage)\")\n",
    "print(\"  - 'quit' to exit\")\n",
    "\n",
    "# use defaults\n",
    "PATCH_SIZE = DEFAULT_PATCH_SIZE\n",
    "module_names = None\n",
    "channels_per_stage = 3\n",
    "\n",
    "arr = get_img_array()\n",
    "print('Loaded image shape (C,D,H,W):', arr.shape)\n",
    "C, D, H, W = arr.shape\n",
    "default_offset = (max(0, (D - PATCH_SIZE[0]) // 2), max(0, (H - PATCH_SIZE[1]) // 2), max(0, (W - PATCH_SIZE[2]) // 2))\n",
    "print('Default center offset:', default_offset)\n",
    "\n",
    "last_patch = None\n",
    "last_offset = default_offset\n",
    "\n",
    "\n",
    "def activation_to_rgb(act: np.ndarray, n_components: int = 3) -> np.ndarray:\n",
    "    \"\"\"Convert activation tensor to RGB image using PCA across channels.\n",
    "    act: numpy array with shape [B, C, D, H, W] or [B, C, H, W]\n",
    "    Returns: RGB image as numpy array with shape (H, W, 3) for 2D activations or (D, H, W, 3) for 3D.\n",
    "    We return the full 3D/2D volume reduced to 3 channels; caller can pick a slice to display.\n",
    "    \"\"\"\n",
    "    # take first batch\n",
    "    a = act[0]\n",
    "    # a shape: (C, D, H, W) or (C, H, W)\n",
    "    if a.ndim == 4:\n",
    "        C, D, H, W = a.shape\n",
    "        spatial = (D, H, W)\n",
    "        flat = a.reshape(C, -1).T  # (N, C)\n",
    "        # run PCA on CPU\n",
    "        p = PCA(n_components=n_components)\n",
    "        comps = p.fit_transform(flat)  # (N, 3)\n",
    "        comps = comps.reshape(D, H, W, n_components)\n",
    "        # normalize per-channel to 0-1\n",
    "        comps_norm = np.zeros_like(comps)\n",
    "        for i in range(n_components):\n",
    "            ch = comps[..., i]\n",
    "            # clip percentiles to avoid outliers\n",
    "            lo, hi = np.percentile(ch, (1, 99))\n",
    "            ch = np.clip((ch - lo) / (hi - lo + 1e-12), 0.0, 1.0)\n",
    "            comps_norm[..., i] = ch\n",
    "        return comps_norm  # (D, H, W, 3)\n",
    "    elif a.ndim == 3:\n",
    "        C, H, W = a.shape\n",
    "        flat = a.reshape(C, -1).T\n",
    "        p = PCA(n_components=n_components)\n",
    "        comps = p.fit_transform(flat)\n",
    "        comps = comps.reshape(H, W, n_components)\n",
    "        comps_norm = np.zeros_like(comps)\n",
    "        for i in range(n_components):\n",
    "            ch = comps[..., i]\n",
    "            lo, hi = np.percentile(ch, (1, 99))\n",
    "            ch = np.clip((ch - lo) / (hi - lo + 1e-12), 0.0, 1.0)\n",
    "            comps_norm[..., i] = ch\n",
    "        return comps_norm  # (H, W, 3)\n",
    "    else:\n",
    "        raise RuntimeError('Unsupported activation rank: ' + str(a.shape))\n",
    "\n",
    "\n",
    "while True:\n",
    "    s = input(\"offset/modules/channels/accept/quit > \").strip()\n",
    "    if not s:\n",
    "        continue\n",
    "    if s.lower() == 'quit':\n",
    "        print('Exiting interactive visualizer')\n",
    "        break\n",
    "    if s.lower().startswith('modules '):\n",
    "        val = s[len('modules '):].strip()\n",
    "        if val.lower() == 'default' or val == '':\n",
    "            module_names = None\n",
    "            print('Using default module list')\n",
    "        else:\n",
    "            module_names = [m.strip() for m in val.split(',') if m.strip()]\n",
    "            print('Set module_names =', module_names)\n",
    "        continue\n",
    "    if s.lower().startswith('channels '):\n",
    "        try:\n",
    "            channels_per_stage = int(s.split()[1])\n",
    "            print('Set channels_per_stage =', channels_per_stage)\n",
    "        except Exception as e:\n",
    "            print('Could not parse channels count:', e)\n",
    "        continue\n",
    "    if s.lower() in ('accept','a'):\n",
    "        if last_patch is None:\n",
    "            print('No patch selected yet. Enter an offset first.')\n",
    "            continue\n",
    "        print('Capturing activations and visualizing stages + PCA->RGB (channels_per_stage=', channels_per_stage,')')\n",
    "        try:\n",
    "            # prepare tensor\n",
    "            x = last_patch\n",
    "            if x.ndim == 4:\n",
    "                x_in = np.expand_dims(x, 0)\n",
    "            else:\n",
    "                x_in = x\n",
    "            input_tensor = torch.from_numpy(x_in).float()\n",
    "            # capture multiple activations (they are moved to CPU inside)\n",
    "            if module_names is None:\n",
    "                module_names = default_stage_names_for(predictor.network, max_stages=4)\n",
    "            activations, _ = capture_activations_multiple(predictor, input_tensor, module_names)\n",
    "\n",
    "            # visualize grid (channels)\n",
    "            visualize_activations_grid(activations, channels_per_stage=channels_per_stage)\n",
    "\n",
    "            # For each activation compute PCA->RGB and display middle slice as RGB\n",
    "            for name, act in activations.items():\n",
    "                print('\\nComputing PCA->RGB for', name)\n",
    "                try:\n",
    "                    rgb_vol = activation_to_rgb(act, n_components=3)\n",
    "                except Exception as e:\n",
    "                    print('PCA->RGB failed for', name, ':', e)\n",
    "                    continue\n",
    "                # rgb_vol: (D,H,W,3) or (H,W,3)\n",
    "                if rgb_vol.ndim == 4:\n",
    "                    mid = rgb_vol.shape[0] // 2\n",
    "                    rgb_im = rgb_vol[mid]\n",
    "                else:\n",
    "                    rgb_im = rgb_vol\n",
    "                # ensure float in 0-1\n",
    "                rgb_im = np.clip(rgb_im, 0.0, 1.0)\n",
    "                plt.figure(figsize=(6,6))\n",
    "                plt.imshow(rgb_im)\n",
    "                plt.title(f'PCA RGB - {name} (middle slice)')\n",
    "                plt.axis('off')\n",
    "                plt.show()\n",
    "        except Exception as e:\n",
    "            print('Visualization failed:', e)\n",
    "        continue\n",
    "\n",
    "    # parse potential offset input\n",
    "    try:\n",
    "        parts = [int(x) for x in s.split(',')]\n",
    "        if len(parts) == 3:\n",
    "            offs = tuple(parts)\n",
    "        else:\n",
    "            print(\"Provide offsets as 'd,h,w' or use commands. Got:\", s)\n",
    "            continue\n",
    "    except Exception:\n",
    "        print(\"Unknown command or bad offset. Use 'modules', 'channels', 'accept', or 'quit'.\")\n",
    "        continue\n",
    "\n",
    "    patch, actual_offset = extract_patch_from_array(arr, offs, PATCH_SIZE)\n",
    "    last_patch = patch.astype(np.float32)\n",
    "    last_offset = actual_offset\n",
    "    print('Extracted patch at offset (clamped):', actual_offset, 'patch shape:', patch.shape)\n",
    "    visualize_patch(patch)\n",
    "    print(\"Type 'a' or 'accept' to visualize stages and PCA->RGB for this patch, or enter a new offset to preview another patch.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6474441c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06c340e4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
