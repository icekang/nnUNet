{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a39a14a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from nnunetv2.inference.predict_from_raw_data import nnUNetPredictor\n",
    "from nnunetv2.utilities.get_network_from_plans import get_network_from_plans\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Initialize predictor\n",
    "predictor = nnUNetPredictor(\n",
    "    tile_step_size=0.5,\n",
    "    use_gaussian=True,\n",
    "    use_mirroring=True,\n",
    "    perform_everything_on_device=True,\n",
    "    device=torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af8df0d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "# Define root directory and dataset paths\n",
    "ROOT_DIR = Path(\"/nfs/erelab001/shared/Computational_Group/Naravich\")\n",
    "datasets = ROOT_DIR / \"datasets\" / \"nnUNet_Datasets\"\n",
    "\n",
    "# Define model path\n",
    "model_path = datasets / \"nnUNet_results/Dataset300_Lumen_and_Wall_OCT/nnUNetTrainer__nnUNetPreprocessPlans__3d_fullres\"\n",
    "# model checkpoint is in fold_all/checkpoint_best.pth\n",
    "\n",
    "# Define images path\n",
    "image_path = datasets / \"nnUNet_preprocessed/Dataset307_Sohee_Calcium_OCT_CrossValidation/nnUNetPlans_3d_fullres\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "607634e1",
   "metadata": {},
   "source": [
    "# nnUNet embedding visualization (PCA + UMAP)\n",
    "\n",
    "This notebook adds utilities to extract intermediate embeddings (feature maps) from a trained nnUNet model and visualize them using PCA and UMAP.\n",
    "\n",
    "How to use:\n",
    "- Ensure `predictor`, `model_path`, and `image_path` variables are set (they are defined above in this notebook).\n",
    "- Run the imports/install cell (Cell N+1).\n",
    "- Run the helper functions cell (Cell N+2).\n",
    "- Update the example run cell with a case filename if necessary and run it (Cell N+3).\n",
    "\n",
    "Notes:\n",
    "- The extraction uses a forward hook on a selected module (by default the last encoder stage / bottleneck). You can change the `target_module_name` in the example to capture other layers.\n",
    "- We sample voxels for UMAP to keep computation light; increase `n_samples` if you want more points.\n",
    "- If your data uses different key names (e.g., `.npz` with `data`/`arr_0`), adjust the loading step accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c62548f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports and installs (will try to install umap-learn if missing)\n",
    "import sys\n",
    "import subprocess\n",
    "import importlib\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Try import umap, install if missing\n",
    "try:\n",
    "    import umap\n",
    "except Exception:\n",
    "    print('umap not found, attempting to install umap-learn...')\n",
    "    subprocess.check_call([sys.executable, '-m', 'pip', 'install', 'umap-learn'])\n",
    "    importlib.invalidate_caches()\n",
    "    import umap\n",
    "\n",
    "print('torch:', torch.__version__)\n",
    "print('umap:', umap.__version__)\n",
    "print('sklearn:', importlib.import_module('sklearn').__version__)\n",
    "\n",
    "# helper - device\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Device:', DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e469bd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions: load model into predictor, extract activations via hook, flatten embeddings\n",
    "from typing import Optional, Tuple, Dict\n",
    "\n",
    "def initialize_predictor_from_folder(predictor: nnUNetPredictor, model_folder: Path, checkpoint_name: Optional[str] = None):\n",
    "    \"\"\"Try to initialize the predictor from a trained model folder.\n",
    "    Falls back to searching for checkpoint files if `initialize_from_trained_model_folder` is not available.\n",
    "    \"\"\"\n",
    "    model_folder = Path(model_folder)\n",
    "    # Prefer built-in initializer if available\n",
    "    try:\n",
    "        predictor.initialize_from_trained_model_folder(str(model_folder), use_folds=\"all\", checkpoint_name=checkpoint_name)\n",
    "        print('Initialized predictor via initialize_from_trained_model_folder')\n",
    "        return\n",
    "    except Exception as e:\n",
    "        print('initialize_from_trained_model_folder not available or failed, trying to load checkpoint directly:', e)\n",
    "\n",
    "    # Fallback: find a checkpoint file\n",
    "    candidates = list(model_folder.rglob('checkpoint*.pth')) + list(model_folder.rglob('*.pth'))\n",
    "    if not candidates:\n",
    "        raise FileNotFoundError(f'No checkpoint found under {model_folder}')\n",
    "    # choose the most recent candidate\n",
    "    ckpt = sorted(candidates)[-1]\n",
    "    ckpt = str(ckpt)\n",
    "    print('Loading checkpoint:', ckpt)\n",
    "    state = torch.load(ckpt, map_location=DEVICE)\n",
    "    # handle different save formats\n",
    "    if isinstance(state, dict) and 'state_dict' in state:\n",
    "        sd = state['state_dict']\n",
    "    elif isinstance(state, dict) and any(k.startswith('network') or k.startswith('model') for k in state.keys()):\n",
    "        # try to find direct network dict\n",
    "        sd = None\n",
    "        for k in ['network', 'model', 'state_dict']:\n",
    "            if k in state:\n",
    "                sd = state[k]\n",
    "                break\n",
    "        if sd is None:\n",
    "            # maybe the state is the state_dict itself\n",
    "            sd = state\n",
    "    else:\n",
    "        sd = state\n",
    "    try:\n",
    "        predictor.network.load_state_dict(sd)\n",
    "    except Exception as e:\n",
    "        print('Failed to load state_dict directly into predictor.network:', e)\n",
    "        # Try to handle prefixed keys\n",
    "        new_sd = {}\n",
    "        for k,v in sd.items():\n",
    "            new_key = k\n",
    "            if k.startswith('module.'):\n",
    "                new_key = k[len('module.'):]\n",
    "            new_sd[new_key] = v\n",
    "        predictor.network.load_state_dict(new_sd)\n",
    "    predictor.network.to(DEVICE)\n",
    "    predictor.device = DEVICE\n",
    "    predictor.network.eval()\n",
    "    print('Checkpoint loaded and network set to eval on', DEVICE)\n",
    "\n",
    "\n",
    "def extract_activations(predictor, input_tensor: torch.Tensor, target_module_name: Optional[str] = None) -> Tuple[Dict[str, torch.Tensor], torch.Tensor]:\n",
    "    \"\"\"Run forward and capture activations from the specified module.\n",
    "\n",
    "    Returns (activations, output_prediction)\n",
    "    - activations: dict name -> tensor (B,C,...) on CPU\n",
    "    - output_prediction: raw network output (if returned by forward)\n",
    "    \"\"\"\n",
    "    net = predictor.network\n",
    "    net = net.to(DEVICE)\n",
    "    activations = {}\n",
    "    hooks = []\n",
    "\n",
    "    # Decide which module to hook\n",
    "    def find_module_by_name(root, name):\n",
    "        # name may be like 'encoder.stages.4' or simple 'encoder'\n",
    "        if not name:\n",
    "            return None\n",
    "        parts = name.split('.')\n",
    "        cur = root\n",
    "        try:\n",
    "            for p in parts:\n",
    "                if p.isdigit():\n",
    "                    cur = cur[int(p)]\n",
    "                else:\n",
    "                    cur = getattr(cur, p)\n",
    "            return cur\n",
    "        except Exception:\n",
    "            return None\n",
    "\n",
    "    if target_module_name is None:\n",
    "        # attempt to find a bottleneck or last encoder stage\n",
    "        candidates = []\n",
    "        if hasattr(net, 'encoder'):\n",
    "            enc = net.encoder\n",
    "            # common attribute names\n",
    "            if hasattr(enc, 'stages'):\n",
    "                try:\n",
    "                    candidates.append(enc.stages[-1])\n",
    "                    target_module = enc.stages[-1]\n",
    "                    target_name = 'encoder.stages.-1'\n",
    "                except Exception:\n",
    "                    target_module = enc\n",
    "                    target_name = 'encoder'\n",
    "            else:\n",
    "                target_module = enc\n",
    "                target_name = 'encoder'\n",
    "        else:\n",
    "            # fallback to entire network\n",
    "            target_module = net\n",
    "            target_name = 'network'\n",
    "    else:\n",
    "        m = find_module_by_name(net, target_module_name)\n",
    "        if m is None:\n",
    "            raise ValueError(f'Could not find module by name {target_module_name}')\n",
    "        target_module = m\n",
    "        target_name = target_module_name\n",
    "\n",
    "    # register forward hook\n",
    "    def hook_fn(module, inp, out):\n",
    "        # detach and move to cpu\n",
    "        activations[target_name] = out.detach().cpu()\n",
    "\n",
    "    hooks.append(target_module.register_forward_hook(hook_fn))\n",
    "\n",
    "    # run forward\n",
    "    was_training = net.training\n",
    "    net.eval()\n",
    "    with torch.no_grad():\n",
    "        x = input_tensor.to(DEVICE).float()\n",
    "        # ensure batch dimension\n",
    "        if x.ndim == len(net.forward.__annotations__.get('return', [])):\n",
    "            pass\n",
    "        # we call the raw network - predictor may have helper wrappers but direct forward usually works\n",
    "        try:\n",
    "            out = net(x)\n",
    "        except Exception as e:\n",
    "            # If forward signature expects (x, do_mirroring, ...), try predictor.run_network\n",
    "            try:\n",
    "                out = predictor.network(x)\n",
    "            except Exception as e2:\n",
    "                raise RuntimeError('Failed to run network forward: ' + str(e2))\n",
    "\n",
    "    # remove hooks\n",
    "    for h in hooks:\n",
    "        h.remove()\n",
    "    if was_training:\n",
    "        net.train()\n",
    "\n",
    "    return activations, out\n",
    "\n",
    "\n",
    "def flatten_activations(activation: torch.Tensor) -> np.ndarray:\n",
    "    \"\"\"Convert activation tensor (B,C,H,W,(D)) to (N_voxels, C) numpy array (for first batch only).\"\"\"\n",
    "    # activation shape [B, C, *spatial]\n",
    "    act = activation.cpu().numpy()\n",
    "    B, C = act.shape[:2]\n",
    "    spatial = act.shape[2:]\n",
    "    N = int(np.prod(spatial))\n",
    "    # take first batch\n",
    "    arr = act[0].reshape(C, -1).T  # (N, C)\n",
    "    return arr, spatial\n",
    "\n",
    "print('Helper functions defined: initialize_predictor_from_folder, extract_activations, flatten_activations')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82bd1fa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: end-to-end extraction -> PCA -> UMAP -> visualize\n",
    "# Assumes `model_path`, `image_path`, and `predictor` exist in the notebook scope.\n",
    "\n",
    "from random import Random\n",
    "import matplotlib\n",
    "\n",
    "# Utility to find a case file in image_path (supports .npz, .npy, .nii.gz)\n",
    "def find_case_file(image_dir: Path):\n",
    "    for ext in ('*.npz', '*.npy', '*.nii.gz', '*.nii'):\n",
    "        files = list(Path(image_dir).rglob(ext))\n",
    "        if files:\n",
    "            return files[0]\n",
    "    raise FileNotFoundError(f'No case file found under {image_dir}')\n",
    "\n",
    "# Load model (safe wrapper)\n",
    "try:\n",
    "    initialize_predictor_from_folder(predictor, model_path, checkpoint_name='checkpoint_best.pth')\n",
    "except Exception as e:\n",
    "    print('initialize_predictor_from_folder failed; ensure predictor and model_path are correct:', e)\n",
    "\n",
    "# Pick a case file\n",
    "case_file = find_case_file(image_path)\n",
    "print('Using case file:', case_file)\n",
    "\n",
    "# Load the case - try multiple key names\n",
    "if case_file.suffix == '.npz':\n",
    "    arr = np.load(case_file)\n",
    "    # common keys: 'data', 'x', 'arr_0'\n",
    "    for k in ('data','x','arr_0','image','images'):\n",
    "        if k in arr:\n",
    "            img = arr[k]\n",
    "            break\n",
    "    else:\n",
    "        # take the first array\n",
    "        img = arr[list(arr.files)[0]]\n",
    "elif case_file.suffix == '.npy':\n",
    "    img = np.load(case_file)\n",
    "else:\n",
    "    # for .nii or .nii.gz use nibabel if available\n",
    "    try:\n",
    "        import nibabel as nib\n",
    "    except Exception:\n",
    "        raise RuntimeError('To load NiFTI files please install nibabel (pip install nibabel)')\n",
    "    img = nib.load(str(case_file)).get_fdata()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17436974",
   "metadata": {},
   "source": [
    "## Interactive patch selection\n",
    "\n",
    "Use the cell below to set a patch size (we'll use 32 x 160 x 128 by default), visualize a patch extracted at an offset you provide, and then run the patch through the network to extract embeddings. The loop lets you adjust the offset interactively until you accept the patch. No stitching is done — this only runs a single patch through the model (good for GPU and quick inspection)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0923e3c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "# Interactive patch extraction, visualization, and single-patch activation pass\n",
    "# Assumes `img` (numpy array in channel-first shape (C, D, H, W)) is already loaded in the notebook scope\n",
    "# If your `img` variable is different, set it before running this cell.\n",
    "\n",
    "import math\n",
    "\n",
    "# Default patch size (D, H, W)\n",
    "PATCH_SIZE = (32, 160, 128)\n",
    "\n",
    "def get_img_array():\n",
    "    \"\"\"Return the current image array in (C, D, H, W) format.\n",
    "    The notebook previously set `img` variable; if `img` has batch dim or other shape,\n",
    "    this function tries to normalize it.\n",
    "    \"\"\"\n",
    "    global img\n",
    "    if 'img' not in globals():\n",
    "        raise RuntimeError('`img` not found in notebook scope. Load one case into variable `img` first.')\n",
    "    a = img\n",
    "    # If user earlier added batch dim: (1, C, D, H, W)\n",
    "    if a.ndim == 5 and a.shape[0] == 1:\n",
    "        a = a[0]\n",
    "    # If shape is (D, H, W) -> assume single channel\n",
    "    if a.ndim == 3:\n",
    "        a = a[np.newaxis, ...]\n",
    "    # If shape is (C, H, W, D) or other order, try to detect common mistakes\n",
    "    if a.ndim == 4:\n",
    "        # assume (C, D, H, W) already\n",
    "        pass\n",
    "    else:\n",
    "        raise RuntimeError(f'unhandled img shape: {a.shape}')\n",
    "    return a\n",
    "\n",
    "\n",
    "def extract_patch_from_array(a: np.ndarray, offset: tuple, patch_size: tuple = PATCH_SIZE) -> np.ndarray:\n",
    "    \"\"\"Extract patch from channel-first array a (C, D, H, W).\n",
    "    offset is (d_off, h_off, w_off). Returns patch shaped (C, pd, ph, pw).\n",
    "    \"\"\"\n",
    "    C, D, H, W = a.shape\n",
    "    pd, ph, pw = patch_size\n",
    "    od, oh, ow = offset\n",
    "    # clamp\n",
    "    sd = int(max(0, min(D - pd, od)))\n",
    "    sh = int(max(0, min(H - ph, oh)))\n",
    "    sw = int(max(0, min(W - pw, ow)))\n",
    "    patch = a[:, sd:sd+pd, sh:sh+ph, sw:sw+pw]\n",
    "    return patch, (sd, sh, sw)\n",
    "\n",
    "\n",
    "def visualize_patch(patch: np.ndarray, cmap='gray'):\n",
    "    \"\"\"Visualize central slices of the patch. Shows one image per channel (central depth slice).\n",
    "    If patch is 2D (pd==1) it will display the single slice.\n",
    "    \"\"\"\n",
    "    C, pd, ph, pw = patch.shape\n",
    "    mid = pd // 2\n",
    "    ncols = min(4, C)\n",
    "    nrows = math.ceil(C / ncols)\n",
    "    fig, axes = plt.subplots(nrows, ncols, figsize=(4*ncols, 4*nrows))\n",
    "    axes = np.array(axes).reshape(-1)\n",
    "    for i in range(nrows*ncols):\n",
    "        ax = axes[i]\n",
    "        ax.axis('off')\n",
    "        if i < C:\n",
    "            im = patch[i, mid]\n",
    "            ax.imshow(im, cmap=cmap)\n",
    "            ax.set_title(f'channel {i} slice {mid} (shape {im.shape})')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Interactive input loop\n",
    "print(f'Default PATCH_SIZE = {PATCH_SIZE} (D, H, W)')\n",
    "arr = get_img_array()\n",
    "print('Loaded image shape (C,D,H,W):', arr.shape)\n",
    "\n",
    "# default offset centers the patch\n",
    "C, D, H, W = arr.shape\n",
    "default_offset = (max(0, (D - PATCH_SIZE[0]) // 2), max(0, (H - PATCH_SIZE[1]) // 2), max(0, (W - PATCH_SIZE[2]) // 2))\n",
    "print('Default center offset:', default_offset)\n",
    "\n",
    "while True:\n",
    "    s = input(\"Enter offset as 'd,h,w' (integers), or 'accept' to run network on current patch, or 'quit' to stop: \")\n",
    "    s = s.strip()\n",
    "    if s.lower() == 'quit':\n",
    "        print('Stopping interactive selection.')\n",
    "        break\n",
    "    if s.lower() == 'accept':\n",
    "        # use last_offset from previous extraction\n",
    "        try:\n",
    "            patch\n",
    "        except NameError:\n",
    "            print('No patch selected yet; set an offset first.')\n",
    "            continue\n",
    "        # prepare tensor and run extract_activations\n",
    "        input_np = patch  # (C, pd, ph, pw)\n",
    "        # ensure batched as earlier code expects (B=1, C, D, H, W)\n",
    "        input_tensor = torch.from_numpy(input_np).unsqueeze(0).float()\n",
    "        print('Running network on patch (may move model and input to DEVICE) ...')\n",
    "        activations, net_out = extract_activations(predictor, input_tensor, target_module_name=None)\n",
    "        act_name = list(activations.keys())[0]\n",
    "        activation = activations[act_name]\n",
    "        print('Captured activation', act_name, 'shape (B,C,...):', activation.shape)\n",
    "        # visualize first feature channel (first batch)\n",
    "        act_np = activation.numpy()\n",
    "        # act_np shape [B, Cf, pd', ph', pw'] — take first batch and first channel\n",
    "        v = act_np[0, 0]\n",
    "        plt.figure(figsize=(6,6))\n",
    "        plt.imshow(v[v.shape[0]//2], cmap='viridis')\n",
    "        plt.title(f'Activation {act_name} - channel 0 middle slice')\n",
    "        plt.colorbar()\n",
    "        plt.show()\n",
    "        print('\\nFinished run for selected patch. You can continue to pick another offset or `quit`.')\n",
    "        continue\n",
    "\n",
    "    # otherwise parse offset\n",
    "    try:\n",
    "        parts = [int(x) for x in s.split(',')]\n",
    "        if len(parts) != 3:\n",
    "            print('Provide 3 ints like: 10,20,30')\n",
    "            continue\n",
    "        offs = tuple(parts)\n",
    "    except Exception as e:\n",
    "        print('Could not parse input:', e)\n",
    "        # if empty, use default\n",
    "        if s == '':\n",
    "            offs = default_offset\n",
    "        else:\n",
    "            continue\n",
    "\n",
    "    patch, actual_offset = extract_patch_from_array(arr, offs, PATCH_SIZE)\n",
    "    print('Extracted patch at (clamped) offset:', actual_offset, 'patch shape:', patch.shape)\n",
    "    visualize_patch(patch)\n",
    "    # store last extracted patch variable for accept\n",
    "    # convert to float32 numpy if not already\n",
    "    if patch.dtype != np.float32:\n",
    "        patch = patch.astype(np.float32)\n",
    "    last_offset = actual_offset\n",
    "    # ask user if they want to accept automatically\n",
    "    ok = input(\"Type 'a' to accept this patch and run the network now, or Enter to continue selecting offsets: \")\n",
    "    if ok.strip().lower() == 'a':\n",
    "        input_tensor = torch.from_numpy(patch).unsqueeze(0).float()\n",
    "        print('Running network on patch (may move model and input to DEVICE) ...')\n",
    "        activations, net_out = extract_activations(predictor, input_tensor, target_module_name=None)\n",
    "        act_name = list(activations.keys())[0]\n",
    "        activation = activations[act_name]\n",
    "        print('Captured activation', act_name, 'shape (B,C,...):', activation.shape)\n",
    "        v = activation.numpy()[0, 0]\n",
    "        plt.figure(figsize=(6,6))\n",
    "        plt.imshow(v[v.shape[0]//2], cmap='viridis')\n",
    "        plt.title(f'Activation {act_name} - channel 0 middle slice')\n",
    "        plt.colorbar()\n",
    "        plt.show()\n",
    "        print('\\nFinished run for selected patch. You can continue to pick another offset or `quit`.')\n",
    "    else:\n",
    "        print('Continue selecting offsets...')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9382ce9b",
   "metadata": {},
   "source": [
    "## Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4e7bdc7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84b789b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Capture and visualize multiple stages' activations\n",
    "# Usage example:\n",
    "#   visualize_stages_for_patch(patch, module_names=None, channels_per_stage=3)\n",
    "# where patch is a numpy array (C, D, H, W) or the same `input_tensor` shape used before.\n",
    "\n",
    "from typing import List\n",
    "\n",
    "\n",
    "def find_module_by_name(root, name: str):\n",
    "    if not name:\n",
    "        return None\n",
    "    cur = root\n",
    "    for p in name.split('.'):\n",
    "        if p.isdigit():\n",
    "            cur = cur[int(p)]\n",
    "        else:\n",
    "            cur = getattr(cur, p)\n",
    "    return cur\n",
    "\n",
    "\n",
    "def default_stage_names_for(net, max_stages=4) -> List[str]:\n",
    "    # choose last `max_stages` encoder stages if available\n",
    "    if hasattr(net, 'encoder') and hasattr(net.encoder, 'stages'):\n",
    "        n = len(net.encoder.stages)\n",
    "        idxs = list(range(max(0, n - max_stages), n))\n",
    "        return [f'encoder.stages.{i}' for i in idxs]\n",
    "    # fallback heuristics\n",
    "    candidates = []\n",
    "    if hasattr(net, 'encoder'):\n",
    "        candidates.append('encoder')\n",
    "    candidates.append('network')\n",
    "    return candidates[:max_stages]\n",
    "\n",
    "\n",
    "def capture_activations_multiple(predictor, input_tensor: torch.Tensor, module_names: List[str]):\n",
    "    \"\"\"Run forward and capture activations from multiple modules. Returns dict name->tensor (on CPU) and network output.\"\"\"\n",
    "    net = predictor.network\n",
    "    net.to(DEVICE)\n",
    "    net.eval()\n",
    "    activations = {}\n",
    "    handles = []\n",
    "\n",
    "    def make_hook(name):\n",
    "        def hook(module, inp, out):\n",
    "            # store on CPU to avoid holding GPU memory across stages\n",
    "            activations[name] = out.detach().cpu()\n",
    "        return hook\n",
    "\n",
    "    for name in module_names:\n",
    "        m = find_module_by_name(net, name)\n",
    "        if m is None:\n",
    "            raise ValueError(f'Could not resolve module {name}')\n",
    "        handles.append(m.register_forward_hook(make_hook(name)))\n",
    "\n",
    "    with torch.no_grad():\n",
    "        x = input_tensor.to(DEVICE).float()\n",
    "        out = net(x)\n",
    "\n",
    "    for h in handles:\n",
    "        h.remove()\n",
    "\n",
    "    return activations, out\n",
    "\n",
    "\n",
    "def visualize_activations_grid(activations: dict, channels_per_stage: int = 3, cmap='viridis'):\n",
    "    \"\"\"Plot a grid: one row per activation (module), columns are feature channels (first channels_per_stage channels).\n",
    "    Each cell shows the central depth slice for 3D activations (or the 2D activation if 4D).\n",
    "    \"\"\"\n",
    "    names = list(activations.keys())\n",
    "    nrows = len(names)\n",
    "    # determine number of columns as min available channels and requested\n",
    "    first_act = next(iter(activations.values()))\n",
    "    if first_act.ndim < 3:\n",
    "        raise RuntimeError('Activation tensors have unexpected rank: ' + str(first_act.shape))\n",
    "\n",
    "    # For each activation determine channels\n",
    "    channels_per_stage = int(channels_per_stage)\n",
    "    fig_cols = channels_per_stage\n",
    "    fig, axes = plt.subplots(nrows, fig_cols, figsize=(4*fig_cols, 4*nrows))\n",
    "    if nrows == 1:\n",
    "        axes = np.atleast_2d(axes)\n",
    "    axes = np.array(axes).reshape(nrows, fig_cols)\n",
    "\n",
    "    for i, name in enumerate(names):\n",
    "        act = activations[name].numpy()  # [B, C, (D,), H, W] or [B, C, H, W]\n",
    "        B, C = act.shape[0], act.shape[1]\n",
    "        spatial = act.shape[2:]\n",
    "        # choose channels to display\n",
    "        nshow = min(C, channels_per_stage)\n",
    "        for j in range(fig_cols):\n",
    "            ax = axes[i, j]\n",
    "            ax.axis('off')\n",
    "            if j < nshow:\n",
    "                ch = j\n",
    "                if len(spatial) == 3:\n",
    "                    # 3D activation: (D,H,W)\n",
    "                    mid = spatial[0] // 2\n",
    "                    img = act[0, ch, mid]\n",
    "                elif len(spatial) == 2:\n",
    "                    img = act[0, ch]\n",
    "                else:\n",
    "                    # higher dims - try to collapse\n",
    "                    img = act[0, ch].squeeze()\n",
    "                im = ax.imshow(img, cmap=cmap)\n",
    "                ax.set_title(f'{name}\\nch {ch} shape {img.shape}')\n",
    "                fig.colorbar(im, ax=ax, fraction=0.046, pad=0.04)\n",
    "            else:\n",
    "                ax.set_visible(False)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def visualize_stages_for_patch(patch_np: np.ndarray, module_names: List[str] = None, channels_per_stage: int = 3):\n",
    "    \"\"\"Convenience wrapper: given patch numpy (C, D, H, W) or (1,C,D,H,W), capture activations and visualize grid.\"\"\"\n",
    "    # normalize patch shape to (B=1,C,...)\n",
    "    x = patch_np\n",
    "    if x.ndim == 4:\n",
    "        x = np.expand_dims(x, 0)\n",
    "    elif x.ndim == 5 and x.shape[0] != 1:\n",
    "        # assume already batched >1; take first\n",
    "        x = x[0:1]\n",
    "    input_tensor = torch.from_numpy(x).float()\n",
    "\n",
    "    net = predictor.network\n",
    "    if module_names is None:\n",
    "        module_names = default_stage_names_for(net, max_stages=4)\n",
    "    print('Capturing from modules:', module_names)\n",
    "    activations, _ = capture_activations_multiple(predictor, input_tensor, module_names)\n",
    "    visualize_activations_grid(activations, channels_per_stage=channels_per_stage)\n",
    "\n",
    "print('Added helpers: visualize_stages_for_patch(patch_np, module_names=None, channels_per_stage=3)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12d2e972",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Integrated interactive patch selector + multi-stage visualizer\n",
    "# This cell reuses the helpers in the notebook:\n",
    "#   - get_img_array()\n",
    "#   - extract_patch_from_array(a, offset, patch_size)\n",
    "#   - visualize_patch(patch)\n",
    "#   - visualize_stages_for_patch(patch_np, module_names=None, channels_per_stage=3)\n",
    "\n",
    "# If any helper is missing, this cell will raise — those helpers were added earlier in this notebook.\n",
    "\n",
    "DEFAULT_PATCH_SIZE = (32, 160, 128)\n",
    "\n",
    "print('Interactive multi-stage patch visualizer')\n",
    "print('Commands:')\n",
    "print(\"  - enter offsets as 'd,h,w' to preview a patch\")\n",
    "print(\"  - 'modules <comma-separated module names>' to set which modules to capture (e.g. modules encoder.stages.2,encoder.stages.3)\")\n",
    "print(\"  - 'channels N' to set number of channels per stage to display\")\n",
    "print(\"  - 'accept' or 'a' to run visualization on the current patch\")\n",
    "print(\"  - 'quit' to exit\")\n",
    "\n",
    "# use defaults\n",
    "PATCH_SIZE = DEFAULT_PATCH_SIZE\n",
    "module_names = None\n",
    "channels_per_stage = 3\n",
    "\n",
    "arr = get_img_array()\n",
    "print('Loaded image shape (C,D,H,W):', arr.shape)\n",
    "C, D, H, W = arr.shape\n",
    "default_offset = (max(0, (D - PATCH_SIZE[0]) // 2), max(0, (H - PATCH_SIZE[1]) // 2), max(0, (W - PATCH_SIZE[2]) // 2))\n",
    "print('Default center offset:', default_offset)\n",
    "\n",
    "last_patch = None\n",
    "last_offset = default_offset\n",
    "\n",
    "while True:\n",
    "    s = input(\"offset/modules/channels/accept/quit > \").strip()\n",
    "    if not s:\n",
    "        continue\n",
    "    if s.lower() == 'quit':\n",
    "        print('Exiting interactive visualizer')\n",
    "        break\n",
    "    if s.lower().startswith('modules '):\n",
    "        val = s[len('modules '):].strip()\n",
    "        if val.lower() == 'default' or val == '':\n",
    "            module_names = None\n",
    "            print('Using default module list')\n",
    "        else:\n",
    "            module_names = [m.strip() for m in val.split(',') if m.strip()]\n",
    "            print('Set module_names =', module_names)\n",
    "        continue\n",
    "    if s.lower().startswith('channels '):\n",
    "        try:\n",
    "            channels_per_stage = int(s.split()[1])\n",
    "            print('Set channels_per_stage =', channels_per_stage)\n",
    "        except Exception as e:\n",
    "            print('Could not parse channels count:', e)\n",
    "        continue\n",
    "    if s.lower() in ('accept','a'):\n",
    "        if last_patch is None:\n",
    "            print('No patch selected yet. Enter an offset first.')\n",
    "            continue\n",
    "        print('Visualizing stages for selected patch with channels_per_stage=', channels_per_stage)\n",
    "        try:\n",
    "            visualize_stages_for_patch(last_patch, module_names=module_names, channels_per_stage=channels_per_stage)\n",
    "        except Exception as e:\n",
    "            print('Visualization failed:', e)\n",
    "        continue\n",
    "\n",
    "    # parse potential offset input\n",
    "    try:\n",
    "        parts = [int(x) for x in s.split(',')]\n",
    "        if len(parts) == 3:\n",
    "            offs = tuple(parts)\n",
    "        else:\n",
    "            print(\"Provide offsets as 'd,h,w' or use commands. Got:\", s)\n",
    "            continue\n",
    "    except Exception:\n",
    "        print(\"Unknown command or bad offset. Use 'modules', 'channels', 'accept', or 'quit'.\")\n",
    "        continue\n",
    "\n",
    "    patch, actual_offset = extract_patch_from_array(arr, offs, PATCH_SIZE)\n",
    "    last_patch = patch.astype(np.float32)\n",
    "    last_offset = actual_offset\n",
    "    print('Extracted patch at offset (clamped):', actual_offset, 'patch shape:', patch.shape)\n",
    "    visualize_patch(patch)\n",
    "    print(\"Type 'a' or 'accept' to visualize stages for this patch, or enter a new offset to preview another patch.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6474441c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
