/home/gridsan/nchutisilp/.conda/envs/nnunet/bin/python
wandb: Tracking run with wandb version 0.16.6
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.

############################
INFO: You are using the old nnU-Net default plans. We have updated our recommendations. Please consider using those instead! Read more here: https://github.com/MIC-DKFZ/nnUNet/blob/master/documentation/resenc_presets.md
############################

Using device: cuda:0

#######################################################################
Please cite the following paper when using nnU-Net:
Isensee, F., Jaeger, P. F., Kohl, S. A., Petersen, J., & Maier-Hein, K. H. (2021). nnU-Net: a self-configuring method for deep learning-based biomedical image segmentation. Nature methods, 18(2), 203-211.
#######################################################################

2024-05-06 18:45:33.127413: Using torch.compile...
/home/gridsan/nchutisilp/.local/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn("The verbose parameter is deprecated. Please use get_last_lr() "
################### Loading pretrained weights from file  /home/gridsan/nchutisilp/datasets/nnUNet_Datasets/nnUNet_results/Dataset300_Lumen_and_Wall_OCT/nnUNetTrainer__nnUNetPreprocessPlans__2d/fold_all/checkpoint_best.pth ###################
Below is the list of overlapping blocks in pretrained model and nnUNet architecture:
encoder.stages.0.0.convs.0.conv.weight shape torch.Size([32, 1, 3, 3])
encoder.stages.0.0.convs.0.conv.bias shape torch.Size([32])
encoder.stages.0.0.convs.0.norm.weight shape torch.Size([32])
encoder.stages.0.0.convs.0.norm.bias shape torch.Size([32])
encoder.stages.0.0.convs.0.all_modules.0.weight shape torch.Size([32, 1, 3, 3])
encoder.stages.0.0.convs.0.all_modules.0.bias shape torch.Size([32])
encoder.stages.0.0.convs.0.all_modules.1.weight shape torch.Size([32])
encoder.stages.0.0.convs.0.all_modules.1.bias shape torch.Size([32])
encoder.stages.0.0.convs.1.conv.weight shape torch.Size([32, 32, 3, 3])
encoder.stages.0.0.convs.1.conv.bias shape torch.Size([32])
encoder.stages.0.0.convs.1.norm.weight shape torch.Size([32])
encoder.stages.0.0.convs.1.norm.bias shape torch.Size([32])
encoder.stages.0.0.convs.1.all_modules.0.weight shape torch.Size([32, 32, 3, 3])
encoder.stages.0.0.convs.1.all_modules.0.bias shape torch.Size([32])
encoder.stages.0.0.convs.1.all_modules.1.weight shape torch.Size([32])
encoder.stages.0.0.convs.1.all_modules.1.bias shape torch.Size([32])
encoder.stages.1.0.convs.0.conv.weight shape torch.Size([64, 32, 3, 3])
encoder.stages.1.0.convs.0.conv.bias shape torch.Size([64])
encoder.stages.1.0.convs.0.norm.weight shape torch.Size([64])
encoder.stages.1.0.convs.0.norm.bias shape torch.Size([64])
encoder.stages.1.0.convs.0.all_modules.0.weight shape torch.Size([64, 32, 3, 3])
encoder.stages.1.0.convs.0.all_modules.0.bias shape torch.Size([64])
encoder.stages.1.0.convs.0.all_modules.1.weight shape torch.Size([64])
encoder.stages.1.0.convs.0.all_modules.1.bias shape torch.Size([64])
encoder.stages.1.0.convs.1.conv.weight shape torch.Size([64, 64, 3, 3])
encoder.stages.1.0.convs.1.conv.bias shape torch.Size([64])
encoder.stages.1.0.convs.1.norm.weight shape torch.Size([64])
encoder.stages.1.0.convs.1.norm.bias shape torch.Size([64])
encoder.stages.1.0.convs.1.all_modules.0.weight shape torch.Size([64, 64, 3, 3])
encoder.stages.1.0.convs.1.all_modules.0.bias shape torch.Size([64])
encoder.stages.1.0.convs.1.all_modules.1.weight shape torch.Size([64])
encoder.stages.1.0.convs.1.all_modules.1.bias shape torch.Size([64])
encoder.stages.2.0.convs.0.conv.weight shape torch.Size([128, 64, 3, 3])
encoder.stages.2.0.convs.0.conv.bias shape torch.Size([128])
encoder.stages.2.0.convs.0.norm.weight shape torch.Size([128])
encoder.stages.2.0.convs.0.norm.bias shape torch.Size([128])
encoder.stages.2.0.convs.0.all_modules.0.weight shape torch.Size([128, 64, 3, 3])
encoder.stages.2.0.convs.0.all_modules.0.bias shape torch.Size([128])
encoder.stages.2.0.convs.0.all_modules.1.weight shape torch.Size([128])
encoder.stages.2.0.convs.0.all_modules.1.bias shape torch.Size([128])
encoder.stages.2.0.convs.1.conv.weight shape torch.Size([128, 128, 3, 3])
encoder.stages.2.0.convs.1.conv.bias shape torch.Size([128])
encoder.stages.2.0.convs.1.norm.weight shape torch.Size([128])
encoder.stages.2.0.convs.1.norm.bias shape torch.Size([128])
encoder.stages.2.0.convs.1.all_modules.0.weight shape torch.Size([128, 128, 3, 3])
encoder.stages.2.0.convs.1.all_modules.0.bias shape torch.Size([128])
encoder.stages.2.0.convs.1.all_modules.1.weight shape torch.Size([128])
encoder.stages.2.0.convs.1.all_modules.1.bias shape torch.Size([128])
encoder.stages.3.0.convs.0.conv.weight shape torch.Size([256, 128, 3, 3])
encoder.stages.3.0.convs.0.conv.bias shape torch.Size([256])
encoder.stages.3.0.convs.0.norm.weight shape torch.Size([256])
encoder.stages.3.0.convs.0.norm.bias shape torch.Size([256])
encoder.stages.3.0.convs.0.all_modules.0.weight shape torch.Size([256, 128, 3, 3])
encoder.stages.3.0.convs.0.all_modules.0.bias shape torch.Size([256])
encoder.stages.3.0.convs.0.all_modules.1.weight shape torch.Size([256])
encoder.stages.3.0.convs.0.all_modules.1.bias shape torch.Size([256])
encoder.stages.3.0.convs.1.conv.weight shape torch.Size([256, 256, 3, 3])
encoder.stages.3.0.convs.1.conv.bias shape torch.Size([256])
encoder.stages.3.0.convs.1.norm.weight shape torch.Size([256])
encoder.stages.3.0.convs.1.norm.bias shape torch.Size([256])
encoder.stages.3.0.convs.1.all_modules.0.weight shape torch.Size([256, 256, 3, 3])
encoder.stages.3.0.convs.1.all_modules.0.bias shape torch.Size([256])
encoder.stages.3.0.convs.1.all_modules.1.weight shape torch.Size([256])
encoder.stages.3.0.convs.1.all_modules.1.bias shape torch.Size([256])
encoder.stages.4.0.convs.0.conv.weight shape torch.Size([512, 256, 3, 3])
encoder.stages.4.0.convs.0.conv.bias shape torch.Size([512])
encoder.stages.4.0.convs.0.norm.weight shape torch.Size([512])
encoder.stages.4.0.convs.0.norm.bias shape torch.Size([512])
encoder.stages.4.0.convs.0.all_modules.0.weight shape torch.Size([512, 256, 3, 3])
encoder.stages.4.0.convs.0.all_modules.0.bias shape torch.Size([512])
encoder.stages.4.0.convs.0.all_modules.1.weight shape torch.Size([512])
encoder.stages.4.0.convs.0.all_modules.1.bias shape torch.Size([512])
encoder.stages.4.0.convs.1.conv.weight shape torch.Size([512, 512, 3, 3])
encoder.stages.4.0.convs.1.conv.bias shape torch.Size([512])
encoder.stages.4.0.convs.1.norm.weight shape torch.Size([512])
encoder.stages.4.0.convs.1.norm.bias shape torch.Size([512])
encoder.stages.4.0.convs.1.all_modules.0.weight shape torch.Size([512, 512, 3, 3])
encoder.stages.4.0.convs.1.all_modules.0.bias shape torch.Size([512])
encoder.stages.4.0.convs.1.all_modules.1.weight shape torch.Size([512])
encoder.stages.4.0.convs.1.all_modules.1.bias shape torch.Size([512])
encoder.stages.5.0.convs.0.conv.weight shape torch.Size([512, 512, 3, 3])
encoder.stages.5.0.convs.0.conv.bias shape torch.Size([512])
encoder.stages.5.0.convs.0.norm.weight shape torch.Size([512])
encoder.stages.5.0.convs.0.norm.bias shape torch.Size([512])
encoder.stages.5.0.convs.0.all_modules.0.weight shape torch.Size([512, 512, 3, 3])
encoder.stages.5.0.convs.0.all_modules.0.bias shape torch.Size([512])
encoder.stages.5.0.convs.0.all_modules.1.weight shape torch.Size([512])
encoder.stages.5.0.convs.0.all_modules.1.bias shape torch.Size([512])
encoder.stages.5.0.convs.1.conv.weight shape torch.Size([512, 512, 3, 3])
encoder.stages.5.0.convs.1.conv.bias shape torch.Size([512])
encoder.stages.5.0.convs.1.norm.weight shape torch.Size([512])
encoder.stages.5.0.convs.1.norm.bias shape torch.Size([512])
encoder.stages.5.0.convs.1.all_modules.0.weight shape torch.Size([512, 512, 3, 3])
encoder.stages.5.0.convs.1.all_modules.0.bias shape torch.Size([512])
encoder.stages.5.0.convs.1.all_modules.1.weight shape torch.Size([512])
encoder.stages.5.0.convs.1.all_modules.1.bias shape torch.Size([512])
encoder.stages.6.0.convs.0.conv.weight shape torch.Size([512, 512, 3, 3])
encoder.stages.6.0.convs.0.conv.bias shape torch.Size([512])
encoder.stages.6.0.convs.0.norm.weight shape torch.Size([512])
encoder.stages.6.0.convs.0.norm.bias shape torch.Size([512])
encoder.stages.6.0.convs.0.all_modules.0.weight shape torch.Size([512, 512, 3, 3])
encoder.stages.6.0.convs.0.all_modules.0.bias shape torch.Size([512])
encoder.stages.6.0.convs.0.all_modules.1.weight shape torch.Size([512])
encoder.stages.6.0.convs.0.all_modules.1.bias shape torch.Size([512])
encoder.stages.6.0.convs.1.conv.weight shape torch.Size([512, 512, 3, 3])
encoder.stages.6.0.convs.1.conv.bias shape torch.Size([512])
encoder.stages.6.0.convs.1.norm.weight shape torch.Size([512])
encoder.stages.6.0.convs.1.norm.bias shape torch.Size([512])
encoder.stages.6.0.convs.1.all_modules.0.weight shape torch.Size([512, 512, 3, 3])
encoder.stages.6.0.convs.1.all_modules.0.bias shape torch.Size([512])
encoder.stages.6.0.convs.1.all_modules.1.weight shape torch.Size([512])
encoder.stages.6.0.convs.1.all_modules.1.bias shape torch.Size([512])
encoder.stages.7.0.convs.0.conv.weight shape torch.Size([512, 512, 3, 3])
encoder.stages.7.0.convs.0.conv.bias shape torch.Size([512])
encoder.stages.7.0.convs.0.norm.weight shape torch.Size([512])
encoder.stages.7.0.convs.0.norm.bias shape torch.Size([512])
encoder.stages.7.0.convs.0.all_modules.0.weight shape torch.Size([512, 512, 3, 3])
encoder.stages.7.0.convs.0.all_modules.0.bias shape torch.Size([512])
encoder.stages.7.0.convs.0.all_modules.1.weight shape torch.Size([512])
encoder.stages.7.0.convs.0.all_modules.1.bias shape torch.Size([512])
encoder.stages.7.0.convs.1.conv.weight shape torch.Size([512, 512, 3, 3])
encoder.stages.7.0.convs.1.conv.bias shape torch.Size([512])
encoder.stages.7.0.convs.1.norm.weight shape torch.Size([512])
encoder.stages.7.0.convs.1.norm.bias shape torch.Size([512])
encoder.stages.7.0.convs.1.all_modules.0.weight shape torch.Size([512, 512, 3, 3])
encoder.stages.7.0.convs.1.all_modules.0.bias shape torch.Size([512])
encoder.stages.7.0.convs.1.all_modules.1.weight shape torch.Size([512])
encoder.stages.7.0.convs.1.all_modules.1.bias shape torch.Size([512])
decoder.encoder.stages.0.0.convs.0.conv.weight shape torch.Size([32, 1, 3, 3])
decoder.encoder.stages.0.0.convs.0.conv.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.norm.weight shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.norm.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.all_modules.0.weight shape torch.Size([32, 1, 3, 3])
decoder.encoder.stages.0.0.convs.0.all_modules.0.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.all_modules.1.weight shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.all_modules.1.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.conv.weight shape torch.Size([32, 32, 3, 3])
decoder.encoder.stages.0.0.convs.1.conv.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.norm.weight shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.norm.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.all_modules.0.weight shape torch.Size([32, 32, 3, 3])
decoder.encoder.stages.0.0.convs.1.all_modules.0.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.all_modules.1.weight shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.all_modules.1.bias shape torch.Size([32])
decoder.encoder.stages.1.0.convs.0.conv.weight shape torch.Size([64, 32, 3, 3])
decoder.encoder.stages.1.0.convs.0.conv.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.norm.weight shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.norm.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.all_modules.0.weight shape torch.Size([64, 32, 3, 3])
decoder.encoder.stages.1.0.convs.0.all_modules.0.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.all_modules.1.weight shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.all_modules.1.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.conv.weight shape torch.Size([64, 64, 3, 3])
decoder.encoder.stages.1.0.convs.1.conv.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.norm.weight shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.norm.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.all_modules.0.weight shape torch.Size([64, 64, 3, 3])
decoder.encoder.stages.1.0.convs.1.all_modules.0.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.all_modules.1.weight shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.all_modules.1.bias shape torch.Size([64])
decoder.encoder.stages.2.0.convs.0.conv.weight shape torch.Size([128, 64, 3, 3])
decoder.encoder.stages.2.0.convs.0.conv.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.norm.weight shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.norm.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.all_modules.0.weight shape torch.Size([128, 64, 3, 3])
decoder.encoder.stages.2.0.convs.0.all_modules.0.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.all_modules.1.weight shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.all_modules.1.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.conv.weight shape torch.Size([128, 128, 3, 3])
decoder.encoder.stages.2.0.convs.1.conv.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.norm.weight shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.norm.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.all_modules.0.weight shape torch.Size([128, 128, 3, 3])
decoder.encoder.stages.2.0.convs.1.all_modules.0.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.all_modules.1.weight shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.all_modules.1.bias shape torch.Size([128])
decoder.encoder.stages.3.0.convs.0.conv.weight shape torch.Size([256, 128, 3, 3])
decoder.encoder.stages.3.0.convs.0.conv.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.norm.weight shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.norm.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.all_modules.0.weight shape torch.Size([256, 128, 3, 3])
decoder.encoder.stages.3.0.convs.0.all_modules.0.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.all_modules.1.weight shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.all_modules.1.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.conv.weight shape torch.Size([256, 256, 3, 3])
decoder.encoder.stages.3.0.convs.1.conv.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.norm.weight shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.norm.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.all_modules.0.weight shape torch.Size([256, 256, 3, 3])
decoder.encoder.stages.3.0.convs.1.all_modules.0.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.all_modules.1.weight shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.all_modules.1.bias shape torch.Size([256])
decoder.encoder.stages.4.0.convs.0.conv.weight shape torch.Size([512, 256, 3, 3])
decoder.encoder.stages.4.0.convs.0.conv.bias shape torch.Size([512])
decoder.encoder.stages.4.0.convs.0.norm.weight shape torch.Size([512])
decoder.encoder.stages.4.0.convs.0.norm.bias shape torch.Size([512])
decoder.encoder.stages.4.0.convs.0.all_modules.0.weight shape torch.Size([512, 256, 3, 3])
decoder.encoder.stages.4.0.convs.0.all_modules.0.bias shape torch.Size([512])
decoder.encoder.stages.4.0.convs.0.all_modules.1.weight shape torch.Size([512])
decoder.encoder.stages.4.0.convs.0.all_modules.1.bias shape torch.Size([512])
decoder.encoder.stages.4.0.convs.1.conv.weight shape torch.Size([512, 512, 3, 3])
decoder.encoder.stages.4.0.convs.1.conv.bias shape torch.Size([512])
decoder.encoder.stages.4.0.convs.1.norm.weight shape torch.Size([512])
decoder.encoder.stages.4.0.convs.1.norm.bias shape torch.Size([512])
decoder.encoder.stages.4.0.convs.1.all_modules.0.weight shape torch.Size([512, 512, 3, 3])
decoder.encoder.stages.4.0.convs.1.all_modules.0.bias shape torch.Size([512])
decoder.encoder.stages.4.0.convs.1.all_modules.1.weight shape torch.Size([512])
decoder.encoder.stages.4.0.convs.1.all_modules.1.bias shape torch.Size([512])
decoder.encoder.stages.5.0.convs.0.conv.weight shape torch.Size([512, 512, 3, 3])
decoder.encoder.stages.5.0.convs.0.conv.bias shape torch.Size([512])
decoder.encoder.stages.5.0.convs.0.norm.weight shape torch.Size([512])
decoder.encoder.stages.5.0.convs.0.norm.bias shape torch.Size([512])
decoder.encoder.stages.5.0.convs.0.all_modules.0.weight shape torch.Size([512, 512, 3, 3])
decoder.encoder.stages.5.0.convs.0.all_modules.0.bias shape torch.Size([512])
decoder.encoder.stages.5.0.convs.0.all_modules.1.weight shape torch.Size([512])
decoder.encoder.stages.5.0.convs.0.all_modules.1.bias shape torch.Size([512])
decoder.encoder.stages.5.0.convs.1.conv.weight shape torch.Size([512, 512, 3, 3])
decoder.encoder.stages.5.0.convs.1.conv.bias shape torch.Size([512])
decoder.encoder.stages.5.0.convs.1.norm.weight shape torch.Size([512])
decoder.encoder.stages.5.0.convs.1.norm.bias shape torch.Size([512])
decoder.encoder.stages.5.0.convs.1.all_modules.0.weight shape torch.Size([512, 512, 3, 3])
decoder.encoder.stages.5.0.convs.1.all_modules.0.bias shape torch.Size([512])
decoder.encoder.stages.5.0.convs.1.all_modules.1.weight shape torch.Size([512])
decoder.encoder.stages.5.0.convs.1.all_modules.1.bias shape torch.Size([512])
decoder.encoder.stages.6.0.convs.0.conv.weight shape torch.Size([512, 512, 3, 3])
decoder.encoder.stages.6.0.convs.0.conv.bias shape torch.Size([512])
decoder.encoder.stages.6.0.convs.0.norm.weight shape torch.Size([512])
decoder.encoder.stages.6.0.convs.0.norm.bias shape torch.Size([512])
decoder.encoder.stages.6.0.convs.0.all_modules.0.weight shape torch.Size([512, 512, 3, 3])
decoder.encoder.stages.6.0.convs.0.all_modules.0.bias shape torch.Size([512])
decoder.encoder.stages.6.0.convs.0.all_modules.1.weight shape torch.Size([512])
decoder.encoder.stages.6.0.convs.0.all_modules.1.bias shape torch.Size([512])
decoder.encoder.stages.6.0.convs.1.conv.weight shape torch.Size([512, 512, 3, 3])
decoder.encoder.stages.6.0.convs.1.conv.bias shape torch.Size([512])
decoder.encoder.stages.6.0.convs.1.norm.weight shape torch.Size([512])
decoder.encoder.stages.6.0.convs.1.norm.bias shape torch.Size([512])
decoder.encoder.stages.6.0.convs.1.all_modules.0.weight shape torch.Size([512, 512, 3, 3])
decoder.encoder.stages.6.0.convs.1.all_modules.0.bias shape torch.Size([512])
decoder.encoder.stages.6.0.convs.1.all_modules.1.weight shape torch.Size([512])
decoder.encoder.stages.6.0.convs.1.all_modules.1.bias shape torch.Size([512])
decoder.encoder.stages.7.0.convs.0.conv.weight shape torch.Size([512, 512, 3, 3])
decoder.encoder.stages.7.0.convs.0.conv.bias shape torch.Size([512])
decoder.encoder.stages.7.0.convs.0.norm.weight shape torch.Size([512])
decoder.encoder.stages.7.0.convs.0.norm.bias shape torch.Size([512])
decoder.encoder.stages.7.0.convs.0.all_modules.0.weight shape torch.Size([512, 512, 3, 3])
decoder.encoder.stages.7.0.convs.0.all_modules.0.bias shape torch.Size([512])
decoder.encoder.stages.7.0.convs.0.all_modules.1.weight shape torch.Size([512])
decoder.encoder.stages.7.0.convs.0.all_modules.1.bias shape torch.Size([512])
decoder.encoder.stages.7.0.convs.1.conv.weight shape torch.Size([512, 512, 3, 3])
decoder.encoder.stages.7.0.convs.1.conv.bias shape torch.Size([512])
decoder.encoder.stages.7.0.convs.1.norm.weight shape torch.Size([512])
decoder.encoder.stages.7.0.convs.1.norm.bias shape torch.Size([512])
decoder.encoder.stages.7.0.convs.1.all_modules.0.weight shape torch.Size([512, 512, 3, 3])
decoder.encoder.stages.7.0.convs.1.all_modules.0.bias shape torch.Size([512])
decoder.encoder.stages.7.0.convs.1.all_modules.1.weight shape torch.Size([512])
decoder.encoder.stages.7.0.convs.1.all_modules.1.bias shape torch.Size([512])
decoder.stages.0.convs.0.conv.weight shape torch.Size([512, 1024, 3, 3])
decoder.stages.0.convs.0.conv.bias shape torch.Size([512])
decoder.stages.0.convs.0.norm.weight shape torch.Size([512])
decoder.stages.0.convs.0.norm.bias shape torch.Size([512])
decoder.stages.0.convs.0.all_modules.0.weight shape torch.Size([512, 1024, 3, 3])
decoder.stages.0.convs.0.all_modules.0.bias shape torch.Size([512])
decoder.stages.0.convs.0.all_modules.1.weight shape torch.Size([512])
decoder.stages.0.convs.0.all_modules.1.bias shape torch.Size([512])
decoder.stages.0.convs.1.conv.weight shape torch.Size([512, 512, 3, 3])
decoder.stages.0.convs.1.conv.bias shape torch.Size([512])
decoder.stages.0.convs.1.norm.weight shape torch.Size([512])
decoder.stages.0.convs.1.norm.bias shape torch.Size([512])
decoder.stages.0.convs.1.all_modules.0.weight shape torch.Size([512, 512, 3, 3])
decoder.stages.0.convs.1.all_modules.0.bias shape torch.Size([512])
decoder.stages.0.convs.1.all_modules.1.weight shape torch.Size([512])
decoder.stages.0.convs.1.all_modules.1.bias shape torch.Size([512])
decoder.stages.1.convs.0.conv.weight shape torch.Size([512, 1024, 3, 3])
decoder.stages.1.convs.0.conv.bias shape torch.Size([512])
decoder.stages.1.convs.0.norm.weight shape torch.Size([512])
decoder.stages.1.convs.0.norm.bias shape torch.Size([512])
decoder.stages.1.convs.0.all_modules.0.weight shape torch.Size([512, 1024, 3, 3])
decoder.stages.1.convs.0.all_modules.0.bias shape torch.Size([512])
decoder.stages.1.convs.0.all_modules.1.weight shape torch.Size([512])
decoder.stages.1.convs.0.all_modules.1.bias shape torch.Size([512])
decoder.stages.1.convs.1.conv.weight shape torch.Size([512, 512, 3, 3])
decoder.stages.1.convs.1.conv.bias shape torch.Size([512])
decoder.stages.1.convs.1.norm.weight shape torch.Size([512])
decoder.stages.1.convs.1.norm.bias shape torch.Size([512])
decoder.stages.1.convs.1.all_modules.0.weight shape torch.Size([512, 512, 3, 3])
decoder.stages.1.convs.1.all_modules.0.bias shape torch.Size([512])
decoder.stages.1.convs.1.all_modules.1.weight shape torch.Size([512])
decoder.stages.1.convs.1.all_modules.1.bias shape torch.Size([512])
decoder.stages.2.convs.0.conv.weight shape torch.Size([512, 1024, 3, 3])
decoder.stages.2.convs.0.conv.bias shape torch.Size([512])
decoder.stages.2.convs.0.norm.weight shape torch.Size([512])
decoder.stages.2.convs.0.norm.bias shape torch.Size([512])
decoder.stages.2.convs.0.all_modules.0.weight shape torch.Size([512, 1024, 3, 3])
decoder.stages.2.convs.0.all_modules.0.bias shape torch.Size([512])
decoder.stages.2.convs.0.all_modules.1.weight shape torch.Size([512])
decoder.stages.2.convs.0.all_modules.1.bias shape torch.Size([512])
decoder.stages.2.convs.1.conv.weight shape torch.Size([512, 512, 3, 3])
decoder.stages.2.convs.1.conv.bias shape torch.Size([512])
decoder.stages.2.convs.1.norm.weight shape torch.Size([512])
decoder.stages.2.convs.1.norm.bias shape torch.Size([512])
decoder.stages.2.convs.1.all_modules.0.weight shape torch.Size([512, 512, 3, 3])
decoder.stages.2.convs.1.all_modules.0.bias shape torch.Size([512])
decoder.stages.2.convs.1.all_modules.1.weight shape torch.Size([512])
decoder.stages.2.convs.1.all_modules.1.bias shape torch.Size([512])
decoder.stages.3.convs.0.conv.weight shape torch.Size([256, 512, 3, 3])
decoder.stages.3.convs.0.conv.bias shape torch.Size([256])
decoder.stages.3.convs.0.norm.weight shape torch.Size([256])
decoder.stages.3.convs.0.norm.bias shape torch.Size([256])
decoder.stages.3.convs.0.all_modules.0.weight shape torch.Size([256, 512, 3, 3])
decoder.stages.3.convs.0.all_modules.0.bias shape torch.Size([256])
decoder.stages.3.convs.0.all_modules.1.weight shape torch.Size([256])
decoder.stages.3.convs.0.all_modules.1.bias shape torch.Size([256])
decoder.stages.3.convs.1.conv.weight shape torch.Size([256, 256, 3, 3])
decoder.stages.3.convs.1.conv.bias shape torch.Size([256])
decoder.stages.3.convs.1.norm.weight shape torch.Size([256])
decoder.stages.3.convs.1.norm.bias shape torch.Size([256])
decoder.stages.3.convs.1.all_modules.0.weight shape torch.Size([256, 256, 3, 3])
decoder.stages.3.convs.1.all_modules.0.bias shape torch.Size([256])
decoder.stages.3.convs.1.all_modules.1.weight shape torch.Size([256])
decoder.stages.3.convs.1.all_modules.1.bias shape torch.Size([256])
decoder.stages.4.convs.0.conv.weight shape torch.Size([128, 256, 3, 3])
decoder.stages.4.convs.0.conv.bias shape torch.Size([128])
decoder.stages.4.convs.0.norm.weight shape torch.Size([128])
decoder.stages.4.convs.0.norm.bias shape torch.Size([128])
decoder.stages.4.convs.0.all_modules.0.weight shape torch.Size([128, 256, 3, 3])
decoder.stages.4.convs.0.all_modules.0.bias shape torch.Size([128])
decoder.stages.4.convs.0.all_modules.1.weight shape torch.Size([128])
decoder.stages.4.convs.0.all_modules.1.bias shape torch.Size([128])
decoder.stages.4.convs.1.conv.weight shape torch.Size([128, 128, 3, 3])
decoder.stages.4.convs.1.conv.bias shape torch.Size([128])
decoder.stages.4.convs.1.norm.weight shape torch.Size([128])
decoder.stages.4.convs.1.norm.bias shape torch.Size([128])
decoder.stages.4.convs.1.all_modules.0.weight shape torch.Size([128, 128, 3, 3])
decoder.stages.4.convs.1.all_modules.0.bias shape torch.Size([128])
decoder.stages.4.convs.1.all_modules.1.weight shape torch.Size([128])
decoder.stages.4.convs.1.all_modules.1.bias shape torch.Size([128])
decoder.stages.5.convs.0.conv.weight shape torch.Size([64, 128, 3, 3])
decoder.stages.5.convs.0.conv.bias shape torch.Size([64])
decoder.stages.5.convs.0.norm.weight shape torch.Size([64])
decoder.stages.5.convs.0.norm.bias shape torch.Size([64])
decoder.stages.5.convs.0.all_modules.0.weight shape torch.Size([64, 128, 3, 3])
decoder.stages.5.convs.0.all_modules.0.bias shape torch.Size([64])
decoder.stages.5.convs.0.all_modules.1.weight shape torch.Size([64])
decoder.stages.5.convs.0.all_modules.1.bias shape torch.Size([64])
decoder.stages.5.convs.1.conv.weight shape torch.Size([64, 64, 3, 3])
decoder.stages.5.convs.1.conv.bias shape torch.Size([64])
decoder.stages.5.convs.1.norm.weight shape torch.Size([64])
decoder.stages.5.convs.1.norm.bias shape torch.Size([64])
decoder.stages.5.convs.1.all_modules.0.weight shape torch.Size([64, 64, 3, 3])
decoder.stages.5.convs.1.all_modules.0.bias shape torch.Size([64])
decoder.stages.5.convs.1.all_modules.1.weight shape torch.Size([64])
decoder.stages.5.convs.1.all_modules.1.bias shape torch.Size([64])
decoder.stages.6.convs.0.conv.weight shape torch.Size([32, 64, 3, 3])
decoder.stages.6.convs.0.conv.bias shape torch.Size([32])
decoder.stages.6.convs.0.norm.weight shape torch.Size([32])
decoder.stages.6.convs.0.norm.bias shape torch.Size([32])
decoder.stages.6.convs.0.all_modules.0.weight shape torch.Size([32, 64, 3, 3])
decoder.stages.6.convs.0.all_modules.0.bias shape torch.Size([32])
decoder.stages.6.convs.0.all_modules.1.weight shape torch.Size([32])
decoder.stages.6.convs.0.all_modules.1.bias shape torch.Size([32])
decoder.stages.6.convs.1.conv.weight shape torch.Size([32, 32, 3, 3])
decoder.stages.6.convs.1.conv.bias shape torch.Size([32])
decoder.stages.6.convs.1.norm.weight shape torch.Size([32])
decoder.stages.6.convs.1.norm.bias shape torch.Size([32])
decoder.stages.6.convs.1.all_modules.0.weight shape torch.Size([32, 32, 3, 3])
decoder.stages.6.convs.1.all_modules.0.bias shape torch.Size([32])
decoder.stages.6.convs.1.all_modules.1.weight shape torch.Size([32])
decoder.stages.6.convs.1.all_modules.1.bias shape torch.Size([32])
decoder.transpconvs.0.weight shape torch.Size([512, 512, 2, 2])
decoder.transpconvs.0.bias shape torch.Size([512])
decoder.transpconvs.1.weight shape torch.Size([512, 512, 2, 2])
decoder.transpconvs.1.bias shape torch.Size([512])
decoder.transpconvs.2.weight shape torch.Size([512, 512, 2, 2])
decoder.transpconvs.2.bias shape torch.Size([512])
decoder.transpconvs.3.weight shape torch.Size([512, 256, 2, 2])
decoder.transpconvs.3.bias shape torch.Size([256])
decoder.transpconvs.4.weight shape torch.Size([256, 128, 2, 2])
decoder.transpconvs.4.bias shape torch.Size([128])
decoder.transpconvs.5.weight shape torch.Size([128, 64, 2, 2])
decoder.transpconvs.5.bias shape torch.Size([64])
decoder.transpconvs.6.weight shape torch.Size([64, 32, 2, 2])
decoder.transpconvs.6.bias shape torch.Size([32])
################### Done ###################
2024-05-06 18:45:43.692887: do_dummy_2d_data_aug: False
2024-05-06 18:45:43.695704: Using splits from existing split file: /home/gridsan/nchutisilp/datasets/nnUNet_Datasets/nnUNet_preprocessed/Dataset302_Calcium_OCTv2/splits_final.json
2024-05-06 18:45:43.743021: The split file contains 3 splits.
2024-05-06 18:45:43.744471: Desired fold for training: 2
2024-05-06 18:45:43.745616: This split has 4 training and 2 validation cases.
using pin_memory on device 0
using pin_memory on device 0

This is the configuration used by this training:
Configuration name: 2d
 {'data_identifier': 'nnUNetPlans_2d', 'preprocessor_name': 'DefaultPreprocessor', 'batch_size': 12, 'patch_size': [512, 512], 'median_image_size_in_voxels': [498.0, 498.0], 'spacing': [1.0, 1.0], 'normalization_schemes': ['ZScoreNormalization'], 'use_mask_for_norm': [False], 'resampling_fn_data': 'resample_data_or_seg_to_shape', 'resampling_fn_seg': 'resample_data_or_seg_to_shape', 'resampling_fn_data_kwargs': {'is_seg': False, 'order': 3, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_seg_kwargs': {'is_seg': True, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_probabilities': 'resample_data_or_seg_to_shape', 'resampling_fn_probabilities_kwargs': {'is_seg': False, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'architecture': {'network_class_name': 'dynamic_network_architectures.architectures.unet.PlainConvUNet', 'arch_kwargs': {'n_stages': 8, 'features_per_stage': [32, 64, 128, 256, 512, 512, 512, 512], 'conv_op': 'torch.nn.modules.conv.Conv2d', 'kernel_sizes': [[3, 3], [3, 3], [3, 3], [3, 3], [3, 3], [3, 3], [3, 3], [3, 3]], 'strides': [[1, 1], [2, 2], [2, 2], [2, 2], [2, 2], [2, 2], [2, 2], [2, 2]], 'n_conv_per_stage': [2, 2, 2, 2, 2, 2, 2, 2], 'n_conv_per_stage_decoder': [2, 2, 2, 2, 2, 2, 2], 'conv_bias': True, 'norm_op': 'torch.nn.modules.instancenorm.InstanceNorm2d', 'norm_op_kwargs': {'eps': 1e-05, 'affine': True}, 'dropout_op': None, 'dropout_op_kwargs': None, 'nonlin': 'torch.nn.LeakyReLU', 'nonlin_kwargs': {'inplace': True}, 'deep_supervision': True}, '_kw_requires_import': ['conv_op', 'norm_op', 'dropout_op', 'nonlin']}, 'batch_dice': True} 

These are the global plan.json settings:
 {'dataset_name': 'Dataset302_Calcium_OCTv2', 'plans_name': 'nnUNetPlans', 'original_median_spacing_after_transp': [1.0, 1.0, 1.0], 'original_median_shape_after_transp': [375, 498, 498], 'image_reader_writer': 'SimpleITKIO', 'transpose_forward': [0, 1, 2], 'transpose_backward': [0, 1, 2], 'experiment_planner_used': 'ExperimentPlanner', 'label_manager': 'LabelManager', 'foreground_intensity_properties_per_channel': {'0': {'max': 0.4977976381778717, 'mean': 0.13332499563694, 'median': 0.08871842920780182, 'min': 0.0, 'percentile_00_5': 0.014754901640117168, 'percentile_99_5': 0.4977976381778717, 'std': 0.12022686749696732}}} 

2024-05-06 18:45:51.794628: unpacking dataset...
2024-05-06 18:45:58.074887: unpacking done...
2024-05-06 18:45:58.099175: Unable to plot network architecture: nnUNet_compile is enabled!
2024-05-06 18:45:58.608487: 
2024-05-06 18:45:58.611316: Epoch 0
2024-05-06 18:45:58.612969: Current learning rate: 0.01
2024-05-06 18:48:40.917440: Validation loss improved from 1000.00000 to -0.62746! Patience: 0/50
2024-05-06 18:48:40.918951: train_loss -0.5377
2024-05-06 18:48:40.920544: val_loss -0.6275
2024-05-06 18:48:40.921567: Pseudo dice [0.7245]
2024-05-06 18:48:40.922736: Epoch time: 162.31 s
2024-05-06 18:48:40.924068: Yayy! New best EMA pseudo Dice: 0.7245
2024-05-06 18:48:42.510369: 
2024-05-06 18:48:42.513068: Epoch 1
2024-05-06 18:48:42.514830: Current learning rate: 0.00999
2024-05-06 18:49:28.876909: Validation loss improved from -0.62746 to -0.65838! Patience: 0/50
2024-05-06 18:49:28.879256: train_loss -0.6889
2024-05-06 18:49:28.880638: val_loss -0.6584
2024-05-06 18:49:28.881747: Pseudo dice [0.7478]
2024-05-06 18:49:28.882740: Epoch time: 46.37 s
2024-05-06 18:49:28.883914: Yayy! New best EMA pseudo Dice: 0.7268
2024-05-06 18:49:30.654244: 
2024-05-06 18:49:30.657009: Epoch 2
2024-05-06 18:49:30.658326: Current learning rate: 0.00998
2024-05-06 18:50:17.015255: Validation loss did not improve from -0.65838. Patience: 1/50
2024-05-06 18:50:17.017498: train_loss -0.7288
2024-05-06 18:50:17.018867: val_loss -0.6552
2024-05-06 18:50:17.019966: Pseudo dice [0.7446]
2024-05-06 18:50:17.021183: Epoch time: 46.36 s
2024-05-06 18:50:17.022320: Yayy! New best EMA pseudo Dice: 0.7286
2024-05-06 18:50:18.850344: 
2024-05-06 18:50:18.853086: Epoch 3
2024-05-06 18:50:18.854736: Current learning rate: 0.00997
2024-05-06 18:51:05.139150: Validation loss did not improve from -0.65838. Patience: 2/50
2024-05-06 18:51:05.140892: train_loss -0.7385
2024-05-06 18:51:05.142045: val_loss -0.5164
2024-05-06 18:51:05.143100: Pseudo dice [0.6304]
2024-05-06 18:51:05.144150: Epoch time: 46.29 s
2024-05-06 18:51:06.415460: 
2024-05-06 18:51:06.417822: Epoch 4
2024-05-06 18:51:06.419421: Current learning rate: 0.00996
2024-05-06 18:51:52.753058: Validation loss did not improve from -0.65838. Patience: 3/50
2024-05-06 18:51:52.754755: train_loss -0.7514
2024-05-06 18:51:52.755769: val_loss -0.5754
2024-05-06 18:51:52.756702: Pseudo dice [0.6809]
2024-05-06 18:51:52.757724: Epoch time: 46.34 s
2024-05-06 18:51:54.504734: 
2024-05-06 18:51:54.506692: Epoch 5
2024-05-06 18:51:54.507827: Current learning rate: 0.00995
2024-05-06 18:52:40.854841: Validation loss did not improve from -0.65838. Patience: 4/50
2024-05-06 18:52:40.857043: train_loss -0.7735
2024-05-06 18:52:40.858060: val_loss -0.6532
2024-05-06 18:52:40.859077: Pseudo dice [0.7375]
2024-05-06 18:52:40.860262: Epoch time: 46.35 s
2024-05-06 18:52:42.107122: 
2024-05-06 18:52:42.109265: Epoch 6
2024-05-06 18:52:42.110367: Current learning rate: 0.00995
2024-05-06 18:53:28.429830: Validation loss did not improve from -0.65838. Patience: 5/50
2024-05-06 18:53:28.431569: train_loss -0.7821
2024-05-06 18:53:28.432599: val_loss -0.6219
2024-05-06 18:53:28.433614: Pseudo dice [0.723]
2024-05-06 18:53:28.434633: Epoch time: 46.33 s
2024-05-06 18:53:29.695900: 
2024-05-06 18:53:29.698053: Epoch 7
2024-05-06 18:53:29.699537: Current learning rate: 0.00994
2024-05-06 18:54:16.031386: Validation loss improved from -0.65838 to -0.66034! Patience: 5/50
2024-05-06 18:54:16.033671: train_loss -0.7908
2024-05-06 18:54:16.034988: val_loss -0.6603
2024-05-06 18:54:16.036290: Pseudo dice [0.7556]
2024-05-06 18:54:16.037464: Epoch time: 46.34 s
2024-05-06 18:54:17.345949: 
2024-05-06 18:54:17.348407: Epoch 8
2024-05-06 18:54:17.349763: Current learning rate: 0.00993
2024-05-06 18:55:03.821768: Validation loss improved from -0.66034 to -0.66175! Patience: 0/50
2024-05-06 18:55:03.823958: train_loss -0.7878
2024-05-06 18:55:03.825487: val_loss -0.6618
2024-05-06 18:55:03.826748: Pseudo dice [0.7468]
2024-05-06 18:55:03.827781: Epoch time: 46.48 s
2024-05-06 18:55:06.077979: 
2024-05-06 18:55:06.081072: Epoch 9
2024-05-06 18:55:06.083432: Current learning rate: 0.00992
2024-05-06 18:55:52.555128: Validation loss did not improve from -0.66175. Patience: 1/50
2024-05-06 18:55:52.557335: train_loss -0.8003
2024-05-06 18:55:52.559086: val_loss -0.6462
2024-05-06 18:55:52.560188: Pseudo dice [0.7445]
2024-05-06 18:55:52.561510: Epoch time: 46.48 s
2024-05-06 18:55:54.295659: 
2024-05-06 18:55:54.297960: Epoch 10
2024-05-06 18:55:54.299269: Current learning rate: 0.00991
2024-05-06 18:56:40.753622: Validation loss did not improve from -0.66175. Patience: 2/50
2024-05-06 18:56:40.756918: train_loss -0.7985
2024-05-06 18:56:40.758132: val_loss -0.6294
2024-05-06 18:56:40.759077: Pseudo dice [0.7291]
2024-05-06 18:56:40.760114: Epoch time: 46.46 s
2024-05-06 18:56:42.064343: 
2024-05-06 18:56:42.066248: Epoch 11
2024-05-06 18:56:42.067375: Current learning rate: 0.0099
2024-05-06 18:57:28.472550: Validation loss did not improve from -0.66175. Patience: 3/50
2024-05-06 18:57:28.474843: train_loss -0.803
2024-05-06 18:57:28.476506: val_loss -0.6407
2024-05-06 18:57:28.477803: Pseudo dice [0.7366]
2024-05-06 18:57:28.479031: Epoch time: 46.41 s
2024-05-06 18:57:29.730052: 
2024-05-06 18:57:29.732940: Epoch 12
2024-05-06 18:57:29.734706: Current learning rate: 0.00989
2024-05-06 18:58:16.118659: Validation loss improved from -0.66175 to -0.68207! Patience: 3/50
2024-05-06 18:58:16.120925: train_loss -0.8026
2024-05-06 18:58:16.122061: val_loss -0.6821
2024-05-06 18:58:16.123015: Pseudo dice [0.7663]
2024-05-06 18:58:16.124002: Epoch time: 46.39 s
2024-05-06 18:58:16.124872: Yayy! New best EMA pseudo Dice: 0.7313
2024-05-06 18:58:17.918135: 
2024-05-06 18:58:17.921165: Epoch 13
2024-05-06 18:58:17.922932: Current learning rate: 0.00988
2024-05-06 18:59:04.318815: Validation loss did not improve from -0.68207. Patience: 1/50
2024-05-06 18:59:04.325006: train_loss -0.8098
2024-05-06 18:59:04.326584: val_loss -0.6208
2024-05-06 18:59:04.327590: Pseudo dice [0.72]
2024-05-06 18:59:04.328760: Epoch time: 46.41 s
2024-05-06 18:59:05.631054: 
2024-05-06 18:59:05.632884: Epoch 14
2024-05-06 18:59:05.633940: Current learning rate: 0.00987
2024-05-06 18:59:52.040408: Validation loss did not improve from -0.68207. Patience: 2/50
2024-05-06 18:59:52.042519: train_loss -0.8134
2024-05-06 18:59:52.044052: val_loss -0.6819
2024-05-06 18:59:52.045216: Pseudo dice [0.7578]
2024-05-06 18:59:52.046350: Epoch time: 46.41 s
2024-05-06 18:59:52.565856: Yayy! New best EMA pseudo Dice: 0.733
2024-05-06 18:59:54.346504: 
2024-05-06 18:59:54.348457: Epoch 15
2024-05-06 18:59:54.349615: Current learning rate: 0.00986
2024-05-06 19:00:40.735198: Validation loss did not improve from -0.68207. Patience: 3/50
2024-05-06 19:00:40.737188: train_loss -0.8183
2024-05-06 19:00:40.738337: val_loss -0.6119
2024-05-06 19:00:40.739681: Pseudo dice [0.7197]
2024-05-06 19:00:40.740930: Epoch time: 46.39 s
2024-05-06 19:00:42.077213: 
2024-05-06 19:00:42.079940: Epoch 16
2024-05-06 19:00:42.081318: Current learning rate: 0.00986
2024-05-06 19:01:28.621285: Validation loss did not improve from -0.68207. Patience: 4/50
2024-05-06 19:01:28.623678: train_loss -0.8155
2024-05-06 19:01:28.625031: val_loss -0.6785
2024-05-06 19:01:28.626163: Pseudo dice [0.7661]
2024-05-06 19:01:28.627179: Epoch time: 46.55 s
2024-05-06 19:01:28.628176: Yayy! New best EMA pseudo Dice: 0.7351
2024-05-06 19:01:30.461719: 
2024-05-06 19:01:30.463697: Epoch 17
2024-05-06 19:01:30.464826: Current learning rate: 0.00985
2024-05-06 19:02:16.968123: Validation loss did not improve from -0.68207. Patience: 5/50
2024-05-06 19:02:16.970099: train_loss -0.8247
2024-05-06 19:02:16.971169: val_loss -0.6112
2024-05-06 19:02:16.972125: Pseudo dice [0.7227]
2024-05-06 19:02:16.973180: Epoch time: 46.51 s
2024-05-06 19:02:18.334135: 
2024-05-06 19:02:18.336109: Epoch 18
2024-05-06 19:02:18.337468: Current learning rate: 0.00984
2024-05-06 19:03:04.845333: Validation loss did not improve from -0.68207. Patience: 6/50
2024-05-06 19:03:04.848199: train_loss -0.8225
2024-05-06 19:03:04.849907: val_loss -0.5849
2024-05-06 19:03:04.850942: Pseudo dice [0.7006]
2024-05-06 19:03:04.852006: Epoch time: 46.52 s
2024-05-06 19:03:06.205754: 
2024-05-06 19:03:06.208523: Epoch 19
2024-05-06 19:03:06.210194: Current learning rate: 0.00983
2024-05-06 19:03:52.706585: Validation loss did not improve from -0.68207. Patience: 7/50
2024-05-06 19:03:52.708262: train_loss -0.8245
2024-05-06 19:03:52.709607: val_loss -0.6289
2024-05-06 19:03:52.710968: Pseudo dice [0.7302]
2024-05-06 19:03:52.712174: Epoch time: 46.5 s
2024-05-06 19:03:54.497265: 
2024-05-06 19:03:54.498730: Epoch 20
2024-05-06 19:03:54.500235: Current learning rate: 0.00982
2024-05-06 19:04:40.998638: Validation loss did not improve from -0.68207. Patience: 8/50
2024-05-06 19:04:41.001058: train_loss -0.8205
2024-05-06 19:04:41.002712: val_loss -0.6206
2024-05-06 19:04:41.003974: Pseudo dice [0.7236]
2024-05-06 19:04:41.005051: Epoch time: 46.51 s
2024-05-06 19:04:42.873940: 
2024-05-06 19:04:42.876779: Epoch 21
2024-05-06 19:04:42.878227: Current learning rate: 0.00981
2024-05-06 19:05:29.411975: Validation loss did not improve from -0.68207. Patience: 9/50
2024-05-06 19:05:29.414527: train_loss -0.8244
2024-05-06 19:05:29.416124: val_loss -0.6307
2024-05-06 19:05:29.417158: Pseudo dice [0.7316]
2024-05-06 19:05:29.418497: Epoch time: 46.54 s
2024-05-06 19:05:30.661252: 
2024-05-06 19:05:30.663150: Epoch 22
2024-05-06 19:05:30.664321: Current learning rate: 0.0098
2024-05-06 19:06:17.193406: Validation loss did not improve from -0.68207. Patience: 10/50
2024-05-06 19:06:17.195431: train_loss -0.8308
2024-05-06 19:06:17.196694: val_loss -0.5651
2024-05-06 19:06:17.197821: Pseudo dice [0.6833]
2024-05-06 19:06:17.199043: Epoch time: 46.54 s
2024-05-06 19:06:18.452659: 
2024-05-06 19:06:18.454689: Epoch 23
2024-05-06 19:06:18.456339: Current learning rate: 0.00979
2024-05-06 19:07:04.959125: Validation loss did not improve from -0.68207. Patience: 11/50
2024-05-06 19:07:04.961189: train_loss -0.8329
2024-05-06 19:07:04.962770: val_loss -0.5518
2024-05-06 19:07:04.964074: Pseudo dice [0.6697]
2024-05-06 19:07:04.965091: Epoch time: 46.51 s
2024-05-06 19:07:06.222353: 
2024-05-06 19:07:06.224659: Epoch 24
2024-05-06 19:07:06.225889: Current learning rate: 0.00978
2024-05-06 19:07:52.812725: Validation loss did not improve from -0.68207. Patience: 12/50
2024-05-06 19:07:52.814370: train_loss -0.8345
2024-05-06 19:07:52.815424: val_loss -0.635
2024-05-06 19:07:52.816567: Pseudo dice [0.7363]
2024-05-06 19:07:52.817578: Epoch time: 46.59 s
2024-05-06 19:07:54.745039: 
2024-05-06 19:07:54.747132: Epoch 25
2024-05-06 19:07:54.748304: Current learning rate: 0.00977
2024-05-06 19:08:41.294587: Validation loss did not improve from -0.68207. Patience: 13/50
2024-05-06 19:08:41.296843: train_loss -0.8382
2024-05-06 19:08:41.298219: val_loss -0.6265
2024-05-06 19:08:41.299340: Pseudo dice [0.7331]
2024-05-06 19:08:41.300353: Epoch time: 46.55 s
2024-05-06 19:08:42.582544: 
2024-05-06 19:08:42.585043: Epoch 26
2024-05-06 19:08:42.586099: Current learning rate: 0.00977
2024-05-06 19:09:29.167001: Validation loss improved from -0.68207 to -0.68919! Patience: 13/50
2024-05-06 19:09:29.168745: train_loss -0.8402
2024-05-06 19:09:29.170071: val_loss -0.6892
2024-05-06 19:09:29.171396: Pseudo dice [0.7749]
2024-05-06 19:09:29.172755: Epoch time: 46.59 s
2024-05-06 19:09:30.425717: 
2024-05-06 19:09:30.428453: Epoch 27
2024-05-06 19:09:30.430061: Current learning rate: 0.00976
2024-05-06 19:10:17.050168: Validation loss did not improve from -0.68919. Patience: 1/50
2024-05-06 19:10:17.052242: train_loss -0.8392
2024-05-06 19:10:17.053309: val_loss -0.6355
2024-05-06 19:10:17.054263: Pseudo dice [0.7325]
2024-05-06 19:10:17.055157: Epoch time: 46.63 s
2024-05-06 19:10:18.347126: 
2024-05-06 19:10:18.349144: Epoch 28
2024-05-06 19:10:18.350376: Current learning rate: 0.00975
2024-05-06 19:11:04.998014: Validation loss did not improve from -0.68919. Patience: 2/50
2024-05-06 19:11:05.000538: train_loss -0.8395
2024-05-06 19:11:05.002168: val_loss -0.5776
2024-05-06 19:11:05.003700: Pseudo dice [0.7016]
2024-05-06 19:11:05.005126: Epoch time: 46.65 s
2024-05-06 19:11:06.302295: 
2024-05-06 19:11:06.304825: Epoch 29
2024-05-06 19:11:06.306393: Current learning rate: 0.00974
2024-05-06 19:11:52.956711: Validation loss did not improve from -0.68919. Patience: 3/50
2024-05-06 19:11:52.958999: train_loss -0.8464
2024-05-06 19:11:52.960245: val_loss -0.5821
2024-05-06 19:11:52.961438: Pseudo dice [0.6957]
2024-05-06 19:11:52.962441: Epoch time: 46.66 s
2024-05-06 19:11:54.732749: 
2024-05-06 19:11:54.735555: Epoch 30
2024-05-06 19:11:54.737001: Current learning rate: 0.00973
2024-05-06 19:12:41.385639: Validation loss did not improve from -0.68919. Patience: 4/50
2024-05-06 19:12:41.387702: train_loss -0.8428
2024-05-06 19:12:41.389105: val_loss -0.6136
2024-05-06 19:12:41.390185: Pseudo dice [0.7181]
2024-05-06 19:12:41.391319: Epoch time: 46.66 s
2024-05-06 19:12:42.706056: 
2024-05-06 19:12:42.708263: Epoch 31
2024-05-06 19:12:42.709373: Current learning rate: 0.00972
2024-05-06 19:13:29.353919: Validation loss did not improve from -0.68919. Patience: 5/50
2024-05-06 19:13:29.355850: train_loss -0.8437
2024-05-06 19:13:29.357140: val_loss -0.6439
2024-05-06 19:13:29.358368: Pseudo dice [0.7398]
2024-05-06 19:13:29.359594: Epoch time: 46.65 s
2024-05-06 19:13:30.673901: 
2024-05-06 19:13:30.676662: Epoch 32
2024-05-06 19:13:30.678707: Current learning rate: 0.00971
2024-05-06 19:14:17.400361: Validation loss did not improve from -0.68919. Patience: 6/50
2024-05-06 19:14:17.402441: train_loss -0.8444
2024-05-06 19:14:17.403843: val_loss -0.6596
2024-05-06 19:14:17.405004: Pseudo dice [0.7548]
2024-05-06 19:14:17.406143: Epoch time: 46.73 s
2024-05-06 19:14:19.284367: 
2024-05-06 19:14:19.287154: Epoch 33
2024-05-06 19:14:19.288472: Current learning rate: 0.0097
2024-05-06 19:15:05.975023: Validation loss did not improve from -0.68919. Patience: 7/50
2024-05-06 19:15:05.976796: train_loss -0.8472
2024-05-06 19:15:05.977769: val_loss -0.6252
2024-05-06 19:15:05.978821: Pseudo dice [0.728]
2024-05-06 19:15:05.979883: Epoch time: 46.69 s
2024-05-06 19:15:07.285666: 
2024-05-06 19:15:07.287831: Epoch 34
2024-05-06 19:15:07.289477: Current learning rate: 0.00969
2024-05-06 19:15:53.958594: Validation loss improved from -0.68919 to -0.70445! Patience: 7/50
2024-05-06 19:15:53.960393: train_loss -0.8483
2024-05-06 19:15:53.961447: val_loss -0.7045
2024-05-06 19:15:53.962475: Pseudo dice [0.7836]
2024-05-06 19:15:53.963485: Epoch time: 46.68 s
2024-05-06 19:15:55.793675: 
2024-05-06 19:15:55.795951: Epoch 35
2024-05-06 19:15:55.797615: Current learning rate: 0.00968
2024-05-06 19:16:42.434043: Validation loss did not improve from -0.70445. Patience: 1/50
2024-05-06 19:16:42.436414: train_loss -0.8501
2024-05-06 19:16:42.437567: val_loss -0.6555
2024-05-06 19:16:42.438564: Pseudo dice [0.7443]
2024-05-06 19:16:42.439675: Epoch time: 46.64 s
2024-05-06 19:16:43.782387: 
2024-05-06 19:16:43.784796: Epoch 36
2024-05-06 19:16:43.786515: Current learning rate: 0.00968
2024-05-06 19:17:30.433470: Validation loss did not improve from -0.70445. Patience: 2/50
2024-05-06 19:17:30.435295: train_loss -0.847
2024-05-06 19:17:30.436438: val_loss -0.6571
2024-05-06 19:17:30.437545: Pseudo dice [0.7479]
2024-05-06 19:17:30.438604: Epoch time: 46.65 s
2024-05-06 19:17:30.439638: Yayy! New best EMA pseudo Dice: 0.7353
2024-05-06 19:17:32.382205: 
2024-05-06 19:17:32.385760: Epoch 37
2024-05-06 19:17:32.386873: Current learning rate: 0.00967
2024-05-06 19:18:19.046988: Validation loss did not improve from -0.70445. Patience: 3/50
2024-05-06 19:18:19.048908: train_loss -0.8476
2024-05-06 19:18:19.050812: val_loss -0.5515
2024-05-06 19:18:19.052573: Pseudo dice [0.6697]
2024-05-06 19:18:19.053681: Epoch time: 46.67 s
2024-05-06 19:18:20.391586: 
2024-05-06 19:18:20.394112: Epoch 38
2024-05-06 19:18:20.395446: Current learning rate: 0.00966
2024-05-06 19:19:07.031229: Validation loss did not improve from -0.70445. Patience: 4/50
2024-05-06 19:19:07.033828: train_loss -0.8487
2024-05-06 19:19:07.035325: val_loss -0.6329
2024-05-06 19:19:07.036460: Pseudo dice [0.7358]
2024-05-06 19:19:07.037640: Epoch time: 46.64 s
2024-05-06 19:19:08.357100: 
2024-05-06 19:19:08.358772: Epoch 39
2024-05-06 19:19:08.360407: Current learning rate: 0.00965
2024-05-06 19:19:55.060439: Validation loss did not improve from -0.70445. Patience: 5/50
2024-05-06 19:19:55.062176: train_loss -0.8548
2024-05-06 19:19:55.063351: val_loss -0.655
2024-05-06 19:19:55.064666: Pseudo dice [0.7509]
2024-05-06 19:19:55.065848: Epoch time: 46.71 s
2024-05-06 19:19:56.988876: 
2024-05-06 19:19:56.991256: Epoch 40
2024-05-06 19:19:56.992779: Current learning rate: 0.00964
2024-05-06 19:20:43.626824: Validation loss did not improve from -0.70445. Patience: 6/50
2024-05-06 19:20:43.629054: train_loss -0.8505
2024-05-06 19:20:43.630214: val_loss -0.6642
2024-05-06 19:20:43.631494: Pseudo dice [0.753]
2024-05-06 19:20:43.632447: Epoch time: 46.64 s
2024-05-06 19:20:45.001972: 
2024-05-06 19:20:45.003742: Epoch 41
2024-05-06 19:20:45.005228: Current learning rate: 0.00963
2024-05-06 19:21:31.675311: Validation loss did not improve from -0.70445. Patience: 7/50
2024-05-06 19:21:31.677709: train_loss -0.8508
2024-05-06 19:21:31.679423: val_loss -0.6526
2024-05-06 19:21:31.680524: Pseudo dice [0.7428]
2024-05-06 19:21:31.681822: Epoch time: 46.68 s
2024-05-06 19:21:32.943262: 
2024-05-06 19:21:32.945987: Epoch 42
2024-05-06 19:21:32.947804: Current learning rate: 0.00962
2024-05-06 19:22:19.629305: Validation loss did not improve from -0.70445. Patience: 8/50
2024-05-06 19:22:19.631150: train_loss -0.854
2024-05-06 19:22:19.632333: val_loss -0.6451
2024-05-06 19:22:19.633449: Pseudo dice [0.749]
2024-05-06 19:22:19.634614: Epoch time: 46.69 s
2024-05-06 19:22:19.635755: Yayy! New best EMA pseudo Dice: 0.7361
2024-05-06 19:22:21.500899: 
2024-05-06 19:22:21.504014: Epoch 43
2024-05-06 19:22:21.505985: Current learning rate: 0.00961
2024-05-06 19:23:08.105808: Validation loss did not improve from -0.70445. Patience: 9/50
2024-05-06 19:23:08.107820: train_loss -0.854
2024-05-06 19:23:08.109184: val_loss -0.5844
2024-05-06 19:23:08.110378: Pseudo dice [0.6956]
2024-05-06 19:23:08.111396: Epoch time: 46.61 s
2024-05-06 19:23:09.815698: 
2024-05-06 19:23:09.818337: Epoch 44
2024-05-06 19:23:09.819956: Current learning rate: 0.0096
2024-05-06 19:23:56.421192: Validation loss did not improve from -0.70445. Patience: 10/50
2024-05-06 19:23:56.423747: train_loss -0.86
2024-05-06 19:23:56.425208: val_loss -0.5332
2024-05-06 19:23:56.426361: Pseudo dice [0.6728]
2024-05-06 19:23:56.427519: Epoch time: 46.61 s
2024-05-06 19:23:58.225611: 
2024-05-06 19:23:58.228297: Epoch 45
2024-05-06 19:23:58.229816: Current learning rate: 0.00959
2024-05-06 19:24:44.801608: Validation loss did not improve from -0.70445. Patience: 11/50
2024-05-06 19:24:44.804574: train_loss -0.8571
2024-05-06 19:24:44.805936: val_loss -0.6382
2024-05-06 19:24:44.807301: Pseudo dice [0.7378]
2024-05-06 19:24:44.808428: Epoch time: 46.58 s
2024-05-06 19:24:46.075085: 
2024-05-06 19:24:46.079634: Epoch 46
2024-05-06 19:24:46.081807: Current learning rate: 0.00959
2024-05-06 19:25:32.655284: Validation loss did not improve from -0.70445. Patience: 12/50
2024-05-06 19:25:32.658591: train_loss -0.8581
2024-05-06 19:25:32.660072: val_loss -0.5592
2024-05-06 19:25:32.661241: Pseudo dice [0.6836]
2024-05-06 19:25:32.662439: Epoch time: 46.58 s
2024-05-06 19:25:33.940180: 
2024-05-06 19:25:33.943822: Epoch 47
2024-05-06 19:25:33.945428: Current learning rate: 0.00958
2024-05-06 19:26:20.510966: Validation loss did not improve from -0.70445. Patience: 13/50
2024-05-06 19:26:20.514126: train_loss -0.8624
2024-05-06 19:26:20.515538: val_loss -0.5991
2024-05-06 19:26:20.516644: Pseudo dice [0.7096]
2024-05-06 19:26:20.517741: Epoch time: 46.58 s
2024-05-06 19:26:21.774744: 
2024-05-06 19:26:21.777745: Epoch 48
2024-05-06 19:26:21.779520: Current learning rate: 0.00957
2024-05-06 19:27:08.316785: Validation loss did not improve from -0.70445. Patience: 14/50
2024-05-06 19:27:08.320336: train_loss -0.8627
2024-05-06 19:27:08.321914: val_loss -0.6094
2024-05-06 19:27:08.323177: Pseudo dice [0.7221]
2024-05-06 19:27:08.324447: Epoch time: 46.55 s
2024-05-06 19:27:09.594527: 
2024-05-06 19:27:09.598706: Epoch 49
2024-05-06 19:27:09.600203: Current learning rate: 0.00956
2024-05-06 19:27:56.174721: Validation loss did not improve from -0.70445. Patience: 15/50
2024-05-06 19:27:56.178481: train_loss -0.8612
2024-05-06 19:27:56.179676: val_loss -0.6242
2024-05-06 19:27:56.181007: Pseudo dice [0.7259]
2024-05-06 19:27:56.182243: Epoch time: 46.59 s
2024-05-06 19:27:57.922920: 
2024-05-06 19:27:57.927043: Epoch 50
2024-05-06 19:27:57.929003: Current learning rate: 0.00955
2024-05-06 19:28:44.496436: Validation loss did not improve from -0.70445. Patience: 16/50
2024-05-06 19:28:44.499612: train_loss -0.8617
2024-05-06 19:28:44.500973: val_loss -0.5678
2024-05-06 19:28:44.502038: Pseudo dice [0.7001]
2024-05-06 19:28:44.503171: Epoch time: 46.58 s
2024-05-06 19:28:45.899741: 
2024-05-06 19:28:45.903434: Epoch 51
2024-05-06 19:28:45.904529: Current learning rate: 0.00954
2024-05-06 19:29:32.525162: Validation loss did not improve from -0.70445. Patience: 17/50
2024-05-06 19:29:32.528534: train_loss -0.8626
2024-05-06 19:29:32.530417: val_loss -0.5998
2024-05-06 19:29:32.531410: Pseudo dice [0.7183]
2024-05-06 19:29:32.532424: Epoch time: 46.63 s
2024-05-06 19:29:33.829324: 
2024-05-06 19:29:33.832139: Epoch 52
2024-05-06 19:29:33.833809: Current learning rate: 0.00953
2024-05-06 19:30:20.391627: Validation loss did not improve from -0.70445. Patience: 18/50
2024-05-06 19:30:20.395125: train_loss -0.8618
2024-05-06 19:30:20.396449: val_loss -0.6503
2024-05-06 19:30:20.397573: Pseudo dice [0.7482]
2024-05-06 19:30:20.398654: Epoch time: 46.57 s
2024-05-06 19:30:21.668372: 
2024-05-06 19:30:21.671933: Epoch 53
2024-05-06 19:30:21.673366: Current learning rate: 0.00952
2024-05-06 19:31:08.253824: Validation loss did not improve from -0.70445. Patience: 19/50
2024-05-06 19:31:08.257078: train_loss -0.8585
2024-05-06 19:31:08.258381: val_loss -0.5612
2024-05-06 19:31:08.259513: Pseudo dice [0.6869]
2024-05-06 19:31:08.260599: Epoch time: 46.59 s
2024-05-06 19:31:09.587903: 
2024-05-06 19:31:09.591951: Epoch 54
2024-05-06 19:31:09.593471: Current learning rate: 0.00951
2024-05-06 19:31:56.179068: Validation loss did not improve from -0.70445. Patience: 20/50
2024-05-06 19:31:56.182321: train_loss -0.8614
2024-05-06 19:31:56.183820: val_loss -0.6501
2024-05-06 19:31:56.184940: Pseudo dice [0.7467]
2024-05-06 19:31:56.186194: Epoch time: 46.6 s
2024-05-06 19:31:58.003093: 
2024-05-06 19:31:58.006799: Epoch 55
2024-05-06 19:31:58.008392: Current learning rate: 0.0095
2024-05-06 19:32:44.576419: Validation loss did not improve from -0.70445. Patience: 21/50
2024-05-06 19:32:44.579595: train_loss -0.8637
2024-05-06 19:32:44.581058: val_loss -0.6046
2024-05-06 19:32:44.582359: Pseudo dice [0.7222]
2024-05-06 19:32:44.583677: Epoch time: 46.58 s
2024-05-06 19:32:46.328727: 
2024-05-06 19:32:46.332863: Epoch 56
2024-05-06 19:32:46.334508: Current learning rate: 0.00949
2024-05-06 19:33:32.933301: Validation loss did not improve from -0.70445. Patience: 22/50
2024-05-06 19:33:32.937138: train_loss -0.867
2024-05-06 19:33:32.938741: val_loss -0.6203
2024-05-06 19:33:32.939844: Pseudo dice [0.7291]
2024-05-06 19:33:32.940917: Epoch time: 46.61 s
2024-05-06 19:33:34.252323: 
2024-05-06 19:33:34.256305: Epoch 57
2024-05-06 19:33:34.257724: Current learning rate: 0.00949
2024-05-06 19:34:20.957619: Validation loss did not improve from -0.70445. Patience: 23/50
2024-05-06 19:34:20.960982: train_loss -0.8692
2024-05-06 19:34:20.962562: val_loss -0.6186
2024-05-06 19:34:20.963848: Pseudo dice [0.7238]
2024-05-06 19:34:20.965023: Epoch time: 46.71 s
2024-05-06 19:34:22.257963: 
2024-05-06 19:34:22.262733: Epoch 58
2024-05-06 19:34:22.264556: Current learning rate: 0.00948
2024-05-06 19:35:08.978542: Validation loss did not improve from -0.70445. Patience: 24/50
2024-05-06 19:35:08.981426: train_loss -0.8679
2024-05-06 19:35:08.982563: val_loss -0.6093
2024-05-06 19:35:08.983662: Pseudo dice [0.7224]
2024-05-06 19:35:08.984683: Epoch time: 46.73 s
2024-05-06 19:35:10.342613: 
2024-05-06 19:35:10.347436: Epoch 59
2024-05-06 19:35:10.349179: Current learning rate: 0.00947
2024-05-06 19:35:57.011895: Validation loss did not improve from -0.70445. Patience: 25/50
2024-05-06 19:35:57.015188: train_loss -0.8678
2024-05-06 19:35:57.016263: val_loss -0.6097
2024-05-06 19:35:57.017351: Pseudo dice [0.7199]
2024-05-06 19:35:57.018408: Epoch time: 46.67 s
2024-05-06 19:35:58.874007: 
2024-05-06 19:35:58.877846: Epoch 60
2024-05-06 19:35:58.879372: Current learning rate: 0.00946
2024-05-06 19:36:45.622702: Validation loss did not improve from -0.70445. Patience: 26/50
2024-05-06 19:36:45.626302: train_loss -0.8658
2024-05-06 19:36:45.627654: val_loss -0.5936
2024-05-06 19:36:45.629000: Pseudo dice [0.7163]
2024-05-06 19:36:45.630181: Epoch time: 46.75 s
2024-05-06 19:36:46.936146: 
2024-05-06 19:36:46.939352: Epoch 61
2024-05-06 19:36:46.941263: Current learning rate: 0.00945
2024-05-06 19:37:33.704034: Validation loss did not improve from -0.70445. Patience: 27/50
2024-05-06 19:37:33.707309: train_loss -0.8679
2024-05-06 19:37:33.708421: val_loss -0.608
2024-05-06 19:37:33.709358: Pseudo dice [0.7222]
2024-05-06 19:37:33.710256: Epoch time: 46.77 s
2024-05-06 19:37:35.046175: 
2024-05-06 19:37:35.050215: Epoch 62
2024-05-06 19:37:35.051560: Current learning rate: 0.00944
2024-05-06 19:38:21.760358: Validation loss did not improve from -0.70445. Patience: 28/50
2024-05-06 19:38:21.764908: train_loss -0.8681
2024-05-06 19:38:21.766875: val_loss -0.6077
2024-05-06 19:38:21.768039: Pseudo dice [0.7271]
2024-05-06 19:38:21.769201: Epoch time: 46.72 s
2024-05-06 19:38:23.097709: 
2024-05-06 19:38:23.102309: Epoch 63
2024-05-06 19:38:23.104438: Current learning rate: 0.00943
2024-05-06 19:39:09.821764: Validation loss did not improve from -0.70445. Patience: 29/50
2024-05-06 19:39:09.824535: train_loss -0.8675
2024-05-06 19:39:09.825878: val_loss -0.6694
2024-05-06 19:39:09.826997: Pseudo dice [0.7644]
2024-05-06 19:39:09.828064: Epoch time: 46.73 s
2024-05-06 19:39:11.153506: 
2024-05-06 19:39:11.157356: Epoch 64
2024-05-06 19:39:11.159487: Current learning rate: 0.00942
2024-05-06 19:39:57.919899: Validation loss did not improve from -0.70445. Patience: 30/50
2024-05-06 19:39:57.923033: train_loss -0.8701
2024-05-06 19:39:57.924831: val_loss -0.6243
2024-05-06 19:39:57.925960: Pseudo dice [0.7347]
2024-05-06 19:39:57.927001: Epoch time: 46.77 s
2024-05-06 19:39:59.762940: 
2024-05-06 19:39:59.766358: Epoch 65
2024-05-06 19:39:59.768735: Current learning rate: 0.00941
2024-05-06 19:40:46.484993: Validation loss did not improve from -0.70445. Patience: 31/50
2024-05-06 19:40:46.488377: train_loss -0.8744
2024-05-06 19:40:46.489564: val_loss -0.6038
2024-05-06 19:40:46.490689: Pseudo dice [0.7175]
2024-05-06 19:40:46.491779: Epoch time: 46.73 s
2024-05-06 19:40:47.802288: 
2024-05-06 19:40:47.807092: Epoch 66
2024-05-06 19:40:47.808975: Current learning rate: 0.0094
2024-05-06 19:41:34.504868: Validation loss did not improve from -0.70445. Patience: 32/50
2024-05-06 19:41:34.508624: train_loss -0.8731
2024-05-06 19:41:34.510210: val_loss -0.5919
2024-05-06 19:41:34.511329: Pseudo dice [0.7217]
2024-05-06 19:41:34.512305: Epoch time: 46.71 s
2024-05-06 19:41:35.833781: 
2024-05-06 19:41:35.837061: Epoch 67
2024-05-06 19:41:35.838359: Current learning rate: 0.00939
2024-05-06 19:42:22.567147: Validation loss did not improve from -0.70445. Patience: 33/50
2024-05-06 19:42:22.571788: train_loss -0.872
2024-05-06 19:42:22.573526: val_loss -0.5995
2024-05-06 19:42:22.574512: Pseudo dice [0.7143]
2024-05-06 19:42:22.575603: Epoch time: 46.74 s
2024-05-06 19:42:24.329643: 
2024-05-06 19:42:24.332476: Epoch 68
2024-05-06 19:42:24.333511: Current learning rate: 0.00939
2024-05-06 19:43:11.061997: Validation loss did not improve from -0.70445. Patience: 34/50
2024-05-06 19:43:11.065959: train_loss -0.8763
2024-05-06 19:43:11.067581: val_loss -0.5587
2024-05-06 19:43:11.068934: Pseudo dice [0.6892]
2024-05-06 19:43:11.070186: Epoch time: 46.74 s
2024-05-06 19:43:12.426008: 
2024-05-06 19:43:12.429460: Epoch 69
2024-05-06 19:43:12.430977: Current learning rate: 0.00938
2024-05-06 19:43:59.065744: Validation loss did not improve from -0.70445. Patience: 35/50
2024-05-06 19:43:59.069062: train_loss -0.8768
2024-05-06 19:43:59.070512: val_loss -0.6278
2024-05-06 19:43:59.071590: Pseudo dice [0.7327]
2024-05-06 19:43:59.072705: Epoch time: 46.64 s
2024-05-06 19:44:00.932675: 
2024-05-06 19:44:00.936871: Epoch 70
2024-05-06 19:44:00.941329: Current learning rate: 0.00937
2024-05-06 19:44:47.586766: Validation loss did not improve from -0.70445. Patience: 36/50
2024-05-06 19:44:47.589576: train_loss -0.8759
2024-05-06 19:44:47.590739: val_loss -0.6569
2024-05-06 19:44:47.591789: Pseudo dice [0.7551]
2024-05-06 19:44:47.592893: Epoch time: 46.66 s
2024-05-06 19:44:48.934823: 
2024-05-06 19:44:48.938597: Epoch 71
2024-05-06 19:44:48.939918: Current learning rate: 0.00936
2024-05-06 19:45:35.618090: Validation loss did not improve from -0.70445. Patience: 37/50
2024-05-06 19:45:35.621045: train_loss -0.8764
2024-05-06 19:45:35.622228: val_loss -0.6279
2024-05-06 19:45:35.623210: Pseudo dice [0.7318]
2024-05-06 19:45:35.624129: Epoch time: 46.69 s
2024-05-06 19:45:36.986759: 
2024-05-06 19:45:36.990323: Epoch 72
2024-05-06 19:45:36.992139: Current learning rate: 0.00935
2024-05-06 19:46:23.671031: Validation loss did not improve from -0.70445. Patience: 38/50
2024-05-06 19:46:23.674238: train_loss -0.8771
2024-05-06 19:46:23.675819: val_loss -0.5849
2024-05-06 19:46:23.677059: Pseudo dice [0.7077]
2024-05-06 19:46:23.678221: Epoch time: 46.69 s
2024-05-06 19:46:25.000220: 
2024-05-06 19:46:25.004204: Epoch 73
2024-05-06 19:46:25.006370: Current learning rate: 0.00934
2024-05-06 19:47:11.651811: Validation loss did not improve from -0.70445. Patience: 39/50
2024-05-06 19:47:11.655126: train_loss -0.8755
2024-05-06 19:47:11.656323: val_loss -0.6088
2024-05-06 19:47:11.657366: Pseudo dice [0.7249]
2024-05-06 19:47:11.658330: Epoch time: 46.66 s
2024-05-06 19:47:12.988219: 
2024-05-06 19:47:12.992330: Epoch 74
2024-05-06 19:47:12.994792: Current learning rate: 0.00933
2024-05-06 19:47:59.630957: Validation loss did not improve from -0.70445. Patience: 40/50
2024-05-06 19:47:59.634256: train_loss -0.8772
2024-05-06 19:47:59.635322: val_loss -0.5551
2024-05-06 19:47:59.636224: Pseudo dice [0.6821]
2024-05-06 19:47:59.637076: Epoch time: 46.65 s
2024-05-06 19:48:01.479409: 
2024-05-06 19:48:01.483688: Epoch 75
2024-05-06 19:48:01.484903: Current learning rate: 0.00932
2024-05-06 19:48:48.165365: Validation loss did not improve from -0.70445. Patience: 41/50
2024-05-06 19:48:48.168944: train_loss -0.8756
2024-05-06 19:48:48.170546: val_loss -0.5841
2024-05-06 19:48:48.171901: Pseudo dice [0.7159]
2024-05-06 19:48:48.173322: Epoch time: 46.69 s
2024-05-06 19:48:49.525245: 
2024-05-06 19:48:49.528840: Epoch 76
2024-05-06 19:48:49.530279: Current learning rate: 0.00931
2024-05-06 19:49:36.192167: Validation loss did not improve from -0.70445. Patience: 42/50
2024-05-06 19:49:36.195114: train_loss -0.874
2024-05-06 19:49:36.196409: val_loss -0.6391
2024-05-06 19:49:36.197598: Pseudo dice [0.7486]
2024-05-06 19:49:36.198701: Epoch time: 46.67 s
2024-05-06 19:49:37.546252: 
2024-05-06 19:49:37.549206: Epoch 77
2024-05-06 19:49:37.550393: Current learning rate: 0.0093
2024-05-06 19:50:24.419545: Validation loss did not improve from -0.70445. Patience: 43/50
2024-05-06 19:50:24.422997: train_loss -0.874
2024-05-06 19:50:24.424529: val_loss -0.6331
2024-05-06 19:50:24.425744: Pseudo dice [0.739]
2024-05-06 19:50:24.427011: Epoch time: 46.88 s
2024-05-06 19:50:25.797036: 
2024-05-06 19:50:25.801049: Epoch 78
2024-05-06 19:50:25.802686: Current learning rate: 0.0093
2024-05-06 19:51:12.677620: Validation loss did not improve from -0.70445. Patience: 44/50
2024-05-06 19:51:12.682504: train_loss -0.8761
2024-05-06 19:51:12.684126: val_loss -0.5815
2024-05-06 19:51:12.685715: Pseudo dice [0.7042]
2024-05-06 19:51:12.687123: Epoch time: 46.89 s
2024-05-06 19:51:14.493662: 
2024-05-06 19:51:14.497317: Epoch 79
2024-05-06 19:51:14.498868: Current learning rate: 0.00929
2024-05-06 19:52:02.236050: Validation loss did not improve from -0.70445. Patience: 45/50
2024-05-06 19:52:02.240750: train_loss -0.8756
2024-05-06 19:52:02.242313: val_loss -0.6248
2024-05-06 19:52:02.243358: Pseudo dice [0.7291]
2024-05-06 19:52:02.244286: Epoch time: 47.75 s
2024-05-06 19:52:04.296715: 
2024-05-06 19:52:04.299862: Epoch 80
2024-05-06 19:52:04.301668: Current learning rate: 0.00928
2024-05-06 19:52:51.152149: Validation loss did not improve from -0.70445. Patience: 46/50
2024-05-06 19:52:51.155621: train_loss -0.8813
2024-05-06 19:52:51.157107: val_loss -0.5563
2024-05-06 19:52:51.158154: Pseudo dice [0.6907]
2024-05-06 19:52:51.159220: Epoch time: 46.86 s
2024-05-06 19:52:52.546384: 
2024-05-06 19:52:52.549814: Epoch 81
2024-05-06 19:52:52.551564: Current learning rate: 0.00927
2024-05-06 19:53:40.848912: Validation loss did not improve from -0.70445. Patience: 47/50
2024-05-06 19:53:40.853937: train_loss -0.88
2024-05-06 19:53:40.855924: val_loss -0.5726
2024-05-06 19:53:40.856968: Pseudo dice [0.6978]
2024-05-06 19:53:40.858184: Epoch time: 48.31 s
2024-05-06 19:53:42.334364: 
2024-05-06 19:53:42.338533: Epoch 82
2024-05-06 19:53:42.339584: Current learning rate: 0.00926
2024-05-06 19:54:29.126261: Validation loss did not improve from -0.70445. Patience: 48/50
2024-05-06 19:54:29.130311: train_loss -0.8769
2024-05-06 19:54:29.131655: val_loss -0.5872
2024-05-06 19:54:29.132957: Pseudo dice [0.7087]
2024-05-06 19:54:29.134231: Epoch time: 46.8 s
2024-05-06 19:54:30.418144: 
2024-05-06 19:54:30.421988: Epoch 83
2024-05-06 19:54:30.423523: Current learning rate: 0.00925
2024-05-06 19:55:17.166142: Validation loss did not improve from -0.70445. Patience: 49/50
2024-05-06 19:55:17.169744: train_loss -0.8791
2024-05-06 19:55:17.171247: val_loss -0.5305
2024-05-06 19:55:17.172313: Pseudo dice [0.6715]
2024-05-06 19:55:17.173335: Epoch time: 46.75 s
2024-05-06 19:55:18.467967: 
2024-05-06 19:55:18.471604: Epoch 84
2024-05-06 19:55:18.473063: Current learning rate: 0.00924
2024-05-06 19:56:05.297105: Validation loss did not improve from -0.70445. Patience: 50/50
2024-05-06 19:56:05.300661: train_loss -0.8799
2024-05-06 19:56:05.302232: val_loss -0.5802
2024-05-06 19:56:05.303443: Pseudo dice [0.708]
2024-05-06 19:56:05.304537: Epoch time: 46.83 s
2024-05-06 19:56:07.140007: Patience reached. Stopping training.
2024-05-06 19:56:07.680362: Training done.
2024-05-06 19:56:08.181445: Using splits from existing split file: /home/gridsan/nchutisilp/datasets/nnUNet_Datasets/nnUNet_preprocessed/Dataset302_Calcium_OCTv2/splits_final.json
2024-05-06 19:56:08.184657: The split file contains 3 splits.
2024-05-06 19:56:08.185632: Desired fold for training: 2
2024-05-06 19:56:08.186557: This split has 4 training and 2 validation cases.
2024-05-06 19:56:08.187608: predicting 401-004
2024-05-06 19:56:08.207530: 401-004, shape torch.Size([1, 375, 498, 498]), rank 0
2024-05-06 19:56:42.414190: predicting 701-013
2024-05-06 19:56:42.439667: 701-013, shape torch.Size([1, 375, 498, 498]), rank 0
2024-05-06 19:57:20.589904: Validation complete
2024-05-06 19:57:20.593860: Mean Validation Dice:  0.6923061777282851
wandb: 
wandb: Run history:
wandb:            ema_fg_dice ▅▆▂▃▅▅▇▇█▇▆▃▄▅▄▆▇█▆█▇▆▄▄▃▃▄▄▄▄▆▅▄▅▅▃▅▄▃▁
wandb:   epoch_end_timestamps ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇████
wandb: epoch_start_timestamps ▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███
wandb:                    lrs ████▇▇▇▇▇▆▆▆▆▆▆▅▅▅▅▅▄▄▄▄▄▄▃▃▃▃▃▃▂▂▂▂▂▁▁▁
wandb:           mean_fg_dice ▄▆▂▄▆▅▇▄▄▅▅▁▅▃▄▆█▆▅▆▃▅▃▄▄▂▅▄▄▅▅▄▂▅▄▄▅▅▃▃
wandb:           train_losses █▄▄▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:             val_losses ▅▃▇▅▃▄▂▅▅▄▄█▅▇▅▃▁▃▄▃▇▄▆▅▆█▅▅▆▅▅▆█▅▅▇▄▅▇▇
wandb: 
wandb: Run summary:
wandb:            ema_fg_dice 0.71173
wandb:   epoch_end_timestamps 1715039765.30047
wandb: epoch_start_timestamps 1715039718.46639
wandb:                    lrs 0.00924
wandb:           mean_fg_dice 0.70802
wandb:           train_losses -0.87988
wandb:             val_losses -0.58019
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/gridsan/nchutisilp/datasets/nnUNet_Datasets/nnUNet_results/Dataset302_Calcium_OCTv2/nnUNetTrainer__nnUNetPlans__2d/fold_2/wandb/offline-run-20240506_184527-34g2q60t
wandb: Find logs at: /home/gridsan/nchutisilp/datasets/nnUNet_Datasets/nnUNet_results/Dataset302_Calcium_OCTv2/nnUNetTrainer__nnUNetPlans__2d/fold_2/wandb/offline-run-20240506_184527-34g2q60t/logs
WARNING:urllib3.connectionpool:Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x7f2a65d32610>: Failed to establish a new connection: [Errno 101] Network is unreachable')': /api/4504800232407040/envelope/
WARNING:urllib3.connectionpool:Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x7f2a6d40aa00>: Failed to establish a new connection: [Errno 101] Network is unreachable')': /api/4504800232407040/envelope/
WARNING:urllib3.connectionpool:Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x7f298a8d6490>: Failed to establish a new connection: [Errno 101] Network is unreachable')': /api/4504800232407040/envelope/
WARNING:urllib3.connectionpool:Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x7f29b0642730>: Failed to establish a new connection: [Errno 101] Network is unreachable')': /api/4504800232407040/envelope/
WARNING:urllib3.connectionpool:Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x7f2a42ed8a90>: Failed to establish a new connection: [Errno 101] Network is unreachable')': /api/4504800232407040/envelope/
WARNING:urllib3.connectionpool:Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x7f2a42d1c160>: Failed to establish a new connection: [Errno 101] Network is unreachable')': /api/4504800232407040/envelope/
