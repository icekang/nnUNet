/home/gridsan/nchutisilp/.conda/envs/nnunet/bin/python
wandb: Tracking run with wandb version 0.16.6
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.

############################
INFO: You are using the old nnU-Net default plans. We have updated our recommendations. Please consider using those instead! Read more here: https://github.com/MIC-DKFZ/nnUNet/blob/master/documentation/resenc_presets.md
############################

Using device: cuda:0

#######################################################################
Please cite the following paper when using nnU-Net:
Isensee, F., Jaeger, P. F., Kohl, S. A., Petersen, J., & Maier-Hein, K. H. (2021). nnU-Net: a self-configuring method for deep learning-based biomedical image segmentation. Nature methods, 18(2), 203-211.
#######################################################################

2024-07-24 02:23:56.197374: Using torch.compile...
/home/gridsan/nchutisilp/.local/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn("The verbose parameter is deprecated. Please use get_last_lr() "
################### Loading pretrained weights from file  /home/gridsan/nchutisilp/projects/OpenAI-CLIP/clip_pretrained_nnUNet.pt ###################
Below is the list of overlapping blocks in pretrained model and nnUNet architecture:
encoder.stages.0.0.convs.0.conv.weight shape torch.Size([32, 1, 3, 3, 3])
encoder.stages.0.0.convs.0.conv.bias shape torch.Size([32])
encoder.stages.0.0.convs.0.norm.weight shape torch.Size([32])
encoder.stages.0.0.convs.0.norm.bias shape torch.Size([32])
encoder.stages.0.0.convs.0.all_modules.0.weight shape torch.Size([32, 1, 3, 3, 3])
encoder.stages.0.0.convs.0.all_modules.0.bias shape torch.Size([32])
encoder.stages.0.0.convs.0.all_modules.1.weight shape torch.Size([32])
encoder.stages.0.0.convs.0.all_modules.1.bias shape torch.Size([32])
encoder.stages.0.0.convs.1.conv.weight shape torch.Size([32, 32, 3, 3, 3])
encoder.stages.0.0.convs.1.conv.bias shape torch.Size([32])
encoder.stages.0.0.convs.1.norm.weight shape torch.Size([32])
encoder.stages.0.0.convs.1.norm.bias shape torch.Size([32])
encoder.stages.0.0.convs.1.all_modules.0.weight shape torch.Size([32, 32, 3, 3, 3])
encoder.stages.0.0.convs.1.all_modules.0.bias shape torch.Size([32])
encoder.stages.0.0.convs.1.all_modules.1.weight shape torch.Size([32])
encoder.stages.0.0.convs.1.all_modules.1.bias shape torch.Size([32])
encoder.stages.1.0.convs.0.conv.weight shape torch.Size([64, 32, 3, 3, 3])
encoder.stages.1.0.convs.0.conv.bias shape torch.Size([64])
encoder.stages.1.0.convs.0.norm.weight shape torch.Size([64])
encoder.stages.1.0.convs.0.norm.bias shape torch.Size([64])
encoder.stages.1.0.convs.0.all_modules.0.weight shape torch.Size([64, 32, 3, 3, 3])
encoder.stages.1.0.convs.0.all_modules.0.bias shape torch.Size([64])
encoder.stages.1.0.convs.0.all_modules.1.weight shape torch.Size([64])
encoder.stages.1.0.convs.0.all_modules.1.bias shape torch.Size([64])
encoder.stages.1.0.convs.1.conv.weight shape torch.Size([64, 64, 3, 3, 3])
encoder.stages.1.0.convs.1.conv.bias shape torch.Size([64])
encoder.stages.1.0.convs.1.norm.weight shape torch.Size([64])
encoder.stages.1.0.convs.1.norm.bias shape torch.Size([64])
encoder.stages.1.0.convs.1.all_modules.0.weight shape torch.Size([64, 64, 3, 3, 3])
encoder.stages.1.0.convs.1.all_modules.0.bias shape torch.Size([64])
encoder.stages.1.0.convs.1.all_modules.1.weight shape torch.Size([64])
encoder.stages.1.0.convs.1.all_modules.1.bias shape torch.Size([64])
encoder.stages.2.0.convs.0.conv.weight shape torch.Size([128, 64, 3, 3, 3])
encoder.stages.2.0.convs.0.conv.bias shape torch.Size([128])
encoder.stages.2.0.convs.0.norm.weight shape torch.Size([128])
encoder.stages.2.0.convs.0.norm.bias shape torch.Size([128])
encoder.stages.2.0.convs.0.all_modules.0.weight shape torch.Size([128, 64, 3, 3, 3])
encoder.stages.2.0.convs.0.all_modules.0.bias shape torch.Size([128])
encoder.stages.2.0.convs.0.all_modules.1.weight shape torch.Size([128])
encoder.stages.2.0.convs.0.all_modules.1.bias shape torch.Size([128])
encoder.stages.2.0.convs.1.conv.weight shape torch.Size([128, 128, 3, 3, 3])
encoder.stages.2.0.convs.1.conv.bias shape torch.Size([128])
encoder.stages.2.0.convs.1.norm.weight shape torch.Size([128])
encoder.stages.2.0.convs.1.norm.bias shape torch.Size([128])
encoder.stages.2.0.convs.1.all_modules.0.weight shape torch.Size([128, 128, 3, 3, 3])
encoder.stages.2.0.convs.1.all_modules.0.bias shape torch.Size([128])
encoder.stages.2.0.convs.1.all_modules.1.weight shape torch.Size([128])
encoder.stages.2.0.convs.1.all_modules.1.bias shape torch.Size([128])
encoder.stages.3.0.convs.0.conv.weight shape torch.Size([256, 128, 3, 3, 3])
encoder.stages.3.0.convs.0.conv.bias shape torch.Size([256])
encoder.stages.3.0.convs.0.norm.weight shape torch.Size([256])
encoder.stages.3.0.convs.0.norm.bias shape torch.Size([256])
encoder.stages.3.0.convs.0.all_modules.0.weight shape torch.Size([256, 128, 3, 3, 3])
encoder.stages.3.0.convs.0.all_modules.0.bias shape torch.Size([256])
encoder.stages.3.0.convs.0.all_modules.1.weight shape torch.Size([256])
encoder.stages.3.0.convs.0.all_modules.1.bias shape torch.Size([256])
encoder.stages.3.0.convs.1.conv.weight shape torch.Size([256, 256, 3, 3, 3])
encoder.stages.3.0.convs.1.conv.bias shape torch.Size([256])
encoder.stages.3.0.convs.1.norm.weight shape torch.Size([256])
encoder.stages.3.0.convs.1.norm.bias shape torch.Size([256])
encoder.stages.3.0.convs.1.all_modules.0.weight shape torch.Size([256, 256, 3, 3, 3])
encoder.stages.3.0.convs.1.all_modules.0.bias shape torch.Size([256])
encoder.stages.3.0.convs.1.all_modules.1.weight shape torch.Size([256])
encoder.stages.3.0.convs.1.all_modules.1.bias shape torch.Size([256])
encoder.stages.4.0.convs.0.conv.weight shape torch.Size([320, 256, 3, 3, 3])
encoder.stages.4.0.convs.0.conv.bias shape torch.Size([320])
encoder.stages.4.0.convs.0.norm.weight shape torch.Size([320])
encoder.stages.4.0.convs.0.norm.bias shape torch.Size([320])
encoder.stages.4.0.convs.0.all_modules.0.weight shape torch.Size([320, 256, 3, 3, 3])
encoder.stages.4.0.convs.0.all_modules.0.bias shape torch.Size([320])
encoder.stages.4.0.convs.0.all_modules.1.weight shape torch.Size([320])
encoder.stages.4.0.convs.0.all_modules.1.bias shape torch.Size([320])
encoder.stages.4.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.4.0.convs.1.conv.bias shape torch.Size([320])
encoder.stages.4.0.convs.1.norm.weight shape torch.Size([320])
encoder.stages.4.0.convs.1.norm.bias shape torch.Size([320])
encoder.stages.4.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.4.0.convs.1.all_modules.0.bias shape torch.Size([320])
encoder.stages.4.0.convs.1.all_modules.1.weight shape torch.Size([320])
encoder.stages.4.0.convs.1.all_modules.1.bias shape torch.Size([320])
encoder.stages.5.0.convs.0.conv.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.5.0.convs.0.conv.bias shape torch.Size([320])
encoder.stages.5.0.convs.0.norm.weight shape torch.Size([320])
encoder.stages.5.0.convs.0.norm.bias shape torch.Size([320])
encoder.stages.5.0.convs.0.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.5.0.convs.0.all_modules.0.bias shape torch.Size([320])
encoder.stages.5.0.convs.0.all_modules.1.weight shape torch.Size([320])
encoder.stages.5.0.convs.0.all_modules.1.bias shape torch.Size([320])
encoder.stages.5.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.5.0.convs.1.conv.bias shape torch.Size([320])
encoder.stages.5.0.convs.1.norm.weight shape torch.Size([320])
encoder.stages.5.0.convs.1.norm.bias shape torch.Size([320])
encoder.stages.5.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.5.0.convs.1.all_modules.0.bias shape torch.Size([320])
encoder.stages.5.0.convs.1.all_modules.1.weight shape torch.Size([320])
encoder.stages.5.0.convs.1.all_modules.1.bias shape torch.Size([320])
decoder.encoder.stages.0.0.convs.0.conv.weight shape torch.Size([32, 1, 3, 3, 3])
decoder.encoder.stages.0.0.convs.0.conv.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.norm.weight shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.norm.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.all_modules.0.weight shape torch.Size([32, 1, 3, 3, 3])
decoder.encoder.stages.0.0.convs.0.all_modules.0.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.all_modules.1.weight shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.all_modules.1.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.conv.weight shape torch.Size([32, 32, 3, 3, 3])
decoder.encoder.stages.0.0.convs.1.conv.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.norm.weight shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.norm.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.all_modules.0.weight shape torch.Size([32, 32, 3, 3, 3])
decoder.encoder.stages.0.0.convs.1.all_modules.0.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.all_modules.1.weight shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.all_modules.1.bias shape torch.Size([32])
decoder.encoder.stages.1.0.convs.0.conv.weight shape torch.Size([64, 32, 3, 3, 3])
decoder.encoder.stages.1.0.convs.0.conv.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.norm.weight shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.norm.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.all_modules.0.weight shape torch.Size([64, 32, 3, 3, 3])
decoder.encoder.stages.1.0.convs.0.all_modules.0.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.all_modules.1.weight shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.all_modules.1.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.conv.weight shape torch.Size([64, 64, 3, 3, 3])
decoder.encoder.stages.1.0.convs.1.conv.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.norm.weight shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.norm.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.all_modules.0.weight shape torch.Size([64, 64, 3, 3, 3])
decoder.encoder.stages.1.0.convs.1.all_modules.0.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.all_modules.1.weight shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.all_modules.1.bias shape torch.Size([64])
decoder.encoder.stages.2.0.convs.0.conv.weight shape torch.Size([128, 64, 3, 3, 3])
decoder.encoder.stages.2.0.convs.0.conv.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.norm.weight shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.norm.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.all_modules.0.weight shape torch.Size([128, 64, 3, 3, 3])
decoder.encoder.stages.2.0.convs.0.all_modules.0.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.all_modules.1.weight shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.all_modules.1.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.conv.weight shape torch.Size([128, 128, 3, 3, 3])
decoder.encoder.stages.2.0.convs.1.conv.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.norm.weight shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.norm.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.all_modules.0.weight shape torch.Size([128, 128, 3, 3, 3])
decoder.encoder.stages.2.0.convs.1.all_modules.0.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.all_modules.1.weight shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.all_modules.1.bias shape torch.Size([128])
decoder.encoder.stages.3.0.convs.0.conv.weight shape torch.Size([256, 128, 3, 3, 3])
decoder.encoder.stages.3.0.convs.0.conv.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.norm.weight shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.norm.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.all_modules.0.weight shape torch.Size([256, 128, 3, 3, 3])
decoder.encoder.stages.3.0.convs.0.all_modules.0.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.all_modules.1.weight shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.all_modules.1.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.conv.weight shape torch.Size([256, 256, 3, 3, 3])
decoder.encoder.stages.3.0.convs.1.conv.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.norm.weight shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.norm.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.all_modules.0.weight shape torch.Size([256, 256, 3, 3, 3])
decoder.encoder.stages.3.0.convs.1.all_modules.0.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.all_modules.1.weight shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.all_modules.1.bias shape torch.Size([256])
decoder.encoder.stages.4.0.convs.0.conv.weight shape torch.Size([320, 256, 3, 3, 3])
decoder.encoder.stages.4.0.convs.0.conv.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.norm.weight shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.norm.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.all_modules.0.weight shape torch.Size([320, 256, 3, 3, 3])
decoder.encoder.stages.4.0.convs.0.all_modules.0.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.all_modules.1.weight shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.all_modules.1.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.4.0.convs.1.conv.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.norm.weight shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.norm.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.4.0.convs.1.all_modules.0.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.all_modules.1.weight shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.all_modules.1.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.conv.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.5.0.convs.0.conv.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.norm.weight shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.norm.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.5.0.convs.0.all_modules.0.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.all_modules.1.weight shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.all_modules.1.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.5.0.convs.1.conv.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.norm.weight shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.norm.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.5.0.convs.1.all_modules.0.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.all_modules.1.weight shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.all_modules.1.bias shape torch.Size([320])
decoder.stages.0.convs.0.conv.weight shape torch.Size([320, 640, 3, 3, 3])
decoder.stages.0.convs.0.conv.bias shape torch.Size([320])
decoder.stages.0.convs.0.norm.weight shape torch.Size([320])
decoder.stages.0.convs.0.norm.bias shape torch.Size([320])
decoder.stages.0.convs.0.all_modules.0.weight shape torch.Size([320, 640, 3, 3, 3])
decoder.stages.0.convs.0.all_modules.0.bias shape torch.Size([320])
decoder.stages.0.convs.0.all_modules.1.weight shape torch.Size([320])
decoder.stages.0.convs.0.all_modules.1.bias shape torch.Size([320])
decoder.stages.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.stages.0.convs.1.conv.bias shape torch.Size([320])
decoder.stages.0.convs.1.norm.weight shape torch.Size([320])
decoder.stages.0.convs.1.norm.bias shape torch.Size([320])
decoder.stages.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.stages.0.convs.1.all_modules.0.bias shape torch.Size([320])
decoder.stages.0.convs.1.all_modules.1.weight shape torch.Size([320])
decoder.stages.0.convs.1.all_modules.1.bias shape torch.Size([320])
decoder.stages.1.convs.0.conv.weight shape torch.Size([256, 512, 3, 3, 3])
decoder.stages.1.convs.0.conv.bias shape torch.Size([256])
decoder.stages.1.convs.0.norm.weight shape torch.Size([256])
decoder.stages.1.convs.0.norm.bias shape torch.Size([256])
decoder.stages.1.convs.0.all_modules.0.weight shape torch.Size([256, 512, 3, 3, 3])
decoder.stages.1.convs.0.all_modules.0.bias shape torch.Size([256])
decoder.stages.1.convs.0.all_modules.1.weight shape torch.Size([256])
decoder.stages.1.convs.0.all_modules.1.bias shape torch.Size([256])
decoder.stages.1.convs.1.conv.weight shape torch.Size([256, 256, 3, 3, 3])
decoder.stages.1.convs.1.conv.bias shape torch.Size([256])
decoder.stages.1.convs.1.norm.weight shape torch.Size([256])
decoder.stages.1.convs.1.norm.bias shape torch.Size([256])
decoder.stages.1.convs.1.all_modules.0.weight shape torch.Size([256, 256, 3, 3, 3])
decoder.stages.1.convs.1.all_modules.0.bias shape torch.Size([256])
decoder.stages.1.convs.1.all_modules.1.weight shape torch.Size([256])
decoder.stages.1.convs.1.all_modules.1.bias shape torch.Size([256])
decoder.stages.2.convs.0.conv.weight shape torch.Size([128, 256, 3, 3, 3])
decoder.stages.2.convs.0.conv.bias shape torch.Size([128])
decoder.stages.2.convs.0.norm.weight shape torch.Size([128])
decoder.stages.2.convs.0.norm.bias shape torch.Size([128])
decoder.stages.2.convs.0.all_modules.0.weight shape torch.Size([128, 256, 3, 3, 3])
decoder.stages.2.convs.0.all_modules.0.bias shape torch.Size([128])
decoder.stages.2.convs.0.all_modules.1.weight shape torch.Size([128])
decoder.stages.2.convs.0.all_modules.1.bias shape torch.Size([128])
decoder.stages.2.convs.1.conv.weight shape torch.Size([128, 128, 3, 3, 3])
decoder.stages.2.convs.1.conv.bias shape torch.Size([128])
decoder.stages.2.convs.1.norm.weight shape torch.Size([128])
decoder.stages.2.convs.1.norm.bias shape torch.Size([128])
decoder.stages.2.convs.1.all_modules.0.weight shape torch.Size([128, 128, 3, 3, 3])
decoder.stages.2.convs.1.all_modules.0.bias shape torch.Size([128])
decoder.stages.2.convs.1.all_modules.1.weight shape torch.Size([128])
decoder.stages.2.convs.1.all_modules.1.bias shape torch.Size([128])
decoder.stages.3.convs.0.conv.weight shape torch.Size([64, 128, 3, 3, 3])
decoder.stages.3.convs.0.conv.bias shape torch.Size([64])
decoder.stages.3.convs.0.norm.weight shape torch.Size([64])
decoder.stages.3.convs.0.norm.bias shape torch.Size([64])
decoder.stages.3.convs.0.all_modules.0.weight shape torch.Size([64, 128, 3, 3, 3])
decoder.stages.3.convs.0.all_modules.0.bias shape torch.Size([64])
decoder.stages.3.convs.0.all_modules.1.weight shape torch.Size([64])
decoder.stages.3.convs.0.all_modules.1.bias shape torch.Size([64])
decoder.stages.3.convs.1.conv.weight shape torch.Size([64, 64, 3, 3, 3])
decoder.stages.3.convs.1.conv.bias shape torch.Size([64])
decoder.stages.3.convs.1.norm.weight shape torch.Size([64])
decoder.stages.3.convs.1.norm.bias shape torch.Size([64])
decoder.stages.3.convs.1.all_modules.0.weight shape torch.Size([64, 64, 3, 3, 3])
decoder.stages.3.convs.1.all_modules.0.bias shape torch.Size([64])
decoder.stages.3.convs.1.all_modules.1.weight shape torch.Size([64])
decoder.stages.3.convs.1.all_modules.1.bias shape torch.Size([64])
decoder.stages.4.convs.0.conv.weight shape torch.Size([32, 64, 3, 3, 3])
decoder.stages.4.convs.0.conv.bias shape torch.Size([32])
decoder.stages.4.convs.0.norm.weight shape torch.Size([32])
decoder.stages.4.convs.0.norm.bias shape torch.Size([32])
decoder.stages.4.convs.0.all_modules.0.weight shape torch.Size([32, 64, 3, 3, 3])
decoder.stages.4.convs.0.all_modules.0.bias shape torch.Size([32])
decoder.stages.4.convs.0.all_modules.1.weight shape torch.Size([32])
decoder.stages.4.convs.0.all_modules.1.bias shape torch.Size([32])
decoder.stages.4.convs.1.conv.weight shape torch.Size([32, 32, 3, 3, 3])
decoder.stages.4.convs.1.conv.bias shape torch.Size([32])
decoder.stages.4.convs.1.norm.weight shape torch.Size([32])
decoder.stages.4.convs.1.norm.bias shape torch.Size([32])
decoder.stages.4.convs.1.all_modules.0.weight shape torch.Size([32, 32, 3, 3, 3])
decoder.stages.4.convs.1.all_modules.0.bias shape torch.Size([32])
decoder.stages.4.convs.1.all_modules.1.weight shape torch.Size([32])
decoder.stages.4.convs.1.all_modules.1.bias shape torch.Size([32])
decoder.transpconvs.0.weight shape torch.Size([320, 320, 1, 2, 2])
decoder.transpconvs.0.bias shape torch.Size([320])
decoder.transpconvs.1.weight shape torch.Size([320, 256, 2, 2, 2])
decoder.transpconvs.1.bias shape torch.Size([256])
decoder.transpconvs.2.weight shape torch.Size([256, 128, 2, 2, 2])
decoder.transpconvs.2.bias shape torch.Size([128])
decoder.transpconvs.3.weight shape torch.Size([128, 64, 2, 2, 2])
decoder.transpconvs.3.bias shape torch.Size([64])
decoder.transpconvs.4.weight shape torch.Size([64, 32, 2, 2, 2])
decoder.transpconvs.4.bias shape torch.Size([32])
################### Done ###################
2024-07-24 02:24:05.891428: do_dummy_2d_data_aug: True
2024-07-24 02:24:05.893517: Using splits from existing split file: /home/gridsan/nchutisilp/datasets/nnUNet_Datasets/nnUNet_preprocessed/Dataset302_Calcium_OCTv2/splits_final.json
2024-07-24 02:24:05.895288: The split file contains 3 splits.
2024-07-24 02:24:05.896415: Desired fold for training: 2
2024-07-24 02:24:05.897399: This split has 4 training and 2 validation cases.
using pin_memory on device 0
using pin_memory on device 0

This is the configuration used by this training:
Configuration name: 3d_32x512x512_b2
 {'data_identifier': 'nnUNetPlans_3d_fullres', 'preprocessor_name': 'DefaultPreprocessor', 'batch_size': 2, 'patch_size': [32, 512, 512], 'median_image_size_in_voxels': [375.0, 498.0, 498.0], 'spacing': [1.0, 1.0, 1.0], 'normalization_schemes': ['ZScoreNormalization'], 'use_mask_for_norm': [False], 'resampling_fn_data': 'resample_data_or_seg_to_shape', 'resampling_fn_seg': 'resample_data_or_seg_to_shape', 'resampling_fn_data_kwargs': {'is_seg': False, 'order': 3, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_seg_kwargs': {'is_seg': True, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_probabilities': 'resample_data_or_seg_to_shape', 'resampling_fn_probabilities_kwargs': {'is_seg': False, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'architecture': {'network_class_name': 'dynamic_network_architectures.architectures.unet.PlainConvUNet', 'arch_kwargs': {'n_stages': 6, 'features_per_stage': [32, 64, 128, 256, 320, 320], 'conv_op': 'torch.nn.modules.conv.Conv3d', 'kernel_sizes': [[3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3]], 'strides': [[1, 1, 1], [2, 2, 2], [2, 2, 2], [2, 2, 2], [2, 2, 2], [1, 2, 2]], 'n_conv_per_stage': [2, 2, 2, 2, 2, 2], 'n_conv_per_stage_decoder': [2, 2, 2, 2, 2], 'conv_bias': True, 'norm_op': 'torch.nn.modules.instancenorm.InstanceNorm3d', 'norm_op_kwargs': {'eps': 1e-05, 'affine': True}, 'dropout_op': None, 'dropout_op_kwargs': None, 'nonlin': 'torch.nn.LeakyReLU', 'nonlin_kwargs': {'inplace': True}, 'deep_supervision': True}, '_kw_requires_import': ['conv_op', 'norm_op', 'dropout_op', 'nonlin']}, 'batch_dice': True, 'inherits_from': '3d_fullres'} 

These are the global plan.json settings:
 {'dataset_name': 'Dataset302_Calcium_OCTv2', 'plans_name': 'nnUNetPlans', 'original_median_spacing_after_transp': [1.0, 1.0, 1.0], 'original_median_shape_after_transp': [375, 498, 498], 'image_reader_writer': 'SimpleITKIO', 'transpose_forward': [0, 1, 2], 'transpose_backward': [0, 1, 2], 'experiment_planner_used': 'ExperimentPlanner', 'label_manager': 'LabelManager', 'foreground_intensity_properties_per_channel': {'0': {'max': 0.4977976381778717, 'mean': 0.13332499563694, 'median': 0.08871842920780182, 'min': 0.0, 'percentile_00_5': 0.014754901640117168, 'percentile_99_5': 0.4977976381778717, 'std': 0.12022686749696732}}} 

2024-07-24 02:24:08.739462: unpacking dataset...
2024-07-24 02:24:13.213190: unpacking done...
2024-07-24 02:24:13.246929: Unable to plot network architecture: nnUNet_compile is enabled!
2024-07-24 02:24:13.286086: 
2024-07-24 02:24:13.287738: Epoch 0
2024-07-24 02:24:13.289205: Current learning rate: 0.01
2024-07-24 02:29:49.854778: Validation loss improved from 1000.00000 to -0.24364! Patience: 0/50
2024-07-24 02:29:49.857038: train_loss -0.2
2024-07-24 02:29:49.858798: val_loss -0.2436
2024-07-24 02:29:49.859975: Pseudo dice [0.434]
2024-07-24 02:29:49.861133: Epoch time: 336.57 s
2024-07-24 02:29:49.862135: Yayy! New best EMA pseudo Dice: 0.434
2024-07-24 02:29:51.245447: 
2024-07-24 02:29:51.247217: Epoch 1
2024-07-24 02:29:51.248282: Current learning rate: 0.00999
2024-07-24 02:33:49.654962: Validation loss improved from -0.24364 to -0.30185! Patience: 0/50
2024-07-24 02:33:49.656484: train_loss -0.3365
2024-07-24 02:33:49.658329: val_loss -0.3018
2024-07-24 02:33:49.660197: Pseudo dice [0.4888]
2024-07-24 02:33:49.662128: Epoch time: 238.41 s
2024-07-24 02:33:49.663768: Yayy! New best EMA pseudo Dice: 0.4394
2024-07-24 02:33:51.240566: 
2024-07-24 02:33:51.243006: Epoch 2
2024-07-24 02:33:51.244583: Current learning rate: 0.00998
2024-07-24 02:37:50.421264: Validation loss did not improve from -0.30185. Patience: 1/50
2024-07-24 02:37:50.422840: train_loss -0.3733
2024-07-24 02:37:50.424602: val_loss -0.2725
2024-07-24 02:37:50.426353: Pseudo dice [0.4266]
2024-07-24 02:37:50.427798: Epoch time: 239.18 s
2024-07-24 02:37:51.710909: 
2024-07-24 02:37:51.713024: Epoch 3
2024-07-24 02:37:51.714232: Current learning rate: 0.00997
2024-07-24 02:41:51.421618: Validation loss improved from -0.30185 to -0.31917! Patience: 1/50
2024-07-24 02:41:51.423648: train_loss -0.408
2024-07-24 02:41:51.425489: val_loss -0.3192
2024-07-24 02:41:51.426884: Pseudo dice [0.5315]
2024-07-24 02:41:51.428176: Epoch time: 239.71 s
2024-07-24 02:41:51.429483: Yayy! New best EMA pseudo Dice: 0.4475
2024-07-24 02:41:53.220204: 
2024-07-24 02:41:53.222198: Epoch 4
2024-07-24 02:41:53.223454: Current learning rate: 0.00996
2024-07-24 02:45:53.465167: Validation loss improved from -0.31917 to -0.38938! Patience: 0/50
2024-07-24 02:45:53.467172: train_loss -0.4443
2024-07-24 02:45:53.468536: val_loss -0.3894
2024-07-24 02:45:53.469625: Pseudo dice [0.5542]
2024-07-24 02:45:53.470986: Epoch time: 240.25 s
2024-07-24 02:45:53.755510: Yayy! New best EMA pseudo Dice: 0.4582
2024-07-24 02:45:55.342036: 
2024-07-24 02:45:55.344158: Epoch 5
2024-07-24 02:45:55.345700: Current learning rate: 0.00995
2024-07-24 02:49:55.934363: Validation loss improved from -0.38938 to -0.41131! Patience: 0/50
2024-07-24 02:49:55.936317: train_loss -0.4888
2024-07-24 02:49:55.937505: val_loss -0.4113
2024-07-24 02:49:55.938617: Pseudo dice [0.5664]
2024-07-24 02:49:55.939629: Epoch time: 240.6 s
2024-07-24 02:49:55.940748: Yayy! New best EMA pseudo Dice: 0.469
2024-07-24 02:49:57.474681: 
2024-07-24 02:49:57.476313: Epoch 6
2024-07-24 02:49:57.477443: Current learning rate: 0.00995
2024-07-24 02:53:58.075503: Validation loss did not improve from -0.41131. Patience: 1/50
2024-07-24 02:53:58.077361: train_loss -0.5118
2024-07-24 02:53:58.078782: val_loss -0.3301
2024-07-24 02:53:58.080065: Pseudo dice [0.5305]
2024-07-24 02:53:58.081243: Epoch time: 240.6 s
2024-07-24 02:53:58.082334: Yayy! New best EMA pseudo Dice: 0.4751
2024-07-24 02:54:00.073083: 
2024-07-24 02:54:00.075299: Epoch 7
2024-07-24 02:54:00.076698: Current learning rate: 0.00994
2024-07-24 02:58:00.666451: Validation loss improved from -0.41131 to -0.41295! Patience: 1/50
2024-07-24 02:58:00.668590: train_loss -0.5235
2024-07-24 02:58:00.670269: val_loss -0.413
2024-07-24 02:58:00.671931: Pseudo dice [0.5748]
2024-07-24 02:58:00.673458: Epoch time: 240.6 s
2024-07-24 02:58:00.674455: Yayy! New best EMA pseudo Dice: 0.4851
2024-07-24 02:58:02.211686: 
2024-07-24 02:58:02.213797: Epoch 8
2024-07-24 02:58:02.214892: Current learning rate: 0.00993
2024-07-24 03:02:02.017849: Validation loss improved from -0.41295 to -0.41565! Patience: 0/50
2024-07-24 03:02:02.020693: train_loss -0.5239
2024-07-24 03:02:02.023067: val_loss -0.4156
2024-07-24 03:02:02.024992: Pseudo dice [0.5924]
2024-07-24 03:02:02.026523: Epoch time: 239.81 s
2024-07-24 03:02:02.028193: Yayy! New best EMA pseudo Dice: 0.4958
2024-07-24 03:02:03.622659: 
2024-07-24 03:02:03.625280: Epoch 9
2024-07-24 03:02:03.627406: Current learning rate: 0.00992
2024-07-24 03:06:04.057850: Validation loss improved from -0.41565 to -0.46367! Patience: 0/50
2024-07-24 03:06:04.059282: train_loss -0.5176
2024-07-24 03:06:04.060427: val_loss -0.4637
2024-07-24 03:06:04.061540: Pseudo dice [0.611]
2024-07-24 03:06:04.062530: Epoch time: 240.44 s
2024-07-24 03:06:04.407304: Yayy! New best EMA pseudo Dice: 0.5073
2024-07-24 03:06:05.965739: 
2024-07-24 03:06:05.967908: Epoch 10
2024-07-24 03:06:05.969616: Current learning rate: 0.00991
2024-07-24 03:10:06.108263: Validation loss did not improve from -0.46367. Patience: 1/50
2024-07-24 03:10:06.109897: train_loss -0.5467
2024-07-24 03:10:06.111486: val_loss -0.4349
2024-07-24 03:10:06.112750: Pseudo dice [0.5861]
2024-07-24 03:10:06.113983: Epoch time: 240.15 s
2024-07-24 03:10:06.115252: Yayy! New best EMA pseudo Dice: 0.5152
2024-07-24 03:10:07.891002: 
2024-07-24 03:10:07.892796: Epoch 11
2024-07-24 03:10:07.894025: Current learning rate: 0.0099
2024-07-24 03:14:08.004925: Validation loss did not improve from -0.46367. Patience: 2/50
2024-07-24 03:14:08.006434: train_loss -0.5721
2024-07-24 03:14:08.007883: val_loss -0.433
2024-07-24 03:14:08.009138: Pseudo dice [0.6072]
2024-07-24 03:14:08.010345: Epoch time: 240.12 s
2024-07-24 03:14:08.011554: Yayy! New best EMA pseudo Dice: 0.5244
2024-07-24 03:14:09.633291: 
2024-07-24 03:14:09.635329: Epoch 12
2024-07-24 03:14:09.636622: Current learning rate: 0.00989
2024-07-24 03:18:10.595913: Validation loss improved from -0.46367 to -0.49878! Patience: 2/50
2024-07-24 03:18:10.597579: train_loss -0.59
2024-07-24 03:18:10.598682: val_loss -0.4988
2024-07-24 03:18:10.599615: Pseudo dice [0.6299]
2024-07-24 03:18:10.600640: Epoch time: 240.97 s
2024-07-24 03:18:10.601621: Yayy! New best EMA pseudo Dice: 0.535
2024-07-24 03:18:12.117083: 
2024-07-24 03:18:12.118836: Epoch 13
2024-07-24 03:18:12.120083: Current learning rate: 0.00988
2024-07-24 03:22:13.151389: Validation loss did not improve from -0.49878. Patience: 1/50
2024-07-24 03:22:13.153432: train_loss -0.5813
2024-07-24 03:22:13.155038: val_loss -0.4344
2024-07-24 03:22:13.156344: Pseudo dice [0.6012]
2024-07-24 03:22:13.157621: Epoch time: 241.04 s
2024-07-24 03:22:13.158721: Yayy! New best EMA pseudo Dice: 0.5416
2024-07-24 03:22:14.752734: 
2024-07-24 03:22:14.755380: Epoch 14
2024-07-24 03:22:14.757060: Current learning rate: 0.00987
2024-07-24 03:26:15.952361: Validation loss did not improve from -0.49878. Patience: 2/50
2024-07-24 03:26:15.954027: train_loss -0.6007
2024-07-24 03:26:15.955700: val_loss -0.4638
2024-07-24 03:26:15.956765: Pseudo dice [0.6238]
2024-07-24 03:26:15.957853: Epoch time: 241.2 s
2024-07-24 03:26:16.294174: Yayy! New best EMA pseudo Dice: 0.5498
2024-07-24 03:26:17.865887: 
2024-07-24 03:26:17.867404: Epoch 15
2024-07-24 03:26:17.868561: Current learning rate: 0.00986
2024-07-24 03:30:18.547081: Validation loss did not improve from -0.49878. Patience: 3/50
2024-07-24 03:30:18.549151: train_loss -0.6133
2024-07-24 03:30:18.550879: val_loss -0.4909
2024-07-24 03:30:18.552087: Pseudo dice [0.6403]
2024-07-24 03:30:18.553294: Epoch time: 240.68 s
2024-07-24 03:30:18.554602: Yayy! New best EMA pseudo Dice: 0.5589
2024-07-24 03:30:20.184986: 
2024-07-24 03:30:20.186709: Epoch 16
2024-07-24 03:30:20.188403: Current learning rate: 0.00986
2024-07-24 03:34:21.795570: Validation loss did not improve from -0.49878. Patience: 4/50
2024-07-24 03:34:21.797618: train_loss -0.6358
2024-07-24 03:34:21.800615: val_loss -0.4736
2024-07-24 03:34:21.808907: Pseudo dice [0.6237]
2024-07-24 03:34:21.810534: Epoch time: 241.61 s
2024-07-24 03:34:21.812094: Yayy! New best EMA pseudo Dice: 0.5653
2024-07-24 03:34:23.467697: 
2024-07-24 03:34:23.469449: Epoch 17
2024-07-24 03:34:23.470363: Current learning rate: 0.00985
2024-07-24 03:38:22.779378: Validation loss did not improve from -0.49878. Patience: 5/50
2024-07-24 03:38:22.780948: train_loss -0.6216
2024-07-24 03:38:22.782395: val_loss -0.4893
2024-07-24 03:38:22.783452: Pseudo dice [0.6469]
2024-07-24 03:38:22.784516: Epoch time: 239.31 s
2024-07-24 03:38:22.785428: Yayy! New best EMA pseudo Dice: 0.5735
2024-07-24 03:38:25.452801: 
2024-07-24 03:38:25.454466: Epoch 18
2024-07-24 03:38:25.455432: Current learning rate: 0.00984
2024-07-24 03:42:23.116420: Validation loss did not improve from -0.49878. Patience: 6/50
2024-07-24 03:42:23.119063: train_loss -0.6227
2024-07-24 03:42:23.121603: val_loss -0.4736
2024-07-24 03:42:23.123870: Pseudo dice [0.624]
2024-07-24 03:42:23.125206: Epoch time: 237.67 s
2024-07-24 03:42:23.127084: Yayy! New best EMA pseudo Dice: 0.5785
2024-07-24 03:42:24.650428: 
2024-07-24 03:42:24.652836: Epoch 19
2024-07-24 03:42:24.654521: Current learning rate: 0.00983
2024-07-24 03:46:21.745580: Validation loss improved from -0.49878 to -0.50381! Patience: 6/50
2024-07-24 03:46:21.748009: train_loss -0.6351
2024-07-24 03:46:21.749553: val_loss -0.5038
2024-07-24 03:46:21.750661: Pseudo dice [0.6525]
2024-07-24 03:46:21.751726: Epoch time: 237.1 s
2024-07-24 03:46:22.087090: Yayy! New best EMA pseudo Dice: 0.5859
2024-07-24 03:46:23.618825: 
2024-07-24 03:46:23.620508: Epoch 20
2024-07-24 03:46:23.621664: Current learning rate: 0.00982
2024-07-24 03:50:21.253203: Validation loss did not improve from -0.50381. Patience: 1/50
2024-07-24 03:50:21.255135: train_loss -0.632
2024-07-24 03:50:21.256420: val_loss -0.4435
2024-07-24 03:50:21.257449: Pseudo dice [0.6185]
2024-07-24 03:50:21.258669: Epoch time: 237.64 s
2024-07-24 03:50:21.259727: Yayy! New best EMA pseudo Dice: 0.5892
2024-07-24 03:50:22.865615: 
2024-07-24 03:50:22.867637: Epoch 21
2024-07-24 03:50:22.868640: Current learning rate: 0.00981
2024-07-24 03:54:21.680636: Validation loss did not improve from -0.50381. Patience: 2/50
2024-07-24 03:54:21.682404: train_loss -0.6139
2024-07-24 03:54:21.683530: val_loss -0.4981
2024-07-24 03:54:21.684707: Pseudo dice [0.6528]
2024-07-24 03:54:21.685796: Epoch time: 238.82 s
2024-07-24 03:54:21.686754: Yayy! New best EMA pseudo Dice: 0.5956
2024-07-24 03:54:23.166226: 
2024-07-24 03:54:23.168659: Epoch 22
2024-07-24 03:54:23.170263: Current learning rate: 0.0098
2024-07-24 03:58:22.014976: Validation loss did not improve from -0.50381. Patience: 3/50
2024-07-24 03:58:22.016578: train_loss -0.6479
2024-07-24 03:58:22.017929: val_loss -0.4972
2024-07-24 03:58:22.019096: Pseudo dice [0.648]
2024-07-24 03:58:22.020379: Epoch time: 238.85 s
2024-07-24 03:58:22.021409: Yayy! New best EMA pseudo Dice: 0.6008
2024-07-24 03:58:23.580824: 
2024-07-24 03:58:23.582434: Epoch 23
2024-07-24 03:58:23.583634: Current learning rate: 0.00979
2024-07-24 04:02:22.616915: Validation loss improved from -0.50381 to -0.53720! Patience: 3/50
2024-07-24 04:02:22.619039: train_loss -0.677
2024-07-24 04:02:22.620459: val_loss -0.5372
2024-07-24 04:02:22.621487: Pseudo dice [0.6752]
2024-07-24 04:02:22.622556: Epoch time: 239.04 s
2024-07-24 04:02:22.623558: Yayy! New best EMA pseudo Dice: 0.6082
2024-07-24 04:02:24.122044: 
2024-07-24 04:02:24.124023: Epoch 24
2024-07-24 04:02:24.125422: Current learning rate: 0.00978
2024-07-24 04:06:23.301623: Validation loss did not improve from -0.53720. Patience: 1/50
2024-07-24 04:06:23.303152: train_loss -0.6702
2024-07-24 04:06:23.304410: val_loss -0.5193
2024-07-24 04:06:23.305460: Pseudo dice [0.6712]
2024-07-24 04:06:23.306657: Epoch time: 239.18 s
2024-07-24 04:06:23.651897: Yayy! New best EMA pseudo Dice: 0.6145
2024-07-24 04:06:25.193327: 
2024-07-24 04:06:25.195384: Epoch 25
2024-07-24 04:06:25.196579: Current learning rate: 0.00977
2024-07-24 04:10:26.111489: Validation loss did not improve from -0.53720. Patience: 2/50
2024-07-24 04:10:26.113222: train_loss -0.6507
2024-07-24 04:10:26.114798: val_loss -0.4543
2024-07-24 04:10:26.116007: Pseudo dice [0.6156]
2024-07-24 04:10:26.117187: Epoch time: 240.92 s
2024-07-24 04:10:26.118362: Yayy! New best EMA pseudo Dice: 0.6146
2024-07-24 04:10:27.671490: 
2024-07-24 04:10:27.673201: Epoch 26
2024-07-24 04:10:27.674415: Current learning rate: 0.00977
2024-07-24 04:14:29.111560: Validation loss improved from -0.53720 to -0.54465! Patience: 2/50
2024-07-24 04:14:29.113360: train_loss -0.6755
2024-07-24 04:14:29.114688: val_loss -0.5446
2024-07-24 04:14:29.115636: Pseudo dice [0.671]
2024-07-24 04:14:29.116560: Epoch time: 241.44 s
2024-07-24 04:14:29.117429: Yayy! New best EMA pseudo Dice: 0.6203
2024-07-24 04:14:30.609554: 
2024-07-24 04:14:30.611196: Epoch 27
2024-07-24 04:14:30.612393: Current learning rate: 0.00976
2024-07-24 04:18:31.300326: Validation loss did not improve from -0.54465. Patience: 1/50
2024-07-24 04:18:31.302186: train_loss -0.6722
2024-07-24 04:18:31.303437: val_loss -0.5081
2024-07-24 04:18:31.304518: Pseudo dice [0.648]
2024-07-24 04:18:31.305486: Epoch time: 240.69 s
2024-07-24 04:18:31.306833: Yayy! New best EMA pseudo Dice: 0.6231
2024-07-24 04:18:32.988822: 
2024-07-24 04:18:32.990382: Epoch 28
2024-07-24 04:18:32.991304: Current learning rate: 0.00975
2024-07-24 04:22:33.885182: Validation loss did not improve from -0.54465. Patience: 2/50
2024-07-24 04:22:33.886697: train_loss -0.6738
2024-07-24 04:22:33.888410: val_loss -0.537
2024-07-24 04:22:33.889719: Pseudo dice [0.6778]
2024-07-24 04:22:33.890981: Epoch time: 240.9 s
2024-07-24 04:22:33.892217: Yayy! New best EMA pseudo Dice: 0.6285
2024-07-24 04:22:35.340015: 
2024-07-24 04:22:35.342140: Epoch 29
2024-07-24 04:22:35.343177: Current learning rate: 0.00974
2024-07-24 04:26:35.826407: Validation loss improved from -0.54465 to -0.55775! Patience: 2/50
2024-07-24 04:26:35.828556: train_loss -0.6851
2024-07-24 04:26:35.829926: val_loss -0.5577
2024-07-24 04:26:35.830961: Pseudo dice [0.6912]
2024-07-24 04:26:35.832051: Epoch time: 240.49 s
2024-07-24 04:26:36.578620: Yayy! New best EMA pseudo Dice: 0.6348
2024-07-24 04:26:38.131808: 
2024-07-24 04:26:38.133318: Epoch 30
2024-07-24 04:26:38.134439: Current learning rate: 0.00973
2024-07-24 04:30:38.869726: Validation loss did not improve from -0.55775. Patience: 1/50
2024-07-24 04:30:38.874256: train_loss -0.667
2024-07-24 04:30:38.875819: val_loss -0.5308
2024-07-24 04:30:38.876989: Pseudo dice [0.6687]
2024-07-24 04:30:38.878246: Epoch time: 240.74 s
2024-07-24 04:30:38.879394: Yayy! New best EMA pseudo Dice: 0.6382
2024-07-24 04:30:40.536415: 
2024-07-24 04:30:40.538576: Epoch 31
2024-07-24 04:30:40.539843: Current learning rate: 0.00972
2024-07-24 04:34:39.844118: Validation loss did not improve from -0.55775. Patience: 2/50
2024-07-24 04:34:39.845652: train_loss -0.6825
2024-07-24 04:34:39.847341: val_loss -0.517
2024-07-24 04:34:39.848658: Pseudo dice [0.6705]
2024-07-24 04:34:39.849941: Epoch time: 239.31 s
2024-07-24 04:34:39.851065: Yayy! New best EMA pseudo Dice: 0.6414
2024-07-24 04:34:41.466124: 
2024-07-24 04:34:41.468200: Epoch 32
2024-07-24 04:34:41.469528: Current learning rate: 0.00971
2024-07-24 04:38:41.157604: Validation loss did not improve from -0.55775. Patience: 3/50
2024-07-24 04:38:41.159258: train_loss -0.6862
2024-07-24 04:38:41.160622: val_loss -0.5549
2024-07-24 04:38:41.161649: Pseudo dice [0.6851]
2024-07-24 04:38:41.162750: Epoch time: 239.69 s
2024-07-24 04:38:41.163741: Yayy! New best EMA pseudo Dice: 0.6458
2024-07-24 04:38:42.740757: 
2024-07-24 04:38:42.742347: Epoch 33
2024-07-24 04:38:42.743452: Current learning rate: 0.0097
2024-07-24 04:42:41.858723: Validation loss did not improve from -0.55775. Patience: 4/50
2024-07-24 04:42:41.860383: train_loss -0.7004
2024-07-24 04:42:41.861689: val_loss -0.5407
2024-07-24 04:42:41.862864: Pseudo dice [0.6784]
2024-07-24 04:42:41.863840: Epoch time: 239.12 s
2024-07-24 04:42:41.864908: Yayy! New best EMA pseudo Dice: 0.649
2024-07-24 04:42:43.446663: 
2024-07-24 04:42:43.448450: Epoch 34
2024-07-24 04:42:43.450112: Current learning rate: 0.00969
2024-07-24 04:46:42.818534: Validation loss did not improve from -0.55775. Patience: 5/50
2024-07-24 04:46:42.820926: train_loss -0.699
2024-07-24 04:46:42.822752: val_loss -0.5428
2024-07-24 04:46:42.824093: Pseudo dice [0.6755]
2024-07-24 04:46:42.825629: Epoch time: 239.38 s
2024-07-24 04:46:43.147721: Yayy! New best EMA pseudo Dice: 0.6517
2024-07-24 04:46:44.766412: 
2024-07-24 04:46:44.768452: Epoch 35
2024-07-24 04:46:44.769510: Current learning rate: 0.00968
2024-07-24 04:50:44.432675: Validation loss improved from -0.55775 to -0.56109! Patience: 5/50
2024-07-24 04:50:44.433993: train_loss -0.6812
2024-07-24 04:50:44.435197: val_loss -0.5611
2024-07-24 04:50:44.436217: Pseudo dice [0.6861]
2024-07-24 04:50:44.437351: Epoch time: 239.67 s
2024-07-24 04:50:44.438830: Yayy! New best EMA pseudo Dice: 0.6551
2024-07-24 04:50:46.050523: 
2024-07-24 04:50:46.053261: Epoch 36
2024-07-24 04:50:46.054926: Current learning rate: 0.00968
2024-07-24 04:54:45.743575: Validation loss did not improve from -0.56109. Patience: 1/50
2024-07-24 04:54:45.745618: train_loss -0.6874
2024-07-24 04:54:45.747007: val_loss -0.5559
2024-07-24 04:54:45.748012: Pseudo dice [0.6889]
2024-07-24 04:54:45.748981: Epoch time: 239.7 s
2024-07-24 04:54:45.750101: Yayy! New best EMA pseudo Dice: 0.6585
2024-07-24 04:54:47.359186: 
2024-07-24 04:54:47.361585: Epoch 37
2024-07-24 04:54:47.363000: Current learning rate: 0.00967
2024-07-24 04:58:47.470325: Validation loss did not improve from -0.56109. Patience: 2/50
2024-07-24 04:58:47.471884: train_loss -0.6886
2024-07-24 04:58:47.473130: val_loss -0.5042
2024-07-24 04:58:47.474206: Pseudo dice [0.6561]
2024-07-24 04:58:47.475339: Epoch time: 240.11 s
2024-07-24 04:58:48.747666: 
2024-07-24 04:58:48.750030: Epoch 38
2024-07-24 04:58:48.751437: Current learning rate: 0.00966
2024-07-24 05:02:48.321105: Validation loss improved from -0.56109 to -0.57376! Patience: 2/50
2024-07-24 05:02:48.322682: train_loss -0.7037
2024-07-24 05:02:48.324049: val_loss -0.5738
2024-07-24 05:02:48.325210: Pseudo dice [0.6949]
2024-07-24 05:02:48.326457: Epoch time: 239.58 s
2024-07-24 05:02:48.327595: Yayy! New best EMA pseudo Dice: 0.6619
2024-07-24 05:02:49.944756: 
2024-07-24 05:02:49.946331: Epoch 39
2024-07-24 05:02:49.947473: Current learning rate: 0.00965
2024-07-24 05:06:50.471676: Validation loss improved from -0.57376 to -0.58011! Patience: 0/50
2024-07-24 05:06:50.473208: train_loss -0.7131
2024-07-24 05:06:50.474341: val_loss -0.5801
2024-07-24 05:06:50.475602: Pseudo dice [0.6965]
2024-07-24 05:06:50.476761: Epoch time: 240.53 s
2024-07-24 05:06:50.821052: Yayy! New best EMA pseudo Dice: 0.6654
2024-07-24 05:06:53.301805: 
2024-07-24 05:06:53.304400: Epoch 40
2024-07-24 05:06:53.305990: Current learning rate: 0.00964
2024-07-24 05:10:53.514535: Validation loss did not improve from -0.58011. Patience: 1/50
2024-07-24 05:10:53.515945: train_loss -0.7133
2024-07-24 05:10:53.517223: val_loss -0.5551
2024-07-24 05:10:53.520982: Pseudo dice [0.6918]
2024-07-24 05:10:53.522877: Epoch time: 240.22 s
2024-07-24 05:10:53.524023: Yayy! New best EMA pseudo Dice: 0.668
2024-07-24 05:10:55.106563: 
2024-07-24 05:10:55.108675: Epoch 41
2024-07-24 05:10:55.109950: Current learning rate: 0.00963
2024-07-24 05:14:55.386263: Validation loss did not improve from -0.58011. Patience: 2/50
2024-07-24 05:14:55.387545: train_loss -0.713
2024-07-24 05:14:55.388802: val_loss -0.5235
2024-07-24 05:14:55.389929: Pseudo dice [0.6698]
2024-07-24 05:14:55.391081: Epoch time: 240.28 s
2024-07-24 05:14:55.392140: Yayy! New best EMA pseudo Dice: 0.6682
2024-07-24 05:14:56.926872: 
2024-07-24 05:14:56.928827: Epoch 42
2024-07-24 05:14:56.930020: Current learning rate: 0.00962
2024-07-24 05:18:57.535040: Validation loss did not improve from -0.58011. Patience: 3/50
2024-07-24 05:18:57.536779: train_loss -0.7107
2024-07-24 05:18:57.538976: val_loss -0.4982
2024-07-24 05:18:57.540727: Pseudo dice [0.651]
2024-07-24 05:18:57.542706: Epoch time: 240.61 s
2024-07-24 05:18:58.704804: 
2024-07-24 05:18:58.706090: Epoch 43
2024-07-24 05:18:58.707215: Current learning rate: 0.00961
2024-07-24 05:22:59.485403: Validation loss did not improve from -0.58011. Patience: 4/50
2024-07-24 05:22:59.487128: train_loss -0.718
2024-07-24 05:22:59.488665: val_loss -0.5604
2024-07-24 05:22:59.507332: Pseudo dice [0.7061]
2024-07-24 05:22:59.511488: Epoch time: 240.78 s
2024-07-24 05:22:59.513282: Yayy! New best EMA pseudo Dice: 0.6705
2024-07-24 05:23:01.141383: 
2024-07-24 05:23:01.143653: Epoch 44
2024-07-24 05:23:01.144974: Current learning rate: 0.0096
2024-07-24 05:27:01.675395: Validation loss improved from -0.58011 to -0.59594! Patience: 4/50
2024-07-24 05:27:01.676971: train_loss -0.7282
2024-07-24 05:27:01.678116: val_loss -0.5959
2024-07-24 05:27:01.679188: Pseudo dice [0.7244]
2024-07-24 05:27:01.680185: Epoch time: 240.54 s
2024-07-24 05:27:02.111592: Yayy! New best EMA pseudo Dice: 0.6758
2024-07-24 05:27:03.633132: 
2024-07-24 05:27:03.635287: Epoch 45
2024-07-24 05:27:03.636983: Current learning rate: 0.00959
2024-07-24 05:31:04.055383: Validation loss did not improve from -0.59594. Patience: 1/50
2024-07-24 05:31:04.057048: train_loss -0.722
2024-07-24 05:31:04.058371: val_loss -0.5385
2024-07-24 05:31:04.059485: Pseudo dice [0.6826]
2024-07-24 05:31:04.060515: Epoch time: 240.43 s
2024-07-24 05:31:04.061631: Yayy! New best EMA pseudo Dice: 0.6765
2024-07-24 05:31:05.577169: 
2024-07-24 05:31:05.579288: Epoch 46
2024-07-24 05:31:05.580740: Current learning rate: 0.00959
2024-07-24 05:35:06.799344: Validation loss improved from -0.59594 to -0.60231! Patience: 1/50
2024-07-24 05:35:06.829251: train_loss -0.7295
2024-07-24 05:35:06.830915: val_loss -0.6023
2024-07-24 05:35:06.832163: Pseudo dice [0.7249]
2024-07-24 05:35:06.833185: Epoch time: 241.25 s
2024-07-24 05:35:06.834219: Yayy! New best EMA pseudo Dice: 0.6814
2024-07-24 05:35:08.442305: 
2024-07-24 05:35:08.444485: Epoch 47
2024-07-24 05:35:08.445556: Current learning rate: 0.00958
2024-07-24 05:39:10.213389: Validation loss did not improve from -0.60231. Patience: 1/50
2024-07-24 05:39:10.215636: train_loss -0.7308
2024-07-24 05:39:10.217283: val_loss -0.5332
2024-07-24 05:39:10.218481: Pseudo dice [0.6853]
2024-07-24 05:39:10.219622: Epoch time: 241.77 s
2024-07-24 05:39:10.220905: Yayy! New best EMA pseudo Dice: 0.6817
2024-07-24 05:39:11.741825: 
2024-07-24 05:39:11.743978: Epoch 48
2024-07-24 05:39:11.745239: Current learning rate: 0.00957
2024-07-24 05:43:13.941246: Validation loss did not improve from -0.60231. Patience: 2/50
2024-07-24 05:43:13.943270: train_loss -0.7162
2024-07-24 05:43:13.944963: val_loss -0.591
2024-07-24 05:43:13.946040: Pseudo dice [0.7122]
2024-07-24 05:43:13.947026: Epoch time: 242.2 s
2024-07-24 05:43:13.948005: Yayy! New best EMA pseudo Dice: 0.6848
2024-07-24 05:43:15.469822: 
2024-07-24 05:43:15.472061: Epoch 49
2024-07-24 05:43:15.473271: Current learning rate: 0.00956
2024-07-24 05:47:16.201427: Validation loss did not improve from -0.60231. Patience: 3/50
2024-07-24 05:47:16.203506: train_loss -0.7294
2024-07-24 05:47:16.205267: val_loss -0.571
2024-07-24 05:47:16.206621: Pseudo dice [0.7149]
2024-07-24 05:47:16.207864: Epoch time: 240.73 s
2024-07-24 05:47:16.551438: Yayy! New best EMA pseudo Dice: 0.6878
2024-07-24 05:47:18.094643: 
2024-07-24 05:47:18.096744: Epoch 50
2024-07-24 05:47:18.097847: Current learning rate: 0.00955
2024-07-24 05:51:18.899761: Validation loss did not improve from -0.60231. Patience: 4/50
2024-07-24 05:51:18.901334: train_loss -0.7344
2024-07-24 05:51:18.902658: val_loss -0.5987
2024-07-24 05:51:18.903837: Pseudo dice [0.7128]
2024-07-24 05:51:18.905056: Epoch time: 240.81 s
2024-07-24 05:51:18.906218: Yayy! New best EMA pseudo Dice: 0.6903
2024-07-24 05:51:20.413185: 
2024-07-24 05:51:20.415501: Epoch 51
2024-07-24 05:51:20.416831: Current learning rate: 0.00954
2024-07-24 05:55:21.053310: Validation loss did not improve from -0.60231. Patience: 5/50
2024-07-24 05:55:21.054865: train_loss -0.7379
2024-07-24 05:55:21.056156: val_loss -0.3991
2024-07-24 05:55:21.057417: Pseudo dice [0.5902]
2024-07-24 05:55:21.058592: Epoch time: 240.64 s
2024-07-24 05:55:22.768939: 
2024-07-24 05:55:22.773848: Epoch 52
2024-07-24 05:55:22.775540: Current learning rate: 0.00953
2024-07-24 05:59:23.638641: Validation loss did not improve from -0.60231. Patience: 6/50
2024-07-24 05:59:23.640116: train_loss -0.7247
2024-07-24 05:59:23.641382: val_loss -0.5068
2024-07-24 05:59:23.642553: Pseudo dice [0.6608]
2024-07-24 05:59:23.643547: Epoch time: 240.87 s
2024-07-24 05:59:24.922689: 
2024-07-24 05:59:24.924902: Epoch 53
2024-07-24 05:59:24.925910: Current learning rate: 0.00952
2024-07-24 06:03:25.664335: Validation loss did not improve from -0.60231. Patience: 7/50
2024-07-24 06:03:25.665730: train_loss -0.7325
2024-07-24 06:03:25.667279: val_loss -0.5863
2024-07-24 06:03:25.668549: Pseudo dice [0.72]
2024-07-24 06:03:25.669799: Epoch time: 240.74 s
2024-07-24 06:03:26.872504: 
2024-07-24 06:03:26.874351: Epoch 54
2024-07-24 06:03:26.875666: Current learning rate: 0.00951
2024-07-24 06:07:27.673470: Validation loss did not improve from -0.60231. Patience: 8/50
2024-07-24 06:07:27.674958: train_loss -0.7398
2024-07-24 06:07:27.676286: val_loss -0.5739
2024-07-24 06:07:27.677327: Pseudo dice [0.7044]
2024-07-24 06:07:27.678454: Epoch time: 240.8 s
2024-07-24 06:07:29.177515: 
2024-07-24 06:07:29.179427: Epoch 55
2024-07-24 06:07:29.180635: Current learning rate: 0.0095
2024-07-24 06:11:29.670241: Validation loss did not improve from -0.60231. Patience: 9/50
2024-07-24 06:11:29.672538: train_loss -0.7377
2024-07-24 06:11:29.674131: val_loss -0.5295
2024-07-24 06:11:29.675292: Pseudo dice [0.6883]
2024-07-24 06:11:29.676290: Epoch time: 240.5 s
2024-07-24 06:11:30.897060: 
2024-07-24 06:11:30.899034: Epoch 56
2024-07-24 06:11:30.900062: Current learning rate: 0.00949
2024-07-24 06:15:29.943275: Validation loss did not improve from -0.60231. Patience: 10/50
2024-07-24 06:15:29.944855: train_loss -0.7429
2024-07-24 06:15:29.946030: val_loss -0.5409
2024-07-24 06:15:29.946930: Pseudo dice [0.681]
2024-07-24 06:15:29.947865: Epoch time: 239.05 s
2024-07-24 06:15:31.114370: 
2024-07-24 06:15:31.115838: Epoch 57
2024-07-24 06:15:31.116935: Current learning rate: 0.00949
2024-07-24 06:19:30.623050: Validation loss did not improve from -0.60231. Patience: 11/50
2024-07-24 06:19:30.624509: train_loss -0.7484
2024-07-24 06:19:30.625893: val_loss -0.5853
2024-07-24 06:19:30.626956: Pseudo dice [0.7154]
2024-07-24 06:19:30.627956: Epoch time: 239.51 s
2024-07-24 06:19:31.806725: 
2024-07-24 06:19:31.808192: Epoch 58
2024-07-24 06:19:31.809280: Current learning rate: 0.00948
2024-07-24 06:23:31.290550: Validation loss did not improve from -0.60231. Patience: 12/50
2024-07-24 06:23:31.292120: train_loss -0.7362
2024-07-24 06:23:31.293292: val_loss -0.5892
2024-07-24 06:23:31.294288: Pseudo dice [0.7107]
2024-07-24 06:23:31.295324: Epoch time: 239.49 s
2024-07-24 06:23:32.511561: 
2024-07-24 06:23:32.514997: Epoch 59
2024-07-24 06:23:32.516677: Current learning rate: 0.00947
2024-07-24 06:27:31.642401: Validation loss did not improve from -0.60231. Patience: 13/50
2024-07-24 06:27:31.643846: train_loss -0.7468
2024-07-24 06:27:31.645372: val_loss -0.5837
2024-07-24 06:27:31.646650: Pseudo dice [0.7116]
2024-07-24 06:27:31.647982: Epoch time: 239.13 s
2024-07-24 06:27:32.001791: Yayy! New best EMA pseudo Dice: 0.6922
2024-07-24 06:27:33.547742: 
2024-07-24 06:27:33.549862: Epoch 60
2024-07-24 06:27:33.551318: Current learning rate: 0.00946
2024-07-24 06:31:32.531804: Validation loss did not improve from -0.60231. Patience: 14/50
2024-07-24 06:31:32.533549: train_loss -0.7556
2024-07-24 06:31:32.535498: val_loss -0.5802
2024-07-24 06:31:32.536862: Pseudo dice [0.7055]
2024-07-24 06:31:32.538210: Epoch time: 238.99 s
2024-07-24 06:31:32.539495: Yayy! New best EMA pseudo Dice: 0.6935
2024-07-24 06:31:34.302800: 
2024-07-24 06:31:34.305137: Epoch 61
2024-07-24 06:31:34.306342: Current learning rate: 0.00945
2024-07-24 06:35:33.399009: Validation loss improved from -0.60231 to -0.63610! Patience: 14/50
2024-07-24 06:35:33.400412: train_loss -0.7555
2024-07-24 06:35:33.401790: val_loss -0.6361
2024-07-24 06:35:33.402989: Pseudo dice [0.7458]
2024-07-24 06:35:33.404240: Epoch time: 239.1 s
2024-07-24 06:35:33.405462: Yayy! New best EMA pseudo Dice: 0.6987
2024-07-24 06:35:34.955801: 
2024-07-24 06:35:34.957348: Epoch 62
2024-07-24 06:35:34.958758: Current learning rate: 0.00944
2024-07-24 06:39:33.477926: Validation loss did not improve from -0.63610. Patience: 1/50
2024-07-24 06:39:33.480912: train_loss -0.758
2024-07-24 06:39:33.482298: val_loss -0.5416
2024-07-24 06:39:33.483263: Pseudo dice [0.6937]
2024-07-24 06:39:33.484318: Epoch time: 238.53 s
2024-07-24 06:39:34.688405: 
2024-07-24 06:39:34.690744: Epoch 63
2024-07-24 06:39:34.692212: Current learning rate: 0.00943
2024-07-24 06:43:34.056390: Validation loss did not improve from -0.63610. Patience: 2/50
2024-07-24 06:43:34.058053: train_loss -0.7489
2024-07-24 06:43:34.059434: val_loss -0.5425
2024-07-24 06:43:34.060663: Pseudo dice [0.6768]
2024-07-24 06:43:34.061919: Epoch time: 239.37 s
2024-07-24 06:43:35.765995: 
2024-07-24 06:43:35.768043: Epoch 64
2024-07-24 06:43:35.769177: Current learning rate: 0.00942
2024-07-24 06:47:35.987813: Validation loss did not improve from -0.63610. Patience: 3/50
2024-07-24 06:47:35.989614: train_loss -0.7677
2024-07-24 06:47:35.990977: val_loss -0.5787
2024-07-24 06:47:35.991991: Pseudo dice [0.699]
2024-07-24 06:47:35.992958: Epoch time: 240.22 s
2024-07-24 06:47:37.527942: 
2024-07-24 06:47:37.529459: Epoch 65
2024-07-24 06:47:37.530567: Current learning rate: 0.00941
2024-07-24 06:51:37.812040: Validation loss did not improve from -0.63610. Patience: 4/50
2024-07-24 06:51:37.813813: train_loss -0.753
2024-07-24 06:51:37.815341: val_loss -0.5618
2024-07-24 06:51:37.816420: Pseudo dice [0.6935]
2024-07-24 06:51:37.817449: Epoch time: 240.29 s
2024-07-24 06:51:38.981605: 
2024-07-24 06:51:38.983693: Epoch 66
2024-07-24 06:51:38.984958: Current learning rate: 0.0094
2024-07-24 06:55:39.626490: Validation loss did not improve from -0.63610. Patience: 5/50
2024-07-24 06:55:39.627827: train_loss -0.7542
2024-07-24 06:55:39.629205: val_loss -0.5638
2024-07-24 06:55:39.630163: Pseudo dice [0.7002]
2024-07-24 06:55:39.631142: Epoch time: 240.65 s
2024-07-24 06:55:40.875710: 
2024-07-24 06:55:40.877474: Epoch 67
2024-07-24 06:55:40.878569: Current learning rate: 0.00939
2024-07-24 06:59:41.514599: Validation loss did not improve from -0.63610. Patience: 6/50
2024-07-24 06:59:41.515889: train_loss -0.7636
2024-07-24 06:59:41.517278: val_loss -0.556
2024-07-24 06:59:41.518410: Pseudo dice [0.6938]
2024-07-24 06:59:41.519416: Epoch time: 240.64 s
2024-07-24 06:59:42.813724: 
2024-07-24 06:59:42.815702: Epoch 68
2024-07-24 06:59:42.817181: Current learning rate: 0.00939
2024-07-24 07:03:44.616181: Validation loss did not improve from -0.63610. Patience: 7/50
2024-07-24 07:03:44.619065: train_loss -0.7605
2024-07-24 07:03:44.620913: val_loss -0.5638
2024-07-24 07:03:44.622168: Pseudo dice [0.6845]
2024-07-24 07:03:44.623369: Epoch time: 241.81 s
2024-07-24 07:03:45.926735: 
2024-07-24 07:03:45.928475: Epoch 69
2024-07-24 07:03:45.929694: Current learning rate: 0.00938
2024-07-24 07:07:47.598507: Validation loss did not improve from -0.63610. Patience: 8/50
2024-07-24 07:07:47.600401: train_loss -0.7356
2024-07-24 07:07:47.601945: val_loss -0.5428
2024-07-24 07:07:47.603183: Pseudo dice [0.6835]
2024-07-24 07:07:47.604224: Epoch time: 241.68 s
2024-07-24 07:07:49.248741: 
2024-07-24 07:07:49.250470: Epoch 70
2024-07-24 07:07:49.251679: Current learning rate: 0.00937
2024-07-24 07:11:49.258716: Validation loss did not improve from -0.63610. Patience: 9/50
2024-07-24 07:11:49.260854: train_loss -0.7559
2024-07-24 07:11:49.262150: val_loss -0.5434
2024-07-24 07:11:49.263399: Pseudo dice [0.6862]
2024-07-24 07:11:49.264616: Epoch time: 240.01 s
2024-07-24 07:11:50.506025: 
2024-07-24 07:11:50.507941: Epoch 71
2024-07-24 07:11:50.508935: Current learning rate: 0.00936
2024-07-24 07:15:49.245577: Validation loss did not improve from -0.63610. Patience: 10/50
2024-07-24 07:15:49.248052: train_loss -0.7616
2024-07-24 07:15:49.249979: val_loss -0.5853
2024-07-24 07:15:49.251307: Pseudo dice [0.7089]
2024-07-24 07:15:49.252551: Epoch time: 238.74 s
2024-07-24 07:15:50.519665: 
2024-07-24 07:15:50.522162: Epoch 72
2024-07-24 07:15:50.523945: Current learning rate: 0.00935
2024-07-24 07:19:49.032309: Validation loss did not improve from -0.63610. Patience: 11/50
2024-07-24 07:19:49.034101: train_loss -0.7567
2024-07-24 07:19:49.035600: val_loss -0.5285
2024-07-24 07:19:49.036695: Pseudo dice [0.676]
2024-07-24 07:19:49.037945: Epoch time: 238.52 s
2024-07-24 07:19:50.396605: 
2024-07-24 07:19:50.398675: Epoch 73
2024-07-24 07:19:50.400075: Current learning rate: 0.00934
2024-07-24 07:23:49.530220: Validation loss did not improve from -0.63610. Patience: 12/50
2024-07-24 07:23:49.532001: train_loss -0.7623
2024-07-24 07:23:49.533542: val_loss -0.5939
2024-07-24 07:23:49.534838: Pseudo dice [0.72]
2024-07-24 07:23:49.535949: Epoch time: 239.14 s
2024-07-24 07:23:51.147079: 
2024-07-24 07:23:51.148698: Epoch 74
2024-07-24 07:23:51.149749: Current learning rate: 0.00933
2024-07-24 07:27:49.229356: Validation loss did not improve from -0.63610. Patience: 13/50
2024-07-24 07:27:49.232190: train_loss -0.7738
2024-07-24 07:27:49.233673: val_loss -0.6112
2024-07-24 07:27:49.234824: Pseudo dice [0.7276]
2024-07-24 07:27:49.235941: Epoch time: 238.09 s
2024-07-24 07:27:49.603559: Yayy! New best EMA pseudo Dice: 0.6988
2024-07-24 07:27:51.810845: 
2024-07-24 07:27:51.812209: Epoch 75
2024-07-24 07:27:51.813213: Current learning rate: 0.00932
2024-07-24 07:31:50.117184: Validation loss did not improve from -0.63610. Patience: 14/50
2024-07-24 07:31:50.118717: train_loss -0.7719
2024-07-24 07:31:50.120172: val_loss -0.5376
2024-07-24 07:31:50.121270: Pseudo dice [0.6746]
2024-07-24 07:31:50.122304: Epoch time: 238.31 s
2024-07-24 07:31:51.397696: 
2024-07-24 07:31:51.399563: Epoch 76
2024-07-24 07:31:51.400767: Current learning rate: 0.00931
2024-07-24 07:35:49.793820: Validation loss did not improve from -0.63610. Patience: 15/50
2024-07-24 07:35:49.795972: train_loss -0.7854
2024-07-24 07:35:49.815975: val_loss -0.6015
2024-07-24 07:35:49.817858: Pseudo dice [0.719]
2024-07-24 07:35:49.819208: Epoch time: 238.4 s
2024-07-24 07:35:52.140002: 
2024-07-24 07:35:52.141756: Epoch 77
2024-07-24 07:35:52.142774: Current learning rate: 0.0093
2024-07-24 07:39:50.435324: Validation loss did not improve from -0.63610. Patience: 16/50
2024-07-24 07:39:50.436843: train_loss -0.7825
2024-07-24 07:39:50.438011: val_loss -0.5884
2024-07-24 07:39:50.441384: Pseudo dice [0.7166]
2024-07-24 07:39:50.443228: Epoch time: 238.3 s
2024-07-24 07:39:50.444355: Yayy! New best EMA pseudo Dice: 0.7004
2024-07-24 07:39:52.035614: 
2024-07-24 07:39:52.037900: Epoch 78
2024-07-24 07:39:52.039220: Current learning rate: 0.0093
2024-07-24 07:43:51.273672: Validation loss did not improve from -0.63610. Patience: 17/50
2024-07-24 07:43:51.276839: train_loss -0.7769
2024-07-24 07:43:51.278202: val_loss -0.5434
2024-07-24 07:43:51.279446: Pseudo dice [0.6852]
2024-07-24 07:43:51.280457: Epoch time: 239.24 s
2024-07-24 07:43:52.545753: 
2024-07-24 07:43:52.547512: Epoch 79
2024-07-24 07:43:52.548662: Current learning rate: 0.00929
2024-07-24 07:47:52.867928: Validation loss did not improve from -0.63610. Patience: 18/50
2024-07-24 07:47:52.871920: train_loss -0.7625
2024-07-24 07:47:52.874333: val_loss -0.5714
2024-07-24 07:47:52.875839: Pseudo dice [0.6978]
2024-07-24 07:47:52.877191: Epoch time: 240.33 s
2024-07-24 07:47:54.455050: 
2024-07-24 07:47:54.457332: Epoch 80
2024-07-24 07:47:54.459106: Current learning rate: 0.00928
2024-07-24 07:51:54.905845: Validation loss did not improve from -0.63610. Patience: 19/50
2024-07-24 07:51:54.907813: train_loss -0.7654
2024-07-24 07:51:54.909373: val_loss -0.4361
2024-07-24 07:51:54.910738: Pseudo dice [0.6]
2024-07-24 07:51:54.911949: Epoch time: 240.45 s
2024-07-24 07:51:56.155088: 
2024-07-24 07:51:56.156958: Epoch 81
2024-07-24 07:51:56.158014: Current learning rate: 0.00927
2024-07-24 07:55:56.268551: Validation loss did not improve from -0.63610. Patience: 20/50
2024-07-24 07:55:56.270237: train_loss -0.7641
2024-07-24 07:55:56.271415: val_loss -0.5965
2024-07-24 07:55:56.272365: Pseudo dice [0.7188]
2024-07-24 07:55:56.273373: Epoch time: 240.12 s
2024-07-24 07:55:57.507936: 
2024-07-24 07:55:57.509716: Epoch 82
2024-07-24 07:55:57.510793: Current learning rate: 0.00926
2024-07-24 07:59:58.177742: Validation loss did not improve from -0.63610. Patience: 21/50
2024-07-24 07:59:58.179163: train_loss -0.7594
2024-07-24 07:59:58.180452: val_loss -0.5224
2024-07-24 07:59:58.181677: Pseudo dice [0.6902]
2024-07-24 07:59:58.182798: Epoch time: 240.67 s
2024-07-24 07:59:59.383450: 
2024-07-24 07:59:59.385497: Epoch 83
2024-07-24 07:59:59.386563: Current learning rate: 0.00925
2024-07-24 08:03:59.869867: Validation loss did not improve from -0.63610. Patience: 22/50
2024-07-24 08:03:59.871440: train_loss -0.7629
2024-07-24 08:03:59.872824: val_loss -0.599
2024-07-24 08:03:59.873970: Pseudo dice [0.7161]
2024-07-24 08:03:59.874972: Epoch time: 240.49 s
2024-07-24 08:04:01.039279: 
2024-07-24 08:04:01.041444: Epoch 84
2024-07-24 08:04:01.042836: Current learning rate: 0.00924
2024-07-24 08:08:01.138170: Validation loss did not improve from -0.63610. Patience: 23/50
2024-07-24 08:08:01.139473: train_loss -0.775
2024-07-24 08:08:01.140805: val_loss -0.5173
2024-07-24 08:08:01.141970: Pseudo dice [0.6674]
2024-07-24 08:08:01.143092: Epoch time: 240.1 s
2024-07-24 08:08:02.667912: 
2024-07-24 08:08:02.670645: Epoch 85
2024-07-24 08:08:02.671802: Current learning rate: 0.00923
2024-07-24 08:12:01.837386: Validation loss did not improve from -0.63610. Patience: 24/50
2024-07-24 08:12:01.839155: train_loss -0.7731
2024-07-24 08:12:01.840312: val_loss -0.5882
2024-07-24 08:12:01.841335: Pseudo dice [0.7128]
2024-07-24 08:12:01.842329: Epoch time: 239.17 s
2024-07-24 08:12:03.733112: 
2024-07-24 08:12:03.735492: Epoch 86
2024-07-24 08:12:03.736864: Current learning rate: 0.00922
2024-07-24 08:16:02.238136: Validation loss did not improve from -0.63610. Patience: 25/50
2024-07-24 08:16:02.240037: train_loss -0.7776
2024-07-24 08:16:02.241397: val_loss -0.5907
2024-07-24 08:16:02.242512: Pseudo dice [0.7246]
2024-07-24 08:16:02.243499: Epoch time: 238.51 s
2024-07-24 08:16:03.449955: 
2024-07-24 08:16:03.451943: Epoch 87
2024-07-24 08:16:03.453071: Current learning rate: 0.00921
2024-07-24 08:20:02.595410: Validation loss did not improve from -0.63610. Patience: 26/50
2024-07-24 08:20:02.597490: train_loss -0.7833
2024-07-24 08:20:02.598934: val_loss -0.5569
2024-07-24 08:20:02.600008: Pseudo dice [0.6906]
2024-07-24 08:20:02.601095: Epoch time: 239.15 s
2024-07-24 08:20:03.762737: 
2024-07-24 08:20:03.765422: Epoch 88
2024-07-24 08:20:03.766604: Current learning rate: 0.0092
2024-07-24 08:24:02.541857: Validation loss did not improve from -0.63610. Patience: 27/50
2024-07-24 08:24:02.543708: train_loss -0.7845
2024-07-24 08:24:02.545113: val_loss -0.5418
2024-07-24 08:24:02.546275: Pseudo dice [0.6753]
2024-07-24 08:24:02.547251: Epoch time: 238.78 s
2024-07-24 08:24:03.705992: 
2024-07-24 08:24:03.707844: Epoch 89
2024-07-24 08:24:03.709077: Current learning rate: 0.0092
2024-07-24 08:28:02.872138: Validation loss did not improve from -0.63610. Patience: 28/50
2024-07-24 08:28:02.874545: train_loss -0.7781
2024-07-24 08:28:02.875999: val_loss -0.5659
2024-07-24 08:28:02.877265: Pseudo dice [0.7019]
2024-07-24 08:28:02.878309: Epoch time: 239.17 s
2024-07-24 08:28:04.461530: 
2024-07-24 08:28:04.463512: Epoch 90
2024-07-24 08:28:04.464552: Current learning rate: 0.00919
2024-07-24 08:32:03.113819: Validation loss did not improve from -0.63610. Patience: 29/50
2024-07-24 08:32:03.115414: train_loss -0.7932
2024-07-24 08:32:03.116826: val_loss -0.6118
2024-07-24 08:32:03.118362: Pseudo dice [0.7343]
2024-07-24 08:32:03.119932: Epoch time: 238.65 s
2024-07-24 08:32:04.298145: 
2024-07-24 08:32:04.300130: Epoch 91
2024-07-24 08:32:04.301636: Current learning rate: 0.00918
2024-07-24 08:36:02.919566: Validation loss did not improve from -0.63610. Patience: 30/50
2024-07-24 08:36:02.921213: train_loss -0.7942
2024-07-24 08:36:02.922608: val_loss -0.5702
2024-07-24 08:36:02.923861: Pseudo dice [0.7059]
2024-07-24 08:36:02.925239: Epoch time: 238.62 s
2024-07-24 08:36:04.146545: 
2024-07-24 08:36:04.148898: Epoch 92
2024-07-24 08:36:04.150204: Current learning rate: 0.00917
2024-07-24 08:40:02.962651: Validation loss did not improve from -0.63610. Patience: 31/50
2024-07-24 08:40:02.965094: train_loss -0.7925
2024-07-24 08:40:02.967077: val_loss -0.5748
2024-07-24 08:40:02.968151: Pseudo dice [0.7014]
2024-07-24 08:40:02.969169: Epoch time: 238.82 s
2024-07-24 08:40:04.138180: 
2024-07-24 08:40:04.140269: Epoch 93
2024-07-24 08:40:04.141329: Current learning rate: 0.00916
2024-07-24 08:44:04.035354: Validation loss did not improve from -0.63610. Patience: 32/50
2024-07-24 08:44:04.036812: train_loss -0.7956
2024-07-24 08:44:04.038001: val_loss -0.5669
2024-07-24 08:44:04.038943: Pseudo dice [0.7151]
2024-07-24 08:44:04.040029: Epoch time: 239.9 s
2024-07-24 08:44:04.040935: Yayy! New best EMA pseudo Dice: 0.7012
2024-07-24 08:44:05.932550: 
2024-07-24 08:44:05.934592: Epoch 94
2024-07-24 08:44:05.935723: Current learning rate: 0.00915
2024-07-24 08:48:06.692640: Validation loss did not improve from -0.63610. Patience: 33/50
2024-07-24 08:48:06.695052: train_loss -0.7859
2024-07-24 08:48:06.696885: val_loss -0.5948
2024-07-24 08:48:06.698189: Pseudo dice [0.725]
2024-07-24 08:48:06.699515: Epoch time: 240.76 s
2024-07-24 08:48:07.037136: Yayy! New best EMA pseudo Dice: 0.7036
2024-07-24 08:48:08.515492: 
2024-07-24 08:48:08.517489: Epoch 95
2024-07-24 08:48:08.518737: Current learning rate: 0.00914
2024-07-24 08:52:08.921350: Validation loss did not improve from -0.63610. Patience: 34/50
2024-07-24 08:52:08.923570: train_loss -0.7907
2024-07-24 08:52:08.924949: val_loss -0.5654
2024-07-24 08:52:08.926019: Pseudo dice [0.6947]
2024-07-24 08:52:08.927158: Epoch time: 240.41 s
2024-07-24 08:52:10.104583: 
2024-07-24 08:52:10.106503: Epoch 96
2024-07-24 08:52:10.107661: Current learning rate: 0.00913
2024-07-24 08:56:10.753189: Validation loss did not improve from -0.63610. Patience: 35/50
2024-07-24 08:56:10.755790: train_loss -0.7786
2024-07-24 08:56:10.757325: val_loss -0.5181
2024-07-24 08:56:10.758502: Pseudo dice [0.6691]
2024-07-24 08:56:10.759633: Epoch time: 240.65 s
2024-07-24 08:56:12.036767: 
2024-07-24 08:56:12.038762: Epoch 97
2024-07-24 08:56:12.039977: Current learning rate: 0.00912
2024-07-24 09:00:12.634326: Validation loss did not improve from -0.63610. Patience: 36/50
2024-07-24 09:00:12.635870: train_loss -0.7807
2024-07-24 09:00:12.637052: val_loss -0.4287
2024-07-24 09:00:12.638135: Pseudo dice [0.5916]
2024-07-24 09:00:12.639271: Epoch time: 240.6 s
2024-07-24 09:00:13.915454: 
2024-07-24 09:00:13.917124: Epoch 98
2024-07-24 09:00:13.918129: Current learning rate: 0.00911
2024-07-24 09:04:15.195256: Validation loss did not improve from -0.63610. Patience: 37/50
2024-07-24 09:04:15.197408: train_loss -0.7507
2024-07-24 09:04:15.198805: val_loss -0.5515
2024-07-24 09:04:15.200062: Pseudo dice [0.6903]
2024-07-24 09:04:15.201323: Epoch time: 241.28 s
2024-07-24 09:04:17.080541: 
2024-07-24 09:04:17.082613: Epoch 99
2024-07-24 09:04:17.083749: Current learning rate: 0.0091
2024-07-24 09:08:18.195022: Validation loss did not improve from -0.63610. Patience: 38/50
2024-07-24 09:08:18.196510: train_loss -0.7583
2024-07-24 09:08:18.198003: val_loss -0.5666
2024-07-24 09:08:18.199306: Pseudo dice [0.686]
2024-07-24 09:08:18.200404: Epoch time: 241.12 s
2024-07-24 09:08:19.868711: 
2024-07-24 09:08:19.870806: Epoch 100
2024-07-24 09:08:19.871968: Current learning rate: 0.0091
2024-07-24 09:12:20.807074: Validation loss did not improve from -0.63610. Patience: 39/50
2024-07-24 09:12:20.808997: train_loss -0.778
2024-07-24 09:12:20.810502: val_loss -0.5462
2024-07-24 09:12:20.811975: Pseudo dice [0.6992]
2024-07-24 09:12:20.813588: Epoch time: 240.94 s
2024-07-24 09:12:22.021776: 
2024-07-24 09:12:22.023593: Epoch 101
2024-07-24 09:12:22.024994: Current learning rate: 0.00909
2024-07-24 09:16:23.771687: Validation loss did not improve from -0.63610. Patience: 40/50
2024-07-24 09:16:23.773431: train_loss -0.7913
2024-07-24 09:16:23.775036: val_loss -0.5538
2024-07-24 09:16:23.776170: Pseudo dice [0.6941]
2024-07-24 09:16:23.777235: Epoch time: 241.75 s
2024-07-24 09:16:24.959090: 
2024-07-24 09:16:24.961456: Epoch 102
2024-07-24 09:16:24.963043: Current learning rate: 0.00908
2024-07-24 09:20:26.812881: Validation loss did not improve from -0.63610. Patience: 41/50
2024-07-24 09:20:26.814830: train_loss -0.7962
2024-07-24 09:20:26.816209: val_loss -0.5645
2024-07-24 09:20:26.817510: Pseudo dice [0.7034]
2024-07-24 09:20:26.818669: Epoch time: 241.86 s
2024-07-24 09:20:28.072576: 
2024-07-24 09:20:28.074478: Epoch 103
2024-07-24 09:20:28.075869: Current learning rate: 0.00907
2024-07-24 09:24:30.512506: Validation loss did not improve from -0.63610. Patience: 42/50
2024-07-24 09:24:30.514337: train_loss -0.8004
2024-07-24 09:24:30.515615: val_loss -0.5441
2024-07-24 09:24:30.516715: Pseudo dice [0.6975]
2024-07-24 09:24:30.517746: Epoch time: 242.44 s
2024-07-24 09:24:31.772133: 
2024-07-24 09:24:31.774106: Epoch 104
2024-07-24 09:24:31.775316: Current learning rate: 0.00906
2024-07-24 09:28:34.174387: Validation loss did not improve from -0.63610. Patience: 43/50
2024-07-24 09:28:34.177391: train_loss -0.8033
2024-07-24 09:28:34.179686: val_loss -0.5453
2024-07-24 09:28:34.180763: Pseudo dice [0.6793]
2024-07-24 09:28:34.181950: Epoch time: 242.41 s
2024-07-24 09:28:35.853728: 
2024-07-24 09:28:35.855476: Epoch 105
2024-07-24 09:28:35.856511: Current learning rate: 0.00905
2024-07-24 09:32:38.084450: Validation loss did not improve from -0.63610. Patience: 44/50
2024-07-24 09:32:38.086212: train_loss -0.801
2024-07-24 09:32:38.087394: val_loss -0.5359
2024-07-24 09:32:38.088468: Pseudo dice [0.6927]
2024-07-24 09:32:38.089749: Epoch time: 242.23 s
2024-07-24 09:32:39.340052: 
2024-07-24 09:32:39.341821: Epoch 106
2024-07-24 09:32:39.342804: Current learning rate: 0.00904
2024-07-24 09:36:41.521030: Validation loss did not improve from -0.63610. Patience: 45/50
2024-07-24 09:36:41.523098: train_loss -0.8042
2024-07-24 09:36:41.525037: val_loss -0.4934
2024-07-24 09:36:41.526555: Pseudo dice [0.6574]
2024-07-24 09:36:41.527621: Epoch time: 242.18 s
2024-07-24 09:36:42.738857: 
2024-07-24 09:36:42.741770: Epoch 107
2024-07-24 09:36:42.743757: Current learning rate: 0.00903
2024-07-24 09:40:44.881089: Validation loss did not improve from -0.63610. Patience: 46/50
2024-07-24 09:40:44.882560: train_loss -0.8072
2024-07-24 09:40:44.883949: val_loss -0.5681
2024-07-24 09:40:44.885369: Pseudo dice [0.7015]
2024-07-24 09:40:44.886732: Epoch time: 242.14 s
2024-07-24 09:40:46.099401: 
2024-07-24 09:40:46.101068: Epoch 108
2024-07-24 09:40:46.102069: Current learning rate: 0.00902
2024-07-24 09:44:48.573912: Validation loss did not improve from -0.63610. Patience: 47/50
2024-07-24 09:44:48.576134: train_loss -0.8074
2024-07-24 09:44:48.577665: val_loss -0.5336
2024-07-24 09:44:48.578685: Pseudo dice [0.6846]
2024-07-24 09:44:48.579771: Epoch time: 242.48 s
2024-07-24 09:44:49.926633: 
2024-07-24 09:44:49.928437: Epoch 109
2024-07-24 09:44:49.929402: Current learning rate: 0.00901
2024-07-24 09:48:52.441344: Validation loss did not improve from -0.63610. Patience: 48/50
2024-07-24 09:48:52.443142: train_loss -0.799
2024-07-24 09:48:52.444586: val_loss -0.5471
2024-07-24 09:48:52.445987: Pseudo dice [0.6926]
2024-07-24 09:48:52.447454: Epoch time: 242.52 s
2024-07-24 09:48:53.986669: 
2024-07-24 09:48:53.989013: Epoch 110
2024-07-24 09:48:53.990439: Current learning rate: 0.009
2024-07-24 09:52:56.590408: Validation loss did not improve from -0.63610. Patience: 49/50
2024-07-24 09:52:56.593587: train_loss -0.8065
2024-07-24 09:52:56.595288: val_loss -0.5545
2024-07-24 09:52:56.596591: Pseudo dice [0.6992]
2024-07-24 09:52:56.598304: Epoch time: 242.61 s
2024-07-24 09:52:58.670333: 
2024-07-24 09:52:58.672285: Epoch 111
2024-07-24 09:52:58.673263: Current learning rate: 0.009
2024-07-24 09:57:01.142826: Validation loss did not improve from -0.63610. Patience: 50/50
2024-07-24 09:57:01.144384: train_loss -0.809
2024-07-24 09:57:01.145747: val_loss -0.5069
2024-07-24 09:57:01.147229: Pseudo dice [0.6644]
2024-07-24 09:57:01.148383: Epoch time: 242.48 s
2024-07-24 09:57:02.336253: Patience reached. Stopping training.
2024-07-24 09:57:02.658449: Training done.
2024-07-24 09:57:02.815298: Using splits from existing split file: /home/gridsan/nchutisilp/datasets/nnUNet_Datasets/nnUNet_preprocessed/Dataset302_Calcium_OCTv2/splits_final.json
2024-07-24 09:57:02.836073: The split file contains 3 splits.
2024-07-24 09:57:02.837508: Desired fold for training: 2
2024-07-24 09:57:02.838731: This split has 4 training and 2 validation cases.
2024-07-24 09:57:02.839940: predicting 401-004
2024-07-24 09:57:02.852501: 401-004, shape torch.Size([1, 375, 498, 498]), rank 0
2024-07-24 09:58:05.935778: predicting 701-013
2024-07-24 09:58:05.963303: 701-013, shape torch.Size([1, 375, 498, 498]), rank 0
2024-07-24 09:58:58.089988: Validation complete
2024-07-24 09:58:58.091756: Mean Validation Dice:  0.6611724353122082
wandb: 
wandb: Run history:
wandb:            ema_fg_dice ▁▁▂▃▃▄▅▅▅▆▆▆▇▇▇▇▇█▇█████████████████████
wandb:   epoch_end_timestamps ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███
wandb: epoch_start_timestamps ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███
wandb:                    lrs ███▇▇▇▇▇▇▆▆▆▆▆▆▅▅▅▅▅▄▄▄▄▄▄▃▃▃▃▃▂▂▂▂▂▂▁▁▁
wandb:           mean_fg_dice ▁▁▄▅▅▆▆▆▆▅▇▇▇▆▇▆▇█▅▇▇█▇▇▇███▇▇█▇▇█▇▇▇▇▇▇
wandb:           train_losses █▆▅▄▄▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▂▂▁▁▁▁▁▂▁▁▁▁
wandb:             val_losses █▇▅▅▄▄▃▃▃▄▂▃▂▃▂▃▂▁▅▂▂▂▂▂▂▁▁▁▂▃▁▂▂▂▃▂▂▂▂▃
wandb: 
wandb: Run summary:
wandb:            ema_fg_dice 0.68739
wandb:   epoch_end_timestamps 1721829421.14422
wandb: epoch_start_timestamps 1721829178.66888
wandb:                    lrs 0.009
wandb:           mean_fg_dice 0.66443
wandb:           train_losses -0.80903
wandb:             val_losses -0.50691
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/gridsan/nchutisilp/datasets/nnUNet_Datasets/nnUNet_results/Dataset302_Calcium_OCTv2/nnUNetTrainer__nnUNetPlans__3d_32x512x512_b2/fold_2/wandb/offline-run-20240724_022351-vo89u4pb
wandb: Find logs at: /home/gridsan/nchutisilp/datasets/nnUNet_Datasets/nnUNet_results/Dataset302_Calcium_OCTv2/nnUNetTrainer__nnUNetPlans__3d_32x512x512_b2/fold_2/wandb/offline-run-20240724_022351-vo89u4pb/logs
WARNING:urllib3.connectionpool:Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x7f85baf0d2e0>: Failed to establish a new connection: [Errno 101] Network is unreachable')': /api/4504800232407040/envelope/
WARNING:urllib3.connectionpool:Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x7f85baf0dd00>: Failed to establish a new connection: [Errno 101] Network is unreachable')': /api/4504800232407040/envelope/
WARNING:urllib3.connectionpool:Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x7f85c00deeb0>: Failed to establish a new connection: [Errno 101] Network is unreachable')': /api/4504800232407040/envelope/
WARNING:urllib3.connectionpool:Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x7f866e579c70>: Failed to establish a new connection: [Errno 101] Network is unreachable')': /api/4504800232407040/envelope/
WARNING:urllib3.connectionpool:Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x7f866e5f4ac0>: Failed to establish a new connection: [Errno 101] Network is unreachable')': /api/4504800232407040/envelope/
WARNING:urllib3.connectionpool:Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x7f866e57f490>: Failed to establish a new connection: [Errno 101] Network is unreachable')': /api/4504800232407040/envelope/
FOLD 2 CONFIG 3d_32x512x512_b2 TRAINER nnUNetTrainer

#######################################################################
Please cite the following paper when using nnU-Net:
Isensee, F., Jaeger, P. F., Kohl, S. A., Petersen, J., & Maier-Hein, K. H. (2021). nnU-Net: a self-configuring method for deep learning-based biomedical image segmentation. Nature methods, 18(2), 203-211.
#######################################################################

There are 2 cases in the source folder
I am process 0 out of 1 (max process ID is 0, we start counting with 0!)
There are 2 cases that I would like to predict

Predicting 101-045:
perform_everything_on_device: True
  0%|          | 0/23 [00:00<?, ?it/s]  4%|▍         | 1/23 [00:11<04:03, 11.08s/it]  9%|▊         | 2/23 [00:11<01:45,  5.05s/it] 13%|█▎        | 3/23 [00:13<01:08,  3.43s/it] 17%|█▋        | 4/23 [00:14<00:50,  2.67s/it] 22%|██▏       | 5/23 [00:16<00:40,  2.25s/it] 26%|██▌       | 6/23 [00:17<00:33,  2.00s/it] 30%|███       | 7/23 [00:19<00:29,  1.83s/it] 35%|███▍      | 8/23 [00:20<00:25,  1.73s/it] 39%|███▉      | 9/23 [00:22<00:23,  1.66s/it] 43%|████▎     | 10/23 [00:23<00:20,  1.61s/it] 48%|████▊     | 11/23 [00:25<00:18,  1.58s/it] 52%|█████▏    | 12/23 [00:26<00:17,  1.56s/it] 57%|█████▋    | 13/23 [00:28<00:15,  1.54s/it] 61%|██████    | 14/23 [00:29<00:13,  1.53s/it] 65%|██████▌   | 15/23 [00:31<00:12,  1.53s/it] 70%|██████▉   | 16/23 [00:32<00:10,  1.52s/it] 74%|███████▍  | 17/23 [00:34<00:09,  1.52s/it] 78%|███████▊  | 18/23 [00:36<00:07,  1.52s/it] 83%|████████▎ | 19/23 [00:37<00:06,  1.52s/it] 87%|████████▋ | 20/23 [00:39<00:04,  1.51s/it] 91%|█████████▏| 21/23 [00:40<00:03,  1.51s/it] 96%|█████████▌| 22/23 [00:42<00:01,  1.51s/it]100%|██████████| 23/23 [00:43<00:00,  1.51s/it]100%|██████████| 23/23 [00:43<00:00,  1.89s/it]
sending off prediction to background worker for resampling and export
done with 101-045

Predicting 706-005:
perform_everything_on_device: True
  0%|          | 0/23 [00:00<?, ?it/s]  4%|▍         | 1/23 [00:00<00:17,  1.24it/s]  9%|▊         | 2/23 [00:02<00:25,  1.22s/it] 13%|█▎        | 3/23 [00:03<00:27,  1.36s/it] 17%|█▋        | 4/23 [00:05<00:26,  1.42s/it] 22%|██▏       | 5/23 [00:06<00:26,  1.45s/it] 26%|██▌       | 6/23 [00:08<00:25,  1.48s/it] 30%|███       | 7/23 [00:09<00:23,  1.49s/it] 35%|███▍      | 8/23 [00:11<00:22,  1.50s/it] 39%|███▉      | 9/23 [00:12<00:21,  1.50s/it] 43%|████▎     | 10/23 [00:14<00:19,  1.51s/it] 48%|████▊     | 11/23 [00:15<00:18,  1.51s/it] 52%|█████▏    | 12/23 [00:17<00:16,  1.51s/it] 57%|█████▋    | 13/23 [00:18<00:15,  1.51s/it] 61%|██████    | 14/23 [00:20<00:13,  1.52s/it] 65%|██████▌   | 15/23 [00:22<00:12,  1.52s/it] 70%|██████▉   | 16/23 [00:23<00:10,  1.52s/it] 74%|███████▍  | 17/23 [00:25<00:09,  1.52s/it] 78%|███████▊  | 18/23 [00:26<00:07,  1.52s/it] 83%|████████▎ | 19/23 [00:28<00:06,  1.52s/it] 87%|████████▋ | 20/23 [00:29<00:04,  1.52s/it] 91%|█████████▏| 21/23 [00:31<00:03,  1.52s/it] 96%|█████████▌| 22/23 [00:32<00:01,  1.52s/it]100%|██████████| 23/23 [00:34<00:00,  1.52s/it]100%|██████████| 23/23 [00:34<00:00,  1.49s/it]
sending off prediction to background worker for resampling and export
done with 706-005
Completed FOLD 2 CONFIG 3d_32x512x512_b2 TRAINER nnUNetTrainer
