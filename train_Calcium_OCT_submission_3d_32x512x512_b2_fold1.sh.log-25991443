/home/gridsan/nchutisilp/.conda/envs/nnunet/bin/python
wandb: Tracking run with wandb version 0.16.6
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.

############################
INFO: You are using the old nnU-Net default plans. We have updated our recommendations. Please consider using those instead! Read more here: https://github.com/MIC-DKFZ/nnUNet/blob/master/documentation/resenc_presets.md
############################

Using device: cuda:0

#######################################################################
Please cite the following paper when using nnU-Net:
Isensee, F., Jaeger, P. F., Kohl, S. A., Petersen, J., & Maier-Hein, K. H. (2021). nnU-Net: a self-configuring method for deep learning-based biomedical image segmentation. Nature methods, 18(2), 203-211.
#######################################################################

2024-05-08 13:59:23.212616: Using torch.compile...
/home/gridsan/nchutisilp/.local/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn("The verbose parameter is deprecated. Please use get_last_lr() "
################### Loading pretrained weights from file  /home/gridsan/nchutisilp/datasets/nnUNet_Datasets/nnUNet_results/Dataset300_Lumen_and_Wall_OCT/nnUNetTrainer__nnUNetPreprocessPlans__3d_fullres/fold_all/checkpoint_best.pth ###################
Below is the list of overlapping blocks in pretrained model and nnUNet architecture:
encoder.stages.0.0.convs.0.conv.weight shape torch.Size([32, 1, 3, 3, 3])
encoder.stages.0.0.convs.0.conv.bias shape torch.Size([32])
encoder.stages.0.0.convs.0.norm.weight shape torch.Size([32])
encoder.stages.0.0.convs.0.norm.bias shape torch.Size([32])
encoder.stages.0.0.convs.0.all_modules.0.weight shape torch.Size([32, 1, 3, 3, 3])
encoder.stages.0.0.convs.0.all_modules.0.bias shape torch.Size([32])
encoder.stages.0.0.convs.0.all_modules.1.weight shape torch.Size([32])
encoder.stages.0.0.convs.0.all_modules.1.bias shape torch.Size([32])
encoder.stages.0.0.convs.1.conv.weight shape torch.Size([32, 32, 3, 3, 3])
encoder.stages.0.0.convs.1.conv.bias shape torch.Size([32])
encoder.stages.0.0.convs.1.norm.weight shape torch.Size([32])
encoder.stages.0.0.convs.1.norm.bias shape torch.Size([32])
encoder.stages.0.0.convs.1.all_modules.0.weight shape torch.Size([32, 32, 3, 3, 3])
encoder.stages.0.0.convs.1.all_modules.0.bias shape torch.Size([32])
encoder.stages.0.0.convs.1.all_modules.1.weight shape torch.Size([32])
encoder.stages.0.0.convs.1.all_modules.1.bias shape torch.Size([32])
encoder.stages.1.0.convs.0.conv.weight shape torch.Size([64, 32, 3, 3, 3])
encoder.stages.1.0.convs.0.conv.bias shape torch.Size([64])
encoder.stages.1.0.convs.0.norm.weight shape torch.Size([64])
encoder.stages.1.0.convs.0.norm.bias shape torch.Size([64])
encoder.stages.1.0.convs.0.all_modules.0.weight shape torch.Size([64, 32, 3, 3, 3])
encoder.stages.1.0.convs.0.all_modules.0.bias shape torch.Size([64])
encoder.stages.1.0.convs.0.all_modules.1.weight shape torch.Size([64])
encoder.stages.1.0.convs.0.all_modules.1.bias shape torch.Size([64])
encoder.stages.1.0.convs.1.conv.weight shape torch.Size([64, 64, 3, 3, 3])
encoder.stages.1.0.convs.1.conv.bias shape torch.Size([64])
encoder.stages.1.0.convs.1.norm.weight shape torch.Size([64])
encoder.stages.1.0.convs.1.norm.bias shape torch.Size([64])
encoder.stages.1.0.convs.1.all_modules.0.weight shape torch.Size([64, 64, 3, 3, 3])
encoder.stages.1.0.convs.1.all_modules.0.bias shape torch.Size([64])
encoder.stages.1.0.convs.1.all_modules.1.weight shape torch.Size([64])
encoder.stages.1.0.convs.1.all_modules.1.bias shape torch.Size([64])
encoder.stages.2.0.convs.0.conv.weight shape torch.Size([128, 64, 3, 3, 3])
encoder.stages.2.0.convs.0.conv.bias shape torch.Size([128])
encoder.stages.2.0.convs.0.norm.weight shape torch.Size([128])
encoder.stages.2.0.convs.0.norm.bias shape torch.Size([128])
encoder.stages.2.0.convs.0.all_modules.0.weight shape torch.Size([128, 64, 3, 3, 3])
encoder.stages.2.0.convs.0.all_modules.0.bias shape torch.Size([128])
encoder.stages.2.0.convs.0.all_modules.1.weight shape torch.Size([128])
encoder.stages.2.0.convs.0.all_modules.1.bias shape torch.Size([128])
encoder.stages.2.0.convs.1.conv.weight shape torch.Size([128, 128, 3, 3, 3])
encoder.stages.2.0.convs.1.conv.bias shape torch.Size([128])
encoder.stages.2.0.convs.1.norm.weight shape torch.Size([128])
encoder.stages.2.0.convs.1.norm.bias shape torch.Size([128])
encoder.stages.2.0.convs.1.all_modules.0.weight shape torch.Size([128, 128, 3, 3, 3])
encoder.stages.2.0.convs.1.all_modules.0.bias shape torch.Size([128])
encoder.stages.2.0.convs.1.all_modules.1.weight shape torch.Size([128])
encoder.stages.2.0.convs.1.all_modules.1.bias shape torch.Size([128])
encoder.stages.3.0.convs.0.conv.weight shape torch.Size([256, 128, 3, 3, 3])
encoder.stages.3.0.convs.0.conv.bias shape torch.Size([256])
encoder.stages.3.0.convs.0.norm.weight shape torch.Size([256])
encoder.stages.3.0.convs.0.norm.bias shape torch.Size([256])
encoder.stages.3.0.convs.0.all_modules.0.weight shape torch.Size([256, 128, 3, 3, 3])
encoder.stages.3.0.convs.0.all_modules.0.bias shape torch.Size([256])
encoder.stages.3.0.convs.0.all_modules.1.weight shape torch.Size([256])
encoder.stages.3.0.convs.0.all_modules.1.bias shape torch.Size([256])
encoder.stages.3.0.convs.1.conv.weight shape torch.Size([256, 256, 3, 3, 3])
encoder.stages.3.0.convs.1.conv.bias shape torch.Size([256])
encoder.stages.3.0.convs.1.norm.weight shape torch.Size([256])
encoder.stages.3.0.convs.1.norm.bias shape torch.Size([256])
encoder.stages.3.0.convs.1.all_modules.0.weight shape torch.Size([256, 256, 3, 3, 3])
encoder.stages.3.0.convs.1.all_modules.0.bias shape torch.Size([256])
encoder.stages.3.0.convs.1.all_modules.1.weight shape torch.Size([256])
encoder.stages.3.0.convs.1.all_modules.1.bias shape torch.Size([256])
encoder.stages.4.0.convs.0.conv.weight shape torch.Size([320, 256, 3, 3, 3])
encoder.stages.4.0.convs.0.conv.bias shape torch.Size([320])
encoder.stages.4.0.convs.0.norm.weight shape torch.Size([320])
encoder.stages.4.0.convs.0.norm.bias shape torch.Size([320])
encoder.stages.4.0.convs.0.all_modules.0.weight shape torch.Size([320, 256, 3, 3, 3])
encoder.stages.4.0.convs.0.all_modules.0.bias shape torch.Size([320])
encoder.stages.4.0.convs.0.all_modules.1.weight shape torch.Size([320])
encoder.stages.4.0.convs.0.all_modules.1.bias shape torch.Size([320])
encoder.stages.4.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.4.0.convs.1.conv.bias shape torch.Size([320])
encoder.stages.4.0.convs.1.norm.weight shape torch.Size([320])
encoder.stages.4.0.convs.1.norm.bias shape torch.Size([320])
encoder.stages.4.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.4.0.convs.1.all_modules.0.bias shape torch.Size([320])
encoder.stages.4.0.convs.1.all_modules.1.weight shape torch.Size([320])
encoder.stages.4.0.convs.1.all_modules.1.bias shape torch.Size([320])
encoder.stages.5.0.convs.0.conv.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.5.0.convs.0.conv.bias shape torch.Size([320])
encoder.stages.5.0.convs.0.norm.weight shape torch.Size([320])
encoder.stages.5.0.convs.0.norm.bias shape torch.Size([320])
encoder.stages.5.0.convs.0.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.5.0.convs.0.all_modules.0.bias shape torch.Size([320])
encoder.stages.5.0.convs.0.all_modules.1.weight shape torch.Size([320])
encoder.stages.5.0.convs.0.all_modules.1.bias shape torch.Size([320])
encoder.stages.5.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.5.0.convs.1.conv.bias shape torch.Size([320])
encoder.stages.5.0.convs.1.norm.weight shape torch.Size([320])
encoder.stages.5.0.convs.1.norm.bias shape torch.Size([320])
encoder.stages.5.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.5.0.convs.1.all_modules.0.bias shape torch.Size([320])
encoder.stages.5.0.convs.1.all_modules.1.weight shape torch.Size([320])
encoder.stages.5.0.convs.1.all_modules.1.bias shape torch.Size([320])
decoder.encoder.stages.0.0.convs.0.conv.weight shape torch.Size([32, 1, 3, 3, 3])
decoder.encoder.stages.0.0.convs.0.conv.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.norm.weight shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.norm.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.all_modules.0.weight shape torch.Size([32, 1, 3, 3, 3])
decoder.encoder.stages.0.0.convs.0.all_modules.0.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.all_modules.1.weight shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.all_modules.1.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.conv.weight shape torch.Size([32, 32, 3, 3, 3])
decoder.encoder.stages.0.0.convs.1.conv.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.norm.weight shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.norm.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.all_modules.0.weight shape torch.Size([32, 32, 3, 3, 3])
decoder.encoder.stages.0.0.convs.1.all_modules.0.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.all_modules.1.weight shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.all_modules.1.bias shape torch.Size([32])
decoder.encoder.stages.1.0.convs.0.conv.weight shape torch.Size([64, 32, 3, 3, 3])
decoder.encoder.stages.1.0.convs.0.conv.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.norm.weight shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.norm.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.all_modules.0.weight shape torch.Size([64, 32, 3, 3, 3])
decoder.encoder.stages.1.0.convs.0.all_modules.0.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.all_modules.1.weight shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.all_modules.1.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.conv.weight shape torch.Size([64, 64, 3, 3, 3])
decoder.encoder.stages.1.0.convs.1.conv.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.norm.weight shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.norm.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.all_modules.0.weight shape torch.Size([64, 64, 3, 3, 3])
decoder.encoder.stages.1.0.convs.1.all_modules.0.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.all_modules.1.weight shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.all_modules.1.bias shape torch.Size([64])
decoder.encoder.stages.2.0.convs.0.conv.weight shape torch.Size([128, 64, 3, 3, 3])
decoder.encoder.stages.2.0.convs.0.conv.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.norm.weight shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.norm.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.all_modules.0.weight shape torch.Size([128, 64, 3, 3, 3])
decoder.encoder.stages.2.0.convs.0.all_modules.0.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.all_modules.1.weight shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.all_modules.1.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.conv.weight shape torch.Size([128, 128, 3, 3, 3])
decoder.encoder.stages.2.0.convs.1.conv.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.norm.weight shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.norm.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.all_modules.0.weight shape torch.Size([128, 128, 3, 3, 3])
decoder.encoder.stages.2.0.convs.1.all_modules.0.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.all_modules.1.weight shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.all_modules.1.bias shape torch.Size([128])
decoder.encoder.stages.3.0.convs.0.conv.weight shape torch.Size([256, 128, 3, 3, 3])
decoder.encoder.stages.3.0.convs.0.conv.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.norm.weight shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.norm.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.all_modules.0.weight shape torch.Size([256, 128, 3, 3, 3])
decoder.encoder.stages.3.0.convs.0.all_modules.0.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.all_modules.1.weight shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.all_modules.1.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.conv.weight shape torch.Size([256, 256, 3, 3, 3])
decoder.encoder.stages.3.0.convs.1.conv.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.norm.weight shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.norm.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.all_modules.0.weight shape torch.Size([256, 256, 3, 3, 3])
decoder.encoder.stages.3.0.convs.1.all_modules.0.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.all_modules.1.weight shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.all_modules.1.bias shape torch.Size([256])
decoder.encoder.stages.4.0.convs.0.conv.weight shape torch.Size([320, 256, 3, 3, 3])
decoder.encoder.stages.4.0.convs.0.conv.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.norm.weight shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.norm.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.all_modules.0.weight shape torch.Size([320, 256, 3, 3, 3])
decoder.encoder.stages.4.0.convs.0.all_modules.0.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.all_modules.1.weight shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.all_modules.1.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.4.0.convs.1.conv.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.norm.weight shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.norm.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.4.0.convs.1.all_modules.0.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.all_modules.1.weight shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.all_modules.1.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.conv.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.5.0.convs.0.conv.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.norm.weight shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.norm.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.5.0.convs.0.all_modules.0.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.all_modules.1.weight shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.all_modules.1.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.5.0.convs.1.conv.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.norm.weight shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.norm.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.5.0.convs.1.all_modules.0.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.all_modules.1.weight shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.all_modules.1.bias shape torch.Size([320])
decoder.stages.0.convs.0.conv.weight shape torch.Size([320, 640, 3, 3, 3])
decoder.stages.0.convs.0.conv.bias shape torch.Size([320])
decoder.stages.0.convs.0.norm.weight shape torch.Size([320])
decoder.stages.0.convs.0.norm.bias shape torch.Size([320])
decoder.stages.0.convs.0.all_modules.0.weight shape torch.Size([320, 640, 3, 3, 3])
decoder.stages.0.convs.0.all_modules.0.bias shape torch.Size([320])
decoder.stages.0.convs.0.all_modules.1.weight shape torch.Size([320])
decoder.stages.0.convs.0.all_modules.1.bias shape torch.Size([320])
decoder.stages.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.stages.0.convs.1.conv.bias shape torch.Size([320])
decoder.stages.0.convs.1.norm.weight shape torch.Size([320])
decoder.stages.0.convs.1.norm.bias shape torch.Size([320])
decoder.stages.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.stages.0.convs.1.all_modules.0.bias shape torch.Size([320])
decoder.stages.0.convs.1.all_modules.1.weight shape torch.Size([320])
decoder.stages.0.convs.1.all_modules.1.bias shape torch.Size([320])
decoder.stages.1.convs.0.conv.weight shape torch.Size([256, 512, 3, 3, 3])
decoder.stages.1.convs.0.conv.bias shape torch.Size([256])
decoder.stages.1.convs.0.norm.weight shape torch.Size([256])
decoder.stages.1.convs.0.norm.bias shape torch.Size([256])
decoder.stages.1.convs.0.all_modules.0.weight shape torch.Size([256, 512, 3, 3, 3])
decoder.stages.1.convs.0.all_modules.0.bias shape torch.Size([256])
decoder.stages.1.convs.0.all_modules.1.weight shape torch.Size([256])
decoder.stages.1.convs.0.all_modules.1.bias shape torch.Size([256])
decoder.stages.1.convs.1.conv.weight shape torch.Size([256, 256, 3, 3, 3])
decoder.stages.1.convs.1.conv.bias shape torch.Size([256])
decoder.stages.1.convs.1.norm.weight shape torch.Size([256])
decoder.stages.1.convs.1.norm.bias shape torch.Size([256])
decoder.stages.1.convs.1.all_modules.0.weight shape torch.Size([256, 256, 3, 3, 3])
decoder.stages.1.convs.1.all_modules.0.bias shape torch.Size([256])
decoder.stages.1.convs.1.all_modules.1.weight shape torch.Size([256])
decoder.stages.1.convs.1.all_modules.1.bias shape torch.Size([256])
decoder.stages.2.convs.0.conv.weight shape torch.Size([128, 256, 3, 3, 3])
decoder.stages.2.convs.0.conv.bias shape torch.Size([128])
decoder.stages.2.convs.0.norm.weight shape torch.Size([128])
decoder.stages.2.convs.0.norm.bias shape torch.Size([128])
decoder.stages.2.convs.0.all_modules.0.weight shape torch.Size([128, 256, 3, 3, 3])
decoder.stages.2.convs.0.all_modules.0.bias shape torch.Size([128])
decoder.stages.2.convs.0.all_modules.1.weight shape torch.Size([128])
decoder.stages.2.convs.0.all_modules.1.bias shape torch.Size([128])
decoder.stages.2.convs.1.conv.weight shape torch.Size([128, 128, 3, 3, 3])
decoder.stages.2.convs.1.conv.bias shape torch.Size([128])
decoder.stages.2.convs.1.norm.weight shape torch.Size([128])
decoder.stages.2.convs.1.norm.bias shape torch.Size([128])
decoder.stages.2.convs.1.all_modules.0.weight shape torch.Size([128, 128, 3, 3, 3])
decoder.stages.2.convs.1.all_modules.0.bias shape torch.Size([128])
decoder.stages.2.convs.1.all_modules.1.weight shape torch.Size([128])
decoder.stages.2.convs.1.all_modules.1.bias shape torch.Size([128])
decoder.stages.3.convs.0.conv.weight shape torch.Size([64, 128, 3, 3, 3])
decoder.stages.3.convs.0.conv.bias shape torch.Size([64])
decoder.stages.3.convs.0.norm.weight shape torch.Size([64])
decoder.stages.3.convs.0.norm.bias shape torch.Size([64])
decoder.stages.3.convs.0.all_modules.0.weight shape torch.Size([64, 128, 3, 3, 3])
decoder.stages.3.convs.0.all_modules.0.bias shape torch.Size([64])
decoder.stages.3.convs.0.all_modules.1.weight shape torch.Size([64])
decoder.stages.3.convs.0.all_modules.1.bias shape torch.Size([64])
decoder.stages.3.convs.1.conv.weight shape torch.Size([64, 64, 3, 3, 3])
decoder.stages.3.convs.1.conv.bias shape torch.Size([64])
decoder.stages.3.convs.1.norm.weight shape torch.Size([64])
decoder.stages.3.convs.1.norm.bias shape torch.Size([64])
decoder.stages.3.convs.1.all_modules.0.weight shape torch.Size([64, 64, 3, 3, 3])
decoder.stages.3.convs.1.all_modules.0.bias shape torch.Size([64])
decoder.stages.3.convs.1.all_modules.1.weight shape torch.Size([64])
decoder.stages.3.convs.1.all_modules.1.bias shape torch.Size([64])
decoder.stages.4.convs.0.conv.weight shape torch.Size([32, 64, 3, 3, 3])
decoder.stages.4.convs.0.conv.bias shape torch.Size([32])
decoder.stages.4.convs.0.norm.weight shape torch.Size([32])
decoder.stages.4.convs.0.norm.bias shape torch.Size([32])
decoder.stages.4.convs.0.all_modules.0.weight shape torch.Size([32, 64, 3, 3, 3])
decoder.stages.4.convs.0.all_modules.0.bias shape torch.Size([32])
decoder.stages.4.convs.0.all_modules.1.weight shape torch.Size([32])
decoder.stages.4.convs.0.all_modules.1.bias shape torch.Size([32])
decoder.stages.4.convs.1.conv.weight shape torch.Size([32, 32, 3, 3, 3])
decoder.stages.4.convs.1.conv.bias shape torch.Size([32])
decoder.stages.4.convs.1.norm.weight shape torch.Size([32])
decoder.stages.4.convs.1.norm.bias shape torch.Size([32])
decoder.stages.4.convs.1.all_modules.0.weight shape torch.Size([32, 32, 3, 3, 3])
decoder.stages.4.convs.1.all_modules.0.bias shape torch.Size([32])
decoder.stages.4.convs.1.all_modules.1.weight shape torch.Size([32])
decoder.stages.4.convs.1.all_modules.1.bias shape torch.Size([32])
decoder.transpconvs.0.weight shape torch.Size([320, 320, 1, 2, 2])
decoder.transpconvs.0.bias shape torch.Size([320])
decoder.transpconvs.1.weight shape torch.Size([320, 256, 2, 2, 2])
decoder.transpconvs.1.bias shape torch.Size([256])
decoder.transpconvs.2.weight shape torch.Size([256, 128, 2, 2, 2])
decoder.transpconvs.2.bias shape torch.Size([128])
decoder.transpconvs.3.weight shape torch.Size([128, 64, 2, 2, 2])
decoder.transpconvs.3.bias shape torch.Size([64])
decoder.transpconvs.4.weight shape torch.Size([64, 32, 2, 2, 2])
decoder.transpconvs.4.bias shape torch.Size([32])
################### Done ###################
2024-05-08 13:59:48.649978: do_dummy_2d_data_aug: True
2024-05-08 13:59:48.674281: Using splits from existing split file: /home/gridsan/nchutisilp/datasets/nnUNet_Datasets/nnUNet_preprocessed/Dataset302_Calcium_OCTv2/splits_final.json
2024-05-08 13:59:48.692191: The split file contains 3 splits.
2024-05-08 13:59:48.693795: Desired fold for training: 1
2024-05-08 13:59:48.694870: This split has 4 training and 2 validation cases.
using pin_memory on device 0
using pin_memory on device 0

This is the configuration used by this training:
Configuration name: 3d_32x512x512_b2
 {'data_identifier': 'nnUNetPlans_3d_fullres', 'preprocessor_name': 'DefaultPreprocessor', 'batch_size': 2, 'patch_size': [32, 512, 512], 'median_image_size_in_voxels': [375.0, 498.0, 498.0], 'spacing': [1.0, 1.0, 1.0], 'normalization_schemes': ['ZScoreNormalization'], 'use_mask_for_norm': [False], 'resampling_fn_data': 'resample_data_or_seg_to_shape', 'resampling_fn_seg': 'resample_data_or_seg_to_shape', 'resampling_fn_data_kwargs': {'is_seg': False, 'order': 3, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_seg_kwargs': {'is_seg': True, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_probabilities': 'resample_data_or_seg_to_shape', 'resampling_fn_probabilities_kwargs': {'is_seg': False, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'architecture': {'network_class_name': 'dynamic_network_architectures.architectures.unet.PlainConvUNet', 'arch_kwargs': {'n_stages': 6, 'features_per_stage': [32, 64, 128, 256, 320, 320], 'conv_op': 'torch.nn.modules.conv.Conv3d', 'kernel_sizes': [[3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3]], 'strides': [[1, 1, 1], [2, 2, 2], [2, 2, 2], [2, 2, 2], [2, 2, 2], [1, 2, 2]], 'n_conv_per_stage': [2, 2, 2, 2, 2, 2], 'n_conv_per_stage_decoder': [2, 2, 2, 2, 2], 'conv_bias': True, 'norm_op': 'torch.nn.modules.instancenorm.InstanceNorm3d', 'norm_op_kwargs': {'eps': 1e-05, 'affine': True}, 'dropout_op': None, 'dropout_op_kwargs': None, 'nonlin': 'torch.nn.LeakyReLU', 'nonlin_kwargs': {'inplace': True}, 'deep_supervision': True}, '_kw_requires_import': ['conv_op', 'norm_op', 'dropout_op', 'nonlin']}, 'batch_dice': True, 'inherits_from': '3d_fullres'} 

These are the global plan.json settings:
 {'dataset_name': 'Dataset302_Calcium_OCTv2', 'plans_name': 'nnUNetPlans', 'original_median_spacing_after_transp': [1.0, 1.0, 1.0], 'original_median_shape_after_transp': [375, 498, 498], 'image_reader_writer': 'SimpleITKIO', 'transpose_forward': [0, 1, 2], 'transpose_backward': [0, 1, 2], 'experiment_planner_used': 'ExperimentPlanner', 'label_manager': 'LabelManager', 'foreground_intensity_properties_per_channel': {'0': {'max': 0.4977976381778717, 'mean': 0.13332499563694, 'median': 0.08871842920780182, 'min': 0.0, 'percentile_00_5': 0.014754901640117168, 'percentile_99_5': 0.4977976381778717, 'std': 0.12022686749696732}}} 

2024-05-08 13:59:58.091944: unpacking dataset...
2024-05-08 14:00:04.482235: unpacking done...
2024-05-08 14:00:04.537998: Unable to plot network architecture: nnUNet_compile is enabled!
2024-05-08 14:00:04.627789: 
2024-05-08 14:00:04.629282: Epoch 0
2024-05-08 14:00:04.630492: Current learning rate: 0.01
2024-05-08 14:06:22.648108: Validation loss improved from 1000.00000 to -0.50107! Patience: 0/50
2024-05-08 14:06:22.755255: train_loss -0.4202
2024-05-08 14:06:22.769493: val_loss -0.5011
2024-05-08 14:06:22.771240: Pseudo dice [0.6925]
2024-05-08 14:06:22.772366: Epoch time: 378.02 s
2024-05-08 14:06:22.773344: Yayy! New best EMA pseudo Dice: 0.6925
2024-05-08 14:06:25.565861: 
2024-05-08 14:06:25.567644: Epoch 1
2024-05-08 14:06:25.569022: Current learning rate: 0.00999
2024-05-08 14:10:15.913465: Validation loss improved from -0.50107 to -0.52149! Patience: 0/50
2024-05-08 14:10:15.915131: train_loss -0.571
2024-05-08 14:10:15.916287: val_loss -0.5215
2024-05-08 14:10:15.917238: Pseudo dice [0.6832]
2024-05-08 14:10:15.918301: Epoch time: 230.35 s
2024-05-08 14:10:17.080118: 
2024-05-08 14:10:17.082565: Epoch 2
2024-05-08 14:10:17.084173: Current learning rate: 0.00998
2024-05-08 14:14:07.426340: Validation loss did not improve from -0.52149. Patience: 1/50
2024-05-08 14:14:07.427946: train_loss -0.6222
2024-05-08 14:14:07.429277: val_loss -0.5128
2024-05-08 14:14:07.430393: Pseudo dice [0.6687]
2024-05-08 14:14:07.431405: Epoch time: 230.35 s
2024-05-08 14:14:08.717544: 
2024-05-08 14:14:08.719667: Epoch 3
2024-05-08 14:14:08.720839: Current learning rate: 0.00997
2024-05-08 14:17:58.972551: Validation loss did not improve from -0.52149. Patience: 2/50
2024-05-08 14:17:58.974098: train_loss -0.658
2024-05-08 14:17:58.975337: val_loss -0.4898
2024-05-08 14:17:58.976547: Pseudo dice [0.6708]
2024-05-08 14:17:58.977878: Epoch time: 230.26 s
2024-05-08 14:18:00.167962: 
2024-05-08 14:18:00.169929: Epoch 4
2024-05-08 14:18:00.171291: Current learning rate: 0.00996
2024-05-08 14:21:50.710459: Validation loss did not improve from -0.52149. Patience: 3/50
2024-05-08 14:21:50.712902: train_loss -0.6768
2024-05-08 14:21:50.714809: val_loss -0.5139
2024-05-08 14:21:50.716574: Pseudo dice [0.6891]
2024-05-08 14:21:50.718109: Epoch time: 230.55 s
2024-05-08 14:21:52.500185: 
2024-05-08 14:21:52.502384: Epoch 5
2024-05-08 14:21:52.504033: Current learning rate: 0.00995
2024-05-08 14:25:43.349760: Validation loss improved from -0.52149 to -0.53752! Patience: 3/50
2024-05-08 14:25:43.352374: train_loss -0.6725
2024-05-08 14:25:43.354017: val_loss -0.5375
2024-05-08 14:25:43.355148: Pseudo dice [0.711]
2024-05-08 14:25:43.356302: Epoch time: 230.85 s
2024-05-08 14:25:44.582073: 
2024-05-08 14:25:44.583989: Epoch 6
2024-05-08 14:25:44.585095: Current learning rate: 0.00995
2024-05-08 14:29:35.129842: Validation loss did not improve from -0.53752. Patience: 1/50
2024-05-08 14:29:35.131143: train_loss -0.675
2024-05-08 14:29:35.132643: val_loss -0.529
2024-05-08 14:29:35.133735: Pseudo dice [0.6862]
2024-05-08 14:29:35.134885: Epoch time: 230.55 s
2024-05-08 14:29:36.744871: 
2024-05-08 14:29:36.746664: Epoch 7
2024-05-08 14:29:36.748240: Current learning rate: 0.00994
2024-05-08 14:33:27.333714: Validation loss improved from -0.53752 to -0.57028! Patience: 1/50
2024-05-08 14:33:27.335023: train_loss -0.692
2024-05-08 14:33:27.336406: val_loss -0.5703
2024-05-08 14:33:27.337829: Pseudo dice [0.7224]
2024-05-08 14:33:27.339048: Epoch time: 230.59 s
2024-05-08 14:33:27.340165: Yayy! New best EMA pseudo Dice: 0.6929
2024-05-08 14:33:28.876815: 
2024-05-08 14:33:28.879116: Epoch 8
2024-05-08 14:33:28.880885: Current learning rate: 0.00993
2024-05-08 14:37:20.285336: Validation loss did not improve from -0.57028. Patience: 1/50
2024-05-08 14:37:20.286891: train_loss -0.7086
2024-05-08 14:37:20.288128: val_loss -0.5404
2024-05-08 14:37:20.289166: Pseudo dice [0.7093]
2024-05-08 14:37:20.290372: Epoch time: 231.41 s
2024-05-08 14:37:20.291453: Yayy! New best EMA pseudo Dice: 0.6945
2024-05-08 14:37:21.876124: 
2024-05-08 14:37:21.877915: Epoch 9
2024-05-08 14:37:21.879218: Current learning rate: 0.00992
2024-05-08 14:41:13.123084: Validation loss did not improve from -0.57028. Patience: 2/50
2024-05-08 14:41:13.124808: train_loss -0.7201
2024-05-08 14:41:13.126009: val_loss -0.5334
2024-05-08 14:41:13.127115: Pseudo dice [0.7031]
2024-05-08 14:41:13.128324: Epoch time: 231.25 s
2024-05-08 14:41:13.461086: Yayy! New best EMA pseudo Dice: 0.6954
2024-05-08 14:41:14.957796: 
2024-05-08 14:41:14.960000: Epoch 10
2024-05-08 14:41:14.961686: Current learning rate: 0.00991
2024-05-08 14:45:06.089961: Validation loss did not improve from -0.57028. Patience: 3/50
2024-05-08 14:45:06.091464: train_loss -0.7198
2024-05-08 14:45:06.093459: val_loss -0.5645
2024-05-08 14:45:06.095872: Pseudo dice [0.72]
2024-05-08 14:45:06.097124: Epoch time: 231.14 s
2024-05-08 14:45:06.098280: Yayy! New best EMA pseudo Dice: 0.6978
2024-05-08 14:45:07.603209: 
2024-05-08 14:45:07.606083: Epoch 11
2024-05-08 14:45:07.607756: Current learning rate: 0.0099
2024-05-08 14:48:58.980527: Validation loss did not improve from -0.57028. Patience: 4/50
2024-05-08 14:48:58.982839: train_loss -0.7309
2024-05-08 14:48:58.985347: val_loss -0.5566
2024-05-08 14:48:58.987038: Pseudo dice [0.6993]
2024-05-08 14:48:58.988277: Epoch time: 231.38 s
2024-05-08 14:48:58.989642: Yayy! New best EMA pseudo Dice: 0.698
2024-05-08 14:49:00.514374: 
2024-05-08 14:49:00.516366: Epoch 12
2024-05-08 14:49:00.517524: Current learning rate: 0.00989
2024-05-08 14:52:51.698501: Validation loss improved from -0.57028 to -0.59791! Patience: 4/50
2024-05-08 14:52:51.699950: train_loss -0.7449
2024-05-08 14:52:51.701109: val_loss -0.5979
2024-05-08 14:52:51.702069: Pseudo dice [0.7334]
2024-05-08 14:52:51.703135: Epoch time: 231.19 s
2024-05-08 14:52:51.704194: Yayy! New best EMA pseudo Dice: 0.7015
2024-05-08 14:52:53.259038: 
2024-05-08 14:52:53.261938: Epoch 13
2024-05-08 14:52:53.263467: Current learning rate: 0.00988
2024-05-08 14:56:44.543936: Validation loss did not improve from -0.59791. Patience: 1/50
2024-05-08 14:56:44.545779: train_loss -0.7332
2024-05-08 14:56:44.547589: val_loss -0.4731
2024-05-08 14:56:44.549217: Pseudo dice [0.6629]
2024-05-08 14:56:44.550241: Epoch time: 231.29 s
2024-05-08 14:56:45.755979: 
2024-05-08 14:56:45.758435: Epoch 14
2024-05-08 14:56:45.759945: Current learning rate: 0.00987
2024-05-08 15:00:36.617040: Validation loss did not improve from -0.59791. Patience: 2/50
2024-05-08 15:00:36.618551: train_loss -0.748
2024-05-08 15:00:36.619664: val_loss -0.5197
2024-05-08 15:00:36.620698: Pseudo dice [0.6803]
2024-05-08 15:00:36.621855: Epoch time: 230.86 s
2024-05-08 15:00:38.659433: 
2024-05-08 15:00:38.661360: Epoch 15
2024-05-08 15:00:38.662828: Current learning rate: 0.00986
2024-05-08 15:04:27.306887: Validation loss did not improve from -0.59791. Patience: 3/50
2024-05-08 15:04:27.308912: train_loss -0.7438
2024-05-08 15:04:27.310212: val_loss -0.5557
2024-05-08 15:04:27.311218: Pseudo dice [0.6985]
2024-05-08 15:04:27.312251: Epoch time: 228.65 s
2024-05-08 15:04:28.527485: 
2024-05-08 15:04:28.529771: Epoch 16
2024-05-08 15:04:28.530904: Current learning rate: 0.00986
2024-05-08 15:08:19.890037: Validation loss did not improve from -0.59791. Patience: 4/50
2024-05-08 15:08:19.908212: train_loss -0.749
2024-05-08 15:08:19.910007: val_loss -0.5944
2024-05-08 15:08:19.911137: Pseudo dice [0.7217]
2024-05-08 15:08:19.912298: Epoch time: 231.37 s
2024-05-08 15:08:22.286210: 
2024-05-08 15:08:22.288759: Epoch 17
2024-05-08 15:08:22.290288: Current learning rate: 0.00985
2024-05-08 15:12:15.248824: Validation loss did not improve from -0.59791. Patience: 5/50
2024-05-08 15:12:15.289584: train_loss -0.7555
2024-05-08 15:12:15.336155: val_loss -0.5864
2024-05-08 15:12:15.337625: Pseudo dice [0.7409]
2024-05-08 15:12:15.341075: Epoch time: 232.99 s
2024-05-08 15:12:15.342396: Yayy! New best EMA pseudo Dice: 0.703
2024-05-08 15:12:21.531562: 
2024-05-08 15:12:21.534263: Epoch 18
2024-05-08 15:12:21.535841: Current learning rate: 0.00984
2024-05-08 15:16:11.699016: Validation loss improved from -0.59791 to -0.61195! Patience: 5/50
2024-05-08 15:16:11.700637: train_loss -0.7672
2024-05-08 15:16:11.702174: val_loss -0.612
2024-05-08 15:16:11.703454: Pseudo dice [0.7498]
2024-05-08 15:16:11.704511: Epoch time: 230.17 s
2024-05-08 15:16:11.705568: Yayy! New best EMA pseudo Dice: 0.7076
2024-05-08 15:16:13.284347: 
2024-05-08 15:16:13.286315: Epoch 19
2024-05-08 15:16:13.287660: Current learning rate: 0.00983
2024-05-08 15:20:03.432895: Validation loss did not improve from -0.61195. Patience: 1/50
2024-05-08 15:20:03.434368: train_loss -0.7664
2024-05-08 15:20:03.435548: val_loss -0.5575
2024-05-08 15:20:03.436524: Pseudo dice [0.7201]
2024-05-08 15:20:03.437608: Epoch time: 230.15 s
2024-05-08 15:20:03.755716: Yayy! New best EMA pseudo Dice: 0.7089
2024-05-08 15:20:05.337927: 
2024-05-08 15:20:05.339887: Epoch 20
2024-05-08 15:20:05.341352: Current learning rate: 0.00982
2024-05-08 15:23:55.633202: Validation loss did not improve from -0.61195. Patience: 2/50
2024-05-08 15:23:55.634758: train_loss -0.7473
2024-05-08 15:23:55.636024: val_loss -0.578
2024-05-08 15:23:55.637169: Pseudo dice [0.7249]
2024-05-08 15:23:55.638207: Epoch time: 230.3 s
2024-05-08 15:23:55.639280: Yayy! New best EMA pseudo Dice: 0.7105
2024-05-08 15:23:57.203444: 
2024-05-08 15:23:57.206479: Epoch 21
2024-05-08 15:23:57.208190: Current learning rate: 0.00981
2024-05-08 15:27:47.588498: Validation loss did not improve from -0.61195. Patience: 3/50
2024-05-08 15:27:47.590125: train_loss -0.7537
2024-05-08 15:27:47.592191: val_loss -0.6043
2024-05-08 15:27:47.593360: Pseudo dice [0.738]
2024-05-08 15:27:47.594782: Epoch time: 230.39 s
2024-05-08 15:27:47.596141: Yayy! New best EMA pseudo Dice: 0.7132
2024-05-08 15:27:49.082693: 
2024-05-08 15:27:49.084656: Epoch 22
2024-05-08 15:27:49.086029: Current learning rate: 0.0098
2024-05-08 15:31:39.253029: Validation loss did not improve from -0.61195. Patience: 4/50
2024-05-08 15:31:39.254468: train_loss -0.7682
2024-05-08 15:31:39.255675: val_loss -0.6101
2024-05-08 15:31:39.256922: Pseudo dice [0.7451]
2024-05-08 15:31:39.258174: Epoch time: 230.17 s
2024-05-08 15:31:39.259227: Yayy! New best EMA pseudo Dice: 0.7164
2024-05-08 15:31:40.742261: 
2024-05-08 15:31:40.744937: Epoch 23
2024-05-08 15:31:40.746544: Current learning rate: 0.00979
2024-05-08 15:35:30.654668: Validation loss did not improve from -0.61195. Patience: 5/50
2024-05-08 15:35:30.656033: train_loss -0.7562
2024-05-08 15:35:30.658274: val_loss -0.4865
2024-05-08 15:35:30.659706: Pseudo dice [0.6542]
2024-05-08 15:35:30.660856: Epoch time: 229.91 s
2024-05-08 15:35:31.877303: 
2024-05-08 15:35:31.879431: Epoch 24
2024-05-08 15:35:31.880876: Current learning rate: 0.00978
2024-05-08 15:39:22.047686: Validation loss did not improve from -0.61195. Patience: 6/50
2024-05-08 15:39:22.048826: train_loss -0.7613
2024-05-08 15:39:22.050238: val_loss -0.5576
2024-05-08 15:39:22.051520: Pseudo dice [0.7183]
2024-05-08 15:39:22.052857: Epoch time: 230.17 s
2024-05-08 15:39:23.637277: 
2024-05-08 15:39:23.639529: Epoch 25
2024-05-08 15:39:23.640909: Current learning rate: 0.00977
2024-05-08 15:43:16.557457: Validation loss did not improve from -0.61195. Patience: 7/50
2024-05-08 15:43:16.558661: train_loss -0.7627
2024-05-08 15:43:16.559925: val_loss -0.579
2024-05-08 15:43:16.561211: Pseudo dice [0.7284]
2024-05-08 15:43:16.562382: Epoch time: 232.92 s
2024-05-08 15:43:17.835473: 
2024-05-08 15:43:17.836737: Epoch 26
2024-05-08 15:43:17.837950: Current learning rate: 0.00977
2024-05-08 15:47:07.858821: Validation loss did not improve from -0.61195. Patience: 8/50
2024-05-08 15:47:07.860007: train_loss -0.7722
2024-05-08 15:47:07.861218: val_loss -0.5923
2024-05-08 15:47:07.862365: Pseudo dice [0.7415]
2024-05-08 15:47:07.863537: Epoch time: 230.03 s
2024-05-08 15:47:09.160939: 
2024-05-08 15:47:09.162240: Epoch 27
2024-05-08 15:47:09.163699: Current learning rate: 0.00976
2024-05-08 15:50:59.282990: Validation loss improved from -0.61195 to -0.61197! Patience: 8/50
2024-05-08 15:50:59.284140: train_loss -0.7822
2024-05-08 15:50:59.285312: val_loss -0.612
2024-05-08 15:50:59.286252: Pseudo dice [0.7484]
2024-05-08 15:50:59.287375: Epoch time: 230.12 s
2024-05-08 15:50:59.288372: Yayy! New best EMA pseudo Dice: 0.7189
2024-05-08 15:51:01.007718: 
2024-05-08 15:51:01.009657: Epoch 28
2024-05-08 15:51:01.010910: Current learning rate: 0.00975
2024-05-08 15:54:56.499417: Validation loss did not improve from -0.61197. Patience: 1/50
2024-05-08 15:54:56.500802: train_loss -0.7834
2024-05-08 15:54:56.501913: val_loss -0.5937
2024-05-08 15:54:56.502937: Pseudo dice [0.7366]
2024-05-08 15:54:56.503954: Epoch time: 235.49 s
2024-05-08 15:54:56.504957: Yayy! New best EMA pseudo Dice: 0.7207
2024-05-08 15:54:58.229062: 
2024-05-08 15:54:58.230248: Epoch 29
2024-05-08 15:54:58.231786: Current learning rate: 0.00974
2024-05-08 15:58:52.366342: Validation loss did not improve from -0.61197. Patience: 2/50
2024-05-08 15:58:52.367542: train_loss -0.7931
2024-05-08 15:58:52.368667: val_loss -0.6003
2024-05-08 15:58:52.369615: Pseudo dice [0.7465]
2024-05-08 15:58:52.370577: Epoch time: 234.14 s
2024-05-08 15:58:52.794738: Yayy! New best EMA pseudo Dice: 0.7232
2024-05-08 15:58:55.020860: 
2024-05-08 15:58:55.022347: Epoch 30
2024-05-08 15:58:55.023539: Current learning rate: 0.00973
2024-05-08 16:02:46.630619: Validation loss did not improve from -0.61197. Patience: 3/50
2024-05-08 16:02:46.631903: train_loss -0.785
2024-05-08 16:02:46.633119: val_loss -0.5833
2024-05-08 16:02:46.634103: Pseudo dice [0.7351]
2024-05-08 16:02:46.635075: Epoch time: 231.61 s
2024-05-08 16:02:46.635928: Yayy! New best EMA pseudo Dice: 0.7244
2024-05-08 16:02:48.396994: 
2024-05-08 16:02:48.398504: Epoch 31
2024-05-08 16:02:48.399493: Current learning rate: 0.00972
2024-05-08 16:06:50.811855: Validation loss did not improve from -0.61197. Patience: 4/50
2024-05-08 16:06:50.812972: train_loss -0.7873
2024-05-08 16:06:50.814113: val_loss -0.554
2024-05-08 16:06:50.815160: Pseudo dice [0.7268]
2024-05-08 16:06:50.816229: Epoch time: 242.42 s
2024-05-08 16:06:50.817163: Yayy! New best EMA pseudo Dice: 0.7247
2024-05-08 16:06:52.792434: 
2024-05-08 16:06:52.794115: Epoch 32
2024-05-08 16:06:52.795354: Current learning rate: 0.00971
2024-05-08 16:10:52.404176: Validation loss did not improve from -0.61197. Patience: 5/50
2024-05-08 16:10:52.455781: train_loss -0.7939
2024-05-08 16:10:52.467649: val_loss -0.5456
2024-05-08 16:10:52.468687: Pseudo dice [0.7158]
2024-05-08 16:10:52.470414: Epoch time: 239.64 s
2024-05-08 16:10:54.649827: 
2024-05-08 16:10:54.651043: Epoch 33
2024-05-08 16:10:54.652150: Current learning rate: 0.0097
2024-05-08 16:15:01.919346: Validation loss improved from -0.61197 to -0.61721! Patience: 5/50
2024-05-08 16:15:01.920509: train_loss -0.7934
2024-05-08 16:15:01.921488: val_loss -0.6172
2024-05-08 16:15:01.922479: Pseudo dice [0.7586]
2024-05-08 16:15:01.923418: Epoch time: 247.27 s
2024-05-08 16:15:01.924307: Yayy! New best EMA pseudo Dice: 0.7273
2024-05-08 16:15:04.994682: 
2024-05-08 16:15:04.995865: Epoch 34
2024-05-08 16:15:04.997068: Current learning rate: 0.00969
2024-05-08 16:19:08.828755: Validation loss did not improve from -0.61721. Patience: 1/50
2024-05-08 16:19:08.831243: train_loss -0.7848
2024-05-08 16:19:08.832640: val_loss -0.6035
2024-05-08 16:19:08.833576: Pseudo dice [0.7419]
2024-05-08 16:19:08.834693: Epoch time: 243.84 s
2024-05-08 16:19:09.614657: Yayy! New best EMA pseudo Dice: 0.7287
2024-05-08 16:19:11.435856: 
2024-05-08 16:19:11.437176: Epoch 35
2024-05-08 16:19:11.438248: Current learning rate: 0.00968
2024-05-08 16:23:13.092374: Validation loss did not improve from -0.61721. Patience: 2/50
2024-05-08 16:23:13.093659: train_loss -0.7965
2024-05-08 16:23:13.094714: val_loss -0.5934
2024-05-08 16:23:13.095733: Pseudo dice [0.7397]
2024-05-08 16:23:13.096717: Epoch time: 241.66 s
2024-05-08 16:23:13.097621: Yayy! New best EMA pseudo Dice: 0.7298
2024-05-08 16:23:14.908016: 
2024-05-08 16:23:14.909281: Epoch 36
2024-05-08 16:23:14.910188: Current learning rate: 0.00968
2024-05-08 16:27:24.234306: Validation loss improved from -0.61721 to -0.62761! Patience: 2/50
2024-05-08 16:27:24.235541: train_loss -0.8018
2024-05-08 16:27:24.236848: val_loss -0.6276
2024-05-08 16:27:24.237977: Pseudo dice [0.7643]
2024-05-08 16:27:24.239073: Epoch time: 249.33 s
2024-05-08 16:27:24.239944: Yayy! New best EMA pseudo Dice: 0.7333
2024-05-08 16:27:26.107944: 
2024-05-08 16:27:26.109777: Epoch 37
2024-05-08 16:27:26.111012: Current learning rate: 0.00967
2024-05-08 16:31:27.344800: Validation loss did not improve from -0.62761. Patience: 1/50
2024-05-08 16:31:27.346011: train_loss -0.7728
2024-05-08 16:31:27.347164: val_loss -0.5746
2024-05-08 16:31:27.348121: Pseudo dice [0.7376]
2024-05-08 16:31:27.349071: Epoch time: 241.24 s
2024-05-08 16:31:27.350021: Yayy! New best EMA pseudo Dice: 0.7337
2024-05-08 16:31:29.180673: 
2024-05-08 16:31:29.182264: Epoch 38
2024-05-08 16:31:29.183635: Current learning rate: 0.00966
2024-05-08 16:35:29.096624: Validation loss did not improve from -0.62761. Patience: 2/50
2024-05-08 16:35:29.098094: train_loss -0.7574
2024-05-08 16:35:29.099354: val_loss -0.5917
2024-05-08 16:35:29.100585: Pseudo dice [0.7386]
2024-05-08 16:35:29.101889: Epoch time: 239.92 s
2024-05-08 16:35:29.103071: Yayy! New best EMA pseudo Dice: 0.7342
2024-05-08 16:35:30.960575: 
2024-05-08 16:35:30.962399: Epoch 39
2024-05-08 16:35:30.963678: Current learning rate: 0.00965
2024-05-08 16:39:31.872342: Validation loss did not improve from -0.62761. Patience: 3/50
2024-05-08 16:39:31.873490: train_loss -0.782
2024-05-08 16:39:31.874564: val_loss -0.5811
2024-05-08 16:39:31.875614: Pseudo dice [0.7319]
2024-05-08 16:39:31.876811: Epoch time: 240.91 s
2024-05-08 16:39:33.707972: 
2024-05-08 16:39:33.709574: Epoch 40
2024-05-08 16:39:33.710729: Current learning rate: 0.00964
2024-05-08 16:43:40.576673: Validation loss did not improve from -0.62761. Patience: 4/50
2024-05-08 16:43:40.577822: train_loss -0.7842
2024-05-08 16:43:40.579039: val_loss -0.6171
2024-05-08 16:43:40.579970: Pseudo dice [0.7538]
2024-05-08 16:43:40.580899: Epoch time: 246.87 s
2024-05-08 16:43:40.581834: Yayy! New best EMA pseudo Dice: 0.736
2024-05-08 16:43:45.523659: 
2024-05-08 16:43:45.524880: Epoch 41
2024-05-08 16:43:45.525786: Current learning rate: 0.00963
2024-05-08 16:47:53.223333: Validation loss did not improve from -0.62761. Patience: 5/50
2024-05-08 16:47:53.224485: train_loss -0.7976
2024-05-08 16:47:53.225586: val_loss -0.5854
2024-05-08 16:47:53.229580: Pseudo dice [0.73]
2024-05-08 16:47:53.230767: Epoch time: 247.7 s
2024-05-08 16:47:54.580729: 
2024-05-08 16:47:54.582147: Epoch 42
2024-05-08 16:47:54.583243: Current learning rate: 0.00962
2024-05-08 16:52:01.939959: Validation loss did not improve from -0.62761. Patience: 6/50
2024-05-08 16:52:01.941424: train_loss -0.8018
2024-05-08 16:52:01.942614: val_loss -0.6166
2024-05-08 16:52:01.943549: Pseudo dice [0.7486]
2024-05-08 16:52:01.944478: Epoch time: 247.36 s
2024-05-08 16:52:01.945447: Yayy! New best EMA pseudo Dice: 0.7367
2024-05-08 16:52:03.744419: 
2024-05-08 16:52:03.746022: Epoch 43
2024-05-08 16:52:03.747329: Current learning rate: 0.00961
2024-05-08 16:56:09.442022: Validation loss did not improve from -0.62761. Patience: 7/50
2024-05-08 16:56:09.443286: train_loss -0.8029
2024-05-08 16:56:09.444329: val_loss -0.6209
2024-05-08 16:56:09.445225: Pseudo dice [0.751]
2024-05-08 16:56:09.446120: Epoch time: 245.7 s
2024-05-08 16:56:09.447051: Yayy! New best EMA pseudo Dice: 0.7381
2024-05-08 16:56:11.204954: 
2024-05-08 16:56:11.206824: Epoch 44
2024-05-08 16:56:11.208236: Current learning rate: 0.0096
2024-05-08 17:00:16.025067: Validation loss did not improve from -0.62761. Patience: 8/50
2024-05-08 17:00:16.026121: train_loss -0.7963
2024-05-08 17:00:16.027131: val_loss -0.6056
2024-05-08 17:00:16.028061: Pseudo dice [0.7293]
2024-05-08 17:00:16.029030: Epoch time: 244.82 s
2024-05-08 17:00:17.783478: 
2024-05-08 17:00:17.784770: Epoch 45
2024-05-08 17:00:17.785986: Current learning rate: 0.00959
2024-05-08 17:04:29.641182: Validation loss did not improve from -0.62761. Patience: 9/50
2024-05-08 17:04:29.642272: train_loss -0.7875
2024-05-08 17:04:29.643492: val_loss -0.611
2024-05-08 17:04:29.644560: Pseudo dice [0.7443]
2024-05-08 17:04:29.645687: Epoch time: 251.86 s
2024-05-08 17:04:31.002586: 
2024-05-08 17:04:31.003912: Epoch 46
2024-05-08 17:04:31.004979: Current learning rate: 0.00959
2024-05-08 17:08:40.280173: Validation loss did not improve from -0.62761. Patience: 10/50
2024-05-08 17:08:40.281276: train_loss -0.8024
2024-05-08 17:08:40.282325: val_loss -0.6131
2024-05-08 17:08:40.283289: Pseudo dice [0.7366]
2024-05-08 17:08:40.284300: Epoch time: 249.28 s
2024-05-08 17:08:41.664326: 
2024-05-08 17:08:41.665695: Epoch 47
2024-05-08 17:08:41.666888: Current learning rate: 0.00958
2024-05-08 17:12:53.798193: Validation loss did not improve from -0.62761. Patience: 11/50
2024-05-08 17:12:53.799315: train_loss -0.8131
2024-05-08 17:12:53.800334: val_loss -0.6205
2024-05-08 17:12:53.801265: Pseudo dice [0.7452]
2024-05-08 17:12:53.802160: Epoch time: 252.14 s
2024-05-08 17:12:53.803000: Yayy! New best EMA pseudo Dice: 0.7386
2024-05-08 17:12:55.540220: 
2024-05-08 17:12:55.541736: Epoch 48
2024-05-08 17:12:55.542733: Current learning rate: 0.00957
2024-05-08 17:17:09.968940: Validation loss did not improve from -0.62761. Patience: 12/50
2024-05-08 17:17:09.971111: train_loss -0.8092
2024-05-08 17:17:09.972778: val_loss -0.623
2024-05-08 17:17:09.973895: Pseudo dice [0.7591]
2024-05-08 17:17:09.975084: Epoch time: 254.43 s
2024-05-08 17:17:09.976025: Yayy! New best EMA pseudo Dice: 0.7406
2024-05-08 17:17:11.908220: 
2024-05-08 17:17:11.910159: Epoch 49
2024-05-08 17:17:11.911464: Current learning rate: 0.00956
2024-05-08 17:21:17.868940: Validation loss did not improve from -0.62761. Patience: 13/50
2024-05-08 17:21:17.883862: train_loss -0.8094
2024-05-08 17:21:17.884903: val_loss -0.5679
2024-05-08 17:21:17.885871: Pseudo dice [0.7115]
2024-05-08 17:21:17.886992: Epoch time: 245.98 s
2024-05-08 17:21:19.823985: 
2024-05-08 17:21:19.825489: Epoch 50
2024-05-08 17:21:19.826730: Current learning rate: 0.00955
2024-05-08 17:25:25.635936: Validation loss did not improve from -0.62761. Patience: 14/50
2024-05-08 17:25:25.637142: train_loss -0.8075
2024-05-08 17:25:25.638352: val_loss -0.6041
2024-05-08 17:25:25.639590: Pseudo dice [0.7361]
2024-05-08 17:25:25.640601: Epoch time: 245.81 s
2024-05-08 17:25:27.013594: 
2024-05-08 17:25:27.014801: Epoch 51
2024-05-08 17:25:27.016268: Current learning rate: 0.00954
2024-05-08 17:29:36.553541: Validation loss did not improve from -0.62761. Patience: 15/50
2024-05-08 17:29:36.554700: train_loss -0.8108
2024-05-08 17:29:36.555732: val_loss -0.5549
2024-05-08 17:29:36.556652: Pseudo dice [0.7256]
2024-05-08 17:29:36.557605: Epoch time: 249.54 s
2024-05-08 17:29:39.629688: 
2024-05-08 17:29:39.630980: Epoch 52
2024-05-08 17:29:39.632138: Current learning rate: 0.00953
2024-05-08 17:33:54.136786: Validation loss did not improve from -0.62761. Patience: 16/50
2024-05-08 17:33:54.137994: train_loss -0.8147
2024-05-08 17:33:54.139152: val_loss -0.6191
2024-05-08 17:33:54.140195: Pseudo dice [0.7549]
2024-05-08 17:33:54.141361: Epoch time: 254.51 s
2024-05-08 17:33:55.498587: 
2024-05-08 17:33:55.500273: Epoch 53
2024-05-08 17:33:55.501476: Current learning rate: 0.00952
2024-05-08 17:38:10.551831: Validation loss did not improve from -0.62761. Patience: 17/50
2024-05-08 17:38:10.553010: train_loss -0.8154
2024-05-08 17:38:10.554051: val_loss -0.5784
2024-05-08 17:38:10.554977: Pseudo dice [0.7403]
2024-05-08 17:38:10.556046: Epoch time: 255.06 s
2024-05-08 17:38:11.920835: 
2024-05-08 17:38:11.922280: Epoch 54
2024-05-08 17:38:11.923537: Current learning rate: 0.00951
2024-05-08 17:42:24.381508: Validation loss did not improve from -0.62761. Patience: 18/50
2024-05-08 17:42:24.382713: train_loss -0.8134
2024-05-08 17:42:24.383701: val_loss -0.6178
2024-05-08 17:42:24.384765: Pseudo dice [0.7392]
2024-05-08 17:42:24.385830: Epoch time: 252.46 s
2024-05-08 17:42:26.159206: 
2024-05-08 17:42:26.160450: Epoch 55
2024-05-08 17:42:26.161520: Current learning rate: 0.0095
2024-05-08 17:46:43.038498: Validation loss did not improve from -0.62761. Patience: 19/50
2024-05-08 17:46:43.040102: train_loss -0.8076
2024-05-08 17:46:43.041306: val_loss -0.5677
2024-05-08 17:46:43.042345: Pseudo dice [0.7303]
2024-05-08 17:46:43.043434: Epoch time: 256.88 s
2024-05-08 17:46:45.199706: 
2024-05-08 17:46:45.201186: Epoch 56
2024-05-08 17:46:45.202291: Current learning rate: 0.00949
2024-05-08 17:50:50.839132: Validation loss improved from -0.62761 to -0.62781! Patience: 19/50
2024-05-08 17:50:50.840466: train_loss -0.8066
2024-05-08 17:50:50.841748: val_loss -0.6278
2024-05-08 17:50:50.842819: Pseudo dice [0.7473]
2024-05-08 17:50:50.843950: Epoch time: 245.64 s
2024-05-08 17:50:52.220363: 
2024-05-08 17:50:52.221648: Epoch 57
2024-05-08 17:50:52.222843: Current learning rate: 0.00949
2024-05-08 17:55:01.670134: Validation loss did not improve from -0.62781. Patience: 1/50
2024-05-08 17:55:01.671355: train_loss -0.8121
2024-05-08 17:55:01.672349: val_loss -0.5833
2024-05-08 17:55:01.673280: Pseudo dice [0.7335]
2024-05-08 17:55:01.674193: Epoch time: 249.45 s
2024-05-08 17:55:03.036487: 
2024-05-08 17:55:03.037839: Epoch 58
2024-05-08 17:55:03.038834: Current learning rate: 0.00948
2024-05-08 17:59:11.018705: Validation loss did not improve from -0.62781. Patience: 2/50
2024-05-08 17:59:11.019897: train_loss -0.8132
2024-05-08 17:59:11.021101: val_loss -0.6053
2024-05-08 17:59:11.022235: Pseudo dice [0.7536]
2024-05-08 17:59:11.023198: Epoch time: 247.98 s
2024-05-08 17:59:12.406411: 
2024-05-08 17:59:12.408591: Epoch 59
2024-05-08 17:59:12.409966: Current learning rate: 0.00947
2024-05-08 18:03:21.402231: Validation loss did not improve from -0.62781. Patience: 3/50
2024-05-08 18:03:21.403744: train_loss -0.8194
2024-05-08 18:03:21.404937: val_loss -0.5857
2024-05-08 18:03:21.405938: Pseudo dice [0.7532]
2024-05-08 18:03:21.406909: Epoch time: 249.0 s
2024-05-08 18:03:21.818483: Yayy! New best EMA pseudo Dice: 0.741
2024-05-08 18:03:23.539715: 
2024-05-08 18:03:23.540946: Epoch 60
2024-05-08 18:03:23.541881: Current learning rate: 0.00946
2024-05-08 18:07:37.279200: Validation loss did not improve from -0.62781. Patience: 4/50
2024-05-08 18:07:37.280407: train_loss -0.8229
2024-05-08 18:07:37.281665: val_loss -0.5697
2024-05-08 18:07:37.282676: Pseudo dice [0.727]
2024-05-08 18:07:37.284426: Epoch time: 253.74 s
2024-05-08 18:07:38.682072: 
2024-05-08 18:07:38.683633: Epoch 61
2024-05-08 18:07:38.684823: Current learning rate: 0.00945
2024-05-08 18:11:53.596767: Validation loss did not improve from -0.62781. Patience: 5/50
2024-05-08 18:11:53.597929: train_loss -0.8239
2024-05-08 18:11:53.599157: val_loss -0.6126
2024-05-08 18:11:53.600316: Pseudo dice [0.7528]
2024-05-08 18:11:53.601548: Epoch time: 254.92 s
2024-05-08 18:11:54.977051: 
2024-05-08 18:11:54.978619: Epoch 62
2024-05-08 18:11:54.980081: Current learning rate: 0.00944
2024-05-08 18:16:08.700663: Validation loss did not improve from -0.62781. Patience: 6/50
2024-05-08 18:16:08.701990: train_loss -0.8172
2024-05-08 18:16:08.703123: val_loss -0.6123
2024-05-08 18:16:08.704098: Pseudo dice [0.7437]
2024-05-08 18:16:08.705145: Epoch time: 253.73 s
2024-05-08 18:16:08.706087: Yayy! New best EMA pseudo Dice: 0.7412
2024-05-08 18:16:10.501495: 
2024-05-08 18:16:10.503041: Epoch 63
2024-05-08 18:16:10.504131: Current learning rate: 0.00943
2024-05-08 18:20:24.436828: Validation loss did not improve from -0.62781. Patience: 7/50
2024-05-08 18:20:24.466976: train_loss -0.8159
2024-05-08 18:20:24.468117: val_loss -0.5865
2024-05-08 18:20:24.469124: Pseudo dice [0.7431]
2024-05-08 18:20:24.470251: Epoch time: 253.97 s
2024-05-08 18:20:24.471442: Yayy! New best EMA pseudo Dice: 0.7414
2024-05-08 18:20:27.311268: 
2024-05-08 18:20:27.312408: Epoch 64
2024-05-08 18:20:27.313603: Current learning rate: 0.00942
2024-05-08 18:24:35.287406: Validation loss did not improve from -0.62781. Patience: 8/50
2024-05-08 18:24:35.289456: train_loss -0.8192
2024-05-08 18:24:35.291747: val_loss -0.5856
2024-05-08 18:24:35.292803: Pseudo dice [0.7328]
2024-05-08 18:24:35.294094: Epoch time: 247.98 s
2024-05-08 18:24:37.092589: 
2024-05-08 18:24:37.093944: Epoch 65
2024-05-08 18:24:37.095141: Current learning rate: 0.00941
2024-05-08 18:28:49.991199: Validation loss did not improve from -0.62781. Patience: 9/50
2024-05-08 18:28:49.993369: train_loss -0.8274
2024-05-08 18:28:49.994886: val_loss -0.6199
2024-05-08 18:28:49.996068: Pseudo dice [0.7489]
2024-05-08 18:28:49.997293: Epoch time: 252.9 s
2024-05-08 18:28:51.447646: 
2024-05-08 18:28:51.450667: Epoch 66
2024-05-08 18:28:51.451979: Current learning rate: 0.0094
2024-05-08 18:33:00.029790: Validation loss did not improve from -0.62781. Patience: 10/50
2024-05-08 18:33:00.031163: train_loss -0.8229
2024-05-08 18:33:00.032202: val_loss -0.6248
2024-05-08 18:33:00.033279: Pseudo dice [0.7558]
2024-05-08 18:33:00.034331: Epoch time: 248.59 s
2024-05-08 18:33:00.035414: Yayy! New best EMA pseudo Dice: 0.7428
2024-05-08 18:33:01.773797: 
2024-05-08 18:33:01.775600: Epoch 67
2024-05-08 18:33:01.776787: Current learning rate: 0.00939
2024-05-08 18:37:08.251293: Validation loss did not improve from -0.62781. Patience: 11/50
2024-05-08 18:37:08.252457: train_loss -0.8236
2024-05-08 18:37:08.253580: val_loss -0.5669
2024-05-08 18:37:08.254617: Pseudo dice [0.7192]
2024-05-08 18:37:08.255633: Epoch time: 246.48 s
2024-05-08 18:37:09.679926: 
2024-05-08 18:37:09.681505: Epoch 68
2024-05-08 18:37:09.682817: Current learning rate: 0.00939
2024-05-08 18:41:25.275378: Validation loss did not improve from -0.62781. Patience: 12/50
2024-05-08 18:41:25.276633: train_loss -0.8259
2024-05-08 18:41:25.278012: val_loss -0.6111
2024-05-08 18:41:25.279127: Pseudo dice [0.7445]
2024-05-08 18:41:25.280087: Epoch time: 255.6 s
2024-05-08 18:41:26.717625: 
2024-05-08 18:41:26.718897: Epoch 69
2024-05-08 18:41:26.720049: Current learning rate: 0.00938
2024-05-08 18:45:38.950303: Validation loss did not improve from -0.62781. Patience: 13/50
2024-05-08 18:45:38.951765: train_loss -0.8259
2024-05-08 18:45:38.953016: val_loss -0.5828
2024-05-08 18:45:38.954086: Pseudo dice [0.7401]
2024-05-08 18:45:38.955181: Epoch time: 252.24 s
2024-05-08 18:45:40.740700: 
2024-05-08 18:45:40.742167: Epoch 70
2024-05-08 18:45:40.743408: Current learning rate: 0.00937
2024-05-08 18:49:57.779848: Validation loss did not improve from -0.62781. Patience: 14/50
2024-05-08 18:49:57.781150: train_loss -0.8263
2024-05-08 18:49:57.782354: val_loss -0.6006
2024-05-08 18:49:57.783285: Pseudo dice [0.7431]
2024-05-08 18:49:57.784450: Epoch time: 257.04 s
2024-05-08 18:49:59.212821: 
2024-05-08 18:49:59.214509: Epoch 71
2024-05-08 18:49:59.215474: Current learning rate: 0.00936
2024-05-08 18:54:13.487826: Validation loss did not improve from -0.62781. Patience: 15/50
2024-05-08 18:54:13.489165: train_loss -0.8281
2024-05-08 18:54:13.490316: val_loss -0.6267
2024-05-08 18:54:13.491256: Pseudo dice [0.7556]
2024-05-08 18:54:13.492193: Epoch time: 254.28 s
2024-05-08 18:54:14.926690: 
2024-05-08 18:54:14.927960: Epoch 72
2024-05-08 18:54:14.928966: Current learning rate: 0.00935
2024-05-08 18:58:27.976982: Validation loss improved from -0.62781 to -0.62795! Patience: 15/50
2024-05-08 18:58:27.978344: train_loss -0.8308
2024-05-08 18:58:27.979429: val_loss -0.628
2024-05-08 18:58:27.980501: Pseudo dice [0.759]
2024-05-08 18:58:27.981620: Epoch time: 253.05 s
2024-05-08 18:58:27.982575: Yayy! New best EMA pseudo Dice: 0.7441
2024-05-08 18:58:29.796962: 
2024-05-08 18:58:29.798352: Epoch 73
2024-05-08 18:58:29.799383: Current learning rate: 0.00934
2024-05-08 19:02:42.274954: Validation loss did not improve from -0.62795. Patience: 1/50
2024-05-08 19:02:42.276011: train_loss -0.8261
2024-05-08 19:02:42.277081: val_loss -0.5721
2024-05-08 19:02:42.278013: Pseudo dice [0.7191]
2024-05-08 19:02:42.279039: Epoch time: 252.48 s
2024-05-08 19:02:45.339626: 
2024-05-08 19:02:45.340832: Epoch 74
2024-05-08 19:02:45.341810: Current learning rate: 0.00933
2024-05-08 19:07:02.065196: Validation loss did not improve from -0.62795. Patience: 2/50
2024-05-08 19:07:02.066218: train_loss -0.8308
2024-05-08 19:07:02.067373: val_loss -0.5554
2024-05-08 19:07:02.068578: Pseudo dice [0.7294]
2024-05-08 19:07:02.069558: Epoch time: 256.73 s
2024-05-08 19:07:03.872038: 
2024-05-08 19:07:03.873352: Epoch 75
2024-05-08 19:07:03.874355: Current learning rate: 0.00932
2024-05-08 19:11:11.385806: Validation loss did not improve from -0.62795. Patience: 3/50
2024-05-08 19:11:11.387264: train_loss -0.8294
2024-05-08 19:11:11.388370: val_loss -0.6167
2024-05-08 19:11:11.389318: Pseudo dice [0.7502]
2024-05-08 19:11:11.390383: Epoch time: 247.52 s
2024-05-08 19:11:12.858500: 
2024-05-08 19:11:12.859883: Epoch 76
2024-05-08 19:11:12.860941: Current learning rate: 0.00931
2024-05-08 19:15:21.927338: Validation loss improved from -0.62795 to -0.63869! Patience: 3/50
2024-05-08 19:15:21.928619: train_loss -0.8218
2024-05-08 19:15:21.929861: val_loss -0.6387
2024-05-08 19:15:21.930923: Pseudo dice [0.7706]
2024-05-08 19:15:21.932062: Epoch time: 249.07 s
2024-05-08 19:15:21.933253: Yayy! New best EMA pseudo Dice: 0.7443
2024-05-08 19:15:23.745847: 
2024-05-08 19:15:23.747280: Epoch 77
2024-05-08 19:15:23.748283: Current learning rate: 0.0093
2024-05-08 19:19:19.539975: Validation loss did not improve from -0.63869. Patience: 1/50
2024-05-08 19:19:19.541030: train_loss -0.8265
2024-05-08 19:19:19.542139: val_loss -0.5544
2024-05-08 19:19:19.543186: Pseudo dice [0.7094]
2024-05-08 19:19:19.544241: Epoch time: 235.8 s
2024-05-08 19:19:20.981715: 
2024-05-08 19:19:20.983438: Epoch 78
2024-05-08 19:19:20.984394: Current learning rate: 0.0093
2024-05-08 19:23:31.893006: Validation loss did not improve from -0.63869. Patience: 2/50
2024-05-08 19:23:31.894248: train_loss -0.8204
2024-05-08 19:23:31.895263: val_loss -0.6196
2024-05-08 19:23:31.896152: Pseudo dice [0.7504]
2024-05-08 19:23:31.897125: Epoch time: 250.91 s
2024-05-08 19:23:33.350153: 
2024-05-08 19:23:33.351624: Epoch 79
2024-05-08 19:23:33.353068: Current learning rate: 0.00929
2024-05-08 19:27:43.482859: Validation loss did not improve from -0.63869. Patience: 3/50
2024-05-08 19:27:43.629521: train_loss -0.8284
2024-05-08 19:27:43.631516: val_loss -0.5607
2024-05-08 19:27:43.632728: Pseudo dice [0.7232]
2024-05-08 19:27:43.678809: Epoch time: 250.17 s
2024-05-08 19:27:46.833640: 
2024-05-08 19:27:46.836617: Epoch 80
2024-05-08 19:27:46.837985: Current learning rate: 0.00928
2024-05-08 19:31:46.695074: Validation loss did not improve from -0.63869. Patience: 4/50
2024-05-08 19:31:46.802281: train_loss -0.8256
2024-05-08 19:31:46.804384: val_loss -0.6273
2024-05-08 19:31:46.805534: Pseudo dice [0.755]
2024-05-08 19:31:46.806810: Epoch time: 239.94 s
2024-05-08 19:31:49.433393: 
2024-05-08 19:31:49.435199: Epoch 81
2024-05-08 19:31:49.436423: Current learning rate: 0.00927
2024-05-08 19:35:53.071784: Validation loss did not improve from -0.63869. Patience: 5/50
2024-05-08 19:35:53.073178: train_loss -0.8306
2024-05-08 19:35:53.074440: val_loss -0.599
2024-05-08 19:35:53.075933: Pseudo dice [0.7388]
2024-05-08 19:35:53.077168: Epoch time: 243.64 s
2024-05-08 19:35:55.418136: 
2024-05-08 19:35:55.420763: Epoch 82
2024-05-08 19:35:55.422398: Current learning rate: 0.00926
2024-05-08 19:40:13.377921: Validation loss did not improve from -0.63869. Patience: 6/50
2024-05-08 19:40:13.379107: train_loss -0.8342
2024-05-08 19:40:13.380492: val_loss -0.5928
2024-05-08 19:40:13.381657: Pseudo dice [0.7345]
2024-05-08 19:40:13.382842: Epoch time: 257.96 s
2024-05-08 19:40:14.880522: 
2024-05-08 19:40:14.882212: Epoch 83
2024-05-08 19:40:14.883294: Current learning rate: 0.00925
2024-05-08 19:44:27.726551: Validation loss did not improve from -0.63869. Patience: 7/50
2024-05-08 19:44:27.727828: train_loss -0.8393
2024-05-08 19:44:27.728971: val_loss -0.5834
2024-05-08 19:44:27.729939: Pseudo dice [0.739]
2024-05-08 19:44:27.730881: Epoch time: 252.85 s
2024-05-08 19:44:29.072705: 
2024-05-08 19:44:29.074136: Epoch 84
2024-05-08 19:44:29.075056: Current learning rate: 0.00924
2024-05-08 19:48:30.460434: Validation loss did not improve from -0.63869. Patience: 8/50
2024-05-08 19:48:30.461606: train_loss -0.8295
2024-05-08 19:48:30.462969: val_loss -0.5894
2024-05-08 19:48:30.464154: Pseudo dice [0.7465]
2024-05-08 19:48:30.465608: Epoch time: 241.39 s
2024-05-08 19:48:32.346153: 
2024-05-08 19:48:32.349083: Epoch 85
2024-05-08 19:48:32.350490: Current learning rate: 0.00923
2024-05-08 19:52:36.774311: Validation loss did not improve from -0.63869. Patience: 9/50
2024-05-08 19:52:36.775711: train_loss -0.8262
2024-05-08 19:52:36.777148: val_loss -0.5934
2024-05-08 19:52:36.778515: Pseudo dice [0.7393]
2024-05-08 19:52:36.779987: Epoch time: 244.43 s
2024-05-08 19:52:39.017143: 
2024-05-08 19:52:39.018725: Epoch 86
2024-05-08 19:52:39.019897: Current learning rate: 0.00922
2024-05-08 19:56:33.810538: Validation loss did not improve from -0.63869. Patience: 10/50
2024-05-08 19:56:33.811933: train_loss -0.8215
2024-05-08 19:56:33.813132: val_loss -0.6226
2024-05-08 19:56:33.814166: Pseudo dice [0.7616]
2024-05-08 19:56:33.815063: Epoch time: 234.8 s
2024-05-08 19:56:35.160342: 
2024-05-08 19:56:35.161695: Epoch 87
2024-05-08 19:56:35.162969: Current learning rate: 0.00921
2024-05-08 20:00:30.999630: Validation loss did not improve from -0.63869. Patience: 11/50
2024-05-08 20:00:31.000932: train_loss -0.8165
2024-05-08 20:00:31.002048: val_loss -0.5955
2024-05-08 20:00:31.003088: Pseudo dice [0.7447]
2024-05-08 20:00:31.004145: Epoch time: 235.84 s
2024-05-08 20:00:32.325553: 
2024-05-08 20:00:32.327263: Epoch 88
2024-05-08 20:00:32.328376: Current learning rate: 0.0092
2024-05-08 20:04:31.683208: Validation loss did not improve from -0.63869. Patience: 12/50
2024-05-08 20:04:31.684486: train_loss -0.8202
2024-05-08 20:04:31.686347: val_loss -0.5932
2024-05-08 20:04:31.687402: Pseudo dice [0.7326]
2024-05-08 20:04:31.688442: Epoch time: 239.36 s
2024-05-08 20:04:33.037215: 
2024-05-08 20:04:33.038803: Epoch 89
2024-05-08 20:04:33.039825: Current learning rate: 0.0092
2024-05-08 20:08:33.492203: Validation loss did not improve from -0.63869. Patience: 13/50
2024-05-08 20:08:33.494721: train_loss -0.8274
2024-05-08 20:08:33.495822: val_loss -0.5411
2024-05-08 20:08:33.496811: Pseudo dice [0.7049]
2024-05-08 20:08:33.497750: Epoch time: 240.46 s
2024-05-08 20:08:35.119786: 
2024-05-08 20:08:35.122356: Epoch 90
2024-05-08 20:08:35.124233: Current learning rate: 0.00919
2024-05-08 20:12:42.166128: Validation loss did not improve from -0.63869. Patience: 14/50
2024-05-08 20:12:42.167428: train_loss -0.8313
2024-05-08 20:12:42.168751: val_loss -0.6208
2024-05-08 20:12:42.169720: Pseudo dice [0.7505]
2024-05-08 20:12:42.170811: Epoch time: 247.05 s
2024-05-08 20:12:43.529683: 
2024-05-08 20:12:43.531009: Epoch 91
2024-05-08 20:12:43.531999: Current learning rate: 0.00918
2024-05-08 20:16:39.235936: Validation loss did not improve from -0.63869. Patience: 15/50
2024-05-08 20:16:39.237277: train_loss -0.8364
2024-05-08 20:16:39.238472: val_loss -0.6153
2024-05-08 20:16:39.239430: Pseudo dice [0.7488]
2024-05-08 20:16:39.240462: Epoch time: 235.71 s
2024-05-08 20:16:40.535560: 
2024-05-08 20:16:40.537574: Epoch 92
2024-05-08 20:16:40.539131: Current learning rate: 0.00917
2024-05-08 20:20:54.740742: Validation loss did not improve from -0.63869. Patience: 16/50
2024-05-08 20:20:54.742187: train_loss -0.8317
2024-05-08 20:20:54.743580: val_loss -0.5889
2024-05-08 20:20:54.744837: Pseudo dice [0.7406]
2024-05-08 20:20:54.745908: Epoch time: 254.21 s
2024-05-08 20:20:56.085275: 
2024-05-08 20:20:56.086733: Epoch 93
2024-05-08 20:20:56.088125: Current learning rate: 0.00916
2024-05-08 20:24:52.543990: Validation loss did not improve from -0.63869. Patience: 17/50
2024-05-08 20:24:52.545429: train_loss -0.8377
2024-05-08 20:24:52.546568: val_loss -0.574
2024-05-08 20:24:52.547661: Pseudo dice [0.7286]
2024-05-08 20:24:52.548879: Epoch time: 236.46 s
2024-05-08 20:24:53.788064: 
2024-05-08 20:24:53.790605: Epoch 94
2024-05-08 20:24:53.792238: Current learning rate: 0.00915
2024-05-08 20:28:49.572640: Validation loss did not improve from -0.63869. Patience: 18/50
2024-05-08 20:28:49.573991: train_loss -0.8443
2024-05-08 20:28:49.575192: val_loss -0.6187
2024-05-08 20:28:49.576161: Pseudo dice [0.7389]
2024-05-08 20:28:49.577466: Epoch time: 235.79 s
2024-05-08 20:28:51.206761: 
2024-05-08 20:28:51.209250: Epoch 95
2024-05-08 20:28:51.210316: Current learning rate: 0.00914
2024-05-08 20:32:51.043807: Validation loss did not improve from -0.63869. Patience: 19/50
2024-05-08 20:32:51.060527: train_loss -0.8374
2024-05-08 20:32:51.062742: val_loss -0.6196
2024-05-08 20:32:51.063940: Pseudo dice [0.7619]
2024-05-08 20:32:51.065367: Epoch time: 239.85 s
2024-05-08 20:32:52.694151: 
2024-05-08 20:32:52.695943: Epoch 96
2024-05-08 20:32:52.697279: Current learning rate: 0.00913
2024-05-08 20:36:51.682083: Validation loss did not improve from -0.63869. Patience: 20/50
2024-05-08 20:36:51.740708: train_loss -0.8424
2024-05-08 20:36:51.742701: val_loss -0.581
2024-05-08 20:36:51.743921: Pseudo dice [0.7305]
2024-05-08 20:36:51.745134: Epoch time: 238.99 s
2024-05-08 20:36:53.234920: 
2024-05-08 20:36:53.236964: Epoch 97
2024-05-08 20:36:53.238103: Current learning rate: 0.00912
2024-05-08 20:40:54.683045: Validation loss did not improve from -0.63869. Patience: 21/50
2024-05-08 20:40:54.684397: train_loss -0.8375
2024-05-08 20:40:54.685447: val_loss -0.5971
2024-05-08 20:40:54.686461: Pseudo dice [0.7591]
2024-05-08 20:40:54.687485: Epoch time: 241.45 s
2024-05-08 20:40:57.500764: 
2024-05-08 20:40:57.502123: Epoch 98
2024-05-08 20:40:57.503110: Current learning rate: 0.00911
2024-05-08 20:44:53.946665: Validation loss did not improve from -0.63869. Patience: 22/50
2024-05-08 20:44:53.948005: train_loss -0.8411
2024-05-08 20:44:53.949283: val_loss -0.6078
2024-05-08 20:44:53.950586: Pseudo dice [0.7561]
2024-05-08 20:44:53.951878: Epoch time: 236.45 s
2024-05-08 20:44:55.223982: 
2024-05-08 20:44:55.225490: Epoch 99
2024-05-08 20:44:55.226725: Current learning rate: 0.0091
2024-05-08 20:48:54.429848: Validation loss did not improve from -0.63869. Patience: 23/50
2024-05-08 20:48:54.431115: train_loss -0.8459
2024-05-08 20:48:54.432215: val_loss -0.5874
2024-05-08 20:48:54.433293: Pseudo dice [0.7339]
2024-05-08 20:48:54.434303: Epoch time: 239.21 s
2024-05-08 20:48:56.345674: 
2024-05-08 20:48:56.347011: Epoch 100
2024-05-08 20:48:56.348206: Current learning rate: 0.0091
2024-05-08 20:52:52.659829: Validation loss did not improve from -0.63869. Patience: 24/50
2024-05-08 20:52:52.661396: train_loss -0.8457
2024-05-08 20:52:52.663028: val_loss -0.5571
2024-05-08 20:52:52.664335: Pseudo dice [0.7445]
2024-05-08 20:52:52.665468: Epoch time: 236.32 s
2024-05-08 20:52:53.894924: 
2024-05-08 20:52:53.897142: Epoch 101
2024-05-08 20:52:53.898915: Current learning rate: 0.00909
2024-05-08 20:56:50.397078: Validation loss did not improve from -0.63869. Patience: 25/50
2024-05-08 20:56:50.398409: train_loss -0.8411
2024-05-08 20:56:50.399995: val_loss -0.573
2024-05-08 20:56:50.401074: Pseudo dice [0.7253]
2024-05-08 20:56:50.402047: Epoch time: 236.5 s
2024-05-08 20:56:51.667774: 
2024-05-08 20:56:51.669486: Epoch 102
2024-05-08 20:56:51.670853: Current learning rate: 0.00908
2024-05-08 21:00:45.834826: Validation loss did not improve from -0.63869. Patience: 26/50
2024-05-08 21:00:45.836392: train_loss -0.8479
2024-05-08 21:00:45.837651: val_loss -0.5634
2024-05-08 21:00:45.839052: Pseudo dice [0.7395]
2024-05-08 21:00:45.840140: Epoch time: 234.17 s
2024-05-08 21:00:47.085268: 
2024-05-08 21:00:47.086681: Epoch 103
2024-05-08 21:00:47.087887: Current learning rate: 0.00907
2024-05-08 21:04:41.669957: Validation loss did not improve from -0.63869. Patience: 27/50
2024-05-08 21:04:41.671331: train_loss -0.847
2024-05-08 21:04:41.672466: val_loss -0.5793
2024-05-08 21:04:41.673456: Pseudo dice [0.7445]
2024-05-08 21:04:41.674486: Epoch time: 234.59 s
2024-05-08 21:04:42.973490: 
2024-05-08 21:04:42.975356: Epoch 104
2024-05-08 21:04:42.976468: Current learning rate: 0.00906
2024-05-08 21:08:38.496504: Validation loss did not improve from -0.63869. Patience: 28/50
2024-05-08 21:08:38.497606: train_loss -0.8418
2024-05-08 21:08:38.498803: val_loss -0.6046
2024-05-08 21:08:38.499845: Pseudo dice [0.7403]
2024-05-08 21:08:38.500799: Epoch time: 235.53 s
2024-05-08 21:08:40.409996: 
2024-05-08 21:08:40.412693: Epoch 105
2024-05-08 21:08:40.413926: Current learning rate: 0.00905
2024-05-08 21:12:35.056185: Validation loss improved from -0.63869 to -0.63933! Patience: 28/50
2024-05-08 21:12:35.057719: train_loss -0.8419
2024-05-08 21:12:35.058890: val_loss -0.6393
2024-05-08 21:12:35.059906: Pseudo dice [0.7585]
2024-05-08 21:12:35.060980: Epoch time: 234.65 s
2024-05-08 21:12:36.272002: 
2024-05-08 21:12:36.274264: Epoch 106
2024-05-08 21:12:36.275373: Current learning rate: 0.00904
2024-05-08 21:16:30.666681: Validation loss did not improve from -0.63933. Patience: 1/50
2024-05-08 21:16:30.667949: train_loss -0.8432
2024-05-08 21:16:30.669135: val_loss -0.6055
2024-05-08 21:16:30.670285: Pseudo dice [0.7496]
2024-05-08 21:16:30.671466: Epoch time: 234.4 s
2024-05-08 21:16:32.018007: 
2024-05-08 21:16:32.019786: Epoch 107
2024-05-08 21:16:32.020747: Current learning rate: 0.00903
2024-05-08 21:20:24.748237: Validation loss did not improve from -0.63933. Patience: 2/50
2024-05-08 21:20:24.750128: train_loss -0.8454
2024-05-08 21:20:24.751623: val_loss -0.6189
2024-05-08 21:20:24.752752: Pseudo dice [0.7573]
2024-05-08 21:20:24.753938: Epoch time: 232.73 s
2024-05-08 21:20:24.754985: Yayy! New best EMA pseudo Dice: 0.745
2024-05-08 21:20:26.483233: 
2024-05-08 21:20:26.485368: Epoch 108
2024-05-08 21:20:26.486989: Current learning rate: 0.00902
2024-05-08 21:24:22.385873: Validation loss did not improve from -0.63933. Patience: 3/50
2024-05-08 21:24:22.387149: train_loss -0.8435
2024-05-08 21:24:22.388395: val_loss -0.5901
2024-05-08 21:24:22.389495: Pseudo dice [0.7348]
2024-05-08 21:24:22.390639: Epoch time: 235.91 s
2024-05-08 21:24:23.669233: 
2024-05-08 21:24:23.671175: Epoch 109
2024-05-08 21:24:23.672379: Current learning rate: 0.00901
2024-05-08 21:28:20.106250: Validation loss did not improve from -0.63933. Patience: 4/50
2024-05-08 21:28:20.108300: train_loss -0.836
2024-05-08 21:28:20.109643: val_loss -0.5595
2024-05-08 21:28:20.110997: Pseudo dice [0.7322]
2024-05-08 21:28:20.112136: Epoch time: 236.44 s
2024-05-08 21:28:23.016483: 
2024-05-08 21:28:23.018888: Epoch 110
2024-05-08 21:28:23.019992: Current learning rate: 0.009
2024-05-08 21:32:19.898895: Validation loss did not improve from -0.63933. Patience: 5/50
2024-05-08 21:32:19.900661: train_loss -0.8403
2024-05-08 21:32:19.901996: val_loss -0.5885
2024-05-08 21:32:19.903155: Pseudo dice [0.7379]
2024-05-08 21:32:19.904294: Epoch time: 236.89 s
2024-05-08 21:32:21.206209: 
2024-05-08 21:32:21.209523: Epoch 111
2024-05-08 21:32:21.211896: Current learning rate: 0.009
2024-05-08 21:36:20.801132: Validation loss did not improve from -0.63933. Patience: 6/50
2024-05-08 21:36:20.910086: train_loss -0.8381
2024-05-08 21:36:20.911089: val_loss -0.5728
2024-05-08 21:36:20.912467: Pseudo dice [0.7133]
2024-05-08 21:36:20.913552: Epoch time: 239.6 s
2024-05-08 21:36:23.026560: 
2024-05-08 21:36:23.028521: Epoch 112
2024-05-08 21:36:23.034975: Current learning rate: 0.00899
2024-05-08 21:40:19.762329: Validation loss did not improve from -0.63933. Patience: 7/50
2024-05-08 21:40:19.819914: train_loss -0.8423
2024-05-08 21:40:19.822853: val_loss -0.5531
2024-05-08 21:40:19.824552: Pseudo dice [0.7168]
2024-05-08 21:40:19.825771: Epoch time: 236.74 s
2024-05-08 21:40:21.570149: 
2024-05-08 21:40:21.571565: Epoch 113
2024-05-08 21:40:21.573383: Current learning rate: 0.00898
2024-05-08 21:44:16.364033: Validation loss did not improve from -0.63933. Patience: 8/50
2024-05-08 21:44:16.366149: train_loss -0.8419
2024-05-08 21:44:16.367214: val_loss -0.5735
2024-05-08 21:44:16.368539: Pseudo dice [0.7297]
2024-05-08 21:44:16.369595: Epoch time: 234.8 s
2024-05-08 21:44:17.701820: 
2024-05-08 21:44:17.703836: Epoch 114
2024-05-08 21:44:17.704864: Current learning rate: 0.00897
2024-05-08 21:48:12.640574: Validation loss did not improve from -0.63933. Patience: 9/50
2024-05-08 21:48:12.642140: train_loss -0.8478
2024-05-08 21:48:12.643368: val_loss -0.6155
2024-05-08 21:48:12.644455: Pseudo dice [0.753]
2024-05-08 21:48:12.645984: Epoch time: 234.94 s
2024-05-08 21:48:18.091354: 
2024-05-08 21:48:18.093444: Epoch 115
2024-05-08 21:48:18.094652: Current learning rate: 0.00896
2024-05-08 21:52:13.464702: Validation loss did not improve from -0.63933. Patience: 10/50
2024-05-08 21:52:13.465859: train_loss -0.8471
2024-05-08 21:52:13.467148: val_loss -0.566
2024-05-08 21:52:13.468465: Pseudo dice [0.7271]
2024-05-08 21:52:13.469584: Epoch time: 235.38 s
2024-05-08 21:52:14.767304: 
2024-05-08 21:52:14.769911: Epoch 116
2024-05-08 21:52:14.771249: Current learning rate: 0.00895
2024-05-08 21:56:08.945579: Validation loss did not improve from -0.63933. Patience: 11/50
2024-05-08 21:56:08.946681: train_loss -0.8459
2024-05-08 21:56:08.947763: val_loss -0.5747
2024-05-08 21:56:08.950166: Pseudo dice [0.7353]
2024-05-08 21:56:08.951267: Epoch time: 234.18 s
2024-05-08 21:56:10.309671: 
2024-05-08 21:56:10.311388: Epoch 117
2024-05-08 21:56:10.312500: Current learning rate: 0.00894
2024-05-08 22:00:05.410992: Validation loss did not improve from -0.63933. Patience: 12/50
2024-05-08 22:00:05.412691: train_loss -0.8505
2024-05-08 22:00:05.414103: val_loss -0.5849
2024-05-08 22:00:05.415227: Pseudo dice [0.7377]
2024-05-08 22:00:05.416295: Epoch time: 235.1 s
2024-05-08 22:00:06.730352: 
2024-05-08 22:00:06.731977: Epoch 118
2024-05-08 22:00:06.733176: Current learning rate: 0.00893
2024-05-08 22:04:01.289333: Validation loss improved from -0.63933 to -0.64542! Patience: 12/50
2024-05-08 22:04:01.291210: train_loss -0.8488
2024-05-08 22:04:01.292802: val_loss -0.6454
2024-05-08 22:04:01.294160: Pseudo dice [0.7658]
2024-05-08 22:04:01.295602: Epoch time: 234.56 s
2024-05-08 22:04:02.593313: 
2024-05-08 22:04:02.595518: Epoch 119
2024-05-08 22:04:02.596727: Current learning rate: 0.00892
2024-05-08 22:08:00.459652: Validation loss did not improve from -0.64542. Patience: 1/50
2024-05-08 22:08:00.460931: train_loss -0.8503
2024-05-08 22:08:00.462424: val_loss -0.6164
2024-05-08 22:08:00.463663: Pseudo dice [0.7646]
2024-05-08 22:08:00.464862: Epoch time: 237.87 s
2024-05-08 22:08:02.076401: 
2024-05-08 22:08:02.078512: Epoch 120
2024-05-08 22:08:02.080001: Current learning rate: 0.00891
2024-05-08 22:11:55.930944: Validation loss did not improve from -0.64542. Patience: 2/50
2024-05-08 22:11:55.932361: train_loss -0.8384
2024-05-08 22:11:55.933696: val_loss -0.6029
2024-05-08 22:11:55.934826: Pseudo dice [0.7548]
2024-05-08 22:11:55.935880: Epoch time: 233.86 s
2024-05-08 22:11:57.362299: 
2024-05-08 22:11:57.885786: Epoch 121
2024-05-08 22:11:57.887866: Current learning rate: 0.0089
2024-05-08 22:15:52.834098: Validation loss did not improve from -0.64542. Patience: 3/50
2024-05-08 22:15:52.835208: train_loss -0.8413
2024-05-08 22:15:52.836187: val_loss -0.5694
2024-05-08 22:15:52.837155: Pseudo dice [0.7246]
2024-05-08 22:15:52.838102: Epoch time: 235.47 s
2024-05-08 22:16:00.123542: 
2024-05-08 22:16:00.126147: Epoch 122
2024-05-08 22:16:00.127653: Current learning rate: 0.00889
2024-05-08 22:19:54.517963: Validation loss did not improve from -0.64542. Patience: 4/50
2024-05-08 22:19:54.519326: train_loss -0.8462
2024-05-08 22:19:54.520554: val_loss -0.5932
2024-05-08 22:19:54.521629: Pseudo dice [0.7535]
2024-05-08 22:19:54.522727: Epoch time: 234.4 s
2024-05-08 22:19:55.787592: 
2024-05-08 22:19:55.789549: Epoch 123
2024-05-08 22:19:55.790633: Current learning rate: 0.00889
2024-05-08 22:23:50.280213: Validation loss did not improve from -0.64542. Patience: 5/50
2024-05-08 22:23:50.281505: train_loss -0.8459
2024-05-08 22:23:50.282654: val_loss -0.5596
2024-05-08 22:23:50.283579: Pseudo dice [0.7322]
2024-05-08 22:23:50.284604: Epoch time: 234.5 s
2024-05-08 22:23:51.575786: 
2024-05-08 22:23:51.578389: Epoch 124
2024-05-08 22:23:51.579412: Current learning rate: 0.00888
2024-05-08 22:27:45.865271: Validation loss did not improve from -0.64542. Patience: 6/50
2024-05-08 22:27:45.866455: train_loss -0.8461
2024-05-08 22:27:45.867646: val_loss -0.5899
2024-05-08 22:27:45.868562: Pseudo dice [0.7371]
2024-05-08 22:27:45.869640: Epoch time: 234.29 s
2024-05-08 22:27:47.600991: 
2024-05-08 22:27:47.602714: Epoch 125
2024-05-08 22:27:47.603755: Current learning rate: 0.00887
2024-05-08 22:31:42.470687: Validation loss did not improve from -0.64542. Patience: 7/50
2024-05-08 22:31:42.472134: train_loss -0.8423
2024-05-08 22:31:42.473622: val_loss -0.5899
2024-05-08 22:31:42.474692: Pseudo dice [0.7323]
2024-05-08 22:31:42.475703: Epoch time: 234.87 s
2024-05-08 22:31:43.750751: 
2024-05-08 22:31:43.752386: Epoch 126
2024-05-08 22:31:43.753360: Current learning rate: 0.00886
2024-05-08 22:35:37.537639: Validation loss did not improve from -0.64542. Patience: 8/50
2024-05-08 22:35:37.538942: train_loss -0.8461
2024-05-08 22:35:37.540270: val_loss -0.6224
2024-05-08 22:35:37.541365: Pseudo dice [0.7529]
2024-05-08 22:35:37.542542: Epoch time: 233.79 s
2024-05-08 22:35:38.920346: 
2024-05-08 22:35:38.922473: Epoch 127
2024-05-08 22:35:38.923540: Current learning rate: 0.00885
2024-05-08 22:39:35.279936: Validation loss did not improve from -0.64542. Patience: 9/50
2024-05-08 22:39:35.281148: train_loss -0.8486
2024-05-08 22:39:35.282385: val_loss -0.6074
2024-05-08 22:39:35.283593: Pseudo dice [0.7464]
2024-05-08 22:39:35.284721: Epoch time: 236.36 s
2024-05-08 22:39:36.580074: 
2024-05-08 22:39:36.582170: Epoch 128
2024-05-08 22:39:36.583900: Current learning rate: 0.00884
2024-05-08 22:43:37.887958: Validation loss did not improve from -0.64542. Patience: 10/50
2024-05-08 22:43:37.901880: train_loss -0.8468
2024-05-08 22:43:37.904819: val_loss -0.5572
2024-05-08 22:43:37.906289: Pseudo dice [0.7237]
2024-05-08 22:43:37.907756: Epoch time: 241.31 s
2024-05-08 22:43:39.515952: 
2024-05-08 22:43:39.517838: Epoch 129
2024-05-08 22:43:39.519301: Current learning rate: 0.00883
2024-05-08 22:47:39.404889: Validation loss did not improve from -0.64542. Patience: 11/50
2024-05-08 22:47:39.437597: train_loss -0.8518
2024-05-08 22:47:39.439101: val_loss -0.6085
2024-05-08 22:47:39.440084: Pseudo dice [0.7464]
2024-05-08 22:47:39.441054: Epoch time: 239.92 s
2024-05-08 22:47:41.845810: 
2024-05-08 22:47:41.848210: Epoch 130
2024-05-08 22:47:41.849298: Current learning rate: 0.00882
2024-05-08 22:51:39.635379: Validation loss did not improve from -0.64542. Patience: 12/50
2024-05-08 22:51:39.636588: train_loss -0.8525
2024-05-08 22:51:39.637737: val_loss -0.5601
2024-05-08 22:51:39.638699: Pseudo dice [0.7321]
2024-05-08 22:51:39.639751: Epoch time: 237.79 s
2024-05-08 22:51:40.973203: 
2024-05-08 22:51:40.975293: Epoch 131
2024-05-08 22:51:40.976441: Current learning rate: 0.00881
2024-05-08 22:55:43.074034: Validation loss did not improve from -0.64542. Patience: 13/50
2024-05-08 22:55:43.075733: train_loss -0.8534
2024-05-08 22:55:43.076929: val_loss -0.5847
2024-05-08 22:55:43.078058: Pseudo dice [0.7233]
2024-05-08 22:55:43.079104: Epoch time: 242.1 s
2024-05-08 22:55:44.807239: 
2024-05-08 22:55:44.810310: Epoch 132
2024-05-08 22:55:44.812007: Current learning rate: 0.0088
2024-05-08 22:59:41.801837: Validation loss did not improve from -0.64542. Patience: 14/50
2024-05-08 22:59:41.803122: train_loss -0.8518
2024-05-08 22:59:41.804223: val_loss -0.608
2024-05-08 22:59:41.805323: Pseudo dice [0.7592]
2024-05-08 22:59:41.806356: Epoch time: 237.0 s
2024-05-08 22:59:49.259309: 
2024-05-08 22:59:49.261919: Epoch 133
2024-05-08 22:59:49.262967: Current learning rate: 0.00879
2024-05-08 23:03:46.267895: Validation loss did not improve from -0.64542. Patience: 15/50
2024-05-08 23:03:46.269141: train_loss -0.8486
2024-05-08 23:03:46.270253: val_loss -0.6219
2024-05-08 23:03:46.271466: Pseudo dice [0.7621]
2024-05-08 23:03:46.272649: Epoch time: 237.01 s
2024-05-08 23:03:47.625793: 
2024-05-08 23:03:47.627458: Epoch 134
2024-05-08 23:03:47.628596: Current learning rate: 0.00879
2024-05-08 23:07:44.821633: Validation loss did not improve from -0.64542. Patience: 16/50
2024-05-08 23:07:44.822997: train_loss -0.853
2024-05-08 23:07:44.824153: val_loss -0.6223
2024-05-08 23:07:44.825231: Pseudo dice [0.7647]
2024-05-08 23:07:44.826232: Epoch time: 237.2 s
2024-05-08 23:07:46.572845: 
2024-05-08 23:07:46.575505: Epoch 135
2024-05-08 23:07:46.577144: Current learning rate: 0.00878
2024-05-08 23:11:44.454139: Validation loss did not improve from -0.64542. Patience: 17/50
2024-05-08 23:11:44.455412: train_loss -0.8544
2024-05-08 23:11:44.456681: val_loss -0.5164
2024-05-08 23:11:44.460207: Pseudo dice [0.6925]
2024-05-08 23:11:44.461866: Epoch time: 237.88 s
2024-05-08 23:11:45.808361: 
2024-05-08 23:11:45.810824: Epoch 136
2024-05-08 23:11:45.812034: Current learning rate: 0.00877
2024-05-08 23:15:44.969993: Validation loss did not improve from -0.64542. Patience: 18/50
2024-05-08 23:15:44.971287: train_loss -0.8555
2024-05-08 23:15:44.972347: val_loss -0.6156
2024-05-08 23:15:44.973425: Pseudo dice [0.7489]
2024-05-08 23:15:44.974462: Epoch time: 239.16 s
2024-05-08 23:15:46.334395: 
2024-05-08 23:15:46.336173: Epoch 137
2024-05-08 23:15:46.337120: Current learning rate: 0.00876
2024-05-08 23:19:41.337965: Validation loss did not improve from -0.64542. Patience: 19/50
2024-05-08 23:19:41.339158: train_loss -0.8557
2024-05-08 23:19:41.340489: val_loss -0.5547
2024-05-08 23:19:41.341777: Pseudo dice [0.716]
2024-05-08 23:19:41.343041: Epoch time: 235.01 s
2024-05-08 23:19:42.686720: 
2024-05-08 23:19:42.689464: Epoch 138
2024-05-08 23:19:42.691336: Current learning rate: 0.00875
2024-05-08 23:23:40.024462: Validation loss did not improve from -0.64542. Patience: 20/50
2024-05-08 23:23:40.025717: train_loss -0.8557
2024-05-08 23:23:40.026825: val_loss -0.578
2024-05-08 23:23:40.027923: Pseudo dice [0.7319]
2024-05-08 23:23:40.028888: Epoch time: 237.34 s
2024-05-08 23:23:41.357422: 
2024-05-08 23:23:41.360829: Epoch 139
2024-05-08 23:23:41.362303: Current learning rate: 0.00874
2024-05-08 23:27:38.853088: Validation loss did not improve from -0.64542. Patience: 21/50
2024-05-08 23:27:38.855574: train_loss -0.8586
2024-05-08 23:27:38.856819: val_loss -0.6287
2024-05-08 23:27:38.857725: Pseudo dice [0.7591]
2024-05-08 23:27:38.858594: Epoch time: 237.5 s
2024-05-08 23:27:40.493238: 
2024-05-08 23:27:40.494752: Epoch 140
2024-05-08 23:27:40.496264: Current learning rate: 0.00873
2024-05-08 23:31:35.160296: Validation loss did not improve from -0.64542. Patience: 22/50
2024-05-08 23:31:35.161496: train_loss -0.8564
2024-05-08 23:31:35.162827: val_loss -0.5805
2024-05-08 23:31:35.163861: Pseudo dice [0.7375]
2024-05-08 23:31:35.164841: Epoch time: 234.67 s
2024-05-08 23:31:36.504665: 
2024-05-08 23:31:36.506975: Epoch 141
2024-05-08 23:31:36.508252: Current learning rate: 0.00872
2024-05-08 23:35:33.491724: Validation loss did not improve from -0.64542. Patience: 23/50
2024-05-08 23:35:33.493068: train_loss -0.8562
2024-05-08 23:35:33.494237: val_loss -0.5039
2024-05-08 23:35:33.495183: Pseudo dice [0.7156]
2024-05-08 23:35:33.496107: Epoch time: 236.99 s
2024-05-08 23:35:34.832482: 
2024-05-08 23:35:34.834178: Epoch 142
2024-05-08 23:35:34.835500: Current learning rate: 0.00871
2024-05-08 23:39:32.506053: Validation loss did not improve from -0.64542. Patience: 24/50
2024-05-08 23:39:32.507710: train_loss -0.8527
2024-05-08 23:39:32.508895: val_loss -0.5567
2024-05-08 23:39:32.509977: Pseudo dice [0.7349]
2024-05-08 23:39:32.510969: Epoch time: 237.68 s
2024-05-08 23:39:33.836523: 
2024-05-08 23:39:33.838484: Epoch 143
2024-05-08 23:39:33.839728: Current learning rate: 0.0087
2024-05-08 23:43:31.409810: Validation loss did not improve from -0.64542. Patience: 25/50
2024-05-08 23:43:31.411470: train_loss -0.8536
2024-05-08 23:43:31.412620: val_loss -0.5554
2024-05-08 23:43:31.413635: Pseudo dice [0.7202]
2024-05-08 23:43:31.414577: Epoch time: 237.58 s
2024-05-08 23:43:32.716699: 
2024-05-08 23:43:32.718600: Epoch 144
2024-05-08 23:43:32.719776: Current learning rate: 0.00869
2024-05-08 23:47:32.308780: Validation loss did not improve from -0.64542. Patience: 26/50
2024-05-08 23:47:32.311535: train_loss -0.8486
2024-05-08 23:47:32.313961: val_loss -0.5987
2024-05-08 23:47:32.315160: Pseudo dice [0.7493]
2024-05-08 23:47:32.316689: Epoch time: 239.6 s
2024-05-08 23:47:35.881731: 
2024-05-08 23:47:35.884279: Epoch 145
2024-05-08 23:47:35.886093: Current learning rate: 0.00868
2024-05-08 23:51:31.463422: Validation loss did not improve from -0.64542. Patience: 27/50
2024-05-08 23:51:31.506801: train_loss -0.8496
2024-05-08 23:51:31.508211: val_loss -0.5515
2024-05-08 23:51:31.509165: Pseudo dice [0.7192]
2024-05-08 23:51:31.510105: Epoch time: 235.63 s
2024-05-08 23:51:32.930875: 
2024-05-08 23:51:32.933329: Epoch 146
2024-05-08 23:51:32.935342: Current learning rate: 0.00868
2024-05-08 23:55:28.853650: Validation loss did not improve from -0.64542. Patience: 28/50
2024-05-08 23:55:28.855348: train_loss -0.8543
2024-05-08 23:55:28.856623: val_loss -0.594
2024-05-08 23:55:28.857683: Pseudo dice [0.7412]
2024-05-08 23:55:28.858727: Epoch time: 235.93 s
2024-05-08 23:55:30.167025: 
2024-05-08 23:55:30.168840: Epoch 147
2024-05-08 23:55:30.170099: Current learning rate: 0.00867
2024-05-08 23:59:25.822353: Validation loss did not improve from -0.64542. Patience: 29/50
2024-05-08 23:59:25.823742: train_loss -0.855
2024-05-08 23:59:25.824877: val_loss -0.5623
2024-05-08 23:59:25.825829: Pseudo dice [0.7123]
2024-05-08 23:59:25.826895: Epoch time: 235.66 s
2024-05-08 23:59:27.258475: 
2024-05-08 23:59:27.260123: Epoch 148
2024-05-08 23:59:27.261372: Current learning rate: 0.00866
2024-05-09 00:03:23.346960: Validation loss did not improve from -0.64542. Patience: 30/50
2024-05-09 00:03:23.348514: train_loss -0.8561
2024-05-09 00:03:23.349915: val_loss -0.5754
2024-05-09 00:03:23.350843: Pseudo dice [0.7199]
2024-05-09 00:03:23.351941: Epoch time: 236.09 s
2024-05-09 00:03:25.630353: 
2024-05-09 00:03:25.631928: Epoch 149
2024-05-09 00:03:25.632996: Current learning rate: 0.00865
2024-05-09 00:07:22.028581: Validation loss did not improve from -0.64542. Patience: 31/50
2024-05-09 00:07:22.029814: train_loss -0.8574
2024-05-09 00:07:22.030810: val_loss -0.5832
2024-05-09 00:07:22.031785: Pseudo dice [0.741]
2024-05-09 00:07:22.032832: Epoch time: 236.4 s
2024-05-09 00:07:24.708102: 
2024-05-09 00:07:24.710111: Epoch 150
2024-05-09 00:07:24.711977: Current learning rate: 0.00864
2024-05-09 00:11:20.943994: Validation loss did not improve from -0.64542. Patience: 32/50
2024-05-09 00:11:20.945449: train_loss -0.8518
2024-05-09 00:11:20.946879: val_loss -0.547
2024-05-09 00:11:20.948163: Pseudo dice [0.7142]
2024-05-09 00:11:20.949527: Epoch time: 236.24 s
2024-05-09 00:11:22.278340: 
2024-05-09 00:11:22.280025: Epoch 151
2024-05-09 00:11:22.281338: Current learning rate: 0.00863
2024-05-09 00:15:18.612001: Validation loss did not improve from -0.64542. Patience: 33/50
2024-05-09 00:15:18.613360: train_loss -0.8544
2024-05-09 00:15:18.614808: val_loss -0.5434
2024-05-09 00:15:18.615857: Pseudo dice [0.7258]
2024-05-09 00:15:18.616988: Epoch time: 236.34 s
2024-05-09 00:15:20.003918: 
2024-05-09 00:15:20.005882: Epoch 152
2024-05-09 00:15:20.007017: Current learning rate: 0.00862
2024-05-09 00:19:14.496386: Validation loss did not improve from -0.64542. Patience: 34/50
2024-05-09 00:19:14.498144: train_loss -0.8598
2024-05-09 00:19:14.499733: val_loss -0.5438
2024-05-09 00:19:14.500946: Pseudo dice [0.7221]
2024-05-09 00:19:14.502320: Epoch time: 234.5 s
2024-05-09 00:19:15.883554: 
2024-05-09 00:19:15.885250: Epoch 153
2024-05-09 00:19:15.886386: Current learning rate: 0.00861
2024-05-09 00:23:12.522786: Validation loss did not improve from -0.64542. Patience: 35/50
2024-05-09 00:23:12.523982: train_loss -0.8514
2024-05-09 00:23:12.525066: val_loss -0.5354
2024-05-09 00:23:12.526068: Pseudo dice [0.6898]
2024-05-09 00:23:12.527091: Epoch time: 236.64 s
2024-05-09 00:23:13.936733: 
2024-05-09 00:23:13.938682: Epoch 154
2024-05-09 00:23:13.939925: Current learning rate: 0.0086
2024-05-09 00:27:12.085504: Validation loss did not improve from -0.64542. Patience: 36/50
2024-05-09 00:27:12.086882: train_loss -0.8475
2024-05-09 00:27:12.088921: val_loss -0.5431
2024-05-09 00:27:12.090374: Pseudo dice [0.7099]
2024-05-09 00:27:12.091505: Epoch time: 238.15 s
2024-05-09 00:27:13.820073: 
2024-05-09 00:27:13.822206: Epoch 155
2024-05-09 00:27:13.823797: Current learning rate: 0.00859
2024-05-09 00:31:09.946803: Validation loss did not improve from -0.64542. Patience: 37/50
2024-05-09 00:31:09.948189: train_loss -0.8505
2024-05-09 00:31:09.949522: val_loss -0.5941
2024-05-09 00:31:09.950460: Pseudo dice [0.7485]
2024-05-09 00:31:09.951516: Epoch time: 236.13 s
2024-05-09 00:31:11.315810: 
2024-05-09 00:31:11.317372: Epoch 156
2024-05-09 00:31:11.318637: Current learning rate: 0.00858
2024-05-09 00:35:07.889724: Validation loss did not improve from -0.64542. Patience: 38/50
2024-05-09 00:35:07.890813: train_loss -0.8537
2024-05-09 00:35:07.891818: val_loss -0.5622
2024-05-09 00:35:07.892706: Pseudo dice [0.7282]
2024-05-09 00:35:07.893614: Epoch time: 236.58 s
2024-05-09 00:35:11.123543: 
2024-05-09 00:35:11.126160: Epoch 157
2024-05-09 00:35:11.127634: Current learning rate: 0.00858
2024-05-09 00:39:06.547037: Validation loss did not improve from -0.64542. Patience: 39/50
2024-05-09 00:39:06.548657: train_loss -0.8541
2024-05-09 00:39:06.549829: val_loss -0.5249
2024-05-09 00:39:06.550788: Pseudo dice [0.6991]
2024-05-09 00:39:06.551744: Epoch time: 235.43 s
2024-05-09 00:39:07.946427: 
2024-05-09 00:39:07.948135: Epoch 158
2024-05-09 00:39:07.949199: Current learning rate: 0.00857
2024-05-09 00:43:04.612552: Validation loss did not improve from -0.64542. Patience: 40/50
2024-05-09 00:43:04.615277: train_loss -0.8538
2024-05-09 00:43:04.616970: val_loss -0.4961
2024-05-09 00:43:04.618269: Pseudo dice [0.6938]
2024-05-09 00:43:04.619674: Epoch time: 236.67 s
2024-05-09 00:43:06.077422: 
2024-05-09 00:43:06.078907: Epoch 159
2024-05-09 00:43:06.079996: Current learning rate: 0.00856
2024-05-09 00:47:01.323504: Validation loss did not improve from -0.64542. Patience: 41/50
2024-05-09 00:47:01.325111: train_loss -0.8535
2024-05-09 00:47:01.326527: val_loss -0.5784
2024-05-09 00:47:01.327417: Pseudo dice [0.7261]
2024-05-09 00:47:01.328346: Epoch time: 235.25 s
2024-05-09 00:47:03.097663: 
2024-05-09 00:47:03.099460: Epoch 160
2024-05-09 00:47:03.100666: Current learning rate: 0.00855
2024-05-09 00:50:59.768181: Validation loss did not improve from -0.64542. Patience: 42/50
2024-05-09 00:50:59.769304: train_loss -0.8448
2024-05-09 00:50:59.770412: val_loss -0.537
2024-05-09 00:50:59.771414: Pseudo dice [0.6951]
2024-05-09 00:50:59.772435: Epoch time: 236.67 s
2024-05-09 00:51:01.365239: 
2024-05-09 00:51:01.366618: Epoch 161
2024-05-09 00:51:01.367786: Current learning rate: 0.00854
2024-05-09 00:54:56.754516: Validation loss did not improve from -0.64542. Patience: 43/50
2024-05-09 00:54:56.756352: train_loss -0.8475
2024-05-09 00:54:56.758628: val_loss -0.5892
2024-05-09 00:54:56.759711: Pseudo dice [0.7441]
2024-05-09 00:54:56.761068: Epoch time: 235.39 s
2024-05-09 00:54:58.279622: 
2024-05-09 00:54:58.281906: Epoch 162
2024-05-09 00:54:58.283332: Current learning rate: 0.00853
2024-05-09 00:58:56.668594: Validation loss did not improve from -0.64542. Patience: 44/50
2024-05-09 00:58:56.670704: train_loss -0.8495
2024-05-09 00:58:56.671701: val_loss -0.4667
2024-05-09 00:58:56.672718: Pseudo dice [0.6583]
2024-05-09 00:58:56.673592: Epoch time: 238.39 s
2024-05-09 00:58:58.083636: 
2024-05-09 00:58:58.085743: Epoch 163
2024-05-09 00:58:58.086816: Current learning rate: 0.00852
2024-05-09 01:02:53.429731: Validation loss did not improve from -0.64542. Patience: 45/50
2024-05-09 01:02:53.431149: train_loss -0.8486
2024-05-09 01:02:53.432385: val_loss -0.6028
2024-05-09 01:02:53.433422: Pseudo dice [0.7437]
2024-05-09 01:02:53.434505: Epoch time: 235.35 s
2024-05-09 01:02:54.811272: 
2024-05-09 01:02:54.813530: Epoch 164
2024-05-09 01:02:54.814993: Current learning rate: 0.00851
2024-05-09 01:06:51.058170: Validation loss did not improve from -0.64542. Patience: 46/50
2024-05-09 01:06:51.059297: train_loss -0.8531
2024-05-09 01:06:51.060615: val_loss -0.5717
2024-05-09 01:06:51.061524: Pseudo dice [0.7191]
2024-05-09 01:06:51.062471: Epoch time: 236.25 s
2024-05-09 01:06:52.949514: 
2024-05-09 01:06:52.950696: Epoch 165
2024-05-09 01:06:52.951609: Current learning rate: 0.0085
2024-05-09 01:10:53.215327: Validation loss did not improve from -0.64542. Patience: 47/50
2024-05-09 01:10:53.216563: train_loss -0.8479
2024-05-09 01:10:53.217792: val_loss -0.6241
2024-05-09 01:10:53.218962: Pseudo dice [0.7585]
2024-05-09 01:10:53.220081: Epoch time: 240.27 s
2024-05-09 01:10:54.718223: 
2024-05-09 01:10:54.720133: Epoch 166
2024-05-09 01:10:54.721355: Current learning rate: 0.00849
2024-05-09 01:14:53.336067: Validation loss did not improve from -0.64542. Patience: 48/50
2024-05-09 01:14:53.337307: train_loss -0.8535
2024-05-09 01:14:53.338451: val_loss -0.5684
2024-05-09 01:14:53.339373: Pseudo dice [0.7273]
2024-05-09 01:14:53.340361: Epoch time: 238.62 s
2024-05-09 01:14:54.701723: 
2024-05-09 01:14:54.704111: Epoch 167
2024-05-09 01:14:54.705747: Current learning rate: 0.00848
2024-05-09 01:18:52.847688: Validation loss did not improve from -0.64542. Patience: 49/50
2024-05-09 01:18:52.848928: train_loss -0.8577
2024-05-09 01:18:52.850150: val_loss -0.5856
2024-05-09 01:18:52.851182: Pseudo dice [0.7431]
2024-05-09 01:18:52.852002: Epoch time: 238.15 s
2024-05-09 01:18:55.765816: 
2024-05-09 01:18:55.767701: Epoch 168
2024-05-09 01:18:55.768769: Current learning rate: 0.00847
2024-05-09 01:22:59.126167: Validation loss did not improve from -0.64542. Patience: 50/50
2024-05-09 01:22:59.127398: train_loss -0.8581
2024-05-09 01:22:59.128586: val_loss -0.5673
2024-05-09 01:22:59.129979: Pseudo dice [0.7428]
2024-05-09 01:22:59.131370: Epoch time: 243.36 s
2024-05-09 01:23:00.529868: Patience reached. Stopping training.
2024-05-09 01:23:01.061649: Training done.
2024-05-09 01:23:02.001916: Using splits from existing split file: /home/gridsan/nchutisilp/datasets/nnUNet_Datasets/nnUNet_preprocessed/Dataset302_Calcium_OCTv2/splits_final.json
2024-05-09 01:23:02.018952: The split file contains 3 splits.
2024-05-09 01:23:02.020439: Desired fold for training: 1
2024-05-09 01:23:02.021596: This split has 4 training and 2 validation cases.
2024-05-09 01:23:02.022855: predicting 101-044
2024-05-09 01:23:02.480258: 101-044, shape torch.Size([1, 404, 498, 498]), rank 0
2024-05-09 01:24:21.624684: predicting 106-002
2024-05-09 01:24:21.764675: 106-002, shape torch.Size([1, 539, 498, 498]), rank 0
2024-05-09 01:25:58.875182: Validation complete
2024-05-09 01:25:58.876039: Mean Validation Dice:  0.709711099937921
wandb: 
wandb: Run history:
wandb:            ema_fg_dice ▁▁▂▂▃▄▄▅▆▇▇▇▇▇▇█▇█▇█▇▇▇███▇▇███▇▇▇▇▆▆▅▅▅
wandb:   epoch_end_timestamps ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███
wandb: epoch_start_timestamps ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▆▇▇▇▇▇███
wandb:                    lrs ███▇▇▇▇▇▇▆▆▆▆▆▆▅▅▅▅▅▄▄▄▄▄▄▃▃▃▃▃▃▂▂▂▂▂▁▁▁
wandb:           mean_fg_dice ▂▄▄▁▆▆▆▇▆▆▇▇▅▆▇▇▆▅▄▆▆▄▆█▇▇▄▅█▆▇▅▅▅▅▆▃▃▇▇
wandb:           train_losses █▆▄▄▃▄▃▃▂▃▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:             val_losses ▆▅▅█▃▂▃▂▂▃▁▁▄▄▃▃▃▃▄▂▂▅▃▂▃▁▃▄▁▄▂▃▄▇▄▃▅▆▃▃
wandb: 
wandb: Run summary:
wandb:            ema_fg_dice 0.72645
wandb:   epoch_end_timestamps 1715232179.12723
wandb: epoch_start_timestamps 1715231935.76456
wandb:                    lrs 0.00847
wandb:           mean_fg_dice 0.7428
wandb:           train_losses -0.8581
wandb:             val_losses -0.56734
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/gridsan/nchutisilp/datasets/nnUNet_Datasets/nnUNet_results/Dataset302_Calcium_OCTv2/nnUNetTrainer__nnUNetPlans__3d_32x512x512_b2/fold_1/wandb/offline-run-20240508_135913-yi4i37j9
wandb: Find logs at: /home/gridsan/nchutisilp/datasets/nnUNet_Datasets/nnUNet_results/Dataset302_Calcium_OCTv2/nnUNetTrainer__nnUNetPlans__3d_32x512x512_b2/fold_1/wandb/offline-run-20240508_135913-yi4i37j9/logs
WARNING:urllib3.connectionpool:Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x7f879d1edd30>: Failed to establish a new connection: [Errno 101] Network is unreachable')': /api/4504800232407040/envelope/
WARNING:urllib3.connectionpool:Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x7f87ed2ab280>: Failed to establish a new connection: [Errno 101] Network is unreachable')': /api/4504800232407040/envelope/
WARNING:urllib3.connectionpool:Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x7f87ed2ab7f0>: Failed to establish a new connection: [Errno 101] Network is unreachable')': /api/4504800232407040/envelope/
WARNING:urllib3.connectionpool:Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x7f86a00e27c0>: Failed to establish a new connection: [Errno 101] Network is unreachable')': /api/4504800232407040/envelope/
WARNING:urllib3.connectionpool:Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x7f86a00f3eb0>: Failed to establish a new connection: [Errno 101] Network is unreachable')': /api/4504800232407040/envelope/
WARNING:urllib3.connectionpool:Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x7f86a00f3a60>: Failed to establish a new connection: [Errno 101] Network is unreachable')': /api/4504800232407040/envelope/
FOLD 1 CONFIG 3d_32x512x512_b2 TRAINER nnUNetTrainer

#######################################################################
Please cite the following paper when using nnU-Net:
Isensee, F., Jaeger, P. F., Kohl, S. A., Petersen, J., & Maier-Hein, K. H. (2021). nnU-Net: a self-configuring method for deep learning-based biomedical image segmentation. Nature methods, 18(2), 203-211.
#######################################################################

There are 2 cases in the source folder
I am process 0 out of 1 (max process ID is 0, we start counting with 0!)
There are 2 cases that I would like to predict

Predicting 101-045:
perform_everything_on_device: True
  0%|          | 0/23 [00:00<?, ?it/s]  4%|▍         | 1/23 [01:05<24:03, 65.62s/it]  9%|▊         | 2/23 [01:07<09:45, 27.89s/it] 13%|█▎        | 3/23 [01:08<05:16, 15.83s/it] 17%|█▋        | 4/23 [01:10<03:13, 10.17s/it] 22%|██▏       | 5/23 [01:11<02:06,  7.04s/it] 26%|██▌       | 6/23 [01:13<01:27,  5.15s/it] 30%|███       | 7/23 [01:14<01:03,  3.95s/it] 35%|███▍      | 8/23 [01:16<00:47,  3.17s/it] 39%|███▉      | 9/23 [01:17<00:36,  2.64s/it] 43%|████▎     | 10/23 [01:18<00:29,  2.28s/it] 48%|████▊     | 11/23 [01:20<00:24,  2.04s/it] 52%|█████▏    | 12/23 [01:21<00:20,  1.87s/it] 57%|█████▋    | 13/23 [01:23<00:17,  1.76s/it] 61%|██████    | 14/23 [01:24<00:15,  1.68s/it] 65%|██████▌   | 15/23 [01:26<00:12,  1.62s/it] 70%|██████▉   | 16/23 [01:27<00:11,  1.58s/it] 74%|███████▍  | 17/23 [01:29<00:09,  1.55s/it] 78%|███████▊  | 18/23 [01:30<00:07,  1.53s/it] 83%|████████▎ | 19/23 [01:32<00:06,  1.52s/it] 87%|████████▋ | 20/23 [01:33<00:04,  1.51s/it] 91%|█████████▏| 21/23 [01:35<00:03,  1.50s/it] 96%|█████████▌| 22/23 [01:36<00:01,  1.50s/it]100%|██████████| 23/23 [01:38<00:00,  1.49s/it]100%|██████████| 23/23 [01:38<00:00,  4.28s/it]
sending off prediction to background worker for resampling and export
done with 101-045

Predicting 706-005:
perform_everything_on_device: True
  0%|          | 0/23 [00:00<?, ?it/s]  4%|▍         | 1/23 [00:00<00:17,  1.25it/s]  9%|▊         | 2/23 [00:02<00:25,  1.21s/it] 13%|█▎        | 3/23 [00:03<00:26,  1.34s/it] 17%|█▋        | 4/23 [00:05<00:26,  1.40s/it] 22%|██▏       | 5/23 [00:06<00:25,  1.43s/it] 26%|██▌       | 6/23 [00:08<00:24,  1.45s/it] 30%|███       | 7/23 [00:09<00:23,  1.46s/it] 35%|███▍      | 8/23 [00:11<00:22,  1.47s/it] 39%|███▉      | 9/23 [00:12<00:20,  1.48s/it] 43%|████▎     | 10/23 [00:14<00:19,  1.48s/it] 48%|████▊     | 11/23 [00:15<00:17,  1.48s/it] 52%|█████▏    | 12/23 [00:17<00:16,  1.49s/it] 57%|█████▋    | 13/23 [00:18<00:14,  1.49s/it] 61%|██████    | 14/23 [00:20<00:13,  1.49s/it] 65%|██████▌   | 15/23 [00:21<00:11,  1.49s/it] 70%|██████▉   | 16/23 [00:23<00:10,  1.49s/it] 74%|███████▍  | 17/23 [00:24<00:08,  1.49s/it] 78%|███████▊  | 18/23 [00:26<00:07,  1.49s/it] 83%|████████▎ | 19/23 [00:27<00:05,  1.49s/it] 87%|████████▋ | 20/23 [00:29<00:04,  1.49s/it] 91%|█████████▏| 21/23 [00:30<00:02,  1.49s/it] 96%|█████████▌| 22/23 [00:32<00:01,  1.49s/it]100%|██████████| 23/23 [00:33<00:00,  1.49s/it]100%|██████████| 23/23 [00:33<00:00,  1.46s/it]
sending off prediction to background worker for resampling and export
done with 706-005
Completed FOLD 1 CONFIG 3d_32x512x512_b2 TRAINER nnUNetTrainer
