/home/gridsan/nchutisilp/.conda/envs/nnunet/bin/python
wandb: Tracking run with wandb version 0.16.6
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.

############################
INFO: You are using the old nnU-Net default plans. We have updated our recommendations. Please consider using those instead! Read more here: https://github.com/MIC-DKFZ/nnUNet/blob/master/documentation/resenc_presets.md
############################

Using device: cuda:0

#######################################################################
Please cite the following paper when using nnU-Net:
Isensee, F., Jaeger, P. F., Kohl, S. A., Petersen, J., & Maier-Hein, K. H. (2021). nnU-Net: a self-configuring method for deep learning-based biomedical image segmentation. Nature methods, 18(2), 203-211.
#######################################################################

2024-05-03 18:24:49.700893: Using torch.compile...
/home/gridsan/nchutisilp/.local/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn("The verbose parameter is deprecated. Please use get_last_lr() "
################### Loading pretrained weights from file  /home/gridsan/nchutisilp/datasets/nnUNet_Datasets/nnUNet_results/Dataset300_Lumen_and_Wall_OCT/nnUNetTrainer__nnUNetPreprocessPlans__3d_fullres/fold_all/checkpoint_best.pth ###################
Below is the list of overlapping blocks in pretrained model and nnUNet architecture:
encoder.stages.0.0.convs.0.conv.weight shape torch.Size([32, 1, 3, 3, 3])
encoder.stages.0.0.convs.0.conv.bias shape torch.Size([32])
encoder.stages.0.0.convs.0.norm.weight shape torch.Size([32])
encoder.stages.0.0.convs.0.norm.bias shape torch.Size([32])
encoder.stages.0.0.convs.0.all_modules.0.weight shape torch.Size([32, 1, 3, 3, 3])
encoder.stages.0.0.convs.0.all_modules.0.bias shape torch.Size([32])
encoder.stages.0.0.convs.0.all_modules.1.weight shape torch.Size([32])
encoder.stages.0.0.convs.0.all_modules.1.bias shape torch.Size([32])
encoder.stages.0.0.convs.1.conv.weight shape torch.Size([32, 32, 3, 3, 3])
encoder.stages.0.0.convs.1.conv.bias shape torch.Size([32])
encoder.stages.0.0.convs.1.norm.weight shape torch.Size([32])
encoder.stages.0.0.convs.1.norm.bias shape torch.Size([32])
encoder.stages.0.0.convs.1.all_modules.0.weight shape torch.Size([32, 32, 3, 3, 3])
encoder.stages.0.0.convs.1.all_modules.0.bias shape torch.Size([32])
encoder.stages.0.0.convs.1.all_modules.1.weight shape torch.Size([32])
encoder.stages.0.0.convs.1.all_modules.1.bias shape torch.Size([32])
encoder.stages.1.0.convs.0.conv.weight shape torch.Size([64, 32, 3, 3, 3])
encoder.stages.1.0.convs.0.conv.bias shape torch.Size([64])
encoder.stages.1.0.convs.0.norm.weight shape torch.Size([64])
encoder.stages.1.0.convs.0.norm.bias shape torch.Size([64])
encoder.stages.1.0.convs.0.all_modules.0.weight shape torch.Size([64, 32, 3, 3, 3])
encoder.stages.1.0.convs.0.all_modules.0.bias shape torch.Size([64])
encoder.stages.1.0.convs.0.all_modules.1.weight shape torch.Size([64])
encoder.stages.1.0.convs.0.all_modules.1.bias shape torch.Size([64])
encoder.stages.1.0.convs.1.conv.weight shape torch.Size([64, 64, 3, 3, 3])
encoder.stages.1.0.convs.1.conv.bias shape torch.Size([64])
encoder.stages.1.0.convs.1.norm.weight shape torch.Size([64])
encoder.stages.1.0.convs.1.norm.bias shape torch.Size([64])
encoder.stages.1.0.convs.1.all_modules.0.weight shape torch.Size([64, 64, 3, 3, 3])
encoder.stages.1.0.convs.1.all_modules.0.bias shape torch.Size([64])
encoder.stages.1.0.convs.1.all_modules.1.weight shape torch.Size([64])
encoder.stages.1.0.convs.1.all_modules.1.bias shape torch.Size([64])
encoder.stages.2.0.convs.0.conv.weight shape torch.Size([128, 64, 3, 3, 3])
encoder.stages.2.0.convs.0.conv.bias shape torch.Size([128])
encoder.stages.2.0.convs.0.norm.weight shape torch.Size([128])
encoder.stages.2.0.convs.0.norm.bias shape torch.Size([128])
encoder.stages.2.0.convs.0.all_modules.0.weight shape torch.Size([128, 64, 3, 3, 3])
encoder.stages.2.0.convs.0.all_modules.0.bias shape torch.Size([128])
encoder.stages.2.0.convs.0.all_modules.1.weight shape torch.Size([128])
encoder.stages.2.0.convs.0.all_modules.1.bias shape torch.Size([128])
encoder.stages.2.0.convs.1.conv.weight shape torch.Size([128, 128, 3, 3, 3])
encoder.stages.2.0.convs.1.conv.bias shape torch.Size([128])
encoder.stages.2.0.convs.1.norm.weight shape torch.Size([128])
encoder.stages.2.0.convs.1.norm.bias shape torch.Size([128])
encoder.stages.2.0.convs.1.all_modules.0.weight shape torch.Size([128, 128, 3, 3, 3])
encoder.stages.2.0.convs.1.all_modules.0.bias shape torch.Size([128])
encoder.stages.2.0.convs.1.all_modules.1.weight shape torch.Size([128])
encoder.stages.2.0.convs.1.all_modules.1.bias shape torch.Size([128])
encoder.stages.3.0.convs.0.conv.weight shape torch.Size([256, 128, 3, 3, 3])
encoder.stages.3.0.convs.0.conv.bias shape torch.Size([256])
encoder.stages.3.0.convs.0.norm.weight shape torch.Size([256])
encoder.stages.3.0.convs.0.norm.bias shape torch.Size([256])
encoder.stages.3.0.convs.0.all_modules.0.weight shape torch.Size([256, 128, 3, 3, 3])
encoder.stages.3.0.convs.0.all_modules.0.bias shape torch.Size([256])
encoder.stages.3.0.convs.0.all_modules.1.weight shape torch.Size([256])
encoder.stages.3.0.convs.0.all_modules.1.bias shape torch.Size([256])
encoder.stages.3.0.convs.1.conv.weight shape torch.Size([256, 256, 3, 3, 3])
encoder.stages.3.0.convs.1.conv.bias shape torch.Size([256])
encoder.stages.3.0.convs.1.norm.weight shape torch.Size([256])
encoder.stages.3.0.convs.1.norm.bias shape torch.Size([256])
encoder.stages.3.0.convs.1.all_modules.0.weight shape torch.Size([256, 256, 3, 3, 3])
encoder.stages.3.0.convs.1.all_modules.0.bias shape torch.Size([256])
encoder.stages.3.0.convs.1.all_modules.1.weight shape torch.Size([256])
encoder.stages.3.0.convs.1.all_modules.1.bias shape torch.Size([256])
encoder.stages.4.0.convs.0.conv.weight shape torch.Size([320, 256, 3, 3, 3])
encoder.stages.4.0.convs.0.conv.bias shape torch.Size([320])
encoder.stages.4.0.convs.0.norm.weight shape torch.Size([320])
encoder.stages.4.0.convs.0.norm.bias shape torch.Size([320])
encoder.stages.4.0.convs.0.all_modules.0.weight shape torch.Size([320, 256, 3, 3, 3])
encoder.stages.4.0.convs.0.all_modules.0.bias shape torch.Size([320])
encoder.stages.4.0.convs.0.all_modules.1.weight shape torch.Size([320])
encoder.stages.4.0.convs.0.all_modules.1.bias shape torch.Size([320])
encoder.stages.4.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.4.0.convs.1.conv.bias shape torch.Size([320])
encoder.stages.4.0.convs.1.norm.weight shape torch.Size([320])
encoder.stages.4.0.convs.1.norm.bias shape torch.Size([320])
encoder.stages.4.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.4.0.convs.1.all_modules.0.bias shape torch.Size([320])
encoder.stages.4.0.convs.1.all_modules.1.weight shape torch.Size([320])
encoder.stages.4.0.convs.1.all_modules.1.bias shape torch.Size([320])
encoder.stages.5.0.convs.0.conv.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.5.0.convs.0.conv.bias shape torch.Size([320])
encoder.stages.5.0.convs.0.norm.weight shape torch.Size([320])
encoder.stages.5.0.convs.0.norm.bias shape torch.Size([320])
encoder.stages.5.0.convs.0.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.5.0.convs.0.all_modules.0.bias shape torch.Size([320])
encoder.stages.5.0.convs.0.all_modules.1.weight shape torch.Size([320])
encoder.stages.5.0.convs.0.all_modules.1.bias shape torch.Size([320])
encoder.stages.5.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.5.0.convs.1.conv.bias shape torch.Size([320])
encoder.stages.5.0.convs.1.norm.weight shape torch.Size([320])
encoder.stages.5.0.convs.1.norm.bias shape torch.Size([320])
encoder.stages.5.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.5.0.convs.1.all_modules.0.bias shape torch.Size([320])
encoder.stages.5.0.convs.1.all_modules.1.weight shape torch.Size([320])
encoder.stages.5.0.convs.1.all_modules.1.bias shape torch.Size([320])
decoder.encoder.stages.0.0.convs.0.conv.weight shape torch.Size([32, 1, 3, 3, 3])
decoder.encoder.stages.0.0.convs.0.conv.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.norm.weight shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.norm.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.all_modules.0.weight shape torch.Size([32, 1, 3, 3, 3])
decoder.encoder.stages.0.0.convs.0.all_modules.0.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.all_modules.1.weight shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.all_modules.1.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.conv.weight shape torch.Size([32, 32, 3, 3, 3])
decoder.encoder.stages.0.0.convs.1.conv.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.norm.weight shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.norm.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.all_modules.0.weight shape torch.Size([32, 32, 3, 3, 3])
decoder.encoder.stages.0.0.convs.1.all_modules.0.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.all_modules.1.weight shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.all_modules.1.bias shape torch.Size([32])
decoder.encoder.stages.1.0.convs.0.conv.weight shape torch.Size([64, 32, 3, 3, 3])
decoder.encoder.stages.1.0.convs.0.conv.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.norm.weight shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.norm.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.all_modules.0.weight shape torch.Size([64, 32, 3, 3, 3])
decoder.encoder.stages.1.0.convs.0.all_modules.0.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.all_modules.1.weight shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.all_modules.1.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.conv.weight shape torch.Size([64, 64, 3, 3, 3])
decoder.encoder.stages.1.0.convs.1.conv.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.norm.weight shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.norm.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.all_modules.0.weight shape torch.Size([64, 64, 3, 3, 3])
decoder.encoder.stages.1.0.convs.1.all_modules.0.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.all_modules.1.weight shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.all_modules.1.bias shape torch.Size([64])
decoder.encoder.stages.2.0.convs.0.conv.weight shape torch.Size([128, 64, 3, 3, 3])
decoder.encoder.stages.2.0.convs.0.conv.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.norm.weight shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.norm.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.all_modules.0.weight shape torch.Size([128, 64, 3, 3, 3])
decoder.encoder.stages.2.0.convs.0.all_modules.0.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.all_modules.1.weight shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.all_modules.1.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.conv.weight shape torch.Size([128, 128, 3, 3, 3])
decoder.encoder.stages.2.0.convs.1.conv.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.norm.weight shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.norm.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.all_modules.0.weight shape torch.Size([128, 128, 3, 3, 3])
decoder.encoder.stages.2.0.convs.1.all_modules.0.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.all_modules.1.weight shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.all_modules.1.bias shape torch.Size([128])
decoder.encoder.stages.3.0.convs.0.conv.weight shape torch.Size([256, 128, 3, 3, 3])
decoder.encoder.stages.3.0.convs.0.conv.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.norm.weight shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.norm.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.all_modules.0.weight shape torch.Size([256, 128, 3, 3, 3])
decoder.encoder.stages.3.0.convs.0.all_modules.0.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.all_modules.1.weight shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.all_modules.1.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.conv.weight shape torch.Size([256, 256, 3, 3, 3])
decoder.encoder.stages.3.0.convs.1.conv.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.norm.weight shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.norm.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.all_modules.0.weight shape torch.Size([256, 256, 3, 3, 3])
decoder.encoder.stages.3.0.convs.1.all_modules.0.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.all_modules.1.weight shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.all_modules.1.bias shape torch.Size([256])
decoder.encoder.stages.4.0.convs.0.conv.weight shape torch.Size([320, 256, 3, 3, 3])
decoder.encoder.stages.4.0.convs.0.conv.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.norm.weight shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.norm.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.all_modules.0.weight shape torch.Size([320, 256, 3, 3, 3])
decoder.encoder.stages.4.0.convs.0.all_modules.0.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.all_modules.1.weight shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.all_modules.1.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.4.0.convs.1.conv.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.norm.weight shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.norm.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.4.0.convs.1.all_modules.0.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.all_modules.1.weight shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.all_modules.1.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.conv.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.5.0.convs.0.conv.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.norm.weight shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.norm.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.5.0.convs.0.all_modules.0.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.all_modules.1.weight shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.all_modules.1.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.5.0.convs.1.conv.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.norm.weight shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.norm.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.5.0.convs.1.all_modules.0.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.all_modules.1.weight shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.all_modules.1.bias shape torch.Size([320])
decoder.stages.0.convs.0.conv.weight shape torch.Size([320, 640, 3, 3, 3])
decoder.stages.0.convs.0.conv.bias shape torch.Size([320])
decoder.stages.0.convs.0.norm.weight shape torch.Size([320])
decoder.stages.0.convs.0.norm.bias shape torch.Size([320])
decoder.stages.0.convs.0.all_modules.0.weight shape torch.Size([320, 640, 3, 3, 3])
decoder.stages.0.convs.0.all_modules.0.bias shape torch.Size([320])
decoder.stages.0.convs.0.all_modules.1.weight shape torch.Size([320])
decoder.stages.0.convs.0.all_modules.1.bias shape torch.Size([320])
decoder.stages.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.stages.0.convs.1.conv.bias shape torch.Size([320])
decoder.stages.0.convs.1.norm.weight shape torch.Size([320])
decoder.stages.0.convs.1.norm.bias shape torch.Size([320])
decoder.stages.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.stages.0.convs.1.all_modules.0.bias shape torch.Size([320])
decoder.stages.0.convs.1.all_modules.1.weight shape torch.Size([320])
decoder.stages.0.convs.1.all_modules.1.bias shape torch.Size([320])
decoder.stages.1.convs.0.conv.weight shape torch.Size([256, 512, 3, 3, 3])
decoder.stages.1.convs.0.conv.bias shape torch.Size([256])
decoder.stages.1.convs.0.norm.weight shape torch.Size([256])
decoder.stages.1.convs.0.norm.bias shape torch.Size([256])
decoder.stages.1.convs.0.all_modules.0.weight shape torch.Size([256, 512, 3, 3, 3])
decoder.stages.1.convs.0.all_modules.0.bias shape torch.Size([256])
decoder.stages.1.convs.0.all_modules.1.weight shape torch.Size([256])
decoder.stages.1.convs.0.all_modules.1.bias shape torch.Size([256])
decoder.stages.1.convs.1.conv.weight shape torch.Size([256, 256, 3, 3, 3])
decoder.stages.1.convs.1.conv.bias shape torch.Size([256])
decoder.stages.1.convs.1.norm.weight shape torch.Size([256])
decoder.stages.1.convs.1.norm.bias shape torch.Size([256])
decoder.stages.1.convs.1.all_modules.0.weight shape torch.Size([256, 256, 3, 3, 3])
decoder.stages.1.convs.1.all_modules.0.bias shape torch.Size([256])
decoder.stages.1.convs.1.all_modules.1.weight shape torch.Size([256])
decoder.stages.1.convs.1.all_modules.1.bias shape torch.Size([256])
decoder.stages.2.convs.0.conv.weight shape torch.Size([128, 256, 3, 3, 3])
decoder.stages.2.convs.0.conv.bias shape torch.Size([128])
decoder.stages.2.convs.0.norm.weight shape torch.Size([128])
decoder.stages.2.convs.0.norm.bias shape torch.Size([128])
decoder.stages.2.convs.0.all_modules.0.weight shape torch.Size([128, 256, 3, 3, 3])
decoder.stages.2.convs.0.all_modules.0.bias shape torch.Size([128])
decoder.stages.2.convs.0.all_modules.1.weight shape torch.Size([128])
decoder.stages.2.convs.0.all_modules.1.bias shape torch.Size([128])
decoder.stages.2.convs.1.conv.weight shape torch.Size([128, 128, 3, 3, 3])
decoder.stages.2.convs.1.conv.bias shape torch.Size([128])
decoder.stages.2.convs.1.norm.weight shape torch.Size([128])
decoder.stages.2.convs.1.norm.bias shape torch.Size([128])
decoder.stages.2.convs.1.all_modules.0.weight shape torch.Size([128, 128, 3, 3, 3])
decoder.stages.2.convs.1.all_modules.0.bias shape torch.Size([128])
decoder.stages.2.convs.1.all_modules.1.weight shape torch.Size([128])
decoder.stages.2.convs.1.all_modules.1.bias shape torch.Size([128])
decoder.stages.3.convs.0.conv.weight shape torch.Size([64, 128, 3, 3, 3])
decoder.stages.3.convs.0.conv.bias shape torch.Size([64])
decoder.stages.3.convs.0.norm.weight shape torch.Size([64])
decoder.stages.3.convs.0.norm.bias shape torch.Size([64])
decoder.stages.3.convs.0.all_modules.0.weight shape torch.Size([64, 128, 3, 3, 3])
decoder.stages.3.convs.0.all_modules.0.bias shape torch.Size([64])
decoder.stages.3.convs.0.all_modules.1.weight shape torch.Size([64])
decoder.stages.3.convs.0.all_modules.1.bias shape torch.Size([64])
decoder.stages.3.convs.1.conv.weight shape torch.Size([64, 64, 3, 3, 3])
decoder.stages.3.convs.1.conv.bias shape torch.Size([64])
decoder.stages.3.convs.1.norm.weight shape torch.Size([64])
decoder.stages.3.convs.1.norm.bias shape torch.Size([64])
decoder.stages.3.convs.1.all_modules.0.weight shape torch.Size([64, 64, 3, 3, 3])
decoder.stages.3.convs.1.all_modules.0.bias shape torch.Size([64])
decoder.stages.3.convs.1.all_modules.1.weight shape torch.Size([64])
decoder.stages.3.convs.1.all_modules.1.bias shape torch.Size([64])
decoder.stages.4.convs.0.conv.weight shape torch.Size([32, 64, 3, 3, 3])
decoder.stages.4.convs.0.conv.bias shape torch.Size([32])
decoder.stages.4.convs.0.norm.weight shape torch.Size([32])
decoder.stages.4.convs.0.norm.bias shape torch.Size([32])
decoder.stages.4.convs.0.all_modules.0.weight shape torch.Size([32, 64, 3, 3, 3])
decoder.stages.4.convs.0.all_modules.0.bias shape torch.Size([32])
decoder.stages.4.convs.0.all_modules.1.weight shape torch.Size([32])
decoder.stages.4.convs.0.all_modules.1.bias shape torch.Size([32])
decoder.stages.4.convs.1.conv.weight shape torch.Size([32, 32, 3, 3, 3])
decoder.stages.4.convs.1.conv.bias shape torch.Size([32])
decoder.stages.4.convs.1.norm.weight shape torch.Size([32])
decoder.stages.4.convs.1.norm.bias shape torch.Size([32])
decoder.stages.4.convs.1.all_modules.0.weight shape torch.Size([32, 32, 3, 3, 3])
decoder.stages.4.convs.1.all_modules.0.bias shape torch.Size([32])
decoder.stages.4.convs.1.all_modules.1.weight shape torch.Size([32])
decoder.stages.4.convs.1.all_modules.1.bias shape torch.Size([32])
decoder.transpconvs.0.weight shape torch.Size([320, 320, 1, 2, 2])
decoder.transpconvs.0.bias shape torch.Size([320])
decoder.transpconvs.1.weight shape torch.Size([320, 256, 2, 2, 2])
decoder.transpconvs.1.bias shape torch.Size([256])
decoder.transpconvs.2.weight shape torch.Size([256, 128, 2, 2, 2])
decoder.transpconvs.2.bias shape torch.Size([128])
decoder.transpconvs.3.weight shape torch.Size([128, 64, 2, 2, 2])
decoder.transpconvs.3.bias shape torch.Size([64])
decoder.transpconvs.4.weight shape torch.Size([64, 32, 2, 2, 2])
decoder.transpconvs.4.bias shape torch.Size([32])
################### Done ###################
2024-05-03 18:25:05.745046: do_dummy_2d_data_aug: False
2024-05-03 18:25:05.774626: Using splits from existing split file: /home/gridsan/nchutisilp/datasets/nnUNet_Datasets/nnUNet_preprocessed/Dataset302_Calcium_OCTv2/splits_final.json
2024-05-03 18:25:05.791880: The split file contains 3 splits.
2024-05-03 18:25:05.793838: Desired fold for training: 2
2024-05-03 18:25:05.795305: This split has 4 training and 2 validation cases.
using pin_memory on device 0
using pin_memory on device 0

This is the configuration used by this training:
Configuration name: 3d_fullres
 {'data_identifier': 'nnUNetPlans_3d_fullres', 'preprocessor_name': 'DefaultPreprocessor', 'batch_size': 2, 'patch_size': [112, 160, 128], 'median_image_size_in_voxels': [375.0, 498.0, 498.0], 'spacing': [1.0, 1.0, 1.0], 'normalization_schemes': ['ZScoreNormalization'], 'use_mask_for_norm': [False], 'resampling_fn_data': 'resample_data_or_seg_to_shape', 'resampling_fn_seg': 'resample_data_or_seg_to_shape', 'resampling_fn_data_kwargs': {'is_seg': False, 'order': 3, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_seg_kwargs': {'is_seg': True, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_probabilities': 'resample_data_or_seg_to_shape', 'resampling_fn_probabilities_kwargs': {'is_seg': False, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'architecture': {'network_class_name': 'dynamic_network_architectures.architectures.unet.PlainConvUNet', 'arch_kwargs': {'n_stages': 6, 'features_per_stage': [32, 64, 128, 256, 320, 320], 'conv_op': 'torch.nn.modules.conv.Conv3d', 'kernel_sizes': [[3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3]], 'strides': [[1, 1, 1], [2, 2, 2], [2, 2, 2], [2, 2, 2], [2, 2, 2], [1, 2, 2]], 'n_conv_per_stage': [2, 2, 2, 2, 2, 2], 'n_conv_per_stage_decoder': [2, 2, 2, 2, 2], 'conv_bias': True, 'norm_op': 'torch.nn.modules.instancenorm.InstanceNorm3d', 'norm_op_kwargs': {'eps': 1e-05, 'affine': True}, 'dropout_op': None, 'dropout_op_kwargs': None, 'nonlin': 'torch.nn.LeakyReLU', 'nonlin_kwargs': {'inplace': True}, 'deep_supervision': True}, '_kw_requires_import': ['conv_op', 'norm_op', 'dropout_op', 'nonlin']}, 'batch_dice': True} 

These are the global plan.json settings:
 {'dataset_name': 'Dataset302_Calcium_OCTv2', 'plans_name': 'nnUNetPlans', 'original_median_spacing_after_transp': [1.0, 1.0, 1.0], 'original_median_shape_after_transp': [375, 498, 498], 'image_reader_writer': 'SimpleITKIO', 'transpose_forward': [0, 1, 2], 'transpose_backward': [0, 1, 2], 'experiment_planner_used': 'ExperimentPlanner', 'label_manager': 'LabelManager', 'foreground_intensity_properties_per_channel': {'0': {'max': 0.4977976381778717, 'mean': 0.13332499563694, 'median': 0.08871842920780182, 'min': 0.0, 'percentile_00_5': 0.014754901640117168, 'percentile_99_5': 0.4977976381778717, 'std': 0.12022686749696732}}} 

2024-05-03 18:25:15.312426: unpacking dataset...
2024-05-03 18:25:21.404369: unpacking done...
2024-05-03 18:25:21.558820: Unable to plot network architecture: nnUNet_compile is enabled!
2024-05-03 18:25:22.025269: 
2024-05-03 18:25:22.027792: Epoch 0
2024-05-03 18:25:22.029217: Current learning rate: 0.01
2024-05-03 18:28:51.608897: Validation loss improved from 1000.00000 to -0.36193! Patience: 0/50
2024-05-03 18:28:51.611199: train_loss -0.3011
2024-05-03 18:28:51.613976: val_loss -0.3619
2024-05-03 18:28:51.615686: Pseudo dice [0.6711]
2024-05-03 18:28:51.616961: Epoch time: 209.59 s
2024-05-03 18:28:51.618228: Yayy! New best EMA pseudo Dice: 0.6711
2024-05-03 18:28:53.082605: 
2024-05-03 18:28:53.085034: Epoch 1
2024-05-03 18:28:53.086811: Current learning rate: 0.00999
2024-05-03 18:29:57.677659: Validation loss improved from -0.36193 to -0.38060! Patience: 0/50
2024-05-03 18:29:57.679077: train_loss -0.4169
2024-05-03 18:29:57.680417: val_loss -0.3806
2024-05-03 18:29:57.681637: Pseudo dice [0.6622]
2024-05-03 18:29:57.682761: Epoch time: 64.6 s
2024-05-03 18:29:58.903574: 
2024-05-03 18:29:58.905909: Epoch 2
2024-05-03 18:29:58.907150: Current learning rate: 0.00998
2024-05-03 18:31:03.648674: Validation loss did not improve from -0.38060. Patience: 1/50
2024-05-03 18:31:03.650151: train_loss -0.4646
2024-05-03 18:31:03.651701: val_loss -0.379
2024-05-03 18:31:03.652834: Pseudo dice [0.6601]
2024-05-03 18:31:03.654108: Epoch time: 64.75 s
2024-05-03 18:31:04.939820: 
2024-05-03 18:31:04.942640: Epoch 3
2024-05-03 18:31:04.944395: Current learning rate: 0.00997
2024-05-03 18:32:09.573188: Validation loss improved from -0.38060 to -0.42770! Patience: 1/50
2024-05-03 18:32:09.574541: train_loss -0.496
2024-05-03 18:32:09.576072: val_loss -0.4277
2024-05-03 18:32:09.577314: Pseudo dice [0.6805]
2024-05-03 18:32:09.578347: Epoch time: 64.64 s
2024-05-03 18:32:10.853325: 
2024-05-03 18:32:10.855240: Epoch 4
2024-05-03 18:32:10.856295: Current learning rate: 0.00996
2024-05-03 18:33:15.551509: Validation loss did not improve from -0.42770. Patience: 1/50
2024-05-03 18:33:15.552990: train_loss -0.5149
2024-05-03 18:33:15.554282: val_loss -0.4162
2024-05-03 18:33:15.555278: Pseudo dice [0.6954]
2024-05-03 18:33:15.556309: Epoch time: 64.7 s
2024-05-03 18:33:15.853250: Yayy! New best EMA pseudo Dice: 0.6728
2024-05-03 18:33:17.385068: 
2024-05-03 18:33:17.387275: Epoch 5
2024-05-03 18:33:17.389199: Current learning rate: 0.00995
2024-05-03 18:34:22.067278: Validation loss did not improve from -0.42770. Patience: 2/50
2024-05-03 18:34:22.068689: train_loss -0.5251
2024-05-03 18:34:22.070017: val_loss -0.3639
2024-05-03 18:34:22.070976: Pseudo dice [0.6466]
2024-05-03 18:34:22.071971: Epoch time: 64.68 s
2024-05-03 18:34:23.272813: 
2024-05-03 18:34:23.274385: Epoch 6
2024-05-03 18:34:23.275218: Current learning rate: 0.00995
2024-05-03 18:35:27.923918: Validation loss improved from -0.42770 to -0.43787! Patience: 2/50
2024-05-03 18:35:27.925608: train_loss -0.5479
2024-05-03 18:35:27.927051: val_loss -0.4379
2024-05-03 18:35:27.928379: Pseudo dice [0.6921]
2024-05-03 18:35:27.929284: Epoch time: 64.65 s
2024-05-03 18:35:29.584658: 
2024-05-03 18:35:29.586539: Epoch 7
2024-05-03 18:35:29.587552: Current learning rate: 0.00994
2024-05-03 18:36:34.289704: Validation loss did not improve from -0.43787. Patience: 1/50
2024-05-03 18:36:34.291123: train_loss -0.5478
2024-05-03 18:36:34.292286: val_loss -0.3897
2024-05-03 18:36:34.293314: Pseudo dice [0.6616]
2024-05-03 18:36:34.294242: Epoch time: 64.71 s
2024-05-03 18:36:35.546036: 
2024-05-03 18:36:35.548486: Epoch 8
2024-05-03 18:36:35.550219: Current learning rate: 0.00993
2024-05-03 18:37:40.411186: Validation loss did not improve from -0.43787. Patience: 2/50
2024-05-03 18:37:40.412800: train_loss -0.5599
2024-05-03 18:37:40.414311: val_loss -0.4165
2024-05-03 18:37:40.415399: Pseudo dice [0.6931]
2024-05-03 18:37:40.416434: Epoch time: 64.87 s
2024-05-03 18:37:40.417660: Yayy! New best EMA pseudo Dice: 0.6735
2024-05-03 18:37:42.017711: 
2024-05-03 18:37:42.020302: Epoch 9
2024-05-03 18:37:42.021726: Current learning rate: 0.00992
2024-05-03 18:38:46.869408: Validation loss did not improve from -0.43787. Patience: 3/50
2024-05-03 18:38:46.870883: train_loss -0.5662
2024-05-03 18:38:46.872262: val_loss -0.4155
2024-05-03 18:38:46.873341: Pseudo dice [0.6664]
2024-05-03 18:38:46.874437: Epoch time: 64.85 s
2024-05-03 18:38:48.415022: 
2024-05-03 18:38:48.417001: Epoch 10
2024-05-03 18:38:48.418586: Current learning rate: 0.00991
2024-05-03 18:39:53.160979: Validation loss improved from -0.43787 to -0.46689! Patience: 3/50
2024-05-03 18:39:53.162506: train_loss -0.5709
2024-05-03 18:39:53.163788: val_loss -0.4669
2024-05-03 18:39:53.164812: Pseudo dice [0.7022]
2024-05-03 18:39:53.165972: Epoch time: 64.75 s
2024-05-03 18:39:53.166961: Yayy! New best EMA pseudo Dice: 0.6757
2024-05-03 18:39:54.725435: 
2024-05-03 18:39:54.727076: Epoch 11
2024-05-03 18:39:54.728083: Current learning rate: 0.0099
2024-05-03 18:40:59.507254: Validation loss did not improve from -0.46689. Patience: 1/50
2024-05-03 18:40:59.508582: train_loss -0.5945
2024-05-03 18:40:59.509989: val_loss -0.4435
2024-05-03 18:40:59.511058: Pseudo dice [0.7001]
2024-05-03 18:40:59.512158: Epoch time: 64.78 s
2024-05-03 18:40:59.513073: Yayy! New best EMA pseudo Dice: 0.6782
2024-05-03 18:41:01.062871: 
2024-05-03 18:41:01.064816: Epoch 12
2024-05-03 18:41:01.066163: Current learning rate: 0.00989
2024-05-03 18:42:05.893202: Validation loss did not improve from -0.46689. Patience: 2/50
2024-05-03 18:42:05.894598: train_loss -0.5988
2024-05-03 18:42:05.895762: val_loss -0.3674
2024-05-03 18:42:05.896698: Pseudo dice [0.6579]
2024-05-03 18:42:05.897702: Epoch time: 64.83 s
2024-05-03 18:42:07.153096: 
2024-05-03 18:42:07.154930: Epoch 13
2024-05-03 18:42:07.156232: Current learning rate: 0.00988
2024-05-03 18:43:11.937846: Validation loss did not improve from -0.46689. Patience: 3/50
2024-05-03 18:43:11.939267: train_loss -0.5908
2024-05-03 18:43:11.940679: val_loss -0.4467
2024-05-03 18:43:11.941733: Pseudo dice [0.7036]
2024-05-03 18:43:11.942739: Epoch time: 64.79 s
2024-05-03 18:43:11.943687: Yayy! New best EMA pseudo Dice: 0.6789
2024-05-03 18:43:13.573757: 
2024-05-03 18:43:13.576284: Epoch 14
2024-05-03 18:43:13.578198: Current learning rate: 0.00987
2024-05-03 18:44:18.531344: Validation loss did not improve from -0.46689. Patience: 4/50
2024-05-03 18:44:18.532714: train_loss -0.6017
2024-05-03 18:44:18.534073: val_loss -0.4478
2024-05-03 18:44:18.535133: Pseudo dice [0.7026]
2024-05-03 18:44:18.536156: Epoch time: 64.96 s
2024-05-03 18:44:18.890146: Yayy! New best EMA pseudo Dice: 0.6812
2024-05-03 18:44:20.489585: 
2024-05-03 18:44:20.491821: Epoch 15
2024-05-03 18:44:20.493367: Current learning rate: 0.00986
2024-05-03 18:45:25.237063: Validation loss did not improve from -0.46689. Patience: 5/50
2024-05-03 18:45:25.238688: train_loss -0.6232
2024-05-03 18:45:25.240584: val_loss -0.4517
2024-05-03 18:45:25.242204: Pseudo dice [0.7064]
2024-05-03 18:45:25.243808: Epoch time: 64.75 s
2024-05-03 18:45:25.245492: Yayy! New best EMA pseudo Dice: 0.6838
2024-05-03 18:45:26.834500: 
2024-05-03 18:45:26.836586: Epoch 16
2024-05-03 18:45:26.838123: Current learning rate: 0.00986
2024-05-03 18:46:31.781785: Validation loss did not improve from -0.46689. Patience: 6/50
2024-05-03 18:46:31.783607: train_loss -0.6278
2024-05-03 18:46:31.785472: val_loss -0.4507
2024-05-03 18:46:31.786571: Pseudo dice [0.7153]
2024-05-03 18:46:31.787803: Epoch time: 64.95 s
2024-05-03 18:46:31.789002: Yayy! New best EMA pseudo Dice: 0.6869
2024-05-03 18:46:33.424531: 
2024-05-03 18:46:33.426672: Epoch 17
2024-05-03 18:46:33.427966: Current learning rate: 0.00985
2024-05-03 18:47:38.384494: Validation loss improved from -0.46689 to -0.47211! Patience: 6/50
2024-05-03 18:47:38.386019: train_loss -0.6275
2024-05-03 18:47:38.387373: val_loss -0.4721
2024-05-03 18:47:38.388613: Pseudo dice [0.7083]
2024-05-03 18:47:38.389837: Epoch time: 64.96 s
2024-05-03 18:47:38.390831: Yayy! New best EMA pseudo Dice: 0.6891
2024-05-03 18:47:40.387539: 
2024-05-03 18:47:40.389545: Epoch 18
2024-05-03 18:47:40.390627: Current learning rate: 0.00984
2024-05-03 18:48:45.305961: Validation loss improved from -0.47211 to -0.49337! Patience: 0/50
2024-05-03 18:48:45.307493: train_loss -0.6376
2024-05-03 18:48:45.308682: val_loss -0.4934
2024-05-03 18:48:45.309776: Pseudo dice [0.7267]
2024-05-03 18:48:45.310884: Epoch time: 64.92 s
2024-05-03 18:48:45.311852: Yayy! New best EMA pseudo Dice: 0.6928
2024-05-03 18:48:47.174474: 
2024-05-03 18:48:47.176104: Epoch 19
2024-05-03 18:48:47.177754: Current learning rate: 0.00983
2024-05-03 18:49:52.168142: Validation loss did not improve from -0.49337. Patience: 1/50
2024-05-03 18:49:52.169766: train_loss -0.6363
2024-05-03 18:49:52.171176: val_loss -0.4655
2024-05-03 18:49:52.172306: Pseudo dice [0.723]
2024-05-03 18:49:52.173430: Epoch time: 65.0 s
2024-05-03 18:49:52.530682: Yayy! New best EMA pseudo Dice: 0.6958
2024-05-03 18:49:54.126383: 
2024-05-03 18:49:54.128287: Epoch 20
2024-05-03 18:49:54.129703: Current learning rate: 0.00982
2024-05-03 18:50:59.030431: Validation loss did not improve from -0.49337. Patience: 2/50
2024-05-03 18:50:59.031816: train_loss -0.6349
2024-05-03 18:50:59.032957: val_loss -0.4203
2024-05-03 18:50:59.034268: Pseudo dice [0.6915]
2024-05-03 18:50:59.035352: Epoch time: 64.91 s
2024-05-03 18:51:00.349947: 
2024-05-03 18:51:00.352057: Epoch 21
2024-05-03 18:51:00.353128: Current learning rate: 0.00981
2024-05-03 18:52:05.313459: Validation loss did not improve from -0.49337. Patience: 3/50
2024-05-03 18:52:05.315858: train_loss -0.6249
2024-05-03 18:52:05.317633: val_loss -0.4548
2024-05-03 18:52:05.318684: Pseudo dice [0.6956]
2024-05-03 18:52:05.319776: Epoch time: 64.97 s
2024-05-03 18:52:06.552067: 
2024-05-03 18:52:06.554036: Epoch 22
2024-05-03 18:52:06.555695: Current learning rate: 0.0098
2024-05-03 18:53:11.485230: Validation loss did not improve from -0.49337. Patience: 4/50
2024-05-03 18:53:11.486715: train_loss -0.6446
2024-05-03 18:53:11.487989: val_loss -0.4699
2024-05-03 18:53:11.489329: Pseudo dice [0.7151]
2024-05-03 18:53:11.490338: Epoch time: 64.94 s
2024-05-03 18:53:11.491387: Yayy! New best EMA pseudo Dice: 0.6974
2024-05-03 18:53:13.064002: 
2024-05-03 18:53:13.066167: Epoch 23
2024-05-03 18:53:13.067987: Current learning rate: 0.00979
2024-05-03 18:54:18.046561: Validation loss did not improve from -0.49337. Patience: 5/50
2024-05-03 18:54:18.048054: train_loss -0.6442
2024-05-03 18:54:18.050698: val_loss -0.4198
2024-05-03 18:54:18.052181: Pseudo dice [0.6899]
2024-05-03 18:54:18.053217: Epoch time: 64.99 s
2024-05-03 18:54:19.271894: 
2024-05-03 18:54:19.273672: Epoch 24
2024-05-03 18:54:19.274824: Current learning rate: 0.00978
2024-05-03 18:55:24.308064: Validation loss did not improve from -0.49337. Patience: 6/50
2024-05-03 18:55:24.309663: train_loss -0.6525
2024-05-03 18:55:24.310861: val_loss -0.4896
2024-05-03 18:55:24.311837: Pseudo dice [0.7276]
2024-05-03 18:55:24.313008: Epoch time: 65.04 s
2024-05-03 18:55:24.668654: Yayy! New best EMA pseudo Dice: 0.6997
2024-05-03 18:55:26.235611: 
2024-05-03 18:55:26.237041: Epoch 25
2024-05-03 18:55:26.238508: Current learning rate: 0.00977
2024-05-03 18:56:31.155048: Validation loss did not improve from -0.49337. Patience: 7/50
2024-05-03 18:56:31.156486: train_loss -0.6566
2024-05-03 18:56:31.158162: val_loss -0.458
2024-05-03 18:56:31.159330: Pseudo dice [0.7131]
2024-05-03 18:56:31.160354: Epoch time: 64.92 s
2024-05-03 18:56:31.161315: Yayy! New best EMA pseudo Dice: 0.7011
2024-05-03 18:56:32.748343: 
2024-05-03 18:56:32.750888: Epoch 26
2024-05-03 18:56:32.752458: Current learning rate: 0.00977
2024-05-03 18:57:37.720668: Validation loss did not improve from -0.49337. Patience: 8/50
2024-05-03 18:57:37.722480: train_loss -0.6522
2024-05-03 18:57:37.724861: val_loss -0.4714
2024-05-03 18:57:37.726430: Pseudo dice [0.7175]
2024-05-03 18:57:37.727719: Epoch time: 64.98 s
2024-05-03 18:57:37.729050: Yayy! New best EMA pseudo Dice: 0.7027
2024-05-03 18:57:39.283607: 
2024-05-03 18:57:39.285384: Epoch 27
2024-05-03 18:57:39.287069: Current learning rate: 0.00976
2024-05-03 18:58:44.194702: Validation loss did not improve from -0.49337. Patience: 9/50
2024-05-03 18:58:44.196149: train_loss -0.6697
2024-05-03 18:58:44.197191: val_loss -0.4632
2024-05-03 18:58:44.198174: Pseudo dice [0.7043]
2024-05-03 18:58:44.199184: Epoch time: 64.91 s
2024-05-03 18:58:44.200185: Yayy! New best EMA pseudo Dice: 0.7029
2024-05-03 18:58:45.775963: 
2024-05-03 18:58:45.778119: Epoch 28
2024-05-03 18:58:45.779985: Current learning rate: 0.00975
2024-05-03 18:59:50.662946: Validation loss did not improve from -0.49337. Patience: 10/50
2024-05-03 18:59:50.664412: train_loss -0.6656
2024-05-03 18:59:50.665893: val_loss -0.4752
2024-05-03 18:59:50.667195: Pseudo dice [0.7185]
2024-05-03 18:59:50.668183: Epoch time: 64.89 s
2024-05-03 18:59:50.669199: Yayy! New best EMA pseudo Dice: 0.7044
2024-05-03 18:59:52.626628: 
2024-05-03 18:59:52.628810: Epoch 29
2024-05-03 18:59:52.630625: Current learning rate: 0.00974
2024-05-03 19:00:57.501383: Validation loss did not improve from -0.49337. Patience: 11/50
2024-05-03 19:00:57.505859: train_loss -0.6721
2024-05-03 19:00:57.507346: val_loss -0.4587
2024-05-03 19:00:57.508623: Pseudo dice [0.7154]
2024-05-03 19:00:57.509935: Epoch time: 64.88 s
2024-05-03 19:00:57.857242: Yayy! New best EMA pseudo Dice: 0.7055
2024-05-03 19:00:59.428575: 
2024-05-03 19:00:59.430775: Epoch 30
2024-05-03 19:00:59.432197: Current learning rate: 0.00973
2024-05-03 19:02:04.249825: Validation loss improved from -0.49337 to -0.51715! Patience: 11/50
2024-05-03 19:02:04.251313: train_loss -0.6736
2024-05-03 19:02:04.252831: val_loss -0.5171
2024-05-03 19:02:04.254200: Pseudo dice [0.7387]
2024-05-03 19:02:04.255496: Epoch time: 64.82 s
2024-05-03 19:02:04.256517: Yayy! New best EMA pseudo Dice: 0.7088
2024-05-03 19:02:05.837101: 
2024-05-03 19:02:05.838671: Epoch 31
2024-05-03 19:02:05.839874: Current learning rate: 0.00972
2024-05-03 19:03:10.697349: Validation loss did not improve from -0.51715. Patience: 1/50
2024-05-03 19:03:10.699631: train_loss -0.6698
2024-05-03 19:03:10.701357: val_loss -0.4449
2024-05-03 19:03:10.702698: Pseudo dice [0.7009]
2024-05-03 19:03:10.704039: Epoch time: 64.86 s
2024-05-03 19:03:11.946561: 
2024-05-03 19:03:11.948502: Epoch 32
2024-05-03 19:03:11.949822: Current learning rate: 0.00971
2024-05-03 19:04:16.668310: Validation loss did not improve from -0.51715. Patience: 2/50
2024-05-03 19:04:16.669852: train_loss -0.6611
2024-05-03 19:04:16.671111: val_loss -0.4094
2024-05-03 19:04:16.672122: Pseudo dice [0.6803]
2024-05-03 19:04:16.673255: Epoch time: 64.72 s
2024-05-03 19:04:17.920202: 
2024-05-03 19:04:17.922068: Epoch 33
2024-05-03 19:04:17.923570: Current learning rate: 0.0097
2024-05-03 19:05:22.902463: Validation loss did not improve from -0.51715. Patience: 3/50
2024-05-03 19:05:22.903831: train_loss -0.6739
2024-05-03 19:05:22.905043: val_loss -0.418
2024-05-03 19:05:22.906163: Pseudo dice [0.6929]
2024-05-03 19:05:22.907185: Epoch time: 64.98 s
2024-05-03 19:05:24.239089: 
2024-05-03 19:05:24.240623: Epoch 34
2024-05-03 19:05:24.241628: Current learning rate: 0.00969
2024-05-03 19:06:28.962819: Validation loss did not improve from -0.51715. Patience: 4/50
2024-05-03 19:06:28.964299: train_loss -0.6775
2024-05-03 19:06:28.965564: val_loss -0.4144
2024-05-03 19:06:28.966799: Pseudo dice [0.6914]
2024-05-03 19:06:28.967711: Epoch time: 64.73 s
2024-05-03 19:06:30.651461: 
2024-05-03 19:06:30.653158: Epoch 35
2024-05-03 19:06:30.654221: Current learning rate: 0.00968
2024-05-03 19:07:35.343369: Validation loss did not improve from -0.51715. Patience: 5/50
2024-05-03 19:07:35.344550: train_loss -0.6761
2024-05-03 19:07:35.346254: val_loss -0.393
2024-05-03 19:07:35.347151: Pseudo dice [0.6772]
2024-05-03 19:07:35.348363: Epoch time: 64.69 s
2024-05-03 19:07:36.627464: 
2024-05-03 19:07:36.629203: Epoch 36
2024-05-03 19:07:36.630392: Current learning rate: 0.00968
2024-05-03 19:08:41.384414: Validation loss did not improve from -0.51715. Patience: 6/50
2024-05-03 19:08:41.385763: train_loss -0.6782
2024-05-03 19:08:41.387107: val_loss -0.37
2024-05-03 19:08:41.388430: Pseudo dice [0.6793]
2024-05-03 19:08:41.389715: Epoch time: 64.76 s
2024-05-03 19:08:42.661733: 
2024-05-03 19:08:42.663416: Epoch 37
2024-05-03 19:08:42.665173: Current learning rate: 0.00967
2024-05-03 19:09:47.397889: Validation loss did not improve from -0.51715. Patience: 7/50
2024-05-03 19:09:47.399458: train_loss -0.6829
2024-05-03 19:09:47.400841: val_loss -0.4717
2024-05-03 19:09:47.401840: Pseudo dice [0.7164]
2024-05-03 19:09:47.402875: Epoch time: 64.74 s
2024-05-03 19:09:48.676827: 
2024-05-03 19:09:48.679084: Epoch 38
2024-05-03 19:09:48.680292: Current learning rate: 0.00966
2024-05-03 19:10:53.434428: Validation loss did not improve from -0.51715. Patience: 8/50
2024-05-03 19:10:53.436094: train_loss -0.6766
2024-05-03 19:10:53.437121: val_loss -0.476
2024-05-03 19:10:53.438225: Pseudo dice [0.7137]
2024-05-03 19:10:53.439216: Epoch time: 64.76 s
2024-05-03 19:10:54.711145: 
2024-05-03 19:10:54.713239: Epoch 39
2024-05-03 19:10:54.714239: Current learning rate: 0.00965
2024-05-03 19:11:59.674038: Validation loss did not improve from -0.51715. Patience: 9/50
2024-05-03 19:11:59.675690: train_loss -0.6814
2024-05-03 19:11:59.676938: val_loss -0.4663
2024-05-03 19:11:59.678137: Pseudo dice [0.7155]
2024-05-03 19:11:59.679308: Epoch time: 64.97 s
2024-05-03 19:12:02.102578: 
2024-05-03 19:12:02.105023: Epoch 40
2024-05-03 19:12:02.106317: Current learning rate: 0.00964
2024-05-03 19:13:07.009661: Validation loss did not improve from -0.51715. Patience: 10/50
2024-05-03 19:13:07.011156: train_loss -0.6792
2024-05-03 19:13:07.012106: val_loss -0.4113
2024-05-03 19:13:07.013125: Pseudo dice [0.6861]
2024-05-03 19:13:07.014012: Epoch time: 64.91 s
2024-05-03 19:13:08.317333: 
2024-05-03 19:13:08.318759: Epoch 41
2024-05-03 19:13:08.319971: Current learning rate: 0.00963
2024-05-03 19:14:13.184410: Validation loss did not improve from -0.51715. Patience: 11/50
2024-05-03 19:14:13.185659: train_loss -0.6869
2024-05-03 19:14:13.186657: val_loss -0.3722
2024-05-03 19:14:13.187691: Pseudo dice [0.6581]
2024-05-03 19:14:13.188735: Epoch time: 64.87 s
2024-05-03 19:14:14.409276: 
2024-05-03 19:14:14.412115: Epoch 42
2024-05-03 19:14:14.413939: Current learning rate: 0.00962
2024-05-03 19:15:19.379981: Validation loss did not improve from -0.51715. Patience: 12/50
2024-05-03 19:15:19.381618: train_loss -0.6859
2024-05-03 19:15:19.382642: val_loss -0.3943
2024-05-03 19:15:19.383550: Pseudo dice [0.6874]
2024-05-03 19:15:19.384519: Epoch time: 64.97 s
2024-05-03 19:15:20.603353: 
2024-05-03 19:15:20.605134: Epoch 43
2024-05-03 19:15:20.606123: Current learning rate: 0.00961
2024-05-03 19:16:25.484115: Validation loss did not improve from -0.51715. Patience: 13/50
2024-05-03 19:16:25.485620: train_loss -0.6786
2024-05-03 19:16:25.486784: val_loss -0.4467
2024-05-03 19:16:25.487899: Pseudo dice [0.6985]
2024-05-03 19:16:25.488936: Epoch time: 64.88 s
2024-05-03 19:16:26.703583: 
2024-05-03 19:16:26.705667: Epoch 44
2024-05-03 19:16:26.706654: Current learning rate: 0.0096
2024-05-03 19:17:31.583380: Validation loss did not improve from -0.51715. Patience: 14/50
2024-05-03 19:17:31.584831: train_loss -0.6891
2024-05-03 19:17:31.586265: val_loss -0.4292
2024-05-03 19:17:31.587617: Pseudo dice [0.7021]
2024-05-03 19:17:31.588823: Epoch time: 64.88 s
2024-05-03 19:17:33.162392: 
2024-05-03 19:17:33.164667: Epoch 45
2024-05-03 19:17:33.166132: Current learning rate: 0.00959
2024-05-03 19:18:38.018637: Validation loss did not improve from -0.51715. Patience: 15/50
2024-05-03 19:18:38.020005: train_loss -0.7019
2024-05-03 19:18:38.021259: val_loss -0.4845
2024-05-03 19:18:38.022269: Pseudo dice [0.7305]
2024-05-03 19:18:38.023602: Epoch time: 64.86 s
2024-05-03 19:18:39.269589: 
2024-05-03 19:18:39.272155: Epoch 46
2024-05-03 19:18:39.273327: Current learning rate: 0.00959
2024-05-03 19:19:44.192819: Validation loss did not improve from -0.51715. Patience: 16/50
2024-05-03 19:19:44.194287: train_loss -0.6924
2024-05-03 19:19:44.195805: val_loss -0.4697
2024-05-03 19:19:44.196976: Pseudo dice [0.7261]
2024-05-03 19:19:44.198131: Epoch time: 64.93 s
2024-05-03 19:19:45.429960: 
2024-05-03 19:19:45.431951: Epoch 47
2024-05-03 19:19:45.433693: Current learning rate: 0.00958
2024-05-03 19:20:50.392680: Validation loss did not improve from -0.51715. Patience: 17/50
2024-05-03 19:20:50.394080: train_loss -0.7074
2024-05-03 19:20:50.395335: val_loss -0.4736
2024-05-03 19:20:50.396465: Pseudo dice [0.7239]
2024-05-03 19:20:50.397529: Epoch time: 64.97 s
2024-05-03 19:20:51.609878: 
2024-05-03 19:20:51.611862: Epoch 48
2024-05-03 19:20:51.613125: Current learning rate: 0.00957
2024-05-03 19:21:56.644191: Validation loss did not improve from -0.51715. Patience: 18/50
2024-05-03 19:21:56.645729: train_loss -0.7065
2024-05-03 19:21:56.647132: val_loss -0.4589
2024-05-03 19:21:56.648394: Pseudo dice [0.7161]
2024-05-03 19:21:56.649459: Epoch time: 65.04 s
2024-05-03 19:21:57.865679: 
2024-05-03 19:21:57.868021: Epoch 49
2024-05-03 19:21:57.869469: Current learning rate: 0.00956
2024-05-03 19:23:02.916207: Validation loss did not improve from -0.51715. Patience: 19/50
2024-05-03 19:23:02.917969: train_loss -0.7122
2024-05-03 19:23:02.919623: val_loss -0.4688
2024-05-03 19:23:02.920941: Pseudo dice [0.721]
2024-05-03 19:23:02.922078: Epoch time: 65.05 s
2024-05-03 19:23:04.483235: 
2024-05-03 19:23:04.485134: Epoch 50
2024-05-03 19:23:04.486630: Current learning rate: 0.00955
2024-05-03 19:24:09.574916: Validation loss did not improve from -0.51715. Patience: 20/50
2024-05-03 19:24:09.576411: train_loss -0.7123
2024-05-03 19:24:09.577564: val_loss -0.4838
2024-05-03 19:24:09.578782: Pseudo dice [0.7235]
2024-05-03 19:24:09.579686: Epoch time: 65.09 s
2024-05-03 19:24:09.580537: Yayy! New best EMA pseudo Dice: 0.7091
2024-05-03 19:24:11.962686: 
2024-05-03 19:24:11.964826: Epoch 51
2024-05-03 19:24:11.966503: Current learning rate: 0.00954
2024-05-03 19:25:17.798926: Validation loss did not improve from -0.51715. Patience: 21/50
2024-05-03 19:25:17.800264: train_loss -0.7047
2024-05-03 19:25:17.801874: val_loss -0.4309
2024-05-03 19:25:17.802984: Pseudo dice [0.6916]
2024-05-03 19:25:17.803995: Epoch time: 65.84 s
2024-05-03 19:25:19.567931: 
2024-05-03 19:25:19.569787: Epoch 52
2024-05-03 19:25:19.571535: Current learning rate: 0.00953
2024-05-03 19:26:24.758418: Validation loss did not improve from -0.51715. Patience: 22/50
2024-05-03 19:26:24.760062: train_loss -0.7109
2024-05-03 19:26:24.761493: val_loss -0.4668
2024-05-03 19:26:24.762452: Pseudo dice [0.718]
2024-05-03 19:26:24.763538: Epoch time: 65.19 s
2024-05-03 19:26:25.971991: 
2024-05-03 19:26:25.974346: Epoch 53
2024-05-03 19:26:25.975352: Current learning rate: 0.00952
2024-05-03 19:27:31.040120: Validation loss did not improve from -0.51715. Patience: 23/50
2024-05-03 19:27:31.041553: train_loss -0.7137
2024-05-03 19:27:31.042783: val_loss -0.4086
2024-05-03 19:27:31.043696: Pseudo dice [0.697]
2024-05-03 19:27:31.044553: Epoch time: 65.07 s
2024-05-03 19:27:32.256465: 
2024-05-03 19:27:32.258525: Epoch 54
2024-05-03 19:27:32.259935: Current learning rate: 0.00951
2024-05-03 19:28:37.338640: Validation loss did not improve from -0.51715. Patience: 24/50
2024-05-03 19:28:37.340131: train_loss -0.7097
2024-05-03 19:28:37.341394: val_loss -0.4976
2024-05-03 19:28:37.342313: Pseudo dice [0.7221]
2024-05-03 19:28:37.343344: Epoch time: 65.09 s
2024-05-03 19:28:38.941847: 
2024-05-03 19:28:38.944123: Epoch 55
2024-05-03 19:28:38.945608: Current learning rate: 0.0095
2024-05-03 19:29:44.278914: Validation loss did not improve from -0.51715. Patience: 25/50
2024-05-03 19:29:44.280684: train_loss -0.7135
2024-05-03 19:29:44.281934: val_loss -0.4568
2024-05-03 19:29:44.283165: Pseudo dice [0.7217]
2024-05-03 19:29:44.284619: Epoch time: 65.34 s
2024-05-03 19:29:44.285949: Yayy! New best EMA pseudo Dice: 0.71
2024-05-03 19:29:46.002249: 
2024-05-03 19:29:46.004230: Epoch 56
2024-05-03 19:29:46.005714: Current learning rate: 0.00949
2024-05-03 19:30:51.511074: Validation loss did not improve from -0.51715. Patience: 26/50
2024-05-03 19:30:51.512301: train_loss -0.7025
2024-05-03 19:30:51.513607: val_loss -0.4779
2024-05-03 19:30:51.514852: Pseudo dice [0.7197]
2024-05-03 19:30:51.516001: Epoch time: 65.51 s
2024-05-03 19:30:51.516798: Yayy! New best EMA pseudo Dice: 0.711
2024-05-03 19:30:53.098684: 
2024-05-03 19:30:53.100979: Epoch 57
2024-05-03 19:30:53.101887: Current learning rate: 0.00949
2024-05-03 19:31:59.087716: Validation loss did not improve from -0.51715. Patience: 27/50
2024-05-03 19:31:59.104261: train_loss -0.7127
2024-05-03 19:31:59.105789: val_loss -0.3784
2024-05-03 19:31:59.107033: Pseudo dice [0.6745]
2024-05-03 19:31:59.108223: Epoch time: 65.99 s
2024-05-03 19:32:01.585693: 
2024-05-03 19:32:01.588078: Epoch 58
2024-05-03 19:32:01.589015: Current learning rate: 0.00948
2024-05-03 19:33:06.757783: Validation loss did not improve from -0.51715. Patience: 28/50
2024-05-03 19:33:06.759386: train_loss -0.7077
2024-05-03 19:33:06.760511: val_loss -0.3972
2024-05-03 19:33:06.761528: Pseudo dice [0.7028]
2024-05-03 19:33:06.762374: Epoch time: 65.17 s
2024-05-03 19:33:08.012270: 
2024-05-03 19:33:08.014530: Epoch 59
2024-05-03 19:33:08.015817: Current learning rate: 0.00947
2024-05-03 19:34:13.763489: Validation loss did not improve from -0.51715. Patience: 29/50
2024-05-03 19:34:13.765957: train_loss -0.7055
2024-05-03 19:34:13.786688: val_loss -0.483
2024-05-03 19:34:13.787905: Pseudo dice [0.7299]
2024-05-03 19:34:13.789138: Epoch time: 65.75 s
2024-05-03 19:34:15.775993: 
2024-05-03 19:34:15.778080: Epoch 60
2024-05-03 19:34:15.779184: Current learning rate: 0.00946
2024-05-03 19:35:20.615760: Validation loss did not improve from -0.51715. Patience: 30/50
2024-05-03 19:35:20.617010: train_loss -0.7179
2024-05-03 19:35:20.618367: val_loss -0.3713
2024-05-03 19:35:20.619525: Pseudo dice [0.6693]
2024-05-03 19:35:20.621635: Epoch time: 64.84 s
2024-05-03 19:35:21.866744: 
2024-05-03 19:35:21.868953: Epoch 61
2024-05-03 19:35:21.870249: Current learning rate: 0.00945
2024-05-03 19:36:26.791040: Validation loss did not improve from -0.51715. Patience: 31/50
2024-05-03 19:36:26.792325: train_loss -0.7162
2024-05-03 19:36:26.793375: val_loss -0.4723
2024-05-03 19:36:26.794482: Pseudo dice [0.7179]
2024-05-03 19:36:26.795498: Epoch time: 64.93 s
2024-05-03 19:36:28.038939: 
2024-05-03 19:36:28.040612: Epoch 62
2024-05-03 19:36:28.041605: Current learning rate: 0.00944
2024-05-03 19:37:32.938226: Validation loss did not improve from -0.51715. Patience: 32/50
2024-05-03 19:37:32.939616: train_loss -0.7296
2024-05-03 19:37:32.940702: val_loss -0.456
2024-05-03 19:37:32.941674: Pseudo dice [0.7192]
2024-05-03 19:37:32.942639: Epoch time: 64.9 s
2024-05-03 19:37:34.194567: 
2024-05-03 19:37:34.196421: Epoch 63
2024-05-03 19:37:34.198039: Current learning rate: 0.00943
2024-05-03 19:38:39.113866: Validation loss did not improve from -0.51715. Patience: 33/50
2024-05-03 19:38:39.115278: train_loss -0.7201
2024-05-03 19:38:39.116440: val_loss -0.4868
2024-05-03 19:38:39.117569: Pseudo dice [0.7342]
2024-05-03 19:38:39.118718: Epoch time: 64.92 s
2024-05-03 19:38:40.971664: 
2024-05-03 19:38:40.973872: Epoch 64
2024-05-03 19:38:40.975043: Current learning rate: 0.00942
2024-05-03 19:39:45.917877: Validation loss did not improve from -0.51715. Patience: 34/50
2024-05-03 19:39:45.919360: train_loss -0.712
2024-05-03 19:39:45.920355: val_loss -0.4552
2024-05-03 19:39:45.921327: Pseudo dice [0.7208]
2024-05-03 19:39:45.922328: Epoch time: 64.95 s
2024-05-03 19:39:46.274222: Yayy! New best EMA pseudo Dice: 0.7114
2024-05-03 19:39:47.855490: 
2024-05-03 19:39:47.857442: Epoch 65
2024-05-03 19:39:47.858342: Current learning rate: 0.00941
2024-05-03 19:40:52.752731: Validation loss did not improve from -0.51715. Patience: 35/50
2024-05-03 19:40:52.754199: train_loss -0.7198
2024-05-03 19:40:52.755595: val_loss -0.4439
2024-05-03 19:40:52.757032: Pseudo dice [0.6863]
2024-05-03 19:40:52.758384: Epoch time: 64.9 s
2024-05-03 19:40:54.009712: 
2024-05-03 19:40:54.012344: Epoch 66
2024-05-03 19:40:54.013809: Current learning rate: 0.0094
2024-05-03 19:41:58.951389: Validation loss did not improve from -0.51715. Patience: 36/50
2024-05-03 19:41:58.956023: train_loss -0.7195
2024-05-03 19:41:58.957615: val_loss -0.3672
2024-05-03 19:41:58.958837: Pseudo dice [0.6682]
2024-05-03 19:41:58.960069: Epoch time: 64.95 s
2024-05-03 19:42:00.228068: 
2024-05-03 19:42:00.230448: Epoch 67
2024-05-03 19:42:00.231732: Current learning rate: 0.00939
2024-05-03 19:43:05.337054: Validation loss did not improve from -0.51715. Patience: 37/50
2024-05-03 19:43:05.338630: train_loss -0.7238
2024-05-03 19:43:05.340152: val_loss -0.435
2024-05-03 19:43:05.341352: Pseudo dice [0.7056]
2024-05-03 19:43:05.342375: Epoch time: 65.11 s
2024-05-03 19:43:06.601480: 
2024-05-03 19:43:06.603446: Epoch 68
2024-05-03 19:43:06.605281: Current learning rate: 0.00939
2024-05-03 19:44:11.685066: Validation loss did not improve from -0.51715. Patience: 38/50
2024-05-03 19:44:11.686670: train_loss -0.7187
2024-05-03 19:44:11.688010: val_loss -0.4616
2024-05-03 19:44:11.689104: Pseudo dice [0.7243]
2024-05-03 19:44:11.690242: Epoch time: 65.09 s
2024-05-03 19:44:12.955656: 
2024-05-03 19:44:12.958220: Epoch 69
2024-05-03 19:44:12.959594: Current learning rate: 0.00938
2024-05-03 19:45:18.134529: Validation loss did not improve from -0.51715. Patience: 39/50
2024-05-03 19:45:18.136327: train_loss -0.7048
2024-05-03 19:45:18.138262: val_loss -0.4631
2024-05-03 19:45:18.139521: Pseudo dice [0.7145]
2024-05-03 19:45:18.140675: Epoch time: 65.18 s
2024-05-03 19:45:19.850593: 
2024-05-03 19:45:19.854566: Epoch 70
2024-05-03 19:45:19.856259: Current learning rate: 0.00937
2024-05-03 19:46:24.979713: Validation loss did not improve from -0.51715. Patience: 40/50
2024-05-03 19:46:24.981480: train_loss -0.7109
2024-05-03 19:46:24.982900: val_loss -0.44
2024-05-03 19:46:24.983997: Pseudo dice [0.7046]
2024-05-03 19:46:24.985220: Epoch time: 65.13 s
2024-05-03 19:46:26.284008: 
2024-05-03 19:46:26.286443: Epoch 71
2024-05-03 19:46:26.287794: Current learning rate: 0.00936
2024-05-03 19:47:31.370930: Validation loss did not improve from -0.51715. Patience: 41/50
2024-05-03 19:47:31.374013: train_loss -0.7339
2024-05-03 19:47:31.375649: val_loss -0.4694
2024-05-03 19:47:31.376927: Pseudo dice [0.7264]
2024-05-03 19:47:31.378038: Epoch time: 65.09 s
2024-05-03 19:47:32.667614: 
2024-05-03 19:47:32.670294: Epoch 72
2024-05-03 19:47:32.671734: Current learning rate: 0.00935
2024-05-03 19:48:37.903368: Validation loss did not improve from -0.51715. Patience: 42/50
2024-05-03 19:48:37.904762: train_loss -0.7383
2024-05-03 19:48:37.906219: val_loss -0.5066
2024-05-03 19:48:37.907485: Pseudo dice [0.7403]
2024-05-03 19:48:37.908726: Epoch time: 65.24 s
2024-05-03 19:48:37.910105: Yayy! New best EMA pseudo Dice: 0.7123
2024-05-03 19:48:39.503805: 
2024-05-03 19:48:39.505872: Epoch 73
2024-05-03 19:48:39.507175: Current learning rate: 0.00934
2024-05-03 19:49:47.751213: Validation loss did not improve from -0.51715. Patience: 43/50
2024-05-03 19:49:47.769320: train_loss -0.7328
2024-05-03 19:49:47.771450: val_loss -0.4349
2024-05-03 19:49:47.773037: Pseudo dice [0.7112]
2024-05-03 19:49:47.774576: Epoch time: 68.27 s
2024-05-03 19:49:49.271998: 
2024-05-03 19:49:49.274273: Epoch 74
2024-05-03 19:49:49.275878: Current learning rate: 0.00933
2024-05-03 19:50:54.266940: Validation loss did not improve from -0.51715. Patience: 44/50
2024-05-03 19:50:54.268392: train_loss -0.7337
2024-05-03 19:50:54.269613: val_loss -0.4196
2024-05-03 19:50:54.270711: Pseudo dice [0.6969]
2024-05-03 19:50:54.271745: Epoch time: 65.0 s
2024-05-03 19:50:56.487256: 
2024-05-03 19:50:56.489293: Epoch 75
2024-05-03 19:50:56.490781: Current learning rate: 0.00932
2024-05-03 19:52:01.678785: Validation loss did not improve from -0.51715. Patience: 45/50
2024-05-03 19:52:01.680343: train_loss -0.7181
2024-05-03 19:52:01.681850: val_loss -0.4223
2024-05-03 19:52:01.683036: Pseudo dice [0.7026]
2024-05-03 19:52:01.684150: Epoch time: 65.19 s
2024-05-03 19:52:02.953207: 
2024-05-03 19:52:02.955073: Epoch 76
2024-05-03 19:52:02.956484: Current learning rate: 0.00931
2024-05-03 19:53:08.161567: Validation loss did not improve from -0.51715. Patience: 46/50
2024-05-03 19:53:08.163363: train_loss -0.7282
2024-05-03 19:53:08.164517: val_loss -0.4169
2024-05-03 19:53:08.165502: Pseudo dice [0.7102]
2024-05-03 19:53:08.166440: Epoch time: 65.21 s
2024-05-03 19:53:09.438570: 
2024-05-03 19:53:09.440542: Epoch 77
2024-05-03 19:53:09.442250: Current learning rate: 0.0093
2024-05-03 19:54:14.552585: Validation loss did not improve from -0.51715. Patience: 47/50
2024-05-03 19:54:14.553970: train_loss -0.7314
2024-05-03 19:54:14.555142: val_loss -0.4476
2024-05-03 19:54:14.556171: Pseudo dice [0.6905]
2024-05-03 19:54:14.557236: Epoch time: 65.12 s
2024-05-03 19:54:15.846849: 
2024-05-03 19:54:15.848596: Epoch 78
2024-05-03 19:54:15.849609: Current learning rate: 0.0093
2024-05-03 19:55:20.956406: Validation loss did not improve from -0.51715. Patience: 48/50
2024-05-03 19:55:20.958708: train_loss -0.7324
2024-05-03 19:55:20.960217: val_loss -0.4855
2024-05-03 19:55:20.961273: Pseudo dice [0.7232]
2024-05-03 19:55:20.962124: Epoch time: 65.11 s
2024-05-03 19:55:22.258990: 
2024-05-03 19:55:22.261203: Epoch 79
2024-05-03 19:55:22.262809: Current learning rate: 0.00929
2024-05-03 19:56:27.383419: Validation loss did not improve from -0.51715. Patience: 49/50
2024-05-03 19:56:27.384782: train_loss -0.7252
2024-05-03 19:56:27.387691: val_loss -0.4553
2024-05-03 19:56:27.388969: Pseudo dice [0.7187]
2024-05-03 19:56:27.390191: Epoch time: 65.13 s
2024-05-03 19:56:29.052225: 
2024-05-03 19:56:29.054157: Epoch 80
2024-05-03 19:56:29.055200: Current learning rate: 0.00928
2024-05-03 19:57:34.081681: Validation loss did not improve from -0.51715. Patience: 50/50
2024-05-03 19:57:34.083706: train_loss -0.721
2024-05-03 19:57:34.085256: val_loss -0.4315
2024-05-03 19:57:34.086528: Pseudo dice [0.7184]
2024-05-03 19:57:34.087589: Epoch time: 65.03 s
2024-05-03 19:57:35.363943: Patience reached. Stopping training.
2024-05-03 19:57:35.721461: Training done.
2024-05-03 19:57:36.092301: Using splits from existing split file: /home/gridsan/nchutisilp/datasets/nnUNet_Datasets/nnUNet_preprocessed/Dataset302_Calcium_OCTv2/splits_final.json
2024-05-03 19:57:36.094860: The split file contains 3 splits.
2024-05-03 19:57:36.096378: Desired fold for training: 2
2024-05-03 19:57:36.097392: This split has 4 training and 2 validation cases.
2024-05-03 19:57:36.098498: predicting 401-004
2024-05-03 19:57:36.111361: 401-004, shape torch.Size([1, 375, 498, 498]), rank 0
2024-05-03 19:59:07.815432: predicting 701-013
2024-05-03 19:59:07.834034: 701-013, shape torch.Size([1, 375, 498, 498]), rank 0
2024-05-03 20:00:46.745321: Validation complete
2024-05-03 20:00:46.747093: Mean Validation Dice:  0.7105577982995952
wandb: 
wandb: Run history:
wandb:            ema_fg_dice ▁▁▂▂▂▂▂▃▄▅▅▆▆▆▇▇▇▆▆▆▅▅▆▇▇▇▇█▇█▇█▇▇▇███▇█
wandb:   epoch_end_timestamps ▁▁▁▁▂▂▂▂▃▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇▇██
wandb: epoch_start_timestamps ▁▁▁▂▂▂▂▂▃▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███
wandb:                    lrs ███▇▇▇▇▇▇▆▆▆▆▆▆▅▅▅▅▅▄▄▄▄▄▄▃▃▃▃▃▂▂▂▂▂▂▁▁▁
wandb:           mean_fg_dice ▂▁▄▄▄▅▁▅▆▇▄▆▇▆▆█▃▄▃▆▁▅▇▇▆▄▄▇▂▇▆█▃▅▆▇▆▅▄▆
wandb:           train_losses █▅▅▄▄▄▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:             val_losses █▇▆▅▆▃█▄▄▂▅▃▂▃▃▁▆▆█▃█▄▂▃▃▅▆▄▇▃▃▂▄▅▃▃▅▅▄▅
wandb: 
wandb: Run summary:
wandb:            ema_fg_dice 0.71121
wandb:   epoch_end_timestamps 1714780654.08355
wandb: epoch_start_timestamps 1714780589.05102
wandb:                    lrs 0.00928
wandb:           mean_fg_dice 0.71842
wandb:           train_losses -0.72104
wandb:             val_losses -0.43153
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/gridsan/nchutisilp/datasets/nnUNet_Datasets/nnUNet_results/Dataset302_Calcium_OCTv2/nnUNetTrainer__nnUNetPlans__3d_fullres/fold_2/wandb/offline-run-20240503_182440-p4woaxtf
wandb: Find logs at: /home/gridsan/nchutisilp/datasets/nnUNet_Datasets/nnUNet_results/Dataset302_Calcium_OCTv2/nnUNetTrainer__nnUNetPlans__3d_fullres/fold_2/wandb/offline-run-20240503_182440-p4woaxtf/logs
WARNING:urllib3.connectionpool:Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x7fb22444b040>: Failed to establish a new connection: [Errno 101] Network is unreachable')': /api/4504800232407040/envelope/
WARNING:urllib3.connectionpool:Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x7fb212407850>: Failed to establish a new connection: [Errno 101] Network is unreachable')': /api/4504800232407040/envelope/
WARNING:urllib3.connectionpool:Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x7fb1eac7d490>: Failed to establish a new connection: [Errno 101] Network is unreachable')': /api/4504800232407040/envelope/
WARNING:urllib3.connectionpool:Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x7fb1f1655c40>: Failed to establish a new connection: [Errno 101] Network is unreachable')': /api/4504800232407040/envelope/
WARNING:urllib3.connectionpool:Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x7fb147ee91c0>: Failed to establish a new connection: [Errno 101] Network is unreachable')': /api/4504800232407040/envelope/
WARNING:urllib3.connectionpool:Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x7fb203722910>: Failed to establish a new connection: [Errno 101] Network is unreachable')': /api/4504800232407040/envelope/
FOLD 2 CONFIG 3d_fullres TRAINER nnUNetTrainer
usage: nnUNetv2_predict [-h] -i I -o O -d D [-p P] [-tr TR] -c C
                        [-f F [F ...]] [-step_size STEP_SIZE] [--disable_tta]
                        [--verbose] [--save_probabilities]
                        [--continue_prediction] [-chk CHK] [-npp NPP]
                        [-nps NPS]
                        [-prev_stage_predictions PREV_STAGE_PREDICTIONS]
                        [-num_parts NUM_PARTS] [-part_id PART_ID]
                        [-device DEVICE] [--disable_progress_bar]
nnUNetv2_predict: error: argument -d: expected one argument

#######################################################################
Please cite the following paper when using nnU-Net:
Isensee, F., Jaeger, P. F., Kohl, S. A., Petersen, J., & Maier-Hein, K. H. (2021). nnU-Net: a self-configuring method for deep learning-based biomedical image segmentation. Nature methods, 18(2), 203-211.
#######################################################################

Traceback (most recent call last):
  File "/home/gridsan/nchutisilp/.local/bin/nnUNetv2_evaluate_simple", line 8, in <module>
    sys.exit(evaluate_simple_entry_point())
  File "/home/gridsan/nchutisilp/projects/nnUNet/nnunetv2/evaluation/evaluate_predictions.py", line 251, in evaluate_simple_entry_point
    compute_metrics_on_folder_simple(args.gt_folder, args.pred_folder, args.l, args.o, args.np, args.il, chill=args.chill)
  File "/home/gridsan/nchutisilp/projects/nnUNet/nnunetv2/evaluation/evaluate_predictions.py", line 213, in compute_metrics_on_folder_simple
    compute_metrics_on_folder(folder_ref, folder_pred, output_file, rw, file_ending,
  File "/home/gridsan/nchutisilp/projects/nnUNet/nnunetv2/evaluation/evaluate_predictions.py", line 135, in compute_metrics_on_folder
    files_pred = subfiles(folder_pred, suffix=file_ending, join=False)
  File "/home/gridsan/nchutisilp/.local/lib/python3.9/site-packages/batchgenerators/utilities/file_and_folder_operations.py", line 40, in subfiles
    res = [l(folder, i) for i in os.listdir(folder) if os.path.isfile(os.path.join(folder, i))
FileNotFoundError: [Errno 2] No such file or directory: '/home/gridsan/nchutisilp/datasets/nnUNet_Datasets/nnUNet_results/Dataset302_Calcium_OCTv2/nnUNetTrainer__nnUNetPlans__3d_fullres/fold_2_finetuned_with_LaW_test'
Completed FOLD 2 CONFIG 3d_fullres TRAINER nnUNetTrainer
