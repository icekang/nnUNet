/home/gridsan/nchutisilp/.conda/envs/nnunet/bin/python
wandb: Tracking run with wandb version 0.16.6
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.

############################
INFO: You are using the old nnU-Net default plans. We have updated our recommendations. Please consider using those instead! Read more here: https://github.com/MIC-DKFZ/nnUNet/blob/master/documentation/resenc_presets.md
############################

Using device: cuda:0

#######################################################################
Please cite the following paper when using nnU-Net:
Isensee, F., Jaeger, P. F., Kohl, S. A., Petersen, J., & Maier-Hein, K. H. (2021). nnU-Net: a self-configuring method for deep learning-based biomedical image segmentation. Nature methods, 18(2), 203-211.
#######################################################################

2024-05-02 17:48:10.584401: Using torch.compile...
/home/gridsan/nchutisilp/.local/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn("The verbose parameter is deprecated. Please use get_last_lr() "
################### Loading pretrained weights from file  /home/gridsan/nchutisilp/datasets/nnUNet_Datasets/nnUNet_results/Dataset300_Lumen_and_Wall_OCT/nnUNetTrainer__nnUNetPreprocessPlans__3d_fullres/fold_all/checkpoint_best.pth ###################
Below is the list of overlapping blocks in pretrained model and nnUNet architecture:
encoder.stages.0.0.convs.0.conv.weight shape torch.Size([32, 1, 3, 3, 3])
encoder.stages.0.0.convs.0.conv.bias shape torch.Size([32])
encoder.stages.0.0.convs.0.norm.weight shape torch.Size([32])
encoder.stages.0.0.convs.0.norm.bias shape torch.Size([32])
encoder.stages.0.0.convs.0.all_modules.0.weight shape torch.Size([32, 1, 3, 3, 3])
encoder.stages.0.0.convs.0.all_modules.0.bias shape torch.Size([32])
encoder.stages.0.0.convs.0.all_modules.1.weight shape torch.Size([32])
encoder.stages.0.0.convs.0.all_modules.1.bias shape torch.Size([32])
encoder.stages.0.0.convs.1.conv.weight shape torch.Size([32, 32, 3, 3, 3])
encoder.stages.0.0.convs.1.conv.bias shape torch.Size([32])
encoder.stages.0.0.convs.1.norm.weight shape torch.Size([32])
encoder.stages.0.0.convs.1.norm.bias shape torch.Size([32])
encoder.stages.0.0.convs.1.all_modules.0.weight shape torch.Size([32, 32, 3, 3, 3])
encoder.stages.0.0.convs.1.all_modules.0.bias shape torch.Size([32])
encoder.stages.0.0.convs.1.all_modules.1.weight shape torch.Size([32])
encoder.stages.0.0.convs.1.all_modules.1.bias shape torch.Size([32])
encoder.stages.1.0.convs.0.conv.weight shape torch.Size([64, 32, 3, 3, 3])
encoder.stages.1.0.convs.0.conv.bias shape torch.Size([64])
encoder.stages.1.0.convs.0.norm.weight shape torch.Size([64])
encoder.stages.1.0.convs.0.norm.bias shape torch.Size([64])
encoder.stages.1.0.convs.0.all_modules.0.weight shape torch.Size([64, 32, 3, 3, 3])
encoder.stages.1.0.convs.0.all_modules.0.bias shape torch.Size([64])
encoder.stages.1.0.convs.0.all_modules.1.weight shape torch.Size([64])
encoder.stages.1.0.convs.0.all_modules.1.bias shape torch.Size([64])
encoder.stages.1.0.convs.1.conv.weight shape torch.Size([64, 64, 3, 3, 3])
encoder.stages.1.0.convs.1.conv.bias shape torch.Size([64])
encoder.stages.1.0.convs.1.norm.weight shape torch.Size([64])
encoder.stages.1.0.convs.1.norm.bias shape torch.Size([64])
encoder.stages.1.0.convs.1.all_modules.0.weight shape torch.Size([64, 64, 3, 3, 3])
encoder.stages.1.0.convs.1.all_modules.0.bias shape torch.Size([64])
encoder.stages.1.0.convs.1.all_modules.1.weight shape torch.Size([64])
encoder.stages.1.0.convs.1.all_modules.1.bias shape torch.Size([64])
encoder.stages.2.0.convs.0.conv.weight shape torch.Size([128, 64, 3, 3, 3])
encoder.stages.2.0.convs.0.conv.bias shape torch.Size([128])
encoder.stages.2.0.convs.0.norm.weight shape torch.Size([128])
encoder.stages.2.0.convs.0.norm.bias shape torch.Size([128])
encoder.stages.2.0.convs.0.all_modules.0.weight shape torch.Size([128, 64, 3, 3, 3])
encoder.stages.2.0.convs.0.all_modules.0.bias shape torch.Size([128])
encoder.stages.2.0.convs.0.all_modules.1.weight shape torch.Size([128])
encoder.stages.2.0.convs.0.all_modules.1.bias shape torch.Size([128])
encoder.stages.2.0.convs.1.conv.weight shape torch.Size([128, 128, 3, 3, 3])
encoder.stages.2.0.convs.1.conv.bias shape torch.Size([128])
encoder.stages.2.0.convs.1.norm.weight shape torch.Size([128])
encoder.stages.2.0.convs.1.norm.bias shape torch.Size([128])
encoder.stages.2.0.convs.1.all_modules.0.weight shape torch.Size([128, 128, 3, 3, 3])
encoder.stages.2.0.convs.1.all_modules.0.bias shape torch.Size([128])
encoder.stages.2.0.convs.1.all_modules.1.weight shape torch.Size([128])
encoder.stages.2.0.convs.1.all_modules.1.bias shape torch.Size([128])
encoder.stages.3.0.convs.0.conv.weight shape torch.Size([256, 128, 3, 3, 3])
encoder.stages.3.0.convs.0.conv.bias shape torch.Size([256])
encoder.stages.3.0.convs.0.norm.weight shape torch.Size([256])
encoder.stages.3.0.convs.0.norm.bias shape torch.Size([256])
encoder.stages.3.0.convs.0.all_modules.0.weight shape torch.Size([256, 128, 3, 3, 3])
encoder.stages.3.0.convs.0.all_modules.0.bias shape torch.Size([256])
encoder.stages.3.0.convs.0.all_modules.1.weight shape torch.Size([256])
encoder.stages.3.0.convs.0.all_modules.1.bias shape torch.Size([256])
encoder.stages.3.0.convs.1.conv.weight shape torch.Size([256, 256, 3, 3, 3])
encoder.stages.3.0.convs.1.conv.bias shape torch.Size([256])
encoder.stages.3.0.convs.1.norm.weight shape torch.Size([256])
encoder.stages.3.0.convs.1.norm.bias shape torch.Size([256])
encoder.stages.3.0.convs.1.all_modules.0.weight shape torch.Size([256, 256, 3, 3, 3])
encoder.stages.3.0.convs.1.all_modules.0.bias shape torch.Size([256])
encoder.stages.3.0.convs.1.all_modules.1.weight shape torch.Size([256])
encoder.stages.3.0.convs.1.all_modules.1.bias shape torch.Size([256])
encoder.stages.4.0.convs.0.conv.weight shape torch.Size([320, 256, 3, 3, 3])
encoder.stages.4.0.convs.0.conv.bias shape torch.Size([320])
encoder.stages.4.0.convs.0.norm.weight shape torch.Size([320])
encoder.stages.4.0.convs.0.norm.bias shape torch.Size([320])
encoder.stages.4.0.convs.0.all_modules.0.weight shape torch.Size([320, 256, 3, 3, 3])
encoder.stages.4.0.convs.0.all_modules.0.bias shape torch.Size([320])
encoder.stages.4.0.convs.0.all_modules.1.weight shape torch.Size([320])
encoder.stages.4.0.convs.0.all_modules.1.bias shape torch.Size([320])
encoder.stages.4.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.4.0.convs.1.conv.bias shape torch.Size([320])
encoder.stages.4.0.convs.1.norm.weight shape torch.Size([320])
encoder.stages.4.0.convs.1.norm.bias shape torch.Size([320])
encoder.stages.4.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.4.0.convs.1.all_modules.0.bias shape torch.Size([320])
encoder.stages.4.0.convs.1.all_modules.1.weight shape torch.Size([320])
encoder.stages.4.0.convs.1.all_modules.1.bias shape torch.Size([320])
encoder.stages.5.0.convs.0.conv.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.5.0.convs.0.conv.bias shape torch.Size([320])
encoder.stages.5.0.convs.0.norm.weight shape torch.Size([320])
encoder.stages.5.0.convs.0.norm.bias shape torch.Size([320])
encoder.stages.5.0.convs.0.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.5.0.convs.0.all_modules.0.bias shape torch.Size([320])
encoder.stages.5.0.convs.0.all_modules.1.weight shape torch.Size([320])
encoder.stages.5.0.convs.0.all_modules.1.bias shape torch.Size([320])
encoder.stages.5.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.5.0.convs.1.conv.bias shape torch.Size([320])
encoder.stages.5.0.convs.1.norm.weight shape torch.Size([320])
encoder.stages.5.0.convs.1.norm.bias shape torch.Size([320])
encoder.stages.5.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.5.0.convs.1.all_modules.0.bias shape torch.Size([320])
encoder.stages.5.0.convs.1.all_modules.1.weight shape torch.Size([320])
encoder.stages.5.0.convs.1.all_modules.1.bias shape torch.Size([320])
decoder.encoder.stages.0.0.convs.0.conv.weight shape torch.Size([32, 1, 3, 3, 3])
decoder.encoder.stages.0.0.convs.0.conv.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.norm.weight shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.norm.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.all_modules.0.weight shape torch.Size([32, 1, 3, 3, 3])
decoder.encoder.stages.0.0.convs.0.all_modules.0.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.all_modules.1.weight shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.all_modules.1.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.conv.weight shape torch.Size([32, 32, 3, 3, 3])
decoder.encoder.stages.0.0.convs.1.conv.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.norm.weight shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.norm.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.all_modules.0.weight shape torch.Size([32, 32, 3, 3, 3])
decoder.encoder.stages.0.0.convs.1.all_modules.0.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.all_modules.1.weight shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.all_modules.1.bias shape torch.Size([32])
decoder.encoder.stages.1.0.convs.0.conv.weight shape torch.Size([64, 32, 3, 3, 3])
decoder.encoder.stages.1.0.convs.0.conv.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.norm.weight shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.norm.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.all_modules.0.weight shape torch.Size([64, 32, 3, 3, 3])
decoder.encoder.stages.1.0.convs.0.all_modules.0.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.all_modules.1.weight shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.all_modules.1.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.conv.weight shape torch.Size([64, 64, 3, 3, 3])
decoder.encoder.stages.1.0.convs.1.conv.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.norm.weight shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.norm.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.all_modules.0.weight shape torch.Size([64, 64, 3, 3, 3])
decoder.encoder.stages.1.0.convs.1.all_modules.0.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.all_modules.1.weight shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.all_modules.1.bias shape torch.Size([64])
decoder.encoder.stages.2.0.convs.0.conv.weight shape torch.Size([128, 64, 3, 3, 3])
decoder.encoder.stages.2.0.convs.0.conv.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.norm.weight shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.norm.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.all_modules.0.weight shape torch.Size([128, 64, 3, 3, 3])
decoder.encoder.stages.2.0.convs.0.all_modules.0.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.all_modules.1.weight shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.all_modules.1.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.conv.weight shape torch.Size([128, 128, 3, 3, 3])
decoder.encoder.stages.2.0.convs.1.conv.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.norm.weight shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.norm.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.all_modules.0.weight shape torch.Size([128, 128, 3, 3, 3])
decoder.encoder.stages.2.0.convs.1.all_modules.0.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.all_modules.1.weight shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.all_modules.1.bias shape torch.Size([128])
decoder.encoder.stages.3.0.convs.0.conv.weight shape torch.Size([256, 128, 3, 3, 3])
decoder.encoder.stages.3.0.convs.0.conv.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.norm.weight shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.norm.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.all_modules.0.weight shape torch.Size([256, 128, 3, 3, 3])
decoder.encoder.stages.3.0.convs.0.all_modules.0.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.all_modules.1.weight shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.all_modules.1.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.conv.weight shape torch.Size([256, 256, 3, 3, 3])
decoder.encoder.stages.3.0.convs.1.conv.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.norm.weight shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.norm.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.all_modules.0.weight shape torch.Size([256, 256, 3, 3, 3])
decoder.encoder.stages.3.0.convs.1.all_modules.0.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.all_modules.1.weight shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.all_modules.1.bias shape torch.Size([256])
decoder.encoder.stages.4.0.convs.0.conv.weight shape torch.Size([320, 256, 3, 3, 3])
decoder.encoder.stages.4.0.convs.0.conv.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.norm.weight shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.norm.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.all_modules.0.weight shape torch.Size([320, 256, 3, 3, 3])
decoder.encoder.stages.4.0.convs.0.all_modules.0.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.all_modules.1.weight shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.all_modules.1.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.4.0.convs.1.conv.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.norm.weight shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.norm.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.4.0.convs.1.all_modules.0.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.all_modules.1.weight shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.all_modules.1.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.conv.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.5.0.convs.0.conv.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.norm.weight shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.norm.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.5.0.convs.0.all_modules.0.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.all_modules.1.weight shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.all_modules.1.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.5.0.convs.1.conv.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.norm.weight shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.norm.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.5.0.convs.1.all_modules.0.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.all_modules.1.weight shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.all_modules.1.bias shape torch.Size([320])
decoder.stages.0.convs.0.conv.weight shape torch.Size([320, 640, 3, 3, 3])
decoder.stages.0.convs.0.conv.bias shape torch.Size([320])
decoder.stages.0.convs.0.norm.weight shape torch.Size([320])
decoder.stages.0.convs.0.norm.bias shape torch.Size([320])
decoder.stages.0.convs.0.all_modules.0.weight shape torch.Size([320, 640, 3, 3, 3])
decoder.stages.0.convs.0.all_modules.0.bias shape torch.Size([320])
decoder.stages.0.convs.0.all_modules.1.weight shape torch.Size([320])
decoder.stages.0.convs.0.all_modules.1.bias shape torch.Size([320])
decoder.stages.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.stages.0.convs.1.conv.bias shape torch.Size([320])
decoder.stages.0.convs.1.norm.weight shape torch.Size([320])
decoder.stages.0.convs.1.norm.bias shape torch.Size([320])
decoder.stages.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.stages.0.convs.1.all_modules.0.bias shape torch.Size([320])
decoder.stages.0.convs.1.all_modules.1.weight shape torch.Size([320])
decoder.stages.0.convs.1.all_modules.1.bias shape torch.Size([320])
decoder.stages.1.convs.0.conv.weight shape torch.Size([256, 512, 3, 3, 3])
decoder.stages.1.convs.0.conv.bias shape torch.Size([256])
decoder.stages.1.convs.0.norm.weight shape torch.Size([256])
decoder.stages.1.convs.0.norm.bias shape torch.Size([256])
decoder.stages.1.convs.0.all_modules.0.weight shape torch.Size([256, 512, 3, 3, 3])
decoder.stages.1.convs.0.all_modules.0.bias shape torch.Size([256])
decoder.stages.1.convs.0.all_modules.1.weight shape torch.Size([256])
decoder.stages.1.convs.0.all_modules.1.bias shape torch.Size([256])
decoder.stages.1.convs.1.conv.weight shape torch.Size([256, 256, 3, 3, 3])
decoder.stages.1.convs.1.conv.bias shape torch.Size([256])
decoder.stages.1.convs.1.norm.weight shape torch.Size([256])
decoder.stages.1.convs.1.norm.bias shape torch.Size([256])
decoder.stages.1.convs.1.all_modules.0.weight shape torch.Size([256, 256, 3, 3, 3])
decoder.stages.1.convs.1.all_modules.0.bias shape torch.Size([256])
decoder.stages.1.convs.1.all_modules.1.weight shape torch.Size([256])
decoder.stages.1.convs.1.all_modules.1.bias shape torch.Size([256])
decoder.stages.2.convs.0.conv.weight shape torch.Size([128, 256, 3, 3, 3])
decoder.stages.2.convs.0.conv.bias shape torch.Size([128])
decoder.stages.2.convs.0.norm.weight shape torch.Size([128])
decoder.stages.2.convs.0.norm.bias shape torch.Size([128])
decoder.stages.2.convs.0.all_modules.0.weight shape torch.Size([128, 256, 3, 3, 3])
decoder.stages.2.convs.0.all_modules.0.bias shape torch.Size([128])
decoder.stages.2.convs.0.all_modules.1.weight shape torch.Size([128])
decoder.stages.2.convs.0.all_modules.1.bias shape torch.Size([128])
decoder.stages.2.convs.1.conv.weight shape torch.Size([128, 128, 3, 3, 3])
decoder.stages.2.convs.1.conv.bias shape torch.Size([128])
decoder.stages.2.convs.1.norm.weight shape torch.Size([128])
decoder.stages.2.convs.1.norm.bias shape torch.Size([128])
decoder.stages.2.convs.1.all_modules.0.weight shape torch.Size([128, 128, 3, 3, 3])
decoder.stages.2.convs.1.all_modules.0.bias shape torch.Size([128])
decoder.stages.2.convs.1.all_modules.1.weight shape torch.Size([128])
decoder.stages.2.convs.1.all_modules.1.bias shape torch.Size([128])
decoder.stages.3.convs.0.conv.weight shape torch.Size([64, 128, 3, 3, 3])
decoder.stages.3.convs.0.conv.bias shape torch.Size([64])
decoder.stages.3.convs.0.norm.weight shape torch.Size([64])
decoder.stages.3.convs.0.norm.bias shape torch.Size([64])
decoder.stages.3.convs.0.all_modules.0.weight shape torch.Size([64, 128, 3, 3, 3])
decoder.stages.3.convs.0.all_modules.0.bias shape torch.Size([64])
decoder.stages.3.convs.0.all_modules.1.weight shape torch.Size([64])
decoder.stages.3.convs.0.all_modules.1.bias shape torch.Size([64])
decoder.stages.3.convs.1.conv.weight shape torch.Size([64, 64, 3, 3, 3])
decoder.stages.3.convs.1.conv.bias shape torch.Size([64])
decoder.stages.3.convs.1.norm.weight shape torch.Size([64])
decoder.stages.3.convs.1.norm.bias shape torch.Size([64])
decoder.stages.3.convs.1.all_modules.0.weight shape torch.Size([64, 64, 3, 3, 3])
decoder.stages.3.convs.1.all_modules.0.bias shape torch.Size([64])
decoder.stages.3.convs.1.all_modules.1.weight shape torch.Size([64])
decoder.stages.3.convs.1.all_modules.1.bias shape torch.Size([64])
decoder.stages.4.convs.0.conv.weight shape torch.Size([32, 64, 3, 3, 3])
decoder.stages.4.convs.0.conv.bias shape torch.Size([32])
decoder.stages.4.convs.0.norm.weight shape torch.Size([32])
decoder.stages.4.convs.0.norm.bias shape torch.Size([32])
decoder.stages.4.convs.0.all_modules.0.weight shape torch.Size([32, 64, 3, 3, 3])
decoder.stages.4.convs.0.all_modules.0.bias shape torch.Size([32])
decoder.stages.4.convs.0.all_modules.1.weight shape torch.Size([32])
decoder.stages.4.convs.0.all_modules.1.bias shape torch.Size([32])
decoder.stages.4.convs.1.conv.weight shape torch.Size([32, 32, 3, 3, 3])
decoder.stages.4.convs.1.conv.bias shape torch.Size([32])
decoder.stages.4.convs.1.norm.weight shape torch.Size([32])
decoder.stages.4.convs.1.norm.bias shape torch.Size([32])
decoder.stages.4.convs.1.all_modules.0.weight shape torch.Size([32, 32, 3, 3, 3])
decoder.stages.4.convs.1.all_modules.0.bias shape torch.Size([32])
decoder.stages.4.convs.1.all_modules.1.weight shape torch.Size([32])
decoder.stages.4.convs.1.all_modules.1.bias shape torch.Size([32])
decoder.transpconvs.0.weight shape torch.Size([320, 320, 1, 2, 2])
decoder.transpconvs.0.bias shape torch.Size([320])
decoder.transpconvs.1.weight shape torch.Size([320, 256, 2, 2, 2])
decoder.transpconvs.1.bias shape torch.Size([256])
decoder.transpconvs.2.weight shape torch.Size([256, 128, 2, 2, 2])
decoder.transpconvs.2.bias shape torch.Size([128])
decoder.transpconvs.3.weight shape torch.Size([128, 64, 2, 2, 2])
decoder.transpconvs.3.bias shape torch.Size([64])
decoder.transpconvs.4.weight shape torch.Size([64, 32, 2, 2, 2])
decoder.transpconvs.4.bias shape torch.Size([32])
################### Done ###################
2024-05-02 17:48:18.300805: do_dummy_2d_data_aug: False
2024-05-02 17:48:18.303804: Using splits from existing split file: /home/gridsan/nchutisilp/datasets/nnUNet_Datasets/nnUNet_preprocessed/Dataset302_Calcium_OCTv2/splits_final.json
2024-05-02 17:48:18.317202: The split file contains 3 splits.
2024-05-02 17:48:18.319909: Desired fold for training: 0
2024-05-02 17:48:18.322403: This split has 4 training and 2 validation cases.
using pin_memory on device 0
using pin_memory on device 0

This is the configuration used by this training:
Configuration name: 3d_fullres
 {'data_identifier': 'nnUNetPlans_3d_fullres', 'preprocessor_name': 'DefaultPreprocessor', 'batch_size': 2, 'patch_size': [112, 160, 128], 'median_image_size_in_voxels': [375.0, 498.0, 498.0], 'spacing': [1.0, 1.0, 1.0], 'normalization_schemes': ['ZScoreNormalization'], 'use_mask_for_norm': [False], 'resampling_fn_data': 'resample_data_or_seg_to_shape', 'resampling_fn_seg': 'resample_data_or_seg_to_shape', 'resampling_fn_data_kwargs': {'is_seg': False, 'order': 3, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_seg_kwargs': {'is_seg': True, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_probabilities': 'resample_data_or_seg_to_shape', 'resampling_fn_probabilities_kwargs': {'is_seg': False, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'architecture': {'network_class_name': 'dynamic_network_architectures.architectures.unet.PlainConvUNet', 'arch_kwargs': {'n_stages': 6, 'features_per_stage': [32, 64, 128, 256, 320, 320], 'conv_op': 'torch.nn.modules.conv.Conv3d', 'kernel_sizes': [[3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3]], 'strides': [[1, 1, 1], [2, 2, 2], [2, 2, 2], [2, 2, 2], [2, 2, 2], [1, 2, 2]], 'n_conv_per_stage': [2, 2, 2, 2, 2, 2], 'n_conv_per_stage_decoder': [2, 2, 2, 2, 2], 'conv_bias': True, 'norm_op': 'torch.nn.modules.instancenorm.InstanceNorm3d', 'norm_op_kwargs': {'eps': 1e-05, 'affine': True}, 'dropout_op': None, 'dropout_op_kwargs': None, 'nonlin': 'torch.nn.LeakyReLU', 'nonlin_kwargs': {'inplace': True}, 'deep_supervision': True}, '_kw_requires_import': ['conv_op', 'norm_op', 'dropout_op', 'nonlin']}, 'batch_dice': True} 

These are the global plan.json settings:
 {'dataset_name': 'Dataset302_Calcium_OCTv2', 'plans_name': 'nnUNetPlans', 'original_median_spacing_after_transp': [1.0, 1.0, 1.0], 'original_median_shape_after_transp': [375, 498, 498], 'image_reader_writer': 'SimpleITKIO', 'transpose_forward': [0, 1, 2], 'transpose_backward': [0, 1, 2], 'experiment_planner_used': 'ExperimentPlanner', 'label_manager': 'LabelManager', 'foreground_intensity_properties_per_channel': {'0': {'max': 0.4977976381778717, 'mean': 0.13332499563694, 'median': 0.08871842920780182, 'min': 0.0, 'percentile_00_5': 0.014754901640117168, 'percentile_99_5': 0.4977976381778717, 'std': 0.12022686749696732}}} 

2024-05-02 17:48:27.179924: unpacking dataset...
2024-05-02 17:48:36.086532: unpacking done...
2024-05-02 17:48:36.451820: Unable to plot network architecture: nnUNet_compile is enabled!
2024-05-02 17:48:36.582824: 
2024-05-02 17:48:36.585251: Epoch 0
2024-05-02 17:48:36.586988: Current learning rate: 0.01
2024-05-02 17:51:14.810902: Validation loss improved from 1000.00000 to -0.34941! Patience: 0/50
2024-05-02 17:51:14.813294: train_loss -0.3036
2024-05-02 17:51:14.815125: val_loss -0.3494
2024-05-02 17:51:14.816344: Pseudo dice [0.6472]
2024-05-02 17:51:14.817470: Epoch time: 158.23 s
2024-05-02 17:51:14.818512: Yayy! New best EMA pseudo Dice: 0.6472
2024-05-02 17:51:16.445366: 
2024-05-02 17:51:16.447701: Epoch 1
2024-05-02 17:51:16.449347: Current learning rate: 0.00999
2024-05-02 17:52:20.237079: Validation loss improved from -0.34941 to -0.42294! Patience: 0/50
2024-05-02 17:52:20.239305: train_loss -0.419
2024-05-02 17:52:20.240932: val_loss -0.4229
2024-05-02 17:52:20.242153: Pseudo dice [0.6997]
2024-05-02 17:52:20.243344: Epoch time: 63.8 s
2024-05-02 17:52:20.244465: Yayy! New best EMA pseudo Dice: 0.6524
2024-05-02 17:52:21.771636: 
2024-05-02 17:52:21.774711: Epoch 2
2024-05-02 17:52:21.777234: Current learning rate: 0.00998
2024-05-02 17:53:25.543412: Validation loss did not improve from -0.42294. Patience: 1/50
2024-05-02 17:53:25.545192: train_loss -0.4576
2024-05-02 17:53:25.546432: val_loss -0.3794
2024-05-02 17:53:25.547599: Pseudo dice [0.6748]
2024-05-02 17:53:25.548731: Epoch time: 63.78 s
2024-05-02 17:53:25.549813: Yayy! New best EMA pseudo Dice: 0.6547
2024-05-02 17:53:27.148423: 
2024-05-02 17:53:27.150762: Epoch 3
2024-05-02 17:53:27.152115: Current learning rate: 0.00997
2024-05-02 17:54:31.491383: Validation loss improved from -0.42294 to -0.42589! Patience: 1/50
2024-05-02 17:54:31.493376: train_loss -0.4702
2024-05-02 17:54:31.495282: val_loss -0.4259
2024-05-02 17:54:31.496639: Pseudo dice [0.6911]
2024-05-02 17:54:31.498108: Epoch time: 64.35 s
2024-05-02 17:54:31.499208: Yayy! New best EMA pseudo Dice: 0.6583
2024-05-02 17:54:33.037409: 
2024-05-02 17:54:33.039627: Epoch 4
2024-05-02 17:54:33.040817: Current learning rate: 0.00996
2024-05-02 17:55:37.587290: Validation loss did not improve from -0.42589. Patience: 1/50
2024-05-02 17:55:37.589276: train_loss -0.5028
2024-05-02 17:55:37.591139: val_loss -0.3764
2024-05-02 17:55:37.592432: Pseudo dice [0.6851]
2024-05-02 17:55:37.593612: Epoch time: 64.55 s
2024-05-02 17:55:37.884148: Yayy! New best EMA pseudo Dice: 0.661
2024-05-02 17:55:39.479996: 
2024-05-02 17:55:39.482775: Epoch 5
2024-05-02 17:55:39.484573: Current learning rate: 0.00995
2024-05-02 17:56:44.068835: Validation loss did not improve from -0.42589. Patience: 2/50
2024-05-02 17:56:44.070838: train_loss -0.5087
2024-05-02 17:56:44.127538: val_loss -0.4123
2024-05-02 17:56:44.129845: Pseudo dice [0.6984]
2024-05-02 17:56:44.131117: Epoch time: 64.59 s
2024-05-02 17:56:44.132487: Yayy! New best EMA pseudo Dice: 0.6647
2024-05-02 17:56:45.645022: 
2024-05-02 17:56:45.647777: Epoch 6
2024-05-02 17:56:45.649059: Current learning rate: 0.00995
2024-05-02 17:57:50.408679: Validation loss did not improve from -0.42589. Patience: 3/50
2024-05-02 17:57:50.411609: train_loss -0.5316
2024-05-02 17:57:50.413455: val_loss -0.355
2024-05-02 17:57:50.415396: Pseudo dice [0.6658]
2024-05-02 17:57:50.417069: Epoch time: 64.77 s
2024-05-02 17:57:50.418481: Yayy! New best EMA pseudo Dice: 0.6648
2024-05-02 17:57:52.379053: 
2024-05-02 17:57:52.381392: Epoch 7
2024-05-02 17:57:52.382713: Current learning rate: 0.00994
2024-05-02 17:58:56.859620: Validation loss improved from -0.42589 to -0.46763! Patience: 3/50
2024-05-02 17:58:56.861688: train_loss -0.528
2024-05-02 17:58:56.863572: val_loss -0.4676
2024-05-02 17:58:56.865029: Pseudo dice [0.7241]
2024-05-02 17:58:56.866191: Epoch time: 64.48 s
2024-05-02 17:58:56.867342: Yayy! New best EMA pseudo Dice: 0.6708
2024-05-02 17:58:58.444498: 
2024-05-02 17:58:58.447546: Epoch 8
2024-05-02 17:58:58.449325: Current learning rate: 0.00993
2024-05-02 18:00:02.973878: Validation loss improved from -0.46763 to -0.47426! Patience: 0/50
2024-05-02 18:00:02.979058: train_loss -0.5486
2024-05-02 18:00:02.980956: val_loss -0.4743
2024-05-02 18:00:02.982504: Pseudo dice [0.7159]
2024-05-02 18:00:02.983813: Epoch time: 64.54 s
2024-05-02 18:00:02.985228: Yayy! New best EMA pseudo Dice: 0.6753
2024-05-02 18:00:04.589245: 
2024-05-02 18:00:04.592070: Epoch 9
2024-05-02 18:00:04.593533: Current learning rate: 0.00992
2024-05-02 18:01:09.183891: Validation loss did not improve from -0.47426. Patience: 1/50
2024-05-02 18:01:09.186072: train_loss -0.5505
2024-05-02 18:01:09.187600: val_loss -0.4405
2024-05-02 18:01:09.188936: Pseudo dice [0.7066]
2024-05-02 18:01:09.190356: Epoch time: 64.6 s
2024-05-02 18:01:09.524529: Yayy! New best EMA pseudo Dice: 0.6784
2024-05-02 18:01:11.010346: 
2024-05-02 18:01:11.013029: Epoch 10
2024-05-02 18:01:11.014448: Current learning rate: 0.00991
2024-05-02 18:02:15.593572: Validation loss did not improve from -0.47426. Patience: 2/50
2024-05-02 18:02:15.595046: train_loss -0.5778
2024-05-02 18:02:15.596441: val_loss -0.426
2024-05-02 18:02:15.597697: Pseudo dice [0.7102]
2024-05-02 18:02:15.599027: Epoch time: 64.59 s
2024-05-02 18:02:15.600549: Yayy! New best EMA pseudo Dice: 0.6816
2024-05-02 18:02:17.134985: 
2024-05-02 18:02:17.136850: Epoch 11
2024-05-02 18:02:17.138120: Current learning rate: 0.0099
2024-05-02 18:03:21.850134: Validation loss did not improve from -0.47426. Patience: 3/50
2024-05-02 18:03:21.852920: train_loss -0.5754
2024-05-02 18:03:21.854841: val_loss -0.4381
2024-05-02 18:03:21.855910: Pseudo dice [0.7039]
2024-05-02 18:03:21.857644: Epoch time: 64.72 s
2024-05-02 18:03:21.859456: Yayy! New best EMA pseudo Dice: 0.6838
2024-05-02 18:03:23.420358: 
2024-05-02 18:03:23.423365: Epoch 12
2024-05-02 18:03:23.424958: Current learning rate: 0.00989
2024-05-02 18:04:28.102936: Validation loss improved from -0.47426 to -0.48988! Patience: 3/50
2024-05-02 18:04:28.105693: train_loss -0.5803
2024-05-02 18:04:28.107979: val_loss -0.4899
2024-05-02 18:04:28.110154: Pseudo dice [0.7192]
2024-05-02 18:04:28.112125: Epoch time: 64.69 s
2024-05-02 18:04:28.113554: Yayy! New best EMA pseudo Dice: 0.6874
2024-05-02 18:04:29.690336: 
2024-05-02 18:04:29.692161: Epoch 13
2024-05-02 18:04:29.693219: Current learning rate: 0.00988
2024-05-02 18:05:34.463976: Validation loss did not improve from -0.48988. Patience: 1/50
2024-05-02 18:05:34.466457: train_loss -0.5836
2024-05-02 18:05:34.468448: val_loss -0.3987
2024-05-02 18:05:34.469563: Pseudo dice [0.6802]
2024-05-02 18:05:34.470753: Epoch time: 64.78 s
2024-05-02 18:05:35.710021: 
2024-05-02 18:05:35.712699: Epoch 14
2024-05-02 18:05:35.714153: Current learning rate: 0.00987
2024-05-02 18:06:40.498018: Validation loss improved from -0.48988 to -0.50734! Patience: 1/50
2024-05-02 18:06:40.500569: train_loss -0.6034
2024-05-02 18:06:40.502646: val_loss -0.5073
2024-05-02 18:06:40.503720: Pseudo dice [0.7377]
2024-05-02 18:06:40.504795: Epoch time: 64.79 s
2024-05-02 18:06:40.846089: Yayy! New best EMA pseudo Dice: 0.6917
2024-05-02 18:06:42.428020: 
2024-05-02 18:06:42.430858: Epoch 15
2024-05-02 18:06:42.432116: Current learning rate: 0.00986
2024-05-02 18:07:47.157512: Validation loss did not improve from -0.50734. Patience: 1/50
2024-05-02 18:07:47.159529: train_loss -0.6094
2024-05-02 18:07:47.161632: val_loss -0.347
2024-05-02 18:07:47.162751: Pseudo dice [0.666]
2024-05-02 18:07:47.163904: Epoch time: 64.73 s
2024-05-02 18:07:48.444721: 
2024-05-02 18:07:48.447458: Epoch 16
2024-05-02 18:07:48.449339: Current learning rate: 0.00986
2024-05-02 18:08:53.507726: Validation loss did not improve from -0.50734. Patience: 2/50
2024-05-02 18:08:53.509726: train_loss -0.5837
2024-05-02 18:08:53.511369: val_loss -0.4673
2024-05-02 18:08:53.512868: Pseudo dice [0.7241]
2024-05-02 18:08:53.514115: Epoch time: 65.07 s
2024-05-02 18:08:53.515229: Yayy! New best EMA pseudo Dice: 0.6927
2024-05-02 18:08:55.173061: 
2024-05-02 18:08:55.176396: Epoch 17
2024-05-02 18:08:55.178043: Current learning rate: 0.00985
2024-05-02 18:10:00.150467: Validation loss did not improve from -0.50734. Patience: 3/50
2024-05-02 18:10:00.152416: train_loss -0.6217
2024-05-02 18:10:00.154263: val_loss -0.4587
2024-05-02 18:10:00.155395: Pseudo dice [0.721]
2024-05-02 18:10:00.156457: Epoch time: 64.98 s
2024-05-02 18:10:00.157432: Yayy! New best EMA pseudo Dice: 0.6955
2024-05-02 18:10:02.256638: 
2024-05-02 18:10:02.258887: Epoch 18
2024-05-02 18:10:02.260393: Current learning rate: 0.00984
2024-05-02 18:11:07.174901: Validation loss did not improve from -0.50734. Patience: 4/50
2024-05-02 18:11:07.176682: train_loss -0.6338
2024-05-02 18:11:07.177945: val_loss -0.4512
2024-05-02 18:11:07.179181: Pseudo dice [0.7161]
2024-05-02 18:11:07.180549: Epoch time: 64.92 s
2024-05-02 18:11:07.181618: Yayy! New best EMA pseudo Dice: 0.6976
2024-05-02 18:11:08.804461: 
2024-05-02 18:11:08.806944: Epoch 19
2024-05-02 18:11:08.808356: Current learning rate: 0.00983
2024-05-02 18:12:13.781651: Validation loss did not improve from -0.50734. Patience: 5/50
2024-05-02 18:12:13.783395: train_loss -0.6157
2024-05-02 18:12:13.784826: val_loss -0.4558
2024-05-02 18:12:13.785996: Pseudo dice [0.7152]
2024-05-02 18:12:13.787121: Epoch time: 64.98 s
2024-05-02 18:12:14.141016: Yayy! New best EMA pseudo Dice: 0.6993
2024-05-02 18:12:15.742208: 
2024-05-02 18:12:15.744121: Epoch 20
2024-05-02 18:12:15.745387: Current learning rate: 0.00982
2024-05-02 18:13:20.725978: Validation loss did not improve from -0.50734. Patience: 6/50
2024-05-02 18:13:20.727972: train_loss -0.6237
2024-05-02 18:13:20.730341: val_loss -0.4836
2024-05-02 18:13:20.732240: Pseudo dice [0.7261]
2024-05-02 18:13:20.733479: Epoch time: 64.99 s
2024-05-02 18:13:20.735049: Yayy! New best EMA pseudo Dice: 0.702
2024-05-02 18:13:22.356817: 
2024-05-02 18:13:22.358952: Epoch 21
2024-05-02 18:13:22.360285: Current learning rate: 0.00981
2024-05-02 18:14:27.381981: Validation loss did not improve from -0.50734. Patience: 7/50
2024-05-02 18:14:27.383365: train_loss -0.6386
2024-05-02 18:14:27.384726: val_loss -0.502
2024-05-02 18:14:27.385736: Pseudo dice [0.7341]
2024-05-02 18:14:27.386750: Epoch time: 65.03 s
2024-05-02 18:14:27.387851: Yayy! New best EMA pseudo Dice: 0.7052
2024-05-02 18:14:28.945859: 
2024-05-02 18:14:28.947971: Epoch 22
2024-05-02 18:14:28.949054: Current learning rate: 0.0098
2024-05-02 18:15:33.931245: Validation loss did not improve from -0.50734. Patience: 8/50
2024-05-02 18:15:33.933289: train_loss -0.6412
2024-05-02 18:15:33.935031: val_loss -0.4774
2024-05-02 18:15:33.936143: Pseudo dice [0.7232]
2024-05-02 18:15:33.937291: Epoch time: 64.99 s
2024-05-02 18:15:33.938374: Yayy! New best EMA pseudo Dice: 0.707
2024-05-02 18:15:35.492675: 
2024-05-02 18:15:35.496527: Epoch 23
2024-05-02 18:15:35.498489: Current learning rate: 0.00979
2024-05-02 18:16:40.461831: Validation loss did not improve from -0.50734. Patience: 9/50
2024-05-02 18:16:40.464094: train_loss -0.6329
2024-05-02 18:16:40.466176: val_loss -0.4222
2024-05-02 18:16:40.467439: Pseudo dice [0.6927]
2024-05-02 18:16:40.468603: Epoch time: 64.97 s
2024-05-02 18:16:41.660662: 
2024-05-02 18:16:41.663512: Epoch 24
2024-05-02 18:16:41.665028: Current learning rate: 0.00978
2024-05-02 18:17:46.839293: Validation loss did not improve from -0.50734. Patience: 10/50
2024-05-02 18:17:46.841836: train_loss -0.643
2024-05-02 18:17:46.843897: val_loss -0.4734
2024-05-02 18:17:46.845062: Pseudo dice [0.7191]
2024-05-02 18:17:46.846231: Epoch time: 65.18 s
2024-05-02 18:17:48.421584: 
2024-05-02 18:17:48.424565: Epoch 25
2024-05-02 18:17:48.426368: Current learning rate: 0.00977
2024-05-02 18:18:53.670922: Validation loss did not improve from -0.50734. Patience: 11/50
2024-05-02 18:18:53.673686: train_loss -0.6516
2024-05-02 18:18:53.675594: val_loss -0.4594
2024-05-02 18:18:53.676679: Pseudo dice [0.7185]
2024-05-02 18:18:53.677809: Epoch time: 65.25 s
2024-05-02 18:18:53.679096: Yayy! New best EMA pseudo Dice: 0.7081
2024-05-02 18:18:55.227123: 
2024-05-02 18:18:55.229508: Epoch 26
2024-05-02 18:18:55.231086: Current learning rate: 0.00977
2024-05-02 18:20:00.501353: Validation loss did not improve from -0.50734. Patience: 12/50
2024-05-02 18:20:00.503947: train_loss -0.6517
2024-05-02 18:20:00.505925: val_loss -0.4687
2024-05-02 18:20:00.507050: Pseudo dice [0.713]
2024-05-02 18:20:00.508782: Epoch time: 65.28 s
2024-05-02 18:20:00.510564: Yayy! New best EMA pseudo Dice: 0.7086
2024-05-02 18:20:02.058826: 
2024-05-02 18:20:02.061482: Epoch 27
2024-05-02 18:20:02.063092: Current learning rate: 0.00976
2024-05-02 18:21:07.300962: Validation loss did not improve from -0.50734. Patience: 13/50
2024-05-02 18:21:07.302697: train_loss -0.6684
2024-05-02 18:21:07.304588: val_loss -0.4357
2024-05-02 18:21:07.306031: Pseudo dice [0.71]
2024-05-02 18:21:07.307056: Epoch time: 65.24 s
2024-05-02 18:21:07.308404: Yayy! New best EMA pseudo Dice: 0.7087
2024-05-02 18:21:08.871347: 
2024-05-02 18:21:08.874365: Epoch 28
2024-05-02 18:21:08.875675: Current learning rate: 0.00975
2024-05-02 18:22:14.133125: Validation loss did not improve from -0.50734. Patience: 14/50
2024-05-02 18:22:14.135378: train_loss -0.6593
2024-05-02 18:22:14.137290: val_loss -0.4861
2024-05-02 18:22:14.138724: Pseudo dice [0.7323]
2024-05-02 18:22:14.140419: Epoch time: 65.27 s
2024-05-02 18:22:14.142352: Yayy! New best EMA pseudo Dice: 0.7111
2024-05-02 18:22:16.095693: 
2024-05-02 18:22:16.098520: Epoch 29
2024-05-02 18:22:16.100637: Current learning rate: 0.00974
2024-05-02 18:23:21.432145: Validation loss did not improve from -0.50734. Patience: 15/50
2024-05-02 18:23:21.435123: train_loss -0.6652
2024-05-02 18:23:21.437116: val_loss -0.4607
2024-05-02 18:23:21.438534: Pseudo dice [0.7211]
2024-05-02 18:23:21.439978: Epoch time: 65.34 s
2024-05-02 18:23:21.800201: Yayy! New best EMA pseudo Dice: 0.7121
2024-05-02 18:23:23.351837: 
2024-05-02 18:23:23.354727: Epoch 30
2024-05-02 18:23:23.356100: Current learning rate: 0.00973
2024-05-02 18:24:28.632348: Validation loss did not improve from -0.50734. Patience: 16/50
2024-05-02 18:24:28.634736: train_loss -0.6748
2024-05-02 18:24:28.636943: val_loss -0.4505
2024-05-02 18:24:28.638592: Pseudo dice [0.7061]
2024-05-02 18:24:28.640139: Epoch time: 65.28 s
2024-05-02 18:24:29.865967: 
2024-05-02 18:24:29.868881: Epoch 31
2024-05-02 18:24:29.870480: Current learning rate: 0.00972
2024-05-02 18:25:35.243674: Validation loss did not improve from -0.50734. Patience: 17/50
2024-05-02 18:25:35.245872: train_loss -0.6806
2024-05-02 18:25:35.247509: val_loss -0.501
2024-05-02 18:25:35.248600: Pseudo dice [0.7403]
2024-05-02 18:25:35.249759: Epoch time: 65.38 s
2024-05-02 18:25:35.250808: Yayy! New best EMA pseudo Dice: 0.7144
2024-05-02 18:25:36.831836: 
2024-05-02 18:25:36.834480: Epoch 32
2024-05-02 18:25:36.836063: Current learning rate: 0.00971
2024-05-02 18:26:42.325981: Validation loss did not improve from -0.50734. Patience: 18/50
2024-05-02 18:26:42.327857: train_loss -0.6892
2024-05-02 18:26:42.329964: val_loss -0.4625
2024-05-02 18:26:42.331742: Pseudo dice [0.7099]
2024-05-02 18:26:42.333127: Epoch time: 65.5 s
2024-05-02 18:26:43.578732: 
2024-05-02 18:26:43.581527: Epoch 33
2024-05-02 18:26:43.583359: Current learning rate: 0.0097
2024-05-02 18:27:49.080262: Validation loss did not improve from -0.50734. Patience: 19/50
2024-05-02 18:27:49.081987: train_loss -0.685
2024-05-02 18:27:49.084232: val_loss -0.4138
2024-05-02 18:27:49.085545: Pseudo dice [0.7056]
2024-05-02 18:27:49.086710: Epoch time: 65.5 s
2024-05-02 18:27:50.337677: 
2024-05-02 18:27:50.340158: Epoch 34
2024-05-02 18:27:50.341151: Current learning rate: 0.00969
2024-05-02 18:28:55.931065: Validation loss did not improve from -0.50734. Patience: 20/50
2024-05-02 18:28:55.933647: train_loss -0.6799
2024-05-02 18:28:55.935483: val_loss -0.4462
2024-05-02 18:28:55.936831: Pseudo dice [0.7113]
2024-05-02 18:28:55.938396: Epoch time: 65.6 s
2024-05-02 18:28:57.639024: 
2024-05-02 18:28:57.641230: Epoch 35
2024-05-02 18:28:57.642469: Current learning rate: 0.00968
2024-05-02 18:30:03.076621: Validation loss did not improve from -0.50734. Patience: 21/50
2024-05-02 18:30:03.078278: train_loss -0.6708
2024-05-02 18:30:03.080560: val_loss -0.4431
2024-05-02 18:30:03.082175: Pseudo dice [0.7092]
2024-05-02 18:30:03.083803: Epoch time: 65.44 s
2024-05-02 18:30:04.364861: 
2024-05-02 18:30:04.367449: Epoch 36
2024-05-02 18:30:04.369101: Current learning rate: 0.00968
2024-05-02 18:31:09.790524: Validation loss did not improve from -0.50734. Patience: 22/50
2024-05-02 18:31:09.792970: train_loss -0.6772
2024-05-02 18:31:09.794909: val_loss -0.4975
2024-05-02 18:31:09.796080: Pseudo dice [0.7304]
2024-05-02 18:31:09.797560: Epoch time: 65.43 s
2024-05-02 18:31:11.075274: 
2024-05-02 18:31:11.077733: Epoch 37
2024-05-02 18:31:11.079200: Current learning rate: 0.00967
2024-05-02 18:32:16.507470: Validation loss did not improve from -0.50734. Patience: 23/50
2024-05-02 18:32:16.509457: train_loss -0.682
2024-05-02 18:32:16.511218: val_loss -0.4725
2024-05-02 18:32:16.512595: Pseudo dice [0.7286]
2024-05-02 18:32:16.513769: Epoch time: 65.44 s
2024-05-02 18:32:16.515500: Yayy! New best EMA pseudo Dice: 0.7157
2024-05-02 18:32:18.123665: 
2024-05-02 18:32:18.126366: Epoch 38
2024-05-02 18:32:18.128558: Current learning rate: 0.00966
2024-05-02 18:33:23.837481: Validation loss did not improve from -0.50734. Patience: 24/50
2024-05-02 18:33:23.839998: train_loss -0.6528
2024-05-02 18:33:23.841792: val_loss -0.4458
2024-05-02 18:33:23.842852: Pseudo dice [0.7126]
2024-05-02 18:33:23.844083: Epoch time: 65.72 s
2024-05-02 18:33:25.124533: 
2024-05-02 18:33:25.126866: Epoch 39
2024-05-02 18:33:25.127871: Current learning rate: 0.00965
2024-05-02 18:34:31.217716: Validation loss did not improve from -0.50734. Patience: 25/50
2024-05-02 18:34:31.219955: train_loss -0.6613
2024-05-02 18:34:31.222023: val_loss -0.4439
2024-05-02 18:34:31.223559: Pseudo dice [0.7007]
2024-05-02 18:34:31.224649: Epoch time: 66.1 s
2024-05-02 18:34:33.346214: 
2024-05-02 18:34:33.348651: Epoch 40
2024-05-02 18:34:33.350316: Current learning rate: 0.00964
2024-05-02 18:35:39.543704: Validation loss did not improve from -0.50734. Patience: 26/50
2024-05-02 18:35:39.545490: train_loss -0.6742
2024-05-02 18:35:39.546895: val_loss -0.4948
2024-05-02 18:35:39.548188: Pseudo dice [0.7416]
2024-05-02 18:35:39.549303: Epoch time: 66.2 s
2024-05-02 18:35:39.550598: Yayy! New best EMA pseudo Dice: 0.7167
2024-05-02 18:35:41.200346: 
2024-05-02 18:35:41.202860: Epoch 41
2024-05-02 18:35:41.204233: Current learning rate: 0.00963
2024-05-02 18:36:47.296629: Validation loss did not improve from -0.50734. Patience: 27/50
2024-05-02 18:36:47.298716: train_loss -0.6906
2024-05-02 18:36:47.300888: val_loss -0.4938
2024-05-02 18:36:47.302641: Pseudo dice [0.7369]
2024-05-02 18:36:47.304118: Epoch time: 66.1 s
2024-05-02 18:36:47.305155: Yayy! New best EMA pseudo Dice: 0.7187
2024-05-02 18:36:48.885730: 
2024-05-02 18:36:48.888304: Epoch 42
2024-05-02 18:36:48.889767: Current learning rate: 0.00962
2024-05-02 18:37:55.108948: Validation loss did not improve from -0.50734. Patience: 28/50
2024-05-02 18:37:55.110703: train_loss -0.7023
2024-05-02 18:37:55.112402: val_loss -0.478
2024-05-02 18:37:55.113593: Pseudo dice [0.7391]
2024-05-02 18:37:55.114768: Epoch time: 66.23 s
2024-05-02 18:37:55.115901: Yayy! New best EMA pseudo Dice: 0.7208
2024-05-02 18:37:56.753122: 
2024-05-02 18:37:56.755464: Epoch 43
2024-05-02 18:37:56.756807: Current learning rate: 0.00961
2024-05-02 18:39:02.808345: Validation loss did not improve from -0.50734. Patience: 29/50
2024-05-02 18:39:02.810292: train_loss -0.7119
2024-05-02 18:39:02.811536: val_loss -0.4903
2024-05-02 18:39:02.812521: Pseudo dice [0.7239]
2024-05-02 18:39:02.813507: Epoch time: 66.06 s
2024-05-02 18:39:02.814460: Yayy! New best EMA pseudo Dice: 0.7211
2024-05-02 18:39:04.383090: 
2024-05-02 18:39:04.385795: Epoch 44
2024-05-02 18:39:04.387164: Current learning rate: 0.0096
2024-05-02 18:40:10.499447: Validation loss did not improve from -0.50734. Patience: 30/50
2024-05-02 18:40:10.501544: train_loss -0.7052
2024-05-02 18:40:10.503453: val_loss -0.4932
2024-05-02 18:40:10.504879: Pseudo dice [0.7318]
2024-05-02 18:40:10.506510: Epoch time: 66.12 s
2024-05-02 18:40:10.864277: Yayy! New best EMA pseudo Dice: 0.7222
2024-05-02 18:40:12.428901: 
2024-05-02 18:40:12.431488: Epoch 45
2024-05-02 18:40:12.433962: Current learning rate: 0.00959
2024-05-02 18:41:18.383257: Validation loss did not improve from -0.50734. Patience: 31/50
2024-05-02 18:41:18.384859: train_loss -0.7023
2024-05-02 18:41:18.386281: val_loss -0.4512
2024-05-02 18:41:18.387553: Pseudo dice [0.7211]
2024-05-02 18:41:18.388789: Epoch time: 65.96 s
2024-05-02 18:41:19.600405: 
2024-05-02 18:41:19.602926: Epoch 46
2024-05-02 18:41:19.604305: Current learning rate: 0.00959
2024-05-02 18:42:25.338936: Validation loss did not improve from -0.50734. Patience: 32/50
2024-05-02 18:42:25.341809: train_loss -0.6918
2024-05-02 18:42:25.344467: val_loss -0.4871
2024-05-02 18:42:25.346444: Pseudo dice [0.7328]
2024-05-02 18:42:25.347809: Epoch time: 65.74 s
2024-05-02 18:42:25.349248: Yayy! New best EMA pseudo Dice: 0.7231
2024-05-02 18:42:26.911092: 
2024-05-02 18:42:26.913763: Epoch 47
2024-05-02 18:42:26.915426: Current learning rate: 0.00958
2024-05-02 18:43:32.562797: Validation loss did not improve from -0.50734. Patience: 33/50
2024-05-02 18:43:32.564581: train_loss -0.7064
2024-05-02 18:43:32.566230: val_loss -0.4704
2024-05-02 18:43:32.567465: Pseudo dice [0.7272]
2024-05-02 18:43:32.568806: Epoch time: 65.65 s
2024-05-02 18:43:32.570019: Yayy! New best EMA pseudo Dice: 0.7235
2024-05-02 18:43:34.123882: 
2024-05-02 18:43:34.126966: Epoch 48
2024-05-02 18:43:34.128507: Current learning rate: 0.00957
2024-05-02 18:44:39.705743: Validation loss did not improve from -0.50734. Patience: 34/50
2024-05-02 18:44:39.707409: train_loss -0.7007
2024-05-02 18:44:39.709106: val_loss -0.4629
2024-05-02 18:44:39.710238: Pseudo dice [0.7233]
2024-05-02 18:44:39.711403: Epoch time: 65.58 s
2024-05-02 18:44:40.945873: 
2024-05-02 18:44:40.947976: Epoch 49
2024-05-02 18:44:40.949757: Current learning rate: 0.00956
2024-05-02 18:45:46.553333: Validation loss did not improve from -0.50734. Patience: 35/50
2024-05-02 18:45:46.555031: train_loss -0.698
2024-05-02 18:45:46.556680: val_loss -0.4504
2024-05-02 18:45:46.558494: Pseudo dice [0.7202]
2024-05-02 18:45:46.560086: Epoch time: 65.61 s
2024-05-02 18:45:48.115589: 
2024-05-02 18:45:48.118277: Epoch 50
2024-05-02 18:45:48.120099: Current learning rate: 0.00955
2024-05-02 18:46:53.676207: Validation loss did not improve from -0.50734. Patience: 36/50
2024-05-02 18:46:53.678273: train_loss -0.7085
2024-05-02 18:46:53.680122: val_loss -0.4531
2024-05-02 18:46:53.681264: Pseudo dice [0.7226]
2024-05-02 18:46:53.682408: Epoch time: 65.56 s
2024-05-02 18:46:54.900794: 
2024-05-02 18:46:54.903740: Epoch 51
2024-05-02 18:46:54.905409: Current learning rate: 0.00954
2024-05-02 18:48:00.388824: Validation loss did not improve from -0.50734. Patience: 37/50
2024-05-02 18:48:00.390759: train_loss -0.7061
2024-05-02 18:48:00.392991: val_loss -0.4783
2024-05-02 18:48:00.394860: Pseudo dice [0.7358]
2024-05-02 18:48:00.396187: Epoch time: 65.49 s
2024-05-02 18:48:00.397298: Yayy! New best EMA pseudo Dice: 0.7244
2024-05-02 18:48:02.420285: 
2024-05-02 18:48:02.422328: Epoch 52
2024-05-02 18:48:02.423999: Current learning rate: 0.00953
2024-05-02 18:49:07.842861: Validation loss did not improve from -0.50734. Patience: 38/50
2024-05-02 18:49:07.844451: train_loss -0.715
2024-05-02 18:49:07.846447: val_loss -0.4916
2024-05-02 18:49:07.848756: Pseudo dice [0.7335]
2024-05-02 18:49:07.850329: Epoch time: 65.43 s
2024-05-02 18:49:07.851676: Yayy! New best EMA pseudo Dice: 0.7253
2024-05-02 18:49:09.438345: 
2024-05-02 18:49:09.441450: Epoch 53
2024-05-02 18:49:09.443244: Current learning rate: 0.00952
2024-05-02 18:50:14.808749: Validation loss did not improve from -0.50734. Patience: 39/50
2024-05-02 18:50:14.810891: train_loss -0.7194
2024-05-02 18:50:14.812360: val_loss -0.4511
2024-05-02 18:50:14.813575: Pseudo dice [0.7166]
2024-05-02 18:50:14.814897: Epoch time: 65.37 s
2024-05-02 18:50:16.028713: 
2024-05-02 18:50:16.031115: Epoch 54
2024-05-02 18:50:16.032358: Current learning rate: 0.00951
2024-05-02 18:51:21.635320: Validation loss did not improve from -0.50734. Patience: 40/50
2024-05-02 18:51:21.637255: train_loss -0.7195
2024-05-02 18:51:21.638522: val_loss -0.4781
2024-05-02 18:51:21.639856: Pseudo dice [0.714]
2024-05-02 18:51:21.641014: Epoch time: 65.61 s
2024-05-02 18:51:23.220652: 
2024-05-02 18:51:23.224624: Epoch 55
2024-05-02 18:51:23.227094: Current learning rate: 0.0095
2024-05-02 18:52:28.866464: Validation loss did not improve from -0.50734. Patience: 41/50
2024-05-02 18:52:28.868434: train_loss -0.7231
2024-05-02 18:52:28.870005: val_loss -0.4688
2024-05-02 18:52:28.871494: Pseudo dice [0.7326]
2024-05-02 18:52:28.873087: Epoch time: 65.65 s
2024-05-02 18:52:30.088640: 
2024-05-02 18:52:30.091093: Epoch 56
2024-05-02 18:52:30.092483: Current learning rate: 0.00949
2024-05-02 18:53:36.749614: Validation loss did not improve from -0.50734. Patience: 42/50
2024-05-02 18:53:36.752716: train_loss -0.72
2024-05-02 18:53:36.754418: val_loss -0.453
2024-05-02 18:53:36.755837: Pseudo dice [0.7223]
2024-05-02 18:53:36.757164: Epoch time: 66.67 s
2024-05-02 18:53:37.990951: 
2024-05-02 18:53:37.993993: Epoch 57
2024-05-02 18:53:37.995805: Current learning rate: 0.00949
2024-05-02 18:54:44.756769: Validation loss did not improve from -0.50734. Patience: 43/50
2024-05-02 18:54:44.758404: train_loss -0.7032
2024-05-02 18:54:44.759848: val_loss -0.4478
2024-05-02 18:54:44.760875: Pseudo dice [0.7136]
2024-05-02 18:54:44.762115: Epoch time: 66.77 s
2024-05-02 18:54:46.172706: 
2024-05-02 18:54:46.175404: Epoch 58
2024-05-02 18:54:46.177383: Current learning rate: 0.00948
2024-05-02 18:55:51.566382: Validation loss did not improve from -0.50734. Patience: 44/50
2024-05-02 18:55:51.568523: train_loss -0.7091
2024-05-02 18:55:51.570517: val_loss -0.4774
2024-05-02 18:55:51.571576: Pseudo dice [0.7347]
2024-05-02 18:55:51.572606: Epoch time: 65.4 s
2024-05-02 18:55:52.802609: 
2024-05-02 18:55:52.805067: Epoch 59
2024-05-02 18:55:52.806262: Current learning rate: 0.00947
2024-05-02 18:56:58.216094: Validation loss did not improve from -0.50734. Patience: 45/50
2024-05-02 18:56:58.218168: train_loss -0.7138
2024-05-02 18:56:58.219650: val_loss -0.476
2024-05-02 18:56:58.220858: Pseudo dice [0.7278]
2024-05-02 18:56:58.223173: Epoch time: 65.42 s
2024-05-02 18:56:59.995487: 
2024-05-02 18:56:59.997758: Epoch 60
2024-05-02 18:56:59.999013: Current learning rate: 0.00946
2024-05-02 18:58:05.360742: Validation loss did not improve from -0.50734. Patience: 46/50
2024-05-02 18:58:05.363191: train_loss -0.715
2024-05-02 18:58:05.365309: val_loss -0.4682
2024-05-02 18:58:05.366661: Pseudo dice [0.723]
2024-05-02 18:58:05.367928: Epoch time: 65.37 s
2024-05-02 18:58:06.583218: 
2024-05-02 18:58:06.586178: Epoch 61
2024-05-02 18:58:06.587642: Current learning rate: 0.00945
2024-05-02 18:59:11.992474: Validation loss did not improve from -0.50734. Patience: 47/50
2024-05-02 18:59:11.994591: train_loss -0.7114
2024-05-02 18:59:11.996667: val_loss -0.3981
2024-05-02 18:59:11.997799: Pseudo dice [0.6877]
2024-05-02 18:59:11.998761: Epoch time: 65.41 s
2024-05-02 18:59:13.240943: 
2024-05-02 18:59:13.243372: Epoch 62
2024-05-02 18:59:13.244673: Current learning rate: 0.00944
2024-05-02 19:00:18.662778: Validation loss did not improve from -0.50734. Patience: 48/50
2024-05-02 19:00:18.667699: train_loss -0.7041
2024-05-02 19:00:18.669290: val_loss -0.4341
2024-05-02 19:00:18.670523: Pseudo dice [0.709]
2024-05-02 19:00:18.671695: Epoch time: 65.43 s
2024-05-02 19:00:19.949393: 
2024-05-02 19:00:19.952444: Epoch 63
2024-05-02 19:00:19.954669: Current learning rate: 0.00943
2024-05-02 19:01:25.323062: Validation loss did not improve from -0.50734. Patience: 49/50
2024-05-02 19:01:25.325584: train_loss -0.7097
2024-05-02 19:01:25.327099: val_loss -0.4572
2024-05-02 19:01:25.328106: Pseudo dice [0.7185]
2024-05-02 19:01:25.329065: Epoch time: 65.38 s
2024-05-02 19:01:28.023570: 
2024-05-02 19:01:28.025935: Epoch 64
2024-05-02 19:01:28.027400: Current learning rate: 0.00942
2024-05-02 19:02:33.275822: Validation loss did not improve from -0.50734. Patience: 50/50
2024-05-02 19:02:33.277168: train_loss -0.7228
2024-05-02 19:02:33.278691: val_loss -0.4567
2024-05-02 19:02:33.280218: Pseudo dice [0.7106]
2024-05-02 19:02:33.281236: Epoch time: 65.26 s
2024-05-02 19:02:34.854819: Patience reached. Stopping training.
2024-05-02 19:02:36.208084: Training done.
2024-05-02 19:02:36.553436: Using splits from existing split file: /home/gridsan/nchutisilp/datasets/nnUNet_Datasets/nnUNet_preprocessed/Dataset302_Calcium_OCTv2/splits_final.json
2024-05-02 19:02:36.570741: The split file contains 3 splits.
2024-05-02 19:02:36.572820: Desired fold for training: 0
2024-05-02 19:02:36.574166: This split has 4 training and 2 validation cases.
2024-05-02 19:02:36.575713: predicting 101-019
2024-05-02 19:02:36.601169: 101-019, shape torch.Size([1, 375, 498, 498]), rank 0
2024-05-02 19:04:39.318595: predicting 704-003
2024-05-02 19:04:39.335856: 704-003, shape torch.Size([1, 375, 498, 498]), rank 0
2024-05-02 19:07:16.437695: Validation complete
2024-05-02 19:07:16.439148: Mean Validation Dice:  0.7066403234012134
wandb: 
wandb: Run history:
wandb:            ema_fg_dice 
wandb:   epoch_end_timestamps 
wandb: epoch_start_timestamps 
wandb:                    lrs 
wandb:           mean_fg_dice 
wandb:           train_losses 
wandb:             val_losses 
wandb: 
wandb: Run summary:
wandb:            ema_fg_dice 0.71858
wandb:   epoch_end_timestamps 1714690953.27703
wandb: epoch_start_timestamps 1714690888.02184
wandb:                    lrs 0.00942
wandb:           mean_fg_dice 0.7106
wandb:           train_losses -0.72275
wandb:             val_losses -0.45667
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/gridsan/nchutisilp/datasets/nnUNet_Datasets/nnUNet_results/Dataset302_Calcium_OCTv2/nnUNetTrainer__nnUNetPlans__3d_fullres/fold_0/wandb/offline-run-20240502_174806-rbxx4e1u
wandb: Find logs at: /home/gridsan/nchutisilp/datasets/nnUNet_Datasets/nnUNet_results/Dataset302_Calcium_OCTv2/nnUNetTrainer__nnUNetPlans__3d_fullres/fold_0/wandb/offline-run-20240502_174806-rbxx4e1u/logs
WARNING:urllib3.connectionpool:Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x7f7ab2241fa0>: Failed to establish a new connection: [Errno 101] Network is unreachable')': /api/4504800232407040/envelope/
WARNING:urllib3.connectionpool:Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x7f7a9bc87460>: Failed to establish a new connection: [Errno 101] Network is unreachable')': /api/4504800232407040/envelope/
WARNING:urllib3.connectionpool:Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x7f79fb1567c0>: Failed to establish a new connection: [Errno 101] Network is unreachable')': /api/4504800232407040/envelope/
WARNING:urllib3.connectionpool:Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x7f7a08c31f10>: Failed to establish a new connection: [Errno 101] Network is unreachable')': /api/4504800232407040/envelope/
WARNING:urllib3.connectionpool:Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x7f7a11bbb6d0>: Failed to establish a new connection: [Errno 101] Network is unreachable')': /api/4504800232407040/envelope/
WARNING:urllib3.connectionpool:Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x7f7ac1bdf6a0>: Failed to establish a new connection: [Errno 101] Network is unreachable')': /api/4504800232407040/envelope/
