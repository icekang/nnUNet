/home/gridsan/nchutisilp/.conda/envs/nnunet/bin/python
Begin training and evaluating FOLD 0 CONFIG 3d_32x160x128_b10 TRAINER nnUNetTrainerScaleAnalysis5 Genesis
wandb: Tracking run with wandb version 0.16.6
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.

############################
INFO: You are using the old nnU-Net default plans. We have updated our recommendations. Please consider using those instead! Read more here: https://github.com/MIC-DKFZ/nnUNet/blob/master/documentation/resenc_presets.md
############################

Using device: cuda:0

#######################################################################
Please cite the following paper when using nnU-Net:
Isensee, F., Jaeger, P. F., Kohl, S. A., Petersen, J., & Maier-Hein, K. H. (2021). nnU-Net: a self-configuring method for deep learning-based biomedical image segmentation. Nature methods, 18(2), 203-211.
#######################################################################

2024-08-12 15:52:50.699475: Using 8 processes for validation.
2024-08-12 15:52:50.716910: Using 12 processes for data augmentation.
2024-08-12 15:52:51.598926: Using torch.compile...
/home/gridsan/nchutisilp/.local/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn("The verbose parameter is deprecated. Please use get_last_lr() "
################### Loading pretrained weights from file  /home/gridsan/nchutisilp/datasets/ModelGenesisOutputs/ModelGenesisNNUNetPretrainingV2_noNorm_correct_orientation/Converted_nnUNet_Genesis_OCT_Best.pt ###################
Below is the list of overlapping blocks in pretrained model and nnUNet architecture:
encoder.stages.0.0.convs.0.conv.weight shape torch.Size([32, 1, 3, 3, 3])
encoder.stages.0.0.convs.0.conv.bias shape torch.Size([32])
encoder.stages.0.0.convs.0.norm.weight shape torch.Size([32])
encoder.stages.0.0.convs.0.norm.bias shape torch.Size([32])
encoder.stages.0.0.convs.0.all_modules.0.weight shape torch.Size([32, 1, 3, 3, 3])
encoder.stages.0.0.convs.0.all_modules.0.bias shape torch.Size([32])
encoder.stages.0.0.convs.0.all_modules.1.weight shape torch.Size([32])
encoder.stages.0.0.convs.0.all_modules.1.bias shape torch.Size([32])
encoder.stages.0.0.convs.1.conv.weight shape torch.Size([32, 32, 3, 3, 3])
encoder.stages.0.0.convs.1.conv.bias shape torch.Size([32])
encoder.stages.0.0.convs.1.norm.weight shape torch.Size([32])
encoder.stages.0.0.convs.1.norm.bias shape torch.Size([32])
encoder.stages.0.0.convs.1.all_modules.0.weight shape torch.Size([32, 32, 3, 3, 3])
encoder.stages.0.0.convs.1.all_modules.0.bias shape torch.Size([32])
encoder.stages.0.0.convs.1.all_modules.1.weight shape torch.Size([32])
encoder.stages.0.0.convs.1.all_modules.1.bias shape torch.Size([32])
encoder.stages.1.0.convs.0.conv.weight shape torch.Size([64, 32, 3, 3, 3])
encoder.stages.1.0.convs.0.conv.bias shape torch.Size([64])
encoder.stages.1.0.convs.0.norm.weight shape torch.Size([64])
encoder.stages.1.0.convs.0.norm.bias shape torch.Size([64])
encoder.stages.1.0.convs.0.all_modules.0.weight shape torch.Size([64, 32, 3, 3, 3])
encoder.stages.1.0.convs.0.all_modules.0.bias shape torch.Size([64])
encoder.stages.1.0.convs.0.all_modules.1.weight shape torch.Size([64])
encoder.stages.1.0.convs.0.all_modules.1.bias shape torch.Size([64])
encoder.stages.1.0.convs.1.conv.weight shape torch.Size([64, 64, 3, 3, 3])
encoder.stages.1.0.convs.1.conv.bias shape torch.Size([64])
encoder.stages.1.0.convs.1.norm.weight shape torch.Size([64])
encoder.stages.1.0.convs.1.norm.bias shape torch.Size([64])
encoder.stages.1.0.convs.1.all_modules.0.weight shape torch.Size([64, 64, 3, 3, 3])
encoder.stages.1.0.convs.1.all_modules.0.bias shape torch.Size([64])
encoder.stages.1.0.convs.1.all_modules.1.weight shape torch.Size([64])
encoder.stages.1.0.convs.1.all_modules.1.bias shape torch.Size([64])
encoder.stages.2.0.convs.0.conv.weight shape torch.Size([128, 64, 3, 3, 3])
encoder.stages.2.0.convs.0.conv.bias shape torch.Size([128])
encoder.stages.2.0.convs.0.norm.weight shape torch.Size([128])
encoder.stages.2.0.convs.0.norm.bias shape torch.Size([128])
encoder.stages.2.0.convs.0.all_modules.0.weight shape torch.Size([128, 64, 3, 3, 3])
encoder.stages.2.0.convs.0.all_modules.0.bias shape torch.Size([128])
encoder.stages.2.0.convs.0.all_modules.1.weight shape torch.Size([128])
encoder.stages.2.0.convs.0.all_modules.1.bias shape torch.Size([128])
encoder.stages.2.0.convs.1.conv.weight shape torch.Size([128, 128, 3, 3, 3])
encoder.stages.2.0.convs.1.conv.bias shape torch.Size([128])
encoder.stages.2.0.convs.1.norm.weight shape torch.Size([128])
encoder.stages.2.0.convs.1.norm.bias shape torch.Size([128])
encoder.stages.2.0.convs.1.all_modules.0.weight shape torch.Size([128, 128, 3, 3, 3])
encoder.stages.2.0.convs.1.all_modules.0.bias shape torch.Size([128])
encoder.stages.2.0.convs.1.all_modules.1.weight shape torch.Size([128])
encoder.stages.2.0.convs.1.all_modules.1.bias shape torch.Size([128])
encoder.stages.3.0.convs.0.conv.weight shape torch.Size([256, 128, 3, 3, 3])
encoder.stages.3.0.convs.0.conv.bias shape torch.Size([256])
encoder.stages.3.0.convs.0.norm.weight shape torch.Size([256])
encoder.stages.3.0.convs.0.norm.bias shape torch.Size([256])
encoder.stages.3.0.convs.0.all_modules.0.weight shape torch.Size([256, 128, 3, 3, 3])
encoder.stages.3.0.convs.0.all_modules.0.bias shape torch.Size([256])
encoder.stages.3.0.convs.0.all_modules.1.weight shape torch.Size([256])
encoder.stages.3.0.convs.0.all_modules.1.bias shape torch.Size([256])
encoder.stages.3.0.convs.1.conv.weight shape torch.Size([256, 256, 3, 3, 3])
encoder.stages.3.0.convs.1.conv.bias shape torch.Size([256])
encoder.stages.3.0.convs.1.norm.weight shape torch.Size([256])
encoder.stages.3.0.convs.1.norm.bias shape torch.Size([256])
encoder.stages.3.0.convs.1.all_modules.0.weight shape torch.Size([256, 256, 3, 3, 3])
encoder.stages.3.0.convs.1.all_modules.0.bias shape torch.Size([256])
encoder.stages.3.0.convs.1.all_modules.1.weight shape torch.Size([256])
encoder.stages.3.0.convs.1.all_modules.1.bias shape torch.Size([256])
encoder.stages.4.0.convs.0.conv.weight shape torch.Size([320, 256, 3, 3, 3])
encoder.stages.4.0.convs.0.conv.bias shape torch.Size([320])
encoder.stages.4.0.convs.0.norm.weight shape torch.Size([320])
encoder.stages.4.0.convs.0.norm.bias shape torch.Size([320])
encoder.stages.4.0.convs.0.all_modules.0.weight shape torch.Size([320, 256, 3, 3, 3])
encoder.stages.4.0.convs.0.all_modules.0.bias shape torch.Size([320])
encoder.stages.4.0.convs.0.all_modules.1.weight shape torch.Size([320])
encoder.stages.4.0.convs.0.all_modules.1.bias shape torch.Size([320])
encoder.stages.4.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.4.0.convs.1.conv.bias shape torch.Size([320])
encoder.stages.4.0.convs.1.norm.weight shape torch.Size([320])
encoder.stages.4.0.convs.1.norm.bias shape torch.Size([320])
encoder.stages.4.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.4.0.convs.1.all_modules.0.bias shape torch.Size([320])
encoder.stages.4.0.convs.1.all_modules.1.weight shape torch.Size([320])
encoder.stages.4.0.convs.1.all_modules.1.bias shape torch.Size([320])
encoder.stages.5.0.convs.0.conv.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.5.0.convs.0.conv.bias shape torch.Size([320])
encoder.stages.5.0.convs.0.norm.weight shape torch.Size([320])
encoder.stages.5.0.convs.0.norm.bias shape torch.Size([320])
encoder.stages.5.0.convs.0.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.5.0.convs.0.all_modules.0.bias shape torch.Size([320])
encoder.stages.5.0.convs.0.all_modules.1.weight shape torch.Size([320])
encoder.stages.5.0.convs.0.all_modules.1.bias shape torch.Size([320])
encoder.stages.5.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.5.0.convs.1.conv.bias shape torch.Size([320])
encoder.stages.5.0.convs.1.norm.weight shape torch.Size([320])
encoder.stages.5.0.convs.1.norm.bias shape torch.Size([320])
encoder.stages.5.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
encoder.stages.5.0.convs.1.all_modules.0.bias shape torch.Size([320])
encoder.stages.5.0.convs.1.all_modules.1.weight shape torch.Size([320])
encoder.stages.5.0.convs.1.all_modules.1.bias shape torch.Size([320])
decoder.encoder.stages.0.0.convs.0.conv.weight shape torch.Size([32, 1, 3, 3, 3])
decoder.encoder.stages.0.0.convs.0.conv.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.norm.weight shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.norm.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.all_modules.0.weight shape torch.Size([32, 1, 3, 3, 3])
decoder.encoder.stages.0.0.convs.0.all_modules.0.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.all_modules.1.weight shape torch.Size([32])
decoder.encoder.stages.0.0.convs.0.all_modules.1.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.conv.weight shape torch.Size([32, 32, 3, 3, 3])
decoder.encoder.stages.0.0.convs.1.conv.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.norm.weight shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.norm.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.all_modules.0.weight shape torch.Size([32, 32, 3, 3, 3])
decoder.encoder.stages.0.0.convs.1.all_modules.0.bias shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.all_modules.1.weight shape torch.Size([32])
decoder.encoder.stages.0.0.convs.1.all_modules.1.bias shape torch.Size([32])
decoder.encoder.stages.1.0.convs.0.conv.weight shape torch.Size([64, 32, 3, 3, 3])
decoder.encoder.stages.1.0.convs.0.conv.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.norm.weight shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.norm.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.all_modules.0.weight shape torch.Size([64, 32, 3, 3, 3])
decoder.encoder.stages.1.0.convs.0.all_modules.0.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.all_modules.1.weight shape torch.Size([64])
decoder.encoder.stages.1.0.convs.0.all_modules.1.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.conv.weight shape torch.Size([64, 64, 3, 3, 3])
decoder.encoder.stages.1.0.convs.1.conv.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.norm.weight shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.norm.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.all_modules.0.weight shape torch.Size([64, 64, 3, 3, 3])
decoder.encoder.stages.1.0.convs.1.all_modules.0.bias shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.all_modules.1.weight shape torch.Size([64])
decoder.encoder.stages.1.0.convs.1.all_modules.1.bias shape torch.Size([64])
decoder.encoder.stages.2.0.convs.0.conv.weight shape torch.Size([128, 64, 3, 3, 3])
decoder.encoder.stages.2.0.convs.0.conv.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.norm.weight shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.norm.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.all_modules.0.weight shape torch.Size([128, 64, 3, 3, 3])
decoder.encoder.stages.2.0.convs.0.all_modules.0.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.all_modules.1.weight shape torch.Size([128])
decoder.encoder.stages.2.0.convs.0.all_modules.1.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.conv.weight shape torch.Size([128, 128, 3, 3, 3])
decoder.encoder.stages.2.0.convs.1.conv.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.norm.weight shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.norm.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.all_modules.0.weight shape torch.Size([128, 128, 3, 3, 3])
decoder.encoder.stages.2.0.convs.1.all_modules.0.bias shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.all_modules.1.weight shape torch.Size([128])
decoder.encoder.stages.2.0.convs.1.all_modules.1.bias shape torch.Size([128])
decoder.encoder.stages.3.0.convs.0.conv.weight shape torch.Size([256, 128, 3, 3, 3])
decoder.encoder.stages.3.0.convs.0.conv.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.norm.weight shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.norm.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.all_modules.0.weight shape torch.Size([256, 128, 3, 3, 3])
decoder.encoder.stages.3.0.convs.0.all_modules.0.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.all_modules.1.weight shape torch.Size([256])
decoder.encoder.stages.3.0.convs.0.all_modules.1.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.conv.weight shape torch.Size([256, 256, 3, 3, 3])
decoder.encoder.stages.3.0.convs.1.conv.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.norm.weight shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.norm.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.all_modules.0.weight shape torch.Size([256, 256, 3, 3, 3])
decoder.encoder.stages.3.0.convs.1.all_modules.0.bias shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.all_modules.1.weight shape torch.Size([256])
decoder.encoder.stages.3.0.convs.1.all_modules.1.bias shape torch.Size([256])
decoder.encoder.stages.4.0.convs.0.conv.weight shape torch.Size([320, 256, 3, 3, 3])
decoder.encoder.stages.4.0.convs.0.conv.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.norm.weight shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.norm.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.all_modules.0.weight shape torch.Size([320, 256, 3, 3, 3])
decoder.encoder.stages.4.0.convs.0.all_modules.0.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.all_modules.1.weight shape torch.Size([320])
decoder.encoder.stages.4.0.convs.0.all_modules.1.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.4.0.convs.1.conv.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.norm.weight shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.norm.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.4.0.convs.1.all_modules.0.bias shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.all_modules.1.weight shape torch.Size([320])
decoder.encoder.stages.4.0.convs.1.all_modules.1.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.conv.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.5.0.convs.0.conv.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.norm.weight shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.norm.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.5.0.convs.0.all_modules.0.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.all_modules.1.weight shape torch.Size([320])
decoder.encoder.stages.5.0.convs.0.all_modules.1.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.5.0.convs.1.conv.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.norm.weight shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.norm.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.encoder.stages.5.0.convs.1.all_modules.0.bias shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.all_modules.1.weight shape torch.Size([320])
decoder.encoder.stages.5.0.convs.1.all_modules.1.bias shape torch.Size([320])
decoder.stages.0.convs.0.conv.weight shape torch.Size([320, 640, 3, 3, 3])
decoder.stages.0.convs.0.conv.bias shape torch.Size([320])
decoder.stages.0.convs.0.norm.weight shape torch.Size([320])
decoder.stages.0.convs.0.norm.bias shape torch.Size([320])
decoder.stages.0.convs.0.all_modules.0.weight shape torch.Size([320, 640, 3, 3, 3])
decoder.stages.0.convs.0.all_modules.0.bias shape torch.Size([320])
decoder.stages.0.convs.0.all_modules.1.weight shape torch.Size([320])
decoder.stages.0.convs.0.all_modules.1.bias shape torch.Size([320])
decoder.stages.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.stages.0.convs.1.conv.bias shape torch.Size([320])
decoder.stages.0.convs.1.norm.weight shape torch.Size([320])
decoder.stages.0.convs.1.norm.bias shape torch.Size([320])
decoder.stages.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])
decoder.stages.0.convs.1.all_modules.0.bias shape torch.Size([320])
decoder.stages.0.convs.1.all_modules.1.weight shape torch.Size([320])
decoder.stages.0.convs.1.all_modules.1.bias shape torch.Size([320])
decoder.stages.1.convs.0.conv.weight shape torch.Size([256, 512, 3, 3, 3])
decoder.stages.1.convs.0.conv.bias shape torch.Size([256])
decoder.stages.1.convs.0.norm.weight shape torch.Size([256])
decoder.stages.1.convs.0.norm.bias shape torch.Size([256])
decoder.stages.1.convs.0.all_modules.0.weight shape torch.Size([256, 512, 3, 3, 3])
decoder.stages.1.convs.0.all_modules.0.bias shape torch.Size([256])
decoder.stages.1.convs.0.all_modules.1.weight shape torch.Size([256])
decoder.stages.1.convs.0.all_modules.1.bias shape torch.Size([256])
decoder.stages.1.convs.1.conv.weight shape torch.Size([256, 256, 3, 3, 3])
decoder.stages.1.convs.1.conv.bias shape torch.Size([256])
decoder.stages.1.convs.1.norm.weight shape torch.Size([256])
decoder.stages.1.convs.1.norm.bias shape torch.Size([256])
decoder.stages.1.convs.1.all_modules.0.weight shape torch.Size([256, 256, 3, 3, 3])
decoder.stages.1.convs.1.all_modules.0.bias shape torch.Size([256])
decoder.stages.1.convs.1.all_modules.1.weight shape torch.Size([256])
decoder.stages.1.convs.1.all_modules.1.bias shape torch.Size([256])
decoder.stages.2.convs.0.conv.weight shape torch.Size([128, 256, 3, 3, 3])
decoder.stages.2.convs.0.conv.bias shape torch.Size([128])
decoder.stages.2.convs.0.norm.weight shape torch.Size([128])
decoder.stages.2.convs.0.norm.bias shape torch.Size([128])
decoder.stages.2.convs.0.all_modules.0.weight shape torch.Size([128, 256, 3, 3, 3])
decoder.stages.2.convs.0.all_modules.0.bias shape torch.Size([128])
decoder.stages.2.convs.0.all_modules.1.weight shape torch.Size([128])
decoder.stages.2.convs.0.all_modules.1.bias shape torch.Size([128])
decoder.stages.2.convs.1.conv.weight shape torch.Size([128, 128, 3, 3, 3])
decoder.stages.2.convs.1.conv.bias shape torch.Size([128])
decoder.stages.2.convs.1.norm.weight shape torch.Size([128])
decoder.stages.2.convs.1.norm.bias shape torch.Size([128])
decoder.stages.2.convs.1.all_modules.0.weight shape torch.Size([128, 128, 3, 3, 3])
decoder.stages.2.convs.1.all_modules.0.bias shape torch.Size([128])
decoder.stages.2.convs.1.all_modules.1.weight shape torch.Size([128])
decoder.stages.2.convs.1.all_modules.1.bias shape torch.Size([128])
decoder.stages.3.convs.0.conv.weight shape torch.Size([64, 128, 3, 3, 3])
decoder.stages.3.convs.0.conv.bias shape torch.Size([64])
decoder.stages.3.convs.0.norm.weight shape torch.Size([64])
decoder.stages.3.convs.0.norm.bias shape torch.Size([64])
decoder.stages.3.convs.0.all_modules.0.weight shape torch.Size([64, 128, 3, 3, 3])
decoder.stages.3.convs.0.all_modules.0.bias shape torch.Size([64])
decoder.stages.3.convs.0.all_modules.1.weight shape torch.Size([64])
decoder.stages.3.convs.0.all_modules.1.bias shape torch.Size([64])
decoder.stages.3.convs.1.conv.weight shape torch.Size([64, 64, 3, 3, 3])
decoder.stages.3.convs.1.conv.bias shape torch.Size([64])
decoder.stages.3.convs.1.norm.weight shape torch.Size([64])
decoder.stages.3.convs.1.norm.bias shape torch.Size([64])
decoder.stages.3.convs.1.all_modules.0.weight shape torch.Size([64, 64, 3, 3, 3])
decoder.stages.3.convs.1.all_modules.0.bias shape torch.Size([64])
decoder.stages.3.convs.1.all_modules.1.weight shape torch.Size([64])
decoder.stages.3.convs.1.all_modules.1.bias shape torch.Size([64])
decoder.stages.4.convs.0.conv.weight shape torch.Size([32, 64, 3, 3, 3])
decoder.stages.4.convs.0.conv.bias shape torch.Size([32])
decoder.stages.4.convs.0.norm.weight shape torch.Size([32])
decoder.stages.4.convs.0.norm.bias shape torch.Size([32])
decoder.stages.4.convs.0.all_modules.0.weight shape torch.Size([32, 64, 3, 3, 3])
decoder.stages.4.convs.0.all_modules.0.bias shape torch.Size([32])
decoder.stages.4.convs.0.all_modules.1.weight shape torch.Size([32])
decoder.stages.4.convs.0.all_modules.1.bias shape torch.Size([32])
decoder.stages.4.convs.1.conv.weight shape torch.Size([32, 32, 3, 3, 3])
decoder.stages.4.convs.1.conv.bias shape torch.Size([32])
decoder.stages.4.convs.1.norm.weight shape torch.Size([32])
decoder.stages.4.convs.1.norm.bias shape torch.Size([32])
decoder.stages.4.convs.1.all_modules.0.weight shape torch.Size([32, 32, 3, 3, 3])
decoder.stages.4.convs.1.all_modules.0.bias shape torch.Size([32])
decoder.stages.4.convs.1.all_modules.1.weight shape torch.Size([32])
decoder.stages.4.convs.1.all_modules.1.bias shape torch.Size([32])
decoder.transpconvs.0.weight shape torch.Size([320, 320, 1, 2, 2])
decoder.transpconvs.0.bias shape torch.Size([320])
decoder.transpconvs.1.weight shape torch.Size([320, 256, 2, 2, 2])
decoder.transpconvs.1.bias shape torch.Size([256])
decoder.transpconvs.2.weight shape torch.Size([256, 128, 2, 2, 2])
decoder.transpconvs.2.bias shape torch.Size([128])
decoder.transpconvs.3.weight shape torch.Size([128, 64, 2, 2, 2])
decoder.transpconvs.3.bias shape torch.Size([64])
decoder.transpconvs.4.weight shape torch.Size([64, 32, 2, 2, 2])
decoder.transpconvs.4.bias shape torch.Size([32])
################### Done ###################
2024-08-12 15:52:52.513411: do_dummy_2d_data_aug: True
2024-08-12 15:52:52.530088: Using splits from existing split file: /home/gridsan/nchutisilp/datasets/nnUNet_Datasets/nnUNet_preprocessed/Dataset302_Calcium_OCTv2/splits_final_5.json
2024-08-12 15:52:52.547478: The split file contains 3 splits.
2024-08-12 15:52:52.549135: Desired fold for training: 0
2024-08-12 15:52:52.550421: This split has 3 training and 2 validation cases.
2024-08-12 15:52:52.551723: WARNING: Some validation cases are also in the training set. Please check the splits.json or ignore if this is intentional.
using pin_memory on device 0
using pin_memory on device 0

This is the configuration used by this training:
Configuration name: 3d_32x160x128_b10
 {'data_identifier': 'nnUNetPlans_3d_fullres', 'preprocessor_name': 'DefaultPreprocessor', 'batch_size': 10, 'patch_size': [32, 160, 128], 'median_image_size_in_voxels': [375.0, 498.0, 498.0], 'spacing': [1.0, 1.0, 1.0], 'normalization_schemes': ['ZScoreNormalization'], 'use_mask_for_norm': [False], 'resampling_fn_data': 'resample_data_or_seg_to_shape', 'resampling_fn_seg': 'resample_data_or_seg_to_shape', 'resampling_fn_data_kwargs': {'is_seg': False, 'order': 3, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_seg_kwargs': {'is_seg': True, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_probabilities': 'resample_data_or_seg_to_shape', 'resampling_fn_probabilities_kwargs': {'is_seg': False, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'architecture': {'network_class_name': 'dynamic_network_architectures.architectures.unet.PlainConvUNet', 'arch_kwargs': {'n_stages': 6, 'features_per_stage': [32, 64, 128, 256, 320, 320], 'conv_op': 'torch.nn.modules.conv.Conv3d', 'kernel_sizes': [[3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3]], 'strides': [[1, 1, 1], [2, 2, 2], [2, 2, 2], [2, 2, 2], [2, 2, 2], [1, 2, 2]], 'n_conv_per_stage': [2, 2, 2, 2, 2, 2], 'n_conv_per_stage_decoder': [2, 2, 2, 2, 2], 'conv_bias': True, 'norm_op': 'torch.nn.modules.instancenorm.InstanceNorm3d', 'norm_op_kwargs': {'eps': 1e-05, 'affine': True}, 'dropout_op': None, 'dropout_op_kwargs': None, 'nonlin': 'torch.nn.LeakyReLU', 'nonlin_kwargs': {'inplace': True}, 'deep_supervision': True}, '_kw_requires_import': ['conv_op', 'norm_op', 'dropout_op', 'nonlin']}, 'batch_dice': True, 'inherits_from': '3d_fullres'} 

These are the global plan.json settings:
 {'dataset_name': 'Dataset302_Calcium_OCTv2', 'plans_name': 'nnUNetPlans', 'original_median_spacing_after_transp': [1.0, 1.0, 1.0], 'original_median_shape_after_transp': [375, 498, 498], 'image_reader_writer': 'SimpleITKIO', 'transpose_forward': [0, 1, 2], 'transpose_backward': [0, 1, 2], 'experiment_planner_used': 'ExperimentPlanner', 'label_manager': 'LabelManager', 'foreground_intensity_properties_per_channel': {'0': {'max': 0.4977976381778717, 'mean': 0.13332499563694, 'median': 0.08871842920780182, 'min': 0.0, 'percentile_00_5': 0.014754901640117168, 'percentile_99_5': 0.4977976381778717, 'std': 0.12022686749696732}}} 

2024-08-12 15:52:55.281284: unpacking dataset...
2024-08-12 15:52:59.606716: unpacking done...
2024-08-12 15:52:59.663070: Unable to plot network architecture: nnUNet_compile is enabled!
2024-08-12 15:52:59.712675: 
2024-08-12 15:52:59.714530: Epoch 0
2024-08-12 15:52:59.716061: Current learning rate: 0.01
2024-08-12 15:57:39.467273: Validation loss improved from 1000.00000 to -0.27430! Patience: 0/50
2024-08-12 15:57:39.468787: train_loss -0.1862
2024-08-12 15:57:39.470391: val_loss -0.2743
2024-08-12 15:57:39.472027: Pseudo dice [0.5897]
2024-08-12 15:57:39.473305: Epoch time: 279.76 s
2024-08-12 15:57:39.474785: Yayy! New best EMA pseudo Dice: 0.5897
2024-08-12 15:57:41.411940: 
2024-08-12 15:57:41.413399: Epoch 1
2024-08-12 15:57:41.414580: Current learning rate: 0.00999
2024-08-12 16:03:14.958012: Validation loss did not improve from -0.27430. Patience: 1/50
2024-08-12 16:03:14.959331: train_loss -0.4054
2024-08-12 16:03:14.960447: val_loss -0.248
2024-08-12 16:03:14.961485: Pseudo dice [0.5773]
2024-08-12 16:03:14.962563: Epoch time: 333.55 s
2024-08-12 16:03:16.324223: 
2024-08-12 16:03:16.325739: Epoch 2
2024-08-12 16:03:16.327308: Current learning rate: 0.00998
2024-08-12 16:10:14.246998: Validation loss improved from -0.27430 to -0.37572! Patience: 1/50
2024-08-12 16:10:14.249217: train_loss -0.4897
2024-08-12 16:10:14.250314: val_loss -0.3757
2024-08-12 16:10:14.251337: Pseudo dice [0.6666]
2024-08-12 16:10:14.252328: Epoch time: 417.93 s
2024-08-12 16:10:14.253253: Yayy! New best EMA pseudo Dice: 0.5963
2024-08-12 16:10:16.063939: 
2024-08-12 16:10:16.065541: Epoch 3
2024-08-12 16:10:16.067302: Current learning rate: 0.00997
2024-08-12 16:17:45.739180: Validation loss improved from -0.37572 to -0.41586! Patience: 0/50
2024-08-12 16:17:45.740536: train_loss -0.5322
2024-08-12 16:17:45.741812: val_loss -0.4159
2024-08-12 16:17:45.743021: Pseudo dice [0.6699]
2024-08-12 16:17:45.744228: Epoch time: 449.68 s
2024-08-12 16:17:45.745335: Yayy! New best EMA pseudo Dice: 0.6037
2024-08-12 16:17:47.547633: 
2024-08-12 16:17:47.549006: Epoch 4
2024-08-12 16:17:47.550128: Current learning rate: 0.00996
2024-08-12 16:23:26.497398: Validation loss did not improve from -0.41586. Patience: 1/50
2024-08-12 16:23:26.498756: train_loss -0.5492
2024-08-12 16:23:26.499904: val_loss -0.3958
2024-08-12 16:23:26.501022: Pseudo dice [0.6594]
2024-08-12 16:23:26.502095: Epoch time: 338.95 s
2024-08-12 16:23:27.298759: Yayy! New best EMA pseudo Dice: 0.6092
2024-08-12 16:23:29.178056: 
2024-08-12 16:23:29.183134: Epoch 5
2024-08-12 16:23:29.184932: Current learning rate: 0.00995
2024-08-12 16:31:03.697943: Validation loss improved from -0.41586 to -0.41589! Patience: 1/50
2024-08-12 16:31:03.699410: train_loss -0.5625
2024-08-12 16:31:03.700873: val_loss -0.4159
2024-08-12 16:31:03.702272: Pseudo dice [0.6795]
2024-08-12 16:31:03.703617: Epoch time: 454.52 s
2024-08-12 16:31:03.704955: Yayy! New best EMA pseudo Dice: 0.6163
2024-08-12 16:31:05.564700: 
2024-08-12 16:31:05.566120: Epoch 6
2024-08-12 16:31:05.567411: Current learning rate: 0.00995
2024-08-12 16:38:24.204939: Validation loss did not improve from -0.41589. Patience: 1/50
2024-08-12 16:38:24.206370: train_loss -0.5789
2024-08-12 16:38:24.207736: val_loss -0.4117
2024-08-12 16:38:24.209125: Pseudo dice [0.667]
2024-08-12 16:38:24.210292: Epoch time: 438.64 s
2024-08-12 16:38:24.211469: Yayy! New best EMA pseudo Dice: 0.6213
2024-08-12 16:38:26.070714: 
2024-08-12 16:38:26.072028: Epoch 7
2024-08-12 16:38:26.073168: Current learning rate: 0.00994
2024-08-12 16:44:47.200105: Validation loss improved from -0.41589 to -0.42329! Patience: 1/50
2024-08-12 16:44:47.201402: train_loss -0.5859
2024-08-12 16:44:47.202497: val_loss -0.4233
2024-08-12 16:44:47.203507: Pseudo dice [0.6814]
2024-08-12 16:44:47.204598: Epoch time: 381.13 s
2024-08-12 16:44:47.205587: Yayy! New best EMA pseudo Dice: 0.6273
2024-08-12 16:44:49.122845: 
2024-08-12 16:44:49.124253: Epoch 8
2024-08-12 16:44:49.125348: Current learning rate: 0.00993
2024-08-12 16:51:44.563523: Validation loss improved from -0.42329 to -0.44898! Patience: 0/50
2024-08-12 16:51:44.565186: train_loss -0.609
2024-08-12 16:51:44.566504: val_loss -0.449
2024-08-12 16:51:44.567688: Pseudo dice [0.6914]
2024-08-12 16:51:44.568837: Epoch time: 415.44 s
2024-08-12 16:51:44.569970: Yayy! New best EMA pseudo Dice: 0.6337
2024-08-12 16:51:46.876408: 
2024-08-12 16:51:46.878151: Epoch 9
2024-08-12 16:51:46.879565: Current learning rate: 0.00992
2024-08-12 16:58:03.526928: Validation loss did not improve from -0.44898. Patience: 1/50
2024-08-12 16:58:03.539370: train_loss -0.6164
2024-08-12 16:58:03.540554: val_loss -0.3803
2024-08-12 16:58:03.541674: Pseudo dice [0.6624]
2024-08-12 16:58:03.542810: Epoch time: 376.66 s
2024-08-12 16:58:04.062464: Yayy! New best EMA pseudo Dice: 0.6366
2024-08-12 16:58:05.883739: 
2024-08-12 16:58:05.885498: Epoch 10
2024-08-12 16:58:05.887153: Current learning rate: 0.00991
2024-08-12 17:05:22.952247: Validation loss did not improve from -0.44898. Patience: 2/50
2024-08-12 17:05:22.954115: train_loss -0.6208
2024-08-12 17:05:22.956144: val_loss -0.3514
2024-08-12 17:05:22.957199: Pseudo dice [0.6619]
2024-08-12 17:05:22.958342: Epoch time: 437.07 s
2024-08-12 17:05:22.959307: Yayy! New best EMA pseudo Dice: 0.6391
2024-08-12 17:05:24.848789: 
2024-08-12 17:05:24.850249: Epoch 11
2024-08-12 17:05:24.851494: Current learning rate: 0.0099
2024-08-12 17:12:45.680517: Validation loss did not improve from -0.44898. Patience: 3/50
2024-08-12 17:12:45.681892: train_loss -0.6291
2024-08-12 17:12:45.683699: val_loss -0.3902
2024-08-12 17:12:45.685281: Pseudo dice [0.6589]
2024-08-12 17:12:45.686570: Epoch time: 440.84 s
2024-08-12 17:12:45.688024: Yayy! New best EMA pseudo Dice: 0.6411
2024-08-12 17:12:47.445490: 
2024-08-12 17:12:47.446800: Epoch 12
2024-08-12 17:12:47.447799: Current learning rate: 0.00989
2024-08-12 17:19:52.189008: Validation loss did not improve from -0.44898. Patience: 4/50
2024-08-12 17:19:52.190472: train_loss -0.6418
2024-08-12 17:19:52.191678: val_loss -0.4225
2024-08-12 17:19:52.192787: Pseudo dice [0.6749]
2024-08-12 17:19:52.193896: Epoch time: 424.75 s
2024-08-12 17:19:52.195127: Yayy! New best EMA pseudo Dice: 0.6445
2024-08-12 17:19:53.970548: 
2024-08-12 17:19:53.972339: Epoch 13
2024-08-12 17:19:53.973757: Current learning rate: 0.00988
2024-08-12 17:26:57.295843: Validation loss did not improve from -0.44898. Patience: 5/50
2024-08-12 17:26:57.297176: train_loss -0.6553
2024-08-12 17:26:57.298530: val_loss -0.4062
2024-08-12 17:26:57.299875: Pseudo dice [0.6812]
2024-08-12 17:26:57.301167: Epoch time: 423.33 s
2024-08-12 17:26:57.302508: Yayy! New best EMA pseudo Dice: 0.6482
2024-08-12 17:26:59.098060: 
2024-08-12 17:26:59.099803: Epoch 14
2024-08-12 17:26:59.101144: Current learning rate: 0.00987
2024-08-12 17:34:15.286533: Validation loss did not improve from -0.44898. Patience: 6/50
2024-08-12 17:34:15.287775: train_loss -0.6599
2024-08-12 17:34:15.289087: val_loss -0.4119
2024-08-12 17:34:15.290451: Pseudo dice [0.6789]
2024-08-12 17:34:15.291920: Epoch time: 436.19 s
2024-08-12 17:34:15.669343: Yayy! New best EMA pseudo Dice: 0.6512
2024-08-12 17:34:17.452960: 
2024-08-12 17:34:17.454782: Epoch 15
2024-08-12 17:34:17.455949: Current learning rate: 0.00986
2024-08-12 17:41:24.426681: Validation loss did not improve from -0.44898. Patience: 7/50
2024-08-12 17:41:24.428419: train_loss -0.664
2024-08-12 17:41:24.429879: val_loss -0.4445
2024-08-12 17:41:24.431399: Pseudo dice [0.6955]
2024-08-12 17:41:24.432979: Epoch time: 426.98 s
2024-08-12 17:41:24.434748: Yayy! New best EMA pseudo Dice: 0.6557
2024-08-12 17:41:26.403137: 
2024-08-12 17:41:26.405300: Epoch 16
2024-08-12 17:41:26.406796: Current learning rate: 0.00986
2024-08-12 17:47:16.626660: Validation loss did not improve from -0.44898. Patience: 8/50
2024-08-12 17:47:16.628290: train_loss -0.6671
2024-08-12 17:47:16.629488: val_loss -0.3951
2024-08-12 17:47:16.630646: Pseudo dice [0.6734]
2024-08-12 17:47:16.631870: Epoch time: 350.23 s
2024-08-12 17:47:16.632901: Yayy! New best EMA pseudo Dice: 0.6574
2024-08-12 17:47:18.495920: 
2024-08-12 17:47:18.497921: Epoch 17
2024-08-12 17:47:18.499500: Current learning rate: 0.00985
2024-08-12 17:54:18.752521: Validation loss did not improve from -0.44898. Patience: 9/50
2024-08-12 17:54:18.754265: train_loss -0.6666
2024-08-12 17:54:18.756565: val_loss -0.4187
2024-08-12 17:54:18.758641: Pseudo dice [0.6937]
2024-08-12 17:54:18.760096: Epoch time: 420.26 s
2024-08-12 17:54:18.761477: Yayy! New best EMA pseudo Dice: 0.6611
2024-08-12 17:54:20.681234: 
2024-08-12 17:54:20.683126: Epoch 18
2024-08-12 17:54:20.684705: Current learning rate: 0.00984
2024-08-12 18:01:20.284053: Validation loss did not improve from -0.44898. Patience: 10/50
2024-08-12 18:01:20.285668: train_loss -0.6783
2024-08-12 18:01:20.286971: val_loss -0.3796
2024-08-12 18:01:20.288207: Pseudo dice [0.6784]
2024-08-12 18:01:20.289355: Epoch time: 419.61 s
2024-08-12 18:01:20.290447: Yayy! New best EMA pseudo Dice: 0.6628
2024-08-12 18:01:22.181025: 
2024-08-12 18:01:22.182619: Epoch 19
2024-08-12 18:01:22.184050: Current learning rate: 0.00983
2024-08-12 18:07:42.045367: Validation loss did not improve from -0.44898. Patience: 11/50
2024-08-12 18:07:42.048063: train_loss -0.6884
2024-08-12 18:07:42.050723: val_loss -0.3778
2024-08-12 18:07:42.052355: Pseudo dice [0.6574]
2024-08-12 18:07:42.053870: Epoch time: 379.87 s
2024-08-12 18:07:45.874214: 
2024-08-12 18:07:45.875885: Epoch 20
2024-08-12 18:07:45.876920: Current learning rate: 0.00982
2024-08-12 18:15:26.396351: Validation loss improved from -0.44898 to -0.46362! Patience: 11/50
2024-08-12 18:15:26.397866: train_loss -0.6938
2024-08-12 18:15:26.399733: val_loss -0.4636
2024-08-12 18:15:26.401361: Pseudo dice [0.7059]
2024-08-12 18:15:26.403188: Epoch time: 460.52 s
2024-08-12 18:15:26.404406: Yayy! New best EMA pseudo Dice: 0.6666
2024-08-12 18:15:28.261798: 
2024-08-12 18:15:28.263367: Epoch 21
2024-08-12 18:15:28.264709: Current learning rate: 0.00981
2024-08-12 18:22:58.037408: Validation loss did not improve from -0.46362. Patience: 1/50
2024-08-12 18:22:58.039062: train_loss -0.7087
2024-08-12 18:22:58.040551: val_loss -0.4196
2024-08-12 18:22:58.041777: Pseudo dice [0.691]
2024-08-12 18:22:58.042921: Epoch time: 449.78 s
2024-08-12 18:22:58.043890: Yayy! New best EMA pseudo Dice: 0.6691
2024-08-12 18:22:59.886035: 
2024-08-12 18:22:59.887826: Epoch 22
2024-08-12 18:22:59.889132: Current learning rate: 0.0098
2024-08-12 18:31:26.498820: Validation loss did not improve from -0.46362. Patience: 2/50
2024-08-12 18:31:26.500460: train_loss -0.7016
2024-08-12 18:31:26.501813: val_loss -0.4384
2024-08-12 18:31:26.503072: Pseudo dice [0.6976]
2024-08-12 18:31:26.506484: Epoch time: 506.62 s
2024-08-12 18:31:26.508314: Yayy! New best EMA pseudo Dice: 0.6719
2024-08-12 18:31:28.327183: 
2024-08-12 18:31:28.328797: Epoch 23
2024-08-12 18:31:28.330013: Current learning rate: 0.00979
2024-08-12 18:39:38.046277: Validation loss did not improve from -0.46362. Patience: 3/50
2024-08-12 18:39:38.047837: train_loss -0.7036
2024-08-12 18:39:38.049052: val_loss -0.3812
2024-08-12 18:39:38.050283: Pseudo dice [0.6777]
2024-08-12 18:39:38.051567: Epoch time: 489.72 s
2024-08-12 18:39:38.052640: Yayy! New best EMA pseudo Dice: 0.6725
2024-08-12 18:39:39.797890: 
2024-08-12 18:39:39.799586: Epoch 24
2024-08-12 18:39:39.800676: Current learning rate: 0.00978
2024-08-12 18:48:06.116868: Validation loss did not improve from -0.46362. Patience: 4/50
2024-08-12 18:48:06.118346: train_loss -0.7045
2024-08-12 18:48:06.120345: val_loss -0.4484
2024-08-12 18:48:06.121742: Pseudo dice [0.7052]
2024-08-12 18:48:06.123543: Epoch time: 506.32 s
2024-08-12 18:48:06.506855: Yayy! New best EMA pseudo Dice: 0.6758
2024-08-12 18:48:08.399335: 
2024-08-12 18:48:08.400894: Epoch 25
2024-08-12 18:48:08.402009: Current learning rate: 0.00977
2024-08-12 18:56:36.571959: Validation loss did not improve from -0.46362. Patience: 5/50
2024-08-12 18:56:36.573411: train_loss -0.716
2024-08-12 18:56:36.574822: val_loss -0.4585
2024-08-12 18:56:36.576094: Pseudo dice [0.7102]
2024-08-12 18:56:36.577250: Epoch time: 508.18 s
2024-08-12 18:56:36.578327: Yayy! New best EMA pseudo Dice: 0.6792
2024-08-12 18:56:38.365162: 
2024-08-12 18:56:38.366778: Epoch 26
2024-08-12 18:56:38.367897: Current learning rate: 0.00977
2024-08-12 19:05:28.117642: Validation loss did not improve from -0.46362. Patience: 6/50
2024-08-12 19:05:28.118995: train_loss -0.7214
2024-08-12 19:05:28.120273: val_loss -0.3777
2024-08-12 19:05:28.121570: Pseudo dice [0.6727]
2024-08-12 19:05:28.122874: Epoch time: 529.76 s
2024-08-12 19:05:29.531806: 
2024-08-12 19:05:29.533759: Epoch 27
2024-08-12 19:05:29.535156: Current learning rate: 0.00976
2024-08-12 19:13:29.650845: Validation loss did not improve from -0.46362. Patience: 7/50
2024-08-12 19:13:29.654717: train_loss -0.7189
2024-08-12 19:13:29.692360: val_loss -0.4228
2024-08-12 19:13:29.694866: Pseudo dice [0.6986]
2024-08-12 19:13:29.699346: Epoch time: 480.12 s
2024-08-12 19:13:29.701432: Yayy! New best EMA pseudo Dice: 0.6806
2024-08-12 19:13:31.718755: 
2024-08-12 19:13:31.721267: Epoch 28
2024-08-12 19:13:31.722557: Current learning rate: 0.00975
2024-08-12 19:19:37.891130: Validation loss did not improve from -0.46362. Patience: 8/50
2024-08-12 19:19:37.892747: train_loss -0.729
2024-08-12 19:19:37.893969: val_loss -0.3681
2024-08-12 19:19:37.895121: Pseudo dice [0.6707]
2024-08-12 19:19:37.896486: Epoch time: 366.18 s
2024-08-12 19:19:39.366102: 
2024-08-12 19:19:39.368425: Epoch 29
2024-08-12 19:19:39.369912: Current learning rate: 0.00974
2024-08-12 19:28:24.805333: Validation loss did not improve from -0.46362. Patience: 9/50
2024-08-12 19:28:24.806976: train_loss -0.7321
2024-08-12 19:28:24.808271: val_loss -0.4034
2024-08-12 19:28:24.809516: Pseudo dice [0.6751]
2024-08-12 19:28:24.810746: Epoch time: 525.44 s
2024-08-12 19:28:26.712509: 
2024-08-12 19:28:26.714578: Epoch 30
2024-08-12 19:28:26.716316: Current learning rate: 0.00973
2024-08-12 19:35:36.030729: Validation loss did not improve from -0.46362. Patience: 10/50
2024-08-12 19:35:36.032296: train_loss -0.7356
2024-08-12 19:35:36.033659: val_loss -0.3877
2024-08-12 19:35:36.034748: Pseudo dice [0.6619]
2024-08-12 19:35:36.035888: Epoch time: 429.32 s
2024-08-12 19:35:37.441917: 
2024-08-12 19:35:37.443805: Epoch 31
2024-08-12 19:35:37.445233: Current learning rate: 0.00972
2024-08-12 19:43:54.195921: Validation loss did not improve from -0.46362. Patience: 11/50
2024-08-12 19:43:54.197515: train_loss -0.7372
2024-08-12 19:43:54.198971: val_loss -0.3858
2024-08-12 19:43:54.200372: Pseudo dice [0.6894]
2024-08-12 19:43:54.201880: Epoch time: 496.76 s
2024-08-12 19:43:56.087296: 
2024-08-12 19:43:56.089365: Epoch 32
2024-08-12 19:43:56.090737: Current learning rate: 0.00971
2024-08-12 19:49:44.497545: Validation loss did not improve from -0.46362. Patience: 12/50
2024-08-12 19:49:44.499597: train_loss -0.7407
2024-08-12 19:49:44.502595: val_loss -0.4074
2024-08-12 19:49:44.504895: Pseudo dice [0.6762]
2024-08-12 19:49:44.506797: Epoch time: 348.41 s
2024-08-12 19:49:45.912394: 
2024-08-12 19:49:45.913928: Epoch 33
2024-08-12 19:49:45.915266: Current learning rate: 0.0097
2024-08-12 19:56:14.944319: Validation loss did not improve from -0.46362. Patience: 13/50
2024-08-12 19:56:14.945576: train_loss -0.7405
2024-08-12 19:56:14.947406: val_loss -0.3034
2024-08-12 19:56:14.948750: Pseudo dice [0.647]
2024-08-12 19:56:14.949852: Epoch time: 389.04 s
2024-08-12 19:56:16.434224: 
2024-08-12 19:56:16.436255: Epoch 34
2024-08-12 19:56:16.437490: Current learning rate: 0.00969
2024-08-12 20:03:39.179952: Validation loss did not improve from -0.46362. Patience: 14/50
2024-08-12 20:03:39.181147: train_loss -0.7409
2024-08-12 20:03:39.182307: val_loss -0.3801
2024-08-12 20:03:39.183591: Pseudo dice [0.6659]
2024-08-12 20:03:39.184850: Epoch time: 442.75 s
2024-08-12 20:03:41.081713: 
2024-08-12 20:03:41.083530: Epoch 35
2024-08-12 20:03:41.084898: Current learning rate: 0.00968
2024-08-12 20:10:41.474968: Validation loss did not improve from -0.46362. Patience: 15/50
2024-08-12 20:10:41.476277: train_loss -0.7471
2024-08-12 20:10:41.477623: val_loss -0.4129
2024-08-12 20:10:41.478951: Pseudo dice [0.6966]
2024-08-12 20:10:41.480192: Epoch time: 420.4 s
2024-08-12 20:10:42.945663: 
2024-08-12 20:10:42.947688: Epoch 36
2024-08-12 20:10:42.949006: Current learning rate: 0.00968
2024-08-12 20:18:36.775109: Validation loss did not improve from -0.46362. Patience: 16/50
2024-08-12 20:18:36.777970: train_loss -0.75
2024-08-12 20:18:36.779516: val_loss -0.3909
2024-08-12 20:18:36.780794: Pseudo dice [0.6753]
2024-08-12 20:18:36.781936: Epoch time: 473.83 s
2024-08-12 20:18:38.244614: 
2024-08-12 20:18:38.246411: Epoch 37
2024-08-12 20:18:38.247592: Current learning rate: 0.00967
2024-08-12 20:24:37.993053: Validation loss did not improve from -0.46362. Patience: 17/50
2024-08-12 20:24:37.994533: train_loss -0.7504
2024-08-12 20:24:37.995969: val_loss -0.406
2024-08-12 20:24:37.997098: Pseudo dice [0.6869]
2024-08-12 20:24:37.998308: Epoch time: 359.75 s
2024-08-12 20:24:39.430927: 
2024-08-12 20:24:39.432422: Epoch 38
2024-08-12 20:24:39.433585: Current learning rate: 0.00966
2024-08-12 20:28:54.849870: Validation loss did not improve from -0.46362. Patience: 18/50
2024-08-12 20:28:54.851497: train_loss -0.7573
2024-08-12 20:28:54.853059: val_loss -0.424
2024-08-12 20:28:54.854558: Pseudo dice [0.6958]
2024-08-12 20:28:54.855710: Epoch time: 255.42 s
2024-08-12 20:28:56.353063: 
2024-08-12 20:28:56.354944: Epoch 39
2024-08-12 20:28:56.356629: Current learning rate: 0.00965
2024-08-12 20:30:37.931005: Validation loss improved from -0.46362 to -0.49507! Patience: 18/50
2024-08-12 20:30:37.932494: train_loss -0.7582
2024-08-12 20:30:37.933635: val_loss -0.4951
2024-08-12 20:30:37.934633: Pseudo dice [0.7325]
2024-08-12 20:30:37.935640: Epoch time: 101.58 s
2024-08-12 20:30:38.422163: Yayy! New best EMA pseudo Dice: 0.6846
2024-08-12 20:30:40.378238: 
2024-08-12 20:30:40.380229: Epoch 40
2024-08-12 20:30:40.381449: Current learning rate: 0.00964
2024-08-12 20:34:36.787528: Validation loss did not improve from -0.49507. Patience: 1/50
2024-08-12 20:34:36.789184: train_loss -0.7634
2024-08-12 20:34:36.790781: val_loss -0.4076
2024-08-12 20:34:36.791845: Pseudo dice [0.6859]
2024-08-12 20:34:36.792938: Epoch time: 236.41 s
2024-08-12 20:34:36.793949: Yayy! New best EMA pseudo Dice: 0.6847
2024-08-12 20:34:38.968267: 
2024-08-12 20:34:38.969887: Epoch 41
2024-08-12 20:34:38.971008: Current learning rate: 0.00963
2024-08-12 20:38:59.679054: Validation loss did not improve from -0.49507. Patience: 2/50
2024-08-12 20:38:59.681133: train_loss -0.7677
2024-08-12 20:38:59.682727: val_loss -0.3606
2024-08-12 20:38:59.690735: Pseudo dice [0.6708]
2024-08-12 20:38:59.692301: Epoch time: 260.71 s
2024-08-12 20:39:01.113125: 
2024-08-12 20:39:01.116476: Epoch 42
2024-08-12 20:39:01.118400: Current learning rate: 0.00962
2024-08-12 20:41:30.900117: Validation loss did not improve from -0.49507. Patience: 3/50
2024-08-12 20:41:30.901209: train_loss -0.7635
2024-08-12 20:41:30.902241: val_loss -0.3639
2024-08-12 20:41:30.903185: Pseudo dice [0.6754]
2024-08-12 20:41:30.904213: Epoch time: 149.79 s
2024-08-12 20:41:35.426736: 
2024-08-12 20:41:35.428087: Epoch 43
2024-08-12 20:41:35.429065: Current learning rate: 0.00961
2024-08-12 20:46:28.244703: Validation loss did not improve from -0.49507. Patience: 4/50
2024-08-12 20:46:28.246162: train_loss -0.7668
2024-08-12 20:46:28.247370: val_loss -0.3942
2024-08-12 20:46:28.248472: Pseudo dice [0.6779]
2024-08-12 20:46:28.249928: Epoch time: 292.82 s
2024-08-12 20:46:29.636989: 
2024-08-12 20:46:29.639528: Epoch 44
2024-08-12 20:46:29.641714: Current learning rate: 0.0096
2024-08-12 20:48:21.721377: Validation loss did not improve from -0.49507. Patience: 5/50
2024-08-12 20:48:21.722774: train_loss -0.7663
2024-08-12 20:48:21.724087: val_loss -0.4359
2024-08-12 20:48:21.725330: Pseudo dice [0.6939]
2024-08-12 20:48:21.726636: Epoch time: 112.09 s
2024-08-12 20:48:23.602783: 
2024-08-12 20:48:23.605572: Epoch 45
2024-08-12 20:48:23.607431: Current learning rate: 0.00959
2024-08-12 20:52:15.362981: Validation loss did not improve from -0.49507. Patience: 6/50
2024-08-12 20:52:15.364174: train_loss -0.7664
2024-08-12 20:52:15.365453: val_loss -0.3228
2024-08-12 20:52:15.366528: Pseudo dice [0.6516]
2024-08-12 20:52:15.367604: Epoch time: 231.76 s
2024-08-12 20:52:16.764078: 
2024-08-12 20:52:16.765452: Epoch 46
2024-08-12 20:52:16.766745: Current learning rate: 0.00959
2024-08-12 20:56:19.422534: Validation loss did not improve from -0.49507. Patience: 7/50
2024-08-12 20:56:19.423976: train_loss -0.7713
2024-08-12 20:56:19.425305: val_loss -0.466
2024-08-12 20:56:19.426684: Pseudo dice [0.7181]
2024-08-12 20:56:19.428334: Epoch time: 242.66 s
2024-08-12 20:56:20.829770: 
2024-08-12 20:56:20.832275: Epoch 47
2024-08-12 20:56:20.834345: Current learning rate: 0.00958
2024-08-12 20:58:54.641365: Validation loss did not improve from -0.49507. Patience: 8/50
2024-08-12 20:58:54.642789: train_loss -0.7751
2024-08-12 20:58:54.644221: val_loss -0.4455
2024-08-12 20:58:54.645536: Pseudo dice [0.7056]
2024-08-12 20:58:54.646773: Epoch time: 153.81 s
2024-08-12 20:58:54.647845: Yayy! New best EMA pseudo Dice: 0.6861
2024-08-12 20:58:56.422932: 
2024-08-12 20:58:56.424659: Epoch 48
2024-08-12 20:58:56.425863: Current learning rate: 0.00957
2024-08-12 21:03:50.266934: Validation loss did not improve from -0.49507. Patience: 9/50
2024-08-12 21:03:50.268164: train_loss -0.7761
2024-08-12 21:03:50.269608: val_loss -0.4054
2024-08-12 21:03:50.270829: Pseudo dice [0.715]
2024-08-12 21:03:50.271990: Epoch time: 293.85 s
2024-08-12 21:03:50.273036: Yayy! New best EMA pseudo Dice: 0.689
2024-08-12 21:03:52.084435: 
2024-08-12 21:03:52.087740: Epoch 49
2024-08-12 21:03:52.089168: Current learning rate: 0.00956
2024-08-12 21:05:45.615536: Validation loss did not improve from -0.49507. Patience: 10/50
2024-08-12 21:05:45.616956: train_loss -0.7785
2024-08-12 21:05:45.618482: val_loss -0.425
2024-08-12 21:05:45.619929: Pseudo dice [0.6998]
2024-08-12 21:05:45.621212: Epoch time: 113.53 s
2024-08-12 21:05:46.029547: Yayy! New best EMA pseudo Dice: 0.69
2024-08-12 21:05:47.893891: 
2024-08-12 21:05:47.895398: Epoch 50
2024-08-12 21:05:47.896574: Current learning rate: 0.00955
2024-08-12 21:09:55.440200: Validation loss did not improve from -0.49507. Patience: 11/50
2024-08-12 21:09:55.441602: train_loss -0.7808
2024-08-12 21:09:55.442937: val_loss -0.375
2024-08-12 21:09:55.444153: Pseudo dice [0.6753]
2024-08-12 21:09:55.445274: Epoch time: 247.55 s
2024-08-12 21:09:56.877803: 
2024-08-12 21:09:56.880301: Epoch 51
2024-08-12 21:09:56.881788: Current learning rate: 0.00954
2024-08-12 21:14:12.597322: Validation loss did not improve from -0.49507. Patience: 12/50
2024-08-12 21:14:12.598762: train_loss -0.7821
2024-08-12 21:14:12.600198: val_loss -0.2876
2024-08-12 21:14:12.601499: Pseudo dice [0.63]
2024-08-12 21:14:12.602852: Epoch time: 255.72 s
2024-08-12 21:14:14.164695: 
2024-08-12 21:14:14.166121: Epoch 52
2024-08-12 21:14:14.167347: Current learning rate: 0.00953
2024-08-12 21:16:16.470976: Validation loss did not improve from -0.49507. Patience: 13/50
2024-08-12 21:16:16.472243: train_loss -0.7787
2024-08-12 21:16:16.473481: val_loss -0.4443
2024-08-12 21:16:16.474756: Pseudo dice [0.7165]
2024-08-12 21:16:16.475948: Epoch time: 122.31 s
2024-08-12 21:16:17.973981: 
2024-08-12 21:16:17.976396: Epoch 53
2024-08-12 21:16:17.978824: Current learning rate: 0.00952
2024-08-12 21:20:41.426409: Validation loss improved from -0.49507 to -0.49552! Patience: 13/50
2024-08-12 21:20:41.428974: train_loss -0.7828
2024-08-12 21:20:41.430326: val_loss -0.4955
2024-08-12 21:20:41.431661: Pseudo dice [0.735]
2024-08-12 21:20:41.432774: Epoch time: 263.46 s
2024-08-12 21:20:41.433829: Yayy! New best EMA pseudo Dice: 0.691
2024-08-12 21:20:43.435895: 
2024-08-12 21:20:43.437359: Epoch 54
2024-08-12 21:20:43.438630: Current learning rate: 0.00951
2024-08-12 21:24:15.085630: Validation loss did not improve from -0.49552. Patience: 1/50
2024-08-12 21:24:15.087958: train_loss -0.7872
2024-08-12 21:24:15.090989: val_loss -0.3509
2024-08-12 21:24:15.092513: Pseudo dice [0.6731]
2024-08-12 21:24:15.094240: Epoch time: 211.65 s
2024-08-12 21:24:17.771540: 
2024-08-12 21:24:17.773283: Epoch 55
2024-08-12 21:24:17.774601: Current learning rate: 0.0095
2024-08-12 21:27:51.368463: Validation loss did not improve from -0.49552. Patience: 2/50
2024-08-12 21:27:51.369674: train_loss -0.7824
2024-08-12 21:27:51.370853: val_loss -0.3703
2024-08-12 21:27:51.371952: Pseudo dice [0.6746]
2024-08-12 21:27:51.373436: Epoch time: 213.6 s
2024-08-12 21:27:52.856184: 
2024-08-12 21:27:52.857840: Epoch 56
2024-08-12 21:27:52.859254: Current learning rate: 0.00949
2024-08-12 21:29:29.808031: Validation loss did not improve from -0.49552. Patience: 3/50
2024-08-12 21:29:29.810217: train_loss -0.7866
2024-08-12 21:29:29.812575: val_loss -0.3623
2024-08-12 21:29:29.814520: Pseudo dice [0.6732]
2024-08-12 21:29:29.815999: Epoch time: 96.96 s
2024-08-12 21:29:31.452522: 
2024-08-12 21:29:31.454232: Epoch 57
2024-08-12 21:29:31.455564: Current learning rate: 0.00949
2024-08-12 21:31:42.620554: Validation loss did not improve from -0.49552. Patience: 4/50
2024-08-12 21:31:42.622295: train_loss -0.7918
2024-08-12 21:31:42.624041: val_loss -0.4251
2024-08-12 21:31:42.625454: Pseudo dice [0.7062]
2024-08-12 21:31:42.626926: Epoch time: 131.17 s
2024-08-12 21:31:44.065799: 
2024-08-12 21:31:44.067531: Epoch 58
2024-08-12 21:31:44.068785: Current learning rate: 0.00948
2024-08-12 21:33:52.646344: Validation loss did not improve from -0.49552. Patience: 5/50
2024-08-12 21:33:52.647911: train_loss -0.7931
2024-08-12 21:33:52.649483: val_loss -0.4516
2024-08-12 21:33:52.650792: Pseudo dice [0.7285]
2024-08-12 21:33:52.651864: Epoch time: 128.58 s
2024-08-12 21:33:52.653218: Yayy! New best EMA pseudo Dice: 0.6923
2024-08-12 21:33:54.485475: 
2024-08-12 21:33:54.487289: Epoch 59
2024-08-12 21:33:54.488534: Current learning rate: 0.00947
2024-08-12 21:35:36.720204: Validation loss did not improve from -0.49552. Patience: 6/50
2024-08-12 21:35:36.721722: train_loss -0.7899
2024-08-12 21:35:36.723082: val_loss -0.444
2024-08-12 21:35:36.724204: Pseudo dice [0.7339]
2024-08-12 21:35:36.725311: Epoch time: 102.24 s
2024-08-12 21:35:37.140525: Yayy! New best EMA pseudo Dice: 0.6965
2024-08-12 21:35:39.002761: 
2024-08-12 21:35:39.004024: Epoch 60
2024-08-12 21:35:39.005040: Current learning rate: 0.00946
2024-08-12 21:38:12.824797: Validation loss did not improve from -0.49552. Patience: 7/50
2024-08-12 21:38:12.826190: train_loss -0.786
2024-08-12 21:38:12.827380: val_loss -0.4429
2024-08-12 21:38:12.828546: Pseudo dice [0.7095]
2024-08-12 21:38:12.829747: Epoch time: 153.83 s
2024-08-12 21:38:12.830787: Yayy! New best EMA pseudo Dice: 0.6978
2024-08-12 21:38:14.718138: 
2024-08-12 21:38:14.720003: Epoch 61
2024-08-12 21:38:14.721343: Current learning rate: 0.00945
2024-08-12 21:39:51.591975: Validation loss did not improve from -0.49552. Patience: 8/50
2024-08-12 21:39:51.593229: train_loss -0.7918
2024-08-12 21:39:51.594570: val_loss -0.34
2024-08-12 21:39:51.595894: Pseudo dice [0.6647]
2024-08-12 21:39:51.597017: Epoch time: 96.88 s
2024-08-12 21:39:53.130582: 
2024-08-12 21:39:53.131888: Epoch 62
2024-08-12 21:39:53.133170: Current learning rate: 0.00944
2024-08-12 21:41:47.898378: Validation loss did not improve from -0.49552. Patience: 9/50
2024-08-12 21:41:47.899860: train_loss -0.7952
2024-08-12 21:41:47.901266: val_loss -0.3485
2024-08-12 21:41:47.902506: Pseudo dice [0.6623]
2024-08-12 21:41:47.903780: Epoch time: 114.77 s
2024-08-12 21:41:49.376774: 
2024-08-12 21:41:49.378298: Epoch 63
2024-08-12 21:41:49.379369: Current learning rate: 0.00943
2024-08-12 21:44:11.542670: Validation loss did not improve from -0.49552. Patience: 10/50
2024-08-12 21:44:11.544137: train_loss -0.7984
2024-08-12 21:44:11.545316: val_loss -0.4364
2024-08-12 21:44:11.546499: Pseudo dice [0.7108]
2024-08-12 21:44:11.548077: Epoch time: 142.17 s
2024-08-12 21:44:13.083261: 
2024-08-12 21:44:13.084920: Epoch 64
2024-08-12 21:44:13.086001: Current learning rate: 0.00942
2024-08-12 21:45:49.948396: Validation loss did not improve from -0.49552. Patience: 11/50
2024-08-12 21:45:49.949656: train_loss -0.7967
2024-08-12 21:45:49.950902: val_loss -0.4555
2024-08-12 21:45:49.952203: Pseudo dice [0.7248]
2024-08-12 21:45:49.953359: Epoch time: 96.87 s
2024-08-12 21:45:51.798612: 
2024-08-12 21:45:51.800296: Epoch 65
2024-08-12 21:45:51.801364: Current learning rate: 0.00941
2024-08-12 21:47:59.755740: Validation loss did not improve from -0.49552. Patience: 12/50
2024-08-12 21:47:59.757288: train_loss -0.7967
2024-08-12 21:47:59.758445: val_loss -0.3877
2024-08-12 21:47:59.759467: Pseudo dice [0.7]
2024-08-12 21:47:59.760768: Epoch time: 127.96 s
2024-08-12 21:48:01.865740: 
2024-08-12 21:48:01.867337: Epoch 66
2024-08-12 21:48:01.868461: Current learning rate: 0.0094
2024-08-12 21:50:05.279958: Validation loss did not improve from -0.49552. Patience: 13/50
2024-08-12 21:50:05.281183: train_loss -0.7985
2024-08-12 21:50:05.282328: val_loss -0.4308
2024-08-12 21:50:05.283301: Pseudo dice [0.7064]
2024-08-12 21:50:05.284287: Epoch time: 123.42 s
2024-08-12 21:50:06.744715: 
2024-08-12 21:50:06.746367: Epoch 67
2024-08-12 21:50:06.747521: Current learning rate: 0.00939
2024-08-12 21:51:44.616460: Validation loss did not improve from -0.49552. Patience: 14/50
2024-08-12 21:51:44.617736: train_loss -0.7973
2024-08-12 21:51:44.619131: val_loss -0.3307
2024-08-12 21:51:44.620534: Pseudo dice [0.6571]
2024-08-12 21:51:44.621726: Epoch time: 97.87 s
2024-08-12 21:51:46.171556: 
2024-08-12 21:51:46.172833: Epoch 68
2024-08-12 21:51:46.173844: Current learning rate: 0.00939
2024-08-12 21:54:11.706150: Validation loss did not improve from -0.49552. Patience: 15/50
2024-08-12 21:54:11.707781: train_loss -0.7971
2024-08-12 21:54:11.764114: val_loss -0.3647
2024-08-12 21:54:11.765502: Pseudo dice [0.6856]
2024-08-12 21:54:11.766703: Epoch time: 145.54 s
2024-08-12 21:54:13.239065: 
2024-08-12 21:54:13.240469: Epoch 69
2024-08-12 21:54:13.241536: Current learning rate: 0.00938
2024-08-12 21:56:03.289541: Validation loss did not improve from -0.49552. Patience: 16/50
2024-08-12 21:56:03.291193: train_loss -0.7975
2024-08-12 21:56:03.295084: val_loss -0.4274
2024-08-12 21:56:03.296382: Pseudo dice [0.7141]
2024-08-12 21:56:03.297631: Epoch time: 110.05 s
2024-08-12 21:56:05.212668: 
2024-08-12 21:56:05.214701: Epoch 70
2024-08-12 21:56:05.215975: Current learning rate: 0.00937
2024-08-12 21:57:47.166668: Validation loss did not improve from -0.49552. Patience: 17/50
2024-08-12 21:57:47.168559: train_loss -0.7991
2024-08-12 21:57:47.170570: val_loss -0.434
2024-08-12 21:57:47.171843: Pseudo dice [0.714]
2024-08-12 21:57:47.173189: Epoch time: 101.96 s
2024-08-12 21:57:48.777970: 
2024-08-12 21:57:48.779371: Epoch 71
2024-08-12 21:57:48.780765: Current learning rate: 0.00936
2024-08-12 22:00:15.341210: Validation loss did not improve from -0.49552. Patience: 18/50
2024-08-12 22:00:15.342849: train_loss -0.807
2024-08-12 22:00:15.344709: val_loss -0.4633
2024-08-12 22:00:15.345954: Pseudo dice [0.7313]
2024-08-12 22:00:15.347353: Epoch time: 146.57 s
2024-08-12 22:00:15.348714: Yayy! New best EMA pseudo Dice: 0.7003
2024-08-12 22:00:17.348713: 
2024-08-12 22:00:17.351134: Epoch 72
2024-08-12 22:00:17.352914: Current learning rate: 0.00935
2024-08-12 22:01:59.772112: Validation loss did not improve from -0.49552. Patience: 19/50
2024-08-12 22:01:59.773242: train_loss -0.8074
2024-08-12 22:01:59.774312: val_loss -0.3304
2024-08-12 22:01:59.775267: Pseudo dice [0.6761]
2024-08-12 22:01:59.776310: Epoch time: 102.43 s
2024-08-12 22:02:01.293710: 
2024-08-12 22:02:01.295226: Epoch 73
2024-08-12 22:02:01.296266: Current learning rate: 0.00934
2024-08-12 22:03:49.377036: Validation loss did not improve from -0.49552. Patience: 20/50
2024-08-12 22:03:49.378406: train_loss -0.8092
2024-08-12 22:03:49.379567: val_loss -0.4873
2024-08-12 22:03:49.380526: Pseudo dice [0.7397]
2024-08-12 22:03:49.381585: Epoch time: 108.09 s
2024-08-12 22:03:49.382671: Yayy! New best EMA pseudo Dice: 0.702
2024-08-12 22:03:51.266879: 
2024-08-12 22:03:51.268429: Epoch 74
2024-08-12 22:03:51.269619: Current learning rate: 0.00933
2024-08-12 22:06:24.268359: Validation loss did not improve from -0.49552. Patience: 21/50
2024-08-12 22:06:24.269919: train_loss -0.8099
2024-08-12 22:06:24.299780: val_loss -0.4003
2024-08-12 22:06:24.300828: Pseudo dice [0.6981]
2024-08-12 22:06:24.301818: Epoch time: 153.0 s
2024-08-12 22:06:26.231339: 
2024-08-12 22:06:26.233032: Epoch 75
2024-08-12 22:06:26.234251: Current learning rate: 0.00932
2024-08-12 22:08:03.416394: Validation loss did not improve from -0.49552. Patience: 22/50
2024-08-12 22:08:03.417828: train_loss -0.8078
2024-08-12 22:08:03.418946: val_loss -0.2977
2024-08-12 22:08:03.419893: Pseudo dice [0.6435]
2024-08-12 22:08:03.420899: Epoch time: 97.19 s
2024-08-12 22:08:04.914881: 
2024-08-12 22:08:04.916272: Epoch 76
2024-08-12 22:08:04.917503: Current learning rate: 0.00931
2024-08-12 22:10:18.052707: Validation loss did not improve from -0.49552. Patience: 23/50
2024-08-12 22:10:18.054227: train_loss -0.8038
2024-08-12 22:10:18.055480: val_loss -0.3952
2024-08-12 22:10:18.056595: Pseudo dice [0.6889]
2024-08-12 22:10:18.057597: Epoch time: 133.14 s
2024-08-12 22:10:19.977962: 
2024-08-12 22:10:19.979346: Epoch 77
2024-08-12 22:10:19.980516: Current learning rate: 0.0093
2024-08-12 22:12:24.912848: Validation loss did not improve from -0.49552. Patience: 24/50
2024-08-12 22:12:24.914018: train_loss -0.8097
2024-08-12 22:12:24.915266: val_loss -0.449
2024-08-12 22:12:24.916301: Pseudo dice [0.7392]
2024-08-12 22:12:24.917412: Epoch time: 124.94 s
2024-08-12 22:12:26.420829: 
2024-08-12 22:12:26.422507: Epoch 78
2024-08-12 22:12:26.423634: Current learning rate: 0.0093
2024-08-12 22:14:17.216544: Validation loss did not improve from -0.49552. Patience: 25/50
2024-08-12 22:14:17.218559: train_loss -0.8083
2024-08-12 22:14:17.224344: val_loss -0.4082
2024-08-12 22:14:17.231331: Pseudo dice [0.7101]
2024-08-12 22:14:17.233500: Epoch time: 110.8 s
2024-08-12 22:14:18.806052: 
2024-08-12 22:14:18.808321: Epoch 79
2024-08-12 22:14:18.809584: Current learning rate: 0.00929
2024-08-12 22:16:39.975924: Validation loss did not improve from -0.49552. Patience: 26/50
2024-08-12 22:16:39.977649: train_loss -0.8111
2024-08-12 22:16:39.979389: val_loss -0.4393
2024-08-12 22:16:39.980749: Pseudo dice [0.7249]
2024-08-12 22:16:39.982182: Epoch time: 141.18 s
2024-08-12 22:16:40.406272: Yayy! New best EMA pseudo Dice: 0.703
2024-08-12 22:16:42.412633: 
2024-08-12 22:16:42.414532: Epoch 80
2024-08-12 22:16:42.415829: Current learning rate: 0.00928
2024-08-12 22:18:38.996941: Validation loss did not improve from -0.49552. Patience: 27/50
2024-08-12 22:18:38.998053: train_loss -0.8129
2024-08-12 22:18:38.999093: val_loss -0.412
2024-08-12 22:18:39.000080: Pseudo dice [0.7175]
2024-08-12 22:18:39.001151: Epoch time: 116.59 s
2024-08-12 22:18:39.002077: Yayy! New best EMA pseudo Dice: 0.7045
2024-08-12 22:18:40.950946: 
2024-08-12 22:18:40.952491: Epoch 81
2024-08-12 22:18:40.953693: Current learning rate: 0.00927
2024-08-12 22:20:22.123406: Validation loss did not improve from -0.49552. Patience: 28/50
2024-08-12 22:20:22.124988: train_loss -0.8131
2024-08-12 22:20:22.126631: val_loss -0.4219
2024-08-12 22:20:22.128182: Pseudo dice [0.7053]
2024-08-12 22:20:22.129738: Epoch time: 101.18 s
2024-08-12 22:20:22.131245: Yayy! New best EMA pseudo Dice: 0.7046
2024-08-12 22:20:24.049711: 
2024-08-12 22:20:24.051395: Epoch 82
2024-08-12 22:20:24.052681: Current learning rate: 0.00926
2024-08-12 22:22:56.195613: Validation loss did not improve from -0.49552. Patience: 29/50
2024-08-12 22:22:56.196685: train_loss -0.8133
2024-08-12 22:22:56.197922: val_loss -0.4233
2024-08-12 22:22:56.199005: Pseudo dice [0.7179]
2024-08-12 22:22:56.200152: Epoch time: 152.15 s
2024-08-12 22:22:56.201321: Yayy! New best EMA pseudo Dice: 0.7059
2024-08-12 22:22:58.089319: 
2024-08-12 22:22:58.091062: Epoch 83
2024-08-12 22:22:58.092409: Current learning rate: 0.00925
2024-08-12 22:24:38.397592: Validation loss did not improve from -0.49552. Patience: 30/50
2024-08-12 22:24:38.409723: train_loss -0.8148
2024-08-12 22:24:38.410914: val_loss -0.41
2024-08-12 22:24:38.412186: Pseudo dice [0.7144]
2024-08-12 22:24:38.413330: Epoch time: 100.32 s
2024-08-12 22:24:38.414456: Yayy! New best EMA pseudo Dice: 0.7067
2024-08-12 22:24:40.501606: 
2024-08-12 22:24:40.503084: Epoch 84
2024-08-12 22:24:40.504156: Current learning rate: 0.00924
2024-08-12 22:26:33.326019: Validation loss did not improve from -0.49552. Patience: 31/50
2024-08-12 22:26:33.327500: train_loss -0.8122
2024-08-12 22:26:33.328778: val_loss -0.3686
2024-08-12 22:26:33.329891: Pseudo dice [0.6896]
2024-08-12 22:26:33.331144: Epoch time: 112.83 s
2024-08-12 22:26:35.120267: 
2024-08-12 22:26:35.121806: Epoch 85
2024-08-12 22:26:35.123089: Current learning rate: 0.00923
2024-08-12 22:28:55.039316: Validation loss did not improve from -0.49552. Patience: 32/50
2024-08-12 22:28:55.040965: train_loss -0.8145
2024-08-12 22:28:55.042209: val_loss -0.3675
2024-08-12 22:28:55.043224: Pseudo dice [0.7001]
2024-08-12 22:28:55.044410: Epoch time: 139.92 s
2024-08-12 22:28:56.483677: 
2024-08-12 22:28:56.485189: Epoch 86
2024-08-12 22:28:56.486281: Current learning rate: 0.00922
2024-08-12 22:30:33.694291: Validation loss did not improve from -0.49552. Patience: 33/50
2024-08-12 22:30:33.695691: train_loss -0.8149
2024-08-12 22:30:33.697415: val_loss -0.3805
2024-08-12 22:30:33.698881: Pseudo dice [0.6949]
2024-08-12 22:30:33.700238: Epoch time: 97.21 s
2024-08-12 22:30:35.190489: 
2024-08-12 22:30:35.192110: Epoch 87
2024-08-12 22:30:35.202698: Current learning rate: 0.00921
2024-08-12 22:32:27.764309: Validation loss did not improve from -0.49552. Patience: 34/50
2024-08-12 22:32:27.765623: train_loss -0.8155
2024-08-12 22:32:27.766779: val_loss -0.3614
2024-08-12 22:32:27.767858: Pseudo dice [0.6802]
2024-08-12 22:32:27.768884: Epoch time: 112.58 s
2024-08-12 22:32:30.957788: 
2024-08-12 22:32:30.960073: Epoch 88
2024-08-12 22:32:30.961995: Current learning rate: 0.0092
2024-08-12 22:34:30.856873: Validation loss did not improve from -0.49552. Patience: 35/50
2024-08-12 22:34:30.858675: train_loss -0.8208
2024-08-12 22:34:30.860242: val_loss -0.3709
2024-08-12 22:34:30.862038: Pseudo dice [0.6996]
2024-08-12 22:34:30.863575: Epoch time: 119.9 s
2024-08-12 22:34:32.964506: 
2024-08-12 22:34:32.966150: Epoch 89
2024-08-12 22:34:32.967321: Current learning rate: 0.0092
2024-08-12 22:36:09.570576: Validation loss did not improve from -0.49552. Patience: 36/50
2024-08-12 22:36:09.572164: train_loss -0.8203
2024-08-12 22:36:09.573524: val_loss -0.4549
2024-08-12 22:36:09.574909: Pseudo dice [0.7295]
2024-08-12 22:36:09.576207: Epoch time: 96.61 s
2024-08-12 22:36:11.476758: 
2024-08-12 22:36:11.478537: Epoch 90
2024-08-12 22:36:11.480150: Current learning rate: 0.00919
2024-08-12 22:37:59.115387: Validation loss did not improve from -0.49552. Patience: 37/50
2024-08-12 22:37:59.116833: train_loss -0.8132
2024-08-12 22:37:59.118859: val_loss -0.4476
2024-08-12 22:37:59.120596: Pseudo dice [0.7304]
2024-08-12 22:37:59.122241: Epoch time: 107.64 s
2024-08-12 22:38:00.580162: 
2024-08-12 22:38:00.581839: Epoch 91
2024-08-12 22:38:00.583372: Current learning rate: 0.00918
2024-08-12 22:40:04.279856: Validation loss did not improve from -0.49552. Patience: 38/50
2024-08-12 22:40:04.281297: train_loss -0.816
2024-08-12 22:40:04.282760: val_loss -0.3436
2024-08-12 22:40:04.284023: Pseudo dice [0.7038]
2024-08-12 22:40:04.285819: Epoch time: 123.7 s
2024-08-12 22:40:05.843032: 
2024-08-12 22:40:05.844294: Epoch 92
2024-08-12 22:40:05.845595: Current learning rate: 0.00917
2024-08-12 22:41:46.786341: Validation loss did not improve from -0.49552. Patience: 39/50
2024-08-12 22:41:46.787691: train_loss -0.8206
2024-08-12 22:41:46.789171: val_loss -0.3042
2024-08-12 22:41:46.790437: Pseudo dice [0.669]
2024-08-12 22:41:46.791682: Epoch time: 100.95 s
2024-08-12 22:41:48.197106: 
2024-08-12 22:41:48.198989: Epoch 93
2024-08-12 22:41:48.200083: Current learning rate: 0.00916
2024-08-12 22:43:24.888158: Validation loss did not improve from -0.49552. Patience: 40/50
2024-08-12 22:43:24.889292: train_loss -0.8212
2024-08-12 22:43:24.890512: val_loss -0.3804
2024-08-12 22:43:24.891578: Pseudo dice [0.6935]
2024-08-12 22:43:24.893068: Epoch time: 96.69 s
2024-08-12 22:43:26.369058: 
2024-08-12 22:43:26.370402: Epoch 94
2024-08-12 22:43:26.371507: Current learning rate: 0.00915
2024-08-12 22:45:26.641659: Validation loss did not improve from -0.49552. Patience: 41/50
2024-08-12 22:45:26.643194: train_loss -0.8241
2024-08-12 22:45:26.644581: val_loss -0.4324
2024-08-12 22:45:26.645962: Pseudo dice [0.7184]
2024-08-12 22:45:26.647390: Epoch time: 120.28 s
2024-08-12 22:45:28.537413: 
2024-08-12 22:45:28.538904: Epoch 95
2024-08-12 22:45:28.540022: Current learning rate: 0.00914
2024-08-12 22:47:14.311404: Validation loss did not improve from -0.49552. Patience: 42/50
2024-08-12 22:47:14.312796: train_loss -0.8232
2024-08-12 22:47:14.314224: val_loss -0.3885
2024-08-12 22:47:14.315707: Pseudo dice [0.7099]
2024-08-12 22:47:14.317343: Epoch time: 105.78 s
2024-08-12 22:47:15.728822: 
2024-08-12 22:47:15.730191: Epoch 96
2024-08-12 22:47:15.731308: Current learning rate: 0.00913
2024-08-12 22:48:52.899685: Validation loss did not improve from -0.49552. Patience: 43/50
2024-08-12 22:48:52.900860: train_loss -0.8273
2024-08-12 22:48:52.901997: val_loss -0.3995
2024-08-12 22:48:52.902950: Pseudo dice [0.7079]
2024-08-12 22:48:52.903882: Epoch time: 97.17 s
2024-08-12 22:48:54.351896: 
2024-08-12 22:48:54.354818: Epoch 97
2024-08-12 22:48:54.356517: Current learning rate: 0.00912
2024-08-12 22:50:49.238266: Validation loss did not improve from -0.49552. Patience: 44/50
2024-08-12 22:50:49.239893: train_loss -0.8275
2024-08-12 22:50:49.241010: val_loss -0.3907
2024-08-12 22:50:49.242038: Pseudo dice [0.7103]
2024-08-12 22:50:49.243086: Epoch time: 114.89 s
2024-08-12 22:50:50.689886: 
2024-08-12 22:50:50.692555: Epoch 98
2024-08-12 22:50:50.694283: Current learning rate: 0.00911
2024-08-12 22:52:39.982819: Validation loss did not improve from -0.49552. Patience: 45/50
2024-08-12 22:52:39.984993: train_loss -0.8273
2024-08-12 22:52:39.986278: val_loss -0.416
2024-08-12 22:52:39.987291: Pseudo dice [0.7185]
2024-08-12 22:52:39.988232: Epoch time: 109.3 s
2024-08-12 22:52:41.420557: 
2024-08-12 22:52:41.422161: Epoch 99
2024-08-12 22:52:41.423574: Current learning rate: 0.0091
2024-08-12 22:54:17.474601: Validation loss did not improve from -0.49552. Patience: 46/50
2024-08-12 22:54:17.475782: train_loss -0.8237
2024-08-12 22:54:17.476945: val_loss -0.3824
2024-08-12 22:54:17.477886: Pseudo dice [0.7068]
2024-08-12 22:54:17.478864: Epoch time: 96.06 s
2024-08-12 22:54:19.812965: 
2024-08-12 22:54:19.814365: Epoch 100
2024-08-12 22:54:19.815383: Current learning rate: 0.0091
2024-08-12 22:55:56.133465: Validation loss did not improve from -0.49552. Patience: 47/50
2024-08-12 22:55:56.134712: train_loss -0.8271
2024-08-12 22:55:56.135846: val_loss -0.3873
2024-08-12 22:55:56.136814: Pseudo dice [0.7095]
2024-08-12 22:55:56.137845: Epoch time: 96.32 s
2024-08-12 22:55:57.598338: 
2024-08-12 22:55:57.599990: Epoch 101
2024-08-12 22:55:57.601203: Current learning rate: 0.00909
2024-08-12 22:57:34.075993: Validation loss did not improve from -0.49552. Patience: 48/50
2024-08-12 22:57:34.077554: train_loss -0.8278
2024-08-12 22:57:34.078894: val_loss -0.3378
2024-08-12 22:57:34.080154: Pseudo dice [0.6977]
2024-08-12 22:57:34.081224: Epoch time: 96.48 s
2024-08-12 22:57:35.523296: 
2024-08-12 22:57:35.524880: Epoch 102
2024-08-12 22:57:35.526090: Current learning rate: 0.00908
2024-08-12 22:59:10.937967: Validation loss did not improve from -0.49552. Patience: 49/50
2024-08-12 22:59:10.939581: train_loss -0.8282
2024-08-12 22:59:10.940918: val_loss -0.3814
2024-08-12 22:59:10.942055: Pseudo dice [0.7044]
2024-08-12 22:59:10.943154: Epoch time: 95.42 s
2024-08-12 22:59:12.459469: 
2024-08-12 22:59:12.461269: Epoch 103
2024-08-12 22:59:12.462514: Current learning rate: 0.00907
2024-08-12 23:00:48.516522: Validation loss did not improve from -0.49552. Patience: 50/50
2024-08-12 23:00:48.517844: train_loss -0.8271
2024-08-12 23:00:48.519172: val_loss -0.3245
2024-08-12 23:00:48.520297: Pseudo dice [0.6764]
2024-08-12 23:00:48.521435: Epoch time: 96.06 s
2024-08-12 23:00:49.961084: Patience reached. Stopping training.
2024-08-12 23:00:50.479147: Training done.
